"Few sights and sounds are as emblematic of the North American southwest as a defensive rattlesnake, reared up, buzzing, and ready to strike. The message is loud and clear, “Back off! If you don’t hurt me, I won’t hurt you.” Any intruders who fail to heed the warning can expect to fall victim to a venomous bite.  But the consequences of that bite are surprisingly unpredictable. Snake venoms are complex cocktails made up of dozens of individual toxins that attack different parts of the target’s body. The composition of these cocktails is highly variable, even within single species. Biologists have come to assume that most of this variation reflects adaptation to what prey the snakes eat in the wild. But our study of the Mohave rattlesnake (Crotalus scutulatus, also known as the Mojave rattlesnake) has uncovered an intriguing exception to this rule. A 20-minute drive can take you from a population of this rattlesnake species with a highly lethal neurotoxic venom, causing paralysis and shock, to one with a haemotoxic venom, causing swelling, bruising, blistering and bleeding. The neurotoxic venom (known as venom A) can be more than ten times as lethal as the haemotoxic venom (venom B), at least to lab mice.  The Mohave rattlesnake is not alone in having different venoms like this – several other rattlesnake species display the same variation. But why do we see these differences? Snake venom evolved to subdue and kill prey. One venom may be better at killing one prey species, while another may be more toxic to different prey. Natural selection should favour different venoms in snakes eating different prey – it’s a classic example of evolution through natural selection. This idea that snake venom varies due to adaptation to eating different prey has become widely accepted among herpetologists and toxinologists. Some have found correlations between venom and prey. Others have shown prey-specific lethality of venoms, or identified toxins fine-tuned for killing the snakes’ natural prey. The venom of some snakes even changes along with their diet as they grow.  We expected the Mohave rattlesnake to be a prime example of this phenomenon. The extreme differences in venom composition, toxicity and mode of action (whether it is neurotoxic or haemotoxic) seem an obvious target for natural selection for different prey. And yet, when we correlated differences in venom composition with regional diet, we were shocked to find there is no link. In the absence of adaptation to local diet, we expected to see a connection between gene flow (transfer of genetic material between populations) and venom composition. Populations with ample gene flow would be expected to have more similar venoms than populations that are genetically less connected. But once again, we drew a blank – there is no link between gene flow and venom. This finding, together with the geographic segregation of the two populations with different venoms, suggests that instead there is strong local selection for venom type.  The next step in our research was to test for links between venom and the physical environment. Finally, we found some associations. The haemotoxic venom is found in rattlesnakes which live in an area which experiences warmer temperatures and more consistently low rainfall compared to where the rattlesnakes with the neurotoxic venom are found. But even this finding is deeply puzzling.  It has been suggested that, as well as killing prey, venom may also help digestion. Rattlesnakes eat large prey in one piece, and then have to digest it in a race against decay. A venom that starts predigesting the prey from the inside could help, especially in cooler climates where digestion is more difficult.  But the rattlesnakes with haemotoxic venom B, which better aids digestion, are found in warmer places, while snakes from cooler upland deserts invariably produce the non-digestive, neurotoxic venom A. Yet again, none of the conventional explanations make sense. Clearly, the selective forces behind the extreme venom variation in the Mohave rattlesnake are complex and subtle. A link to diet may yet be found, perhaps through different kinds of venom resistance in key prey species, or prey dynamics affected by local climate. In any case, our results reopen the discussion on the drivers of venom composition, and caution against the simplistic assumption that all venom variation is driven by the species composition of regional diets. From a human perspective, variation in venom composition is the bane of anyone working on snakebite treatments, or antidote development. It can lead to unexpected symptoms, and antivenoms may not work against some populations of a species they supposedly cover. Anyone living within the range of the Mohave rattlesnake can rest easy though – the available antivenoms cover both main venom types.  Globally, however, our study underlines the unpredictability of venom variation, and shows again that there are no shortcuts to understanding it. Those developing antivenoms need to identify regional venom variants and carry out extensive testing to ensure that their products are effective against all intended venoms."
"

With friends like Jerome R. Corsi, the American free‐​enterprise system is in more trouble than we feared.



Corsi should be on the right side of the battle to defend economic freedom. Before the 2008 election, he wrote a popular book with a great title, _The Obama Nation_ , warning that Barack Obama was not the man we needed in the White House. But in his latest book, _America for Sale: Fighting the New World Order, Surviving a Global Depression, and Preserving USA Sovereignty_ , he sounds a lot more like Obama than like Ronald Reagan, the former president he professes to admire. 



While Corsi takes passing shots at government spending and climate‐​change legislation, his real target is “free trade,” “globalism,” and international agreements such as NAFTA and the World Trade Organization. The villains in his book are trade agreements, trade deficits, growing Chinese foreign‐​currency reserves, and oil imports, all of which threaten to undermine the U.S. dollar, American independence, and the middle class. Orchestrating our decline is a cast of characters from around the globe, both familiar and obscure, working in public and in secret.



Boosted by a friendly Fox News interview with Sean Hannity in October, the book has been selling reasonably well. The people buying it are presumably the same conservative‐​leaning folks who opposed Obama’s election and are now fueling the Tea Party movement. Libertarians and conservatives who value free markets and limited government, however, should keep their distance from this bestselling prophet.



Coming from an author who claims to be a friend of free enterprise, and who earned a Ph.D. in political science from Harvard in 1972, the book is striking in its economic ignorance. It would fail an economics course at a community college. It repeats just about every anti‐​trade cliché that has been uttered by the AFL-CIO, Public Citizen, and Sen. Bernie Sanders (Socialist, Vt.).



The only “invisible hand” to be found in this book is that of an international conspiracy to sell off our national assets, sell out our sovereignty, and abolish the dollar. The “globalists” behind this movement are mostly academics and former officials who don’t run in current circles of power, such as Peter Drucker, Benn Steil, Herbert Grubel, Nouriel Roubini, Robert Mundell, George Soros, Henry Kissinger, Zbigniew Brzezinski, and David Rockefeller. Of course, former officials and academics are putting forward all sorts of ideas every week, some sensible, others far‐​fetched. A string of quotes from secondary players doesn’t prove that an idea is about to be foisted upon us.



Of special interest to Corsi is the plan to construct a NAFTA transportation corridor from Texas through the nation’s midsection. To demonstrate that such a scheme exists, he simply describes various plans to improve infrastructure in North America to accommodate increased freight traffic caused by expanding trade. As evidence that something nefarious is going on, he quotes an official with the Canadian National Railroad, who lets slip that his company is “now positioned to provide shippers with a seamless door‐​to‐​door transportation solution and to ensure the safe and secure flow of goods throughout the North American continent.” Isn’t that exactly what transportation companies should be doing?



Corsi tries to tarnish NAFTA further with arguments that sound like they could have come from Dennis Kucinich. Corsi cites a recent study by the North American Center for Transborder Studies that claims that 40 million jobs were created in the United States, Canada, and Mexico between 1993 (the year NAFTA passed) and 2007. Expressing skepticism, he writes, “Typically, the study failed to articulate the methodology by which these job estimates were derived, nor did it indicate whether the job creation was a net 40 million, after taking into account the jobs lost from NAFTA or other free‐​trade agreements such as those under the World Trade Organization.”



There is nothing mysterious about the methodology. The figure comes from comparing the number of people employed in each country in 2007 to the number employed in 1993. Those numbers are readily available on the Internet from public sources. From 1993 to 2007, employment in the United States grew from 120 to 146 million, in Mexico from 31 to 42 million, and in Canada from 13 to 17 million. Simple subtraction will tell us that the net number of jobs added in the United States after NAFTA was 26 million, in Mexico 11 million, and in Canada 4 million, and simple addition will give us the total of 41 million.



Instead of performing basic arithmetic, Corsi cites the growth in America’s bilateral trade deficits with both Canada and Mexico since NAFTA. He uncritically swallows the formula of the labor‐​union Left that trade deficits by definition mean net job losses, even though this is clearly not the case. This faulty premise leads Corsi to conclude, “What these data suggest is that the net new jobs created under NAFTA in North America are likely being created in Mexico and Canada, not the United States.” In fact, as the real employment numbers show, the United States accounted for the large majority of the net new jobs created in North America since NAFTA.



Corsi often gets even simple facts wrong. Take, for example, this passage:



What is clear is that the United States has been losing manufacturing jobs steadily since the end of World War II. In 1945, at the conclusion of the war, the service industries accounted for only 10 percent of nonfarm employment, compared to 38 percent for manufacturing, according to the U.S. Bureau of Labor Statistics. The crossover point came in 1982, when for the first time services surpassed manufacturing as the largest employer among major industry groups. By 1996, services accounted for 29 percent of nonfarm employment, and manufacturing, at 15 percent, had reduced to being somewhat smaller than retail trade. By 2008, manufacturing was less than 10 percent of nonfarm employment, and service‐​producing employment had risen to approximately 84 percent.



The number of manufacturing jobs was actually rising until the late 1970s, from 15 million in 1960 to a peak of 20 million in 1979. It has indeed been generally declining since then, but not steadily. Net U.S. manufacturing employment actually rose by 700,000 in the first half decade after the passage of NAFTA before resuming its decline in 2000.



Also, if we accept Corsi’s numbers at face value, more than half the nonfarm U.S. labor force shifted to the service sector over the span of a dozen years (from 29 percent in 1996 to 84 percent in 2006) — which would be perhaps the most radical economic transformation in such a short period of any country in history. But according to BLS and Census Bureau data I cite in my own recent book, _Mad about Trade: Why Main Street America Should Embrace Globalization_ , a majority of Americans were working in the service sector by the end of the 1920s. In fact, there has never been a time in our history when manufacturing workers outnumbered service‐​sector workers, and thus there was no “crossover,” much less a revolution on the scale Corsi claims. 



Corsi repeats the union mantra that globalization has caused the loss of high‐​paying manufacturing jobs in exchange for low‐​paying service jobs. He states, without any citation, that globalism means “American workers must exchange manufacturing jobs paying in excess of $35 an hour for service jobs paying $10 to $15 an hour.” In fact, as I document in _Mad about Trade_ , two‐​thirds of the jobs our economy added in the past two decades have been in the service fields of health care, education, and business and professional services — all of which pay higher wages on average than does manufacturing. Despite the nostalgia for manufacturing work, the American middle class today earns its keep in the service sector.



The examples of sloppy scholarship just keep coming. Corsi repeatedly describes the recent economic downturn as “the U.S. recession that officially began in December 2008.” There is no definition of “recession” under which this is true. The National Bureau of Economic Research, the accepted authority on the U.S. business cycle, puts the data a year earlier; that is when employment and industrial output began to fall. 



To date the recession to December of 2008 — the month after Obama’s election — Corsi accepts the informal definition of a recession as two consecutive quarters of negative GDP growth, and then defines the beginning of a recession as the end of the second consecutive quarter. This makes no sense: Under this definition, whenever there are two and only two consecutive quarters of negative growth, a recession begins and ends on the same day. Using the real two‐​consecutive‐​quarters definition — placing the beginning of the recession at the point at which the negative growth started — the recession started in July 2008, not December.



The errors don’t end there. Corsi writes, “Between December 2008, the date the recession officially began, and February 2009, the U.S. lost approximately 4,384,000 jobs.” Actually, the BLS data show a loss of 2.1 million jobs during those three months. The U.S. economy also lost nearly 2.3 million net jobs in the year leading up to December 2008, which Corsi apparently just added in.



There’s more. Corsi writes that “China’s economy, heavily dependent on making cheap goods for the U.S. market, was cast into its own deep recession by the U.S. economic downturn.” In reality, China didn’t experience a recession, much less a deep one. According to the generally accepted figures from China’s National Bureau of Statistics, its economy grew 12 percent in 2007, 9 percent in 2008, and at an annual rate of 7.6 percent through the first three quarters of 2009.



He writes that our soaring trade deficit with China “reflects imports from China growing nearly 250 percent, from $100.1 [b]illion in 2000 to $243.5 [billion] in 2005.” Actually, while the latter number is about 250 percent of the former, the former only grew about 150 percent. He outdoes himself on page 182, declaring that “the U.S. negative trade balance with China in 1985 was under $1 billion; in 2008, the U.S. negative trade balance with China had grown more than 250 percent, to a negative $266 billion.” An increase from 1 to 266 would be an increase not of 266 percent, but of 26,500 percent (or 266 _times_ ).



Granted, many math‐​challenged adults and journalists struggle with percentages, but then again, those same adults do not pose as experts qualified to write books on global trade and finance. And typos and random errors creep into many books, but serious books by serious authors do not contain such widespread, obvious, and systematically biased errors as those teeming in America for Sale.



 _America for Sale_ is not a complete loss. In a chapter titled “The Mortgage Bubble Bursts,” Corsi names many of the right names: a Federal Reserve Board that kept rates too low for too long, the Community Reinvestment Act, Fannie Mae and Freddie Mac, and the abuses of many subprime‐​mortgage lenders. But even this section is not original; other authors have ably made the same case, including my Cato colleague Johan Norberg in his recent book _Financial Fiasco_. And nothing in this section implicates globalism, free trade, China, or the WTO as a cause of the recession.



In the following chapter, he argues, quite reasonably, that Americans could reduce their reliance on imported oil by more aggressively developing domestic sources. He expresses skepticism toward renewable and other alternative energy sources, and describes in some detail potential domestic gas and oil fields that could yield significant amounts of energy. But he never attempts to analyze what a dramatic ramping up of domestic oil and gas production would mean for oil prices, the dollar, and our living standards. For example, a sharp drop in oil imports would mean fewer U.S. dollars flowing into international exchange markets, a stronger dollar, and relatively fewer exports of U.S. manufactured or agricultural goods.



Corsi invokes the name of Ronald Reagan, but his book could not be farther from the spirit of Reagan when it comes to our economic engagement in the world and our future as a nation. As president, Reagan embraced the idea of a North American free‐​trade area and moved it closer to reality by signing a free‐​trade agreement with Canada in 1988. As far back as 1980, Reagan talked about joining the United States with Mexico and other countries of Latin America in a hemispheric free‐​trade zone. Reagan’s able U.S. trade representative, Clayton Yeutter, was instrumental in launching the Uruguay Round in 1986, which led to the founding of the World Trade Organization in 1995. In his farewell address, Reagan shared his vision of America as a shining city on a hill “teeming with people of all kinds living in harmony and peace; a city with free ports that hummed with commerce and creativity. And if there had to be city walls, the walls had doors and the doors were open to anyone with the will and heart to get here.”



If Jerome R. Corsi is now the conservative defender of free enterprise and the America Way, we are in deeper trouble than the Gipper ever imagined.
"
"For nearly 40 years, black coal has been mined at Myuna, an underground operation a short drive south-west of Newcastle. Each year about 2 million tonnes is dug up, dropped on to an overland conveyor and sent to the Eraring power plant next door to be burned. Although the New South Wales mine isn’t new, its operation under owner Centennial Coal has changed over the past couple of years, leading to a dramatic increase in greenhouse gas escaping its coal seams.  Emissions at the mine in 2017-18 were 65% above the government-agreed limit for the site. New data published just before Christmas show Centennial was also in breach last financial year, with carbon pollution at Myuna 47% above its limit. In an era in which political battles are fought over how to meet climate targets, an emissions rise of this proportion – nearly half a million tonnes at one site over just a couple of years – is noteworthy. Along with similar examples at other industrial sites, including those owned by BHP, Chevron and a range of other fossil-fuel companies, it helps explain why official data says the Morrison government will fail to cut emissions to 5% below 2000 levels in 2020 as it claims. It was not supposed to be this way. Among the issues on the agenda for the Morrison government this year as it considers where to head on climate policy is whether this trend in industrial emissions can continue. How did we get here? Back in 2015 when Tony Abbott was still prime minister, the Coalition released details of what it called a “safeguard mechanism”, a policy it said would deal with the problem of rising pollution from big industry. Greg Hunt, the environment minister who designed the policy, told ABC Radio National it would put “a limit on the emissions that individual firms can have”. Specifically, it would set a limit – a baseline – for about 140 industrial facilities that emitted more than 100,000 tonnes each year. While the government chose not to emphasise the point, companies that breached their baseline at a particular site without federal approval would have to pay by buying carbon credits created through cuts elsewhere. As Hunt described it, the safeguard mechanism was half the Coalition’s direct action climate policy (a name long since dropped). The first half of direct action was the emissions reduction fund, a $2.5bn incentive scheme under which the government would pay for pollution cuts, mostly from landowners who signed up to plant or look after native vegetation. The second half would “safeguard” those cuts – ensure they were not just wiped out – by preventing “significant increases in emissions above business-as-usual levels” elsewhere. In practice, the scheme has run quite differently. In the case of Centennial Coal, the Australian Conservation Foundation found it could reasonably have been expected to pay more than $6m to offset its extra emissions, based on the price the government pays for carbon credits. Instead it followed rules set up by the government that allowed it to retrospectively apply for a change to its baseline arrangements. Most people paying attention to the scheme have been left to wonder: why have a policy to limit emissions that routinely allows companies to ignore their limit? Suzanne Harter, a climate campaigner with the ACF, is among those who says it makes no sense. “If the government keeps increasing the pollution baselines, there is no point to the safeguard mechanism,” she says. Tennant Reed, who runs climate, energy and environment policy for the Australian Industry Group, which represents the interests of more than 60,000 businesses, agrees. “If it never has to do something to actually reduce emissions it will have been a waste of time for everyone involved,” he says. The numbers tell a pretty basic story. In the first two years of the scheme, analysts at energy and carbon consultancy RepuTex found the regulator had approved changes that allowed big industry to emit up to 32% more without penalty than when the safeguard was introduced. Companies used only some of this additional headroom. Actual emissions under the scheme rose 12% over those two years. Based on this, RepuTex found the safeguard was likely to lead to an extra 280m tonnes of pollution over the next decade, more than six months’ worth of Australia’s total carbon pollution. It would more than eclipse the 193m tonnes of cuts contracted under the emissions reduction fund. RepuTex’s executive director, Hugh Grossman, said it meant taxpayers’ dollars spent on storing carbon dioxide in vegetation was “effectively money going down the drain”. The rise in emissions under the safeguard mechanism reflects a longer-term trend. In a separate report, RepuTex found industrial emissions had risen 60% since 2005, the year against which the government has pledged at least a 26% cut by 2030. The surge has been driven by the creation of a $50bn liquefied natural gas export industry across northern Australia and increases in direct combustion at mining sites, venting of fugitive emissions in fossil fuel extraction and pollution from metals, chemicals and minerals processing. The rise in industrial emissions is the primary reason national emissions have stubbornly refused to fall while there has been a historic drop in pollution from power plants and a significant dip from farming due to the drought. That could start to change this year as more of the recent record investment in solar and wind power, spurred by the now-reached national renewable energy target, is comes online. But analysts say Australia cannot hope to meet its climate targets while major industry is left unchecked. The industrial sector is expected to pass power generation to become the country’s most polluting sector within two or three years. A key question for the government is whether it intends to address this as it considers new climate policies this year. One option likely to be before it will be a recommendation in a review of its climate policies led by the businessman Grant King, that quietly submitted its report earlier this month. In a discussion paper sent to some interest groups late last year, King’s panel floated changing the safeguard mechanism so companies that emit less than their baseline would be rewarded with carbon credits they could sell to the government or business. The scheme was clearly was designed with that in mind. In 2015 Hunt said it would be used to cut emissions by 200m tonnes over the decade to 2030. It implied limits would be enforced and tightened so companies had to either reduce pollution over time or trade in carbon credits to offset it. In other words, a return to a form of carbon pricing. That idea was dropped before the last election - the safeguard mechanism was not included among policies that would be used to meet Australia’s commitments under the Paris climate agreement. The current minister for emissions reduction, Angus Taylor, says the mechanism is “designed to support growth while encouraging businesses to lower their emissions intensity”. The shift proposed by King in last year’s discussion paper would come with challenges – for example, working out how to guarantee that businesses were rewarded for changes in practice that cut pollution, and not for things over which they had no control, such as a downturn in the economy. It would also require the government to commit to forcing industry to reduce pollution over time, a shift seemingly at odds with Morrison’s “technology over taxation” mantra. But the shift could have the support of the Australian Industry Group, which backs turning the safeguard mechanism into a meaningful emissions policy as long as it includes protection for export industries so they are not disadvantaged against overseas competitors. Reed says if there is an intention to reduce industrial emissions, ratcheting down baselines under the safeguard mechanism is an obvious option. “That would be a substantial step, and a substantial change,” he says. “The safeguard mechanism could be part of how we get there, but right now it’s not doing much other than creating paperwork for industry and the government.” The government has already begun to make changes to the safeguard that would allow that sort of shift. When the scheme began, emissions baselines were initially based on either a facility’s historic emissions or an independent forecast of future pollution. Under changes being introduced this year, all facilities will be moved to limits based not on their total emissions, but on emissions intensity – how much they emit each unit of production. In one sense this just locks in what is already happening in cases such as the Myuna colliery – if companies lift production they will be able to put out more carbon pollution without risking a penalty. But emissions intensity baselines could also be more obviously reduced over time to drive a shift to cleaner practice without putting pressure on production levels. This is part of what Labor proposed to do – without releasing much detail – when it said before last year’s election it would cut emissions under the safeguard by 45% by 2030. Erwin Jackson, policy director with the Investor Group on Climate Change, believes the government can delay action on industrial emissions for only so long. The longer it waits, the greater the risk of a more “dramatic and draconian” shift as the pressure to act – from investors, from other countries, from the planet – escalates. He says the Australian climate debate remains stuck on the idea that pumping emissions into the atmosphere does not have a cost that will be borne eventually, one way or the other. “To address climate change you have to reduce emissions, and someone has to pay for that. Investors are already pricing this risk,” he says. “The longer we delay action the higher the cost will be.”"
"
Share this...FacebookTwitterWhile German politicians, alarmist scientists, activists, and media are staying super-glued stuck on stupid, i.e. remaining mired in the stupidity of dogmatism and closed-mindedness, the climate debate and controversy in Germany is, well, shall we say, heating the hell up.Mark the following time and place on your calender:
Wednesday, 25 May 2011, 10 pm.
http://www.mdr.de/mdr-figaro/
Once facing a hostile climate of intolerance and threats, German climate-catastrophe-skeptic scientists are increasingly coming out and choosing to exercise their human right to express themselves freely, without fear of mobbing and bashing. Good for them I say. It’s past high time.
“Do politicians really know what they are talking about?”
Hans von Storch’s Klimazwiebel site here informs us that German MDR public radio station will broadcast a special on climate change, Wednesday evening at 10 p.m. The show is produced by Kai-Uwe Kohlschmidt and according to the MDR website here, its description (emphasis added):
The show looks at just how much is climate change caused by man, or is it more a cyclic phenomena? The science is everything but in agreement when it comes to the interpretation of the huge number of facts, theories and model predictions of weather. There are plenty of loud and serious voices out there claiming climate swindle. The mainstream media are purveying a clear picture of coming catastrophe that is freshened up on a daily basis. When politicians call on us to prevent climate change, do they really know what they are talking about?

Kai-Uwe Kohlschmidt provides a look into the jungle of science, the media focus and political correctness, and invites you on a playful science expedition to Spitzbergen, Masdar, the Brandenburg Lindenberg and other locations.
Directed by: Kai-Uwe Kohlschmidt; Holger Kuhla
Production: RBB 2011


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany, on the subject of climate change, “do they really know what they are talking about” is one of the most provocative questions that’s been posed in a long time. Let’s hope that this show is really serious about being fair and balanced. I have no reason to doubt that they won’t be, and look forward to listening in.
I can almost hear Messieurs Schellnhuber and Rahmstorf screeching, at high pitches, to MDR in a bid to get the station to drop the skeptics and to stop spreading Big Oil’s and Fred Singer’s “disinformation”. But what else should we expect from zealot dogmatists who are hopelessly super-glued on stupid.
Klimazwiebel has a list of scientists who will be featured on the show:
Spitzbergen
Prof. Hauke Trinks (Sea Ice Researcher)
Prof. Steve Coulsen (UNIS Terrestial Ecology)
Prof. Jørgen Berge (UNIS Marine Biology)
Andreas Umbreidt (Terra-Polaris)
Abu Dhabi / Masdar City
Joachim Kundt (CEO Abu Dhabi Siemens)
Rene Umlauft (CEO Renewable Energies Siemens)
Dolf Gehlen (CEO International Renewable Energy Agency)
Germany
Christoph Hein (Writer)
Mike Kess / Udo Schulze (Citizens’ Initiative “CO2 Endlager”)
Dr. Franz Berger (Weather Station Lindenberg )
Prof. Hartmut Grassl (Hamburg Max Planck Institute)
Dr. Wolfgang Thüne (Meteorologist)
Prof. Dr. Werner Kirstein (Institute for Geography, University of Leipzig)
Prof. Friedrich Wilhelm Gerstengarbe (Potsdam Institute For Climate Impact Research)
Michael Limburg  (European Institute for Climate and Energy)
Dr. Joachim Bublath (Science Publicist)
Prof. Hans von Storch (Institute for Coastal Research, Geestacht)
Prof. Jan Veizer (Evolution Geologist, University of Ottawa)
Dr. Nico Bauer  (Potsdam Institute For Climate Impact Research)
Prof. Dr. Claudia Kemfert (German Institute for Economics)
Plenty of warmists, but still with a number of skeptics. MDR has framed the description in a way that tells listeners that not all is well in climate science. It’ll be interesting to see if they deliver on this. Let’s hope so.
Hope to have interesting results to report on Thursday.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Ed Caryl
The California Independent System Operator (ISO) manages the high-voltage wholesale power grid for the state of California. On their web site, they have several links including one to yesterday’s hourly breakdown of power usage and sources. The figures below are for May 25 2011.

Figure 1. The hourly breakdown of renewable power fed to the California grid. Source: California ISO.
 http://www.caiso.com/outlook/SystemStatus.html
In California, most wind power is generated in three high wind locations: Altamont pass east of San Francisco, Tehachapi pass east of Bakersfield, and San Gorgonio pass near Palm Springs. Notice the huge drop in the wind farm output centered on 10 AM local time. The wind power output at all the wind farms dropped from over 1800 Megawatts to less than 200 Megawatts in less than six hours. Solar power picked up about 400 Megawatts of that, but solar had a glitch of it’s own at about 5 PM, when a cloud obscured the sun at the major solar plant in the Mojave Desert. The grid had to replace this power, just when the load was reaching maximum in the middle of the day. Where did the backup power come from? As you can clearly see in figure 1, none of the backup power came from a renewable source.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2. The hourly breakdown of all electrical power production in California on May 25th 2011. Source: California ISO, same link as above.
California must import up to 33% of its power, most from thermal and nuclear plants in other southwest states, and some from hydroelectric dams in the Pacific Northwest. As you can see from the plot in figure 2, most of the slack when the wind died came from thermal power plants in California and imported power from other states. Nuclear plants are difficult to throttle up and down, so most of that make-up power came from thermal sources (fossil fueled).
To be available on a moments notice, (or even on a few hours notice) thermal power plants are on “standby” status. In most cases this means that they have turbines already turning, feeding minimal power to the grid, so that they can be “throttled up’ quickly. The fastest responding are the natural gas powered plants. Coal powered plants take a bit longer to be fired up.
Planning is a big part of managing a power grid. The load can be predicted with good accuracy. Even fluctuations in temperature affecting load are predicted more than 24 hours in advance. But wind is more difficult, and clouds over solar plants more difficult yet. Today’s wind, for instance, is fluctuating over a scale of minutes, giving power output fluctuations of 200 Megawatts in 30 minutes. This must be giving the operators headaches.
California plans to build renewable power resources to the tune of 33% by 2020. The Pacific Northwest already has grid problems with Oregon and Washington wind farms. California will need three to five thousand Megawatts of reserve fossil fueled or hydroelectric plants to back up the renewable power resources. Given the May 25th 2011 wind power glitch, that may be low.
Share this...FacebookTwitter "
"
From CNN An earthquake with a magnitude of 7.9 struck in the Samoan Islands region Tuesday, the U.S. Geological Survey said.

The temblor generated a nearly 10-foot (3-meter) tsunami — measured from crest to trough — according to preliminary data, said Chip McCreery, the director of the Pacific Tsunami Warning Center in Ewa Beach, Hawaii.
BULLETIN
TSUNAMI MESSAGE NUMBER   2
NWS PACIFIC TSUNAMI WARNING CENTER EWA BEACH HI
857 AM HST TUE SEP 29 2009
TO – CIVIL DEFENSE IN THE STATE OF HAWAII
SUBJECT – TSUNAMI WATCH SUPPLEMENT
A TSUNAMI WATCH CONTINUES IN EFFECT FOR THE STATE OF HAWAII.
AN EARTHQUAKE HAS OCCURRED WITH THESE PRELIMINARY PARAMETERS
NOTE MOMENT MAGNITUDE INCREASE TO 8.3
ORIGIN TIME – 0748 AM HST 29 SEP 2009
COORDINATES – 15.3 SOUTH  171.0 WEST
LOCATION    – SAMOA ISLANDS REGION
MAGNITUDE   – 8.3  MOMENT
MAGNITUDE   – 8.0  RICHTER (MS)
MEASUREMENTS OR REPORTS OF TSUNAMI WAVE ACTIVITY
GAUGE LOCATION        LAT   LON    TIME        AMPL         PER
——————-  —– ——  —–  —————  —–
APIA UPOLU WS        13.8S 171.8W  1832Z   0.70M /  2.3FT  08MIN
PAGO PAGO AS         14.3S 170.7W  1812Z   1.57M /  5.1FT  04MIN
LAT  – LATITUDE (N-NORTH, S-SOUTH)
LON  – LONGITUDE (E-EAST, W-WEST)
TIME – TIME OF THE MEASUREMENT (Z IS UTC IS GREENWICH TIME)
AMPL – TSUNAMI AMPLITUDE MEASURED RELATIVE TO NORMAL SEA LEVEL.
IT IS …NOT… CREST-TO-TROUGH WAVE HEIGHT.
VALUES ARE GIVEN IN BOTH METERS(M) AND FEET(FT).
PER  – PERIOD OF TIME IN MINUTES(MIN) FROM ONE WAVE TO THE NEXT.
EVALUATION
BASED ON ALL AVAILABLE DATA A TSUNAMI MAY HAVE BEEN GENERATED BY
THIS EARTHQUAKE THAT COULD BE DESTRUCTIVE ON COASTAL AREAS EVEN
FAR FROM THE EPICENTER. AN INVESTIGATION IS UNDERWAY TO DETERMINE
IF THERE IS A TSUNAMI THREAT TO HAWAII.
IF TSUNAMI WAVES IMPACT HAWAII THE ESTIMATED EARLIEST ARRIVAL OF
THE FIRST TSUNAMI WAVE IS
0111 PM HST TUE 29 SEP 2009
MESSAGES WILL BE ISSUED HOURLY OR SOONER AS CONDITIONS WARRANT.
h/t to Hotrod “Larry”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93191e82',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A sliver of hope against a backdrop of gloom: 18 countries showed a sustained decline in their carbon emissions from fossil fuel use over the past decade. This trend, averaging 2.2% a year over the period 2005-2015, is evident in less than 10% of the world’s countries, mostly in the EU, but accounts for 28% of global emissions. Our new research published in Nature Climate Change explains why these 18 countries have downward emission trends. This work involved an international team of researchers led by Corinne Le Quéré from the Global Carbon Project and the Tyndall Centre at the University of East Anglia. We found that part of the answer was country specific. For instance, in the US, a fracking boom meant coal was replaced by gas, while Eastern European states joined the EU and cleaned up their inefficient infrastructure. However, there are three common elements shared across these distinct national histories: the declining relevance of fossil fuels, a fall in energy demand, and strong national policy frameworks. We began by unpicking the drivers of falling CO₂ emissions in the sample of 18 “peak-and-decline” countries. We looked at four factors: lower energy use, such as more efficient vehicles, appliances or homes; lower share of fossil fuels in energy generation, thanks to new renewable or nuclear power; improved fossil fuel utilisation rate, for instance through leaking less electricity from cables or overhead lines; and lower carbon-intensity of fossil fuels, typically caused by switching from coal to gas. We found that a declining share of fossil fuels was responsible for about half of the fall in emissions, with a further third attributable to a decrease in energy use. The relative emphasis of these two factors varied across countries. Whereas Austria, Finland and Sweden relied more heavily on decarbonising their energy mix, Ireland, the Netherlands and the UK saw a stronger effect from reducing energy consumption. In general, though, both factors were important across the board. To check our claims were robust, we examined two other factors that may have influenced the results. The first was the global finalcial crisis, which we found did cause a slowdown in economic growth in the peak-and-decline countries, enough to partly explain some of the decline in energy use. Next was the well-documented effect of consumption in developed economies driving up emissions in industrialising economies – are Europeans who buy clothes or TVs made in China simply “outsourcing” their emissions? In our 18 peak-and-decline countries we found this process had slowed and largely ended prior to 2005, so had no significant effect on our results. Next, we tested whether falling carbon emissions in our 18 countries were associated with policies. We collected data on the numbers of energy efficiency, renewable energy, and climate policies (including frameworks and targets) adopted in law per country during the 2005-2015 study period. In each case, we found that these policy count statistics were strongly and significantly correlated with corresponding energy or emission trends.  To understand whether these findings were unique to the peak-and-decline countries rather than part of a more general phenomena, we repeated our analysis for two “controls”: a group of 31 countries with rising emissions and slow economic growth (such as Japan, Brazil and South Africa), and a group of 30 countries with rising emissions and fast economic growth (such as Turkey, India, and China). As expected, we found that none of the policy variables in the control group countries were significantly correlated with energy and emissions trends. Correlation is not causation. We cannot claim that national climate policies are directly responsible for falling emissions. And counting policies does not account for their stringency, enforcement, and credibility, all of which are important. However, there is some precedent in the literature for using count statistics to assess the effect of policy on emission reductions both in Europe and the US. Our findings are also consistent with political science, innovation studies, and energy assessments, which repeatedly emphasise the importance of stable policy environments for low-carbon innovation and clean energy deployment.  Another important takeaway from our research is the importance of energy use. Renewables, nuclear, fracking, coal, and carbon capture and storage tend to hog the headlines as well as the attention of policymakers. But our analysis found that even double digit growth rates for renewables did not make a dent in rising emissions in those countries with rapidly expanding energy systems dominated by fossil fuels as new solar panels or wind turbines were simply being added at the margins. In contrast, downsizing the entire energy system by reducing demand makes the whole process of decarbonisation much more manageable."
"Every budget is billed in advance as the most crucial in recent times, but then most are instantly forgotten. The one Rishi Sunak will deliver in just over two weeks’ time may be one of the few that justifies the hype. The reason so much is resting on the shoulders of the tyro chancellor is that the budget needs to satisfy a number of different audiences: the voters in the Midlands and the north of England who gave Boris Johnson his 80-seat majority; traditional Conservative voters; the financial markets; and foreign governments looking to see whether the UK will take a lead before the Cop26 climate change conference in Glasgow in November. A package that pushes all the necessary buttons is not going to be easy. To take one example, making tax relief on pension contributions less generous for those on higher incomes would help the chancellor’s sums add up and win credibility with the financial markets but antagonise the Tory party’s natural supporters. Sunak’s immediate task is to announce targets for the public finances that are easier to hit than the ones currently in place, but not so weak that the markets take fright. Moving the goalposts will give the government more scope to borrow for infrastructure projects that need to be underway soon if they are to be completed in time to deliver a political dividend for Johnson at the next general election. But unless he can also find a way of making the budget consistent with the government’s 2050 net zero carbon target for the economy a diplomatic failure of catastrophic proportions looms at the end of the year. The Cop26 is the most important summit the UK has hosted since the G8 met at Gleneagles in 2005 – and the task facing the government is much more daunting than it was then. The Gleneagles summit was all about the rich countries of the west agreeing to provide debt relief and higher levels of aid for poor nations. Most of the debts would never have been paid anyway and the doubling of aid was easily affordable at a time when the global economy was booming. Even so, it took a lot of time and effort to chisel out a deal. The then prime minister, Tony Blair, and the then chancellor, Gordon Brown, both lobbied hard to overcome resistance to their plan, expending plenty of political capital in the process. Public opinion – channeled through the Make Poverty History campaign – was effectively mobilised. Crucially, the Labour government showed leadership by committing to the UN target to spend 0.7% of national income on aid. A deal in Glasgow is going to be immensely more difficult than it was up the road in Perthshire 15 years ago. For a start, there are many more countries involved. For another, some of the biggest players are actively hostile to the idea of setting tougher emissions targets. The contrast between George W Bush – who was interested in Africa – and the climate emergency denier who currently occupies the White House is stark. But the US is not going to be alone in Glasgow: Brazil, Australia and Saudi Arabia will all prove hard to break down. After failing to persuade David Cameron to do the job, Johnson has put the business secretary, Alok Sharma, in charge of summit preparations. But Sharma does not have the heavy-hitting international reputation that is going to impress other governments. That will require Johnson to demonstrate his personal commitment to making Glasgow a success. All of which brings us back to the budget, which provides an opportunity for the government to announce measures that will accelerate the UK’s progress towards a decarbonised economy. These need to be more than the mooted increase in fuel duty. The Green New Deal Group (of which I am a member) has estimated it will cost about £100bn a year for 20 years to make the transition to a net zero carbon economy. Investment on that sort of scale would be necessary to make the UK’s 30m buildings energy efficient, turn buildings into power stations through the use of solar panels, and invest in renewable energy. So where’s the money going to come from? One answer would be a form of green quantitative easing – money creation by the Bank of England that would pay for the decarbonisation of the economy rather than, as was the case during and after the financial crisis, being pumped into the banking system. The government doesn’t seem keen on this approach, even though there are plenty of economists who think it is wholly feasible. Another possibility would be for the government to borrow the money in the usual way, but this doesn’t appeal to ministers either. There is, though, a third option. At present about £100bn year is paid into pension schemes, all of it eligible for tax relief currently worth £54bn a year. There is also tax relief on the £70bn a year invested in Isas. The GND proposal is that 25% of pension contributions should go into green new deal investment in exchange for that tax relief and that all new Isa contributions – which currently go into cash or shares – should be invested in green new deal bonds issued by the government at a guaranteed rate of interest. The idea is to provide a stream of income to transform the economy as well as offering a new secure investment vehicle for savers. Insurance companies and pension funds no longer risk being left with stranded fossil fuel assets and the City would be the place to do green finance. Above all, a strong signal of intent would be sent to the rest of the world."
"

Given the recent claims that hurricanes are getting dramatically worse because of global warming, it’s too bad we’ve already exhausted the letter “G” for this hurricane season. “Gasbag” would have been a pretty good moniker for the next storm. 



In case you’ve missed the hype, MIT’s Kerry Emanuel has a paper in the online version of _Nature_ magazine saying that hurricanes are becoming dramatically more powerful as a result of global warming. 



Merely venturing into the discussion of hurricanes and global warming is more dangerous than most tropical cyclones. About Emanuel’s article, William Gray of Colorado State University — the guy who issues the annual hurricane forecast that grabs headlines every summer — told the _Boston Globe,_ “It’s a terrible paper, one of the worst I’ve ever looked at.” 



There’s also nastiness if you say hurricanes aren’t getting worse. A month ago, University of Colorado’s Roger Pielke, Jr., posted a paper that was accepted in the _Bulletin of The American Meteorological Society_ concluding there is little if any sign of global warming in hurricane patterns. In a pre‐​emptive strike, Kevin Trenberth from the federally funded National Center for Atmospheric Research in Boulder, Colorado, told the local newspaper, “I think he [Pielke] should withdraw his article. This is a shameful article.” 



Six months earlier, Christopher Landsea of the National Hurricane Research Laboratory, another federal entity, quit the United Nations’ Intergovernmental Panel on Climate Change. Landsea is probably the world’s most respected hurricane scientist. He was furious that Rajenda Pauchari, director of the panel, condoned Trenberth’s statements that hurricanes were worsening because of global warming. 



What is going on here? Nothing unusual. Behavior like this takes place every day at faculty meetings across academia. But global warming and hurricanes are hot topics right now, so the bickering spills over into the press. 



What is unusual is the especially shoddy nature of the current scientific review process on global warming papers. 



Consider the recent _Nature_ article. If hurricanes had doubled in power in the last few decades as Emanuel claims, the change would be obvious; you wouldn’t need a weatherman to know which way this wind was blowing. All of these feuding scientists would have agreed on the facts long ago. 



Damages caused by doubling the strength of hurricanes would be massive and increasing dramatically. Figures on this are pretty easy to come by, at least in the United States. The insured value of property from Brownsville, Texas to Eastport, Maine — our hurricane prone Atlantic Coast — is greater than a year of our Gross Domestic Product. If hurricanes had actually doubled in power, the losses in the insurance industry would be catastrophic. 



Pielke has studied this, and his work is well known. Hurricanes are causing greater dollar damages because more and more people are building increasingly expensive beachfront monstrosities that have financially appreciated during the recent real‐​estate bubble. Account for these and there is no significant change in hurricane expenses along our coast. Illinois climatologist Stanley Changnon has also studied this for non‐​hurricane weather damage over the entire country with similar results. 



Pielke told me that, “analysis of hurricane damage over the past century shows no trend in hurricane destructiveness, once the data are adjusted to account for the dramatic growth along the nation’s coasts.” 



You would think that reviewers of Emanuel’s paper at _Nature_ would have thought to ask whether, in fact, there was evidence for increasingly powerful storms. 



But they didn’t. There is just no incentive in the scientific community to kill the remarkably fertile global warming goose, a beast that feeds on public fears. 



The federal outlay on climate research is now $4.2 billion per year, roughly the same amount given to the National Cancer Institute. The climate research community sees a grave threat when research shows there’s no threat from the climate. So papers that hawk climate disaster get superficial reviews and uncritical headlines, while those that argue otherwise are “shameful.”
"
"

Today the _Washington Post_ has a big story on efforts by the coal industry to get public schools to teach positive things about — you guessed it — coal. The impetus for the article is no doubt a recent kerfuffle over education mega‐​publisher Scholastic sending schools free copies of the industry‐​funded lesson plan “The United States of Energy.” Many parents and environmentalists were upset over businesses putting stealthy moves on kids, and Scholastic eventually promised to cease publication of the plan.   
  
  
Loaded curricula designed to coerce specific sympathies from children, however, hardly come just from industry, as the _Post_ story notes. Indeed, as I write in the new Cato book _Climate Coup: Global Warming’s Invasion of Our Government and Our Lives_ , much of the curricular material put out at least on climate change is decidedly alarmist in![](http://wac.0873.edgecastcdn.net/800873/cato/store/sites/default/files/imagecache/product/climate_coup_cover_130.jpg) nature, and is funded by you, the taxpayer. In other words, lots of people are trying to use the schools to push their biases on your kids, which is an especially dangerous thing considering how unsettled, uncertain, and multi‐​sided so many issues are.   
  
  
In light of the huge question marks that exist in almost all subjects that schools address, the best education system is the one that is most decentralized, in which ideas can compete rather than having one (very likely flawed) conclusion imposed as orthodoxy. And it would be a system in which no level of government — either district, state, or federal — would decide what view is correct, or what should be taught based on the existence of some supposed consensus, as if “consensus” were synonymous with “absolute truth.” What is truth should not be decided by who has the best lobbyists or most political weight, nor should children be forced to learn what government simply deems to be best.   
  
  
Of course, there are some people who will decide that they are so correct about something that it would be abusive not to have government force children to learn it. If their conclusion is so compelling and obvious, however, no coercion should be necessary to get people to teach it to their children — it should be overwhelmingly clear. More importantly, if there is controversy, efforts to impose a singular view are likely to fail not just with the children of unbelievers, but for many of the children whose parents share the view. As significant anecdotal evidence over the teaching of human origins has stongly suggested — and new empirical work has substantiated — when public schools are confronted with controversial issues, they tend to avoid them altogether rather than teach any side. In other words, efforts at compulsion don’t just fail, they hurt everyone.   
  
  
Educational freedom, then, is the only solution to the curricular problem. If you want full power to avoid the imposition of unwanted materials on your children, you must be able to choose schools. And if you want to ensure that your kids get the instruction you think every child should have, everyone else must have that ability, too.
"
"

It has been said that when the United States sneezes, the world catches a cold. While this metaphor still prevails, the world is now heavily vested in China’s well being as well.



A decade of near double‐​digit annual economic growth, tens of billions of dollars of annual investment inflows, an industrial complex that churns out an ever‐​increasing share of the world’s consumption, and the implications these developments have had on everything from raw materials prices to shipping rates have thrust China into the spotlight in 2004.



But success and the capacity to influence events bring responsibility. Thus, how China manages its economic policies, including its trade relations with the United States, Europe, and its Asian neighbors, will impact profoundly the direction of the world economy and institutions linked to it, such as the World Trade Organization.



China is the pretty girl with whom everyone wants to dance. The countries of ASEAN, New Zealand, and Australia have been tripping over themselves to get a date — for a free trade agreement. Argentina, Brazil, and Chile are giddy about commitments from the Chinese premiere for direct investment and big ticket purchases. American, European, and Asian companies continue unabated the process of setting up manufacturing operations in China. And, the reality of a growing consumer class — the prospects of which are made all the more alluring by new infrastructure projects in the country’s interior — has producers throughout the world dreaming of the possibilities.



In large measure, China owes the rapid pace of its economic progress to its World Trade Organization accession. Joining the WTO leveled the playing field for Chinese exporters, who until then often faced steeper trade barriers than those applied to standing WTO members. Facing equivalent trade barriers, Chinese exporters have been able to capitalize on their wage and other logistical advantages to capture growing shares of foreign markets.



Perhaps more importantly, WTO membership forced China to engage in a process of liberalizing its own rules on investment, foreign ownership, tariffs, and other barriers to trade. While much remains to be accomplished in these areas, WTO membership and the initial steps taken by China to honor its commitments have provided confidence to foreign investors, business partners, and importers that China would not be subject to whimsical and unpredictable changes in the business climate. Thus, the investment and purchase orders have poured in.



Going forward, it is important that China continue to recognize and honor this linkage. It may be tempting for China to be smitten by the acclaim and succumb to the fallacy that bilateral or regional trade agreements are viable alternatives to the WTO. But ASEAN’s countries are seeking to link their fortunes to China’s because under the WTO the latter has become an investment magnet — largely at the expense of the former. 



China is seeking copper and iron ore and other raw materials in South America because it wants to continue large infrastructure projects and it needs to feed its industrial machine. These developments are attributable to the willingness of Americans and others to buy Chinese‐​made products, again outcomes inspired by the WTO.



But Americans can continue to consume Chinese products only if the reserves are recycled. If China intends to diversify its portfolio by foregoing American bonds and securities in favor of purchasing hard assets in other countries — a perfectly legitimate choice — it will have to purchase more American services, technology, bio‐​technology, entertainment, and pharmaceutical products. This will require further liberalization of rules that disadvantage foreign providers, and substantially better enforcement of intellectual property protections.



The Bush administration has been a fairly decent steward of trade relations with China. It has overruled imposing sanctions under the China safeguard law on each of four occasions that the U.S. International Trade Commission recommended that prescription. It declined to initiate an investigation into what U.S. unions were calling unfair labor practices in China, and it dismissed a similar investigation into currency undervaluation. 



On matters of antidumping and textile safeguards, the Bush administration has acted less commendably. But its actions on these fronts reflect a prominent anti‐​trade — indeed, anti‐​China — strain in certain U.S. business and policy circles that will only grow worse if China does not, at a minimum, show greater progress on intellectual property rights enforcement.



Multilateral trade rules embodied in the WTO have been the infrastructure used by China — and indeed most of the world — to create wealth and opportunity over the past decade. Should China lose sight of its importance, the world may catch more than a cold. 
"
"**Christmas is coming but how can you celebrate it without giving the unwelcome gift of coronavirus?**
Cosy rooms packed with people, chatting, laughing, even singing, and sharing food and drink for hours are the norm for the festive season.
Unfortunately, almost everything that's great for lifting our spirits at Christmas is also ideal for fuelling the pandemic.
So here are the key questions to ask about any festivities.
No-one will be popular for saying this, but the evidence is clear - the larger the group, the greater the risk.
If it was summer and we could meet outside, where the virus gets dispersed in fresh air, it would be less of a problem. But it's winter, so everyone's inside.
And the more people who are involved, the greater the likelihood that someone may be a carrier of the virus - maybe without realising.
A study by Sage, the government's science advisory panel, concludes that if you double the number of people getting together, you get a fourfold increase in the odds of infection.
It also matters how many different households are meeting - the fewer the safer - because the more different homes which are mixing, the greater the potential for the virus to spread.
Passing around dishes and bottles, encouraging everyone to tuck in, is one of the most natural of instincts at Christmas.
But the coronavirus can survive on surfaces, possibly for several hours, so plates and cutlery can become contaminated, which means you could be handing round the virus as well as the sprouts.
In the US, the official advice for this year's Thanksgiving dinners is to break totally with tradition by asking guests to bring their own food and drinks.
It's also recommended that you control who's allowed in the kitchen, with one lucky person doing all the serving.
Amid the excitement of reunions, it's perfectly normal for voices to be raised.
Add a little alcohol, and maybe have a TV or music on as well, and things get even noisier.
But if someone is infected, the louder they speak, the more virus they release.
A lot of research shows that when voices are projected, people emit more tiny droplets of the kind that can carry the coronavirus.
That's why for Thanksgiving gatherings, the US government advises: ""Encourage guests to avoid singing or shouting, especially indoors.
""Keep music levels down so people don't have to shout or speak loudly to be heard.""
Maybe the safest option is to hum Silent Night.
Popping in for a quick visit is safer than lingering over dinner for several hours.
Researchers say an event's duration has a big impact on the infection risk.
In March, more than 50 members of a choir in the US were confirmed or suspected of being infected, after a two-and-a-half-hour rehearsal session.
Scientists reckoned that if it had lasted less than an hour, the number of infections would have been reduced by more than half.
Prof Cath Noakes, one of the study's authors, says the problem is that tiny particles carrying the virus, known as aerosols, can accumulate in the air.
""There is growing evidence that if you're in a poorly-ventilated space for a long period of time with people who are infected, you may breathe in those aerosols and that might be one of the routes of infection,"" she said.
The obvious answer is: ""Of course not, it's too cold outside.""
But fresh air dilutes any virus that might be lingering in a crowded room.
A Sage report says infection risks can be increased by four times without proper ventilation.
And in this context, ""ventilation"" doesn't mean having fans blowing the air around, but a flow of air from outside.
And if people feel too cold? Wear another layer.
According to Prof Noakes, people must dream up creative new options for Christmas
That could range from meeting virtually on Zoom, going for a walk, braving the weather for a picnic, or even delaying big gatherings until next summer.
If you are planning a meal indoors, she says, make sure you keep everyone as far apart as possible and be careful to keep everything clean.
Also try to avoid having people from different households sitting opposite each other because speaking face-to-face is a route of transmission.
Any social setting increases the risk of infection, says Prof Noakes, and we will have to compromise, reduce our contacts with people and do things in a different way.
""The virus doesn't know it's Christmas,"" she says. ""It's just a virus and it thrives on human contact."""
"**England enters a tougher version of its three tier system of restrictions on Wednesday, as a four-week lockdown ends.**
Northern Ireland has a two-week circuit-breaker lockdown, while Wales is banning the sale of alcohol in pubs, cafes and restaurants from Friday. Scotland has its own five-tier system.
Across the UK, some restrictions will be relaxed over Christmas, to allow three households to form a ""Christmas bubble"".
From just after midnight on Wednesday 2 December, areas will be placed in one of three tiers: medium, high and very high.
About 99% of England has been placed into the high and very high coronavirus risk category - tiers two and three.
The placing of areas in each tier will be reviewed every 14 days, with the first review on 16 December.
**Areas in tier two**
**Tier two (high) rules**
**Areas in tier three**
**Tier three (very high) rules**
Additional restrictions apply:
**Areas in tier one**
Only three areas have been placed in the lowest tier:
**Tier one (medium) rules**
Areas in the lowest tier will have some restrictions relaxed:
There are exceptions in all tiers for childcare and support bubbles. More details of the plan are here.
The new coronavirus tier restrictions will mean 55 million people will be banned from mixing with other households indoors. The decision about which tier to place an area in is based on:
Lockdown restrictions in Wales were eased on 9 November.
**The current rules say:**
People who you don't live with still cannot come into your home socially, unless you are in an extended household (bubble) with them. Tradespeople can enter your home to carry out work.
However, from **Friday 4 December:**
Read Wales' official guidance.
Northern Ireland started a two-week circuit-breaker lockdown from 00:01 GMT on Friday 27 November.
Read Northern Ireland's official guidance.
Each area of Scotland has been placed in one of five tiers.
Eleven local authority areas in west and central Scotland have recently moved from level three to level four, affecting two million people.
First Minister Nicola Sturgeon told MSPs the level four measures would be lifted at 18:00 GMT on Friday 11 December.
**Areas in level zero**
No areas have been placed in the lowest tier.
**Level zero (nearly normal) rules**
**Areas in level one**
**Level one (medium) rules**
Additional restrictions apply:
**Areas in level two**
**Level two (high) rules**
Additional restrictions apply:
**Areas in level three**
**Level three (very high) rules**
Additional restrictions apply:
**Areas in level four**
**Level four (lockdown) rules**
Additional restrictions apply:
Schools stay open in all levels, and here must also be no non-essential travel between Scotland the rest of the UK.
**Do you meet other people for exercise? Have you been out walking during the November lockdown? You can share your experiences by emailing**haveyoursay@bbc.co.uk **.**
Please include a contact number if you are willing to speak to a BBC journalist. You can also get in touch in the following ways:"
"

Now that the election’s finally over, the Clinton administration has a last chance to do some real damage to George W. Bush’s economy.



President Clinton believes passionately that part of his legacy will be to put in place a mechanism that will forever mire America in the United Nations’ infamous Kyoto Protocol on global warming. Last month, the signatories met at The Hague, where Clinton proposed that we meet almost 90 percent of our obligations to reduce net emissions of major greenhouse gases by cutting energy use. Originally, the United States had proposed to lock up 50 percent of such emissions through trees and soil management — a relatively inexpensive proposition — but the European Union insists on emissions reductions, a course that will cause us grave economic harm.



So we caved all the way to a 90 percent reduction, and the EU still said no, we need more. Then, last week, the Clinton administration tried again in a closed‐​door meeting in Ottawa, Canada. Still no agreement. Finally, on Dec. 13, Norway’s Environment Ministry invited everyone to Oslo — before Christmas — for a third try.



This will be the last go‐​round, and Clinton has every incentive to give away the store. The result is a twofold “legacy” — being the first U.S. leader to commit to major reductions in greenhouse gases, and saddling the incoming president with a massive political and economic burden that will have absolutely no detectable effect on global weather and climate.



The political gains are obvious. Bush either gets clobbered in 2004 or the Republicans suffer in 2008. While Kyoto agreements go into force in 2008, major taxes and infrastructural changes have to begin long before then to meet these massive reductions in energy use. First, say good‐​bye to affordable electricity. Currently 56 percent of our juice is produced by burning coal, but because it emits a bit more greenhouse gas per unit energy than natural gas (which costs more), well, coal’s gotta go.



California, as usual, is leading the way here. Thanks to a moratorium on production of fossil fuel power plants, they’re out of power. It is a sad day when our Grinch‐​green friends compel us to turn off the Christmas lights, but that is the case right now in Los Angeles.



Second, we hope you like your new hybrid automobile. The technology’s really cool. My Honda Insight really does get 70 miles per gallon on a good day, and it is an engineering marvel. The only problem is that Honda’s losing at least $8,000 per car, and the company only sold 3,502 through November. It seats two comfortably.



So either we’re going to have to pay about 50 percent more for a mid‐​range hybrid car, or we’re all going to have to make it up in taxes to subsidize those who do buy them. And it might require quite a subsidy, too. Insight sales in November, at 291 units, were down 40 percent from August, despite giveaway prices.



Third, the $2‐​a‐​gallon gas of spring 2000 will be just a fond memory, thanks to the taxes required to discourage enough consumption to make you buy that subsidized hybrid.



High gas prices, tax‐​mandated technology, and dark Christmas trees are not the correlates of political popularity. But that is exactly where Clinton could force Bush to go if he gives away the store in Oslo.



All this for an agreement, the Kyoto Protocol, that is not the law of the land. It hasn’t been ratified by the Senate, and it stands little chance. And even if it were in force, the Clinton administration’s own scientists say it would only change global temperature by seven hundredths of a degree in 50 years. That’s too small to measure.



There will almost certainly be some weather disaster during the Bush administration. Right now, the insured value of property along the East Coast is almost equal to our annual Gross Domestic Product. We haven’t had a Category 5 hurricane hit since 1969. Even a lower Category 4, well‐​aimed, will cause unimaginable destruction. Federal scientist Christopher Landsea (the most appropriately named hurricanologist in the world) has shown that even this class of hurricane, if it hits Miami/​Fort Lauderdale, will be good for about $70 billion. On the high end, $100 billion from a Category 5 isn’t out of the question. People will blame global warming rather than admit it’s pretty stupid to sink one’s life savings in a sand dune on a hurricane‐​prone beach.



In Bush Sr.‘s administration, the Senate was adamantly opposed to a different climate treaty — the Montreal Protocol to ban chlorofluorocarbons (CFC) refrigerants. NASA scientist Bob Watson — now the powerful head of the U.N.‘s Panel on Climate Change — announced an imminent ozone hole over North America, and five days later, the Senate passed a ban on CFCs, 99 to 1. A senator by the name of Al Gore whipped up the troops with an impassioned speech about an “ozone hole over Kennebunkport,” Bush’s home.



Never mind that the predicted disaster never happened. NASA had made a measurement error. But Bob and his friend Al had correctly calculated the political trajectory that would bring in the ban on CFCs.



So it can happen, and next week in Oslo, the Clinton administration may sow the seeds that trash the future of George W. Bush.
"
"We had driven the land cruiser for half a day across the seemingly endless reaches of the Great Basin, the vast, near-waterless region of valleys and plains that stretches out from Nevada in the western US and into neighbouring states. A mile away on the side of a mountain I spotted through binoculars the object of my quest: a small band of wild horses ambling slowly downhill, the alpha mare in the lead with her brood stallion bringing up the rear. Locating wild horses in the Great Basin has occupied me for years.  For all the time and effort, success still relies more on luck and persistence than skill. Yet as often as I’ve seen them, they never fail to inspire. Human fascination with wild horses, known as mustangs in the US, is as undeniable as it is inexplicable.  Perhaps anthropologist Elizabeth Atwood Lawrence had it right when she said: “Of all animals perhaps the horse is uniquely suited to represent the conquest of the wild – the extension of culture into nature.”   With only two humans per square mile it is by far the least densely populated part of America, but it is currently home to some 50,000 free-roaming wild horses and burros under the care and protection of the Bureau of Land Management (BLM), a US government agency.   The animals are scattered throughout the desert typically surviving in small family bands. They have no single coat color, and can appear as bay, brown, black, sorrel, chestnut, white, buckskin, gray, palomino, pinto, and blue, red and strawberry roans.  The breeds are mixtures of everything from draft horses and thoroughbreds to everyday grade horses. They are largely the descendants of strays from western cattle ranching, mining and the military. Only a small number are direct descendants of the horses introduced by the Spanish conquistadors and passed through Native American hands – true living history. Wild horses may provide inspiration to some – see the sports car and fighter plane that bear their name. But to others such as the ranchers who rely upon the federal lands for their livelihood and for whose cattle the horses compete for scarce feed and water, they are a curse on the land.  With such divergence between wild horse admirers and their haters, it is small wonder wild horses rank among the most challenging of public land issues. Cattlemen just prior to and after World War II, in an attempt to clear their grazing allotments of these unwanted animals, hired agents known as “mustangers”, whose job was to remove the wild horses by any means possible.  Frightened horses were pursued with motorised vehicles until exhausted, lassoed, tied down, and hauled to slaughter houses. Western ranchers removed these animals from their federal grazing allotments at such an alarming rate that by the late 1960s they numbered fewer than 18,000.   The struggle to save what remained of the West’s wild horses began with a secretary named Velma Bronn Johnston.  The particular event that brought this woman into the fray occurred not on the open range, but rather on the streets of downtown Reno, Nevada.  One day in 1950, while driving to work, she happened alongside a cattle truck filled with wild horses headed for slaughter and saw blood dripping from underneath it onto the city street.   The incident so disturbed Johnson that she soon dedicated her life to seeking protection for her “wild ones” as she called them.  Along the way, she even acquired the nickname “Wild Horse Annie”, first given by her enemies as a sign of contempt, but then worn as a badge of honour.  Johnson’s journey would eventually carry her far beyond her enemies, from Reno to Washington, DC and the Halls of Congress.   In 1971 Congress passed, and President Richard Nixon signed into law, the Wild Free-Roaming Horses and Burros Act.  The product of more than a decade of tireless effort on the part of Johnson and her colleague, Congressman Walter Baring of Nevada, the law was intended to protect the few remaining mustangs and burros on public lands as “living symbols of the pioneer spirit of the West”. The rangeland these animals now occupy, alongside cattle, wildlife and human hikers, is owned and managed by the federal government.  And it is the fierce human competition over these “multiple-use lands” that rests at the heart of the wild horse controversy.  Extreme positions have all but ruled-out compromise thus contributing to a serious state of policy gridlock. The single greatest problem facing the BLM is the excessive reproduction of wild horses.  Each year their numbers increase by about 20%, doubling their population every five years.  Round-ups and public adoptions are held, but these are not sufficient to offset the natural increase.  As a result, some 47,000 animals are currently in long-term holding facilities waiting out their lives far from the free roaming life originally envisioned by Wild Horse Annie.  The BLM has set a target population of about 27,000 for the free roaming animals.  As previously noted, the current population is nearly double that.   While it’s possible, extinction seems an unlikely fate for the wild horses.  The challenge is more one of proper management, especially with respect to controlling overpopulation. In this regard, the BLM is experimenting with fertility control.  One can only hope that the disputing factions can come together with BLM and workout an amicable solution. "
"**The total number of deaths occurring in the UK is nearly a fifth above normal levels, latest figures show.**
Data from national statisticians show there were almost 14,000 deaths in the week ending 13 November.
Some 2,838 of the deaths involved Covid - 600 more than the preceding week, according to the analysis of death certificates.
The North West and Yorkshire have seen the most excess deaths.
The number of deaths in both regions were more than a third above expected levels.
By comparison, the number of deaths in the South East was just 2% above the five-year average.
But there is hope the rise in the number deaths may soon start slowing.
The daily figures published by government - which rely on positive tests - show deaths are not rising as quickly as they were, and may be levelling off.
And unlike in the first wave, when the lack of testing meant the government figures underestimated the number of Covid deaths, the two sets of data are mirroring each other.
Sarah Scobie, of the Nuffield Trust health think tank, said: ""Despite the end of the second national lockdown in England coming into focus, today's figures are a sobering reminder of the dreadful impact of this virus.
She said the high number of deaths was ""piling on the pressure"" on the NHS.
""For some hospitals, particularly in Covid hotspot areas, it will feel as if they are in the depths of winter already."""
"We are in the middle of one of the biggest experiments in human history. At its core is the homogenisation of global food systems, which increasingly must deliver the same products to an expanding population (in all senses) across the world.  I now live in Kajang, in the Klang Valley around Kuala Lumpur, Malaysia. This area typifies many fast emerging economies where increasing wealth and aspirations lead to an appetite for global brands – to buy and to eat.  Within a few kilometres of my house I can purchase the same fast-food as in New York, London or Sydney.  The first McDonald’s in Kuala Lumpur opened in 1982. Now, there are more than 250 restaurants in Malaysia, with 42% of the local fast-food market in the Klang Valley. It is hard to imagine that when the McDonald brothers opened their first branch in California in 1940, they would initiate a global phenomenon whereby 70m customers in 118 countries would consume an estimated 1% of the food eaten every day on the planet in a McDonald’s outlet.  Kajang actually claims to be the home of satay. However, it seems inconceivable that a local “mamak” stall owner could ever sell satay on virtually every street corner around the world. McDonald’s now serves 144m “happy meals” in Malaysian outlets each year. Presumably, this saves 144m bored Malaysians from staring into their bowls of curry mee, satay and Roti Canai. The homogenisation of global food systems means that any fast-food outlet must depend on a long, complex and increasingly vulnerable supply chain to source products whose ingredients are derived from a tiny range of plant and animal species. While there are an estimated 30,000 edible plant species, just three (wheat, rice and maize) now account for more than 60% of the calories consumed by 7 billion people across the world. If we disturb the supply chains or the productivity of these major crops we are in trouble – wherever we live. Precisely because of their global significance and the consequences of their failure, virtually all our agricultural research, funding and promotion focuses exclusively on squeezing more out of these major crops grown as monocultures. As the climate changes, our increasing reliance on a few major crops will jeopardise food security. The recent IPCC (2014) report predicts that, without adaptation, temperature increases of above about 1o C from pre-industrial levels will negatively affect yields on the major crops in both tropical and temperate regions for the rest of the century.   These impacts need to be seen in the context of crop demand, which is predicted to increase by about 14% per decade until 2050. In a recent study in Nature, an international team of scientists found that iron and zinc concentrations were substantially reduced in wheat, rice, soybean and pea crops grown under the CO2 levels expected by 2050. In other words, climate change will reduce both the yield and the nutritional content of the world’s major crops – leaving many hungry and malnourished. While we might modify the characteristics and management of major crops sufficiently to yield under the lower range of temperature increases, we are unlikely to succeed at higher temperatures. So what should we do for agriculture in hotter, drier climates? A good start would be to explore the many hundreds of underutilised crops that have survived, yielded and fed people for millennia despite, not because of, agricultural science. For example, bambara groundnut (Vigna subterranea) is a highly nutritious, drought-tolerant African food legume. However, during Africa’s colonial period it was increasingly displaced by the oil-rich peanut, grown for its cash and export potential. Bambara – “the groundnut of the women” – has survived more through its own resilience and the tenacity of the communities that have cultivated it than the contribution of agricultural scientists to its improvement or extension agencies to its expansion. Our entire food system is in a precarious state, propped up by a narrow elite range of major crops backed by global research and advocacy.  Meanwhile everything else, including the underutilised and ignored crops that could sustain us in the future, is increasingly starved of resources.  Without urgent, serious and comparative research on crops that can yield in hotter, volatile climates of the future, the global food system will increasingly depend on only a few crops. Future generations will not thank us for allowing the rest to wither away."
"**P &O will not resume cruise sailings until at least April next year.**
The company ended its cruises in March in response to the coronavirus pandemic and has not resumed any of its voyages since.
The Southampton-based firm, which is part of the Carnival group, said the continued pause in operations was because of ""the current uncertainty around European ports of call"".
The latest round of cancellations affects 19 planned cruises.
P&O President Paul Ludlow said: ""With hopeful news headlines clearly we do not want to extend our pause in operations any further than absolutely necessary, but given the ever-changing guidance around international travel and the varying regulations in many European ports of call we felt it prudent to cancel these additional dates.
""In addition, as the final payments are due for these cruises very soon we felt it was the right thing to do for our guests.
""We are so sorry to disappoint those who were due to travel but really hope they will rebook for later in the year or for our new programme of 2022 holidays which went on sale earlier this month with strong demand, showing great confidence in cruising in the future.""
Guests with bookings on cancelled cruises will be offered credit on a future cruise or a refund.
In March, P&O Cruises brought its ships back to Southampton as the pandemic worsened.
Two months later Carnival UK said it planned to cut 450 jobs across P&O Cruises and its other cruise line, Cunard, to ""ensure the future sustainability"" of the business.
In July, P&O Cruises announced it was selling one of its oldest vessels, Oceana.
The sight of idle cruise ships anchored off the south coast became an unusual tourist attraction over the summer."
"
Guest post by David W. Schnare, Esq. Ph.D.
When Phil Jones suggested that if folks didn’t like his surface temperature reconstructions, then perhaps they should do their own, he was right.  The SPPI analysis of rural versus urban trends demonstrates the nature of the overall problem.  It does not, however, go into sufficient detail.  A close examination of the data suggests three areas needing address.  Two involve the adjustments made by NCDC (NOAA) and by GISS (NASA).  Each made their own adjustments and typically these are serial, the GISS done on top of the NCDC.  The third problem is organic to the raw data and has been highlighted by Anthony Watts in his Surface Stations project.  That involves the “micro-climate” biases in the raw data.
As Watts points out, while there are far too many biased weather station locations, there remain some properly sited ones.  Examination of the data representing those stations provides a clean basis by which to demonstrate the peculiarities in the adjustments made by NCDC and GISS.
One such station is Dale Enterprise, Virginia.  The Weather Bureau has reported raw observations and summary monthly and annual data from this station since 1891 through the present, a 119 year record.  From 1892 to 2008, there are only 9 months of missing data during this 1,404 month period, a missing data rate of less than 0.64 percent.  The analysis below interpolates for this missing data by using an average of the 10 years surrounding the missing value, rather than basing any back-filling from other sites.  This correction method minimizes the inherent uncertainties associated with other sites for which there is not micro-climate guarantee of unbiased data.
The site itself is in a field on a farm, well away from buildings or hard surfaces.  The original thermometer remains at the site as a back-up to the electronic temperature sensor that was installed in 1994.

The Dale Enterprise station site is situated in the rolling hills east of the Shenandoah Valley, more than a mile from the nearest suburban style subdivision and over three miles from the center of the nearest “urban” development, Harrisonburg, Virginia, a town of 44,000 population.

Other than the shift to an electronic sensor in 1994, and the need to fill in the 9 months of missing reports, there is no reason to adjust the raw temperature data as reported by the Weather Bureau.
Here is a plot of the raw data from the Dale Enterprise station.

There may be a step-wise drop in reported temperature in the post-1994 period.  Virginia does not provide other rural stations that operated electronic sensors over a meaningful period before and after the equipment change at Dale Enterprise, nor is there publicly available data comparing the thermometer and electronic sensor data for this station.  Comparison with urban stations introduces a potentially large warm bias over the 20 year period from 1984 to 2004.  This is especially true in Virginia as most such urban sites are typically at airports where aircraft equipment in use and the pace of operations changed dramatically over this period.
Notably, neither NCDC nor GISS adjusts for this equipment change.  Thus, any bias due to the 1994 equipment change remains in the record for the original data as well as the NCDC and GISS adjusted data.
The NCDC adjustment
Although many have focused on the changes GISS made from the NCDC data, the NCDC “homogenization” is equally interesting, and as shown in this example, far more difficult to understand.
NCDC takes the originally reported data and adjusts it into a data set that becomes a part of the United States Historical Climatology Network (USHCN).  Most researchers, including GISS and the East Anglia University Climate Research Center (CRU) begin with the USHCN data set.   Figure 2 documents the changes NCDC made to the original observations and suggests why, perhaps, one ought begin with the original data.

The red line in the graph shows the changes made in the original data.  Considering the location of the Dale Enterprise station and the lack of micro-climate bias, one has to wonder why NCDC would make any adjustment whatever.  The shape of the red delta line indicates these are not adjustments made for purposes of correcting missing data, or for any obvious other bias.  Indeed, with the exception of 1998 and 1999, NCDC adjusts the original data in every year!  [Note, when a 62 year old Ph.D. scientist uses an exclamation point, their statement is rather to be taken with some extraordinary attention.]
This graphic makes clear the need to “push the reset button” on the USHCN.  Based on this station, alone, one can argue the USHCN data set is inappropriate for use as a starting point for other investigators, and fails to earn the self-applied moniker as a “high quality data set.”
The GISS Adjustment
GISS states that their adjustments reflect corrections for the urban heat island bias in station records.  In theory, they adjust stations based on the night time luminosity of the area within which the station is located.  This broad-brush approach appears to have failed with regard to the Dale Enterprise station.  There is no credible basis for adjusting station data with no micro-climate bias conditions and located on a farm more than a mile from the nearest suburban community, more than three miles from a town and more than 80 miles from a population center of greater than 50,000, the standard definition of a city.  Harrisonburg, the nearest town, has a single large industrial operation, a quarry, and is home to a medium sized (but hard drinking) university (James Madison University).  Without question, the students at JMU have never learned to turn the lights out at night.  Based on personal experience, I’m not sure most of them even go to bed at night.  This raises the potential for a luminosity error we might call the “hard drinking, hard partying, college kids” bias.  Whether it is possible to correct for that in the luminosity calculations I leave to others.  In any case, the lay out of the town is traditional small town America, dominated by single family homes and two and three story buildings.  The true urban core of the town is approximately six square blocks and other than the grain tower, there are fewer than ten buildings taller than five stories.  Even within this “urban core” there are numerous parks.  The rest of the town is quarter-acre and half-acre residential, except for the University, which has copious previous open ground (for when the student union and the bars are closed).
Despite the lack of a basis for suggesting the Dale Enterprise weather station is biased by urban heat island conditions, GISS has adjusted the station data as shown below.  Note, this is an adjustment to the USHCN data set.   I show this adjustment as it discloses the basic nature of the adjustments, rather than their effect on the actual temperature data.

While only the USHCN and GISS data are plotted, the graph includes the (blue) trend line of the unadjusted actual temperatures.
The GISS adjustments to the USHCN data at Dale Enterprise follow a well recognized pattern.  GISS pulls the early part of the record down and mimics the most recent USHCN records, thus imposing an artificial warming bias.  Comparison of the trend lines is somewhat difficult to see in the graphic.  The trends for the original data, the USHCN data and the GISS data are: 0.24,
-0.32, and 0.43 degrees C. per Century, respectively.
If one presumes the USHCN data reflect a “high quality data set”, then the GISS adjustment does more than produce a faster rate of warming, it actually reverses the sign of the trend of this “high quality” data.  Notably, compared to the true temperature record, the GISS trend doubles the actual observed warming.
This data presentation constitutes only the beginning analysis of Virginia temperature records.  The Center for Environmental Stewardship of the Thomas Jefferson Institute for Public Policy plans to examine the entire data record for rural Virginia in order to identify which rural stations can serve as the basis for estimating long-term temperature trends, whether local or global.  Only a similar effort nationwide can produce a true “high quality” data set upon which the scientific community can rely, whether for use in modeling or to assess the contribution of human activities to climate change.
David W. Schnare, Esq. Ph.D.
Director
Center for Environmental Stewardship
Thomas Jefferson Institute for Public Policy
Springfield Virginia
===================================
UPDATE: readers might be interested in the writeup NOAA did on this station back in 2002 here (PDF, second story). I point this out because initially NCDC tried to block the surfacestations project saying that I would compromise “observer privacy” by taking photos of the stations. Of course I took them to task on it when we found personally descriptive stories like the one referenced above and they relented. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e27da78',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
WUWT readers may remember when Bishop Hill wrote Caspar and the Jesus paper. It was a wonderful narrative of the complex subject of tree rings and Steve McIntyre’s quest with debunking the Mann MBH98 paper, which created the original hockey stick. Now Bishop Hill has done it again with another great narrative. – Anthony

The Yamal implosion
September 29, 2009   Climate
There is a great deal of excitement among climate sceptics over Steve McIntyre’s recent posting on Yamal. Several people have asked me to do a layman’s guide to the story in the manner of Caspar and the Jesus paper. Here it is.
The story of Michael Mann’s Hockey Stick reconstruction, its statistical bias and the influence of the bristlecone pines is well known. McIntyre’s research into the other reconstructions has received less publicity, however. The story of the Yamal chronology may change that.
The bristlecone pines that created the shape of the Hockey Stick graph are used in nearly every millennial temperature reconstruction around today, but there are also a handful of other tree ring series that are nearly as common and just as influential on the results. Back at the start of McIntyre’s research into the area of paleoclimate, one of the most significant of these was called Polar Urals, a chronology first published by Keith Briffa of the Climate Research Unit (CRU) at the University of East Anglia. At the time, it was used in pretty much every temperature reconstruction around. In his paper, Briffa made the startling claim that the coldest year of the millennium was AD 1032, a statement that, if true, would have completely overturned the idea of the Medieval Warm Period.  It is not hard to see why paleoclimatologists found the series so alluring.
Keith Briffa
 
Some of McIntyre’s research into Polar Urals deserves a story in its own right, but it is one that will have to wait for another day. We can pick up the narrative again in 2005, when McIntyre discovered that an update to the Polar Urals series had been collected in 1999. Through a contact he was able to obtain a copy of the revised series. Remarkably, in the update the eleventh century appeared to be much warmer than in the original – in fact it was higher even than the twentieth century. This must have been a severe blow to paleoclimatologists, a supposition that is borne out by what happened next, or rather what didn’t: the update to the Polar Urals was not published, it was not archived and it was almost never seen again.
Read the rest here at Bishop Hill’s blog, and be sure to leave a nice comment if you like his writing.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92b07887',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"It’s now possible to predict how likely an endangered species is to go extinct, with mathematical models acting as windows into the future. These models help scientists foresee how a population is likely to react to changes in the environment and therefore how likely it is to die out.  As the sixth mass extinction event rumbles on, this represents a powerful tool in the arsenal of conservationists. However, accurate information about species in the wild is still crucial to inform these models and divine the fate of wildlife populations. A species’ risk of extinction depends on how many individuals in its populations can reproduce and how long they can survive. To design management programmes that can prevent extinction, it’s essential to understand how survival and reproductive rates change within a population as the environment changes. We know that the number of individuals surviving and reproducing – known collectively as a species’ demographic rates – vary each year in response to environmental conditions. These include the availability of food and water, rainfall or extreme temperatures. As global temperatures increase and disrupt local weather, how variable these environmental conditions become will increase the extinction risk for many species. To predict if a population is likely to go extinct, we need to predict how changes in these environmental conditions will affect the population’s demographic rates and how the number of individuals in the population will change annually. 


      Read more:
      Climate change: effect on sperm could hold key to species extinction


 Models can take the likelihood of individuals in the population to survive and reproduce at different ages, and “shock” them in the same way the environment does.  It would be impossible to directly model how species respond to changes in rainfall or other conditions in the environment, as different species thrive under different conditions.  However, these “shocks” in the model – which disturb a species’ survival and reproductive rates – reproduce the random nature of the environment. By running the model many times to generate multiple outcomes, we can calculate what percentage of the populations will go extinct as the environment changes, and how long it will take them to die out.  In our recent study we found that simplifying the information used to construct these models can distort the predictions. Survival and reproductive rates vary in individuals as they age and each species has a very particular trend associated with their age. Until recently, researchers and wildlife managers assumed it was enough to represent these rates as constant as individuals age. This would assume that juveniles and adults all respond uniformly to changes in the environment. However, in our study we showed that reducing a population into general age classes can greatly distort the population’s predicted growth rate and our understanding of a population’s chances of avoiding extinction. Grouping individuals into general age classes can give the illusion that a population’s chances of extinction are slim when in reality, it will fast go extinct. We also found that it’s important to consider trade-offs between survival and reproduction. A good year for reproduction may result in a bad year for survival. This is because more energy invested in reproducing means there’s less left over for the healthy upkeep of the body. Despite increasing evidence of these trade-offs in nature, models tend to ignore them. We showed that ignoring these seemingly minor issues can harm a model’s accuracy in predicting the fate of populations in their natural environment. This reduces their capacity to anticipate the impacts of climate change, invasive species or habitat loss.  There is still hope for many endangered species, but preparation needs to be made. For this, we need to continue developing more sophisticated models with more accurate information about each species and their environment. With reliable foresight, we can give them the fighting chance they need in an uncertain future."
"Sometimes it has felt as if the rain might never stop. These storms have gone beyond the point of simply being storms now, each blurring into the next to create a strangely end-of-days feeling. Everything is freakishly sodden and swollen, and while the rural flood plain on which I live fortunately hasn’t flooded anything like as badly as some, the rivers are rising alarmingly. Yet still the lashing winds and biblical downpours keep coming. Suddenly the 40 Days of Action campaign that Extinction Rebellion (XR) will launch on Ash Wednesday (26 February), encouraging people to reflect on the environmental consequences of their actions in a kind of green Lent, feels ominously well named.  This week’s stunt in Cambridge, where XR activists dug up Trinity College’s lawn in symbolic protest at the college’s plans to build on land it owns in rural Suffolk, may be just the beginning. Some ask why these activists aren’t out stacking sandbags for the poor householders of the Wye valley, or canoeing through the streets of Mytholmroyd, West Yorkshire, highlighting the risks of a climate crisis that can only mean more freak flooding. Yet in some ways that was the point of targeting Trinity in the first place. Of all the Cambridge colleges, it’s the one identified by student journalists – using freedom of information requests – as the biggest investor in fossil fuel companies blamed for aggravating the climate crisis. Activists blockaded a research building run by the oil exploration company Schlumberger as well as making holes in the lawn. The clear aim is to make it toxic for institutions to maintain ties to polluting industries; and what makes universities tempting targets is that they’re already being hammered from inside by students raging against what they see as dirty money. But universities are not alone. This week Amazon chief Jeff Bezos announced he was giving $10bn (£7.7bn) to fight the climate crisis, provoking much the same complaints that greeted BP’s recent vow to go carbon-neutral by 2050: it’s not enough, it’s too vague, it’s just greenwashing. And yes, obviously Bezos should tackle his own company’s carbon footprint first, not to mention treating staff better and paying more tax if he has billions to spare. But until governments have the guts to legislate for all of that, then we are where we are, which is in danger of missing a sea change in corporate life. Executives in polluting industries haven’t quite reached the nadir of bankers after the financial crash, cold-shouldered at school gates and berated in the street, but the more enlightened can see something similar coming if they’re not careful. When BP’s new chief executive, Bernard Looney, made his carbon neutral announcement, following a similar pledge from British Airways, one key factor cited was pressure from staff. It hurts when your company is spurned by the likes of the Royal Shakespeare Company, which ended its sponsorship deal with BP last year amid climate protests, not long after the actor Mark Rylance compared the firm to an arms dealer or tobacco company. At Amazon, too, Bezos had felt the heat internally with hundreds of staff protesting publicly against the company’s links to oil and gas exploration. And if younger staff are making waves now, then the climate will be an even harder red line for the graduates these companies need to recruit in future. Two-thirds of American teens now think oil and gas companies create more problems in the world than they solve, according to a report from management consultants EY. Generation Z want to work for ethical companies that make them feel good about themselves, and increasingly see jobs that fuel climate change as morally suspect. Who wants to spend a first date plaintively explaining why working in Big Oil doesn’t make them a bad person? It may sound ridiculous to their parents’ generation, for whom energy companies were the ones keeping the lights on, but even those with no such qualms must wonder if there’s much future with fossil fuel companies – squeezed between the political rock of legal commitments to hit zero emissions by 2050 and a public hard place that gets harder with every flood or bush fire. It’s not consumer boycotts driving this, so much as social stigma. It’s tough to go without these companies’ products – there was outrage when a bursar at St John’s College, Oxford, responded to student demands to divest immediately from fossil fuels with a tongue-in-cheek offer to turn their heating off if they were that worried – although the intention was to make the students think, not freeze. But noisy public disapproval costs absolutely nothing, which makes it a powerful weapon. Add in shareholder pressure, driven by new government rules requiring pension funds to take account of climate risk, and the heat is really on. Suddenly Bezos’s gesture starts to look positively cheap in comparison with being forced to change his business model. Yet, whatever the motivation, it’s still one of the biggest philanthropic donations in recent history, and it shows which way a howling wind is blowing. How to spend those billions? Bill and Melinda Gates argued this month that private philanthropists should be “swinging for the fences”, taking the big risks governments can’t take with public money. So perhaps the Bezos fund will simply go on a few high-profile scientific gambles. But the radical choice would be to spend some of it funding movements within corporate and institutional life, pushing the foot-draggers to act while they’re still in control of the situation. Better to move fast than wait for activists to dig up your lawn; better to act now, before the river of public anger bursts its banks. • Gaby Hinsliff is a Guardian columnist"
"Imagine “carbon emissions”, and what springs to mind? Most people tend to think of power stations belching out clouds of carbon dioxide or queues of vehicles burning up fossil fuels as they crawl, bumper-to-bumper, along congested urban roads. But in Britain and many other countries, carbon emissions have another source, one that is almost completely invisible. In the UK, these overlooked emissions come from our most extensive semi-natural habitat, yet it is a habitat which is almost invisible within the national consciousness. The source of these emissions can be seen in the rich black peat soils of the East Anglian Fens, the Lancashire lowland plain, the Somerset Levels, the Forth Valley and indeed many lowland river flood plains, as well as in the hugely damaged peat soils of the UK’s uplands. The common thread here is “peat”, a soil derived almost entirely from semi-decomposed plant remains which have accumulated over thousands of years because the ground is waterlogged. Such peat soils are immensely carbon-rich because they largely consist of organic matter. Globally, peatlands contain more carbon than all the world’s vegetation combined.  Despite this, peatlands rarely feature in our cultural consciousness other than as areas of struggle – “stuck in the mire”– or as places of despair or danger. In the uplands, beyond the boundary of cultivated land, extensive peat bogs are lost in the all-embracing term “moorland”, which is more of a cultural term than anything ecologically meaningful. At lower altitudes, living peatland has all but vanished. Britain has drained its fens and converted the land into highly productive fields. Much of East Anglia was once a vast fen peatland, for instance, but just 3% of the original habitat remains today, in small scattered fragments. Such losses are mirrored throughout Europe, while much of the debate about palm oil and forest fires in South-East Asia is actually about the draining and conversion of peatland swamp forest. When peat soils are drained, the ground surface sinks, which is why large parts of East Anglia and the western Netherlands now lie below sea level. This is partly because peat shrinks and becomes more compact when it dries out, but there is also another key reason. Carbon in the now-dry peat reacts with oxygen to form carbon dioxide so each year some of the soil simply vanishes into the atmosphere as a greenhouse gas. While a sinking ground surface does pose ever-increasing flood risk, it is the release of CO₂ that has far wider implications. Every hectare (one and a bit football pitches) of tilled peat soil with a water table lowered to 50 cm or more below the ground surface emits somewhere between 12 and 30 tonnes of CO₂ equivalent (that is, all greenhouse gases, including CO₂) per year. To put this into context, that’s ten times the emissions of an average modern car travelling 10,000 miles per year. In fact, the total CO₂ emitted each year from just the East Anglian Fens and the UK’s damaged upland peat soils may be equivalent to around 30% of the country’s annual car emissions. The irony here is that, although these peat soils were created precisely because they were wetlands, and wetlands are some of the most productive ecosystems on Earth, farming tends to celebrate dryness. Our agricultural system is based on ideas that spread from the dry semi-desert conditions of the Middle East during the Neolithic shift from hunter-gathering to settled farming. Farming has thus been dominated for the past 5,000 years by the principle that dry land is good and wet land is bad – indeed, a farmer who tolerates significant areas of wet ground on the farm is still widely regarded as a poor farmer. Change is in the air, however. International climate obligations mean that countries are having to reduce their greenhouse gas emissions, and in many parts of the world there are also increasing concerns about the spiralling costs of flooding. No wonder many researchers are now looking at the agricultural possibilities of re-wetting former wetlands in order to establish new forms of farming based on productive wetland species.  In Germany, for instance, a type of “bulrush” is already being used to produce fire-resistant building board. At the University of East London we are currently testing two potential crops: sphagnum bog moss as a replacement for peat in garden-centre “grow bags”, and “sweet grass” as a food crop.  In only a few decades, traditional dryland farming on drained peat soils will be increasingly difficult as the rich organic soils vanish and flood prevention becomes too costly. By instead re-establishing wetland conditions, farms could reduce the risk of floods and retain the existing reservoirs of soil carbon but also potentially add new carbon to these long-term stores. Indeed, the longer-term vision of farming for carbon as well as food, and all the other ecosystem benefits that come from healthy peatland ecosystems, may already be upon us. It is part of the UK government’s 25 year environment plan, and environment secretary Michael Gove has pointedly signalled his support.  Such a longer-term vision is also deftly expressed in a film titled “The Carbon Farmer” by Andrew Clark, which looks at what life might be like for a carbon farmer three or four generations from now:  Everything in the film is already at least possible in one form or another. Our task is now to make it probable."
"With the California Democratic primary taking place on Super Tuesday this presidential season, the most populous, delegate-rich state in the US will have more influence than ever over choosing the party’s nominee. That influence will reflect the particular priorities of California Democratic primary voters, who in a December poll named the climate crisis as their highest priority for the next president. In the Golden state, climate priorities are not just a matter of lowering emissions and preventing further catastrophe, but also planning to adapt for the kinds of disasters Californians have come to know all too well. Rising seas lap at communities up and down the Pacific coast, and devastating wildfires since 2017 have killed more than 150 people and destroyed more than 35,000 structures. All the candidates have pledged to end new fossil fuel operations on federal lands, after the Trump administration approved new leases for oil drilling on federal land in California in December. But how else do their plans compare on climate and California?  Joe Biden’s $5tn climate plan leans hard on his record as vice-president and role in passing the 2009 Recovery Act, which made the biggest investment in clean energy in US history. It calls for a new collaborative agency focused on climate and innovation and 100% net zero emissions by 2050. Biden’s plan is unique in blaming urban sprawl for its impact on climate. He says he will work on altering local regulations to allow for denser housing near public transit to cut commute times and decrease the carbon footprint of sprawly areas. Policies like those have proven highly controversial in California, where the legislature has repeatedly rejected statewide zoning reforms that would promote denser, greener development. Biden’s plan is unique in calling for the insurance industry to collaborate on lowering premiums for homeowners and communities that invest in resilience – particularly relevant for Californians who live in high wildfire risk areas and have faced soaring insurance premiums or loss of policies altogether. Since he entered the race late, Mike Bloomberg’s policy proposals have escaped much of the scrutiny that other candidates have weathered. His climate plan pledges 80% clean energy by 2028 and 100% emission-free new passenger vehicles by 2035. Bloomberg has a separate climate resilience plan, pledging to invest in underserved communities, prioritize the most vulnerable and mitigate climate hazards. And in an apparently continuation of the candidate’s focus on California, there’s also a specific wildfire resilience plan, by far the most extensive of any of the candidates’ efforts, which would raise federal funding for fire resilience and management to $10bn, prevent insurers from dropping homeowners in high fire-risk areas and create a new “wildfire corps” with thousands of new jobs, all with the goal of reducing wildfire-caused loss of life and property by 50% by 2024. Pete Buttigieg’s $2tn climate plan pledges to reach net-zero electricity by 2035 and net-zero emissions from industry by 2050. The plan calls for investments in clean technology and climate resilience alike. The plan calls for establishing a National Catastrophic Extreme Weather Insurance program that would cover Americans facing climate-driven disasters and earthquakes – good news for Californians. “The government would also create an exchange for families to purchase catastrophic insurance subsidized by the government depending on income level,” the plan reads. It also references the impacts of drought on California farmers, and disproportionate impacts on immigrant farmworkers specifically. Amy Klobuchar’s climate plan includes a $1tn package of investment for climate research, tax incentives to spur research and development through public-private partnerships and other projects. She says she would bring back fuel economy standards, which have proven a serious point of contention as the Trump administration has weakened the standards for cars and light trucks and has challenged the right of California and other states to follow more stringent standards. Klobuchar wants to “mobilize the heartland” to build for climate resiliency. Her plan would upgrade levees for more frequent and severe floods in the midwest – but there’s no plan for California fires. Bernie Sanders is pitching a $16.3tn public investment to reduce emissions and prepare America for the impacts of climate crisis. His plan includes 100% renewable energy for electricity and transportation by 2030, and full decarbonization by 2050. Sanders’ climate plan includes a pledge to transition America’s investor-owned utility companies to public ownership – a big dig at troubled PG&E. Extensive grants to purchase new electric vehicles would be a boost to California drivers, who are on the road more than the American average. The plan also a long section on firefighting, pledging to invest $18bn in the federal firefighting workforce. Community resiliency funding would go toward preparing for climate impacts, including wildfire evacuation plans, and additional sea walls – which, in California, have actually been known to contribute to coastal erosion. Elizabeth Warren’s $10tn climate plan calls for 100% net-zero emission electricity by 2035, 100% new clean vehicles by 2030, and 100% fossil-fuel-free new buildings by 2028 – the three industries she blames for making consumers think the climate crisis is their own fault for using plastic straws. The plan aims to create more than 10m green jobs. Those investments would prioritize frontline communities and environmental justice. Overall Warren’s plan focuses on addressing the causes of climate change more than its impacts. While there is copious detail on how and why to transition the industries to blame, there’s little to nothing on adaptation and community resiliency against the threats the candidate briefly outlines at the start of her plan."
"A worldwide wave of school climate strikes, begun by the remarkable Greta Thunberg, has reached the UK. Some critics claim these activist-pupils are simply playing truant, but I disagree. Speaking as both a climate campaigner and an academic philosopher, I believe school walkouts are morally and politically justifiable. Philosophy can help us tackle the question of whether direct action is warranted via the theory of civil disobedience. This states that, in a democratic society, one is justified in disobeying the law only when other alternatives have been exhausted, and the injustice being protested against is grave. In the case of the climate school strikes, it is without question that the injustice – the threat – is grave. There is none graver facing us. It appears reasonable to claim furthermore that other alternatives have indeed been exhausted. After all, people have been trying to wake governments up to the climate threat for decades now, and we are still as a society way off the pace set out even by a conservative organisation such as the IPCC. But if that claim was strongly contested, and it was suggested that climate activism should continue to focus on conventional electoral politics, then attention might revert to the assumed premise that society is democratic. Do people in Britain and elsewhere really live in “democracies”, given (for instance) the vastly greater power of the rich, and of owners of media, to influence elections, compared to everyone else?  I don’t want to adjudicate whether we really live in a democracy. But what of course makes this a particularly salient question for school strikes is the simple fact that in any case children have no voice in this democratic system. And yet the climate crisis and the perhaps equally catastrophic biodiversity crisis will affect children much more than adults.  Our “democratic” system seems to have a built in present-centricness, and a weakness in relation to issues of long-term significance, that seriously undermines its claims to democratic legitimacy. Thus philosophers have sometimes argued, beginning with Edmund Burke in the 18th century, that to make the system truly democratic we would need to somehow include – and give real power to – the voices of the past and the future in that system. Most especially, for they are at risk of suffering the worst: the voices of children and indeed of unborn future generations. 


      Read more:
      Why don't teenagers have a greater say in their future?


 So, a forceful argument could be made that it must be legitimate for children to take part in climate actions, for they do not even have recourse to the democratic channels (such as they are) that adults take for granted. This is especially true once we add that it seems reasonable for children to object to schooling that may well be rendered irrelevant by a climate-induced catastrophe. For example, much of the way that economics, business studies and IT are taught presupposes a world that will probably soon cease to exist. If you are convinced by this, then all well and good. However, at this point, I want to pull the rug slightly from under the argument that I’ve made so far. I put it to you that, if you are an adult, as I am, then your view in any case is somewhat beside the point.  For the brutal fact is that, try hard though some of us have done, we adults have categorically failed our children. This is a grievous wrong, perhaps the worst thing that mammals, primates, such as ourselves, can do: to have let down those who we claim to love more than life itself. We have set our children on a path to a “future” in which society as we know it may have collapsed. And even if we accomplish an unprecedented societal transformation over the next decade, the massive time-lags built into the climate system mean things will still get worse for a long time to come. And so on this occasion we adults ought to humbly realise that it is no longer for us to tell our children what to do. We ought rather to take up the role of supporting them in their uprising, asking how we can help them in their struggle for survival. They are inspiring us, now. The ultimate reason why we should support these school strikes, as I and hundreds of other UK academics have just declared we will do, is that, through our inaction that has led the world they will inherit to this pretty pass, we adults have forfeited the moral right to do anything else. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitter Reader Bernd Felsche has a public service message for those of us living in Germany:
Summer in Germany this year will be on August 2, 2011, from 1:30 pm to 3:30 pm.
A whole 2 hours. Sorry, but that’s all that’s in store for summer in Germany this year.
Germany over the last weeks has been gripped by unusually cool and wet weather, and so many are wondering whatever happened to  summer. The forecast for the days ahead according to the English-language The Local doesn’t look good either as rain and cool with temperatures stuck in the 60s remaining the rule.
Things are not expected to improve until Monday, with summer briefly returning on Tuesday. The Local writes:
Tuesday would be the pick of next week, with highs ranging from 24 to 29 degrees and plenty of sunshine right across the country.
Cold and wet for the next 4 weeks
Are the barbecue summer conditions going to persist? The Local writes:
And the good news? There isn’t any, said DWD [German Weather Service] meteorologist Andreas Friedrich.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




‘A look at the meteorological crystal ball, meaning predictions for the next four weeks, offers nothing good for anyone who still hopes for steady and warm summer weather,’ he said. ‘For the period to the end of August, we can only infer trends but, well, this shows the arrow regarding temperatures pointing further downward’.
Long-term forecasts, Friedrich reminds us, are only 60% accurate, and so there is still some hope that we may end up getting some nice weather to salvage the summer before it ends.
Peddling quackery
Speaking of the DWD (German Weather Service) – which has become one of the key propaganda arms of the German global warming movement – it says here Germans will be able to enjoy nicer summers by the year 2100. The DWD writes in its July 26 press release:
Climate simulations show for Germany further warming of 2 to 4°C by the year 2100. Drier summers and wetter winters and more extreme weather events are anticipated.”
Here they are not talking about plain old longer-term weather forecasts over the next 4 weeks, but of “climate simulations” for the next 90 years. Their accuracy, scientists claim, with the DWD agreeing, are really worth taking to the bank!
Forget it folks. Nobody can make such predictions. That’s pure utter quackery.

Unfortunately, this year’s German summer is turning out to be just the opposite of heat and sunshine (once again!).
Share this...FacebookTwitter "
"
click for larger image
From environmentalist Jennifer Marohasy’s blog in Australia, please pay her a visit here – Anthony
There has been criticism of the potential for official weather stations in the USA to record artificially high temperatures because of the changing environments in which they exist, for example, new asphalt, new building or new air conditioning outlets.   Meteorologist, Anthony Watts, has documented evidence of the problem and Canadian academic, Ross McKitrick, has attempted to calculate just how artificially elevated temperatures might be as a consequence.
A reader of this blog, Michael Hammer, recently studied the official data from the US official weather stations and in particular how it is adjusted after it has been collected.   Mr Hammer concludes that the temperature rise profile claimed by the US government is largely if not entirely an artefact of the adjustments applied after the raw data is collected from the weather stations.
 
 
Does the US Temperature Record Support Global Warming?
By Michael Hammer
IN the US, the National Oceanic and Atmospheric Administration (NOAA) collects, analyses and publishes temperature data for the United States.   As part of the analysis process, NOAA applies several adjustments to the raw data.
If we consider, the above graph, which shows, their plot of the raw data  (dark pink) and the adjusted data (pale pink), it is obvious that the adjustments have little impact on data from early in the 20th century but adjust later temperature readings upwards by an increasing amount.  This means that the adjustments will create an apparent warming trend over the 20th century.  [Click on the above chart for a better larger view, this chart can also be viewed at http://cdiac.ornl.gov/epubs/ndp/ushcn/ndp019.html .]
NOAA state that they adjust the raw data for five factors.  The magnitude of the adjustments are shown in Figure 2.

 
Figure 2.  Form of individual corrections applied by NOAA. The black line is the adjustment for time of observation.  The red line is for a change in maximum/minimum thermometers used.  The yellow line is for changes in station siting. The pale blue line is for filling in missing data from individual station records. The purple line is for UHI effects (this correction is now removed).  [Click on the chart for a better larger view or visit the same website as for Figure 1.]
It is obvious that the only adjustment which reduces the reported warming is UHI which is a linear correction of 0.1F or about 0.06C per century, Figure 2.  Note also that the latest indications are that even this minimal UHI adjustment has now been removed in the latest round of revisions to the historical record.  To put this in perspective, in my previous article on this site I presented bureau of meteorology data which shows that the UHI impact for Melbourne Australia was 1.5C over the last 40 years equivalent to 3.75C per century and highly non linear.
Compare the treatment of UHI with the adjustments made for measuring stations that have moved out of the city centre, typically to the airport.  These show lower temperatures at their new location and the later readings have been adjusted upwards so as to match the earlier readings.  The airport readings are lower because the station has moved away from the city UHI.  Raising the airport readings, while not adding downwards compensation for UHI, results in an overstatement of the amount of warming. This would seem to be clear evidence of bias.  It would be more accurate to lower the earlier city readings to match the airport readings rather than vice versa.
Note also the similarity between the shape of the time of observation adjustment and the claimed global warming record over the 20th century especially the steep rise since 1970.  This is even more pronounced if one looks at the total adjustment shown in Figure 3 (again from the same site as Figure 1).  As a comparison, a recent version of the claimed 20th century global temperature record downloaded from  www.giss.nasa.gov is shown in Figure 4.

Figure 3.  Magnitude of the total correction applied by NOAA  
 
[Click on the charts for a larger/better view.]

Figure 4.  Temperature anomaly profile from NASA GISS
Since the total corrections for the US look so similar to the claimed temperature anomaly, it begs the questions as to what the raw data looks like without any corrections.  Does it show the claimed rapidly accelerating warming trend claimed by the AGW advocates?  To determine this I took the raw data from the USHCN graph shown in Figure 1 and plotted this using  a 5 year mean (blue trace), matching the smoothing in the NASA GISS profile shown in Figure 4.  The result is shown in Figure 5.  Please note that while the plot is one that I generated, the data comes directly from the raw data from Figure 1 published by NOAA.

Figure 5  Plot of raw temperature data versus time (from fig 1) 5 point smoothing. Vertical axis degrees Fahrenheit.  Red line is a linear trend line. Green line is a 2nd order (parabolic) trend line. 
Clearly the shape of this graph bears no similarity at all to the graph shown in Figure 4.  The graph does not even remotely correlate to the shape of the CO2 versus time graph.  The warming was greatest in the 1930’s before CO2 started to rise rapidly.  The rate of rise in 1920, the early 1930’s and the early 1950’s is significantly greater than anything in the last 30 years.  Despite the rapid rise in CO2 since 1960, the 1970’s to early 1980’s was the time of the global cooling scare and looking at the graph in Figure 5 one can see why (almost 2F cooling over 50 years).
A linear least squares trend line, created using the Excel trend line function (Red trace)  shows a small temperature rise of 0.09C per century which is far less than the rise claimed by AGW supporters and clearly of no concern.  However, the data shown in figure 5 bears little if any resemblance to a linear function.  One can always fit a linear trend line to any data but that does not mean the fitted line has any significance.  For example, if instead I fit a second order trend line (a parabolic) the result is extremely different.  That suggests a temperature peak around 1950 with an underlying cooling trend since.  Which trend line is the more significant one?  If there was really a strong underlying linear rise over the time period it should have shown up in the 2nd order trend line as well.  This suggests that it is questionable whether any relevant underlying trend can be determined from the data.
It would appear that the temperature rise profile claimed by the adjusted data is largely if not entirely an artefact arising from the adjustments applied (as shown in Figure 3), not from the experimental data record.  In fact, the raw data does not in any way support the AGW theory.
Based on this data, the US temperature data does not correlate with carbon dioxide levels.  The warming over the last 3 decades is completely unremarkable and if present at all is significantly less than occurred in the 1930’s.  It is questionable whether any long term temperature rise over the 20th century can be inferred from the data but if there is any it is far less than claimed by the AGW proponents.
The corrected data from NOAA has been used as evidence of anthropogenic global warming yet it would appear that the rising trend over the 20th century is largely if not entirely an artefact arising from the “corrections” applied to the experimental data, at least in the US, and is not visible in the uncorrected experimental data record.
This is an extremely serious issue.  It is completely unacceptable, and scientifically meaningless, to claim experimental confirmation of a theory when the confirmation arises from the “corrections” to the raw data rather than from the raw data itself.  This is even more the case if the organisation carrying out the corrections has published material indicating that it supports the theory under discussion.  In any other branch of science that would be treated with profound scepticism if not indeed rejected outright.  I believe the same standards should be applied in this case.
*********************
Notes and Links
Interestingly, there was an earlier version of the NASA GISS data shown in Figure 4 which was originally published at http://www.giss.nasa.gov/data/update/gistemp/graphs/FigD.txt While this site has now been taken down the data was apparently archived by John Daly and available at his website http://www.john-daly.com/usatemps.006.  The data is presented in tabular form rather than graphical form but appears to be either identical or extremely similar to that shown in my Figure 5.
Other contributions from Michael Hammer can be read here: http://jennifermarohasy.com/blog/author/michael-hammer/
[scroll down, click on the title for the full article]
Ross McKitrick, Ph.D. – Quantifying the Influence of Anthropogenic Surface Processes on Gridded Global Climate Data
http://www.heartland.org/events/NewYork08/newyork2008-video.html
Anthony Watts – http://wattsupwiththat.com/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e953748b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Amur tiger (Panthera tigris ssp. altaica), also known as the Siberian Tiger, is among the world’s rarest and most endangered cat species. The largest and northernmost tiger, it is believed only around 450 of these magnificent, 200kg, three-metre-long cats remain in the wild. However even this precarious situation is better than in the 1930s, when hunting reduced them to the brink of extinction – around only 20 animals. Today the threat they face comes from illegal poaching for body parts taken for Chinese medicine, poaching of their prey such as deer and boar, and steady destruction by logging of their habitat which makes their long term survival precarious.  Such low numbers in the 1930s and 1940s left the species with a “bottleneck” of low genetic diversity, susceptibility to disease, and poor cub survival rates. Recently an infectious disease that causes viral encephalitis, canine distemper, has been identified as a significant threat to the species’ survival. Researchers estimate that this virus has killed at least 1% of Amur tigers since 2009.  But the worldwide network of zoos can provide a bulwark against extinction. The population of Amur tigers in captivity, at around 480 in 185 institutions, is perhaps larger than those in the wild. The World Association of Zoos and Aquariums (WAZA) made the Amur tiger a focus of its global species management plan, bringing together the world’s four key regional zoo associations to maintain sustainable captive populations of Amur tigers as a genetic “lifeboat” for those in the wild. The plan also provides an important political framework through which conservationists and governments can share information and cooperate internationally. Almost all the remaining Amur tigers live in the far east of Russia, mostly in the Sikhote Alin mountain region, and there is little genetic exchange between tigers there and the much smaller sub-population found in southwest Primorye province.  A small population exists in China but it depends on animals moving across the border with Russia. It’s not known whether there are still tigers in North Korea.  The forests of the Russian Far East are declining rapidly due to large-scale illegal logging, mainly to supply Chinese furniture and flooring manufacturers, many of which then export to the US and Europe. This over-harvesting of trees reduces the supply of pine nuts and acorns, which are the main food sources for the tigers’ prey.  Uncontrolled forest fires and agricultural burn-offs are also reducing the tiger’s habitat. Russia became the first country in the world to grant the tiger full protection, and its ban on tiger hunting in 1947 was instrumental in preventing the species from being hunted to extinction. Assisted by organisations such as WWF, Russia now has a national action plan for the Amur tiger, with a major step forward being the establishment of the Sredneussuriisky Wildlife Refuge in 2012. Covering nearly 180,000 acres, this allows Amur tigers to cross the Russia-China border, bringing together otherwise isolated tiger sub-populations.  So while conservation efforts in recent years have seen the IUCN reclassify the species down from critically endangered to endangered, poaching and logging of their habitat is still a problem. Stricter penalties for wildlife crime and banning pine harvesting in tiger forests is crucial.  Sergei Bereznuk is a key player in Russian tiger conservation. The recipient of a 2012 Rolex Award for Enterprise, Bereznuk lobbies against illegal poaching and threats to tiger habitat and heads a small NGO, the Phoenix Fund.  In partnership with the Zoological Society of London and the Wildlife Conservation Society, the Phoenix Fund has improved anti-poaching efforts using better data recording and software tools, and promotes awareness and education for locals living in the area. A key educational event is the annual Tiger Day Festival in Vladivostok, in which over 4,000 schoolchildren and students dressed in tiger costumes paraded through the city in 2013. The Phoenix Fund has also created a network of volunteer fire-fighting teams to tackle forest fires. Russian conservation projects also benefit from the skills of the likes of British veterinary charity Wildlife Vets International, for which John Lewis provides tiger capture and anaesthesia training to local conservation vets, and makes genetic and health assessments of samples from wild Amur tigers. But as well as providing financial support to tiger conservation, consumers in the West can play their part in preventing illegal logging in the tigers’ forests by only purchasing Forest Stewardship Council-certified (FSC) products, which provides assurance that legal, environmental, and social protections are in place and forests are managed responsibly. Perhaps the most difficult challenge is to stem the demand for tiger bone from China. Despite all trade in tiger parts being banned under international law, growing Chinese affluence has seen demand for these products – seen as a symbol of high status and wealth – increase. When one tiger can provide ten years’ income on the black market it seems an irresistible incentive for poachers."
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"

Environmental activists usually critical of electrified America must have mixed emotions this time of the year. Though it is a season of good cheer and goodwill toward all, it is also a time of conspicuous energy consumption. To many people, America the Beautiful is at her best in December when so much of the nation is illuminated by billions of tiny stringed light bulbs. Holiday lighting is a great social offering — a positive externality, in the jargon of economics — given by many to all.



While a few energy doomsayers such as Paul Ehrlich rile against “garish commercial Christmas displays,” few of today’s headline grabbers (Arianna Huffington, where are you?) have attempted to stir up debate over the generator‐​hours devoted to making the season glow. Indeed, holiday lighting seems a dazzling exception to the activists’ goal of reducing discretionary energy usage. 



But if holiday energy guzzling can be overlooked, why not excuse outdoor heating and cooling, one‐​switch centralized lighting, and instant‐​on appliances that “leak” electricity, not to mention SUVs? Prancing around to turn on individual lights or waiting for the photocopier to warm up wastes the scarcest and one truly depleting resource: A person’s time. 



Known world oil reserves are more than 20 times greater now than they were when record keeping began in the 1940s; world gas reserves are almost four times greater than they were in the 1960s; world coal reserves have risen fourfold since 1950. Transient developments, often political, can drive supplies down and prices up, but the raw mineral resource base is abundant — and expanding in economic terms thanks to an inexhaustible supply of human ingenuity and exploratory capital.



Record energy consumption has been accompanied by improving air quality. Urban air quality is a third better today than in 1970. The U.S. Environmental Protection Agency reported that air emissions of the criteria pollutants declined by 25 percent, as energy usage increased by 150 percent. Further air emission reductions are expected, but they will not be accomplished by forcing higher prices or inconvenience on consumers. Future reductions will be accomplished with market incentives, technological improvement, and regulation based on sound science, not alarmism.



Should good citizens think twice about holiday lighting, given global warming and other suspected climate changes supposedly caused by increasing atmospheric concentrations of carbon dioxide? Hardly. A moderately warmer, wetter world, whether natural or anthropogenic, such as experienced in the 20th century, is a better world. Carbon dioxide from the combustion of fossil fuels “greens” the biosphere through the well‐​documented carbon fertilization effect. But most importantly, the wealth created from affordable, plentiful energy provides the primary means for societies to improve the environment. In the final analysis, wealth produces environmental health, which explains why increasing energy usage and environmental improvement have gone hand in hand in the Western world.



There is much to be thankful for this holiday season with our energy economy. But thoughts about the less fortunate should be with us, too. The World Energy Council estimates that 1.6 billion people lack electricity for lighting, heating, cooling, or cooking. A Christmas tree for us is likely to be firewood for those living in energy poverty. For fully a fourth of the world’s population, there could be no greater holiday gift than affordable electricity, explaining why the developing world has flatly rejected proposals from environmental elites to forsake future energy usage in the quixotic quest to “stabilize climate.”



Energy consumption is good — for comfort, convenience, and even celebration. May one and all in good conscience enliven this holiday season with lights aplenty. With sources of conventional fuels steadily expanding and energy technologies rapidly advancing, Americans can look forward to even more energetic celebrations and shared goodwill in the holiday seasons ahead. 
"
"

The way we measure global temperature is once again facing scrutiny for over‐​estimating the planet’s warming trends. Our government _homogenizes_ weather data so that all nearby weather stations are all singing the same tune. It’s done to weed out bad stations or failing weather equipment. We discovered such a thing earlier this year when we found that perhaps the nation’s most politically iconic weather station — Washington DC’s Reagan National Airport — was reading temperatures that were far too hot to be plausible.



Now it turns out that the homogenization _itself_ is suspect and also producing way too much warming. Anthony Watts, a prominent climate blogger without any external financial support, revealed this in a blockbuster presentation at the fall meeting of the American Geophysical Union in San Francisco, a few days before Christmas. Along with three colleagues, he may have invalidated much of the warming in recent years in the U.S. temperature history from our National Oceanic and Atmospheric Administration.



For years, Watts and a team of volunteers set about to photograph, or obtain photos or satellite imagery, of just about every weather station that forms NOAA’s “Historical Climate Network” (HCN), which our government claims was pretty much free of nagging problems like temperature sensors being close to parking lots or, even worse, heat sources like air‐​conditioning exhaust.



It turns out they weren’t, and after assiduously poring over all the pictures, Watts and his crew classified the stations into two general groups, well‐​sited, “compliant” stations, and poorly‐​sited “non‐​compliant” ones. From the compliant group, Watts’ team further selected only those stations which had no changes whatsoever in location or observation timing during their analysis period, 1979–2008, leaving 92 of the best quality stations distributed across the U.S.





‘Homogenization’ adds systematic errors to the data rather than accounting for them.



Watts then plotted up the average temperature from the government network’s homogenized data compared to his ultra‐​clean stations.



Around 1979, the second warming trend of the 20th century began in both US and global records. The first one, from 1910 to 1945, is about the same magnitude as the second one, but couldn’t have been from dreaded carbon dioxide because we had not emitted very much for most of that period.



The second warming is important because there’s likely to be some human component to it. If you want to know what that really means, here is a shameless plug for our new book, _Lukewarming: The New Climate Science that Changes Everything_. It’s very important to see how much of it is human, or, to put it more indelicately, if a substantial fraction of it is measurement error caused by compromised weather stations, there’s likely to be quite a bit less warming in our future than has been forecast by some computer models.



Watts’ findings are spectacular. Averaged across the U.S., the government’s homogenized stations are warming at a rate _more than 50 percent greater_ than Watts’ clean ones. A whopping difference.



There’s more. Average U.S. temperatures warmed from 1979 through 1997 and then levelled off. There was also a cooling period earlier in this century. The compliant data shows less warming _and_ less cooling than the homogenized data. In other words, “homogenization” adds systematic errors to the data rather than accounting for them. Basically, the government’s procedures adjust the observations of well‐​sited stations to be closer to that of poorly‐​sited stations, rather than the obviously preferable and more scientifically appropriate _vice versa_. Thus, the government’s procedure results in an enhanced warming signal.



The U.S. surface record turns out to be in many ways representative of the behavior of the entire Northern Hemisphere, and it turns out that NOAA does the same thing to their global land records, which means that there is the very real probability that not only has the global warming been overestimated by computer models, it has been over‐​measured by homogenized data. This is yet another piece of strong evidence that the Earth is not warming as much as the UN says it should have.



For much of the last year, Washington has been abuzz with rumors that NOAA manipulated the global temperature records to get them to “disappear” the “hiatus” in global warming since the mid‐​1990s, a phenomenon that is obvious in global satellite data. Congressman Lamar Smith (R-TX), chair of the House Committee on Science, Space and Technology, seems to smelling smoke over this. It appears that Anthony Watts has found the fire.
"
"
People get busy when questions get raised, and they send me things. I got an email today with a link and quote that read:
The student dissertation the IPCC used in AR4 doesn’t even support their claims.  The student states in his dissertation: “In how far the changes observed  indicate a global change of climate can only be guessed and will show in the  future.”
Huh.
In our last story, referencing the work of the Telegraph, we touched on the what many consider inappropriate citations in the 2007 IPCC AR4 report. See here: IPCC Gate Du Jour: UN climate change panel based claims on student dissertation and magazine article One citation was an article in Climbing Magazine issue 208, while another was a student dissertation. Some said that there’s nothing wrong with citing a student dissertation. Perhaps, but hold that thought until after reading this story from “ClimateQuotes”: (Note – for those who can’t delineate what part is my writing and what part is from ClimateQuotes, that website’s portion is everything after this – you know who you are 🙂 )
The story of the Geography Major’s Dissertation

Dario-Andri Schwörer

A big story in climate science right now is the fact that the IPCC relied on a mountain magazine and a graduate student’s dissertation as their citations for a specific claim in their Fourth Assessment Report. However there are few details, so I decided to do some digging. I found out a bit about the dissertation.
I believe this is the dissertation. It is written by this man, Dario-Andri Schwörer, also here. He was a student at the Geographical Institute of the Universities of Berne and Zurich, which is where he wrote his dissertation in or before 1997. He is now an avid outdoors-men, and a self-described ‘well known expert on the impact of climate change in the Alps’. Right now he is engaged in the TOPtoTOP program to promote climate protection.
The dissertation itself is titled:
An Inquiry into Possible Effects of Climatic Change on the Mountain Guide Trade in the Bernina Region
Subtitled:
Geography Major Dissertation
by
SCHWÖRER DARIO-ANDRI
carried out at the Geographical Institute of the Universities of Berne and Zurich
The dissertation itself is not entirely about climate change. In fact, he mentions the number one reason that mountain guides give for decreased climbs is not climate change, but:
“They attribute this decrease in the first place to the recession and the high exchange rate of the Swiss franc in relation to the German mark. In the second place they mention changes of the natural environment.”
That wasn’t mentioned in the AR4. The ambiguity continues:
Read the rest at ClimateQuotes.
This is the link: The story of the Geography Major’s Dissertation
(for those that have trouble following links to referenced sources, click on the bold portion, you know who you are :-))


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e6d08ed',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"The speed and scale of China’s rapid economic growth has led to widespread degradation of its densely inhabited coastlines, according to an analysis of 60 years of social, economic and environmental data.  This study, published in the Nature group journal Scientific Reports, examined historical trends in 15 different ways humans have affected coastal ecosystems, including fishing, pollution, coastal habitat transformation, and disturbance from shipping. China’s 1978 economic reforms mark a major ramping up of China’s economic development, and since then the country has rapidly shifted from an agrarian society stretched across the country’s vast interior to an industrial economy concentrated in coastal regions.  The analysis found that in all 15 measures human impact had increased over the period, and especially so after 1978. For the ten measures with data that covered the period before and after the reforms, the increase rates of six were significantly higher, four about the same, and none lower following the reforms. Analysis of the environmental data shows that China’s coastal ecosystems have been degraded rapidly since the 1978 economic reforms. For example, across China’s seas, the diversity and body size of marine fish did not change before 1978, but both have steadily shrunk since. Harmful algal blooms have become more frequent, and generally the extent of coral reefs, mangroves and coastal wetlands have been decreasing. The study also examined the relationships between the economic measure of GDP per capita and affects of humans upon China’s coasts. According to the environmental Kuznets curve theory, in the early stages of economic development the impact of humans increases with per capita income, but decreases when per capita income reaches a turning point. We found evidence of these turning points for some of the types of human affects we measured, but not others.  Some appeared to have just been reached, with no meaningful decreases yet in evidence, for example excess fertiliser causing algal blooms. Others are still too distant. For example, if habitat transformation for fish farming is to reach its predicted turning point, this will need considerably more coastline to be made available for fish farming – more coastline in fact than China has. And other turning points show how the impacts of human activity is shifting from richer to poorer regions, meaning the affects on the environment are not decreasing country-wide so much as merely moving around. The study also shows that it is economic growth, rather than population growth, that is the major cause of the damage to China’s coasts. Perhaps because China is the world’s most populous country, it is often assumed – particularly in the west – that population pressure is the major problem. Similarly, previous studies of the impact on coastal and marine ecosystems often address the role of increased population density along coastlines.  In contrast, this study shows that the rate of population increase in China’s coastal provinces remained constant after 1978, and that including population change or population density in the analysis had little effect on the results. The study suggests that strict conservation measures are needed to protect China’s coastal ecosystems and to sustain future growth. China’s GDP per capita remains very low, and without strict conservation efforts, as economic growth pushes per capital income up and spurs further development China’s coastal ecosystems will suffer still further. "
"**Remote jury centres for sheriff court trials are to be created in Odeon cinema complexes in Ayr, East Kilbride, Dundee and Dunfermline.**
The facilities are in addition to two remote jury centres already identified in Edinburgh and Glasgow, where trials are set to get under way next week.
Arrangements for jury venues in Aberdeen and Inverness are currently being finalised for a February launch.
The pandemic has caused a major backlog of cases across the country.
David Fraser, chief operations officer for the Scottish Courts and Tribunals Service, said: ""The commencement of trials in Edinburgh and Glasgow next week sees the restart of sheriff court jury trials.
""There has been exceptional progress to secure remote jury centre venues required and we intend to move as quickly as possible to the pre-Covid number of sheriff court jury trials proceeding in Scotland.""
Under the plans, published on Tuesday, jurors will be based as follows:
The Odeon in Braehead, Renfrewshire, is currently operating as a remote jury centre for Glasgow High Court.
Last month the SCTS confirmed that six juries would be based remotely in the Odeon at Glasgow Quay for Glasgow Sheriff Court trials.
For trials running initially in Edinburgh Sheriff Court and then Livingston Sheriff Court, three juries will be based remotely in the Odeon complex on Lothian Road, Edinburgh."
"On Monday, the government did something remarkable. In the windiest country in Europe, it finally ended a five-year block on new onshore wind turbines. It’s a victory for campaigners, and anyone who wants action on the climate crisis and cares about lower energy bills in future.  The government has hopefully ended a strategy – begun by David Cameron in 2015 – of making energy policy in direct contradiction to public opinion. No two issues define this tendency more than the government’s seemingly unshakeable support for fracking – and the insistent de facto ban on new onshore wind turbines. Of course, it didn’t actually ban new onshore wind turbines in 2015. Making them illegal would have been absurd. Instead, post-election, it swiftly attacked from two angles. First, it imposed onerous planning barriers on new wind projects in England, which led planning applications to shrivel by 95% by 2018. Second, it prevented onshore wind projects from bidding for the kind of long-term clean energy contracts that are available to other power sources like nuclear and offshore wind farms that are needed to get them built. Perversely, this was only possible by also blocking solar power from the auctions – meaning the two cheapest sources of clean energy were suddenly out in the cold. In subsequent years, the government’s own in-depth attitude trackers showed public support for fracking plummeting as low as 15% by 2018, yet it backed the industry to the hilt – even proposing at one point to make fracking wells “permitted developments”, effectively reducing them to the planning status of a garden wall. Over the same period, 75% of the public supported onshore wind turbines – by this point cheaper than any source of power from fossil fuels. The government response was to look the other way, despite the UK having a world-class wind resource, and new turbines being the cheapest way to provide power for its citizens. That was until the announcement on Monday that the next round of clean energy auctions, to be held in 2021, would include both onshore wind and solar once more. These auctions award contracts that effectively set a floor price for power sold from new projects, which in effect acts as a subsidy over the lifetime of the project if the floor price is higher than the market price for power. Onshore wind is now so cheap its floor price is expected to be at, or even below, current market prices. To translate, that means new onshore wind projects should be free of government subsidy, another bell tolling on the future of fossil fuels. It shouldn’t have been this hard – and we’ve certainly lost valuable time – but climate campaigners should celebrate forcing the reversal of fortunes between fracking and wind. The former is now fighting for survival, the latter is back in from the cold – just as the public wanted. Add in bringing forward the petrol and diesel car ban to 2035, a pledge to spend £9bn on domestic energy efficiency in the Conservative election manifesto and the decision not to appeal the seismic Heathrow judgment, and you might be forgiven for wondering if we’re seeing something bigger at work within this new government. Of course, it’s too early to tell for sure. The fracking industry was brought to its knees by a combination of brave and indefatigable grassroots campaigning and Tory MPs vexed at what fracking meant for their constituencies. It became clear the government was backing a losing horse. With the economics and social licence of fossil fuels receding weekly, it seems unlikely to remount any time soon. An almost equally long-standing – though less high-profile – public campaign helped ensure the thorn of onshore wind never left the government’s side. But its success is perhaps indicative that from atop his 80-seat majority, Boris Johnson is indeed willing to upset backbenchers to take action on the climate crisis. Make no mistake: there will be parliamentary colleagues of his for whom this change in direction is the stuff of nightmares. A backlash from the usual suspects orbiting 55 Tufton Street – the London home of many groups associated with climate denial – can’t be far off. So, will it stick? This is a question with relevance not just to wind turbines, but to much of what lies ahead if we are to cut emissions rapidly. Whether it’s the intrusive work of replacing domestic gas boilers, the fraught territory of reducing private car use, or delicate nudges to shift diets, around each policy corner lies a potential battle. As ever, this risk is exacerbated when policies are seen to be imposed from above. Yet new wind turbines could be owned and led directly by communities, councils, local businesses or key services such as hospitals. There is no reason wind projects can’t become synonymous with bottom-up climate action that communities are proud of and benefit from. That would put them beyond the reach of reactionary scaremongering. Yet for this to happen, yesterdays’s announcement only takes us so far. In England, while the block on onshore wind competing for clean energy contracts has been lifted, onerous planning blocks on turbines still remain. This rules out a full wind renaissance south of the border. The government must have the courage to remove them, so we can start building our new energy system – this time from the ground up. • Max Wakefield is director of campaigns at the climate action organisation Possible"
"Labor’s federal president Wayne Swan will accuse Scott Morrison of engaging in “predatory centrism” on climate policy by styling himself as the pragmatist between the extremes of climate emergencies and denialism, when the government has no intention of driving meaningful emissions reduction. In a speech to be delivered on Sunday, Swan will argue Labor will only win the decade-long climate wars if it approaches the challenge with “pragmatic policy and ruthless organisation”.  According to speech notes, Swan will say Labor needs to articulate a roadmap for the domestic coal powered industry “which manages its decline”. “There must be a strong dialogue between government, industry, and the unions, and operate on the principle that no one gets left behind. We have to work closely with coal-dependent regions and put in a plan so that everyone gets a good future.” Swan will also warn his former parliamentary colleagues to apply “large doses of healthy scepticism about the hand-on-heart last minute conversion of the Business Council of Australia to the cause”. The BCA criticised Labor policy at the last federal election, but has recently backed a legislated target of net zero emissions by 2050, which aligns with the policy outlined on February 20 by Anthony Albanese. Swan will say Labor is a party that accepts the science of climate change, supports blue-collar jobs, and has sought a way forward to reduce emissions across the economy with “no help from the right or the extreme left”. “Proposals that talk about shutting down the export coal industry instead of focusing on the hard and tough policy which includes reducing emissions across the whole of our economy are entirely counter-productive,” the ALP president will say. “Over the next 20 years there will be a dramatic reduction in coal production delivered by the market, and we will see coal-fired power generation provide less and less of our electricity”. “But the notion put forward by some green groups that we could phase out coal-fired power by 2030 is impossible and a recipe for blackouts and the further erosion of public support for strong economy-wide action on emissions reduction.” Swan will argue Morrison does not have a serious climate policy, and will not develop one, “but as you would expect from a marketing guy, [he has] a clearly articulated PR strategy to use climate as a wedge aimed not just at coal miners but working people more generally and particularly the elderly”. He will say Morrison’s approach will be to argue he is “the reasonable guy in the middle who says the climate is changing” but do little or nothing about it. “We in the Labor party need to expose this marketing strategy for what it is. “And we need to set against it our hard work on what I simply call solving the bloody problem.” The Morrison government has blasted Labor for adopting a 2050 target without a roadmap to get there. On Friday the government signalled plans to shift investment from wind and solar to hydrogen, carbon capture and storage, lithium and advanced livestock feed supplements, as part of a “bottom up” strategy to reduce emissions by 2050. The government will shortly release a technology roadmap outlining its investment strategies, and further policy work is under way. As well as the roadmap, the government is reviewing its much-criticised emissions reduction fund and the operation of the safeguard mechanism, and is working on an electric vehicles strategy, despite blasting Labor during last year’s election, claiming measures to drive the takeup of EVs were a “war on the weekend”. Despite blasting Labor for adopting net zero, the government has not ruled out following suit and adopting a mid-century target. As a signatory to the Paris agreement, the Coalition has implicitly adopted a position of carbon neutrality by mid-century. Albanese spent Saturday in coal country in New South Wales. In a speech to the country Labor conference, Albanese said net zero meant opportunity for regional Australia, not catastrophe."
nan
"
From the U.S. Senate Committe on Environment and Public Works
Democrats Delay Global Warming Bill – Again


Obama Agenda In “Disarray”
Washington, D.C. – U.S. Senator James Inhofe (R-Okla.), Ranking Member of the Environment & Public Works Committee, today said that he was not surprised to learn that Senate Democrats were forced once again to delay introduction of their global warming cap-and-trade bill. Throughout hearing after hearing in the EPW Committee this summer, it became apparent that Democrats were a long way off from reaching the votes necessary in the Senate to pass the largest tax increase in American history.
“The news today-that Sen. Boxer and Sen. Kerry will delay introduction of their cap-and-trade bill-came as no surprise.  The delay is emblematic of the division and disarray in the Democratic Party over cap-and-trade and health care legislation-both of which are big government schemes for which the public has expressed overwhelming opposition.  With the climate change debate on Capitol Hill, it’s safe to report that bipartisanship is nowhere in evidence.  Cap-and-trade has pitted Democrat against Democrat, or, put another way, it centers on those in the party supporting the largest tax increase in American history against those in the party who oppose it.  As to just who will win this intra-party squabble, I put money down on those representing the vast majority of the American people, who are clear that cap-and-trade should be rationed out of existence.”
In the last hearing before the EPW Committee before the August recess,  Senator Inhofe spoke directly to the mounting concerns raised by Senate Democrats to cap-and-trade legislation:
Full opening statement provided below:
Climate Change and Ensuring that America Leads the Clean Energy Transformation
August 6, 2009
Madame Chairman, thank you for holding this hearing today. This is the last hearing on climate change before the August recess, so I think it’s appropriate to take stock of what we’ve learned.
Madame Chairman, since you assumed the gavel, this committee has held over thirty hearings on climate change. With testimony from numerous experts and officials from all over the country, these hearings explored various issues associated with cap-and-trade-and I’m sure my colleagues learned a great deal from them.
But over the last two years, it was not from these, at times, arcane and abstract policy discussions that we got to the essence of cap-and-trade. No, it was the Democrats who cut right to the chase; it was the Democrats over the last two years who exposed what cap-and-trade really means for the American public.
We learned, for example, from President Obama that under his cap-and-trade plan, “electricity prices would necessarily skyrocket.”
We learned from Rep. John Dingell (D-Mich.) that cap-and-trade is “a tax, and a great big one.”
We learned from Rep. Peter DeFazio (D-Ore.) that “a cap-and-trade system is prone to market manipulation and speculation without any guarantee of meaningful GHG emission reductions. A cap-and-trade has been operating in Europe for three years and is largely a failure.”
We learned from Sen. Dorgan (D-N.D.) that with cap-and-trade “the Wall Street crowd can’t wait to sink their teeth into a new trillion-dollar trading market in which hedge funds and investment banks would trade and speculate on carbon credits and securities. In no time they’ll create derivatives, swaps and more in that new market. In fact, most of the investment banks have already created carbon trading departments. They are ready to go. I’m not.”
We learned from Sen. Cantwell (D-Wash.) that “a cap-and-trade program might allow Wall Street to distort a carbon market for its own profits.”
We learned from EPA Administrator Lisa Jackson that unilateral U.S. action to address climate change through cap-and-trade would be futile. She said in response to a question from me that “U.S. action alone will not impact world CO2 levels.”
We learned from Sen. Kerry (D-Mass.) that “there is no way the United States of America acting alone can solve this problem. So we have to have China; we have to have India.”
We learned from Sen. McCaskill (D-Mo.) that if “we go too far with this,” that is, cap-and-trade, then “all we’re going to do is chase more jobs to China and India, where they’ve been putting up coal-fired plants every 10 minutes.”
In sum, after a slew of hearings and three unsuccessful votes on the Senate floor, the Democrats taught us that cap-and-trade is a great big tax that will raise electricity prices on consumers, enrich Wall Street traders, and send jobs to China and India-all without any impact on global temperature.
So off we go into the August recess, secure in the knowledge that cap-and-trade is riddled with flaws, and that Democrats are seriously divided over one of President Obama’s top domestic policy priorities.
And we also know that, according to recent polling, the American public is increasingly unwilling to pay anything to fight global warming.
But all of this does not mean cap-and-trade is dead and gone. It is very much alive, as Democratic leaders, as they did in the House, are eager to distribute pork on unprecedented scales to secure the necessary votes to pass cap-and-trade into law.
So be assured of this: We will markup legislation in this committee, pass it, and then it will be combined with other bills from other committees. And we will have a debate on the Senate floor.
Throughout the debate on cap-and-trade, we will be there to say that:
According to the American Farm Bureau, the vast majority of agriculture groups oppose it;
According to GAO, it will send our jobs to China and India;
According to the National Black Chamber of Commerce, it will destroy over 2 million jobs;
According to EPA and EIA, it will not reduce our dependence on foreign oil;
According to EPA, it will do nothing to reduce global temperature;
And when all is said and done, the American people will reject it and we will defeat it.
Thank you, Madame Chairman.
# # #


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93530e16',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A plan to resolve the UK’s housing crisis by adding garden city extensions to 40 towns and cities has won a prestigious economic prize. Urban designer David Rudlin was last night awarded the £250,000 Wolfson Prize for his proposal to build connected satellite settlements around existing large towns. The UK has long had an issue with housing, and the policies that have sought to solve this problem have shaped the urban form of the country. Industrialisation, suburbanisation, inner city estates with tower blocks, and new towns or major extensions have all responded to the need to meet urgent housing demand. Today, a renaissance of the Edwardian garden city idea seeks to challenge the piecemeal in-fill of urban centres and former industrial sites, or the slow creep of suburbia via urban extensions. The original idea was to build new, self-supporting settlements beyond green belts around existing towns and cities. This way people could escape the slums of the cities, or the “slum on wheels” of long-distance rail commuting.  However, while garden cities had a huge influence on large scale suburbanisation in the 20s and 30s the concept was relaunched as a major solution to the post-war housing crisis. It was a programme seen as equivalent in scale to the creation of the NHS. Around 30 new towns in total (from Stevenage, Harlow and Basildon, culminating in Milton Keynes) were built.  What the Wolfson Prize has done is prompt planning and urbanism professionals to look again at the methods and intentions of the garden city idea and apply it to the needs of today’s world to meet today’s housing crisis. As someone who has studied the history of this idea in depth, I believe this is a major moment. David Rudlin’s winning entry is notable in that it describes the principle of new settlements being built as satellites to any existing large town or small city. This revives the original concept of a cluster of garden city settlements linked together by transport. Each can be distinctive, but each can support the other in terms of sharing amenities, from leisure centres to employment hubs. Another advantage is that by providing the model for building from existing towns and cities, local authorities and local communities may be more empowered. Development will therefore take place where it has local support. While Britain’s post-war new towns have much in common with other places subject to major urban redevelopment or expansion in the mid to late 20th century (think Swindon, Basingstoke or Watford), or those re-engineered with ring roads, multi-storeys and shopping malls, the original garden city vision offered a more bucolic vision of genteel Edwardian serenity.  Each was a reflection of its era: the interwar ideal of an Englishman’s suburban castle, versus 1960s technological optimism. Perhaps it is a desire to return to the former that will emerge as such plans come forward, or perhaps it is the latter that will appeal more to the future Generation X and Yers who will become home owners when these places get built. There is an underlying idea that such places should enable greener ways of living (decent sized gardens, cycle paths, public transport) and indeed these same ideas were present in the post-war new towns. However, it is significant that these towns not only sought to build new housing for workers from the bombed out inner cities, but also to provide space for new industries, especially in the white heat sectors of aerospace, electronics, plastics and, to take advantage of the brand new motorway network, logistics.  The fresh start came with a fundamental attempt to redesign how the city, or rather town, would work, and drive an industrial strategy for the country. It was to be clean, healthy, filled with nature, with jobs for residents and innovative, modern facilities.  While the question of where to build sees a distinct logic of linked new settlements, reflected in the Rudlin proposal, the real issue is the underlying economic drivers for a place. While the Rudlin proposal supports localism, economic development and connectivity will still be the main factors behind new projects.  As Rudlin’s report puts it, quality depends on economics as much as design. This was a clear factor behind the relative success of the new towns, with Milton Keynes benefiting from its location midway between London and Birmingham, Warrington at the heart of the motorway network between Manchester and Liverpool, and Crawley next to Gatwick.   If HS2 goes ahead, a new stop en route may justify an urban development. Various key sites like old airforce bases or Didcot power station in Oxfordshire may receive local support under a holistic strategy led by local authorities. Tagging “Garden City” on the name of these areas of employment growth will prompt the view that these places should be built to high quality design standards. The deadline for local authorities to put in bids for support under the Government’s garden city programme has just passed. We will soon discover where such places may take off and make the crucial jump from ideal and vision onto the slow path to delivery.  In the meantime, let’s congratulate all those involved in this work for helping push forward a new era in the country’s urban development. The garden city is a good idea, now receiving cross-party support, which helps bring new impetus to the debate about where we should build the much needed homes for the future."
"
Share this...FacebookTwitterHerman Cain on the Mark Levin Show.
And at the 3:45 mark: “We know that those scientists who tried to concoct the science to say that we had a hockey stick global warning and they were busted because they manipulated the data.”

Share this...FacebookTwitter "
"

 _The Current Wisdom_ is a series of monthly articles in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.



 _The Current Wisdom_ only comments on science appearing in the refereed, peer‐​reviewed literature, or that has been peer‐​screened prior to presentation at a scientific congress.



Previous editions of _Wisdom_ , which began in 2010, are available at our blog Cato@Liberty (www​.cato​-at​-lib​er​ty​.org/).



A major pillar supporting the catastrophic vision of climate change is a large and rapid rise in the level of the global oceans. There’s an awful lot of infrastructure and property by the shore – and people who live there. Total insured property values along the Atlantic coast of the U.S. (which includes the Gulf of Mexico) are roughly equal to the nation’s annual GDP.



So should you sell your beach house because of the impending doom? I say yes. You need to beat the rush, put it on the market at a bargain‐​basement price, and sell it to me. And then I will keep it until the cows come home.



In its Fourth Assessment Report (AR4) published in 2007, the United Nations’ Intergovernmental Panel on Climate Change (IPCC) projected that sea levels would rise somewhere between 7 and 23 inches with a central value of about 15 inches by century’s end. Alarmists were quick to argue that these numbers were far too low because the IPCC did not include dynamic changes that may occur to the vast ice sheets located in Greenland and Antarctica. Since the publication of the AR4, a host of papers (e.g., Vermeer and Rahmstorf, 2009; Grinsted et al., 2009) have been published that suggest that a sea level rise of 3 to 6 feet by the year 2100 is a much more likely expectation than is 15 inches.



But, as we have noted previously in this _Wisdom_ , many of the proposed mechanisms for such a rapid rise — which is caused by a sudden and massive loss of ice from atop Greenland and/​or Antarctica — don’t seem to operate in such a way as to produce a rapid and sustained ice release.



But the rapid sea level rise beat goes on. In global warming science, we note, the number of scientific papers with the conclusion “it’s worse than we thought” vastly outnumbers those saying “new research indicates things aren’t so dire as previous projections.” In a world of unbiased models and data, they should roughly be in balance. 



This just in: it’s worse than we thought! In research published last month, Eric Rignot and colleagues have combined observations and models to derive a history of the surface mass balance (SMB) of ice (gains from snowfall minus losses from surface melting and glacial discharge) over Greenland and Antarctica. What they find is that over the past two decades, the SMB of ice in both locations has gone from positive (i.e., net gains) to negative (i.e., net loss). In other words, the amount of ice was increasing annually on both landmasses back in the early 1990s, but now ice is currently being lost. And if that were not bad enough, the negative trend in SMB means that the rate of ice loss is increasing (i.e., ice loss is accelerating). The acceleration is about 50% greater in Greenland than it is in Antarctica.



Extrapolating the acceleration forward, Rignot et al. find that the combined ice loss from Greenland and Antarctica becomes the largest contributor to sea level rise by the mid‐​to‐​late 21st century, exceeding that from thermal expansion and ice loss from mountain glaciers in other parts of the world. This is a significant finding because in the AR4 the IPCC actually projects that, in combination, Greenland and Antarctica gain ice over the 21st century and thus act to retard the rise in sea level from other contributors. The implication from Rignot et al. is that the IPCC AR4 sea level rise projections for the end of the century (which average about 15 inches) are too low — by about 2 feet. Thus, Rignot et al. lends support for the recent projection of sea level rise over the 21st century of 3 feet or more (e.g., Vermeer and Rahmstorf, 2009; Grinsted et al., 2009).



So please stop here. Read no more, and, in a panic, sell me the house.



The SMB history for Antarctica and Greenland history is very noisy. The magnitude of recent trends (which are derived from less than 20 years worth of data) may not be terribly representative of the value of the long‐​term trend. Nor is the glacial behavior in Greenland and Antarctica well‐​understood; recent papers have suggested that a larger speed‐​up in the rates of glacial flow is likely not sustainable (see previous _Current Wisdom_ articles for more details).



Now there is more.



We (my research team) just published a paper in the _Journal of Geophysical Research_ in which we estimate the history of the extent of surface ice melt (a major component of SMB) in Greenland for the last two and a quarter centuries. Our goal was to provide some longer context it which to place the relatively short period of direct ice melt observations (such as those included in Rignot’s analysis). What we found is that, while the current rate of surface ice melt is high (and increasing), there have been times in the past (primarily from the late 1920s through the early 1960s) during which the ice melt across Greenland was just as high (Figure 1). _And_ , _this is important_ , the period of the lowest ice melt extent across Greenland for more than a century occurred from the early 1970s through the late 1980s – or very near the beginning the time period analyzed by Rignot et al.1







We describe this in our paper:



It is worth noting that the satellite observations of Greenland’s total ice melt, which begin in the late 1970s, start during a time that is characterized by the lowest sustained extent of melt during the past century. Thus, the positive melt extent trend [during the past 2–3 decades] includes nearly equal contributions from the relatively high melt extents in recent years but also from the relatively low ice melt extent in the early years of the available satellite record. The large values of ice [melt] extent observed in recent years are much less unusual when compared against conditions typical of the early to mid 20th century, than when compared against conditions at the beginning the of the satellite record.



In other words, the recent increase in melt across Greenland (contributing to a negative trend in SMB) may in part a result of rising temperatures from sources other than dreaded greenhouse warming, and therefore extrapolating the observed trends in SMB forward may not be such a great idea.



We put the impact of recent melt in historical perspective:



However, there is no indication that the increased contribution from the Greenland melt in the early to mid 20th century, a roughly 40 year interval when average annual melt was more or less equivalent to the average of the most recent 10 years (2000–2009), resulted in a rate of total global sea level rise that exceeded ~3 mm/​yr. **This suggests that Greenland’s contribution to global sea level rise, even during multidecadal conditions as warm as during the past several years, is relatively modest.** [emphasis added]



Or, to put it another way, the IPCC forecasts of sea level rise don’t appear to be in jeopardy from ice loss from Greenland — a conclusion in contrast to that of Rignot et al.



Further evidence of this can be found in a just‐​published paper in the _Journal of Coastal Research_ by James Houston (Director Emeritus of the Engineer Research and Development Center of the Army Corps of Engineers) and Robert Dean (Professor Emeritus in the Department of Civil and Coastal Engineering at the University of Florida)2. Houston and Dean analyzed long‐​term observations (more than 60 years in length) from tide gauges installed along the U.S. coast looking for signs of an accelerated rise. They couldn’t find any — in fact, they found a _deceleration_. Expanding their analysis to included tide gauges from around the world produced the same thing — a recent _slowing of the rate of sea level rise_.



According to Houston and Dean:



Our analyses do not indicate acceleration in sea level in U.S. tide gauge records during the 20th century. Instead, for each time period we consider, the records show small decelerations that are consistent with a number of earlier studies of worldwide‐​gauge records. The decelerations that we obtain are opposite in sign and one to two orders of magnitude less than the +0.07 to +0.28 mm/​y2 accelerations that are required to reach sea levels predicted for 2100 by Vermeer and Rahmsdorf (2009), Jevrejeva, Moore, and Grinsted (2010), and Grinsted, Moore, and Jevrejeva (2010). Bindoff et al. (2007) note an increase in worldwide temperature from 1906 to 2005 of 0.74Ã�Â°C. It is essential that investigations continue to address why this worldwide‐​temperature increase has not produced acceleration of global sea level over the past 100 years, and indeed why global sea level has possibly decelerated for at least the last 80 years.



 _Please_ disregard these findings, get with the “consensus,” and sell me your beach house. Now.





**Notes:**



 **1.** As you might guess, this problem afflicts other satellite‐​derived measurements of polar ice, too. The sensors were launched in 1978, which just happens to be at the end of the coldest period of summers in the Arctic since the early 1920s. As a result, the satellite began to measure Arctic ice when it was unusually expanded. It is worth noting at the time that one of the justifications for the platforms was the spectre of advancing ice from global cooling.  
  
 **2.** It has been noted others that many scientists wait until they are retired before attempting to publish papers that go against the disastrous vision of global warming. This is not a sign of a healthy science.



 **References:**



Frauenfeld, O.W., P.C. Knappenberger, and P.J. Michaels, 2011. A reconstruction of annual Greenland ice melt extent, 1784–2009. _Journal of Geophysical Research_ , doi: 10.1029/2010JD014918, in press.  
Grinsted, A., J.C. Moore, and S. Jevrejeva. 2009. Reconstructing sea level from paleo and projected temperatures 200 to 2100AD. _Climate Dynamics_ , doi: 10.1007/s00382-008‑0507-2.  
Houston, J.R., and R.G. Dean, 2011. Sea‐​level acceleration based on U.S. tide gauges and extension of previous global‐​gauge analyses. _Journal of Coastal Research_ , in press.  
Rignot, E., I. Velicogna, M.R. van den Broeke, A. Monaghan, and J.T.M. Lenaerts, 2011. Acceleration of the contribution of the Greenland and Antarctic ice sheets to sea level rise. _Geophysical Research Letters_ , **38** , L05503, doi: 10.1029/2011GL046583.  
Vermeer, M., and S. Rahmstorf, 2009. Global sea level linked to global temperature. _Proceedings of the National Academy of Sciences_ , **106** , 51, doi: 10.1073/pnas.0907765106, 21527–21532.
"
"
Share this...FacebookTwitterNews from Belgium…Yet another professor, Dr. Ir. Henri A. Masson, has resigned from yet another once prestigious organisation, which too has succumbed to the darkness of climate dogmatism and censorship.
In late August the Société Européenne des Ingénieurs et Industriels (European Society of Engineers and Industrialists – abbreviated  SEII) had organised a conference where scientists S. Fred Singer and Prof. Claes Johnson, of the Royal Institute of Technology in Stockholm, had been scheduled to speak on climate change.
I wrote about this here.
This all came to the attention of IPCC Vice Chair Jean-Pascal van Ypersele, who found that skeptic views have no place in the climate religion, and so moved quickly and demanded the SEII disinvite the 2 distinguished speakers. The conference had to be moved.
For SEII event coordinator Dr. Henri A. Masson, this closed-minded attitude by the SEII and van Ypersele became intolerable and so he has submitted his strongly-worded resignation, written in French below, and sent to me by e-mail  (Sorry, no translation in English. But use Google to get the gist of it).
==========================================================
A l’attention de: M. Philippe WAUTERS, Président
Objet: démission de tous mes mandats exercés au sein de la SEII et annulation de ma qualité de membre de la SEII.
Monsieur le Président,
Je viens de prendre connaissance du document officiel établi par le Secrétaire Général de la SEII relatif à l’affaire « Climategate », par lequel il informe les Administrateurs que le Bureau Exécutif, à une très large majorité, vous a réitéré sa confiance, malgré les évidences factuelles, que j’ai fournies antérieurement, qui établissent la réalité des mensonges que vous leur avez faits.
Il appert que, après l’avoir nié par écrit, vous avez bien dû reconnaître que vous avez agi suite à une intervention d’une « tierce personne », comme le qualifie pudiquement le Secrétaire Général dans sa lettre aux Administrateurs, cette intervention d’une tierce personne étant en fait une lettre de protestation du Professeur VAN YPERSELE. Pour moi, en clair, il ne s’agit de rien d’autres que d’avoir participé à un trafic d’influence, basé sur des déclarations diffamantes que vous n’avez même paspris la peine de vérifier, et vous avez bien cédé à des pressions externes à la SEII visant à censurer des intervenants défendant un point de vue opposé à celui de M. VAN YPERSELE et des instances qu’il représente.
Ces faits sont incontestables, quels que soient les arguments casuistiques que vous tentez de développer pour évoquer une faute de procédure grave que j’aurais commise. En l’absence de définition des limites extrêmement floues du mandat qui m’a été confié dans le cadre des activités de formation de la SEII, et plus précisément dans celles visant à animer un « café philosophique pilote consacré à la controverse climatique », je ne vois vraiment pas quelle est la procédure je n’aurais pas suivie, dans la simple exécution d ‘une activité récurrente de ce groupe de travail que j’anime depuis plus d’un an.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Lorsqu’il s’agit d’envoyer un agenda ou un procès-verbal de réunion d’un tel groupe de travail de la SEII, ou encore de tenter d’inviter de nouveaux membres à se joindre à lui, car c’est bien et uniquement de cela qu’il s’agit en l’occurrence, il me semble qu’il est de règle d’employer un papier à en-tête SEII à cet effet, et cela sans avoir à faire intervenir le Bureau à chaque fois; D’ailleurs, sans l’intervention de M. VAN YPERSELE, vous n’auriez plus que vraisemblablement rien trouvé à y redire.
Mais, évidemment, rien n’oblige le Bureau à rester cohérent et objectif dans ses jugements.
Les faits que je vous reproche sont de nature stratégique pour la SEII. Essayer de s’en disculper en utilisant des arguments de procédure spécieux ne vous grandit pas. Il aurait été beaucoup plus judicieux de reconnaître que vous avez été grugé par M. VAN YPERSELE, sur base de la réputation dontil jouit encore en Belgique, malgré ses liens avec la branche la plus radicale de Greenpeace. Preuves à l’appui, Je vous ai fourni l’occasion, pendant une semaine de revoir votre position; vous n’avez pas voulu la saisir.
Je ne peux donc que constater que ni vous, ni le Bureau Exécutif ne partagez un certain nombre de valeurs qui me sont chères et sur lesquelles je n’ai jamais transigé et ne transigerai pas à l’avenir.
En conséquence de quoi, je vous présente ma démission de toutes les fonctions que j’occupais au sein de la SEII. Je souhaite également ne plus figurer sur la liste des membres et ne plus recevoir vos mailings.
Je me réserve, par ailleurs, la liberté de plaider ma bonne foi, preuves à l’appui, dans l’affaire qui nous oppose, auprès des personnes et instances de mon choix.
Je vous prie d’agréer mes sentiments de circonstance.
Prof. Dr. Ir. Henri A. Masson
Cc: Administrateurs de la SEII
===============================================
Expect to see more resignations in the future as once respected societies keep taking up the practices of the Dark Ages.
PS: I don’t know what hurts the warmists the most: their blatant censorship of debate, or them showing up to debate? If it’s truly them showing up to debate, then their science has got to be embarrassingly bad.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGerman online daily DIE WELT reports here on how Nestlé CEO Peter Brabeck-Letmathe says that the farming of biofuels dramatically exacerbates global hunger. Nestlé is the world’s largest food conglomerate. Hat-tip: oekowatch.org.
Recently a truly lame-brain study (later debunked here) was published claiming that global warming caused war. It would be advisable for the incompetent authors of that study to look at the connection between food prices and war instead.
Die Welt quotes Brabeck-Letmathe:
‘Through biofuels, we have returned hundreds of millions of people back into extreme poverty,’ he said in an interview with the Frankfurter Rundschau with regards to the hunger catastrophe in Somalia and rising food prices.
It is truly amazing that today more than one half of American corn and one fifth of the entire sugar cane harvest gets converted into biofuels while there is not enough food to feed humanity’.”
Brabeck-Letmathe says that the food shortage and spiralling prices are directly caused by the massive biofuel consumption and that the price spiral is leading to civil unrest in poor countries.
Biofuels were once enthusiastically supported by environmental groups, like Greenpeace,and were seen as a way of reducing human CO2 emissions, and thus curbing global warming. Governments worldwide are massively subsidizing the agriculture of biofuels. Farmers now prefer to grow crops for fuels rather than food for feeding the planet because it is simply more profitable.
Now hundreds of millions more people are starving needlessly.
Photo credit: Wikipedia
Share this...FacebookTwitter "
"

 _Every_ time there is some sort of weather disaster somewhere, someone blames it on human-caused global warming. Maybe not directly, but the implication is clear. “While we can’t link individual events to global warming, the increase of this type of event is consistent with our expectations, blah, blah…”   
  
Most recently this came in testimony from White House Science Adviser John Holdren before the Committee on Science, Space, and Technology of the U.S. House of Representatives:   




In general, one cannot say with confidence that an individual extreme weather event (or weather-related event)—for example, a heat wave, drought, flood, powerful storm, or large wildfire—was caused by global climate change. Such events usually result from the convergence of multiple factors, and these kinds of events occurred with some frequency before the onset of the discernible, largely human-caused changes in global climate in the late 20th and early 21st centuries. But there is much evidence demonstrating that extreme weather events of many kinds are beginning to be influenced—in magnitude or frequency—by changes in climate.



Holdren then goes to list a bunch of types of extreme weather whose characteristics have changed (remarkably, all becoming worse), adding that:   




There are good scientific explanations, moreover, supported by measurements, of the mechanisms by which the overall changes in climate resulting from the human-caused build-up of heat-trapping substances are leading to the observed changes in weather-related extremes.



Holdren’s implication is pretty clear—human-caused global warming is leading to changes in extreme weather. And just for good measure, he added this zinger:   




[I]t is reasonable to say that most weather in most places is being influenced in modest to significant ways by the changes in climate that have occurred as a result of human activities.



If this were the case, then there is a lot of good news to be found here, for by and large the weather is pretty good, with rare examples to the contrary.   
  
Take, for instance, what has been all abuzz this week in Washington, D.C.: how great the weather has been. The _Washington Post_ ’s Capital Weather Gang, which keeps close tabs on the pulse of D.C. weather, has commented repeatedly on how remarkable and enjoyable it has been. According to Holdren's logic, we have global warming to thank, and yet I have not seen one news story that links the pleasant weather to human-caused climate change.   
  
Across the country in Tucson, Ariz. (where I reside), the news this week has been dominated by the threat of the passage of the remnants of Hurricane Odile, which were forecast to move into the region from out of the Gulf of California. The predictions were for record-breaking rainfall amounts with the potential for widespread damage from flooding. The outlook stirred up memories of the passage of Tropical Storm Octave in 1983, which resulted in over $500 million (in 1983 dollars) of damage to the region. Thankfully, this did not come to pass. Instead, the heavy rains associated with Odile passed well east of the city, over much more sparsely populated country. Since apparently all weather is influenced by anthropogenic global warming, we have it to thank for averting what could have been a very costly and hugely disruptive situation affecting upwards of a million people.   
  
And speaking of hurricanes, the first major hurricane (category 3 or greater) in almost two years formed in the Atlantic Ocean. But, in encountering conditions arguably consistent with human-caused climate change, Hurricane Edouard quickly weakened and remained far out in the open Atlantic, steering well clear of the U.S. mainland. Major disaster averted. It has now been nearly nine _years_ since the last major hurricane made landfall in the United States, the longest such occurrence going back at least to the year 1900. Thanks, global warming!   
  
I could go on, because there are a lot more cases of non-extreme weather than there are of extreme weather, and as many or more cases to be made for weather catastrophes _averted_ by conditions “consistent with global warming” than caused by it.   
  
So if you want to play the all-weather-is-influenced-by-global-warming game, you are going to lose.   
  
Best bet would be to stick with the science, which for most types of extreme weather events and for most places indicates that a definitive link between event characteristics and human-caused climate change has not been established. Either talk about that situation or leave the attribution issue alone.


"
nan
"
New predictions for sea level rise
Sea level graph from the University of Colorado is shown below:

University of Bristol Press release issued 26 July 2009
 Fossil coral data and temperature records derived from ice-core measurements have been used to place better constraints on future sea level rise, and to test sea level projections.
The results are published today in Nature Geoscience and predict that the amount of sea level rise by the end of this century will be between 7- 82 cm (0.22 to 2.69 feet)

– depending on the amount of warming that occurs – a figure similar to that projected by the IPCC report of 2007.
Placing limits on the amount of sea level rise over the next century is one of the most pressing challenges for climate scientists. The uncertainties around different methods to achieve accurate predictions are highly contentious because the response of the Greenland and Antarctic ice sheets to warming is not well understood.
Dr Mark Siddall from the Earth Sciences Department at the University of Bristol, together with colleagues from Switzerland and the US, used fossil coral data and temperature records derived from ice-core measurements to reconstruct sea level fluctuations in response to changing climate for the past 22,000 years, a period that covers the transition from glacial maximum to the warm Holocene interglacial period.
By considering how sea level has responded to temperature since the end of the last glacial period, Siddall and colleagues predict that the amount of sea level rise by the end of this century will be similar to that projected by the Fourth Assessment Report of the Intergovernmental Panel on Climate Change (IPCC).
Dr Siddall said: “Given that the two approaches are entirely independent of each other, this result strengthens the confidence with which one may interpret the IPCC results. It is of vital importance that this semi-empirical result, based on a wealth of data from fossil corals, converges so closely with the IPCC estimates.
“Furthermore, as the time constant of the sea level response is 2,900 years, our model indicates that the impact of twentieth-century warming on sea level will continue for many centuries into the future. It will therefore constitute an important component of climate change in the future.”
The IPCC used sophisticated climate models to carry out their analysis, whereas Siddall and colleagues used a simple, conceptual model which is trained to match the sea level changes that have occurred since the end of the last ice age.
The new model explains much of the variability observed over the past 22,000 years and, in response to the minimum (1.1 oC) and maximum (6.4 oC) warming projected for AD 2100 by the IPCC model, this new model predicts, respectively, 7 and 82 cm of sea-level rise by the end of this century. The IPCC model predicted a slightly narrower range of sea level rise – between 18 and 76 cm.
The researchers emphasise that because we will be at least 200 years into a perturbed climate state by the end of this century, the lessons of long-term change in the past may be key to understanding future change.
Please contact                        Cherry Lewis for further information.

Further information:
The paper: Constraints on future sea-level rise from past sea-level reconstructions. Mark Siddall, Thomas F. Stocker and Peter U. Clark. Nature Geoscience .



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94186d9a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"JP Morgan Chase is to end fossil fuel loans for Arctic oil drilling and phase out loans for coal mining under new climate initiatives. The world’s largest financier of fossil fuels set out its plans at an investor event on Tuesday, days after the bank’s economists warned that the climate crisis threatened the survival of humanity. JP Morgan will aim to offer $200bn (£153bn) in environmental and economic development deals to help support clean energy and other sustainable projects instead.  JP Morgan’s green pledges put the bank on a par with Goldman Sachs which became the first large US bank to rule out future financing of oil drilling or exploration in the Arctic and in new mines for thermal coal. But environmental groups said the bank’s green pledges were dwarfed by its huge financial support for fossil fuels. JP Morgan has provided $75bn in financial support to expand shale fracking, and Arctic oil and gas exploration since the Paris climate agreement. Eli Kasargod-Staub, of ethical shareholder group Majority Action, said: “These steps pale in comparison to JP Morgan Chase’s responsibility to confront the climate crisis and the systemic risks it poses to investors and global financial stability.” The group called on JP Morgan to disclose its climate impact and realign its lending with the global goal of limiting heating to 1.5C.  JP Morgan’s climate strategy is expected to fall short of the green pledges announced by BlackRock, the world’s largest hedge fund, which will cut companies that rely on thermal coal for more than a quarter of their revenues from its actively managed portfolios. JP Morgan’s coal finance restrictions will apply to companies whose primary business is coal mining, but could allow a loophole to continue financing conglomerates that earn less than half of their revenue from coal. Jeanne Martin, a campaigner at the investment charity ShareAction, said JP Morgan’s climate pledges were an “anticlimax” but proved that “even the world’s largest fossil fuel financier has no choice but to listen to its shareholders and civil society on climate change”. The leaked document from JP Morgan’s economists, dated 14 January, warns that the bank “cannot rule out catastrophic outcomes where human life as we know it is threatened” by rising global temperatures. “Although precise predictions are not possible, it is clear that the Earth is on an unsustainable trajectory. Something will have to change at some point if the human race is going to survive,” the report says. ShareAction, which promotes responsible investing, said JP Morgan’s new policy “is at best an anticlimax and at worst dangerously omissive of a huge part of the coal market”. Martin said: “If the world was waiting for JP Morgan to move meaningfully on its funding of the climate crisis after warning that human life ‘as we know it’ could be threatened by climate change, it will be sorely disappointed.”"
" Kabay Tamu slows his dusty white ute to walking speed on the dirt road that runs along the south-western shoreline of Warraber, a tiny coral cay in the Torres Strait that is home to about 250 people. “This was the best spot for a day out,” 28-year-old Tamu says, recalling his childhood.   Most of the beach where Tamu used to play is gone, along with several enormous wongai trees that were a barrier of sorts, protecting the dirt road and the nearby dam, which supplies the island’s drinking water, from the sea. Warraber is just 1.4km long, and half as wide, but shrinking fast. Some data suggests that sea levels in the Torres Strait could be rising at twice the global rate. Now, Islanders dump their green waste to hold back the rising sea. The Intergovernmental Panel on Climate Change (IPCC) estimates that by 2100 tides will rise 30–60cm with immediate cuts to carbon emissions, and 61-110cm without.  A 2010 report by researchers from James Cook University found that “continued erosion may necessitate further action in the medium-term” to protect Warraber’s dam. If saltwater breaches it, life would be difficult to maintain. “Being removed, forcibly removed, from here, just because of rising sea levels and the effect of climate change, and becoming climate change refugees, it’s just something that really haunts my mind,” Tamu says. He pulls his ute up on the north-eastern side of the island, where the cemetery and church are protected by a haphazard, failing defence made up of rocks, coral, split sandbags and tyres. Warraber man Danny Billy is worried that the waves will soon inundate his parents’ graves. He says he cannot bear the thought of leaving his ancestors behind. “It’s our right to be here, to have a voice, to be recognised, and to make others realise this is our home … We have the right to live a healthy life, here, on our island.” The Billy family also live on nearby Poruma island, where Danny spent part of his childhood. Just a 15-minute flight away, Poruma is smaller and thinner. On the western shore, a road and buildings are threatened, and 250 coconut trees – a source of food, shelter and leaves used in traditional ceremonies – have already been washed away. Local man Phillemon Mosby feels that loss keenly. The picturesque plantation should be a place to share with children and grandchildren, who would ordinarily take over the nurturing of the site. “That experience was taken away because of climate change, because of the rising sea levels. We’ve seen areas where we used to go fishing that are no longer there. We’ve seen rocks where people used to go diving that are covered.” Community elder Uncle Frank Fauid says being denied the opportunity to walk, garden and fish in peace on homelands would be devastating. “We can’t leave our community and go live down south. The living system … is totally not the way we live up here.” Tamu, Billy and Uncle Frank’s cousin, Nazareth Fauid, are among the eight Torres Strait Islanders who lodged a complaint with the United Nations human rights committee last May against the Australian government, alleging that its failure to reduce emissions or pursue proper adaptation measures across the region impedes their human rights to culture and life. Sophie Marjanac, a lawyer with environmental non-profit ClientEarth, is representing the group, who want the government to meet its targets under the Paris agreement, to reach net zero emissions by 2050 and to phase out thermal coal. In December, the federal government matched an earlier commitment of $20 million from the Queensland governmentto build new seawalls. But there is widespread scepticism among Islanders about when the new walls will be constructed. In early 2018, emergency funding of $650,000 was granted to Poruma to protect its western shore, but the wall was built using geotextile sandbags with a 50-year life expectancy, rather than the rock or brick asked for by the community. More work is required to protect the shoreline. Where the coast remains exposed, coconut trees lie on the beach, their roots slowly ripping away from the island. Other islands including Boigu, Masig and Iama need new seawalls. It is unclear which islands will be prioritised, and if the new funding will cover them all. Tamu is quick to point out that “sea walls are only to buy us time” – the best fix is emissions reduction. “The thing that got me was [the federal government] didn’t announce [the new funding] as seawalls to combat climate change. They said it was ‘an infrastructure development in the community’. They’re still trying to cover up climate change and the rising sea levels here.” Tamu gained international headlines when he asked prime minister Scott Morrison during the UN climate summit in New York last September to visit Warraber. He maintains that the damage visible on Warraber and other islands would shock them into action on climate and coal. “They would actually see for themselves how close the shoreline is to infrastructure and housing up here. And also, so a lot of the leaders can stop denying the connection between climate change and what’s happening to our planet.” The invitation was rejected via email in November, and Tamu says that the government is still “hearing, but not listening” when it comes to nationwide pleas for climate action. “We really want them to lead the way. If we change and go to more renewables and hit all the targets, it won’t change the world, it won’t change everything. But it will [mean] we’re leading the way and showing all the other countries that there’s another way to be.” While the UN complaint won’t be settled until 2021, Danny Billy says Islanders won’t stop making noise until Australia finally offers global leadership on climate change “We won’t stop until justice is served.” Travel and research was supported by funding from the Melbourne Press Club’s Michael Gordon Fellowship program. This story is co-published with The Citizen, a publication of the Centre for Advancing Journalism."
"More investment in flood defences and improved planning for future disasters are urgently needed, scientists have warned. They predict that the number of extreme wet days – which have already increased this century – will continue to rise in the coming decades and will bring even greater devastation than that experienced this month after Storm Ciara and Storm Dennis swept across the country.  Ciara brought rain and wind gusts of up to 97mph, triggering more than 190 flood alerts. More than 500 properties were flooded and about 25,000 homes left without power. A week later Storm Dennis followed, which in some areas caused more than a month’s rain to fall within 24 hours. Thousands of people had to be moved from their flooded homes, rivers – including the Wye in Hereford – rose to record levels, while the Environment Agency issued a record number of flood warnings and alerts, including more than 600 last weekend. Four people were killed during Storm Dennis. “We are simply not prepared for the flooding coming our way in future,” said Prof Hannah Cloke, of Reading University. “We need to carry out a complete overhaul of our defences and be prepared to spend a lot more on them over a longer period of time.” So far the government has committed to spending £4bn over the next five years on improving flood defences. But both the amount and timescale were criticised for being insufficient last week. “Extremely wet days during UK winters are currently up by around 15% compared with previous decades,” said Dann Mitchell of Bristol University’s Cabot Institute for the Environment. “Wetter future winters is a consistent projection with some predicting a 30% to 35% increase in rain by 2070. Our government and town planners need to invest significantly in UK flood defences.” Last week George Eustice, the environment secretary, said he wanted to see more nature-based solutions, such as the construction of dams made of natural materials and the planting of trees in upper catchment areas. These would hold on to water and prevent it from pouring too quickly into rivers and estuaries. But this approach was dismissed as inadequate by Roger Falconer, professor of water management at Cardiff University. “It is like putting a small sticking plaster on a major open wound to control profuse bleeding. It would certainly be insufficient when dealing with the 30% increase in winter rainfall which the Met Office has predicted for some areas.” Instead, Falconer called for the construction of a large number of flow-through or perforated dams above towns at high risk of flooding. “Such a dam fills during flooding in the upper parts of the river basin and is then emptied, under controlled conditions, after the flood,” Falconer said. “We need many more of these.” Engineers and hydrologists also pointed out to modifications made to river channels, flood plains, land cover and drainage, and these often have serious impacts on water flow and bedevil attempts to predict how rivers will react to downpours. For example, one recent study of the River Afan in Wales found 259 barriers had been erected along its course over the past 200 years, although only 33 had been officially recorded. Work on pinpointing barriers like these urgently needs to be carried out, scientists have said. “Blame [for flooding] under these circumstances is misguided and unhelpful, and politicians should be very careful to ensure they understand the facts of flooding before seeking to champion any particular action,” said Prof David Sear, of Southampton University. Bringing a halt to the construction of houses on flood plains has also emerged as a key issue. One in 10 new homes built in England since 2013 has been built on ground at high risk of flooding, official figures show. Prof Robert Wilby, of the University of Loughborough, told the Guardian that the government should review its housebuilding targets in view of the increased risks from floods. This was backed by Mohammad Heidarzadeh, head of coastal engineering at Brunel University. “The UK’s flood defence systems were developed decades ago and are not fit to address the current climate situation,” he said. “While the interval for major floods was 15 to 20 years in the past century, it has shortened to two to five years in the past decade. “The country needs further investment in its flood systems, but such investment should be within a holistic and integrated framework.”"
"
Share this...FacebookTwitterMichael Brzoska (Photo credit: NATO)
Folks, here’s a polite something to illustrate what climate science in Germany has decomposed to.
Here’s an interview with a seemingly true believer Michael Brzoska, scientific director of the Institute for Peace Research and Security Policy at the University of Hamburg and a principal investigator at the Integrated Climate System Analysis and Prediction’s (CliSAP) research group “Climate Change and Security”.
The CliSAP is a cluster funded with 32 million euros over five years and was started in October 2007.
As you read the interview you’ll quickly get the impression that Brzoska is convinced that since about 1900 all the world’s conflicts have increasingly been due to man-made climate change and will increasingly be so in the future. The way to stop war is to cut CO2 emissions.
Obviously he lacks historical literacy. History shows that cold periods led to food shortages, and thus uncontrollable social strife. But during warm periods, societies prospered. I’ll submit to Brzoska that war and conflict result much more from political and diplomatic failure by leaders, and much less from imagined man-made weather.
So indeed – isn’t climate change wonderful? Thanks to people like Brzoska, world leaders today have carte blanche to shirk all their responsibilities, completely and without any apprehension, and to blame everything (like war) on man-made climate change, i.e. their own citizens. But hey, maybe these thinkers are accomplishing something valuable with those €32 million they have been generously given.
Q: What do you see as CliSAPs largest achievements so far?
A: I have too little knowledge on most of the research topics and disciplines in CliSAP to answer this question with any confidence. My impression is that CliSAP has advanced quite a bit in its attempt to study climate change issues comprehensively. In social sciences, where I have a better judgment, main gaps remain, but CliSAP has clearly raised the interest of colleagues to get involved.”
Recall that the Cluster was started in October, 2007. So after 4 years he has “too little knowledge to answer this question with any confidence”? Could someone please tell me what we are paying them for? To raise interest of colleagues?
And now here’s Brzoska’s advice for young scientists:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Q: What constitutes “good” science?
A: Max Weber once wrote that it is the purpose of scientists to strive to disprove their own research results. Thus good science is critical science, including of what seems established even by oneself.”
Here I could not decide whether to laugh or to spill my dinner all over my keyboard.
Hint to Brzoska: That advice does not only apply to young scientists, but it also especially applies for the older ones too. Has Brzoska ever considered, for just a fleeting moment, that the true warmist believers could be, ahem, wrong? Seems he hasn’t.
And has he ever considered the horrific human consequences of massively and obstinately planning for the wrong scenario? Bear in mind we are not far from proving willful intent by scientists in producing wrong scenarios. There exists a massive amount of data that indicate things may very well turn out completely different than what they insist. Yet they refuse to acknowledge it.
His advice is of course correct. Science is about being open minded. Unfortunately too many climate scientists have been far too obstinate, elitist, overly pampered, corrupted and cemented deeply in dogmatism, and so the advice falls on many deaf ears. Indeed the greatest risk that global security faces today is not warming, but governments heeding the senseless advice that many scientists demand we accept without question.
Question for Prof Brzoska: 
What should society do with highly influential scientists who absolutely refuse to consider they may be wrong, obstinately insist no matter what that they are right, and actually spend an entire career propping up falsehoods? Should society resist? Well, resisting has led to things like the WBGU advocating a watering down of democracy.
Sorry Mr Brzoska, but your perspective of the world, and on the causes of war, really do frighten me. I expect they frighten many others too. And they ought to be frightened. After all, nothing is more dangerous than a science that becomes decoupled from reality and truth.
Welcome to modern climate science in Germany.
Share this...FacebookTwitter "
"The school climate strikes show that young people want to fight climate change, but their enthusiasm  for collective action is largely untapped. A volunteer conservation army could mobilise their talent and passion by channelling it into work to restore ecosystems. The Green New Deal – endorsed by US Congresswoman Alexandria Ocasio-Cortez and numerous presidential candidates – is a plan to eliminate carbon emissions in ten years, provide full employment in building clean energy infrastructure and redistribute wealth to tackle inequality. The Green New Deal has encouraged people to embrace radical solutions to climate change by sharing its name and ethos with the New Deal of the 1930s. President Franklin D. Roosevelt’s New Deal was a transformation of America’s economy which put thousands to work in manufacturing and redistributed wealth to help the country recover from the Depression.   One of the first and most popular programmes of the New Deal was the Civilian Conservation Corps (CCC) – a public work relief programme that enlisted millions of young men in conservation work throughout the natural environment of the US. Reviving the scheme could prove a popular and effective way for countries to mobilise the climate strike generation in environmentally beneficial work. During the 1930s dust storms devastated the ecology of the Southern Plains in the US. Severe drought and a failure to apply shallow plowing to prevent wind erosion created the Dust Bowl, which forced tens of thousands of poverty-stricken families to abandon their farms, unable to pay mortgages or grow crops.  Doing “the kind of public work that is self-sustaining” in the president’s words, CCC members planted more than 2 billion trees on more than 40 million farm acres between 1933 and 1942. These trees acted as wind breaks and helped bind moisture in the soil – halting the erosion that caused the Dust Bowl. Members also built flood barriers, fought forest fires and maintained forest roads and trails. By enlisting three million men aged between 18 and 25, the CCC helped restore and repair ecosystems throughout the US with hundreds of projects in forestry and conservation. The CCC made many Americans mindful of the sustainability of timber, soil and water for the first time and introduced them to the efforts needed to ensure their preservation. Today, most people are aware of climate change, pollution and biodiversity loss. Through the internet, promoting awareness is certainly easier than in Roosevelt’s era. But the environmental problems themselves are more serious and will require radical changes in society and the economy to overcome. Leaving behind the programme’s legacy of racial segregation, a modern CCC could mobilise any young person who wants to get their hands dirty fighting climate change. A modern volunteer army of conservationists could get to work in every country, adjusting their efforts according to the environmental needs of each setting. The first task set could be in environmental monitoring – collecting data on pollution and wildlife abundance. These surveys would provide invaluable information about the health of ecosystems and how they are changing. Ecosystems could then benefit from projects which reintroduce species and restore habitats. Mass tree planting could absorb atmospheric carbon and provide new habitat for returning wildlife. Wetlands – coastal ecosystems which protect against sea level rise – could be expanded with vegetation which would also create sanctuaries for migratory birds. Reintroduced beavers and other ecosystem engineers could act as animal recruits who create new habitats, such as dams and lakes, which allow even more species to thrive.  Planting trees around river banks in particular provides a food source for aquatic organisms and provides nutrient input to the system. Volunteers could build fencing around freshwater environments to prevent livestock entering the water and  transferring organic material and fertiliser from the surrounding fields into the water. This can cause eutrophication which strips oxygen from the water, eventually causing mass dead zones in coastal waters where nutrient-loaded water is discharged. Legions of litter pickers in parks and on beaches could significantly reduce the amount of plastic pollution which reaches the ocean too. Volunteers could be trained to test water quality and take an active role in monitoring pollution and local sea life. 


      Read more:
      Plastic pollution: seaside communities coming together will save us – not technology


 In rural areas, building drystone walls without mortar encourages mosses and lichens to grow and provides nooks and crannies for birds, toads, newts and insects to set up home. For every tonne of cement manufactured and used in a traditional wall, approximately a tonne of carbon dioxide is released into the atmosphere. Building new infrastructure which uses as little as possible or entirely different materials could be another task for volunteers. Planting hedgerows could create corridors of vegetation which link wildlife to wooded habitat and provide food and shelter. Volunteers could also construct habitat highways – corridors of vegetation which provide safe passage for wildlife under or over major road networks, allowing reproduction between populations to continue. As well as being rewarding and educational, young people taking part in the scheme would develop transferable skills. Working in nature has a positive impact on wellbeing and participants would also benefit from a healthy dose of exercise.  The New Deal of the 1930s sought to tackle an environmental crisis while reorienting the American economy to delivering social justice. Today’s Green New Deal could harness the same ethos, but to more ambitious ends, with a socially inclusive CCC that restores ecosystems and fights climate change. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
I found this press release on the UC Davis website interesting, because it discusses something new to me, “winter chill”. I found it interesting. But immediately, I thought of this study on irrigation by Dr. John Christy of the University of Alabama, Huntsville.
Irrigation most likely to blame for Central California warming
Given that the UC Davis researchers seem to have only looked at temperature records to establish trends, it looks like they may have missed a significant contributor to the trends – increased humidity due to irrigation. – Anthony
From UC Davis News: Warming Climate Threatens California Fruit and Nut Production
July 21, 2009
 






No more cherry picking?







  Winter chill, a vital climatic trigger for many tree crops, is likely to decrease by more than 50 percent during this century as global climate warms, making California no longer suitable for growing many fruit and nut crops, according to a team of researchers from the University of California, Davis, and the University of Washington.
In some parts of California’s agriculturally rich Central Valley, winter chill has already declined by nearly 30 percent, the researchers found.
“Depending on the pace of winter chill decline, the consequences for California’s fruit and nut industries could be devastating,” said Minghua Zhang, a professor of environmental and resource science at UC Davis.
Also collaborating on the study were Eike Luedeling, a postdoctoral fellow in UC Davis’ Department of Plant Sciences and UC Davis graduate Evan H. Girvetz, who is now a postdoctoral research associate at the University of Washington, Seattle. Their study  appears July 22 in the online journal PLoS ONE.
The study is the first to map winter chill projections for all of California, which is home to nearly 3 million acres of fruit and nut trees that require chilling. The combined production value of these crops was $7.8 billion in 2007, according to the California Department of Food and Agriculture.
“Our findings suggest that California’s fruit and nut industry will need to develop new tree cultivars with reduced chilling requirements and new management strategies for breaking dormancy in years of insufficient winter chill,” Luedeling said.
About winter chill
Most fruit and nut trees from nontropical locations avoid cold injury in the winter by losing their leaves in the fall and entering a dormant state that lasts through late fall and winter.
In order to break dormancy and resume growth, the trees must receive a certain amount of winter chill, traditionally expressed as the number of winter chilling hours between 32 and 45 degrees Fahrenheit. Each species or cultivar is assumed to have a specific chilling requirement, which needs to be fulfilled every winter.
Insufficient winter chill plays havoc with flowering time, which is particularly critical for trees such as walnuts and pistachios that depend on male and female flowering occurring at the same time to ensure pollination and a normal yield.
Planning for a warmer future
Fruit and nut growers commonly use established mathematical models to select tree varieties whose winter chill requirements match conditions of their local area. However, those mathematical models were calibrated based on past temperature conditions, and establishing chilling requirements may not remain valid in the future, the researchers say. Growers will need to include likely future changes in winter chill in their management decisions.
“Since orchards often remain in production for decades, it is important that growers now consider whether there will be sufficient winter chill in the future to support the same tree varieties throughout their producing lifetime,” Zhang said.
To provide accurate projections of winter chill, the researchers used hourly and daily temperature records from 1950 and 2000, as well as 18 climate scenarios projected for later in the 21st century.
They introduced the concept of “safe winter chill,” the amount of chilling that can be safely expected in 90 percent of all years. They calculated the amount of safe winter chill for each scenario and also quantified the change in area of a safe winter chill for certain crop species.
New findings
The researchers found that in all projected scenarios, the winter chill in California declined substantially over time. Their analysis in the Central Valley, where most of the state’s fruit and nut production is located, found that between 1950 and 2000, winter chill had already declined by up to 30 percent in some regions.
Using data from climate models developed for the Intergovernmental Panel on Climate Change Fourth Assessment Report (2007), the researchers projected that winter chill will have declined from the 1950 baseline by as much as 60 percent by the middle of this century and by up to 80 percent by the end of the century.
Their findings indicate that by the year 2000, winter chill had already declined to the point that only 4 percent of the Central Valley was still suitable for growing apples, cherries and pears — all of which have high demand for winter chill.
The researchers project that by the end of the 21st century, the Central Valley might no longer be suitable for growing walnuts, pistachios, peaches, apricots, plums and cherries.
“The effects will be felt by growers of many crops, especially those who specialize in producing high-chill species and varieties,” Luedeling said. “We expect almost all tree crops to be affected by these changes, with almonds and pomegranates likely to be impacted the least because they have low winter chill requirements.”
Developing alternatives
The research team noted that growers may be able change some orchard management practices involving planting density, pruning and irrigation to alleviate the decline in winter chill. Another option would be transitioning to different tree species or varieties that do not demand as much winter chill.
There are also agricultural chemicals that can be used to partially make up for the lack of sufficient chilling in many crops, such as cherries. A better understanding of the physiological and genetic basis of plant dormancy, which is still relatively poorly understood, might point to additional strategies to manage tree dormancy, which will help growers cope with the agro-climatic challenges that lie ahead, the researchers suggested.
Funding for this study was provided by the California Department of Food and Agriculture and The Nature Conservancy.
About UC Davis
For 100 years, UC Davis has engaged in teaching, research and public service that matter to California and transform the world. Located close to the state capital, UC Davis has 31,000 students, an annual research budget that exceeds $500 million, a comprehensive health system and 13 specialized research centers. The university offers interdisciplinary graduate study and more than 100 undergraduate majors in four colleges — Agricultural and Environmental Sciences, Biological Sciences, Engineering, and Letters and Science — and advanced degrees from six professional schools — Education, Law, Management, Medicine, Veterinary Medicine and the Betty Irene Moore School of Nursing.
Media contact(s):
• Minghua Zhang, Land, Air and Water Resources, (530) 752-4953, mhzhang@ucdavis.edu
• Eike Luedeling, Plant Sciences, (530) 574-3794, eluedeling@ucdavis.edu
• Pat Bailey, UC Davis News Service, (530) 752-9843, pjbailey@ucdavis.edu


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e946da7f7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter Some scientists and media have gotten much attention claiming that the world’s coral reefs could disappear in as little as 20 to 30 years – all because of humans consuming fossil fuels and whatever.
Now the Financial Times Germany reports on a study that claims this is all exaggerated.
The world’s largest coral reef off the east coast of Australia is not going to disappear as fast as once previously thought, according to a new study. Warnings that the Great Barrier Reef could die off due to climate change over the next 20 to 30 years are exaggerated says Sean Connolly of the James Cook University.”
This comes to no surprise for skeptics. How many millions of years and through what ranges of  temperature swings have the coral reefs survived so far? Indeed a few tenths of a degree Celsius of change over decades will have no impact on the reefs. And I seriously doubt the reefs are going to do what the models tell them.
The James Cook University Press release here says:
…some current projections of global-scale collapse of reefs within the next few decades probably overestimate the rapidity and uniformity of the decline.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Again, if the relatively sudden transition from ice age to optimum did not kill them, why would a few tenths of a degree over decades or centuries do it?
Wikipedia writes that coral reefs in the Persian Gulf have adapted to temperatures of 13 °C (55 °F) in winter and 38 °C (100 °F) in summer, i.e. 25°C change in 6 months. Like any species on the planet, reefs are always threatened by something. The press release writes:
However reefs are naturally highly diverse and resilient, and are likely to respond to the changed conditions in different ways and at varying rates.”
The James Cook press release, despite its obvious findings, still tries to convey an aura of alarm (for funding) yet admits that climate change is a natural process that has occurred time and again in the past.
Past extinction crises in coral reef ecosystems appear to coincide with episodes of rapid global warming and ocean acidification, they say. This has led some to predict rapid, dramatic, global-scale losses of coral reefs.”
The rapid changes they mention here were measured in degrees per decade and century, and not tenths of a degree as is the case with today’s relatively boring rate of change.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently the German Weather Service (Deutsche Wetterdienst DWD) released its 100-year temperature prognosis chart to the public, claiming that Germans should expect more hot days in the future.
German Weather Service: More hot days in the future
DWD releases highly misleading 100-year outlook
Light blue line (before 2010): temperature record from 1881-2010
Yellow line (before 2010): 5th degree polynomial smoothing (annual mean temp.)
Orange line (after 2010): optimistic prognosis
Red line (after 2010): pessimistic prognosis
Not surprisingly, the chart has the typical catastrophic hockey stick shape. Peter Heller at Science Skeptical here closely examined the chart and found a number of deceptive irregularities that all serve to dramatize the future. As a result he dubbed it: a spot-the-errors diagram. 
Can you spot the errors? Peter Heller has.
========================================
German Weather Service Publishes A “Spot The Errors” Diagram
By Peter Heller
For some people, especially children, a spot-the-errors diagram is a lot of fun. But there is nothing funny about errors and inaccuracies in scientific diagrams. This at times is purposely done in political debates in order to hide just how poorly certain claims truly are. How diagrams can be used to mislead is well known. There are books and plenty of articles about it everywhere. Still, it’s attempted time and again, and often successfully. One particularly perfidious example has been produced by the German Weather Service (Deutsche Wetterdienst – DWD). The following graphic was brought to my attention by the European Institute for Climate and Energy (EIKE) in a recent article.

Figure 1: The DWD “spot the errors” diagram. (Click to enlarge)
Figure 1 shows the diagram with curves and three dots I’ve added to denote 1) the error, 2) the deception done on purpose, and 3) the inconsistency.

Dot no. 1: the error 
The DWD uses a 5th degree polynomial curve (yellow) to smooth Germany’s temperature record (annual mean 1881-2010) in order to clearly illustrate the trend. No question here – that’s perfectly okay to do. Figure 2 is my attempt to reproduce the curves and it shows a somewhat different polynomial trend curve, namely at the end it is flat and in no way increasing upwards as calculated by the DWD. The flattening of the curve is due to the comparatively cool 2010, for which the DWD calculated a mean temperature of 7.8° C. Using the polynomial curve that the DWD uses, I also get a flat end…that is when I use a temperature of 8.6°C for the year 2010. Probably an error.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2: My reconstruction. (Click to enlarge)
Dot no. 2: the deception
Of course the purpose of the graph is to show the horrific climate change that lies ahead for Germany. This becomes especially pronounced when one draws curves that rise upwards as dramatically as possible after 2010. To amplify the the rate of increase, one simply employs a more effective horizontal scale. Hey, who is going to notice? And so after only 40 years the curves shoot up into an apocalypse. But notice how the horizontal axis shows the year 2100. Maybe DWD just wanted to make some room for the thermometer to make the chart look prettier. In the end, this ends up being an intentional deception in order to make the future look more dramatic.

Figure 3: Graphic using a correct scale for the future. (Click to enlarge)
Dot no. 3: the inconsistency
In Figure 3 I simply superimposed my diagram over the original DWD diagram and gave the chart the correct scale on the right side of 2010. You can already see the difference in both polynomial curves (yellow). The gray line is the linear trend (which the DWD opted not to show) that allows a prognosis. Suddenly much of the drama gets lost. Here one must recall the following: A prognosis is always the extrapolation of a known trend. One can make a prognosis completely independent of having knowledge of the fundamentals behind the trend. And in the following case, making a prognosis would simply mean extending the fluctuation of the annual mean temperature (yellow line) about the gray line. The orange and red lines in Fig. 3, on the other hand, are DWD projections and not prognoses.
To do these projections, models were used that most likely are in accordance to what is propagated by the IPCC, see Figure 4. In any case, there is good agreement with the calculations used in the recently DWD published “Regional Climate Atlas“, which is green in the graphic. The “Klimaatlas Deutschland” has quite identical results and can be seen at the DWD website.

Figure 4: Flattening already in 2100? (Click to enlarge)
 
Interestingly the course of the DWD projection between 2011 and 2100 in Figure 1 is completely different than what is shown in Figure 4. It is concave, turning sharply upwards, and not convex. I would at least call this an inconsistency.
We can summarize as follows: The DWD, as a public institute directly subordinate to the German Federal Government (Ministry of Transportation, Construction and Urban Development), believes it is correct and important to take a clear position in a highly political issue in a press conferencez. And to support this position it resorts to using a diagram that has errors, deceptions and inconsistencies that contradict to its own calculations. All of these are perfidiously placed in the diagram in a way they are not easily detectable. One indeed has to look very closely. All three of the elements happen to dramatize the situation. That’s not a good way to build trust.
Peter Heller
(Translation/editing by P Gosselin)
=================================================
Peter Heller points out that he does not value it as very important, and adds. “Handling of diagrams of all kind in this way happens everywhere and every time – especially in climate related discussions.”

Share this...FacebookTwitter "
"**The owners of a card and gift shop say they are defying lockdown laws ""on principle"" and to pay their bills.**
Alasdair Walker-Cox, of Grace Cards and Books in Droitwich, said despite police visits and a council prohibition order they would stay open during lockdown.
He said: ""If we shut we won't be able to pay suppliers, the rent, let alone support the family. If we open we can.""
Wychavon District Council said it was a type of business that must ""close by law"" and issued a Â£1,000 fine.
Footage of West Mercia Police officers telling the owners they may lose their licence if they ignored the council's order has been circulated on social media.
Non-essential shops were among places in England told to close for four weeks on 5 November to curb the spread of coronavirus.
Businesses can be fined by local authorities or the police if they fail to comply, with penalties ranging from Â£1,000 for a first offence to Â£10,000 for the fourth and all subsequent offences.
The prohibition notice was issued on 19 November. The fine was issued on Monday but the shop remained open on Tuesday.
Mr Walker-Cox, who runs the shop with wife Lydia, said he believed lockdowns did not ""work"" against the virus and ""on principle"" wanted to open and support their family and suppliers instead.
Mrs Walker-Cox added it was an ""essential shop"" because it sold icing and edible decorations to cake makers working at home.
The district council said the shop could trade by delivery or ""click and collect"" and said its food offering is not substantial enough to detract from its core activity which is a card and gift shop.
It added it was supporting businesses through the pandemic \- paying out more than Â£600,000 during the current lockdown.
Ch Supt Paul Moxley said West Mercia Police remained ""hugely sympathetic to the difficult times"" faced by business owners.
""We understand the restrictions can be challenging, and we know this business is well-loved in Droitwich, but the government legislation is in place to minimise the spread of Covid-19 and to keep us all safeâ¦. we all have a critical part to play in that,"" he said.
_Follow BBC West Midlands on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to:_newsonline.westmidlands@bbc.co.uk"
"Mining giant Rio Tinto says it wants its globe-spanning operations to reach net zero greenhouse gas emissions by 2050 and will spend US$1bn over the next five years to reduce its carbon footprint. The second biggest miner in the world has also committed to reducing its emissions by 15% by 2030.  Rio Tinto’s decision, which follows pressure from investors, puts it in line with Australian business groups and unions and at odds with an Australian government that has devoted itself to attacking the Labor opposition over its commitment to the same net zero target. Its commitment to cut emissions is easier for Rio Tinto than BHP and other big global miners because unlike BHP, it does not mine coal or oil. However, unlike BHP, Rio Tinto has refused to set a target to reduce so-called “scope 3” emissions produced by its customers. Announcing Rio Tinto’s full-year results on Wednesday, the company’s chief executive Jean-Sébastien Jacques also warned that the coronavirus outbreak could hurt its operations. Chinese steel mills are major Rio Tinto customers, however Jacques said the company’s iron order books were full.  “But we are likely to see some short term impacts such as supply chains and possibly even provision of services from Chinese suppliers,” Jacques said. “We acknowledge that there will be some short-term volatility and uncertainty, but we are very well positioned.” He said Rio Tinto’s new commitments on climate added to a 46% cut in the company’s emissions since 2008, although much of that reduction was due to it selling operations that produce a lot of pollution. To reduce its total emissions by 15% by 2030, every new business opened by Rio Tinto in the next decade will need to be carbon neutral. Jacques said the company was open to buying carbon offsets if necessary but this was a “last port of call”. He said the emissions cut to 2030 would be achieved using existing technology but Rio Tinto would also be investing part of the $1bn on developing new ways of eliminating carbon production. “We have approved around US$100m last week for the Pilbara, I think it would be a good example of things we may look at going forward,” he said. “It’s a 34MW solar photovoltaic plant and a battery system of 12MW per hour storage facility at one of our operations. “By doing this investment … we’ll be able to take out about 90,000 tonnes compared to commercial gas power generation.” This was equivalent to taking 28,000 cars off the road or about 3% of the company’s emissions from the Pilbara, he said. “At the same time we are investing serious money in order to find the technology for the future. We have only a pathway for the next 10 years. If we don’t work today on options beyond the next 10 years, we’ll never get there.” He defended Rio Tinto’s decision not to set scope 3 targets – something rival BHP did in July as part of a $400m program to cut its emissions. “We will not set targets for our customers,” Jaques said. “Having said that, we are looking to partner with our customers and the customers of our customers to look for ways to work together in order to improve emissions across the value chain.” He said examples included a deal the company struck with giant Chinese steel mill and key customer Baosteel in September to reduce emissions from steelmaking and another deal struck with Apple and the government of Quebec two years ago relating to aluminium. “Remember, we are the only large diversified mining and metal company that is not selling either coal, and the carbon associated with coal, or drilling oil and gas and the carbon associated with oil and gas. “So if you step back, if you believe in climate change – and we do believe in climate change – we know we need to have more high quality copper, high quality aluminium, in order to be part of the solution.” Australian prime minister Scott Morrison’s conservative Coalition government campaigned hard against Labor’s net zero target as part of its re-election strategy at the federal election last May, and in recent weeks has revived its attacks on the opposition over the policy by demanding costings and raising the spectre of soaring energy prices. Asked if Rio Tinto would like the government to commit to net zero by 2050, Jaques said: “I don’t think that would make a massive difference.” “What is absolutely clear is that for us to be able to meet our net carbon target by 2050, there will be a need for new technology. “Any government – so I’d make a broad point – that can provide us with some smart, clever policy to create a pathway, a framework for us to accelerate our development in that space, would be more than welcome.” Andrew Gray, the head of environmental, social and governance issues at Australia’s biggest superannuation fund, the $175bn AustralianSuper, said Rio Tinto’s move followed pressure from investor group Climate Action 100+. “These are issues that AustralianSuper as a lead investor has been engaging with Rio on for more than two years as part of the Climate Action 100+ initiative,” he said. “We look forward to continuing to engage with Rio to fulfil these commitments.” The company declared a profit after tax for 2019 of US$7bn, down from US$13.9bn the previous year."
"

Weeks after the National Science Foundation released a report about the connection between increases in atmospheric carbon dioxide and the acidity of the oceans, doomsayers continue to prophesy that global warming will kill the coral reefs off our picturesque Florida coast.



The NSF study, released with two other federal research entities and entitled “Impacts of Ocean Acidification on Coral Reefs,” landed with a thud, and it is remarkable how the press has received it. Writers have editorialized about it, literally with one voice, without any critical fact‐​checking. In a July 11 editorial, the editors of the _Cincinnati Post_ wrote, “This report is a fraction of the available evidence indicating anthropogenic climate change.…The evidence is clear and convincing. The global‐​warming critics are neither.” On July 12, the _Albuquerque Tribune_ , in its own in‐​house editorial, printed the same words (without attribution).



It could have done something more original and scrutinized the NSF report. There’s a major problem with it, right at the beginning. Its first paragraph states correctly that, as a result of the burning of fossil fuel and other activities, atmospheric carbon dioxide concentration is rising. From there, however, the report loses its way. “Rates of increase,” it says, “have risen from 0.25% [per year] in the 1960s to 0.75% [per year] in the last five years.”



Really? The standard reference for atmospheric carbon dioxide concentration is that registered at Mauna Loa Observatory, beginning in 1958. The average rate of change in the 1960s was 0.30% per year, and in the last five years, it was 0.55%. This last value is not statistically distinguishable from the average rate for the past 25 years. The real change from the 1960s to the last five years is 0.25% per year, while the NSF‐​sponsored report gives it as twice that.



The precise figure is important, because the rate of increase of atmospheric carbon dioxide is directly related to the amount of warming it creates and to changes in the acidity of the oceans; computer models using a carbon dioxide increase rate twice that which is observed show twice as much warming. And that is precisely what has occurred: there are now four separate, taxpayer‐​supported reports “intercomparing” the dozens of climate models for global warming that have evolved in recent years. Each one uses a carbon dioxide increase of 1% per year, or twice the real rate. Ever wonder why they predict so much warming?



It gets better (worse). The coral report then states that “The current atmospheric CO2 concentration…is expected to continue to rise by about 1% [per year] over the next few decades.”



“Continue”? The average increase for the last decade was 0.49 per year, for the decade before that was 0.42%, and for the decade before that was 0.43%. Again, about half of what the report expects to “continue.”



The current concentration of carbon dioxide in the atmosphere is about 380 parts per million. Before we industrialized — back when life expectancy was in the 40s — the concentration was about 280.



Fewer than 100 million years ago, or 400 million years _after_ corals first arose, the carbon dioxide concentration was a bit less than 3,000 ppm. Around 175 million years ago it was pushing 6,000. If there was that much more carbon dioxide around, the oceans would have been that much more acidic, which would have killed the corals. And yet they lived.



How does the report take this problem into account? It balances the increase in acidification that these concentrations of carbon dioxide would bring about with some countervailing change in its opposite, alkalinity. So the report speculates that “ocean alkalinities _could_ have been higher during periods with high CO2 levels.” (Emphasis added.)



Then there’s the problem of identifying a definite decline in corals. The report says that it is “difficult” to find this effect, and that “on average” it does not exist, because the rates of coral growth are controlled by many other factors that are apparently obscuring their decline.



How on earth did all of this make it through peer review? Or do we no longer care enough to get the facts right before expressing opinions under the mantle of scientific authority?



To many editorialists, when it comes to global warming, facts don’t matter. But here are a few: corals have been around for half a billion years, on a planet that was much, much warmer, had much more carbon dioxide in its atmosphere, got hit by an asteroid or two, experienced ice ages and is now in the midst of a slight warming trend. You can bet that they’ll be around a long time after humans have come to the end of the evolutionary road.
"
"**A nurse who struggled to get face masks to fit her has inspired the design of custom-fitted ones for frontline healthcare workers.**
Gareth Smith set up MyMaskFit after his wife, intensive care nurse Valerie Bednar, struggled to find a filtering face mask (FFP) to fit her.
Based in Swansea, the firm is working with a number of UK universities.
Ms Bednar said the masks are reusable so will reduce stress among staff and be better for the environment.
""I'm one of the people the standard disposable FFP3 masks doesn't fit my face,"" said Ms Bednar, who worked at Morriston Hospital in Swansea at the start of the pandemic but is currently on maternity leave.
""It was just the stress of trying to do what you need to do - the reason we go into nursing is to take care of people, and then the added level of 'am I being safe and do I have the protection that I need?'
""That uncertainty I think was stressful for everyone.""
The company hopes to further develop a prototype designed by researchers at Birmingham University and King's College London.
Swansea University's School of Engineering will help test and manufacture the face mask, which it is hoped will be available to the NHS in Wales in the new year.
MyMaskFit said it is aiming to become the first to make a fully custom-fitted, reusable, filtering face piece masks made to a medical grade standard in the UK.
""We want to make a reusable mask so that staff can feel confident when they come in for their shift it will be there,"" Ms Bednar explained.
""You're involved in cleaning it and owning it - all of that gives people the sense of security and protection.""
To speed up the design process and to achieve a seal which will fit anyone, the company has launched an app which will scan the face and send the data for a mould to be created and 3D printed.
MyMaskFit technology director Paul Perera said current masks vary widely in terms of design.
""There is an inevitable variation in the shape of human faces, and BMA surveys have shown that over 20% of hospital doctors have to try one or more masks to find one that fits,"" he said.
Mr Perera said the firm was also working on a face mask which is made with ""renewable plastics that are transparent"" to aid communication.
He added: ""We're also using a copper, embedded into the plastics, which kills the virus. Therefore the masks can be reusable and therefore more sustainable for the environment.""
The initial manufacturing process and further testing of the prototypes will take place at Swansea University."
"**The north of England faces a return to 1980s-style prolonged economic decline, the Labour mayor of Greater Manchester Andy Burnham has warned.**
This is in spite of a government promise to ""double down on levelling up"" at the Spending Review this week.
Regional inequalities have increased due to the Covid-19 crisis, Mr Burnham said.
But the government pointed to billions spent supporting the UK economy during the pandemic.
Mr Burnham told the BBC: ""We could be looking at another period like the 1980s in the north of England, coming out of the Covid crisis, the 2020s could even be worse than the 1980s.
""So that is the test facing this government. And it's still unclear whether or not they're going to pass it,"" Mr Burnham said.
He argued the Covid crisis had increased the UK's regional inequalities and that the country had been ""levelled down"", and said the billions expected to be spent on big new infrastructure projects will not be enough.
""It won't be good enough for the chancellor to come to the Commons on Wednesday, and promise railway lines in 20 or 30 years time when people's lives are basically on hold now.
""If the chancellor ignores all of those things, I don't think people will take seriously his claims to spend all of this money in decades to come.
""He has to support people. He has to support businesses now. Otherwise, there will be no economy to rebuild in 2021 or 2022.""
The mayor, a former chief secretary to the Treasury, said that changes to the way the government evaluates where to make investments in big projects were ""moves in the right direction"" to address ""a bias against the north which goes back decades to governments of all colours"".
The changes to the Treasury's Green Book process will be detailed in the Spending Review on Wednesday.
A landmark government rail investment - the fast trans-Pennine link known as ""Northern Powerhouse Rail"" - may not pass the current Treasury tests, he said.
""There is a real risk that the Treasury will say Northern powerhouse rail doesn't pass the test. So this is why the chancellor's commitment to rewrite the Green Book is very welcome,"" he said.
But Mr Burnham, who clashed with the government over support to businesses in lockdown earlier in the autumn, stressed that funding and reforms that enabled lower bus fares in Manchester than London should be the priority.
""If the government wants people to buy into levelling up, they've got to do something to bring down the cost of transport, make it more reliable, and do that within a timeframe. That's a matter of a couple of years, not 20 or 30 years,"" he said.
The government pointed to the tens of billions in support given to wages and businesses during the Covid crisis.
New economic forecasts will be released by the independent Office for Budget Responsibility tomorrow, alongside the Spending Review."
"The UK is widely seen as a climate leader. Its Climate Change Act, which passed into law ten years ago, is the envy of the world. It has targets for carbon reduction enshrined in law, and recently, the government hinted that it would adopt a target of zero greenhouse gas emissions by 2050 (the current target is an 80% reduction). Four years ago, the government, with cross-party support, announced it would phase out coal-fired power generation by 2025. And yet, at a planning committee meeting in the northern English county of Cumbria, where I live, local councillors have voted unanimously to approve a new deep coal mine, Britain’s first in three decades. The mine would extract nearly 3m tonnes of coal a year, primarily for the steel industry rather than power generation. According to Scientists for Global Responsibility, this would result in more than 9m tonnes of carbon dioxide being released into the atmosphere, every year for 50 years – that’s equal to the emissions from a million households. How can a country with such strong ambitions to reduce carbon emissions, approve a plan to increase them so significantly? My research, which is based on interviews with MPs and looks at how politicians understand and respond to climate change, suggests why such a contradictory situation could have arisen. For the past decade, there has been a cross-party consensus in support of long-term carbon targets. Just five out of 650 MPs voted against the Climate Change Act. Yet a side-effect of this consensus has been that politicians have not talked about climate change very much. Political debate is conspicuous by its absence, the climate is rarely discussed in parliament, and few MPs champion the issue. One climate-conscious MP said that he was “known as a freak” for speaking out, while another told me “it’s important not to be a climate change zealot”. They might have voted for carbon targets, but they are reluctant to discuss what this means for our society or economy. Up until now, you could argue that it hasn’t mattered much. The UK has reduced its carbon emissions through structural changes to the economy, and switching away from coal in electricity generation, towards gas and renewables. But there are now fewer and fewer savings like this to be had. As the government’s official advisers, the Committee on Climate Change notes, the UK is not on track to meet future targets.  Now, we need carbon savings right across the economy, through changes to transport, housing, land use and industry. This in turn means changes in the jobs we do, what our houses look like, and how we travel. These changes can bring many benefits – not just reduced emissions, but healthier cities, warmer homes and stronger communities. But it’s the job of politicians to articulate these benefits, alongside the scientific case for climate action, and to build a mandate for the changes needed. In short, there is a need for proper political debate on climate change. Which takes us back to those local politicians in Cumbria who decided to approve the coal mine. Look at it from their point of view. Local authorities have no clear responsibilities or targets to reduce carbon (though it is a factor in planning law). Local politics, in an economically deprived area, is dominated by the need for good employment. Dangle 500 jobs, even high carbon jobs, in front of a local planning committee; make a claim that this is in line with climate commitments; add in the cultural norms that I described above, which make it difficult for politicians to make a political case for climate action; and it’s not surprising that the answer comes out as a yes. Of course, the local councillors should take responsibility for the decision they have made. But responsibility lies elsewhere as well. National climate policy failed Cumbria in two ways. First, a decade of ambiguity and inconsistency – the price paid for lack of proper debate – means that there is no direct line of sight between carbon targets set at a national level, and individual decisions taken by local councils. And second, climate policy has been top-down and expert-led, with no attempt made to engage citizens or local areas in the need for, and benefits of, the transition to a zero-carbon society. This is a serious failing. And yet it points to the way forward. What the UK now needs is an open, positive political debate about how to meet its carbon targets, in place of the quiet ambiguity that has characterised the past decade. Such a debate would encourage honesty about some fundamentals: that high-carbon infrastructure like coal mines and new runways are simply incompatible with the climate challenge. This honesty would, in turn, allow discussion of what the zero-carbon transition looks like in areas like Cumbria – and what it would mean for jobs and communities. Scotland’s Just Transition Commission, and the Green New Deal in the US, could be models for this. Local politicians, and local communities, will need to design and own these strategies – and national government must give them the powers and resources to do so. Approving the coal mine was the wrong decision. But it may just force a new, more honest politics onto the table – and one which benefits Cumbria as well as the wider world. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"It’s been suggested that a recent fall in recycling rates is due to green fatigue, caused by the confusing number of recycling bins presented to householders for different materials. Recycling rates would rise, according to waste recycling firm Sita UK which carried out the survey, if the process was simplified, for example by collecting all waste for recycling together (known as co-mingling). But the same survey also found that reduced glass and paper consumption had led to a drop in the volume of those materials collected. Perhaps this is the underlying issue and not “green fatigue”. It is well known that fewer people are buying newspapers and that many products previously sold in glass, such as milk, are now sold in plastic. Thus, lower consumption of products in previously widely recycled containers leads to less waste and may be causing lower recycling rates. So is this really a cause for concern, if the drop in recycling rates is down to there being less waste generated in the first place? Well, yes, if this means the 2020 target of 50% household recycling is not met. In that case, the UK government will be fined, and this may affect householders indirectly through higher taxes or lower provision of public services. However, this concern may be misplaced and perhaps is more to do with the way the target is defined, rather than recycling levels.  The relationship between recycling and the waste we generate implies that a drop in the amount of waste produced or an increase in less-recyclable material will lead to a fall in recycling volumes and possibly a drop in the recycling rate. The former should be recognised as a welcome development, whereas the latter reflects changing patterns of consumption. This should prompt new innovation in the waste sector to deal with these types of waste. For example, by developing improved recycling methods or technology to deal with different types or combinations of materials.  So the current preoccupation with the headline recycling rate may be unhelpful. If the concern is the impact that excessive waste is having on the environment, then less recycling due to less waste being generated in the first place should not be a cause for concern. If the drop in recycling reflects changing consumption patterns, any penalties should be aimed at generating incentives to innovate in terms of packaging and new and better ways to deal with different types of waste. We need better analysis to identify the true causes of any change in recycling patterns, and the legislation – designed to drive greater recycling – should recognise why these two situations are different. It may be that green fatigue is one cause among several. To the extent that a lack of information or sheer inconvenience exacerbates the problem, there is evidence to show that improving both these aspects does promote recycling – particularly among those whose attitude is not especially pro-environmental. On the other hand, if the only means to make recycling more convenient is through co-mingling, there is the valid point that the risk of contamination is high and levels of useful recycling may actually fall.  Research indicates that there are several factors that underpin the rate of recycling, from the way the service is provided, to whether recycling is considered a social norm among families, communities or groups banded by age, ethnicity or location. Perhaps the role for government in tackling any dip in the recycling rate is to highlight the prevalence of recycling among certain groups as a way to demonstrate the existence of that recycling norm – and by doing so, encourage it in others."
"**Here are five things you need to know about the coronavirus pandemic this Wednesday morning. We'll have another update for you at 18:00 GMT.**
We now know that three households of any size will be able to get together for five days over the festive period and travel restrictions will be lifted to allow people to move freely across the UK. The leaders of all four nations, though, have issued pleas for caution, especially when it comes to vulnerable relatives. Some scientists have warned the relaxation could spark another wave of infections and further deaths, but ministers hope with compliance up to and after the window the impact can be minimised. Read the rules in detail and our guide to the sort of questions to consider if you're deciding whether to get to together.
The chancellor will explain later how he hopes to protect jobs and help the economy recover from the devastating impact of coronavirus. Rishi Sunak will announce more money for the coming year for the NHS and schools, but less for overseas aid and possibly the majority of public sector workers, in the form of a pay freeze. We'll also get official forecasts about how long the impact of the pandemic will be felt. We'll bring you his speech live at 12:30 GMT and help make sense of it afterwards. In the meantime, read more about why it matters to you, and four things to look out for in particular.
A shortage of personal protective equipment at the start of the pandemic led to the government paying Â£10bn more to secure sufficient supplies than if they'd bought it a year earlier. Spending watchdog the National Audit Office said not enough PPE had been stockpiled and unprecedented demand led to very high prices. The government said the report acknowledged NHS providers had been able to get what they needed - although the NAO heard feedback from staff who believed they were ""not adequately protected during the height of the first wave"".
For many victims of domestic abuse coronavirus has only worsened their suffering - trapped at home with their tormentors, cut off from family, friends and colleagues. About two thirds of women in England living with domestic abuse told Women's Aid their ordeal had got worse during the UK's first lockdown. The UN is calling it the ""shadow pandemic"" and it's a focus of BBC 100 Women this year. Read the story of one woman, Victoria, who managed to escape. See the full list of 100 Women, including those making a difference to those suffering from abuse around the world.
From Christmas bubbles to the Bake Off bubble... the contestants on this year's show stayed together for the duration of filming and provided a much-needed lift to many of our spirits over the last 10 weeks. The final even saw the creation of a ""Bonkers Bake Off bubble cake"". This year's winner has now been crowned, but we won't reveal who it is here, just in case. Click through to find out.
Get a longer news briefing from the BBC in your inbox, each weekday morning, by signing up here.
Find more information, advice and guides on our coronavirus page.
Plus, the BBC News website's health editor Michelle Roberts explains why we should have confidence in the safety of any coronavirus vaccines given the final go-ahead.
**What questions do you have about coronavirus?**
_ **In some cases, your question will be published, displaying your name, age and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read our**_terms & conditions _ **and**_privacy policy.
Use this form to ask your question:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or send them via email to YourQuestions@bbc.co.uk. Please include your name, age and location with any question you send in."
"
Share this...FacebookTwitterHere’s something you won’t find in the MSM, but thanks to NoTricksZone, a few of us out there will hear about it. (Just imagine if they had been tea-partiers).It’s official: Some of Greenpeace’s members have found guilty. Eleven activists from Britain, Denmark, the Netherlands, Norway, Spain, Sweden, Switzerland and the United States were found guilty today in Denmark of falsifying documents and number plates, this according to Univision.com here. They were sentenced to 14 days suspended prison terms.
On 17 December, 2009, the head of Greenpeace Spain Juan Lopez de Uralde and Norwegian Nora Christiansen crashed the Danish parliament’s security and unfurled a banner with the words: “Politicians Talk, Leaders ACT”.
Yet, the impatient radical activists ought to be happy with the judgement, as they got off easy. Univision.com writes:
In its ruling, the court explained the low sentences by “the character of the circumstances on the one hand, and on the other, the fact that the transgressions were part of a peaceful political happening with the goal of causing debate,” pointing out that “no particular serious rights were infringed upon.”
The goal of causing debate? Right. The last things these kooks want is debate. These are dogmatists who are incapable of listening to other opinions, they deny contradictory data,  and demand that their views of the world be enacted as law immediately. Democracy for them is a just a bothersome obstacle. Luckily democracy is still keeping these loons in check.
Feeling unappreciated and peeved, the planet-saving López de Uralde added:
We were treated almost like terrorists.”
The court also found the Greenpeace Nordic wing guilty of organising the demonstration and fined the group  $15,000.
Of course don’t expect them to be repentant now that they’ve been slapped on the wrist. They believe their actions were justified. Just listen to former Juan López de Uralde, as quoted recently by the leftwing online German Tageszeitung (TAZ) here:
The whole process was completely over the top. and a bit surreal. The worst is: Although the most recent data on climate change are alarming and emissions have increased since the failed summit in Copenhagen, rising 5% just last year, massive action is being taken against activists. There’s a huge and obvious contradiction between the will to take on climate change and the persecution of activists.”
Share this...FacebookTwitter "
"A German teenager dubbed the “anti-Greta” – climate sceptics’ answer to the schoolgirl activist Greta Thunberg – is set to address the biggest annual gathering of US grassroots conservatives. Naomi Seibt, 19, who styles herself as a “climate sceptic” or “climate realist”, will this week address the Conservative Political Action Conference (CPAC) near Washington, joining speakers including Donald Trump and Vice-President Mike Pence. Seibt is in the pay of the Heartland Institute, a thinktank closely allied with the White House that denies established science showing humans are heating the planet with dangerous consequences. CPAC will be the biggest stage yet for Seibt, a so-called “YouTube influencer” who tells her followers Thunberg and other activists are whipping up unnecessary hysteria by exaggerating the climate crisis. “Climate change alarmism at its very core is a despicably anti-human ideology,” she has said. The teenager, from Münster in western Germany, claims she is “without an agenda, without an ideology”. But she was pushed into the limelight by leading figures on the German far right and her mother, a lawyer, has represented politicians from the Alternative für Deutschland (AfD) party in court. Seibt had her first essay published by the “anti-Islamisation” blog Philosophia Perennis and was championed by Martin Sellner, leader of the Austrian Identitarian Movement, who has been denied entry to the UK and US because of his political activism. A Facebook post by the AfD youth wing names Seibt as a member and she spoke at a recent AfD event, though she has denied membership of the party. In May 2019 she posted her first video on YouTube, reading out verses submitted for a poetry slam competition organised by the AfD. The impact of the clip and its follow-ups put her on the radar of the Heartland Institute, which is based in Chicago. It has lobbied on behalf of the tobacco and coal industries but recently concentrated its efforts on challenging the scientific consensus on climate change. Last December, as Thunberg addressed the United Nations’ Cop25 global warming summit in Madrid, Seibt gave the keynote speech at a rival conference organised by the Heartland Institute a few miles away. In a sting operation carried out for German broadcaster ZDF and investigative outlet Correctiv, the Heartland Institute strategist James Taylor told journalists posing as potential donors his thinktank had signed up Seibt to record climate change sceptic videos for young people. Seibt has admitted that she receives “an average monthly wage” from the institute. According to official figures, the average net monthly income in Germany is just under €1,900 (£1,590, $2,066). The Heartland website features a low-budget video introducing Seibt, who speaks to the camera from what appears to be a home. “I’ve got very good news for you,” she says. “The world is not ending because of climate change. In fact, 12 years from now we will still be around, casually taking photos on our iPhone 18s “We are currently being force-fed a very dystopian agenda of climate alarmism that tells us that we as humans are destroying the planet. And that the young people, especially, have no future – that the animals are dying, that we are ruining nature.” In another film, Naomi Seibt vs Greta Thunberg: Whom Should We Trust?, Seibt says: “Science is entirely based on intellectual humility and it is important that we keep questioning the narrative that is out there instead of promoting it, and these days climate change science really isn’t science at all.” Seibt has also uploaded a video with the title Message to the Media – HOW DARE YOU – an obvious reference to a speech by Thunberg at the UN in which she rebuked world leaders: “We are in the beginning of a mass extinction, and all you can talk about is money, and fairytales of eternal economic growth. How dare you!” Thunberg began her activism at 15 by missing school and camping outside the Swedish parliament. She has since met the pope, addressed members of Congress in Washington and heads of state at the UN and helped inspire 4 million people to join a global climate strike. Last year she became the youngest Time magazine Person of the Year, much to Trump’s chagrin. The Washington Post observed: “If imitation is the highest form of flattery, Heartland’s tactics amount to an acknowledgment that Greta has touched a nerve, especially among teens and young adults.” Since Trump’s election, CPAC has paraded hard-right figures such as the former White House officials Steve Bannon and Sebastian Gorka as well as numerous climate sceptics. In his speech there last year, the president mocked the Green New Deal, proposals championed by Democrats including Alexandria Ocasio-Cortez. “No planes,” the president said. “No energy. When the wind stops blowing, that’s the end of your electric. ‘Let’s hurry up. Darling, darling, is the wind blowing today? I’d like to watch television, darling.’” Connor Gibson, a researcher for Greenpeace USA, said: “Climate science is understood by a majority of Americans, liberal and conservative alike. Unfortunately, you won’t meet any of those people, or any climate scientists, at an event like CPAC. “The Heartland Institute is funnelling anonymous money from the US to climate denial in other countries. It relies on the media to advance false equivalence strategies to attempt to normalise fringe beliefs. Climate denial is not a victimless crime, and it’s time for the perpetrators to be held accountable.”"
"
The Warning in the Stars
By David Archibald
If climate is not a random walk, then we can predict  climate if we understand what drives it.  The energy that stops the  Earth from looking like Pluto comes from the Sun, and the level and type  of that energy does change.  So the Sun is a good place to start if we  want to be able to predict climate.  To put that into context, let’s  look at what the Sun has done recently.  This is a figure from “Century to millenial-scale temperature variations for  the last two thousand years indicated from glacial geologic records of  Southern Alaska” G.C.Wiles, D.J.Barclay, P.E.Calkin and T.V.Lowell 2007:

The red line is the C14 production rate, inverted.  C14 production is  inversely related to solar activity, so we see more C14 production  during solar minima.  The black line is the percentage of ice-rafted  debris in seabed cores of the North Atlantic, also plotted inversely.   The higher the black line, the warmer the North Atlantic was.  The grey  vertical stripes are solar minima.  
As the authors say, “Previous  analyses of the glacial record showed a 200- year rhythm to glacial  activity in Alaska and its possible link to the de Vries 208-year solar  (Wiles et al., 2004). Similarly, high-resolution analyses of lake  sediments in southwestern Alaska suggests that century-scale shifts in  Holocene climate were modulated by solar activity (Hu et al., 2003).  It  seems that the only period in the last two thousand years that missed a  de Vries cycle cooling was the Medieval Warm Period.”
The same periodicity over the last 1,000 years is also evident in this  graphic of the advance/retreat of the Great Aletsch Glacier in  Switzerland:

The solar control over climate is also shown in this graphic of Be10 in  the Dye 3 ice core from central Greenland:

The modern retreat of the world’s glaciers, which started in 1860,  correlates with a decrease in Be10, indicating a more active Sun that is  pushing galactic cosmic rays out from the inner planets of the solar  system.
The above graphs show a correlation between solar activity and climate  in the broad, but we can achieve much finer detail, as shown in this  graphic from a 1996 paper by Butler and Johnson (below enlarged here)::

Butler and Johnson applied Friis-Christensen and Lassen theory to one  temperature record – the three hundred years of data from Armagh in  Northern Ireland.  There isn’t much scatter around their line of best  fit, so it can be used as a fairly accurate predictive tool.  The Solar  Cycle 22/23 transition happened in the year of that paper’s publication,  so I have added the lengths of Solar Cycles 22 and 23 to the figure to  update it.  The result is a prediction that the average annual  temperature at Armagh over Solar Cycle 24 will be 1.4C cooler than over  Solar Cycle 23.  This is twice the assumed temperature rise of the 20th  Century of 0.7 C, but in the opposite direction.
To sum up, let’s paraphrase Dante: The darkest recesses of Hell are  reserved for those who deny the solar control of climate.
This essay is also available in PDF form: TheWarningintheStars


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8dbeb0d9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Billions of small, jellyfish-like creatures known as “by-the-wind sailors” have washed ashore all along the west coast of North America this summer, from southern California to British Columbia. Images of vast swarms of electric-blue sails covering the ocean’s surface and littering the sand are indeed spectacular, but people might well wonder what exactly these strange-looking beings actually are. And this of course leads to the next question – should we be afraid of them? Velella velella (to give them their scientific name) are often assumed to be a type of jellyfish but, while biology does lump them in with jellyfish, sea anenomes, and corals in a group known as Cnidaria, Velellas are not all that closely related to the common or moon jellyfish, Aurelia aurita. Cnidarians have two body forms: the umbrella-shaped, tentacle-trailing “medusa”, your classic jellyfish; and “polyps” such as seas anemonies that typically live attached to the seabed. Velella is a colony of specialised individual polyps, much like their fellow sailors the Portuguese Man o’ War. Instead of living attached to rocks on the seabed, the water surface has become its substrate. The by-the-wind sailor’s body is a flat oval disk 6-7 cm in diameter containing a series of air-filled chambers that provide buoyancy. Below hangs a central mouth surrounded by specialised reproductive bodies that produce tiny medusae, little “jellyfish”, and stinging tentacles – which are harmless to humans.  Projecting vertically up is a stiff translucent triangular vane made of chitin, a substance derived from glucose that is also used in crab and insect skeletons or squid beaks. This vane acts like a small sail. Interestingly, the sail runs diagonally across the top of the float, so that the individual sails at a 45 degree angle to the prevailing wind, just like a sailing boat.  Another striking feature is the bright blue colour, which is thought to serve as camouflage and/or protection from the sun’s rays. Animals that wash up on the beach dry up and become bleached white within a day or two. Velella velella use their stinging tentacles to capture and feed on small fish larvae and zooplankton – microscopic animals that drift in the sea. But this is not their only source of food. If you look closely, you will also see a golden-brown colour inside the tissues which are zooxanthellae – symbiotic photosynthetic microalgae – that provide the host animal an additional source of nutrition. By-the-wind sailor is a very common open ocean organism, living in warm to warm-temperate waters throughout the world’s oceans. It is thought that there is a difference in preferred sailing direction in the northern and southern hemispheres, and on the eastern and western shores of oceans, but this has been hard to prove.  Nevertheless, research suggests that California Velella have a sail which is angled to the right of the main axis. This means that as the wind pushes it along, Velella tacks to the right of the northwesterly prevailing wind and so these animals are usually kept offshore. Occasionally winds come from the southwest so that populations get blown ashore, as in the recent cases in the US.  Similarly, there have been years when large numbers were blown onto the southern coast of the UK, particularly following strong southwesterly winds blowing off the Atlantic. The fact is that every spring and summer, millions of these strange creatures are blown ashore on the west coast of America. But this year, the numbers have been much greater and the strandings even more impressive.  One reason for this is that storms in the eastern Pacific are likely to have blown the Velella on to the beaches. California’s beaches recently saw their largest swells since 1997, as surfers rode monster waves caused by tropical storms hundreds of miles out to sea.  Warmer waters associated with a build up to an El Niño year could have stimulated greater production of new baby Velella out in the mid ocean. Jellyfish and their relatives are all very flexible and are able to rapidly take advantage of favourable conditions. They are relatively short-lived, less than a year, can grow and reproduce very quickly, and produce large numbers of offspring. Therefore when conditions are ripe – waters are warm and food is plentiful – their numbers can suddenly erupt.  The occurrence of “good years” and “bad years” is common across all jelly-like creatures. This year in the UK, for instance, there have been reports of large numbers of Barrel jellyfish sightings along the southwest coast. While this is not unique, certainly these sorts of numbers had not been recorded in that part of the English Channel for a considerable number of years. Like Velella velella, barrel jellyfish mainly live offshore, and it is thought that the very warm spring and early summer coupled with altered water currents enabled large numbers to move in closer to land.  In the Bering and North Seas, where scientists have recorded jellyfish numbers over time, we know that fluctuations have been caused by changing sea temperatures, food availability and long-term climate cycles. At a global scale, analysis carried out by researchers in the Global Jellyfish Group has also revealed large-scale oscillations in the presence of jellyfish and jellyfish-like creatures over the decades. It’s a boom and bust existence. Many are worried that these “jellyfish” blooms are likely to become more common as a result of human-induced climate change, and there may be some truth in this. Huge blooms of giant jellyfish in Japan, or the mauve stinger in the Mediterranean, have indeed become more frequent in recent years, harming tourism, fisheries and aquaculture, and power plants (jellyfish have a habit of clogging up nuclear reactor cooling pipes). But this is not a universal trend; jellyfish aren’t about to take over the world, and neither are their sailor cousins."
nan
"Almost half of the world’s sandy beaches will have retreated significantly by the end of the century as a result of climate-driven coastal flooding and human interference, according to new research. The sand erosion will endanger wildlife and could inflict a heavy toll on coastal settlements that will no longer have buffer zones to protect them from rising sea levels and storm surges. In addition, measures by governments to mitigate against the damage are predicted to become increasingly expensive and in some cases unsustainable. In 30 years, erosion will have destroyed 36,097km (22,430 miles) or 13.6% of sandy coastlines identified from satellite images by scientists for the Joint Research Centre (JRC) of the European commission. They predict the situation will worsen in the second half of the century, washing away a further 95,061km or 25.7% of Earth’s beaches. These estimates are far from the most catastrophic; they rely on an optimistic forecast of international action to fight climate breakdown, a scenario known as RCP4.5. In this scenario of reduced ice-cap melting and lower thermal expansion of water, oceans will only have risen by 50cm by 2100. However, if the world continues to emit carbon at its current rate, sea levels will rise by an estimated 80cm, according to the Intergovernmental Panel on Climate Change. If this happens, a total of 131,745km of beaches, or 13% of the planet’s ice-free coastline, will go under water. Around the globe, the average shoreline retreat will be 86.4 metres in the RCP4.5 scenario or 128.1 metres in the high-carbon scenario, though amounts will vary significantly between locations. Flatter or wilder coastlines will be more affected than those where waterfronts are steeper, or those artificially maintained as part of coastal development. In the best-case scenario, the UK will lose 1,531km or 27.7% of its sandy coast, and 2,415km (43.7%) in the worst case. Australia (14,849km lost) and Canada (14,425km) are predicted to be the worst-affected countries, followed by Chile (6,659km), Mexico (5,488km), China (5,440km) and the US (5,530km). The Gambia and Guinea-Bissau have short coastlines, but both are predicted to lose more than 60% of theirs. The study predicts that the hardest-hit areas in the UK will be west Dorset, north Devon, Great Yarmouth, Barrow-in-Furness and north-east Lincolnshire. In these areas, beach retreat is predicted to be five times the national average. “The length of threatened seashores incorporates locations that will be submerged by more than 100 metres, assuming there are no physical limits to potential retreat,” said Michalis Vousdoukas, an oceanographer at the JRC and lead author of the study, published in the journal Nature Climate Change. “Our 100-metre threshold is conservative since most beaches’ width is below 50 metres, especially near human settlements and in small islands, such as the Caribbean and the Mediterranean.” Large beaches will narrow by 100-200 metres on Atlantic and Pacific coasts and the Australian side of the Indian Ocean, wiping out more than 60% of sand deposits in a number of developing countries that are economically fragile and heavily dependent on coastal tourism. But swift action to limit emissions and fight climate breakdown could help reduce the impact, experts say. “Moderate emissions mitigation could prevent 17% of the shoreline retreat in 2050 and 40% in 2100, thus preserving on average 42 metres of sand between land and sea,” Vousdoukas said. The researchers projected the future anthropogenic and geological changes based on 30 years of observations. Sea-level rise is exacerbating problems caused by construction and barriers on the shoreline such as buildings, roads or dams, which have changed the natural replenishment cycle of sandy beaches. “In the UK, part of manmade erosion results from protecting cliffs whose wearing would normally top up the associated beaches with gravel,” said Robert Nicholls, the director of the Tyndall Centre at the University of East Anglia in Norwich. “This happens, for example, in Bournemouth, to safeguard luxury properties built on top of fancy viewpoints.” In some regions such as the Baltic, marine erosion is compensated by land rise. Sediments may also be brought by rivers, either naturally as in the Amazon, or resulting from artificial activities as in the Chinese deltas that accumulate residues from industrial sites upstream. A third driver of erosion is the intensification of storms, which is associated with climate breakdown. These look on course to further erode the most vulnerable beaches; the study predicts that the British seashores facing most erosion are the east and west coasts, which are more exposed to tidal surges than the south. By the end of the century up to 63% of low-lying coastal regions worldwide will be threatened. In these areas, both population density and development tend to be higher than inland. “Seaward human expansion will continue, mostly in unspoiled coastlines that are particularly extensive in Asia and Africa,” Vousdoukas said. “Adaptive measures are urgently needed.” In many places, the cost of protecting the shoreline often outweighs the benefits. For example, in 2017 a £62m sea wall was built to protect the tourist resort of Blackpool. Besides requiring indefinite spending on maintenance, such concrete defences are seen as more of a problem than a solution as they can disrupt the process by which sand is deposited by ocean currents, exacerbating erosion. In some places the Environmental Agency has chosen to replenish beaches with sand dredged offshore. Not only is this harmful to marine habitats, mining sand from the seabed is expensive. Since 1994 millions of pounds have been spent yearly to replenish the 20km seafront between Skegness and Mablethorpe in Lincolnshire, also helping to preserve 35,000 hectares of farmland. Dr Sally Brown, deputy head of life and environmental sciences at Bournemouth University, said: “Building defences helps maintain coastline position, but defences are known to reduce beach width or depth over multiple decades. Responding to sea-level rise means looking strategically at how and where we defend coasts today, which may mean protecting only limited parts of the coast. “Beach nourishment schemes can help the problem, such as in Bournemouth, but these beaches need a regular top-up. Ultimately, we cannot nourish everywhere for ever, meaning that hard decisions need to be made about how much to spend and how to manage the coast in decades to come. This could affect those living on the coast, and tourists who enjoy the sandy beaches too. Sea-level rise will only make this situation worse.” • An interactive map showing beaches that will lose or gain ground can be found here."
nan
"It is great news that the National Trust has bought Hambledon Hill, a Iron Age hill fort in southwest England, for the nation. Now the expertise of its in-house archaeologists and conservators can be used to preserve this enormously important site.  Why is this hill north-west of Blandford in Dorset so important? It is a site that has been inhabited throughout many periods of early British history, from the early Neolithic (Stone Age), through the Bronze Age, Iron Age, and into the contemporary (by comparison) record of the Romano-British and later Anglo-Saxons. After farming arrived in Dorset and Somerset around 4,200BC, an agricultural society developed in the vicinity of Hambledon Hill over five centuries before the immense undertaking of building the enclosures began that would transform the hill into a fort. The complex was inhabited for 400 years, determined by radiocarbon dating of 160 carefully selected samples. The earliest stage of construction was the main enclosure, dated to around 3,680-3,630BC. A sequence of four periods of building activity followed, with subsequent extensions to the east, south and west continuing through 3,350-3,310BC. The Hambledon Hill fort is a physically impressive, societally significant construction that initially faced east across the River Iwerne onto Cranborne Chase and provided defence from that direction. After a period during which the fort appears to have been attacked on at least two occasions, with young men killed by arrowshot, outworks were built to the west that reversed the role of the site. The hill changed from providing defence to the east, and impressing those living there, to asserting power towards the west and the Vale of Blackmore. This may be the first glimpse of shifting power politics in early Britain, more than 5,000 years ago. At every stage of the detailed investigations of Hambledon Hill, the conclusion has been that while the people had come from farming settlements, Hambledon stood on marginal land, still forested, and certainly not intensively farmed. This was the natural habitat of red and roe deer, marten and badger, whose remains are found on the site. This was border country between populations, relatively remote from settlement. The evidence suggests periodic, probably seasonal, visitation by large groups of people. They brought with them cattle, pigs and sheep for slaughter and feasting, and ready-processed grain and other foods. Judged by the imported goods, these visitors came from the north and west, from the river Severn and as far as Devon if not further. Prestigious objects are largely found in the main ritual enclosure, where pits occur with rich deposits. The ditches of this enclosure and its long barrow were repeatedly recut to deposit food debris and objects. Human skeletons occur frequently, sometimes with traces of exposure and defleshing, and skulls are also frequently found. Recurring patterns are everywhere – an extreme example being the burial of two children, each with a possibly inherited skull deformity, in the same segment of ditch but several generations apart. Once the site was deserted, the Beaker people that came around 1,000 years later recognised and marked the then defunct enclosures. Early Bronze Age fields were laid out, and a Middle Bronze Age settlement was built atop one of the Neolithic enclosures. Later Bronze Age burnt mound activity took place in the Iwerne valley. Eventually, sometime in the middle of the first millennium BC, there was again a need to create an easily defensible fortified habitable centre that could accommodate a large number of people, if only for psychological reasons. Such a fortified centre was built on the northern point of the spur of Hambledon Hill around 600-700BC.  This fortification was apparently succeeded by the main Iron Age phase of the occupation and fortification of Hambledon Hill’s north spur when a defensive ring of multiple ramparts (multivallate) was built on the hill. It remains massively impressive to this day, with highly skilled engineering required to achieve this daunting effect. Some time around 600BC hundreds of relatively tiny round houses were built, seven to nine metres in diameter, originally probably with conical thatched roofs, on carefully dug platforms installed throughout the encloure, until the entire interior was filled with perhaps four or five hundred.  Houses so closely organised and spaced can only have been occupied for a short time. Was this another seasonal meeting place, as it has been suggested the hill was in Neolithic times, 3,000 years earlier? Designed to impress those in the countryside around, it may have heralded the end of a period of relatively untroubled individual farming and the return of more fraught, difficult circumstances.  This fortress-cum-ceremonial centre may have lasted some time before a less remote and more accessible centre was required, probably built on Hod Hill immediately to the south. Later still a Romano-British field system covered the hill, and an Anglo-Saxon cemetery was set by the parish boundary on the spur towards Stepleton Spur. Oliver Cromwell’s roundheads fought a skirmish here in 1645 against local villagers calling themselves the Clubmen who had banded together against the looting by troops of both sides of the Civil War. And a century later, Colonel (later General) Wolfe exercised his troops there before going onto scale the Heights of Abraham in the Battle of Quebec. Hambledon Hill is a beautiful part of the Dorset landscape, rich in plants, flowers and wildlife. But it is also a precious looking glass into 5,000 years of British history – hopefully now preserved in perpetuity for the nation."
"Air pollution is recognised as a major threat to human health worldwide. Nine out of ten people breathe polluted air, resulting in 7m premature deaths a year.  While air pollution respects no boundaries, and affects almost all of us, it impacts some populations more than others. Deaths attributed to air pollution are ten times more likely in low and middle income countries compared to high income countries. Sources of outdoor air pollution include industry, traffic and agriculture. Sources of indoor air pollution are mostly cooking and heating using solid fuels (including wood and charcoal). Many people living in urban informal settlements (or slums) are exposed to high levels of indoor and outdoor air pollution. Despite efforts to tackle exposure levels, reductions in air pollution have not been observed. Life in an informal settlement is not easy and there are many daily challenges, of which air pollution is just one. If the choice is between using dirty fuel or not feeding your kids, then is there a choice? Current approaches to reducing exposure to air pollution in informal settlements include  awareness raising and campaigns on how to reduce exposure. But these methods have very little input from the people they target. As a result, they may have a low rate of acceptance. Campaigns also generally focus on one source of air pollution, but effective solutions and improvements to health need to take into account all sources of exposure. And so community-centred approaches are needed to ensure an understanding of the local context and to explore concerns and challenges faced by residents. This will ensure that solutions are culturally relevant, inclusive and therefore more likely to be effective. This is what we have been doing in Mukuru, which is an informal settlement in Nairobi, Kenya. More than 100,000 families live in crowded conditions with limited access to basic services. Exposure to air pollution can lead to respiratory infection, chronic lung disease, heart disease stroke and lung cancer. In Mukuru, exposure is continuous due to burning of rubbish and industrial emissions. The immediate effects reported by residents include burning eyes, sore nasal passages, coughing and asthma attacks.  Along with a series of interdisciplinary colleagues, we set up the AIR Network so that residents of Mukuru could work together with African and European researchers to explore how best to raise awareness and begin to develop solutions to tackle local air pollution issues. Our creative methods and the involvement of the community allowed us to recognise a series of sources of pollution that we might not have otherwise. To minimise “Western” and “academic” preconceptions, which can result in a blinkered view, and to maximise engagement, trust and participation, our network used a variety of creative methods. These included theatre, storytelling, photography and drawing. We were determined from the start to create a democratic and participatory research project so that we could begin to understand the challenges that informal settlement dwellers encounter day to day, with the community deeply involved from the start. We began with a week-long workshop in Mukuru. For many of us, the creative approaches used were novel and we became a collective, learning together – as well as laughing, eating, sharing and building trust. Barriers were broken down not just between community and researcher, but also between researchers from different disciplines.  This is a community that is marginalised, with very few rights or regulations in place to protect them and limited access to basic resources. It is also a youthful community that is hugely self-motivated, bursting with talent, energy and activism. It is key that the voices of communities such as this are heard. The community educated us on which of the creative methods would work well in Mukuru, and for the next six months, we worked on putting our plans into action.  Our team included talented film makers, and we used digital storytelling to document personal experiences of air pollution. Here, for example, Dennis Waweru  talks about the impact of air pollution on the health of his community. Artists from the Mukuru-based Wajuuku Arts Centre painted maps on canvas and took these out into the community so that local residents could use them to identify pollution hotspots and pollution sources. Music was also highlighted as an effective and important communication tool. Local musicians and rappers composed songs to raise awareness about air pollution and the AIR Network itself.  We also used forum theatre (also known as theatre of the oppressed) to develop short plays about key air pollution problems in Mukuru, and then invite local people to become actors and explore potential solutions to the problems presented on stage.  These forum theatre plays were subsequently developed into legislative theatre pieces, which were performed to people in positions of influence or power. Audience members were then also invited to take part in playing out solutions to key air pollution issues, allowing a dialogue to develop between the “ordinary person” and the policy maker, shifting the usual direction of flow and breaking down existing hierarchies. Industry, burning of waste and bad drainage were identified as key sources of air pollution in Mukuru. It turns out that dangerous unregulated working conditions and lack of protective clothing are a major cause of exposure. As is a lack of infrastructure for firefighting, waste disposal (the smoke and smell of burning plastic is constant) and sanitation (sewage was identified by residents as a major source of air pollution). If we had gone into the community with aims and ambitions that had already been decided according to the commonly acknowledged causes of air pollution (traffic, industry, cooking methods) we may not have had space to reveal or acknowledge these other sources. Instead, we identified issues that the community recognises as indirect causes of air pollution, such as workers rights, alleyways between dwellings that are too narrow for fire-fighting equipment, and poor waste management. In September 2018, these activities culminated in an arts festival, Hood2Hood, at the local football ground. A stage and a sound system appeared out of nowhere. Forum theatre and storytelling pieces were performed. Rappers, MCs and dance groups played live. A mural was created. Visual and interactive games were used to collect data. Around 1,500 local people attended the festival during the course of the day, to find out what we had been doing and to make their own contributions to discussions around air pollution.  Wickedly complex global problems such as air pollution, climate change and antimicrobial resistance can only be properly addressed by using multidisciplinary approaches, real world actionable strategies and buy in from the public. Using creativity is key: it allows non-experts to participate more fully in this process so that initiatives and interventions will be culturally relevant and more effective."
"Many species of flower-visiting insect are in trouble in Britain, according to a new report from the Centre for Ecology and Hydrology (CEH) near Oxford, which drew on almost 750,000 observations of insects between 1980 and 2013. The study used population records of 353 wild bee and hoverfly species over large areas of Great Britain to show that one third of these pollinating species declined in range during this time. Most of these losses were in species that were already relatively rare. Some big losers were the red-shanked carder bee, the smooth-gastered furrow bee and the large shaggy bee, all of which had vanished from around half of their previous locations in 1980.  However, the same report also found that other species of bee and hoverfly, about 10% of the total, actually increased. Some of these, like the ashy mining bee and the lobe-spurred furrow bee, are pollinators of field crops like oilseed rape. These two species increased their ranges five-fold during the same period, suggesting that crop-specialist species are thriving at the expense of most others.  The other winners were actually invaders. The ivy bee – most often seen on the plant of the same name – only colonised mainland Britain in 2001 and the range over which it can be found has been expanding by 16% every year since. Despite what may appear to be a mixed bag, the overall diversity of British pollinator species has fallen steadily since 1980. The new study underlines the already alarming downward trend in insect numbers seen in several other studies conducted in the UK, Germany and Central America. In February 2019, a report claimed that current rates of decline might lead to “the extinction of 40% of the world’s insect species over the next few decades”. This almost apocalyptic claim was rapidly taken up by the world’s press and attracted much attention. Even if that story was exaggerated, it is pretty clear that something is wrong in the state of nature. Massive losses of insects are so serious because insects are essential components in almost every ecosystem. It’s their job to eat plants which convert the energy of sunlight into biomass – the foundation of most terrestrial food webs. In turn, these herbivorous insects are eaten by carnivorous insects, which are themselves eventually eaten by larger insect-eating animals. If insects are in trouble, then so is everything else in that ecosystem as serious losses in insect biodiversity threaten all kinds of wildlife. 


      Read more:
      What happens to the natural world if all the insects disappear?


 Wild bee and hoverfly species are globally important in fertilising flowering plants by transferring pollen between them, causing them to set seed. Without them, seed production in many wild flowers is reduced and plant populations decline. With fewer flowers to visit and less nectar and pollen to gather, pollinator numbers decline even further in a vicious cycle. It’s not only wild plants that are affected, but also agricultural crops. Strawberries, apples and oilseed rape are just three of many crops that benefit from pollination by bees and other insects. Production of seeds for planting is also dependent on insects. Without “ecological services” like insect pollination, some of these crops could no longer be grown. The annual value of insect pollinators for the UK alone has been estimated at £603m. Globally, pollination adds US$153 billion to the economy each year. The new report notes that among those pollinating insects whose ranges have expanded, species associated with field crops are well represented. This could be because measures have been taken to encourage them such as planting wildflowers, which provide pollen when crops are not in flower. Alternatively, it may simply be that some species are more tolerant of the progressive intensification of farming practices than others. Superficially, the increases in crop pollinators seem encouraging, but it may not be good news. Loss of pollinator diversity decreases crops yields, and this may be more important than insect numbers. Decreased pollinator diversity may leave insect populations more vulnerable to viral diseases that spread readily among social insects. Such viruses interact with widely-used pesticides and are known to adversely affect both honey bees and bumblebees. Broadly speaking, biodiversity losses in farmland habitats are likely due to increases in the efficiency of farming. Farmers seek to grow the greatest crop yields on the area of land that’s available to them. This ensures that agriculture captures more and more of the sun’s energy, converting it into human food.  As farming efficiency increases, less space and fewer resources are left for anything other than human food crops. The recent study shows that a few crop-specialist pollinators have increased while the majority have not, which shows that fewer plants and animals are thriving in ecosystems which are increasingly dominated by agriculture. There’s a trade-off between wild nature and farming efficiency and it seems that we have to decide how much wild nature we want."
"**A pub which ""flouted"" Covid-19 rules by allowing people in to drink during lockdown and serving alcohol without food has been closed by a council.**
Burnley's The Angel Inn had been the subject of the ""highest number"" of coronavirus-related reports in the local area, Lancashire Police told Burnley Council's licensing committee.
The force said in one raid, officers found three drinkers hiding upstairs.
The committee suspended the pub's licence ahead of a 4 December hearing.
In a statement to the committee, PC Michael Jones said a colleague had spotted people drinking illegally at the pub on Accrington Road after investigating lights and music coming from it at about 21:15 GMT on 13 November.
He said those inside had tried to escape, but officers sealed the exits and, after being let in by a woman who said she was drinking alone, ""found three people hiding in an upstairs room"".
All four had been fined for breaching Covid-19 regulations, he said.
PC Jones also stated that during two previous inspections before the second lockdown, the pub was found to be ignoring coronavirus rules about the serving of alcohol.
He said on 17 October, the day Burnley moved from tier two to tier three restrictions, a PC found men sitting near the bar with ""four or five pints of beer lined up on a table [and] little or no evidence of food consumption"".
A week later, a second officer found about 30 people drinking in the pub and a single tray holding servings of pie and peas on a table, he added.
Tier three restrictions at the time banned pubs from serving alcohol without substantial meals.
Following the hearing, the pub's owners said they would be meeting with solicitors on Friday to discuss the situation.
_Why not follow BBC North West on_Facebook _,_Twitter _and_Instagram _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
"**Some parents could not wait to get their children back to school after the first coronavirus lockdown.**
But Louise's two children remain at home - initially because she was shielding, but increasingly because ""they are flourishing"".
They are among 806 children removed from school registers by parents in Wales between March and September this year - up almost 50% since last year.
The Welsh Government said it had given councils Â£400,000 for home-schooling.
But some parents teaching their children at home have called for more support.
Figures obtained by BBC Wales Live show 552 children were ""deregistered"" from school between March and September in 2019. In 2018, the figure was 466.
The actual number will be higher as six local authorities have not given their figures.
Local authorities said children were being deregistered for a number of reasons, including anxiety, coronavirus and lifestyle choice.
Louise, 38, from Abergavenny, said her daughter Orlena struggled with learning at home during the first lockdown, ""but when it came to the children going back to school I was afraid and anxious"".
""It was the unknown with Covid - it just filled both me and them with uncertainty.""
Louise suffers with an autoimmune disease which means she would be likely to be ""very sick"" if she caught coronavirus, which made her children ""extremely concerned about bringing it home"".
""So at the end of August we just decided they weren't going to go back for now,"" she explained.
Armed with online resources loosely based around the suggested curriculum for their age, Louise deregistered her children in September.
She has been surprised by how readily Orlena, 12, and nine-year-old Roy have adapted to their new style of education.
""We were just plodding along with school - I was aware Orlena was slow in some things and that Roy was a bit rebellious,"" Louise said.
""Home-educating has helped me see their weaknesses and work on them.
""I ask them how they feel about going back to school and both of them say they don't get as much one-on-one with teachers as they have 30 children to deal with.
""They are flourishing now with someone having the time to sit with them and do things. I do learning through play, and if something doesn't tickle their mind I don't force them to do it.""
She said she now sees home education as a potentially long-term arrangement for the children, with the possibility of them studying for their GCSEs at home.
""If the children want to go back once it is safe for them to do so, they can. But I have decided to leave that up to them,"" Louise added.
""We might look at flexi-schooling, where they go to school for part of the week, but spend a couple of days being home educated too.""
Although she is sometimes unable to teach them because of her illness, Louise said help from friends, her children's desire to learn and online resources makes home education possible.
She said: ""There hasn't been any support, the only support has been through other home-educating parents online.""
Monmouthshire council said it worked with home-schooling parents ""in line with Welsh Government guidance"", and that no parents had been in touch to express disappointment with the level of support.
Freedom of Information requests by Wales Live to local authorities found there were a total of 2,250 children currently being home educated in Wales. In 2019 there were 2,171 while in 2018 that figure was 1,878.
The number may be higher as it is not compulsory for parents to register their children as being home schooled if they have never attended school.
Lockdown has been like an ""extended free trial"" for home schooling and the ""biggest single boost in home education ever"", according to Alastair Lawson, from education resource website Twinkl.
But home schooling has been more challenging for other parents.
Polly, from Ceredigion, started home schooling her 12-year-old daughter Meg in September to shield her husband - who previously had a kidney transplant - from the virus.
""I can't describe how hard I find it having to be teacher, mother, best friend,"" said Polly.
""I've had no official support. Ceredigion council were very supportive when we chose to withdraw her from education - they didn't oppose it and we didn't have any fines and I think we had one letter with a link to online resources.""
Ceredigion council said it was ""not in a position to direct what learning takes place"" at home, but it was ""eager"" to help families find the right learning resources.
She said the experience has made them a lot closer as a mother and daughter, but added: ""Some sort of mentoring for me would really help. Some sort of guidelines.""
Plans to set up a compulsory register of children being home schooled were shelved by the Welsh Government in June due to the pandemic.
As a result, Children's Commissioner Sally Holland has launched a review into regulations surrounding home schooling to see if it should be better regulated.
A Welsh Government spokeswoman said it was updating a leaflet for home-educating parents concerned about coronavirus, and they needed to be aware of the implications of home-schooling.
She said: ""We're aware that some parents may have made the decision to remove their children from school due to Covid-related fears, however, we encourage schools and local authorities to work together with families through a supportive approach to enable a return to school during these challenging times.""
The spokeswoman added the Welsh Government had allocated Â£400,000 to local authorities this year to support home-schooling families, which is ""unique to Wales"".
Watch Wales Live on BBC One Wales at 22.35 GMT on Wednesday, and afterwards on BBC iPlayer."
"

The editors of the _New Republic_ say we have a “moral responsibility” to invade Burma in order to distribute disaster relief. The editors observe that no one taken seriously is seriously advocating doing this and lament: 



This is, put simply, an unacceptable abdication of our moral responsibilities. Even though our standing in the world has been severely diminished by Iraq, we should at least be debating intervention in Burma. There are, no doubt, many logistical complications and unintended consequences that would follow from such a policy. But there are also reasons why it should be a live option. The goal of such an intervention need not be regime change; it should simply be to make sure that a vulnerable population receives the supplies it desperately needs. Of course, if violating the sovereignty of a murderous regime happens to undermine that regime’s legitimacy, then that would not be such a terrible result. But this does not necessarily have to be our goal.



One should not, I suppose, be too surprised that this sort of slipshod advocacy still emanates from the epicenter of liberal imperialism, a publication that was as influential as any in urging the Iraq war on the American people. (Neither should the fact that its leadership attempted to make their non‐​apology apology for Iraq look magnanimous.) The piece’s curtsy at post‐​Iraq reality is even sort of endearing, in a child‐​like way.   
  
  
Note also the focus not on the particular policy of invading and taking responsibility for disaster relief in Burma, but rather on the importance of “debating” such a policy. After all, the _New Republic’s_ writers aren’t going to be the ones to invade the country and deliver the aid. Rather, the _important_ question is whether the political climate will allow for TNR’s writers to churn out tough‐​minded and uncompromising articles that allow them to stretch their rhetorical legs yet still keep them within the beloved Broderian mainstream of American politics.   
  
  
But maybe the most disappointing point of that paragraph is that instead of the rote “to be sure” formulation, the editors chose to dodge completely the substance of the policy they’re advocating for by using the more indirect “there are, no doubt, many logistical complications…” phrasing. Write what you know, guys.
"
"

The United Nations will throw its biggest environmental party in 10 years later this month in Johannesburg. In preparation, the U.N. has rushed to publication a preliminary report about a new environmental pestilence, the so‐​called Asian Brown Cloud (ABC). The U.N. says the Brown Cloud will kill millions and wreck the Asian Monsoon, which is responsible for feeding about 2 billion people in one way or another. But like many U.N. environmental reports this one fails to mention some crucial points.



Nightmarish reports like the ABC have a way of appearing right before big U.N. environmental conferences — and being proven wrong not long thereafter. In 1995, a Geneva meeting, which gave rise to the infamous Kyoto Protocol on global warming, was prefaced with a breathless pronouncement that we now had climate models that matched the real atmosphere, lending credibility to gloom‐​and‐​doom forecasts of climate change. Months later, Nature magazine was compelled to publish a paper showing that the data which the U.N. cited was incomplete, and when all the numbers were put in, the correspondence vanished.



The U.N.‘s most recent world environment confab occurred last fall in Marrakech. Days before that one, we learned that the poor islanders of Tuvalu were being drowned by sea level rises caused by global warming. Within days, an article appeared in Science magazine showing that sea level around Tuvalu has been falling, not rising, for most of the last 50 years.



Lest anyone think the U.N. has learned anything about its environmental misrepresentations, let’s examine the Brown Cloud story.



Summarizing the U.N.‘s report, CNN said that the ABC is so awful that it has “scientists warning that it could kill millions of people in the area, and pose a global threat.” Further, the cloud “could cut rainfall over northwest Pakistan, Afghanistan, western China and west Central Asia by up to 40%.” 



Sleazy air exiting Asia is nothing new to climatologists. Reid Bryson, the eminent scientist who many believe is the progenitor of the modern notion of human‐​induced climate change, wrote about it in the 1950s. Since then, climate scientists have searched and searched through Indian monsoon data to try to find any systematic changes, and there have been none. 



Don’t take my word for it. Look at page 144 of the 2001 compendium on climate change published by the selfsame United Nations, and you won’t find any systematic changes in South Asian rainfall.



The U.N.‘s pre‐​Johannesburg hype prompted CNN to write that the ABC “has led to some erratic weather, including flooding in Bangladesh, Nepal, and northeastern India, [and] drought in Pakistan, and northwestern India.” The fact is that there isn’t a single shred of scientific evidence to back up those claims. In fact, in its 2001 report, the U.N. noted that there’s no evidence for any systematic changes in extreme weather around the planet. 



What’s really killing people in Bangladesh and causing the ABC is poverty. The place is so low‐​lying and poor that a tropical storm, which would harm no one in America, kills 10,000 in the Ganges Delta.



Speaking of tropical storms, they feed on the heat of the surface of the ocean. The more it warms, the more energy can be directed to spin up their fearsome winds. But the ABC blocks out sunlight, reducing the amount of warming at the ocean surface. Everything else being equal, it would reduce the frequency or magnitude of tropical storms in Bangladesh.



When we get near these worldwide gatherings, there isn’t a piece of U.N. science that isn’t political. That’s because what these meetings are about is blaming the West (read: the United States) for environmental degradation, and holding us up for money. Poverty — not America — is the cause of the ABC. Poverty requires the use of cheap fuels, such as dung, and lousy, inefficient ways of combustion, such as cooking fires. And, more than any of my green friends do, poverty recycles: Families grow, which leads to more and more dung fires, and lousier and lousier air.



Rather than shaking down the United States, the U.N. would be better advised to encourage free market development — -which everyone knows is highly correlated with cleanliness — -and discourage its favorite form of political economy, socialism. The history of the Soviet Union, Eastern Europe, and present‐​day China show a clear correlation between Big Government socialism, pollution and poverty. In freer societies, there is less government, less poverty and less pollution.



It’s time for the U.N. to stop hyping pseudo‐​science in support of inefficient, dirty governments, and to get on with the future — -where free markets breed efficiency and environmental protection.
"
"
In case you missed it live, Christopher Monckton spent an entire hour on the Glenn Beck program today on the topic of global warming, skepticism, and the Copenhagen Treaty.

The video is now available.
Watch it below.
I think Lord Monckton did a splendid job.
To see the proposed Copenhagen Treaty, see this essay on the subject here.

Parts 1-7 of the hour long video are below. YouTube has time limits on clips, so it is broken up into parts 1-7.









			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92002122',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Spending time in nature is good for you. A person’s access to parks and green, open spaces is important for their health, as research from the NHS and the OPENSpace research group at Edinburgh and Heriot Watt universities shows. Spending time in parks lowers the risk of cardiovascular disease and asthma, helps address obesity and mitigates mental health issues. National parks are often considered the best places to escape to and enjoy the benefits of immersion in nature. Occupying whole landscapes in picturesque rural areas, they provide space for hiking, bird watching and mountain biking. Due to their size, they also perform critical environmental functions by providing a home to biodiversity and storing atmospheric carbon in vegetation.   But approximately 50% of the UK’s poorest people live over 15 miles from a national park and most people require transport to get to them. For the most disadvantaged people in Britain, who predominantly live in urban areas, these places can seem largely inaccessible.  Within low income communities, opportunities to explore national parks are hindered by inadequate transport options compared to communities of higher socioeconomic standing. The most affluent 20% of wards in the UK also have five times the amount of green space than the most deprived 10%. Promoting the value of these green spaces for health and well-being is therefore disingenuous without acknowledging that access isn’t equal. Local parks, meanwhile, are embedded within neighbourhoods and could ensure that immersion in nature isn’t just a luxury for the rich to enjoy. Typically starting at about two hectares in size and located within a ten-minute walk of residential areas, local parks provide everyday spaces where people can connect with nature. These are the places where kids play football and ride their bikes and where there is the opportunity for daily contact between people, nature and their communities, all of which is essential for social cohesion.  If people can’t use national parks, perhaps local parks can provide the health and social well-being benefits within the community. As a result of austerity, however, local governments and the environment sector, which are responsible for managing local parks, are underfunded. This affects the quality of these parks and means they are less attractive to their communities. Some local authorities are even considering the sale of their green spaces to limit the annual costs of maintaining them. National parks generate significant income from tourism. In many cases, this is their main economic support. Without the opportunity to exploit similar revenue streams or draw from alternative funding sources, issues of access and quality continue to hit local parks more heavily. “Destination parks”, such as Hyde Park in London or Heaton Park in Manchester, are exceptions – situated in large, urban areas, they enjoy similar opportunities to generate additional revenue as national parks, using events and tourism to generate income. Local parks within poorer communities are unable to attract commercial events with the same frequency or magnitude and are therefore more vulnerable to funding cuts.    All people, regardless of their wealth, should have access to attractive and functional green spaces. However, the UK government’s announced funding boost of £13m for local parks in February 2019 shows an ignorance of the scale of austerity felt by some parks managers. Many have experienced budget cuts of up to 90% since 2010. This means lower quality parks with staff redundancies, reduced maintenance and a falling number of council-run activities.   The falling quality of local parks will hit lower income residents hardest – restricting their interaction with nature and their opportunities to socialise and relax. These are also the people in society who already have the least access to personal transport, disposable income or affiliations with organisations such as the National Trust.   If the slashing of funding for local parks continues, there may be a corresponding fall in attachment to nature among poorer communities, as described by psychologist Richard Louv. Public desire to spend time in nature and concern for its welfare could be lost.  


      Read more:
      Parklife: Britain's beloved urban parks need a funding boost to save them


 Properly funding the environmental and health services provided by parks is essential to ensure liveable places for a majority of the British public. Local parks in particular deserve more attention as they are used more frequently by disadvantaged communities and have ongoing benefits for community cohesion.  While national parks offer ecosystem services and the opportunity to escape for a day among natural beauty, if we don’t acknowledge that access to all green spaces is as important as the benefits they can provide, we risk overlooking the inequality that holds many back from enjoying them."
"It takes an hour from the surface of the Indian Ocean, descending 3,000 metres in a submersible research pod, to reach the bizarre creatures that cluster around hydrothermal vents on the seabed. “You’re in a titanium sphere that is about two metres in diameter,” says evolutionary biologist Julia Sigwart, describing her voyage to Kairei hydrothermal vent field, east of Madagascar.  The vessel is equipped with robotic arms, probes and cameras – like a manned, underwater version of the Mars rover. In lieu of seats, there’s a padded floor. “So you’re hunched up together with the two pilots who are driving it and manipulating it,” she says. With not even a loo on board, it’s definitely on the bijou side for an eight-hour working day, but for Sigwart, director of the marine laboratory at Queen’s University, Belfast, the experience is worth it. “As you go down the light fades out rapidly. When you turn off the lights of the submersible you can see all of the bioluminescence of everything that’s alive in the water all around you – big and small. It’s like a beautiful starscape.” While much of the ocean floor looks like a ghost town to the naked eye, the concentrated patches of life around hydrothermal vents are as densely, if not as diversely, populated as coral reefs. The vents are where mineral-rich hot water, between 300C and 400C (572-752F), bursts out from below the Earth’s crust, swirling into the cold seawater like black smoke. “These smoking chimneys loom up at you, out of the blackness. They’re just incredible,” says Sigwart. One current evolutionary hypothesis is that the special conditions around deep-sea thermal vents sparked the beginnings of life on Earth.  But these rare and vital ecosystems are under serious threat from deep-sea mining for minerals such as zinc, used for car batteries and mobile phone circuit boards, say campaigners. You might expect that in open water, which does not belong to anyone, the seabed would be safe from commodification, but in 2019 Greenpeace reported that 30 floor-exploration licences had been granted worldwide by the International Seabed Authority (ISA), a UN body. Deep-sea mining is the process of retrieving mineral deposits from the deep sea – the area of the ocean below 200 metres, which is the largest and least explored environment on Earth, occupying 65% of the planet’s surface. Metals found there such as copper, nickel, aluminium, lithium, cobalt and mangen are increasingly needed to make batteries, smartphones and solar panels.   When will it happen?So far, 30 exploration licences have been granted by the International Seabed Authority (ISA), a UN body. In total, 1.5m km2 has been set aside for mineral exploration (equivalent to an area the size of Mongolia) in the Pacific and Indian Oceans as well as along the mid-Atlantic ridge. No exploitation contracts have yet been allocated but they are expected to be given out as early as this year when ISA’s Mining Code is expected to be approved. This will be a set of rules “to regulate prospecting, exploration and exploitation of marine minerals in the international seabed area”. Why is it a problem?Critics are concerned mining could do huge damage to the deep sea and the creatures and ecosystems that exist there. Underwater ecosystems like volcanic mountains, hydrothermal vents and deep-sea trenches are still poorly understood. Many endemic deep-sea species could be wiped out by the creation of a single large mine, while more mobile creatures will be indirectly affected by noise and light pollution.  What can be done? Comprehensive studies need to be carried out to assess the potential damage to biodiversity before deep-sea mining goes ahead, according to the International Union for Conservation of Nature (IUCN). Fundamentally, the IUCN also says people need to recycle and reuse products so there is less demand for extraction of natural resources. Researchers have created a list of priorities for deep-sea conservation. The survey, published in Nature Ecology & Evolution, included the responses of 112 scientists.  Phoebe Weston Mining companies in Germany, China, Korea, India and the UK are among the recipients. “They’re not supposed to be used for commercial-scale mining, but several of the licences have been renewed and they’re into a second 10-year term, says Sigwart, adding that the ISA is currently developing a regulatory framework for commercial mining in the high seas. The race is now on for Sigwart and other biologists to identify and learn more about the vent-dwelling creatures and lobby for their protection. Many are only found in these unique and isolated places. The vivid mottled orange snail, Gigantopelta aegis, has only been located in one area estimating 8km squared. Elin Thomas, Sigwart’s PhD student, has set to work assessing the vent species discovered so far against the criteria for the International Union for the Conservation of Nature’s red list. Because of its small, singular habitat and the threat of mining in the Indian Ocean, the Gigantopelta aegis is now classified as “critically endangered”. Another colourful character on the vent scene is Alviniconcha strummeri, named after the Clash’s Joe Strummer on account of its spiky shell resembling punk rockers. Its red list status is “vulnerable”. In total, 15 hydrothermal vent species – described fondly as “super weird” by Sigwart – have been added to the red rist. The mythical-looking sea pangolin, AKA the scaly-foot snail, was the first to be identified as at risk (status: “vulnerable”). Resplendent in armoured skirts that would be the envy of any Roman centurion, the layers of black flaps around its foot, along with its helmet-like black shell, are a result of the very mineral riches that are attracting the mining industry. “The iron that precipitates out of the vent fluid,” says Sigwart, “is incorporated into the shell and the scales of the scaly foot. It hasn’t grown an iron shell, but the available environmental iron on the surface has integrated into it.” The scaly-foot snail and Gigantopelta aegis are the most fascinating to Sigwart, because each has independently evolved a cunning way to bypass the whole kerfuffle of having to eat. All life around the vents depends on bacteria for energy. There are no plants, so the creatures have to either graze on slimy microbial mats, or eat each other. Rather than bother with any of that, however, these two evolutionary geniuses have an internal organ inside which microbes live, providing all their energy needs. It’s not all snails and germs down there, however. There are giant ghostly white crabs scuttling about, stalked barnacles, tube worms, shrimps and mussels. Different vent systems around the world have their own assemblages of animals. The first vents, discovered in the late 1970s, were in the east Pacific and are known for their metre-long lipstick worms. In Sigwart’s experience, people often assume these ecosystems are “out of harm’s way, nobody can reach them. It’s all fine. But it’s no longer fine, because now we’re on a path to developing commercial-scale deep-sea mining and vents are a target. We can no longer naively hope that the depths of the oceans are still pristine and untouched.” “More and more,” says Sigwart, “it’s clear that they are already impacted by human activities. We find plastic in deep-sea sediments, the ocean circulation patterns are being altered by global climate change.” Crucially, she says, the tide is turning when it comes to scientists becoming more vocal about the animals that would otherwise stay out of sight and out of mind. “Deep-sea biology is fascinating and exciting, and it inspires a sense of wonder in everybody,” says Sigwart. “There are very few of us that have the privilege of actually working on these animals and habitats. We have a burden of responsibility to try to explain them to other people before the damage is done.” Find more age of extinction coverage here, and follow biodiversity reporters Phoebe Weston and Patrick Greenfield on Twitter for all the latest news and features"
"When we look at the path to zero net emissions by 2050 two things stand out – firstly it is exactly in line with Labor’s old policy of a 45% cut by 2030, and secondly the government’s target of a 26% cut is woefully below what is needed. This week, Anthony Albanese announced that Labor will commit to achieving zero net emissions by 2050. And let’s state clearly – this is a damn good thing. You might want to argue that we need to get to zero net earlier, and that’s a worthy argument, but let us not get silly and suggest this target shows the ALP is somehow hostage to fossil fuel companies. This is a massive step for our economy. Last year Australia expelled 532m tonnes of greenhouse gasses. Aiming to get to zero net in 30 years is not something you can dismiss as a sign of a party doing nothing. The government is currently refusing to set any target. Instead, the prime minister is talking up his “technology over taxation” approach. There is a very easy way for the government to demonstrate it agrees with the science on climate change and is committed to reducing emissions – set a target to reduce emissions that agrees with the science. The IPCC made this pretty clear for governments in its “summary for policymakers” report released in October 2018. It stated that to limit global warming to 1.5C above pre-industrial levels, “human-caused emissions of carbon dioxide would need to fall by about 45% from 2010 levels by 2030, reaching ‘net zero’ around 2050”. We should pause here and note that within six months of this very clear statement, the Coalition went to the election with a target of a 26% cut from 2005 levels by 2030, while the ALP went with a target of a 45% cut. And of course the focus of the debate from most of the media was on the cost of the 45% cut rather than why the Coalition was ignoring the science. A damning moment for our media. Boris Johnson of all people has committed to zero net emissions by 2050, but our government will not, cowed as it is by people such as senator Matt Canavan who tweeted this week: “There is a simple reason that Labor won’t cost its net zero emissions policy because net zero emissions = net zero jobs!” As the Australian Institute’s Matt Grudnoff noted, “net zero jobs” actually means any jobs lost will be balanced by job gains, which is perhaps not what Canavan is trying to argue. This brings me to the second point – this is not an astonishing target. It is in effect just what the ALP was already planning. Getting to zero net emissions by 2050 goes along the same path as aiming for a 45% cut by 2030, as we can see in the graph. I say this not to discount the policy, but to highlight that given it is in line with what the ALP was already targeting and also is what a conservative government in the UK is planning, the big issue is not that the ALP has this target, but that the government does not. It is based on the science and it also will greatly benefit our economy. I have written in the past that getting to zero net zero emissions is going to be tough. If it was easy it would have been done already, so let us not lie and say it can be done with no pain. It will require a huge change in our economy. But the good news is it will hugely benefit our economy and nation. The CSIRO has estimated that if we continue on our current path, our GDP out to 2060 will grow by just 2.1% annually; but if we target zero net emissions by 2050, our economy will be so transformed and improved that our GDP will grow on average each year by 2.75% and real wages will be greatly improved. Doing nothing is not the answer – it is economic degradation. We must get to zero net emissions by at least 2050, and the ALP target – which will need strong interim targets as well – must be the baseline from which all climate-change policy debate occurs. • Greg Jericho writes on economics for Guardian Australia"
"The past may be a surprisingly useful guide for predicting responses to future climate change. This is especially important for places where extreme weather has been the norm for a long time, such as the Indian subcontinent. Being able to reliably predict summer monsoon rainfall is critical to plan for the devastating impact it can have on the 1.7 billion people who live in the region.  The onset of India’s summer monsoon is linked to heat differences between the warmer land and cooler ocean, which causes a shift in prevailing wind direction. Winds blow over the Indian Ocean, picking up moisture, which falls as rain over the subcontinent from June to September. The monsoon season can bring drought and food shortages or severe flooding, depending on how much rain falls and in what duration. Understanding how the monsoon responded to an abrupt climate transition in the past can therefore help scientists better understand its behaviour in the future. When we researched this weather system’s ancient past, we found it was highly sensitive to climate warming 130,000 years ago. Our new study published in Nature Geoscience showed that the Indian summer monsoon pulled heat and moisture into the northern hemisphere when Earth was entering a warmer climate around 130,000 years ago. This caused tropical wetlands to expand northwards – habitats that act as sources of methane, a greenhouse gas. This amplified global warming further and helped end the ice age. The rate at which today’s climate is changing is unprecedented in the geological record, but our study shows how sensitive the Indian summer monsoon was during a global transition into warming in the past and may still be. Over the last one million years, the climate fluctuated between a cold glacial – known as an ice age – and a warm interglacial as the Earth’s position relative to the sun wobbled in its orbit. The last transition from an ice age into the warm climate of the present interglacial – known as the Holocene – occurred around 18,000 years ago. This period of Earth’s history is relatively well understood, but how Earth system processes responded to these climate changes deeper in time is still something of a mystery. A recent expedition to drill deep into the ocean floor of the Bay of Bengal gave an opportunity to reconstruct past Indian monsoon behaviour over hundreds of years before the last ice age. Our study used these deep sea sediments from the northern Bay of Bengal to capture a direct signal of the Indian summer monsoon from 140,000 to 128,000 years ago, hidden in the fossilised shells of tiny microscopic creatures called foraminifera. These plankton species once lived in the upper ocean water column and captured the environmental conditions of the surrounding seawater in the chemical make up of their shells. We detected the ocean surface water freshening from river discharge induced by the rains of the Indian summer monsoon from 140,000 to 128,000 years ago – a sign of the strengthening monsoon system. This occurred when the Earth was coming out of a glacial state and into the interglacial which occurred before the one we live in, separated by a single ice age. During this period – which we’ll refer to as the penultimate deglaciation – sea levels rose from six to nine metres worldwide. Ice-core records show that Antarctica began to warm first during the penultimate deglaciation. Southern Hemisphere warming provided a source of heat and moisture which fuelled the strengthening of the Indian summer monsoon, as seen in our records of surface freshening and river runoff from the northern Bay of Bengal. During this warming period around 130,000 years ago, the Indian summer monsoon responded to southern hemisphere warming while the northern hemisphere and other monsoon systems, such as the East Asian summer monsoon – which affects modern day China, Japan and the Far East – remained in a glacial state. The Indian summer monsoon pulled heat and moisture northwards, driving glacial melting in the northern hemisphere and helping tropical wetlands expand their range. These expanding tropical wetlands resulted in more methane release into the atmosphere which caused even more warming, setting changes in motion which ended the global ice age. The Indian summer monsoon is an incredibly dynamic system. Though confined to the tropics, the system is sensitive to climatic conditions in both hemispheres. Due to its role in contributing to methane emissions, the Indian summer monsoon also has an outsize impact on the global climate. Monsoons should not be viewed in isolation, just as the polar ice sheets shouldn’t. Earth’s internal climate system is intrinsically linked and abrupt changes at one place can have significant consequences over time elsewhere. Climate change is inevitable. Our response to it isn’t. Click here to subscribe to our climate action newsletter."
"A group of forestry and climate scientists are calling for an immediate and permanent end to the logging of all native forests across Australia as part of a response to climate change and the country’s bushfire crisis. In an open letter, the group said forestry workers involved in logging in native forests should be redeployed to support the management of national parks.  A briefing document to back the letter, coordinated by The Australia Institute thinktank, argues logging in wet eucalypt forests promotes more flammable regrowth. Dr Jennifer Sanger, a forest ecologist who is in Canberra today to deliver the letter told Guardian Australia: “As we face this climate crisis, we see our forests are worth far more standing. “We have to start taking this climate emergency more seriously and protective native forests is a simple step we could take and in my mind, a logical call.” Some experts told Guardian Australia they disagreed, saying it could effectively rule out one potential response to managing forests in the face of climate change. Among the signatories to the letter are University of Tasmania’s distinguished conservation ecologist Prof Jamie Kirkpatrick, James Cook University ecologist Prof Bill Laurance, and Prof Tim Flannery, of the University of Melbourne. The letter says: “We write to ask you to respond to the climate, fire, drought and biodiversity loss crises with an immediate nationwide cessation of all native forest logging.” Large old-growth trees are important for capturing and storing carbon, the letter said, adding that native forest logging “is heavily subsidised by our taxes, which can be better spent on fire mitigation”. Government data shows that 5m hectares of native forests are open to logging and that annually, 73,000 hectares are harvested. According to the briefing document, 12% of logs harvested in Australia come from native forests, and an end to native forest logging would directly impact 3,250 workers.  “The best economic use for native forests would be to leave the forests intact and push for inclusion in a carbon trading scheme,” the document said. When wet eucalypt forests are cleared the regrowth and understorey is drier and more flammable, according to the document. Species that live in forests make up 80% of all Australia’s threatened species. Sanger added: “Native forest logging just isn’t beneficial. It is not profitable, and there are not a lot of jobs that rely on it. “Ecologically [forests] are under a lot of stress from other impacts, including climate change and habitat destruction, and it does not make sense to be logging these forests.” The call to ban native forest logging comes after the government announced a Royal Commission into the bushfire crisis that focuses on adapting to climate change, with measures including the use of hazard reduction to be investigated. Prof Rod Keenan, the University of Melbourne’s chair of forest and ecosystem science, told Guardian Australia he did not agree that all native forest logging should cease. “The letter proposes a simplistic solution to a complex problem. Current timber harvesting is not the problem,” he said. Native forest logging had declined over the last 20 years and was heavily regulated to protect habitats, he said, “so the environmental benefits of such a ban are unclear”. He argued a ban would have “significant social and economic impacts for local communities” that had already been hit hard by fires. He said: “The suggestion we can supply all our wood requirements from plantations is also incorrect. We have a large trade deficit in wood products, there is no immediate replacement for native timbers to industry and the plantation estate has taken a significant hit from recent fires. “The industry will need to adjust to recent fire impacts and adapt to a changing climate. New types of silviculture, including timber harvesting, can be part of the solution in reducing the impacts of future fires. “Rather than knee-jerk decisions, we need to keep all options on the table as we work through the best responses to these catastrophic fires.” Prof Peter Kanowski, an international forest governance expert at the Australian National University’s Fenner School of Environment and Society, said he also could not support a ban on native forest logging. “We need to protect populations of plants and animals post-fire, and we need to organise any timber harvesting cognisant of that,” he said. “But beyond that, we have to think differently about a much more adaptive and integrated approach to how we manage forested landscapes in the future under climate change.” He said that banning native forest logging would “be precluding options that we should not be precluding”. The Australian Forest Products Association declined to comment on the open letter."
"One reason why people find it difficult to think about climate change and the future may be their understanding of human history. The present day is believed to be the product of centuries of development. These developments have led to a globalised world of complex states, in which daily life for most people is highly urbanised, consumerist and competitive. By this account, humanity has triumphed over the dangers and uncertainties of the natural world, and this triumph will continue to unfold in the future. Anything else would seem to be going “backwards”, in a world where “backwardness” is pitied or despised. But it is now clear that we haven’t triumphed. The future has become very uncertain and our way of thinking needs to change. Could new historical narratives help? How might they look? The current view of the past, present and future as a trajectory of progress is constantly reiterated by politicians and taught to children in schools. It does not offer many alternatives to the ideas and practices driving climate change and ecological breakdown.  There is a reassuring promise in this narrative that things naturally improve with time, requiring no commitment from ordinary people. Progress is delivered through steady work by governments and scientists, with moments of transformation by activists or visionaries. The direction of history itself is towards the general good.  It is very hard, then, for anyone thinking in this framework to imagine a future in which societies adapt to the challenges of climate change. This is especially the case where adaptions might have to take the form of significantly reduced consumption, unfamiliar forms of social organisation, and harder work to produce food or manage local environments.  These ideas about the future look very different from the technologically advanced and globalised tomorrow that the progress narrative seemed to promise. At present, ideas in popular culture about the impact of climate change are often apocalyptic and dystopian. Ideas about mitigating climate change seem limited to fantasies of last-minute salvation by scientific genius or alien intervention. In this respect, climate change stands in contrast to other issues that are more rooted in a cultural understanding of history. Arguments around Britain’s departure from the European Union, for example, matter to people across the political spectrum because they’re integrated with ideas about the nation’s past trajectory, as well as the immediate concerns of people and communities.  Responding to climate change, meanwhile, demands a collective rupture from several centuries of development within a timescale of decades. This poses both a challenge and an opportunity to the study of history. 


      Read more:
      European colonisation of the Americas killed 10% of world population and caused global cooling


 Fields such as climate, environmental or global history help to think about the past in planetary rather than national terms. Some of that questions the western interpretation of history and the exploitation of people and nature which punctuates it. Recovering the stories of people marginalised from these narratives helps people think about life in a different light. Many indigenous peoples, for example, have ideas about the past that situate humans within complex ecosystems. Environmental historians also ask how past societies interacted with their surroundings and consider how and why more ecologically stable ways of living were destroyed through colonisation by powerful, expanding empires. Bruce Pascoe’s Dark Emu looks at the sustainable land management techniques of Australia’s First Peoples, which were ignored by British settlers. He suggests a way forward for Australian agriculture based on those practices. Their subject also explores how climatic and environmental change affected earlier civilisations. The fall of Rome, for example, fits into a global shift in climate conditions around 500 C.E. that also resulted in the “fall” of complex states in China, India, Mesoamerica, Peru, and Mexico.  Population health and biodiversity improved significantly in the following period, popularly known as the “Dark Ages”. So were powerful states always a good thing? The destruction of indigenous populations by Europeans from 1500 onwards may have caused huge environmental changes on the American continent. As 56 million lives were extinguished, the regrowth of forests on abandoned farms may have absorbed enough atmospheric carbon to cool the global climate in the Little Ice Age. Societies across the world suffered during this period. In Europe, it was a time of savage persecution of “witches”, partly due to the belief that they were deliberately causing the “unnatural” weather conditions.  The Dutch Republic did show resilience in the harsher climate conditions of “the frigid golden age”. Its innovations for harnessing the energy of changing weather and wind patterns in shipping fuelled an aggressive trading empire. While such strategies are not templates for future action, they do underline the fact that humans have and can adapt with radically altered lifestyles, expectations, aspirations and standards of living. They needn’t always aspire to more of the same that they have at present. This idea begs questions about the nature of history itself. Must history continue to be a story of humans alone? Could it become the study of humans in complex ecosystems, exploring the entangled pasts of people, animals, insects, microbes, plants, trees, forests, soils, oceans, glaciers, stones, volcanic eruptions, solar cycles and orbital variations? Narrating a richer past would lessen the shock of discovering that we are, after all, earthbound inhabitants of the only planet where life is known to exist. It could show us that our survival is dependent on countless complicated and delicate relationships. Relationships that “progress” narratives have required us to ignore, despise and even fear.  In recognising that the established view of human history can and must change, people can think radically about society, rather than following the present course out of a failure of imagination."
nan
"

The American pika ( _Ochotona princeps_ ) is an insanely cute critter often found in above-timberline rock fields in the western U.S.   






Because they often live near mountain peaks, there’s been concern that global warming could push them over the top, to extinction. Writing in the _Journal of Mammalogy_ , Smith and Nagy (2015) state that American pikas ( _Ochotona princeps_ ) “have been characterized as an indicator species for the effects of global warming on animal populations,” citing the works of Smith _et al_. (2004), Beever and Wilkening (2011) and Ray _et al_. (2012). Indeed, as they continue, “a consideration of the effects of climate, primarily recent warming trends due to climate change, has dominated much of the recent literature on American pikas and their persistence.” Hoping to provide some additional insight on the subject, the two Arizona State University researchers set out to investigate the resilience of a pika metapopulation residing near Bodie, California, USA, that was exposed to several decades of natural warming.   




The investigation, which Smith and Nagy characterized as “the longest study of any pika species,” focused on the Bodie metapopulation for two primary reasons. First, it is “situated at the warmest locality of any longitudinal study of the American pika.” As such, its area of habitat is comparatively warm and fully capable of inducing warm temperature stress. Second, the population has been well-studied, having been censused (for patch occupancy data) several times since the early 1970s. Given these two characteristics of the Bodie metapopulation (location and well-studied) the two researchers were able to test for a relationship between pika extinctions/recolonizations and chronic/acute temperature warming. So what did their analysis reveal?   
  
With respect to _chronic_ temperature warming, Smith and Nagy report that despite a relatively high rate of patch (islands of pika-suitable habitat) turnover across the study location, there was “a near balance” of pika patch extinctions and recolonizations during the past four decades of intense data collection (see figure below). Furthermore, a series of statistical analyses that were performed on the patch turnover and historic temperature data revealed there was “no evidence that warming temperatures have directly and negatively affected pika persistence at Bodie.” In fact, the only significant correlation they found among these two parameters occurred between mean maximum August temperature and the number of pika recolonizations the following year, which correlation was _positive_ , indicating that _higher_ August temperatures lead to a _greater_ rate of pika recolonization the next year, a result which the authors describe as “in the opposite direction of the expectation that climate stress inhibits recolonizations.”   






_Two decades of patch extinctions and recolonizations in a Bodie, California, American pika (Ochotona princeps) metapopulation. Source: Smith and Nagy (2015)._



With respect to _acute_ temperature warming, defined as the number of hot summer days exceeding a temperature threshold of 25°C or 28°C, Smith and Nagy write that “neither warm chronic nor acute temperatures increased the frequency of extinctions of populations on patches, and relatively cooler chronic or acute temperatures did not lead to an increase in the frequency of recolonization events.”   
  
Taken together, the above findings demonstrate that the Bodie metapopulation of American pikas is “resilient at the individual (Smith, 1974) and population scales” to both chronic and acute temperature warming, and has “been so for at least 60 years.” And, as an “indicator species” for the effects of global warming on animal populations, the future for American pikas and other animal species looks bright!   
  
**References**   
  
Beever, E.A. and Wilkening, J.L. 2011. Playing by new rules: altered climates are affecting some pikas dramatically -- and rapidly. _The Wildlife Professional_ **5** : 38-41.   
  
Ray, C., Beever, E. and Loarie, S. 2012. Retreat of the American pika: up the mountain or into the void? Pp. 245-270 in _Wildlife conservation in a changing climate_ (J.F. Brodie, E. Post, and D.F. Doak, eds.). University of Chicago Press, Chicago, Illinois.   
  
Smith, A.T. 1974. The distribution and dispersal of pikas: influences of behavior and climate. _Ecology_ **55** : 1368-1376.   
  
Smith, A.T., Li, W. and Hik, D. 2004. Pikas as harbingers of global warming. _Species_ **41** : 4-5.   
  
Smith, A.T. and Nagy, J.D. 2015. Population resilience in an American pika ( _Ochotona princeps_ ) metapopulation. _Journal of Mammalogy_ **96** : 394-404.


"
"**The hospitality industry in Northern Ireland could reopen before Christmas, the chief medical officer has said.**
However, Dr Michael McBride said it would only be possible with adherence to the current and upcoming restrictions.
Dr McBride called for people to stick to the regulations being introduced for two weeks from Friday 27 November.
He also said with promising vaccine developments, ""we are hopefully at the beginning of the end of this"".
Northern Ireland's top medical official was speaking on BBC News NI's Coronavirus Catch-up.
When asked about hospitality, a sector that has been impacted heavily by restrictions, Dr McBride said it was possible there could be some positive news around reopening.
""If we all work hard and we all make the most of the two weeks of wider restrictions, then I think that's a realistic possibility,"" he said.
""We cannot keep the hospitality sector closed indefinitely.
""It is just not sustainable for us to do so and we know the huge impact that's having, not just in terms of from an economic viewpoint, but actually in stress and anxiety levels of those working in the sector.""
He also said gyms were closing for two weeks because data had shown ""some sizeable outbreaks associated with gyms"" despite protective measures being in place.
However, he also said that there was hope on the horizon with the news of coronavirus vaccine developments.
He revealed extensive planning has already been undertaken for the eventual distribution of a vaccine, if one or more are approved.
Dr McBride confirmed that they would seek support from all sectors, including logistical help from the army, but that that would not entail ""boots on the ground"".
""We have peer vaccinators within our trusts, we have recruited a cadre of volunteers, those who wish to do the online training and become vaccinators,"" he said.
""We have retired doctors, dentists, nurses, a range of other individuals who have volunteered, last we looked we had 600 volunteers in the 72 hours since we out up the online web address for people to volunteer.
""Four hundred or so of those have already been accepted and have gone through the first phase of the training, so we have a very well co-ordinated and thoughtful planned approach to the rollout of this vaccine.""
The chief medical officer expects that some of those vaccine doses, if approved, may arrive and be administered before Christmas.
""We will not however, see the vaccine begin to have a significant impact in terms of reduced need for some of the restrictions until, I would have thought, the springtime,"" he said.
""It's only then we will have sufficient numbers of the vulnerable people isolated that we can begin then to ensure the pressures on our health service are reduced and also the risk to those people who are most at risk is reduced.
""I think we are hopefully at the beginning of the end of this, by the summer we will be in a very, very different place.
""But it is crucially dependent on that vaccine being approved, crucially dependent on the supply of the vaccine but most importantly of all, it's important that we all take up the offer of the vaccine when it's made available to us."""
"

You’ve got to hand it to Tony Blair. When polls showed 80 percent of the British citizenry against America’s military position, Blair stood fast with President Bush. And, as has happened here, his overall popularity (and support for his Iraq policy) has soared since hostilities began and the outrageous nature of Saddam’s regime and tactics became common knowledge.



But selfless political sacrifice is as foreign as chastity in Washington. After the dust settles, Blair wants Bush to drop his steadfast opposition to the Kyoto Protocol on global warming. 



The Kyoto Protocol is wildly popular in Britain largely because the country seems to lack scientists courageous enough to point out that the government’s alarmist view of climate change is without merit. That’s not the case here. And as everyone in the Bush administration knows, warming in the next 100 years, given a very small range of error, is likely to mirror what has happened in the last 40 years. Further, the administration knows that the Kyoto Protocol, while enormously expensive, would stop less than one tenth of a degree (C) of warming in the next half‐​century, an amount too small to be reliably measured.



Soon after Bush took office and National Security Agency head Condolezza Rice said “Kyoto is Dead,” the BBC reported that Blair was under considerable pressure to oppose Bush. In April 2001, Blair’s deputy prime minister, John Prescott, “want[ed] to end cooperation [with the United States] on global trade, national missile defence, and even British support for the U.S. stand against China.” Others in Blair’s cabinet agreed, including International Development Secretary Clare Short and Foreign Secretary Robin Cook. When he came to Washington three months later, Blair made plain his differences with Bush on the Protocol. 



Fast forward to the radically changed world after 9/11. Speaking before the U.N.‘s Earth Summit in Johannesburg in September 2002, the London Guardian reported that “Tony Blair launched into an unexpected broadside against George Bush on climate change,” and added that “what makes it more surprising is that his [Blair’s] aides appeared to be emphasizing the split with Washington.… In what aides said was a direct message to the White House, Mr. Blair said that Kyoto was not enough.” Going even further than the Europe’s radical greens, Blair said, “Kyoto is not radical enough.”



Blair shares more with the discredited Hans Blix than he does with George Bush on global warming. Last month, Blix said, “I’m more worried about global warming than I am of any major military conflict.” On February 25, just three weeks before the start of war, Environmental News Network reported that Blair “said world leaders must not let the crisis in Iraq and the fight against terrorism distract them from long term but equally important environmental problems.”



Blair said, “The only answer is to construct a common agenda that recognizes that both sets of issues have to be confronted for the world’s security and prosperity to be guaranteed.” Further, sounding more radical than Al Gore, he continued: “There will be no genuine security if the planet is ravaged by climate change. We will continue to make the case to the U.S. and to others that climate change is a serious threat that we must address together as an international community.”



It is doubtless that Blair has told Bush the price of military alliance in Iraq: Drop U.S. opposition to Kyoto. 



This won’t happen in a very public fashion. Instead, watch the legislation. The current Senate energy bill contains three provisions that come pretty close to enacting Kyoto. If the administration lets them slide through, the deal has been done. 



One creates a permanent Office of Climate Policy in the White House, which gives radical environmentalists direct access to the president. The legislation also requires a national strategy to cut carbon dioxide emissions, which is a complete surrender by the administration to the nonexistent science propping up a hypothesis of dramatic and disastrous warming. Finally, the bill creates an “early credit” for industries that cut emissions now. These “credits” only have value if some type of legal limit on emissions is imposed, so expect all these creditors to lobby for that limit. That is precisely what Enron pleaded for from the Clinton administration in a well‐​publicized letter from dethroned CEO Ken Lay. 



Bush I and Bush II are men of their word. In the first Gulf War, Bush I promised the Saudis that we would not dethrone Saddam Hussein as the price for usage of their airbases. He kept it, inadvertently creating today’s war. His son’s word is equally his bond, which will become evident if the White House rolls over on Kyoto in the next month. 
"
"**The aviation sector is ""on the brink"" according to the boss of a regional airport which has shut down temporarily due to a lack of demand.**
Cornwall Airport Newquay hopes to reopen in December ""for a rally of demand ahead for the Christmas period"".
Managing director Pete Downes said airlines had made it ""clear there is not sufficient demand to make it viable"".
Cornwall Chamber of Commerce said it was ""absolutely devastated"".
""If we are serious about Cornwall being a 21st century business destination then we have really got have an airport just as we have to have roads and digital infrastructure like 5G and superfast broadband,"" its chief executive Kim Conchie said.
The airfield at Cornwall Airport Newquay will remain available for emergency flights and staff have been furloughed.
The airport is subsidised by Cornwall Council, who agreed in September that Â£5.6m could be diverted from its Â£12m funding for the Cornwall spaceport, should no other support be secured from the government.
Mr Downes said it was a ""turning point for the industry"" and called on the government to offer greater support to regional airports.
""The government has to decide how much of this industry it wants to have left at the end of this period,"" he said.
""Everybody has been surviving with drastically reduced revenues and continuing to run airports for the benefit of regional connectivity around the UK, with no sector specific support from the government.
""People have been holding on but unless some support is forthcoming over the winter we will start to lose parts of this industry. And once they are gone we will not be able to get them back.""
The Department for Transport said it recognised the impact of the second lockdown on the travel industry, including regional airports, and said work was being done to ""develop measures to support aviation's long-term future"".
""The furlough scheme has been extended, and we're providing wider support through action on airport slots, loans and tax deferrals,"" a spokesman said."
"
Share this...FacebookTwitterUpdated 4/25/2011, 16:53 CETMSNBC, surprisingly, reports here on the very dark side of Earth Day co-founder Ira Einhorn. Let us recall that the first Earth Day took place on April 22, 1970 (April 22 happens to be the birthday of Vladimir Lenin).
MSNBC starts with (emphasis added):
Ira Einhorn was on stage hosting the first Earth Day event at the Fairmount Park in Philadelphia on April 22, 1970. Seven years later, police raided his closet and found the ‘composted’ body of his ex-girlfriend inside a trunk.”
According to the report, he was a hippie guru who advocated peace, love and flower power, but then murdered his girlfriend Helen “Holly” Maddux after she had broken up with him. After she went to his apartment to gather her things, she was never seen again. Einhorn told the police:
…that she had gone out to the neighborhood co-op to buy some tofu and sprouts and never returned.”
Tofu and sprouts at the neighborhood co-op? It gets even funnier. Einhorn was arrested but he jumped bail and fled to Europe. Eventually, after having been on the run for 23 years, he was extradited to the States by the French. Here’s how he explained the murder to police:
Taking the stand in his own defense, Einhorn claimed that his ex-girlfriend had been killed by CIA agents who framed him for the crime because he knew too much about the agency’s paranormal military research.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Maybe the real reason she was composted was probably because she had voted Republican, or criticised Paul Ehrlich. who knows? Indeed there are a number of theories Einhorn used to explain the death of his former girlfriend, including claiming that it was a set-up by the CIA and that he was surprised when he learned Holly Maddux’s body was found in his steamer chest in his closet in his apartment – months later and after foul liquids had been oozing through the floor and into the apartment below.
Wikipedia writes he was a friend and contemporary of Jerry Rubin and Abbie Hoffman. Einhorn’s past is naturally an embarrassment for the Green movement, and like the Medieval Warm Period, they would prefer for us forgetting about it. MSNBC writes:
Understandably, Earth Day’s organizers have distanced themselves from his name, citing Gaylord Nelson, an environmental activist and former Wisconsin governor and U.S. senator who died in 2005, as Earth Day’s official founder and organizer.”
So let’s add Einhorn to the list of green psychopaths along with Bin Laden and Charles Manson – with Bill “bummer outer” McKibben, Michael “f-ing” Tobis, Richard “1010” Curtis and Ben “beat-the-crap” Santer on the potential list.
Finally, claiming that one invented the Internet seems to be a sort of mental bug greens are prone to having. According to http://cmm.lefora.com, Einhorn, like Al Gore, also claimed he created the Internet:
He further claims he ‘left the money economy in 1963, creating a lifestyle based on information transfer.’ Einhorn credits himself with ‘creating an international information network under the auspices of Bell of Pa. and AT&T, called the Internet before the Internet that reached into 27 countries.’  Then he provided a (long) list of accomplishments.”
Weird.
And here’s much more on the violent background of the man who was part of the founding of Earth Day: http://cmm.lefora.com/2009/01/20/ira-einhorn-bully-conman-mooch-murderer/.
Share this...FacebookTwitter "
"
NODC Ocean Heat Content (0-700 Meters) – 2007, 2008 & 2009 Corrections
Guest post by Bob Tisdale
The National Oceanographic Data Center (NODC) recently updated its 4th quarter and annual 2009 Ocean Heat Content (OHC) data. The data that was presented in conjunction with the Levitus et al (2009) Paper now covers the period of 1955 to 2009. There have been changes that some might find significant.
This post presents:
1. A brief look at the revisions (corrections) to the data in 2007 and 2008 OHC data
2. A comparison of the NODC OHC data for the period of 2003 to 2009 versus the GISS projection
REVISIONS (Corrections) TO THE 2007 AND 2008 NODC OHC DATA
Figure 1 is a gif animation of two Ocean Heat Content graphs posted on the NODC GLOBAL OCEAN HEAT CONTENT webpage. It shows the differences between the current (January 2010) version and one that appears to include data through June or September 2009. So this is an “Official” correction (not more incompletely updated data posted on the NODC website discussed in NODC’s CORRECTION TO OHC (0-700m) DATA, which required me to make corrections to a handful of posts). I have found nothing in the NODC OHC web pages that discuss these new corrections. Due to the years involved, is it safe to assume these are more corrections for ARGO biases? As of this writing, I have not gone through the individual ocean basins to determine if the corrections were to one ocean basin, a group of basins, or if they’re global; I’ll put aside the multipart post I’ve been working on for the past few weeks and try to take a look over the next few days.
 http://i48.tinypic.com/14e6wjn.gif
Figure 1
NODC OHC OBSERVATIONS VERSUS GISS PROJECTION (2003-2009)

One of the posts that needed to be corrected back in October was NODC Ocean Heat Content (0-700 Meters) Versus GISS Projections (Corrected). The final graph in that post was a comparison of global ocean heat content observations for the period of 2003 through year-to-date 2009 versus the projection made by James Hansen of GISS of an approximate accumulation of 0.98*10^22 Joules per year. Figure 2 is an updated version of that comparison. Annual Global OHC data was downloaded from the NODC website (not through KNMI). The trend of the current version of the NODC OHC data is approximately 1.5% of the GISS projection. That is, GISS projected a significant rise, while the observations have flattened significantly in recent years. The apparent basis for the divergence between observations and the GISS Projection was discussed in the appropriately titled post Why Are OHC Observations (0-700m) Diverging From GISS Projections?
 http://i47.tinypic.com/20kvhwn.png
Figure 2
Note: The earlier version of that graph (with the NODC’s October 15, 2009 correction)…
http://i37.tinypic.com/i6xtnl.png
…shows a linear trend of ~0.08*10^22 Joules/year. The current linear trend is ~0.015*10^22 Joules/year. Some might consider that decrease to be significant.
NOTE:  I DELETED THE THIRD AND FOURTH PARTS OF THIS POST…
3. GLOBAL, HEMISPHERIC, AND INDIVIDUAL BASIN OHC UPDATE THROUGH DECEMBER 2009, AND
4. TREND COMPARISONS
…UNTIL I TRACK DOWN DISCREPANCIES I CAN’T EXPLAIN. I WILL REPOST THOSE SECTIONS IN A NEW POST. I BELIEVE I UNDERSTAND THE DIFFERENCES, BUT I NEED TO CHECK WITH KNMI.
SOURCES

NODC Annual Global OHC data used in Figure 2 is available here:
ftp://ftp.nodc.noaa.gov/pub/data.nodc/woa/DATA_ANALYSIS/3M_HEAT_CONTENT/DATA/basin/yearly/h22-w0-700m.dat


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e8071b9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

“It’s fun to be in a place where our crowd is still in office.”  
–Bill Clinton in a speech to the British Labor Party conference 



Socialist Luiz Inacio “Lula” da Silva’s election as Brazil’s next president is another nail in the coffin of the Third Way, the decade‐​long attempt by some leftist politicians to occupy the allegedly middle ground between socialism and capitalism. This political movement recently held sway in the United States and abroad but, as the following survey demonstrates, today many proponents are playing electoral defense. It’s also apparent that those Third Way politicians who are prospering owe their electoral survival more to astute political marketing than to successful policy performance.



During the 1990s, Third Way candidates proved electorally attractive throughout Latin America. After a decade of free market gains, respective national elections saw relatively moderate center‐​left candidates triumph over more conservative opponents. 



But, on Oct. 27, the world’s fourth‐​largest democracy experienced the after‐​effects of a failed Third Way government. Outgoing Brazilian President Fernando Henrique Cardoso was a leading Third Way spokesman. But neither conquering inflation nor privatizing some state‐​owned utilities compensated politically for the anemic economy, rising unemployment, falling real incomes, and high interest rates that stemmed from insufficient deregulation, tax cutting and trade liberalization.



So, Cardoso was unable to provide a boost to his party’s chosen successor, Jose Serra. This compounded the problem of the Third Way’s grudging acceptance of globalization, as reflected in support for greater global economic governance, including capital controls. Such an outlook gave succor to Lula’s more robust economic populism. 



In Europe, British Prime Minister Tony Blair was reelected in July 2001. But this prominent Third Way leader’s electoral success reflected the inadequacies of his conservative opponents rather than a stellar record of policy accomplishment. 



Although Blair has withstood the pressure from his party’s socialist base to raise income taxes, such fiscal parsimony doesn’t tell the whole story. After all, various sources of indirect taxation have increased significantly. Frequent microeconomic meddling, including the introduction of a minimum wage, has balanced Blair’s relative inactivity on the macroeconomic front. Regrettably, Blair brought a somewhat illiberal streak to criminal justice and social regulation issues that emphasizes social conformity over the pursuit of individual preferences.



German Chancellor Gerhard Schroeder, continental Europe’s leading Third Wayer, has had a far rougher political ride. Given the poor economy, symbolized by significant joblessness, Schroeder’s September reelection was surprising. Early on, his government did introduce minor tax and spending reductions and proposed a market‐​oriented reform of the government‐​funded social security retirement program. 



But Schroeder quickly lost his reformist way. He did little to change an overly regulated labor market, the principal source of high unemployment. Fiscal policy was reversed and the government provided taxpayer‐​funded corporate bailouts. Schroeder’s political fortunes were rescued by two fateful events during the campaign: his adroit handling of a flooding crisis (he promised to throw lots of money at the problem) and his vociferous opposition to a U.S.-led invasion of Iraq.



Dutch Prime Minister Wim Kok was also a pillar of the Third Way movement. But in May Dutch voters threw out Kok’s eight‐​year‐​old center‐​left coalition government and replaced it with the right‐​wing opposition. Such was the extent of popular discontent that the parties comprising the center‐​left coalition were relegated to third and fourth place, respectively. 



During the 1990s, Prime Ministers Romano Prodi and Massimo D’Alema expounded the promise of an Italian Third Way. However, the governing center‐​left coalition’s unwillingness to address the structural problems handicapping the Italian economy (e.g., high taxes and government spending combined with heavy state ownership) inadvertently produced a political climate ripe for change. In last year’s election, the incumbent Third Wayers were deposed by Silvio Berlusconi’s right‐​wing Freedom Alliance.



Back here, President Clinton’s own brand of Third Wayism reflected a timid, government‐​knows‐​best inclination. Clinton raised taxes and attempted to micromanage individuals’ financial decision‐​making through tax credits. Some Clinton policies resembled fuzzy versions of social conservatism. His extension of the failed War on Drugs is an obvious example. Revealingly, neither Clinton nor Vice President Al Gore could definitively decide whether they were pragmatic, “New Democrat” centrists or economic populists. As a result, Gore wasn’t elected president in 2000. 



But how will the Third Way fare in the midterm elections? 



It’s helpful, if dispiriting, to look at California. The Third Way’s New Democrat strand has an important adherent in Gov. Gray Davis. But since his 1998 election, Davis has placed particular emphasis upon appeals to labor unions and racial minorities, hence his opposition to school vouchers and support of immoderate affirmative action. The Cato Institute recently awarded Davis an “F” for his fiscal performance. 



Consequently, Davis’s expected reelection reflects two realities: first, the political limitations of his conservative opponent, rather than an endorsement of Davis’s flawed performance; and, second, a political ruthlessness on Davis’s part that enthusiastically embraces cutting‐​edge political marketing techniques. 



Davis may be the poster boy for the successful Third Way politician. Although he’s more rhetorically pleasing than the old‐​school liberal‐​socialist, in practice he’s equally disposed to limiting our freedom.
"
"**Stormont minister Edwin Poots has refused to comment on why he sent an email criticising the Department of Health's response to Covid-19.**
The agriculture minister replied ""no comment"" when asked about it at an event in the Mournes in County Down.
Speaking on Wednesday he also said the email was ""self-explanatory"".
In the email, Mr Poots said the ""failure"" of the health department's response would ""inevitably lead to the failure of the economy"".
The original email was sent by a member of public to MLAs criticising the government for the ""devastating effect"" the tougher restrictions will have.
In a reply to all, Mr Poots agreed, saying the majority of the Executive see things differently.
During the event on Wednesday morning, the DUP minister said the quicker the vaccine and mass testing could be rolled out, the sooner lockdowns could end.
Asked whether he was on board with the latest restrictions, he said he did not have any choice.
He added: ""The Executive has made a decision and we all will abide by that decision.""
He said spread in households contributed significantly to the incidence of the disease and needed to be ""clamped down on"".
When asked about the Christmas bubbling plan he raised concerns.
He said: ""I don't think there's any alternative - people want to have Christmas. They have had an awful year and they need some respite from Covid-19, albeit there will be problems arise from that and there'll be a greater level of spread as a consequence of it.
""I've absolutely no doubt the R-rate will rise in the two weeks before Christmas and over the Christmas period and there'll be a consequence thereafter. That goes without saying.""
Two weeks of Covid-19 lockdown restrictions will take force across NI from Friday.
Mr Poots, who is a former Stormont health minister, was criticised by other parties for the contents of his email.
Ulster Unionist health spokesman Alan Chambers MLA said Mr Poots' comments showed ""how detached from reality he is"".
Mr Poots' party colleague, the South Antrim MLA Pam Cameron, who is DUP deputy chair of the health committee, had previously described Mr Poots' reasoning as ""a little simplistic"".
Stormont sources said the minister, who has previously spoken out against imposing tighter lockdown measures, said it was illogical to close non-essential retail as it could severely damage the high street.
It is believed Mr Poots did not ask for the measures to be put to a vote by the executive and said he would accept whatever measures were agreed by the executive."
"

We’re all expected to love baseball — it’s America’s sport, after all — but I know a few taxpayers in the greater Washington area, maybe even a few thousand, who don’t. You know, people who weren’t — horrors! — glued to their TV sets, rooting for the luckless Red Sox or the jinxed Cubs to finally make it back to the World Series. People who haven’t spent every waking moment since 1971, when the Senators left, plotting to lure a team to town. People who don’t think the city’s image and its future depend on spending millions of taxpayer dollars on a state‐​of‐​the‐​art stadium for a transitory collection of athletes, artificially assembled through league drafts, franchise trades and high salaries. 



Alas, the idea that such taxpayers might exist doesn’t even seem to be on the radar screens of many local officials, who are falling all over themselves trying to scrounge together enough public money to lure a major league baseball team back to the national capital area. District Mayor Anthony Williams is proposing $339 million to build a stadium in town (even though there’s no commitment yet from any team), while baseball boosters in Northern Virginia are pushing millions in state construction bonds to win a franchise. 



These politicians are offering the usual justifications for providing the modern version of bread and circuses to their constituents: municipal prestige, business development, new jobs. But in the end, publicly funded stadiums come down to money — and I don’t mean money for the city that gets the team. I’m talking about money for wealthy sports moguls who have turned extorting taxpayers into an art form. 



Franchise owners typically win taxpayer support only through threats: Pay us off, or we will leave, they say. Give us a new stadium, or we’ll go someplace that will. Take the current competition for the Montreal Expos, who have been up for grabs ever since Major League Baseball assumed ownership of the financially ailing franchise. The only question seems to be, who will offer the biggest inducements to get them? 



Virginia and the District shouldn’t play this game. Stadiums don’t constitute a great unmet social need. Sports should be a private enterprise, privately funded, just as it was during most of the first half of the 20th century. 



Yet the willingness of political elites to sacrifice taxpayers on the omnipresent sports altar spans the country. Oregon faces a serious budget crisis, but that didn’t stop the legislature from recently approving $150 million for a new baseball stadium in a bid to win the Expos for Portland. In Oakland, Al Davis, the irrepressible owner of the Raiders football team, won a $34.2 million verdict against the city stadium authority for failing, he argued, to deliver on its promise of sold‐​out games. In San Diego, meanwhile, negotiations continue between the city and the Chargers, who want a new stadium — eight years after the city renovated the old one. 



Today, government involvement in the sports business seems unexceptional. But as Raymond Keating, chief economist for the Washington‐​based Small Business Survival Committee, observes, “Before the Great Depression, sports subsidies were rare.” Since then, he figures, government has poured well over $20 billion (in current dollars) into sports ventures. Such subsidies cannot be justified in principle. Making some people pay so others, whether franchise owners or restaurateurs or developers, can profit is a misuse of government. The only benefit is private, not public. 



Not only were sports facilities built privately earlier in the last century, they still can be. In 1987 Joe Robbie, owner of the Miami Dolphins, constructed his own stadium. Even the Redskins’ FedEx field was primarily a private venture by owner Jack Kent Cooke when it originally opened in 1997 (as Jack Kent Cooke Stadium), though the state of Maryland chipped in for roads and other public improvements. 



Proponents argue that franchises provide prestige, but the nation’s capital doesn’t have to worry about a lack of prestige. Is Los Angeles impoverished because it can’t keep a football team? Do people move to San Diego to see the Chargers — or for the climate? 



On the flip side, the financial benefits of government support for teams are obvious. Teams avoid having to finance a stadium. They are able to upgrade their facilities at taxpayer expense, even as they cater to a wealthier corporate clientele. 



Most teams are owned by extremely wealthy businessmen, such as the Redskins’ Dan Snyder and the Orioles’ Peter Angelos, (not to mention one former owner by the name of George W. Bush), who are able to resell at a profit. Professional sports investments often are a dilettantish affectation, especially for limited partners, who — besides a quick return on their cash — simply crave proximity to the team. Businessman John Imlay Jr. recently parlayed his $6 million investment in the Atlanta Falcons into $35 million, explaining to The Post’s Thomas Heath: “In ten years, I made five times my money and had a heck of a good time.” 



The taxpayers are not so lucky. Public finance experts Roger Noll of Stanford and Andrew Zimbalist of Smith College found in a recent study that “no recent facility appears to have earned anything approaching a reasonable return on investment and no recent facility has been self‐​financing in terms of its impact on net tax revenues.” Even better stadium projects, such as Baltimore’s Camden Yards, require continuing aid for upkeep. As F.W. Walz, a Cleveland city councilman who opposed the nation’s first subsidized sports facility, a baseball stadium, observed in 1928: “Of course, they say the stadium will pay for itself, but we’ve heard that story before.” 



Stadium proponents argue that owner enrichment is merely incidental to increased regional economic activity and tax collections. And they routinely produce studies claiming significant financial gains. But even if there is an economic benefit, it is small. University of Maryland economists Dennis Coates and Brad Humphreys figure that annual sports‐​oriented tax revenues and personal earnings from sports have been much less than 1 percent of the total earnings and revenues for Baltimore and Maryland. As they explain, “Although the absolute numbers seem large and impressive, they are small compared with the existing tax revenues and local economy, even if one grants that the proponents’ estimates are correct.” 



There’s much to criticize in such estimates, however. For instance, what’s the right “multiplier”? That is, how much is ultimately generated by a dollar spent on sports? Official figures tend to assume, unrealistically, that all of the money, including, for instance, players’ salaries, is spent locally. 



Even more important, though, is that sports spending primarily substitutes for other outlays. Stanford’s Noll figures that the vast majority of those attending games — more than 90 percent — are local residents. They are merely diverting their spending from other leisure activities. Money might shift a bit within a region — from suburbs to city, or from outer to inner suburbs. But, as economists have consistently found, the amount of new economic growth is minimal. Economists Robert Baade of Lake Forest College and Allen Sanderson of the University of Chicago have looked at 10 metropolitan areas that brought in sports teams, and found no net employment increase, as spending was simply realigned. And there was no evident difference in economic performance between cities with or without teams during the 1994 baseball strike, says the University of Akron’s John Zipp. 



So if the goal is trickle‐​down consumer spending and business development, why not build a new automobile factory, retail outlet, grocery store or software facility to attract and maintain companies, jobs and economic growth? Forget a sports team for D.C. Just erect a string of buildings for restaurants. That should draw suburban residents, and their money, here. 



But neither sports boosters nor their political allies are much interested in overall economic impact. Fans want a team, potential franchise owners desire subsidies, and elected officials expect political gain — and the opportunity to snag an invitation to the owner’s box. Government stadiums benefit economic and political elites, not the public. 



Yes, refusing to play the subsidy game might mean losing a franchise. But if the only way to prevent a team from moving or to get one to come to your town is to shovel corporate welfare into a billionaire’s hands, trust the research — it isn’t worth it. 
"
"
From the Times of India – a “put up or shut up” moment – “we’ll go along if you pay us”.

Excerpts below:
BEIJING: In an unprecedented move, India on Saturday joined China and two other developing countries to prepare for a major offensive on rich nations at the Copenhagen conference on climate change next month.
The four countries, which include Brazil and South Africa, agreed to a strategy that involves jointly walking out of the conference if the developed nations try to force their own terms on the developing world, Jairam Ramesh, the Indian minister for environment and forests (independent charge), said.
“We will not exit in isolation. We will co-ordinate our exit if any of our non-negotiable terms is violated. Our entry and exit will be collective,” Ramesh told reporters in Beijing. 
The move comes after reports suggested that rich nations led by Denmark are trying to set the agenda of the conference by presenting a draft containing a set of specific proposals.
…
The four nations issued a joint press release, which made it clear the developed nations should be ready to contribute funds and share green technology if they expected the developing and poor nations to take major actions on environmental protection.
…
The developing nations will also not accept any pressure from developed countries to establish legally binding emission targets at Copenhagen. Developing countries want to be allowed to reduce emissions voluntarily and take what they consider to be “nationally appropriate actions” he said.
Ramesh said India will under no circumstances accept the concept of a peaking year under which each country will have to indicate on what date they will reach the highest level of pollution before beginning to come down.
India will also not accept any unsupported mitigation actions without any effort by developed countries to provide funds and technology support to improve environment in developing nations.
Read the complete article at the Times of India


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e909c85fc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWe’ve all heard how climate change causes everything that’s bad for humans and the planet to increase and to intensify. Well, ironically, that also now includes boredom on the subject of climate change itself. Many are simply just getting bored to death by it.
Indeed a spate of recent studies have shown that people are growing tired and unconcerned about climate change in a number of countries. Much of this is due to the lack of credibility the science is suffering in the wake of all the scandals, scams and exaggerations that have been exposed over the last couple of years.
Climate boredom has also gripped Sweden too, as the English-language The Local writes here:
Swedes’ environmental interest plummets: report

Public interest in climate issues has dropped to its lowest point in five years, according to the upcoming annual SOM report from the University of Gothenburg. Only 14 percent of Swedes currently consider the environment to be among our most important problems, reports daily newspaper Svenska Dagbladet.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This is the lowest figure since 2006, and far behind the record-high figures of the late eighties, when Swedes’ environmental involvement truly boomed.”
When one considers how often people have been exposed to the same, worn-out climate disaster reruns about surging sea levels, storms, weather extremes, droughts, floods, dying oceans, etc., then no one ought wonder why boredom on the subject is so deeply entrenched.
According to The Local, greens blame the lack of media interest in climate change. The figures show a plummet in Swedish public interest:
This trend has now been broken, and last year this number had decreased heavily, from 21 to 14 percent.”
To be honest, it has nothing to do with media slacking off on the subject. It has everything to do with them making false projections time and again. You can only cry wolf so many times before you lose all credibility and respect. Once shown to be a liar and a charlatan, it later becomes virtually impossible to convince others of anything else.
Today climate change is probably not boring only for climate skeptics. Indeed for us it is now just beginning to get interesting.
Share this...FacebookTwitter "
"

Scientific American has sicced the big dogs on Danish statistician Bjorn Lomborg for having the audacity to publish a highly referenced book, “The Skeptical Environmentalist,” which argues that global warming and many other environmental “threats” are overblown. What gives?



Scientific American now joins the magazines Science and Nature in blasting Lomborg. They all editorialize that his “book is a failure” and call out four well‐​traveled attack dogs from the Washington big government/​greenie/​lefty establishment in support. They include:



*John Holdren, a defense expert from Harvard. In 1995, he published a paper for the United Nations University advocating “a condition in which no nation ‘s military forces were strong enough to threaten the existence of other states.” Good thing we didn’t listen. *Tom Lovejoy, former director of the World Wildlife Fund, the biggest green lobbying organization in the history of the planet. *John Bongaarts, vice president of the Population Council, the most influential lobby in the Down With People crowd. And, *Steve Schneider from Stanford. Compared to the rest, Schneider is a real atmospheric scientist, and (naturally) he wrote the nastiest of the four Fatwas on Lomborg.



Why draw so much attention to a book you don’t want to sell? Clearly, the editorial boards of Nature and Scientific American, as well as the leadership of the American Association for the Advancement of Science (AAAS, the publisher of Science) perceive a big threat if Lomborg goes unanswered.



This writer has hung around D.C. enough to smell the danger: “The Skeptical Environmentalist” threatens billions of taxpayer dollars that go into the global change kitty every year. The AAAS isn’t located on H Street in Washington — known locally as “gravy train lane” for its packs of lobbyists — for nothing.



Do the arguments against Lomborg have merit? Let’s examine two of the many assertions made by Schneider against Lomborg.



Emissions Scenarios. Schneider complains that “Lomborg …dismisses all but the lowest” scenarios for future carbon dioxide emissions and consequent global warming.



Lomborg does so with good reason. An analysis of atmospheric carbon dioxide concentrations over the last quarter‐​century reveals that the standard assumption of strong exponential growth is wrong. You could read about that it in NASA scientist James Hansen’s recent writings in the “Proceedings of the National Academy of Sciences.”



Future Warming. Schneider takes great exception with Lomborg’s statement that “temperatures will increase much less than the maximum estimates from the IPCC” with the likely change less than 2ºC (3.6ºF) by 2100.



The truth is that Lomborg is behaving like a scientist here, and Schneider and Scientific American don’t like the result.



As is shown graphically in the latest report by the UN Intergovernmental Panel on Climate Change (IPCC), the ensemble of future climate models predicts a warming that, once started, continues at a virtually constant rate for the next century. However, they differ in the rates of their projected warming.



It is also the consensus of the IPCC, first stated in 1996 and repeated in 2001, that “the balance of evidence suggests a discernible human influence on global climate.” In other words, alterations of the atmosphere resulting from human emissions are producing a detectable signal in global and regional temperatures. By using the combination of those two realities, Lomborg is forced to conclude that warming will be relatively modest. That is all a scientist can do: reconcile disparate models with observed data.



The reason Scientific American is apoplectic about that argument is because it is scientific and convincing. If it convinced the Bush administration to walk away from Kyoto, how long will it be before it convinces Congress to derail the multibillion‐​dollar gravy train feeding the global warming claque?



Then there’s the jealousy component. Each of Scientific American’s four writers also has their own books. While Lomborg’s is immensely popular, ranking #1 in sales under “Environmental Science” (and 354th overall), the others aren’t so hot. Comparative sales from ama​zon​.com for Jan. 4 show the following: John Bongaarts’ “Beyond Six Million” ranks 463,784; Tom Lovejoy’ s “Blueprint for a Green School” is at 583,463; Schneider’s “Are we Entering the Greenhouse Century” comes in at 574,469; and trailing this field of glueboxes is John Holdren’s “Global Ecology,” the 1,340,727th best selling book at Ama​zon​.com. If these were horses, you’d have to clock them with a calendar.



The bottom line is that all of the Lomborg‐​bashing makes Scientific American look like a bunch of attack dogs, manufacturing arguments that won’t hunt and are in fact canine themselves.
"
"Where is Boris Johnson? As flooding devastates large parts of the north, the Midlands and Wales, the prime minister’s strategy is to hide from public scrutiny and hope the whole thing blows over. It’s now 10 days and counting since his last public appearance. This is a huge miscalculation. Because for all of Dominic Cummings’ talk of “superforecasting”, the government is ignoring the issue that will define politics for decades to come: our response to the climate crisis. From wildfires in Australia and the Amazon, to extreme heat in India, to drought in east Africa, climate-induced extreme weather is already reshaping our planet in disastrous ways.  To win, Labour needs to make sure 2024 is a climate election. Too often we have fought on terms defined by the Tories. David Cameron and George Osborne dominated politics in the aftermath of the financial crisis, winning two elections by pinning the blame on Labour’s fabled “overspending”. Last December, the narrative was defined by “Get Brexit done”, and we failed to provide a convincing answer. Labour can only win the next election by confidently setting the agenda. Our shift to the politics of anti-austerity under Jeremy Corbyn put the Tories on the back foot and enabled the gains made in 2017. But as our devastating loss to Johnson showed, standing still is not enough. That’s why as leader, I’ll centre Labour’s plans to tackle the climate crisis in everything we do, and make sure it defines the next election. That starts with being clear about where the blame for climate breakdown lies: with the fossil fuel executives who are knowingly burning our planet, and with the Tories who refuse to take the action necessary to stop them. On this terrain, Labour can win. I know that we can build a winning electoral coalition around our green industrial revolution, uniting working people everywhere with a just, aspirational response to the climate crisis. Long before the next election, people in every community up and down the country need to know concretely what this will mean for them. Well-paid, unionised green jobs in every town, city and village; economic renewal and pride for communities left behind by deindustrialisation; and a leading role for our country in combating the climate crisis across the world. This isn’t just a set of policies that need to be sold better, it has to be the core of our vision for a socialist future. The groups of voters Labour must unite in order to win are often pitted against each other. We’re told they are polarised and hold opposing interests: city against town, young against old, homeowner against renter. But they all want a livable planet, and all aspire to a future that is healthier, wealthier and more free. Labour’s green industrial revolution can unite diverse communities across the country on this basis, forming the coalition we need to win. But we cannot wait until 2024. Labour needs a leader who will relentlessly hold the Conservative government to account over its failings on climate change here and now. These failings couldn’t be starker than in Johnson’s abandonment of communities devastated by flooding. The government must take urgent steps to seriously address the crisis. First, immediate action must be taken to tackle the root cause, by rapidly phasing out the UK’s greenhouse gas emissions, working to a 2030 decarbonisation target. We cannot afford to delay action any longer, with vague or distant targets. Second, the government must follow the recommendations of the Environment Agency and National Infrastructure Commission and increase its capital spend on flood defences to £1bn per year. Alongside restoring the UK’s natural landscapes, and ensuring our firefighters are properly funded, equipped and resourced, these are simple steps that could be taken immediately to protect at-risk communities from further flood damage. Lastly, with the damage from flooding projected to increase substantially, I would question why costs should be borne by homeowners rather than those most responsible for causing the climate crisis. That is why I have called for a Climate Justice Fund, which would make fossil fuel companies pay damage costs incurred by flooding and wildfires, as well as contributing to losses suffered by countries in the global south. Soon, all politics will be climate politics. Labour needs a leader who recognises this, and has a strategy to build a winning coalition around a plan for rapid decarbonisation and economic renewal, working with trade unions, community groups and the climate movement. This is the basis on which we will win in 2024, transform Britain – and help save the planet.  • Rebecca Long-Bailey is the Labour MP for Salford and Eccles and a candidate for the Labour party leadership"
"
The Amplification Invitation 
 
Guest Post by Willis Eschenbach
Tropical: the ITCZ from space. Source: NASA Earth Observatory. Click for larger image
A  while ago I started studying the question of the amplification of the tropical  tropospheric temperature with respect to the surface. After months of research  bore fruit, I started writing a paper. My intention was to have it published in  a peer-reviewed journal. I finished the writing about a week ago.
During  that time, I also wrote and published The Temperature Hypothesis here on WUWT. This got me to thinking about science, and about how we establish  scientific facts. In climate science, the peer review process is badly broken.  Among other problems, it is often an “old boy” system where very poor work is  waved through. In common with other sciences, turnaround of ideas in journals  takes weeks. Under pressure to publish, journals often do only the most cursory  examination of the papers.
Upon reflection, I have decided to try a  different way to examine the truth content of my paper. This is to invite all of  the authors whose work I discuss, and other interested scientists of all  stripes, to comment on the paper and on whether they can find any flaws in it.  To facilitate the process I have provided all of the code and data that I used  to do the analysis.
To make this process work will require cooperation.  First, I ask for science and science only. No discussions of motives. No ad  homs. No generalizations to larger spheres. No asides. No disrespect, we can be  gentlemen and gentlewomen. No comments on politics, CO2, or AGW, no snowball  earth. This thread has one purpose only, to establish whether my ideas stand:  to either attack and destroy the ideas I put forth in the paper below, or to  provide evidence and data to support the ideas I put forth below.

People think science is a cooperative endeavor. It is not. It is a  war of ideas. An idea is put out, and scientists gather round to attack it and  disprove it. Sometimes, other scientists may support and defend it. The more  fierce the attack, the better … because if it can withstand the strongest  attack, it is more likely true. When your worst scientific enemies and greatest  disbelievers can’t show that you are wrong, your ideas are taken as scientific  fact. (Until your ideas in turn are perhaps overthrown). Science is a blood  sport, but all of the attack and parry is historically done in private. I  propose to bring it out in public, and I offer my contribution below as the  first victim.
Second, I will insist on a friendly, appreciative attitude  towards the contributions of others. We are interested in working together to  advance our primitive knowledge of how the climate works. We are doing that by  trying to tear my ideas down, to disprove them, to find errors in them. To make  this work we must do this with respect for the people involved and the ideas  they put forwards. You don’t have to smash the guy to smash the idea, in fact it  reduces your odds.
Third, no anonymous posting, please. If you are think  your ideas are scientifically valid, please put your name on them.
With  that in mind, I’d like to invite any and all of the following authors whose work  I discuss below, to comment on and/or tear to shreds this study.
J. S.  Boyle,  J. R. Christy,  W. D. Collins,  K. W. Dixon,  D. H. Douglass,  C.  Doutriaux,  M. Free, Q. Fu,  P. J. Gleckler,  L. Haimberger,  J. E. Hansen,  G.  S. Jones,   P. D. Jones,  T. R. Karl,  S. A. Klein,  J. R. Lanzante,  C. Mears,   G. A. Meehl,  D. Nychka,  B. D. Pearson,  V Ramaswamy,  R. Ruedy, G. Russell,  B. D. Santer,  G. A. Schmidt,  D. J. Seidel,  S. C. Sherwood, S. F. Singer,  S.  Solomon,  K. E. Taylor,  P. W. Thorne,  M. F. Wehner,  F. J. Wentz,  and T. M.  L. Wigley
(Man, all those 34 scientists on that side … and on this side  … me. I’d better attack quick, while I have them outnumbered … )
I  also invite anyone who has evidence, logic, theory, or data to disprove or to  support my analysis to please contribute to the thread. Because at the end of  this process, where I have exposed my ideas and the data and code to the attacks  of anyone and everyone who can find flaws with them, I will have my own answer.  If no one is able to disprove or find flaws in my analysis, I will consider it  to be established science until someone can do so. Whether you consider it  established science is up to you. However, it is certainly a more rigorous  process than peer-review, and anyone who disagrees has had an opportunity to do  so.
I see something in this nature, a web-based process, as the future of  science. We need a place where scientific ideas can be brought up, discussed and  debated by experts from all over the world, and either shot down or  provisionally accepted in real time. Consider this an early experiment in that  regard.
Three months to comment on a Journal paper is so 20th century.  I’m amazed that the journals haven’t done something akin to this on the web,  with various restrictions on reviewers and participants. Nothing sells journals  like blood, and scientific blood is no different from any other.
So  without further ado, here is my paper. Tear it apart or back it up, enjoy, ask  questions, that’s science.
My best to everyone.
w.



A New Amplification Metric
 
 
ABSTRACT: A new method is proposed for exploring  the amplification of the tropical tropospheric temperature with respect to the  surface. The method looks at the change in amplification with the length of the  record. The method is used to reveal the similarities and differences between  the HadAT2 balloon, UAH MSU satellite, RSS MSU satellite, RATPAC balloon, AMSU  satellite, NCEP Reanalysis, and five climate model datasets In general, the  observational datasets (HadAT2, RATPAC, and satellite datasets) agree with each  other. The climate model and reanalysis datasets disagree with the observations.  They also disagree with each other, with no two being  alike.

Background

“Amplification” is the term  used for the general observation that in the tropics, the tropospheric  temperatures at altitude tend to vary more than the surface temperature. If the  surface temperature goes up by a degree, the tropical temperature aloft often  goes up by more than a degree. If surface and tropospheric temperatures were to  vary by exactly the same amount, the amplification would be 1.0. If the  troposphere varies more than the surface, the amplification will be greater than  one, and vice versa.
There are only a limited number of observational  datasets of tropospheric temperatures. These include the HadAT2 and RATPAC  weather balloon datasets, and two versions of the Microwave  Sounding Unit (MSU) satellite data. At present the satellite record is about  thirty years long, and the two balloon datasets are about 50 years in length.

Recently there has been much discussion of the the Santer et al.  2005, Douglass et al. 2007, and the Santer et al. 2008 papers on tropical  tropospheric amplification. The issue involved is posed by Santer et al. 2005 in  their abstract:
The month-to-month variability of tropical  temperatures
is larger in the troposphere than at the Earth’s  surface.
This amplification behavior is similar in a range of
observations  and climate model simulations, and is
consistent with basic theory. On  multi-decadal timescales,
tropospheric amplification of surface warming is a  robust
feature of model simulations, but occurs in only one
observational  dataset [the RSS dataset]. Other observations show weak or
even negative  amplification. These results suggest that
either different physical  mechanisms control
amplification processes on monthly and  decadal
timescales, and models fail to capture such behavior, or
(more  plausibly) that residual errors in several
observational datasets used here  affect their
representation of long-term  trends.


Metrics of  Amplification

Santer 2005 utilizes two different metrics of  amplification, viz:
We examine two different amplification metrics:  RS(z), the ratio between the temporal standard deviations of monthly-mean  tropospheric and TS anomalies, and Rβ(z), the ratio between the multi-decadal  trends in these quantities.


Neither of  these metrics, however, measures the amount of the amplification at altitude  which is related to the surface variations. Ratios of standard deviations  merely measure the size of the swings. They cannot indicate whether one is an  amplified version of the other. The same is true of trend ratios. All they can  show is the size of the difference, not whether the surface and atmosphere are  actually correlated.
In order to measure whether one dataset is an  amplified version of another, the simplest measure is the slope of the ordinary  least squares regression line. This measures how much one temperature varies in  relation to another.
Despite a variety of searches, I was unable to find  published studies showing that the “amplification behavior is similar in a range  of observations and climate model simulations” as Santer et al. 2005 states.  To investigate whether the tropical amplification is  “robust” at various timescales, I decided to calculate the tropical and global  amplification (average slope of the regression line) at all time scales between  three months and thirty years or more for a variety of temperature datasets.  I  started with the UAH and the RSS versions of the satellite record. Next I looked  at the HadAT2 balloon (radiosonde) dataset. The results are shown in Figure 1.
To measure the amplification, for every time interval (e.g. 5 months) I  calculated the amplification of all contiguous 5-month periods in the entire  dataset. In other words, I exhaustively sub-sampled the full record for all  possible contiguous 5-month periods. I took the average of the results for each  time period. Details of the method are given in the Supplementary Online  Material (SOM) Sections 2 & 3 below.
I plotted the results as a curve  which shows the average amplification for each of the various time periods from  three months to thirty years (the length of the MSU datasets). This shows the  “temporal evolution” of amplification, how it changes as we look at longer and  longer timescales. I show the results at a variety of pressure levels in Figure  1. In general, at all pressure levels, short term (3 – 48 month) amplifications  are much smaller than longer term amplifications.

Figure 1. Change of amplification with  length of observation. Left column is amplification in the tropics (20N/S), and  the right column is global amplification. T2 and TMT are middle troposphere  measurements. T2LT and TLT are lower troposphere. Starting date is January 1979.  Shortest period shown is amplification over three months. Effective weighted  altitudes (from the RSS weighting curves) are about 450 hPa for the lower  altitude TLT (~ 6.5 km) and 350 hPa (~ 8 km) for the higher altitude  TMT.

Tropical Observations  1979-2008
 Figure 1(a). UAH and RSS  satellite data. This was the first analysis done.  It confirmed the sensitivity of this temporal evolution method, as it shows a  clear difference between the two versions (RSS and MSU) of the MSU satellite  data. Both of the datasets (UAH and RSS) are quite  close in the first half of the record. They diverge in the second  half.

The higher and lower altitude amplifications are very similar in both  the RSS and the UAH versions. The oddity in Fig 1(a)  is that I had expected the amplification at higher altitude (T2 and TMT) to be larger than at the  lower altitude (T2LT and TLT) amplification. Instead, the higher altitude record  had lower amplification. This suggests a strong stratospheric influence on the  T2 and TMT datasets. Because of this, I have used only the lower altitude T2LT  (UAH) and TLT (RSS) datasets in the remainder of this analysis.
Figure  1(b). HadAT2 radiosonde data. (Note the difference in vertical scale.)  Despite the widely discussed data problems with the radiosonde data, this shows  a clear picture of amplification increasing with altitude to 200 hPa, and  decreasing above that. It confirms the existence of the tropical tropospheric  “hot spot”, where the amplification is large. It conforms with the result  expected from theoretical consideration of the effect of lapse rate. It also  shows remarkable internal consistency. The amplification increases steadily with  altitude up to 200 hPa, and decreases steadily with altitude above that. Note  that the 1998 El Nino is visible as a “bump” in the records at about month 240.  It is also visible in the satellite record, in Fig. 1(a).
Figure 1(c).  HadAT2, overlaid with MSU satellite data. Same vertical scale as (a). The  satellite and balloon data all agree in the first half of the record. In the  latter half, the fit is much better for the UAH satellite data analysis than the  corresponding RSS analysis. Note that the amplification of the UAH version is a  good fit with the 500 hPa level of the HadAT2 data. This agrees with the  theoretical effective weighted altitude of the T2LT measurement.

Global Observations 1979-2008

Figure 1(d). Global UAH and RSS satellite data. Note  difference in vertical scale. There is little  amplification at the global level.
Again, the UAH  and RSS records are similar in the short term but not the long  term.
Figure 1(e). Global HadAT2 radiosonde data. Note difference  in vertical scale. Here we see that the amplification is clearly a tropical  phenomenon. We do not see significant amplification at any  level.
Figure 1(f). Global HadAT2, overlaid with MSU satellite data. Same vertical scale as (d). Once again, the satellite and balloon data all  agree in the first half of the record. In the latter half, again the UAH  analysis generally agrees with the observations. And again, the RSS version is a  clear outlier.
Both in the tropics and globally, amplification of the  levels above 850 hPa start low. After that they show a slow increase. The  greatest amplification occurs at 8-10 years. After that, they all (except RSS)  show a gradual decrease up to the 30 years end of the record.

Having seen the agreement between the UAH T2LT and the HadAT2  datasets, I next compared the tropical RATPAC data with the HadAT2 data. The  RATPAC data is annual and quarterly. I averaged the HadAT2 data annually and  quarterly  to match. They are shown in Fig. 2. Note that these are fifty-year  datasets, a much longer timespan than Fig. 1.

Figure 2. RATPAC and HadAT2 Tropical Amplification, 3 years to  50 years. Left column is annual data. Right column is quarterly  data.

There is very close agreement between the HadAT2 and the RATPAC  datasets. The annual version shows a number of levels of pressure altitude. The  quarterly version averages the troposphere down into two levels, one from  850-300 hPh, and one from 300-100 hPa. Both annual and quarterly RATPAC versions  agree well with HadAT2.
Before going further, let me draw some  conclusions about tropical amplification from Figs. 1 & 2.
1. Three  of the four observational datasets (HadAT2, RATPAC, and UAH MSU) are in  surprisingly close agreement. The fourth, the RSS MSU dataset, is a clear  outlier. The very close correspondence between HadAT2 and RATPAC at all levels  gives increased confidence that the observations are dependable. This is  reinforced by the good agreement in Figs. 1(c) and 1(f) between the UAH MSU and  the HadAT2 500 hPa level amplifications, both in the tropics and  globally.
2. Figure 1(c) clearly shows the theoretically predicted  “tropical hot spot”. It is most pronounced at 200 hPa at about 8-10 years. At  its peak the 200 hPa level has an amplification of about 2. However, this  gradually decays over time to a long-term (fifty year) amplification of about  1.5.
3. The lowest level, 850 hPa, has a short-term amplification of just  under 1. It gradually increases over time to an amplification of about 1. It  varies very little with the length of observations. RATPAC and HadAT2 are in  excellent agreement regarding the amplification at the 850 hPa level.
4.  The amplification of the next two levels (700 and 500 hPa) are quite similar.  The higher level (500 hPa) has slightly greater amplification than the lower.  Again, both datasets (RATPAC and HadAT2) agree very closely. This is supported  by the UAH MSU satellite data, which agrees with the 500 hPa level of both the  other datasets.
5. The amplification of the 300 and 200 hPa levels are  also quite similar. The amplification of the higher level (200 hPa) exceeds that  of the next lower level in the early part of the record. However, the 200 hPa  amplification decreases over time more than that of the lower level (300 hPa).  This accelerated long-term decay in amplification is also seen in the 150 and  100 hPa levels.
6. Between 700 and 200 hPa, amplification rises to a peak  at around 8-10 years, and declines after that.
Observations and  Models

Because it is the most detailed of the observational records,  I will take the HadAT2 as a comparison standard. Fig. 3(a) shows the full length  (50 year) HadAT2 record. Figures 3(b) to (e) show the outputs from five climate  models. These models were selected at random. They are simply the first five  models I found to investigate.
In Fig. 3(a) the decline in the observed  amplification at the 200 hPa level seen in the shorter 30 year record in Fig.  1(c) continues unabated to the end of the 50 year record. The 200 hPa  amplification crosses over the 300 hPa level and keeps declining. The models in  Fig. 3(b-f), however, show something completely different.


 Figure 3. Three month to fifty year  amplification for HadAT2 and for the output of various computer  models.
The model results shown in Figs  3(b) to (e) were quite unexpected. It was not a surprise that the models  disagreed with observations. It was a surprise that the model results varied so  widely among themselves. The atmospheric amplification at the various pressure  levels are very different in each of the models
In the observations, the  greatest amplification is at 200 hPa at around eight to ten years. Only one of  the models, the GISSE-R, Fig. 3(d), reproduces that slow buildup of  amplification. Unfortunately, the model amplification continues to increase from  there on to the end of the record, the opposite of the observations.
The  observed amplification at all levels except 850 hPa peaks at 8-10 years and then  decreases over time. This again is opposite to the models. Amplification in all  levels above 850 hPa of all of the models either stay level, or they increase  over time.
In the observations, amplification increases steadily with  altitude from 850 hPa to 200 hPa. Not a single model showed that result. All of  the models examined showed one or more inversions, instead of the expected  steady increase in amplification with altitude shown in the  observations.
The 850 hPa observations start slightly below 1, and  gradually increase to 1. Only one model, BCCR (c), correctly reproduced this  lowest level.
Variability of Observations and  Models

To investigate the natural variability in  the amplification of both observations and models, I looked at thirty-year  subsets of the various 50-year datasets. Figure 4 shows the amplification of  thirty-year subsets of the datasets and model output. This shows how much  variability there is in thirty year records. I show subsets taken at 32 month  intervals, with the earliest ones at the back of the stack.
Once again,  there are some conclusions that can be drawn from first looking at the  observations, which are shown in Fig 4 (a).
1. The relationship between  the various layers is maintained in all of the subsets. The lowest level (850  hPa) is always at the bottom. It is invariably below (smaller amplification  than) the rest of the levels up to 200 hPa. The 700/500 hPa pair are always very  close, with the higher almost always having the greater amplification. The 200  hPa level is above the 300 hPa level for all of the early part of the record.  This order, with amplification steadily increasing with altitude up to 200 hPa,  holds true for every one of the thirty-year subsets of the  observations.
2. The 700 hPa amplification is never less than the 850 hPa  amplification. As one goes lower, so does the other. They cross only at the  short end of the time scale.
3. At all but the 850 hPa level,  amplification peaks at somewhere around 8-10 years, and subsequently generally  declines from that peak.
4. The amplification at 200 hPa is much larger  than at 300 hPa at short (decade) timescales, but decreases faster than the 300  hPa amplification.
5. There is a surprising amount of variation in these  thirty-year overlapping subsets. This implies that the satellite record is still  too short to provide more than a snapshot of the variation in  amplification.

Figure 4. Evolution of amplification in thirty year subsets of  fifty-year datasets. The interval between subsets is 32 months. Fig. 4(a) is  observations (HadAT2). The rest are model hindcasts.
The most obvious  difference between the models and the observations is that most of the models  have much less variability than the observations.
The next apparent  difference is that the amplification in the models trend level or upwards with  time, while the observed amplifications generally trend downwards.
Next,  the pairings of levels are different. The only model which has the same pairings  as the observations (700/500 and 300/200 hPa) is the HadCM3 model. And even in  that model, both pairs are reversed from the observations. The other models have  700 hPa paired with (and often below) the 850 hPa level.
There is a  final oddity in the model results. The short term (say four year) amplification  at 200 hPa is very different in the various models. But at thirty years the  models converge to a range of around 1.5 to 1.9 or so. This has led to a  mistaken idea that the models reveal a “robust” long term amplification.

DISCUSSION
Having  examined the changes in amplification over time for both observations and  models, let us return to re-examine, one by one, the issues involved as stated  at the beginning:
The month-to-month variability of tropical  temperatures
is larger in the troposphere than at the Earth’s  surface.
 
This is clearly  true. There is an obvious tropical “hot spot” of high amplification in the upper  troposphere. It peaks at a pressure altitude of 200 hPa at about 8-10  years.


This amplification behavior is similar in a  range of
observations and climate model simulations, and is
consistent  with basic theory.


This  amplification behavior is in fact similar in a range of observations. However,  it is very dissimilar in a range of climate model simulations. While the  observations are consistent with basic theory (amplification increasing with  altitude up to 200 hPa), the climate models are inconsistent with that basic  theory (higher levels often have lower amplitude than lower  levels).
On multi-decadal timescales,
tropospheric  amplification of surface warming is a robust
feature of model simulations,  but occurs in only one
observational dataset [the RSS dataset].


There are no “robust” features of  amplification in the model simulations. They have very little in common. They  are all very different from each other.
Multi-decadal amplification  decreases gradually over the 50-year observational record. Three of the  observation datasets (UAH, RATPAC, and HadAT2) all agree with each other in this  regard. The RSS dataset is the outlier among the observations, staying level  over time. This RSS behavior is similar to several of the models, which also  stay level over time. One possible explanation of the RSS difference is that I  understand it uses computer climate modeling to inform a portion of its analysis  of the underlying MSU data.
Other observations show weak or
even  negative amplification.


None of the  tropical observational datasets above 850 hPa show “negative amplification”  (amplification less than 1) at timescales longer than a few years. On the other  hand, all of the observational datasets show negative amplification at short  timescales, as do all of the models.
These results suggest that
either  different physical mechanisms control
amplification processes on monthly and  decadal
timescales, and models fail to capture such behavior, or
(more  plausibly) that residual errors in several
observational datasets used here  affect their
representation of long-term  trends.


The problem is not that the observations  fail to capture the long term trends. It is that every model disagrees with  every other model, as well as disagreeing with the observations.
It  appears that different physical mechanisms do indeed control the amplification  at different timescales. The models match the observations in part of this, in  that amplification starts out low and then rises to a peak over a period of  years. In most of the models examined to date, however, this happens on a much  shorter time scale (months to a few years) compared with observations (8-10  years).
However, the models seem to be missing a longer-term mechanism  which leads to long-term decrease in amplification. I suspect that the problem  is related to the steady racheting up of the model temperature by CO2, which  increases the longer term amplification and leads to upward trends. Whatever the  cause may be, however, that behaviour is not seen in the observations, which  decrease over time.

Conclusions

1. Temporal evolution of  amplification appears to be a sensitive metric of the state of the atmosphere.  It shows similar variations in the various balloon and satellite datasets with  the exception of the RSS MSU satellite temperature dataset.
2. The wide  difference between the individual models was unexpected. It was also surprising  that none of them show steadily increasing amplification with altitude up to 200  hPa, as basic theory suggests and observations confirm Instead, levels are often  flipped, with higher levels (below 200 hPa) having less amplification than lower  levels.
3. From this, it appears that we have posed the wrong question  regarding the comparison of models and data. The question is not why the  observations do not show amplification at long timescales. The real question is  why model amplification is different, both from observations and from other  models, at almost all timescales.
4. Even in scientific disciplines which  are well understood, taking the position when models disagree with observations  that “more plausibly” the observations are incorrect is adventurous. In climate  science, on the other hand, it is downright risky. We do not know enough about  either the climate or the models to make that claim.
5. Observationally,  amplification varies even at “climate length” time scales. The thirty year  subsets of the observations showed large changes over time. Climate is ponderous  and never at rest. Clearly there are amplification cycles and/or variations at  play with long timescales.
6. While at first blush this analysis only  applies to temperatures in the tropical troposphere, it would not be surprising  to find this same kind of amplification behavior (different amplification at  different timescales) in other natural phenomena. The concept of amplification,  for example, is used in “adjusting” temperature records based on nearby  stations. However, if the relationship (amplification) between the stations  varies based on the time span of the observations, this method could likely be  improved upon.

Additional Information
 The Supplementary  Online Material contains an analysis of amplification in the AMSU satellite  dataset and the NCEP Reanalysis dataset. It also contains the data, the data  sources, notes on the mathematical methods used, and the R function and R  program used to do the analyses and to create the graphics used in this  paper.

References

Douglass DH, Christy JR, Pearson BD,  Singer SF. 2007. A comparison of tropical temperature trends with model  predictions. International Journal of Climatology 27:  Doi:10.1002/joc.1651.
Santer BD, et al. 2005. Amplification of surface  temperature trends and variability in the tropical atmosphere. Science 309:  1551–1556.
Santer BD et al. 2008. Consistency of modelled and observed  temperature trends in the tropical troposphere. Int. J. Climatol. 28:  1703–1722
SUPPLEMENTARY ONLINE MATERIAL
 
SOM Section 1. Further  investigations.


AMSU  dataset
 A separate dataset from a single AMSU (Advanced Microwave  Sounding Unit) on the NOAA-15 satellite is maintained at  <http://discover.itsc.uah.edu/amsutemps/>. Although the dataset is short,  it has the advantage of not having any splices in the record. The amplification  of that dataset is shown in Fig. S2.

Figure S-1  Short-term Global Amplification of AMSU satellite data, MSU data, and HadAT2  data . The dataset is only ten years long.
Figure S-1(a). AMSU satellite data. This is a curious  dataset. The 400 hPa level is a clear and dubious outlier. The distinctive  “duck’s head facing right” shape of the second half of the 900 and 600 hPa  levels is similar, while that of the 400 hPa level is quite different. It is  very doubtful that one level would be that different from the levels above and  below it. This was a strong indication of some unknown error with the 400 hPa  data that is affecting the longer term amplification.

Figure S-1(b). Adjusted and unadjusted AMSU satellite data. To attempt to correct this error, I added a simple linear trend to the 400  hPa level. I did not adjust it until the amplification was right. Instead, I  adjusted it until its long-term trend ended up proportionally between the  long-term trends of the layers above and below. This gave the adjusted version  (light green) of the amplification.

Figure  S-1(c). Adjusted AMSU satellite data. After the adjustment of the 400 hPa  trend, the amplifications fit together well. Curiously, despite being adjusted  by a linear trend, the shape of the latter half of the 400 hPa level has  changed. Now it has the “duck’s head facing right” shape of the 600 and 900 hPa  levels. This unexpected change supports the idea that there is indeed an error  in the trend of the 400 hPa data.

Figure  S-1(d). Interpolated AMSU satellite data. Unfortunately, the referenced  levels in the two datasets are at different pressure altitudes than HadAT2. To  compare the AMSU to HadAT2, we need to interpolate. Fortunately, the HadAT2  dataset range (850 to 200) fits neatly inside the AMSU range (900 to 150). This,  along with the similar and close nature of the signals at various levels, allows  for linear interpolation to give the equivalent AMSU amplification at the HadAT2  levels. The interpolated version is shown.
Figure S-1(e). HadAT2 and  RSS/UAH satellite data. The global observational data over this short period  (ten years) is scattered. Also, the HadAT2 data has a more jagged and variable  shape. We may be seeing the effects of the paucity of the observations. In the  short-term (this is ten years and less) the RSS and UAH amplification records  are quite similar. As before, both are close to the 500 hPa HadAT2  amplification.
Figure S-1(f). AMSU and HadAT2 data. Close, but not  a good match. The 200, 700, and 850 hPa levels match, but 300 and 500 hPa are  quite different. Overall, the satellite data seems more reliable. The AMSU data  shows a gradual change in amplification with altitude. The HadAT2 data is  bunched and sometimes inverted.
My conclusions from the AMSU dataset  are:
1. It contains an error, which appears to be a linear trend error,  in the 400 hPa level.
2. Other than that, it is the most internally  coherent of the observational datasets.
3. It points up the weakness of  using short (one decade) subsets of the HadAT2  dataset.
NCEP REANALYSIS

One of the attempts to  provide a spatially and temporally complete global dataset despite having  limited observational data is the NCEP reanalysis dataset. Figure S-2 compares  the temporal evolution of amplification of HadAT2 and NCEP Reanalysis  output

Figure S-2 Evolution of amplification of HadAt2 and NCAR. Left column  is amplification from 3 months to 50 years, right column is amplification of 30  year subsets of the 50 year datasets. The interval between the individual  realizations in the right column is 32 months.
The NCEP reanalysis  data in Fig. S-2 (b) shows a fascinating pattern. The three middle levels  (700,500, and 300 hPa) are close to the HadAT2 observations. The 300 hPa levels  agree extremely well. And while the 700 and 500 hPa levels are flipped in NCEP,  still they are in the right location and are very close to the observed  values.
But at the same time, the amplification of the lowest and highest  levels are way off the rails. The 850 hPa amplification starts at 1, and just  keeps rising. And the 200 hPa amplification starts out reasonably, but then  takes a big jump with a peak around thirty years. That seems  doubtful.
The observation that there are problems at the lowest and  highest levels is reinforced by the analysis of the variation of thirty year  subsets in the right column of Fig. S-2. These show the 200 hPa amplification  varying wildly over all of the different 30 year datasets. In one of the thirty  year subsets the 200 hPa amplification dips down to almost touch the highest 850  hPa line. There is clearly something wrong with the NCEP output at the 200 hPa  level.
In addition, in the full NCEP record shown in Fig. S2(b) and all  of the 30 year subsets shown in Fig. S2(d), the lowest level (850 hPa) increases  steadily over time. After about 20 years it has more amplification than either  of the 700 and 500 hPa levels. This behavior is not seen in either the  observations or any of the models.

Conclusions  from the NCEP reanalysis

1. The 700, 500, and 300 hPa level of the  NCEP reanalysis are accurate. The 850 and 200 hPa levels suffer from large  problems of unknown origin.
2. Use of the NCEP reanalysis in other work  seems inadvisable until the 850 and 200 hPa amplification problems are  resolved.

SOM Section 2. Data Sources

KNMI was the source for much of  the data. It has a wide range of monthly data and model results that you can  subset in various ways. Start at http://climexpknmi.nl/selectfield_co2.cgi?someone@somewhere . It contains both Hadley and UAH data, as well as a few model atmospheric  results. My thanks to Geert for his excellent site.
 
Surface data for all  observational datasets is from the CruTEM dataset at http://climexp.knmi.nl/data/icrutem3_hadsst2_0-360E_-20-20N_n.dat
UAH  data is at http://www.nsstc.uah.edu/data/msu/t2lt/uahncdc.lt
RSS  data is at http://www.remss.com/data/msu/monthly_time_series/
HadAT2  balloon data is at http://hadobs.metoffice.com/hadat/hadat2/hadat2_monthly_tropical.txt
CGCM3.1  model atmospheric data is at http://sparc.seos.uvic.ca/data/cgcm3/cgcm3.shtml in the form of a large (250Mb) NC file.
Data for all other models is from  the “ta” and “tas” datasets from the WCRP CMIP3 multi-model database at  <https://esg.llnl.gov:8443/home/publicHomePage.do>
In particular,  the datafiles used were:
GISSE-R: ta_A1.GISS1.20C3M.run1.nc, and  tas_A1.GISS1.20C3M.run1.nc
HadCME:  ta_A1_1950_Jan_to_1999_Dec.HadCM3.20c3m.run1.nc, and  tas_A1.HadCM3.20c3m.run1.nc
BCCR: ta_A1_2.bccr_bcm2.0.nc, and  tas_A1_2.bccr_bcm2.0.nc
INCM3: ta_A1.inmcm3.nc, and  tas_A1.inmcm3.nc

As all of  these are very large (1/4 to 1/2 a gigabyte) files, I have not included them in  the online data. Instead, I have extracted the data of interest and saved this  much smaller file with the rest of the online data.

SOM Section 3. Notes on the Function and Code.

The main function  that does the calculations and created the graphics is called “amp”.
 
The  input variables to the function, along with their default values are as follows:
datablock=NA : the input data for the function. The function requires  the data to be in matrix form. By default the date is in the first column, the  surface data in the second column, and the atmospheric data in any remaining  columns. If the data is arranged in this way, no other variables are required  The function can be called as amp(somedata), as all other variables have  defaults.
sourcecols=2 : if the surface data is in some column other  than #2, specify the column here
datacols=c(3:ncol(datablock)) : this is  the default position for the atmospheric data, from column three  onwards.
startrow=1 : if you wish to use some start row other than 1,  specify it here.
endrow=nrow(datablock) : if you wish to use some end row  other than the last datablock row, specify it here.
newplot=TRUE :  boolean, “TRUE” indicates that the data will be plotted on a new blank  chart
colors=NA : by default, the function gives a rainbow of colors.  Specify other colors here as necessary.
plotb=-2 : the value at the  bottom of the plot
plott=2 : the value at the top of the  plot
periods_per_year=12 : twelve for monthly data, four for quarterly  data, one for annual data
plottitle=”Temporal Evolution of Amplification”  : the value of the plot title
plotsub=”Various Data” : the value of the  plot subtitle
plotxlab=”Time Interval (months)” : label for the x  values
plotylab=”Amplification” : label for the y  values
linewidth=1 : width of the plot lines
linetype=”solid” :  type of plot lines
drawlegend=TRUE : boolean, whether to draw a legend  for the plot
SOM Section 4. Notes on the Method.

An example  will serve to demonstrate the method used in the “app” function. The function  calculates the amplification column by column. Suppose we want to calculation  the amplification for the following dataset, where “x” is surface temperature,  “y” is say T200, and each row is one month:
x   y
1   4
4   7
3    9
Taking the “x” value, I create the following 3X3 square matrix, with  each succeeding column offset vertically by one row. This probably has some kind  of special matrix name I don’t know, and an easy way to calculate it. I do it by  brute force in the function:
1     4     3
4     3     NA
3    NA     NA
I do the same for the “y” value:
4     7     9
7     9      NA
9    NA    NA
I also create same kind of 3X3 matrix for x times  y, and for x squared.
Then I take the cumulative sums of the columns of  the four matrices. These are used in the standard least squares trend formula to  give a fifth square matrix:
slope of regression line = (N*sum(xy) –  sum(x)*sum(y)) / (N*sum(x^2) – sum(x)^2)


I then  average the rows to give me the average amplification at each  timescale.
This method exhaustively samples to find all contiguous  sub-samples of each given length. This means that there will be extensive  overlap (samples will not be independent). However, despite the lack of  independence, using all available samples improves the accuracy of the method.  This can be appreciated by considering a fifty year dataset. There are a number  of thirty year contiguous subsets of a fifty year dataset, but if you restrict  your analysis to non-overlapping subsets, you only can pick one of them …  which way will give the best estimate of the true 30-year  amplification?
SOM Section 5. Of Averages and Medians.

The  distribution of the short-term amplifications is far from normal. In fact, it is  not particularly normal at any scale. This is because the amplification is  calculated as the slope of a line, and any slope is the result of a division.  When the divisor approaches zero, very large numbers can result. This makes  averages (means) inaccurate, particularly at the shorter time scales.
One  alternative is the median. The problem with the median is that it is not a  continuous function. This limits its accuracy, particularly in small samples. It  also makes for a very ugly stair-step kind of graph.
Frustrated by this,  I devised a continuous Gaussian mean function which outperforms the mean for  some varieties of datasets, and outperforms the median on other datasets. It is  usually in between the mean and median in value. In all datasets I tested, it  equals or outperforms either the mean or the median.
To create this  Gaussian mean I reasoned as follows. Suppose I have three numbers picked at  random from some unknown stable distribution, let’s say they are 1,2, and 17.  What is my best guess as to the actual underlying true center of the  distribution?
Since we don’t know the true distribution, the best guess  as to its shape has to be a normal distribution. With such a distribution, if we  know the standard deviation, we can iteratively calculate the mean.
To  do so, we start by calculating the standard deviation, and picking a number for  the estimated mean, say 5. If that is the mean, the numbers (1,2,17) when  measured in standard deviation units is (-0.6, -0.5,  1.2). Each of those  standard deviations has an associated probability. I figured that the sum of  these individual probabilities is proportional to the probability that the mean  actually is 5.
I then iteratively adjust the estimated mean to maximize  the total probability (the sum of the three individual probabilities. It turns  out that the gaussian mean of (1, 2, 17) calculated by my method is 3.8. This  compares with an average for the three numbers of 6.7, and a median of  2.
In general there is very little difference between my gaussian mean,  the arithmetic mean, and the median. However, it is much better behaved than the  mean in non-normal datasets. And unlike the median, it is a continuous function,  which gives greater accuracy.
All three options are included in the  amp() function, with two of them remarked out, so you can see the effects of  each one. The only noticeable difference is that the mean is not very accurate  at short time scales, and the gaussian mean and the mean are not discontinuous  like the median.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94dcb0fd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Drivers hoping their new plug-in hybrid car will help cut down their carbon footprint may have an unlikely enemy: cold weather. Zero-emissions driving can be impossible for some of the UK’s bestselling plug-in hybrid models when the air is chilly or if passengers do as little as switching on the heating – even if the battery is fully charged. Plug-hybrids allow drivers to switch between battery electric power and an internal combustion engine, delivering significant emissions improvements over conventional cars. However, environmental campaigners have long harboured concerns that plug-in hybrids do not offer the environmental benefits suggested by carmakers’ advertising or regulators’ laboratory tests. All of the UK’s top 11 bestselling plug-in hybrids have limitations on pure electric driving ability. Cold temperatures can trigger the internal combustion engine in Volvo’s XC90 SUV, the Mercedes-Benz E Class executive car, and Kia’s Niro crossover. The Mitsubishi Outlander SUV, the UK’s bestselling plug-in last year, has an “EV” button that switches on “EV priority” mode. However, the internal combustion engine will kick in if the driver switches on the adaptive cruise control, to automatically maintain a safe distance from the car in front, or if the battery gets too hot or too cold in more extreme conditions. Jaguar Land Rover’s Range Rover and Range Rover Sport plug-ins will start their internal combustion engines if more power is required than the electric engine can provide alone, as will Porsche’s Cayenne. There are also speed limits on all-electric driving for BMW’s 2, 3 and 5 Series cars as well its Mini Countryman plug-in. The carmakers all carefully avoid making any incorrect claims on their products’ green credentials in their marketing materials. However, campaigners have criticised the emphasis in many brochures and advertisements on zero-emissions driving capabilities, when these may be difficult to achieve in normal use. Greg Archer, UK director at campaign group Transport & Environment, said one leading carmaker “is conning its customers”, after it was approached by an unhappy owner of a plug-in hybrid. The group, which passed the correspondence to the Guardian, has been highly critical of plug-in hybrids, some of which it labels “fake EVs” because of their continued use of internal combustion engines. “A [plug-in hybrid] is not driving with zero emissions if it switches on its engine when the driver de-mists the windscreen,” Archer said. “This is another example of carmakers attempting to mislead their customers about the real emissions from their car.” Separate data from the Miles Consultancy, which tracks fuel use by companies, found that in real life almost all plug-in hybrid cars failed to achieve the mileage found in lab tests, suggesting that many users do not charge them sufficiently. Updated analysis for the Guardian of 1,388 plug-ins used over eight months found they achieved an average of more than 40 miles per gallon when using a mixture of petrol and electric power, only a third of the 127 miles per gallon advertised by their manufacturers. Last month the government said it plans to ban all hybrids from sale in the UK from 2035 or earlier, signalling that promoting battery electric cars with zero exhaust emissions was its priority. Buyers of plug-in hybrids could also risk being caught by tightening emissions limits in British cities. The prospect of a hybrid ban infuriated carmakers, who say the technology is the only way to cut emissions quickly. The carbon dioxide emissions of cars sold in the UK rose for the third consecutive year in 2019. Mike Hawes, chief executive of the Society of Motor Manufacturers and Traders, said: “Plug-in hybrids are an important and attractive stepping stone for people not yet suited or able to invest in a fully electric vehicle, giving the flexibility of zero-emission miles on urban commutes and extended range for longer, out-of-town journeys. “Drivers also have the peace of mind that the engine will kick in to provide the necessary boost if the battery level falls too low to deliver sufficient power for energy-intensive operations such as high-speed overtaking or windscreen de-icing, thus guaranteeing safety and the most efficient use of energy at all times.” Selling thousands of plug-in hybrids is also a key part of carmakers’ plans to meet tightening emissions limits and avoid heavy fines, and industry analysts expect a hybrid “price war” over the coming year as companies try to shift the cars in large volumes. Companies including Toyota, BMW and Daimler have bet heavily on hybrid technologies. “The vast majority of owners we surveyed use their Outlander [plug-in hybrid] as it was engineered and are enjoying a lower carbon footprint and lower running costs as a result,” a spokeswoman for Mitsubishi said."
nan
"
Share this...FacebookTwitterHere is a sampling of the media reaction coming from Germany on CERN’s cosmic ray cloud seeding experiment.
Normally the German mainstream media is quick to report on new scientific developments, especially anything indicating catastrophic global warming. But this time they have been slow and cautious.
FOCUS magazine online starts with:
Climate skeptics doubt that man-made greenhouse gases are to blame for global warming. A new study appears to confirm their claims.”
FOCUS, in its comprehensive 6-part piece, goes on to concede that the questions behind the causes of global warming are far from being answered. FOCUS tries to play down the CERN results, and so resorts to quoting 2 hardline warmists, Jochem Marotzke, Director of the Hamburg Max Planck Institute for Meteorology and Stefan Rahmstorf of the Potsdam Institute for Climate Impact Research (2 institutes that would be pretty much be out of business if the cosmic ray theory proved right).
‘The mechanism is plausible, but it is not quantitatively enough to explain the observed warming’, assessed Jochem Marotzke. Stefan Rahmstorf completely threw out the the idea recently: ‘Cosmic rays have been measured since 1953; they show no increasing or decreasing trend analog to solar brightness. Without such a trend, one can also explain no change in cloud cover,’ he insists.”
Die Welt writes:
And: Which role do clouds play in all this?  ‘Cloud’ has found the first preliminary answers to that. The sun could play a bigger role than first thought. “Could!”, emphasizes Kirkby und Curtius.
Actually Danish scientist Henrik Svensmark found the preliminary answers, and CLOUD simply added a huge dose of confirmation. And funny how Die Welt in the past never emphasized the word “could” in its numerous articles on kook warmist scenarios, but is quick to do so here.
Die Welt also quotes Kirby who compares two charts: one of temperature vs solar activity and Mann’s hockey stick chart. Die Welt writes:
‘Look here’, he said ‘at how striking the correlation between solar activity and global temperature has been over the last 1000 years.’  In comparison another chart stands right next to it, the famous ‘Hockey Stick Chart’, which suggests that there was hardly any climate fluctuation over the last 1000 years and that a sharp rise began only 150 years ago. That would mean: Only man drives the climate, and the sun not. “It turned out to be false”, he [Kirby] said.”
Indeed Mann’s view of the past 1000 years is looking more and more like a fairy tale of epic proportions. Die Welt did find space to mention Svensmark:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Also Danish physicist Henrik Svensmark is working on the interaction between solar activity and climate change. ‘Although our experiments were not very complex, we had similar results three months ago’.”
Yes, Svensmark produced good data – on a shoestring budget. Compare his cost-effectiveness to the countless tens of billions of dollars wasted on the loads of useless junk science produced by warmists. A few million dollars are proving the $100+ billion wrong.
Finally Die Welt writes about the chief of CERN and his controversial request of last July of “not to interpret the results”.
Curtius denies having received such a request. ‘Of course we have to interpret our results’, he said, “otherwise other people will do it’.”
German skeptics slam German stubbornness
Skeptic blogs and sites have been blasting German stubbornness and warming dogmatism, and their refusal to acknowledge CERN’s results. Science journalist Edgar Gärtner at his blog slammed obstinate German science, writing a piece called: Cloud Experiment Exposes Climate Swindle: He writes:
Already 200 years ago it was detected by famous English astronomer William Herschel that the price of bread always increased when the number of sunspots was very low. Svensmark believed he could explain why it was so. But when he published his hypothesis together with his boss Eigil Friis-Christensen, then IPCC Chairman Bert Bolin called them ‘naive and irresponsible’.
Funny how who’s turning out to be “naive and irresponsible”. Gärtner also writes:
One can suspect that the results contain political dynamite, also when taken alone they do not suffice to bring down the greenhouse gas house-of-lies. But in combination with the recently published NASA satellite measurements, which we reported on not long ago, it could very well happen. These measurements have clearly shown that a man-made heat trap in the atmosphere just cannot be. The temperature increase recorded over the earth’s land mass at the end of the last century, which in the meantime has stagnated, has to be attributed to other causes. The successfully completed Cloud experiment on fluctuating solar activity offers a solution. The trillions of euros that the EU wants to have for fighting the supposedly man-made climate change are purely for nothing.”
So could it bring down the “greenhouse gas house-of-lies” as Gärtner suggests? Don’t bet on it.
AGW is now fatally embedded in all German institutions
It’s going to take Germany a long time to wake up from it’s global warming science folly, if at all. The prospects are poor. All of Germany’s major institutions like the public media, government, political parties, science bodies (such as the PIK, German Weather Service, Max Planck Institute), schools, etc., all have negligently and wrecklessly embedded the global warming dogma deep and firmly into their structures and psyche, thus making it the main pillar on which the architecture of Germany’s future society will rest. Now that pillar is cracking and crumbling. Suddenly Germany’s grand plans for a Green Empire are facing the scrap heap.
Will German leaders be able to come to terms with that? They seemingly (and stupidly) have gone beyond a point of no return with their zeal. They’re pushing their heads deeper into the sand. Expect them to get shriller. Germany is stuck in a self-made dilemma. Once a brand of science gets institutionalized nationally, and to the extent that it has in Germany, it is very very difficult – if not impossible – to remove. Germany has tragically been though something eerily similar before. Will a complete demise be the only way out? Germany has a way of hanging in – all the way to the bitter end.
Share this...FacebookTwitter "
"A warm winter means that for the first time in years Germany’s vineyards will produce no ice wine, an expensive golden nectar made from grapes left to freeze on the vine. The German Wine Institute said on Sunday that temperatures had not dropped to the prerequisite low of -7C (19F) in any of the country’s wine regions.  A succession of warm winters have reduced ice wine production in recent years, the wine industry’s marketing arm said. Only seven producers managed to make it in 2017, and only five in 2013. It did not say how far back records went. “If warm winters become more frequent over the coming years, ice wines from Germany’s regions will will soon become an even more expensive rarity than they already are,” said Ernst Büscher, a spokesman for the institute. Freezing the grapes before they are crushed concentrates the sugar and leads to an intensely sweet wine often served with dessert. It has always been a niche product, accounting for about 0.1% of German production, and the low volumes make it expensive. Making ice wine is a tricky business. Workers must race into the vineyards to harvest the grapes with only a few hours notice when the temperature falls, often at night or in the early morning. The grapes have to be pressed while still frozen, so the winemakers work in unheated facilities. Vineyard owners also face the risk that grapes set aside for ice wine will rot on the vine before the temperature drops far enough. Canada’s Niagara peninsula is one of several other places where ice wine is produced, thanks to its cold winters. It is also made in the US in northern Michigan and Ashtabula county, Ohio, near Lake Erie. Major markets for German ice wine include Japan, China, Scandinavia and the US, the institute said.  "
"A clear and growing majority of Coalition voters support the Morrison government adopting a net zero target for 2050, with support for that proposition climbing 12 points in a month, according to the latest Guardian Essential poll. The latest fortnightly survey shows a majority of Australian voters support net zero either strongly or somewhat (75%, up four points in a month), and 68% of Coalition voters in the sample hold that positive view. Last month, the proportion of supportive Coalition voters was 56%. The decisive shift in positive sentiment from Coalition voters follows calls from within the government to consider the 2050 target, and Labor’s decision late last week to sign on to net zero – confirmation that has reignited the climate wars in Canberra. While the Morrison government is leaving its options open on a 2050 target, its current messaging is suggestive of substituting a technology roadmap for a target. The debate about net zero dominated parliament on Monday. The Coalition is blasting Labor for adopting the target in the absence of a fleshed-out plan to get there, while Labor and the Sydney independent MP Zali Steggall are pressuring the government to detail the impact of failing to act to prevent dangerous global heating. More than 70 countries and 398 cities say they have adopted a net zero position. Every Australian state has signed up to net zero emissions by 2050, and these commitments are expressed either as targets or aspirational goals. Net zero is also supported by large companies and by their lobby group, the Business Council of Australia. Given the resumption of partisan brawling about climate policy and tensions within both of the major parties about the future of coal, voters in the Guardian Essential survey were asked a number of questions about their attitudes to a transition to low emissions. A majority of voters in the survey (75%) believe improvements in renewable energy means it will become less necessary to burn coal for electricity, and 65% say both advances in technology and global agreements on emissions reduction will result in coal becoming uneconomical to extract in the future. A majority (64%) says if Australia is serious about climate action, we will need to get out of coal as soon as possible. All of those propositions attract majority support amongst Coalition voters (70%, 60%, 54%). But a majority in the sample (61%, and 72% of Coalition voters) also say Australia should continue to export coal for steel production, even if we stop exporting coal for use in power plants. The survey also points to divided sentiment between the city and regional areas. For example, city dwellers are more likely to agree with a statement that if Australia is serious about the climate emergency we need to exit coal (67%) than people in the regions (56%). Voters were also asked about the future of coal-fired power plants, given the Morrison government has allocated $4m for a feasibility study examining a new facility at Collinsville. Also in prospect is taxpayer underwriting of coal – a development championed by Nationals but criticised by some moderate Liberals. Just under half the sample (47%) say coal plants should continue to operate as long as they are profitable, but the industry should not be subsidised or expanded. Coalition voters are more comfortable than other voting cohorts with subsidies. Greens voters are the most supportive of moves to shut down the coal industry (62%) with that position endorsed by 36% of Labor voters and 21% of Coalition voters in the sample. As well as the climate questions, voters were asked about the coronavirus, with experts saying the world is fast approaching a tipping point in the spread of the illness. Just under one-third of survey respondents have changed their behaviour in some way to try to avoid contracting the virus – either avoiding restaurants and shopping centres, or cancelling an overseas trip. While 70% of people in the survey say their personal behaviour has been business as usual, four out of five respondents agree that because of global movements in people, humanity is more vulnerable to the spread of viruses (81%). There is also strong support (80%) for the travel ban that prevents Chinese visitors entering Australia, and only 20% say the border with China should remain open to protect revenue from tourism and overseas students. Both the Morrison government and the media get the thumbs up from the sample for managing the risks and reporting the latest developments. There has been controversy post-election about the reliability of opinion polling, as none of the major surveys – Newspoll, Ipsos, Galaxy or Essential – correctly predicted a Coalition victory last May. The polls instead projected Labor in front on a two-party-preferred vote of 51-49 and 52-48. The lack of precision in the polling has prompted public reflection at Essential, as has been flagged by its executive director, Peter Lewis. Guardian Australia is not now publishing measurements of primary votes or a two-party-preferred calculation, but is continuing to publish survey results of responses to questions about the leaders and a range of policy issues. The poll’s margin of error is plus or minus 3%. The sample size this fortnight is 1,090 respondents."
"

Rather than wait on the market to demand more fuel efficient trucks, President Obama, bypassing Congress, has directed the Environmental Protection Agency to draw up a new round of regulations raising the fuel efficiency standards on heavy‐​duty trucks. He promises that this will save billions of dollars in fuel costs, lower prices and reduce greenhouse gas emissions—or, as he describes it, a “win‐​win‐​win” situation.   
  
  
Thank you, Mr. President for taking such good care of us.   
  
  
Apparently, we are too stupid to have realized the manifold benefits of this chain of events ourselves.   
  
  
Or is it that we realize these actions will have no impact of climate change and will probably result in higher prices for new trucks and everything that they transport?   
  
  
You can use our “Handy‐​Dandy Temperature Change Calculator” to see that, using the EPA’s own computer model, if Americans cut _all_ of our carbon dioxide emissions to _zero, today_ , the amount of warming that would be prevented (assuming a warming forecast that is itself probably too high) by 2100 is around two‐​tenths of a degree—an amount that would be virtually impossible to measure against natural climate variability. Increasing the fuel efficiency of heavy trucks would have considerably less of an effect than cutting all carbon dioxide emissions and would simply not be discernible in climate data.   
  
  
And the President’s claim that increasing the fuel efficiency will lower the price of all things neglects the fact that we simply do not know what technology would accomplish this end.   
  
  
Perhaps he should have said—“if you like your truck, you can keep your truck,” that is, until you have to replace it with something that will cost much more than you would have otherwise purchased and not do what it is supposed to do.   
  
  
Again, thank you, Mr. President.
"
"

The media is increasingly embracing the idea that anyone in the scientific community who doesn’t wet their bed over the prospect of future warming is some sort of (a) flat‐​earth know‐​nothing, or or (b) a cynical money grubber who allows oil and coal companies to buy their expertise despite knowing full well that doom is on the horizon.   
  
  
Well, today you can judge for yourself. At a conference co‐​sponsored by the Western Business Roundtable and the Business Industry Political Action Committee (BIPAC), Cato senior fellow Patrick J. Michaels (who, more relevantly, is a professor of environmental science at the University of Virginia and a member of the International Panel on Climate Change) will debate Klaus Lackner, a professor of geophysics at the Earth Sciences center at Columbia University. The debate begins at 1:30 Mountain Standard Time and will be webcast live for all interested. If you count yourself among them, you can go sign up here to listen. 
"
"With veganism on the rise and entire supermarket aisles now dedicated to veggie and vegan food ranges, it’s a good time to consider what motivates people to go vegan.  There are many reasons why people decide to cut animal products from their diet, but the negative health effects of excessive meat and dairy consumption and the enormous environmental impacts of industrial agriculture are popular ones. However, the suffering of billions of animals each year in factory farming, referred to in a 2015 Guardian article as one of the “worst crimes in history”, is the most powerful motivation for many, including myself.  Refraining from something that causes so much harm and suffering is laudable, but there’s one argument occasionally used in vegan and animal rights campaigns that warrants closer attention – the idea that consuming other creatures is morally wrong in its own right.  Such views are often bolstered by powerful moral arguments framing animals as subjects of a life, able to experience pain, and as leaders of complex emotional lives. Opposing meat eating on ontological grounds – meaning, simply because animals are sentient beings, we shouldn’t eat them – separates humans from nature and prevents truly ethical relationships between humans, animals and the natural world. The late environmental philosopher Val Plumwood coined “ontological veganism” to describe this absolute opposition. Ontological veganism asserts that beings that count as ethical subjects should not be eaten, in the same way that there’s a widespread taboo about eating humans. While this thinking erects another unhelpful boundary between animals and other life forms, it’s also ironic that the rationale underlying taboos against eating humans is the desire to radically separate humans from other animals. By framing the consumption of other living beings as an inherent moral wrong, ontological veganism also risks demonising predation. In order to avoid this, a common approach is to “excuse” animal predation by arguing that the latter is part of “nature” while humans, as cultural beings, should be exempt. Some of us – especially those living in wealthy countries – can indeed choose to opt for vegan products, but this argument reproduces another false dichotomy: nature vs. culture. Life is entanglement, with no clear boundaries between “humans” and other species, or between “nature” and “society”.  Come among the deer on the hill, the fish in the river, the quail in the meadows. You can take them, you can eat them, like you they are food. They are with you, not for you. This quote is from the late utopian author Ursula Le Guin, in her novel Always Coming Home. Her idea is akin to Plumwood’s theory of ecological animalism, which seeks to replace human supremacy over nature with mutual and respectful use between humans and other species. Ontological veganism would frame using or consuming animals itself as inherently exploitative. But consider forms of mutual use seen in symbiotic relationships, such as those between pollinating insects and plants. In such scenarios, use isn’t oppressive or exploitative. It’s the form of use seen within industrial capitalism, where humans and non-humans alike are treated only as a means to an end, that prevents ethical relationships. Ecosystems and all living beings depend upon mutual use and consumption. Orcas consume fish and other marine mammals, we must consume living vegetable matter at least, and when we die, we become food for a host of microorganisms, nourishing them in turn.  If humans are indeed animals who differ from other species only by degrees rather than kind, then like them, we are food. To deny this is to deny that humans are embedded within the ecosystems they originate from and are sustained by. The horrific cruelty involved in industrial factory farming reduces living beings to mere profitable commodities. This is why I am a vegan, and it is here where calls for eradicating or at least reforming animal agriculture find firmer ground. The ways in which animals are currently treated in agriculture represent the exact opposite of respect and mutuality. No wonder Aldous Huxley observed in his poignant ecotopian work, Island, that For animals… Satan, quite obviously, is Homo sapiens. Ecological animalism offers a powerful basis for truly ethical and egalitarian ways of relating to other species. We are all food, and crucially, so much more. We are with and not for one another, and we are all worthy of respect. Go vegan whenever and wherever possible, but be mindful of the underlying rationales involved, lest we reproduce the same harmful dualisms we want to dismantle. More on evidence-based articles about veganism and diets: Vegan diet: how your body changes from day one Why aren’t more people vegetarian? Five ways to encourage people to reduce their meat intake – without them even realising"
"As temperatures at Kew Gardens soared past 21℃, February 26 2019 became the UK’s warmest winter day on record. That same day, a number of wildfires broke out in several different parts of the UK and Ireland – there was a substantial blaze on Marsden Moor, Yorkshire, gorse fires on Arthur’s Seat overlooking Edinburgh, and in the Dublin Mountains, and two separate fires in Ashdown Forest, East Sussex. Given the unusual weather, and the unusual winter fires, an obvious question is: has human-caused climate change played a role? I research the impact of climate change and look at questions exactly like this, so I’m well aware these things are complicated and need proper study – it’s not possible to give a simple yes or no answer immediately. But we can make a first assessment based on general understanding. A first thing to clarify is exactly what we are talking about – increased risk of fire due to the environmental conditions, or the actual occurrence of fires. The fires themselves were almost certainly started by people in some way or other, either deliberately or, more likely, accidentally (lightning fires are rare and there were no storms). But the fact that so many fires took hold and spread at the same time is a clear indication that the environmental conditions were conducive to fire, and that’s what I’ll focus on here. By “environmental conditions”, I mean both the weather at the time of the fire, and the state of the dead leaves, twigs and branches, and in some locations, peat, that  fuel the fire. The amount of fuel and its dryness is crucial, and depends not just on the weather in the preceding days, but also the weeks and months before.  Land management methods including peatland drainage can also play a role. The warm conditions in the days up to Feb 26 were certainly very unusual for this time of year. There was high pressure centred over central Europe and the British Isles, which not only brought settled, dry conditions beneath it but also featured winds moving clockwise around it. This meant the UK and Ireland, on the western edge of the high pressure, experienced warmer air from the south. Whether that kind of weather system is becoming more likely due to climate change is a difficult question to answer. However, at this point we can say that when such conditions occur, they are likely to be hotter than they would have been without climate change, as a result of the general warming trend. Research is already planned to establish how much of the hotter, drier conditions were due to climate change. A further question is whether the dry conditions preceding those few days were due to climate change. This includes both last summer and the winter in between.  Climate change is expected to mean hotter, drier summers in the British Isles, and it made the 2018 heatwave 30 times more likely. So if vegetation was made more susceptible to fire by last summer’s heatwave, this would be a link to climate change. Again, this also needs further research. However, while winters are becoming generally milder, scientists also expect more rainfall. So on the face of it, this year’s dry January was different to what we expect from climate change. However, in a complex, variable system such as the weather, it’s important not to just look at averages but also the ups and downs, and also look at combinations of different weather factors and not just individual factors like rain or temperature in isolation. Although we expect winters to become wetter on average, not every winter will be wetter – we’ll still get dry Januaries sometimes. And given the long-term warming trend, we can expect that when we do get winters with lower rainfall, the fuel will dry quicker due to increased evaporation. So while on average the UK and Ireland might expect reduced fire danger in winter, in years when winters are dry and mild, it could be higher. Of course all this needs working through properly, and no doubt will be. Colleagues and I did do a very preliminary study on this for the 1st UK Climate Change Risk Assessment years ago which suggested that annual average fire danger would increase with climate change. This was since backed up by a more detailed study which suggested that wetter conditions would generally reduce the risk of fire in winter, partly offsetting the increased risk in summer in the annual average. However, this is not inconsistent with my argument here – although overall we might expect climate change to reduce the chances of winter fires in most years, in years when winter rainfall bucks the trend and is drier than usual, we may actually see increased fire risk. So on the question of whether these specific fires are linked to human-caused climate change, I’d say “maybe – we need to look into it more”. But on a more general question of whether we should expect more fires in the British Isles as the world continues to heat up, the answer is clear: yes."
"
This is an official NCAR News Release (National Center for Atmospheric Research) Apparently, they have solar forecasting techniques down to a “science”, as boldly demonstrated in this press release. – Anthony
Scientists Issue Unprecedented Forecast of Next Sunspot Cycle
BOULDER—The next sunspot cycle will be 30-50% stronger than the last one and begin as much as a year late, according to a breakthrough forecast using a computer model of solar dynamics developed by scientists at the National Center for Atmospheric Research (NCAR). Predicting the Sun’s cycles accurately, years in advance, will help societies plan for active bouts of solar storms, which can slow satellite orbits, disrupt communications, and bring down power systems.
The scientists have confidence in the forecast because, in a series of test runs, the newly developed model simulated the strength of the past eight solar cycles with more than 98% accuracy. The forecasts are generated, in part, by tracking the subsurface movements of the sunspot remnants of the previous two solar cycles. The team is publishing its forecast in the current issue of Geophysical Research Letters.
“Our model has demonstrated the necessary skill to be used as a forecasting tool,” says NCAR scientist Mausumi Dikpati, the leader of the forecast team at NCAR’s High Altitude Observatory that also includes Peter Gilman and Giuliana de Toma.
Understanding the cycles
The Sun goes through approximately 11-year cycles, from peak storm activity to quiet and back again. Solar scientists have tracked them for some time without being able to predict their relative intensity or timing.




NCAR scientists Mausumi Dikpati (left), Peter Gilman, and Giuliana de Toma examine results from a new computer model of solar dynamics. (Photo by Carlye Calvin, UCAR)



Forecasting the cycle may help society anticipate solar storms, which can disrupt communications and power systems and affect the orbits of satellites. The storms are linked to twisted magnetic fields in the Sun that suddenly snap and release tremendous amounts of energy. They tend to occur near dark regions of concentrated magnetic fields, known as sunspots.
The NCAR team’s computer model, known as the Predictive Flux-transport Dynamo Model, draws on research by NCAR scientists indicating that the evolution of sunspots is caused by a current of plasma, or electrified gas, that circulates between the Sun’s equator and its poles over a period of 17 to 22 years. This current acts like a conveyor belt of sunspots.
The sunspot process begins with tightly concentrated magnetic field lines in the solar convection zone (the outermost layer of the Sun’s interior). The field lines rise to the surface at low latitudes and form bipolar sunspots, which are regions of concentrated magnetic fields. When these sunspots decay, they imprint the moving plasma with a type of magnetic signature. As the plasma nears the poles, it sinks about 200,000 kilometers (124,000 miles) back into the convection zone and starts returning toward the equator at a speed of about one meter (three feet) per second or slower. The increasingly concentrated fields become stretched and twisted by the internal rotation of the Sun as they near the equator, gradually becoming less stable than the surrounding plasma. This eventually causes coiled-up magnetic field lines to rise up, tear through the Sun’s surface, and create new sunspots.
The subsurface plasma flow used in the model has been verified with the relatively new technique of helioseismology, based on observations from both NSF– and NASA–supported instruments. This technique tracks sound waves reverberating inside the Sun to reveal details about the interior, much as a doctor might use an ultrasound to see inside a patient.




NCAR scientists have succeeded in simulating the intensity of the sunspot cycle by developing a new computer model of solar processes. This figure compares observations of the past 12 cycles (above) with model results that closely match the sunspot peaks (below). The intensity level is based on the amount of the Sun’s visible hemisphere with sunspot activity. The NCAR team predicts the next cycle will be 30-50% more intense than the current cycle. (Figure by Mausumi Dikpati, Peter Gilman, and Giuliana de Toma, NCAR.)



Predicting Cycles 24 and 25
The Predictive Flux-transport Dynamo Model is enabling NCAR scientists to predict that the next solar cycle, known as Cycle 24, will produce sunspots across an area slightly larger than 2.5% of the visible surface of the Sun. The scientists expect the cycle to begin in late 2007 or early 2008, which is about 6 to 12 months later than a cycle would normally start. Cycle 24 is likely to reach its peak about 2012.
By analyzing recent solar cycles, the scientists also hope to forecast sunspot activity two solar cycles, or 22 years, into the future. The NCAR team is planning in the next year to issue a forecast of Cycle 25, which will peak in the early 2020s.
“This is a significant breakthrough with important applications, especially for satellite-dependent sectors of society,” explains NCAR scientist Peter Gilman.
The NCAR team received funding from the National Science Foundation and NASA’s Living with a Star program.
IMPORTANT NOTE:
The date of this NCAR News Release is March 6, 2006
Source: http://www.ucar.edu/news/releases/2006/sunspot.shtml
(hat tip to WUWT reader Paul Bleicher)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95a3cab5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterCCS process. (Public domain graphic)
The German media today are reporting on the decision by Germany’s upper house of parliament, the Bundesrat, to reject a proposal to capture and sequester carbon dioxide emitted by power plants by pumping it into the ground, read here in English.
This rejection is a major setback for climate hero Angela Merkel and her silly plans to control GHG emissions.
Merkel’s government was hoping to compress, liquefy and store millions of tons of CO2 underground in a bid “to rescue the planet from “dangerous climate change” at a cost of billions to consumers (CCS is estimated to cost about ($30/ton). Indeed many of Germany’s politicians view the dumping of billions of euros into the ground to lower the global temperature by a few ten thousandths of a degree as a wise investment.
The truth is that many are involved in sweetheart deals with special interests and stand to make a killing. Unfortunately the Bundesrat, stirred by activists, thought the scheme was technically “too dangerous”. Just call it one stupidity killing another. But we’ll take a good decision any way we can – even if it is based on the wrong reasons.
Another reason the draft law was slapped down was because of a clause allowing individual German states to prevent CCS facilities from being built, thus enabling them to shirk their responsibilities.
As bad as nuclear energy!
Environmental kook groups like Greenpeace have been leading the protest against the CCS process, claiming dangerous CO2 “poison gas” poses “incalculable risks” and could explode or contaminate groundwater. Germany’s English-language The Local writes there are “fears of possible explosion-like uncontrolled emissions of the gas” and that “pressurised carbon dioxide storage underground was like nuclear power in that it was uncontrollable and not possible to secure.”
So what is left in Germany? Wind parks are facing mounting protests, burning fossil fuels face growing hurdles, nuclear power is being shut down, bio-fuels such as sun-diesel and ethanol are sinking further into controversy. Germany is boxing itself into a darkroom. Thank God the Eastern European countries are not rushing down the same path of folly. Soon Germany is going to need them to supply power.
Share this...FacebookTwitter "
"

While the social cost of carbon (SCC) is still being mulled over by the Office of Management and Budget, other federal agencies continue to push ahead with using the SCC to help justify their many regulations.   
  
  
The way this works is that for every ton of carbon dioxide (CO2) that any new regulation is supposed to keep from being emitted into the atmosphere, the proposing agency gets about $32 credit to use to offset the costs that the new regulation will generate. This way, new regulations seem less costly—an attractive quality when trying to gain acceptance.   
  
  
The idea is that the damage resulting from future climate changes will be decreased by $32 for every ton of carbon dioxide that is not emitted.   
  
  
There is so much wrong with the way the government arrives at this number that we have argued that the SCC should be tossed out and barred from use in all federal rulemaking. It is far better not to include any value for the SCC cost/​benefit analyses, than to include one which is knowingly improper, inaccurate and misleading.   
  
  
Further, that the federal regulations limiting carbon dioxide emission will have any detectable impact on future climate change is highly debatable. To see for yourself, try out our global warming calculator that lets you select the magnitude of future carbon dioxide emissions reductions as well as which countries participate in your plan. The best that the U.S. can do—even if it were to halt all CO2 emissions now and forever—is to knock off about 0.1°C from the total climate model‐​projected global temperature rise by the year 2100. In other words, U.S. actions are not very effective in limiting future climate change.   
  
  
Apparently, the feds, too, agree that their plethora of proposed regulations will have little impact on carbon dioxide emissions and future climate change. But that doesn’t stop them from issuing them.   
  
  
The passage below is from the proposed rulemaking from the Department of Energy to alter the Energy Conservation Standards for Commercial and Industrial Electric Motors (this is only one of many proposed regulations making this claim):   




The purpose of the SCC estimates presented here is to allow agencies to incorporate the monetized social benefits of reducing CO2 emissions into cost‐​benefit analyses of regulatory actions that have small, or “marginal,” impacts on cumulative global emissions.



In other words, DoE’s regulations won’t have any real impact on global CO2 emissions (and, in that manner, climate change), but nevertheless they’ll take a monetary credit for reduced damages that supposedly will result from the non‐​effective regulations.   
  
  
(I wonder if can try that on my taxes)   
  
  
It seems a bit, uh, cheeky, to take credit for something that you admit won’t happen.   
  
  
But that’s the logic of the federal government for you!
"
"
If you are just joining us, the story is this. After 10 years of data being withheld that would allow true scientific replication, and after dozens of requests for that data, Steve McIntyre of Climate Audit finally was given access to the data from Yamal Peninsula, Russia. He discovered that only 12 trees had been used out of a much larger dataset of tree ring data. When the larger data set was plotted, there is no “hockey stick” of temperature, in fact it goes in the opposite direction. Get your primer here.
Red = 12 hand picked Yamal trees Black = the rest of the Yamal dataset
Now there’s independent confirmation from a study presented at the American Geophysical Union Conference in 2008 that there is no “hockey stick of warming” at Yamal.
The presentation is” Cumulative effects of rapid climate and land-use changes on the
Yamal Peninsula, Russia by D.A. Walker, M.O. Leibman, B.C. Forbes, H.E. Epstein. (click link for PDF)
In the hallway poster for their AGU presentation, they have this graph, with the caption saying a “nearly flat temperature trend” for Yamal, especially for the late 20th century period where the “hockey stick” from those 12 trees emerges:


See the AGU poster here (warning, big 18 MB PDF file)
Here is how they summarize the graph above in the AGU presentation:

Sea ice: -25%
Summer surface temperature: +4%
Maximum NDVI: +3%
None of the trends are significant at p =0.05 because of high interannual variability.

NDVI is the vegetation index.
There’s also an interesting polar sea ice, temperature, and vegetation index trend map that is similar to what Lucy Skywalker recently plotted.
Click for larger image
I’m sure we’ll see an explosion from “Tamino” any minute now to refute this, oh wait, he’s gone on record as saying:
As for Steve McIntyre’s latest: I’m really not that interested. He just doesn’t have the credibility to merit attention. I have way better things to do.
OK then, one less angry, sciency, rant by an anonymous coward who won’t put his name to his own work to worry about. Talk about credibility. Sheesh.
Here is the conclusion Walker et al makes in their AGU presentation

Satellite data suggest that there has been only modest summer land-surface warming and
only slight greening changes across the Yamal during the past 24 years. (Trend is much
stronger in other parts of the Arctic, e.g. Beaufort Sea.)
Kara-Yamal: negative sea ice, positive summer warmth and positive NDVI are correlated
with positive phases of the North Atlantic Oscillation and Arctic Oscillation.

So it seems sea ice extent, the NAO, and the AO are the bigger factors for temperature in Yamal. It also appears that the Arctic is getting slightly more green.
If anyone has access links to the full paper, feel free to post it here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e929ad868',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Many of you have probably heard by now of  the UN. Report saying that “global warming is killing 300,000 people a year”. There’s a Times Online Story (h/t to Gary Boden) about it today that has some startling admissions. Here are some excerpts:
Climate change is already killing 300,000 people a year in a “silent crisis”  that is seriously affecting hundreds of millions more, an influential  humanitarian group warned today.
A report by the Global Humanitarian Forum, led by Kofi Annan, the former UN  Secretary-General, says that the effects of climate change are growing in  such a way that it will have a serious impact on 600 million people, almost  ten per cent of the world’s population, within 20 years. Almost all of these  will be in developing countries.
“Climate change is the greatest emerging humanitarian challenge of our time,  causing suffering to hundreds of millions of people worldwide,” Mr Annan  said.
“As this report shows, the first hit and worst affected are the world’s  poorest groups, and yet they have done least to cause the problem.”
//
  
    
The report claims that 90 per cent of the deaths are related to gradual  environmental degradation caused by a warming climate, which exacerbates  existing threats — mainly malnutrition, diarrhoea and malaria. The rest are  said to be the result of weather disasters.
     But here is the kicker (emphasis mine):
Mr Annan said the report could never be as rigorous as a scientific study, but  said: “We feel it is the most plausible account of the current impact of  climate change today.”
Translation: “close enough for government work” (click for definition)
Worse, the U.N. didn’t even do the report themselves. The farmed it out:
The research was carried out by Dalberg Global Advisers, a consultancy firm,  who collated all existing statistics on the human impacts of climate change.  The report acknowledges a “significant margin of error” in its estimates.
But it is good enough for the MSM to use to scare the crap out of everybody and guilt the gullible into “action”.
‘Bogus’, doesn’t even begin to describe this political ploy.
For a real report, using real data, reflecting the real world situation, please read these reports by WUWT contributor Indur Goklany:
Going Down: Death Rates Due to Extreme Weather Events
How the IPCC Portrayed a Net Positive Impact of Climate Change as a Negative
Wrong: World Health Organization claims that health goes down as carbon goes up
Dealing with climate change in the context of other, more urgent threats to human and environmental well-being


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95b6658f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Have you heard the Morrison government has a plan? The Plan® was very hard to miss, given the prime minister used the word “plan” or “plans” more than 20 times when he answered his first Dorothy Dixer in question time on Thursday. Lest any ambiguity remain, Peter Dutton went on to utter “plan” another 16 times if you happened to be counting, and our indefatigable live blogger, Amy Remeikis, bless her, was counting.  Plans revealed themselves in the preambles to questions, and in the answers to questions; a thicket of plans for potential pandemics, for managing the economy, a plan for beating up Labor over its increasingly uncontroversial commitment to net zero emissions by 2050. Or perhaps that one was a strategy. The urgency and intensity of the messaging was such that I wondered whether we might at any moment see live broadcasts interrupted with a solemn message to the nation at the top of the hour: “Fellow Australians, this is your government speaking. Did we mention we have a plan? Did we mention we have a plan for a plan?” Before I get branded a shocking cynic, let’s be clear. It is obvious the government needs to have a considered and credible plan for managing a mass outbreak of the coronavirus, because the spread of this illness, and the negative economic consequences associated with it, is a deeply serious issue, not some passing bit of political confection or rank opportunism. The world is watching unfolding events in a state of shared nervousness. Leaders are warning an epidemic is on the way and the trajectory of infections prompted a record plunge in the US stock market on Friday as analysts warned the outbreak could wreak economic havoc on a scale not seen since the 2008 financial crisis. In short, this is all profoundly serious, and it is not a drill. So I’m relieved Australia has a plan, because we do need one, and it would great if the government can both develop and execute it competently. Having acknowledged there is a serious challenge to manage and the country absolutely needs a plan to manage it, let’s drill down a bit further. It’s pretty obvious why the government is intent on telling voters there is a plan for managing coronavirus and other potential crises. To put it bluntly, Morrison’s theatre of The Plan® is atonement for the disaster of the summer. If we take a minute or two to recap the sorry story of the bushfires, the only thing the government mastered was looking like a shambles. As the fire emergency spiralled, Morrison presented during a time of terrible crisis as a leader making it up as he went along. In the court of public opinion, the government was constantly running behind events. Haplessness could be measured in increments, from the ill-fated Hawaii holiday through to calling out the defence forces without telling the poor bloke managing the bulk of the blazes. As well as conveying a compelling impression of a government extemporising inelegantly in full public view, Morrison looked resentful when obvious deficiencies and inconsistencies were pointed out to him. Presenting to the public with pursed lips and barely restrained pique, not every time, but often enough to be memorable, I reckon compounded negative voter perceptions and made the inevitable backlash worse. Our Guardian Essential poll in mid-January delivered a stark snapshot of the credibility hit. Morrison’s net approval rating shifted from plus two to negative 12, and the Labor leader Anthony Albanese sprinted ahead of Morrison as preferred prime minister (despite the fact the poll shows consistently voters are still getting a fix on the Labor leader). Because everything is so polarised, because politics has substituted conflict (easy) for reform (hard), because technology is herding people into tribes, and because default mainstream media culture tells people these days it is OK, in fact, desirable, to sit in an enclave that confirms your biases, preferably with noise-cancelling headphones – Morrison didn’t take too much of a hit among rusted-on Coalition voters. They, largely, stuck. But if our poll snapshot is accurate, across Labor and (critically) undecided voters, one in seven people changed their minds about Morrison between December and January. The prime minister also took a significant hit on the attributes questions we ask the Guardian Essential sample regularly about leaders. There was a nine-point drop in the number of voters rating Morrison a capable leader. Worse, there was a 19-point drop in the number of voters saying he was good in a crisis. Now I want to consider these specific attributes a little. Given the Coalition’s defining pitch to Australian voters is always we will keep you safe and secure, because we are conservatives, and that’s what conservatives do (in contrast to wild-eyed progressives, who favour social change over security) – a loss of public confidence in attributes like “capable”, and “good in a crisis” is potentially dangerous for Morrison and the government. The language of safety, security and managerial competence is the most powerful language Australian conservatives have in their toolkit. That language is resonant enough to win elections. Security is such a critical part of the pitch that the disastrous summer Morrison just presided over actually begs an existential question for a Liberal prime minister who likes to style himself as the Generation X John Howard. It’s a simple question. What good is a conservative who can’t keep Australians safe? One more brief observation before we wrap up. Looking ahead, I reckon climate change poses a significant dilemma for the Coalition. Since the Abbott era, the Coalition has been able to weaponise climate change to its political advantage. But weaponising climate change is a whole lot easier when global heating is an abstract risk, or in the minds of some, an entirely hypothetical possibility. I think that becomes harder to navigate when heating is a lived reality – when people are dealing with the practical consequences of natural disasters, like the summer we’ve just experienced. But as they say in the classics, only time will tell. More immediately, Morrison’s challenge is to stabilise and turn public perceptions. This is a critical mission. The prime minister, a former campaign director, respects research and data, so he will know that recent political history suggests voters make up their minds about prime ministers and governments pretty early in a term. Those perceptions, once formed, are hard to budge. I reckon Julia Gillard, Abbott and Malcolm Turnbull could all share a story or two about the increasingly transient nature of political honeymoons in Australia. So, in summary, The Plan® you keep hearing about carries a lot of freight. There are actual, practical plans to manage risks, and there are political projections of risk management. For the government, both will feel important. Implicitly Morrison wants to tell you he is learning on the job. The past week in politics tells us this. The prime minister and the government need a way to apologise for the summer and reboot, without admitting any liability, because introspection and contrition is really not a hallmark of this government, at least not in public. If contrition is impossible, because sorry is such a hard word to say, then perhaps competence, assuming that materialises, can serve as a substitute."
"I recently found myself in the surreal world of the Consumer Electronics Show in Las Vegas discussing the next generation of pollution sensors that one day you might find inside your phone. The exhibits I saw suggested the next big thing in home technology could be anything from intelligent cat litters to internet-enabled teapots, with everything powered by mysterious machine learning and the unfathomable blockchain. But there was no escaping that air quality and air purification is now a seriously big thing in the consumer products world. Most major white goods manufacturers have a range of products. There are also plenty of start-ups offering new variants – including purifying robots that wander forlornly around your home and bizarre bio-inspired devices that blow air over the leaves of poor unsuspecting houseplants.  If you live in Europe it could be easy to dismiss these as tech gadgets that may never catch on, but that would be badly misjudging the ever-expanding user base for home air filtration that already exists in Asia and beyond. These devices are for sale because people want them, and the market could be worth in excess of US$30 billion per year by 2023. In some regards, indoor air purification is an individually empowering technology. In a well-sealed home, filtration-based purifiers clearly make a difference and can noticeably reduce concentrations of tiny harmful particles, particularly if the home is somewhere with lots of pollution outdoors, such as central Beijing or Delhi.  The evidence for the removal of harmful gases indoors, including volatile organic compounds from paints and glues, is sketchier. Some systems get the gases to stick to a charcoal-based filter, but there is little independent data that shows these actually work. In other types of purifiers UV radiation is used to accelerate a chemical reaction that turns those gases into carbon dioxide and water. However manufacturers have not yet published data to show that this process doesn’t actually end up converting relatively benign compounds into something more harmful. Outdoor air filtration demonstrators have so far proved ineffective, simply because the atmosphere is so huge relative to the size of the filtering system. However, indoors, the balance shifts. Homes have internal volumes measured in the hundreds to maybe several thousands of cubic metres and, simply due to natural drafts and leaks, the indoor air is swapped with outdoor air perhaps once per hour. That is still a lot of cubic metres of air to clean, but the maths begins to stack up. Yet the costs of filtration are possibly larger than they first appear. Most air purifiers use cellulose or polymer membranes that are replaced every month or so, often as part of a regular service contract. The air is pushed through the filters with fans and pumps which use energy, perhaps anywhere between 100 watts (equivalent to a bright lightbulb) and 1000 watts (a microwave), depending on the size of the air cleaner and home.  Poor air quality in this sense then impacts on climate by increasing energy demand in the home and the city, and of course it adds directly to the user’s electricity bill. The power demands of air filtration are not as great as air cooling, but would potentially run 365 days a year, not just in the summer months. If you add 500 watts of continuous demand to millions of homes, this becomes a big deal. Then there is the elephant in the room. What happens to all those millions of microfiber particle filters or traps full of activated carbon? I asked that question more than 20 times in Las Vegas and the answer was always the same – you put them in the bin.  Should we care? Possibly, yes. Filters in the home that collect particles end up concentrating some rather unpleasant toxic chemicals gathered from air outside – heavy metals from brake wear, polycyclic aromatic compounds from wood and coal fires, nitrosamines from cigarette smoke, the list goes on.  A filter may end up holding milligrams (and maybe more) of individual chemicals that were initially found in air at very diluted concentrations, and whose previous fate was probably to deposit as a very thin layer over huge areas of land. If hundreds of millions of filters from millions of homes are then all dumped in the same few city landfills we double down on the concentration process. Are we simply shifting a problem from the air into a problem of those same chemicals now leaching out into the soil and water? It’s unclear how much thinking has gone into this, or the energy demand consequences should hundreds of millions of people start purifying their own air at home. (Thinking more positively for a moment: perhaps those millions of waste filters would offer someone an opportunity to “mine” the trace metals collected?) There are some obvious conclusions to be drawn, the most striking being that there is a financial opportunity for someone in every crisis. But this particular solution comes with costs that we haven’t yet well quantified. Air filtration adds electricity demand for sure, it needs raw materials and resources to build, maintain and support and it is possibly creating chemical disposal problems we’ haven’t yet evaluated. It does however reinforce the well-trodden scientific principle that it’s always more efficient to stop pollution at source than try to clean up afterwards."
"**England enters a tougher version of its three tier system of restrictions on Wednesday, as a four-week lockdown ends.**
Northern Ireland has a two-week circuit-breaker lockdown, while Wales is banning the sale of alcohol in pubs, cafes and restaurants from Friday. Scotland has its own five-tier system.
Across the UK, some restrictions will be relaxed over Christmas, to allow three households to form a ""Christmas bubble"".
From just after midnight on Wednesday 2 December, areas will be placed in one of three tiers: medium, high and very high.
About 99% of England has been placed into the high and very high coronavirus risk category - tiers two and three.
The placing of areas in each tier will be reviewed every 14 days, with the first review on 16 December.
**Areas in tier two**
**Tier two (high) rules**
**Areas in tier three**
**Tier three (very high) rules**
Additional restrictions apply:
**Areas in tier one**
Only three areas have been placed in the lowest tier:
**Tier one (medium) rules**
Areas in the lowest tier will have some restrictions relaxed:
There are exceptions in all tiers for childcare and support bubbles. More details of the plan are here.
The new coronavirus tier restrictions will mean 55 million people will be banned from mixing with other households indoors. The decision about which tier to place an area in is based on:
Lockdown restrictions in Wales were eased on 9 November.
**The current rules say:**
People who you don't live with still cannot come into your home socially, unless you are in an extended household (bubble) with them. Tradespeople can enter your home to carry out work.
However, from **Friday 4 December:**
Read Wales' official guidance.
Northern Ireland started a two-week circuit-breaker lockdown from 00:01 GMT on Friday 27 November.
Read Northern Ireland's official guidance.
Each area of Scotland has been placed in one of five tiers.
Eleven local authority areas in west and central Scotland have recently moved from level three to level four, affecting two million people.
First Minister Nicola Sturgeon told MSPs the level four measures would be lifted at 18:00 GMT on Friday 11 December.
**Areas in level zero**
No areas have been placed in the lowest tier.
**Level zero (nearly normal) rules**
**Areas in level one**
**Level one (medium) rules**
Additional restrictions apply:
**Areas in level two**
**Level two (high) rules**
Additional restrictions apply:
**Areas in level three**
**Level three (very high) rules**
Additional restrictions apply:
**Areas in level four**
**Level four (lockdown) rules**
Additional restrictions apply:
Schools stay open in all levels, and here must also be no non-essential travel between Scotland the rest of the UK.
**Do you meet other people for exercise? Have you been out walking during the November lockdown? You can share your experiences by emailing**haveyoursay@bbc.co.uk **.**
Please include a contact number if you are willing to speak to a BBC journalist. You can also get in touch in the following ways:"
"**A freeze on public sector workers' pay would damage the country's economic recovery, the shadow chancellor will warn in a speech on Monday.**
Chancellor Rishi Sunak needs to find ways to protect public finances after borrowing large amounts to fight Covid.
He declined to say if he would consider freezing public pay but added that it had to be considered in ""the context of the overall economic climate"".
On Wednesday Mr Sunak will set out government spending for the next year.
One think tank, the Centre for Policy Studies, has suggested a three-year pay freeze across the public sector could save up to Â£23bn.
But in a speech on Monday, Ms Dodds will argue that ""freezing the pay of firefighters, hospital porters and teaching assistants will make them worried about making ends meet ahead of Christmas - that means they'll cut back on spending and our economy won't recover as quickly.
""The British people shouldn't have to pay the price for a government that doesn't know the value of public money, splurging it on outsourced contracts to Tory-linked firms that don't deliver.""
Frances O'Grady - head of the Trades Union Congress - also expressed concern, telling Sky News: ""We saw ministers join millions of us clapping firefighters, refuse collectors, social care workers - I don't think this would be the time to reward them with a real pay cut.
""This is not smart politics, it is morally obscene and it is bad economics, too,"" she added.
Also speaking to Sky News' Sophy Ridge On Sunday programme, Mr Sunak said he would not comment on public pay before the spending review but added that it was reasonable to consider the subject in ""the context of the overall economic climate"".
""We need to see what is going on with wages, jobs and hours across the economy,"" he said.
He insisted that the spending review would not signal a return to austerity, arguing that government spending on public services was increasing.
Government borrowing has increased massively as a result of the coronavirus pandemic and a recent estimate by the Office for Budget Responsibility says the government would have to borrow Â£372bn for the current financial year - that compares to the Â£55bn it had expected to borrow pre-pandemic.
Ms Dodds will also use her speech to say that her party would ""make responsible choices"" to protect the economy.
Her plan includes bringing forward Â£30bn in capital spending over the next 18 months to spend on the ""clean industries"", setting up an emergency programme to retrain workers and establishing a National Investment Bank.
She is also expected to blame Mr Sunak for blocking a 'circuit beaker' - a set of time-limited restrictions - which Labour proposed earlier this year. She will argue this led to ""a longer, more painful lockdown"".
""The chancellor's irresponsible choices and unacceptable delays are damaging the economy. That's why we're in the grip of a jobs crisis - and it's got Rishi Sunak's name all over it.""
Speaking to the BBC's Andrew Marr show, Mr Sunak said the economy was ""experiencing significant stress"", adding: ""I think now is the right time to focus on responding to the crisis and that means, yes we will be borrowing quite frankly an enormous sum this year to help us do that.""
Defending the government's use of contracts in the fight against coronavirus, he said the government had been right, at the height of the pandemic, to ""act fast"" rather than relying on 60-day procurement processes.
A Treasury spokesman said: ""This government's actions have protected millions of jobs and businesses across the country - including 9.6 million people on furlough and over a million companies taking government-backed loans.
""Later this week, the chancellor will set out the Spending Review, which will benefit all nations and regions of the UK, as part of our commitment to build back better."""
"
Russ Steele writes: President Obama will soon be on his his way to Copenhagen and his Bagdad Bob moment in Air Force One.  Climategate is sure to create some turbulence.

From  The Chilling Effect
Got any political cartoons on Climategate? Post links to them below.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90be2e05',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On July 23, 178 nations agreed to a new draft of the Kyoto Protocol on global warming. The United States did not. The United States, alone in the world, did the right thing–whether or not you care about global warming. If you don’t care, the bottom line is that our economy will prosper. And if you do care, the bottom line is that our economy will prosper and produce technologies that must reduce the relative production of greenhouse gases, which we’ll gladly sell to everyone else who ratifies the “New Kyoto.” 



The “New Kyoto” is a revision of a 1997 instrument that must be ratified by nations that produce 55 percent or more of the world’s carbon dioxide emissions in order for it to enter into force. The United States isn’t going along (our Senate may currently sport about 12 of the 67 votes required for ratification), and the only way this magic number can be reached is if the Japanese come on board. They had signaled that they would not, until the Old Kyoto was modified in their favor. 



In a nutshell, while the Old Kyoto required that the major industrialized nations reduce their emissions of carbon dioxide to an impossibly low 5.2 percent below 1990 levels, beginning 6.4 years from now, the New Kyoto requires an impossibly low 1.8 percent below 1990 levels at the same time. This is impossible because most of the world is already running about 12 percent above 1990, as emissions grow with population and prosperity. Most of the 178 signatory nations, including China and India, have no commitments to reduce emissions under this Protocol. The lion’s share falls onto Europe, Canada and Japan. Almost all of the industrial emissions of carbon dioxide come from the combustion of fossil fuels. 



The “New Kyoto” replaces the Old Kyoto as the most ineffectual environmental treaty ever proposed. Here are the numbers: 



Assume that the earth’s temperature is destined to rise 4.5ºF for a doubling of atmospheric carbon dioxide. This is a standard U.N. assumption, subject to considerable debate but it is a common reference point. (Note that temperatures rose 1.0ºF in the last 100 years and most people prospered.) Also assume that the nations–mainly European–that have to do something under the New Kyoto live up to their commitments (they won’t), and compare the projected temperature changes to what would happen if no one made any special attempt to reduce emissions–the so‐​called “business‐​as‐​usual” approach. 



The New Kyoto produces a world, in 2050, whose surface temperature is 0.04ºF lower than it would be if no one did anything. That’s four hundredths of a degree. This is about 30 percent of the warming “saved” by the Old Kyoto, which itself was small beer. By 2100, the saved warming is 0.11ºF. These numbers come from the U.N.‘s own computer models. 



The warming rate in the U.N. models is fairly constant, once you chose your “storyline” (that’s what they call their future social projections these days). The mean “storyline” in vogue these days warms the surface about 4.5ºF in 100 years, or 0.045ºF per year. Thus does the New Kyoto signify nothing. Do the math. If everyone does what they say they will, the mean global surface temperature that would have normally been expected on January 1, 2050, will appear on September 18, 2050. The New Kyoto delays this warming by 288 days. 



This, of course, assumes that the United States does nothing, while the other nations raise taxes enough to drive emissions to 1.8 percent below 1990 levels. That’s the only way we know to reduce the energy use that produces these emissions. No one knows what the total cost will be. But it certainly means that European governments are going to gobble up more of their people’s income and corporate profits than they do now. 



This will have the salutary effect of forcing multinational business over to our side of the ocean, where people will have more money to invest. Like stockholders everywhere, they are going to demand more production with increased efficiency. Thus the New Kyoto will in fact force investment in technologies that are more likely to produce things that cost less energy to operate. 



The irony of all of this is that our European friends have sentenced themselves to economic stagnation while doing nothing about global climate change. At the same time, they have insured a vibrant United States that will, with the investment dollars that the New Kyoto diverts in our direction, produce a cleaner future. 



All of this is inevitable if only President Bush stays the course and stays away from the New Kyoto.
"
"**The Government of Jersey has been given the power by the States Assembly to make wearing masks in shops mandatory.**
Members also approved laws to limit the size of gatherings, as part of updated Covid-19 regulations.
Currently the wearing of masks is not legally enforceable, with the government continuing to promote them in guidance.
The regulations have not created new rules, rather they have set the terms of possible restrictions.
The maximum penalty for individuals breaching any mask or gathering law would be set at Â£1,000.
Children under 12 years old would not have to wear a mask, along with people exempt for health or disability reasons, according to the regulations.
If a law requiring wearing of face coverings is introduced, it would apply to specific workplaces where a member of the public is present as a customer.
The regulations also allow orders to oblige businesses to collect personal data to aid contact tracing and to refuse service to those not wearing masks.
Any restrictions on the size of gatherings will only apply to groups of 10 or more people.
Visiting people's homes in Jersey was banned during the first wave of the pandemic before the ban was lifted in May, although public health guidance has discouraged meeting indoors since."
"
Share this...FacebookTwitterThe online Financial Times Deutschland reports that a British team of astronomers, led by Jane Greaves of the University of St. Andrews in Scotland, have found strong evidence of global warming of Pluto’s atmosphere using the 15-meter James Clerk Maxwell Telescope in Hawaii. They also detected carbon monoxide in its atmosphere.
The researchers also say that new findings show that Pluto’s atmosphere extends to more than 1860 miles (3000 km) above the surface –  or a quarter of the distance out to its largest moon, Charon. Before it was thought to be only 100 km thick. Greaves will present the new discovery today at the Royal Astronomical Society’s National Astronomy Meeting in Wales.
Pluto’s atmosphere appears to have expanded due to warming. Greaves says:
The change in brightness over the last decade is startling. We think the atmosphere may have grown in size, or the carbon monoxide abundance may have been boosted.”
The Financial Times writes;.
Pluto’s extremely low density atmosphere has a fragile balance made up of the coolant carbon monoxide and the greenhouse gas methane. It is probably the most sensitive in the solar system, Greaves said.”
The far away dwarf planet is probably currently experiencing climate change, said Greaves. ‘We believe that the expansion of the atmosphere has grown. in 1989 Pluto passed its closest point to the sun in its orbit. Probably the stronger solar radiation vapourised additional ice and the atmosphere expanded.”
In the new study, scientists found that the carbon monoxide gas on Pluto is extremely cold, at about minus 364°F (-220°C ).
‘This simple, very cold atmosphere, which is greatly influenced by the sun’s warmth, could give us important information on the fundamental physical interactions and thus a better understanding of the earth’s atmosphere,’ Greaves said.”
Yeah – like the sun plays the major role on atmospheric behaviour and climate, even when it is 3 billion miles away (the earth is only 93 million miles away) and that everything else, like oceans and atmospheres, reacts to its changes and orbital changes.
Share this...FacebookTwitter "
"Scott Morrison has acknowledged there are “costs associated with climate change” but has declined to spell out what 3C heating would do to job creation and economic growth in Australia. Ahead of the release of its technology roadmap, the federal government is attempting to ramp up political pressure on Labor over its commitment to a net zero target by 2050, blasting the opposition for adopting a target without a fleshed-out strategy to meet it, and pointing out that CSIRO research cited positively by Labor assumes a carbon price of more than $200 to drive the transition.  But the government is also having to fend off sustained questions about basic contradictions in its own messaging. In question time on Monday, Labor asked why the government was criticising the opposition’s 2050 target when the Paris agreement, which the current government signed and ratified, required carbon neutrality by mid-century. The emissions reduction minister, Angus Taylor, declared that was incorrect, because Paris involved “the world achieving net zero in the second half of the next century”. The independent MP Zali Steggall – who is championing a bill to lock Australia in on a net zero target – also asked Morrison to detail the costs of inaction, given the government has been blasting Labor for days for failing to outline the cost of action. In response to Steggall’s question, Morrison stepped around the specifics, but said “we do understand there are costs associated with climate change that we are indeed taking action on to reduce emissions”. The prime minister said it was important to build resilience, and develop adaptation measures “to ensure that Australians can thrive in the climate we live in while taking the necessary action when it comes to emission reduction”. During the run of questions on Monday, Morrison told parliament the government had “a clear plan” to achieve a 2030 emissions reduction target of 26%, without mentioning that almost 100 megatonnes of the Coalition’s proposed reductions are booked to unspecified “technology improvements” and additional pollution cuts are attributed to an electric vehicles strategy that the government has not yet announced. The government is also counting, in its 2030 plan, a 367-megatonne contribution from carryover credits – an accounting system that allows countries to count carbon credits from exceeding their targets under the soon-to-be-obsolete Kyoto protocol periods against their Paris commitment for 2030. The latest emissions data confirms pollution has dipped slightly on the back of new clean energy and a sharp fall from agriculture due to the drought, but the decline was almost entirely wiped out by surging industrial pollution. While keeping all its options open, the government has been signalling for some days it is unlikely to adopt a 2050 target. On Monday, the government attempted to ramp up a parliamentary attack against Labor’s proposal, contending such a transition would be devastating for sectors like agriculture. At one point, the deputy prime minister, Michael McCormack, asked Labor to “look a steer in the eye, and say how are you going to stop your methane!” Asked repeatedly on Monday whether the government would nominate a long-term target, Taylor said the government would be “focusing on technology”. After question time, the Labor leader, Anthony Albanese, said he was confident that championing a net zero position would not cost the ALP seats in Queensland at the next federal election. He said if it was reasonable for Morrison to demand Labor to detail the costs of action, it was also reasonable to ask the government about the costs of inaction. “We saw a bit of the cost of inaction over the bushfire season, when we lost 33 lives, when we lost over 3,000 homes, when we lost 12 million hectares of land,” Albanese told Sky News. “What we know is that the cost of that season certainly won’t be $2bn that the government talks about. We know the consequences of dangerous climate change are catastrophic for our economy. “Global economists predict that the cost of a greater than 2C increase in global temperatures will be between 15 and 25% lower economic growth – a greater cost than we saw during the great depression.”"
"**The government is defending a decision not to negotiate with local authorities over which coronavirus tiers they will be in when lockdown ends next week.**
Matt Hancock said the battle in October with Greater Manchester over funding when it was moved it into Tier 3 had been ""bad for public health"".
He said it would not happen again under the new system coming in next month.
The row saw Labour mayor Andy Burnham strongly object to plans to put the region into the strictest restrictions.
Before the second national lockdown in England, ministers undertook negotiations with local authorities in order to settle on a package of measures to control rising rates of coronavirus infection in their areas, along with financial support to help mitigate any impact.
But the Labour mayor accused ministers at the time of treating the region as a ""sacrificial lamb"" by asking it to accept a proposal which the ""government's own advisers say won't work"".
Under new tiers announced by the Prime Minister on Monday, due to come into force once the lockdown ends on 2 December, there will no longer be a set of negotiations with local areas, with ministers instead relying on a formula to decide which areas are placed in what tier.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Mr Hancock told MPs: ""The reason we are doing it differently is, whilst in most cases when we negotiated with most areas in the previous tiered arrangement, we had a high quality discussion which led to better outcomes.
""A case in point is Liverpool, where the case rate has fallen by over two thirds in the last three weeks.
""Unfortunately that wasn't the case in all local areas.""
Asked by Labour MP Graham Stringer whether he was referring to Greater Manchester, Mr Hancock said: ""That would be one example but not the only one.
""Sadly, in the case of Greater Manchester, cases carried on going up whilst we were trying to put in place the measures that were necessary.
""So, instead, we've proposed a set of measures within the tiers which are fixed, also financial support which is agreed by formula rather than negotiation.
""We will have engagement but what we won't have is a two-week long negotiation while the cases still go up. That is bad for public health.""
Speaking to Sky News, Mr Burnham accused the government of ""walking away"" from negotiations with him and ten other council leaders.
He further accused ministers of ignoring advice to bring forward a national circuit breaker lockdown in September."
"
This is an interesting survey that cuts across a number of lines and held beliefs. I believe it to be worthwhile to participate in this survey. – Anthony

Guest post by Tom Fuller
If you are tired of having everybody trying to tell you what you think, and especially if what you think isn’t what’s being reported, I heartily encourage you to take this survey. I will be doing the analysis for free and for fun over the next few weeks, and I hope that we will be able to break new ground on the debate over global warming.
Thank you for participating in Examiner.com’s First Annual Survey on Global Warming. The introduction is below. Have fun!
First, let’s start with the ground rules. Your participation is completely anonymous, and no attempt will be made to contact you for any reason as a result of your participation or anything you write in this survey.
Second, this survey is not intended to be used as an opinion poll or a census, and will not be used as such. We are not trying to find out how many people ‘believe’ or ‘disbelieve’ in global warming. Our purpose is to try and find out if there are areas of agreement on possible policy initiatives going forward.
Click here to get started. Examiner.com’s First Annual Survey on Global Warming. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91bf420a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter Germany’s leading political daily the Frankfurter Allgemeine Zeitung (FAZ) published an open letter written by geology Professor Dr. Friedrich-Karl Ewert rejecting the claims made by the Potsdam Institute for Climate Impact Research (PIK) that sea level rise is accelerating. The open letter is now available at the European Institute for Climate and Energy (EIKE), read here (part after Schellnhuber). 
According to the PIK:
“Due to global warming, sea level is rising today faster than at any time in the last 2000 years. Since the start of the Industrial Revolution, the curve is going ‘up steeply’.”
These claims are based on cherry-picked data from a single location at the North Carolina coast. In response Ewert’s wrote the open letter, excerpts of which follow. Ewert writes (emphasis added):
This claim must be refuted because the declaration made by the PIK is factually false. Correct is that scientists have determined just the opposite. The Journal of Coastal Research reported in journal 27/3 (May 2011).”
Here according to the journal, the global temperature increase during the last 100 years did not accelerate sea level rise, Indeed sea level rise has recently slowed down.
Ewert writes that there is a simple proof for the falsehood of PIK’s claim:
At the coral reefs in the Great Barrier Reef and also at the coasts of the Caribbean Islands one finds dry coral reefs which were about 3.5 meters over today’s sea level, When these reefs formed, the sea level was accordingly higher. No one knows how fast the sea level rose during the time before that.”
Ewert also rejects the PIK’s claim that the Western Antarctic Ice Shelf is about to tip and that global warming is racing ahead. Ewert writes in his letter:
Detailed evaluation of temperature data shows a completely other picture: In many parts of the globe a rewarming took place after 1700, after the Little Ice Age, and has reached its peak in the mid 1990s. During this long period a number of warming and cooling phases took place, and were earlier often much stronger; the last pronounced warming phase took place in the first half of the 20th century, before man made carbon emissions; and since 1998 a new cooling phase has started in many regions of the earth, and is still taking place and is at times quite stark. The results of the evaluation will be published after the conclusion of a comprehensive analysis. ‘Our CO2’ has not had any real impact on this development.”
Share this...FacebookTwitter "
"
The Monthly Energy Review for August 2009 has been published by the US Energy Information Administration and it has some interesting CO2 production data which you can see here in tabular form.
I’ve graphed the data of interest in two separate graphs. First we have the annual plot of “Carbon Dioxide Emissions From Fossil Fuel Consumption by Source” with data to the end of 2008 for the USA:
Click for larger image
Note that in 2008 a significant drop was seen in total CO2 produced. Corresponding to the drop is a drop in CO2 produced by petroleum, which seems to indicate that high gasoline prices last year which contributed to less miles driven, may have been the dominant factor.
The Department of Transportation notes in U.S. Traffic Volume Trends:
Cumulative Travel for 2008 changed by -3.6% (-107.9 billion vehicle miles). The Cumulative estimate for the year is 2,921.9 billion vehicle miles of travel.
Gas prices receded though in late 2008 and into 2009. But our economy continued its slide with layoffs, store closings, and less demand for durable goods during that same period.
Click for larger image
The graph above compares USA CO2 output by source for the first 5 months of 2008 and 2009. As you’d expect, there is a seasonal drop in coal and natural gas related to less heating requirements, but there remains significant offsets compared to the same months in 2009 for petroleum and coal use. With the severe winter and cool spring seen in much of the US eastern areas with the heaviest population, one might expect increased demand for heating. In fact, this EIA report shows that average heating degree days from 2008 to 2009 tripled, with significant jumps in the east, Midwest, Great Lakes, and New England.
With heating demand actually went up in the first 5 months of 2009, one explanation for this 2008 to 2009 drop in CO2 production could be our sagging economy. With less demands for durable goods, manufacturing and transport are reduced. This affects coal due to lowered electricity demands and petroleum is less for for lowered goods transport. Unemployment may also figure in lowering petroeum usage due to less daily commuting.
I found it interesting that despite all the eco-pronouncements of reducing fossil fuel use, the one thing that appears to have made a significant difference is our sagging economy.
The EIA web page with additional reports is available here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93adf856',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Labour activists in the US say big retailers like Amazon and Walmart must do more to protect workers as surging coronavirus cases coincide with the holiday shopping rush.**
They are calling for hazard pay, paid sick leave and better communication about outbreaks, among other things.
The campaign comes as workers across the US have spoken out about condition and concerns over their health.
""Associates like me are scared,"" said Walmart worker Melissa Love.
The workers rights campaign launched on Monday was organised by United for Respect, a workers rights non-profit that says it represents more than 16 million people across the US.
Separately, the labour union UFCW, whose members include grocery and meatpacking plant workers, also called on employers to do more to protect staff.
""Simply put, frontline workers are terrified because their employers and our elected leaders are not doing enough to protect them and stop the spread of this virus,"" UFCW International President Marc Perrone said.
""As holiday shopping begins this Thanksgiving, we are already seeing a huge surge of customer traffic. Unless we take immediate actions beginning this holiday week, many more essential workers will become sick and more, tragically, will die.""
Ms Love, a member of United for Respect who has worked at Walmart for five years, said on a call organised for reporters that she feared a rush of holiday shoppers could turn Walmart into a ""super-spreader"" hub.
""Working Black Friday this year comes with an obvious danger,"" said Ms Love, who is based in California. ""I do not believe Walmart should be trying to entice crowds into our stores on Friday and risk a super-spreader event.""
Courtenay Brown, another member of the group, who picks groceries for Amazon, said the company has had to send out repeated notifications in recent days about infected staff. Amazon had boosted pay for frontline staff by $2 (Â£1.50) an hour, but that policy ended in June.
""Right now it's what we call the turkey apocalypse, where we are forced to just push out as much as we possibly can,"" said Ms Brown, who works in New Jersey.
She said she's happy to have a job, but to Amazon ""my life doesn't really mean much - it's just a means for Bezos to continue making billions off of us"".
Workers said the companies had the means to spend more to protect and compensate workers, noting the way profits have soared during the pandemic, which has shifted purchases to essentials and kept many smaller competitors closed.
Amazon and Walmart did not respond to requests for comment on Monday, but they have defended their practices in the past.
Walmart has changed how it is handling Black Friday discounts, offering the deals online and over several days to try to avoid crowding at its stores.
Amazon last month said nearly 20,000 people, or 1.4% of its staff, at Amazon and Whole Foods, the grocery store it owns, had tested positive for Covid-19 in the US since the start of the pandemic.
It said that was lower than would be expected given wider infection rates.
""All in, we've introduced or changed over 150 processes to ensure the health and safety of our teams, including distributing over 100 million face masks, implementing temperature checks at sites around the world, mandating enhanced cleaning procedures at all of our sites, and introducing extensive social distancing measures to reduce the risk for our employees,"" the company said.
More than 250,000 people in the US have died from coronavirus since the start of the pandemic. In recent weeks, case numbers, death rates and hospitalisations have soared, straining the health system and prompting many places to re-impose restrictions.
Some major retailers, including Amazon and Walmart, have posted strong results this year as many customers opted to buy online instead of venturing out to the local store.
A recent analysis by the Brookings Institute, a Washington think tank, found that company profits at 13 of America's biggest retailers increased by an average of 39% this year, while pay for frontline workers had increased by just $1.11 per hour - ""a 10% increase on top of wages that are often too low to meet a family's basic needs"".
Workers said they are well aware of the disparity.
""They closed the corporate office until July 2021 because of the virus meanwhile we're expected to keep risking our lives to pay for their big salaries,"" Ms Love said."
"**People arriving in England from abroad will be soon able to reduce their quarantine by more than half if they pay for a Covid test after five days, the transport secretary has announced.**
The rules will come into force from 15 December and the tests from private firms will cost between Â£65 and Â£120.
Grant Shapps said the scheme would ""bolster international travel while keeping the public safe"".
The travel industry welcomed the policy but described it as ""long overdue"".
It follows Boris Johnson's announcement that England will come under ""toughened"" three-tiered regional restrictions when the lockdown ends on 2 December.
Under the new travel rules, passengers who arrive from a foreign destination not on the government's travel corridors list will still need to enter self-isolation.
However, if they pay for a test after five days and it comes back negative, they will no longer need to self-isolate.
Results will normally be issued in 24 to 48 hours. This means people could be released from quarantine six days after arrival.
Mr Shapps said: ""Our new testing strategy will allow us to travel more freely, see loved ones and drive international business. By giving people the choice to test on day five, we are also supporting the travel industry as it continues to rebuild out of the pandemic.""
Scotland and Northern Ireland are considering a similar scheme while Wales said it was ""supportive in principle of the proposals for a Test and Release scheme"".
A Welsh government spokesperson said: ""We will need to consider the data and evidence underpinning this scheme before making any decisions on changing international travel restrictions in Wales.""
Scotland said it was working with the main commercial airports in Scotland and clinical advisers to understand the risks and benefits of the scheme as well as the capacity of private sector labs to conduct testing to a minimum standard.
A spokesperson added: ""It is important that any travellers arriving in the UK understand, and respect, the different restrictions in place in the different nations.""
That means that if someone is flying into England but their final destination is an address in Scotland, it would not be possible for passengers to take the test in England.
Northern Ireland's Department of Health said: ""Consideration is currently being given to the implementation of testing scheme which could potentially allow those who have arrived in Northern Ireland from a non-exempt country to end their self-isolation following receipt of a negative test, which would be privately provided.""
A step in the right direction - that's how airlines are describing the government's decision to ease the quarantine regime.
The industry is in survival mode, desperate for money so anything that could open up routes and bring in much-needed cash is being welcomed. And this does raise the prospect of Christmas holiday ticket sales.
But they say the new plan is still far from perfect.
Passengers will still have to go into quarantine, and realistically, a test on day five is still likely to leave them in isolation for the best part of a week in total, as they wait for the results to come through.
There's also the cost. The test has to be done privately, and typical prices range from Â£100-150. For a family of four, for example, that's a sizeable extra chunk on the cost of a winter holiday.
What airlines are calling for is something more radical. They want a pre-departure testing regime, or a system of quick, regular and cheap tests - which would allow quarantine to be avoided altogether, until a vaccine is ready.
But they say this announcement means there is, at least, some light at the end of the tunnel.
Tim Alderslade, chief executive of Airlines UK, the industry association representing UK-registered carriers, said the announcement provided ""light at the end of the tunnel"" for the aviation industry and people wanting to go on holiday.
He predicted demand for air travel will ""tentatively return"" following the decision but said a pre-departure testing regime that can completely remove the need to self-isolate is ""the only way we're going to comprehensively reopen the market"".
Michael O'Leary, Ryanair's chief executive, told BBC Radio 4's Today programme quarantine for arrivals was a ""fig leaf that doesn't work"" and that testing for travellers coming to England should happen before departure, rather than after arrival.
He also ruled out requiring proof of vaccination for passengers on short-haul routes after Australian carrier Qantas said it would require it for travellers.
When Mike Hansford, 27, travels to the Canary Islands for Christmas next month, he and his family will already have paid a private company Â£120 each for a Covid test. The Spanish island insists that visitors produce a negative Covid test done within the previous 72 hours when they reach its shores.
""We have looked into the NHS's service,"" said Mr Hansford. ""However, what we were explicitly told by the Foreign Office was: 'Do not go to the NHS for your Covid test. You must have one done privately'.""
Mr Hansford, who is travelling with his wife Lucy, her parents and two friends, said he assumed they were told to go private to keep tests free for people who are not well, as well as guaranteeing the results arrive in time.
On the way back, Mr Hansford said it was highly unlikely he and his wife would pay a further Â£120 for another private test as both work from home and are happy to quarantine for 14 days.
""But I'm not sure how my friends who I'm travelling with will feel,"" he said. ""One of them is a builder and a carpenter by trade so I think it is safe to say he'd rather probably avoid a two-week quarantine.""
The PM said the latest news regarding the University of Oxford's vaccine was""incredibly exciting"".
However, he also warned that the virus would not grant a ""Christmas truce"" and urged families to make a ""careful judgement about the risks of visiting elderly relatives"" ahead of a UK-wide approach to Christmas being announced later this week."
"Not everyone cheered for the school children striking against climate change. In the US, democratic senator Dianne Feinstein accused them of “my way or the highway” thinking. German Liberal Democrats leader Christian Lindner said that the protesters don’t yet understand “what’s technically and economically possible”, and should leave that to experts instead. The UK’s prime minister, Theresa May, criticised the strikers for “wasting lesson time”. These criticisms share a common accusation – that the striking children, while well-intentioned, are behaving counter-productively. Instead of having a rational response towards climate change, they let emotions like fear and anger cloud their judgement. In short, emotional responses to climate change are irrational and need to be tamed with reason. The view that emotions are intrusive and obscure rational thinking dates back to Aristotle and the Stoics – ancient Greek philosophers who believed that emotions stand in the way of finding happiness through virtue. Immanuel Kant – an 18th-century German philosopher – saw acting from emotions as not really agency at all. Today, much of political debate is moderated with the understanding that emotions must be tamed for the sake of rational discourse. While this view stands in a long tradition of Western philosophy, it invites Jordan Peterson and Ben Shapiro to insist that “facts, reason and logic” can dismiss an emotional response to anything in debates. However, the view that emotions aren’t part of rationality is false. There’s no clear way of separating emotions from rationality, and emotions can be rationally assessed just like beliefs and motivations. Imagine you’re walking in the woods, and a huge bear approaches you. Would it be rational for you to feel fear? Emotions can be rational in the sense of being an appropriate response to a situation. It can be the correct kind of response to your environment to feel an emotion, an emotion might just fit a situation. Fear from a bear coming towards you is a rational response in this sense: you recognise the bear and the potential danger it represents to you, and you react with an appropriate emotional response. It could be said to be irrational not to feel fear as the bear walks towards you, as this wouldn’t be a correct emotional response to a dangerous situation. Imagine you find out that a meteor will kill millions of people across the world, displace hundreds of millions more, and make life for the remainder of humanity much worse. The world’s governments neither put a defence system in place, nor do they evacuate the people threatened. Fear from the meteor, and anger at the inaction of governments, would be a rational response as they are an appropriate reaction to danger. And if you don’t feel fear and anger, you’re not appropriately responding to a dangerous situation. As you’ve probably guessed, the meteor is climate change. The world’s governments aren’t addressing the causes of climate change or preparing to mitigate its impact. For the people of Mozambique, who are reeling from the devastation of Cyclone Idai, anger is entirely appropriate. Climate change is largely a product of economic development in richer countries, while the world’s poorest are bearing the brunt of its effects. Regardless of how fitting an emotional response is, it may sometimes be unhelpful for what a person wants to achieve. Theresa May makes this point about the school strike: understandable, but young people missing valuable lessons makes it harder for them to solve climate change. As others have already pointed out, climate change demands rapid action – waiting until some vague point in the future when the children are old enough to do something is relinquishing responsibility instead of meaningful action. It is, however, hard to deny that fear and anger sometimes lead people to choices they regret. However, dismissing emotional responses on this basis is too quick. There are many examples where fear and anger have triggered the correct response and created a motivational push for change. As Amia Srinivasan, an Oxford philosopher working on the role of anger in politics, puts it,  Anger can be a motivating force for organisation and resistance; the fear of collective wrath, in both democratic and authoritarian societies, can also motivate those in power to change their ways. A lot of social change has happened because of anger against injustice, empowering the weak and oppressed, while causing those in power to fear they may be ousted leads to reforms and change. We do need scientific understanding of the climate crisis to solve it, but banning emotions from the debate and dismissing rational fear and anger about climate change may encourage people to do nothing.  So, not only are children, who are angry and scared about climate change, rational, they might be more so than the adults criticising them. Emotions play a bigger part in life beyond rationality – they mark values and indicate what people care about. Fear of the future and anger at inaction are ways young people can express their values. Their emotions are, in the words of feminist writer Audra Lorde, an invitation to the rest of society to speak. Dismissing the emotions of school children not only invalidates their rational responses to a grave situation – it implicitly states that their values aren’t taken seriously, and that adults don’t want to reach out to them. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Like Others Of Its Ilk, The Minnesota Public Radio Censors Comments On Its Climate Blog
Guest Post by Bob Tisdale
Click to visit website
This morning while checking blogs with the phrase “sea surface temperature” I happened on the Minnesota Public Radio Updraft © climate change blog. Meteorologist Paul Huttner authored a post there titled “Could 2010 be the hottest year ever?” Link:
http://minnesota.publicradio.org/collections/special/columns/updraft/archive/2009/10/could_2010_be_the_hottest_year.shtml
The post begins with, “The numbers are in, and it looks like the “global cooling” theory just melted away.” It has the requisite link to the typical news release (Seth Borenstein’s (AP) article “Statistics experts reject global cooling claims”) and a two-year-old GISS Annual Global Temperature Anomaly Graph, even though a graph of current data would have better helped his cause. But what struck me and caused me to comment there was, first, Huttner’s use of the Climate Change Attribution graph…
 http://i39.tinypic.com/2s0o2uo.jpg
…which he wrongly attributes to Kerry Emanuel, and, second, his projection that 2010 could be the warmest on record while hinting that ENSO would ultimately be responsible for it.
I felt obligated to advise him of his error in attribution of the graph and of the fact that the Climate Change Attribution graph uses outdated TSI data. I also reinforced the ENSO-global climate link over the past decade by quoting from Knight et al (2009), but noting that Knight et al make an error in their assumption that the relationship between ENSO and global temperature is linear. Here’s what I wrote:
############
Paul Huttner: A few things. You attribute the Climate Change Attribution graph to Kerry Emanuel, but it’s actually from Global Warming Art:
http://www.globalwarmingart.com/wiki/Image:Climate_Change_Attribution_png
The graph is obsolete. It relies on an outdated (1993) Hoyt and Schatten TSI reconstruction that was manufactured, in part, to explain the rise in global temperature in the first half of the 20th Century. The current understanding of TSI variability shows little change in solar minimum:
 http://i40.tinypic.com/zjb977.jpg
I discussed this in detail here:
http://bobtisdale.blogspot.com/2009/01/agw-proponents-are-two-faced-when-it.html
As you imply, global temperature variations are dictated by ENSO. This is confirmed by Knight et al (2009) “Do global temperature trends over the last decade falsify climate predictions?”:
http://www.metoffice.gov.uk/corporate/pressoffice/2009/global_temperatures_09.pdf
They write, “El Nino–Southern Oscillation is a strong driver of interannual global mean temperature variations. ENSO and non-ENSO contributions can be separated by the method of Thompson et al. (2008) (Fig. 2.8a). The trend in the ENSO-related component for 1999–2008 is +0.08 +/- 0.07 deg C decade–1, fully accounting for the overall observed trend. The trend after removing ENSO (the “ENSO-adjusted” trend) is 0.00 +/- 0.05 deg C decade–1, implying much greater disagreement with anticipated global temperature rise.”
So there hasn’t been the anticipated rise in global temperature because, after you remove the effects of ENSO, the trend is zero. Therefore, if this year is a record year, it should be attributable to ENSO, not AGW.
Also note that Knight et al (2009) assume the relationship between ENSO and global temperature is linear. It is not.
http://bobtisdale.blogspot.com/2009/09/relationship-between-enso-and-global.html
Have a nice day.
############
And what did Meteorologist Paul Huttner do?
He rejected my comment.
UPDATE from MPR:
The missing comments were indeed being caught up in a spam filter. I’ve released the unpublished comments and they should be visible on the site now.
The comments that didn’t post had a large number of hyperlinks – suspect that’s why the filter didn’t like them.
Paul does not screen comments beforehand.
— Ken Paulman, managing editor for online news, MPR
Posted by Ken Paulman, MPR News | October 30, 2009  5:24 PM
It did take them almost 36 hours to find this and correct it.

 
< Sun wakes up: Strongest sunspot this year? | Main | Fall photos on a sunny day >

Could 2010 be the hottest year on record?
Posted at  8:59 AM on October 27, 2009    by Paul Huttner   (67 Comments)
The numbers are in, and it looks like the “global cooling” theory just melted away.
A new independent statistical analysis of climate records for the past 130 years confirms that the global temperature trend continues upward. The study was performed for The Associated Press by four independent university statistics experts. The four were given blind data sets and asked to analyze the trends, not knowing they were analyzing temperature data to remove any possible bias.

NASA annual surface temperature anomaly relative to 1951-1980 mean, based on surface air measurements at meteorological stations and ship and satellite measurements of sea surface temperature.
Some climate change skeptics have been claiming that the earth has been cooling since 1998, which until that time was the hottest year in the 130 global year surface record. 2005 was slightly hotter according to a NASA analysis. According to NOAA the last 10 years are the hottest decade anywhere in the modern global historical record.
It is remarkable statistically that the 13 warmest years in the modern record have all occurred since 1990. The fact that the 13 warmest years since 1880 could have occured by accident after 1990 corresponds to a likelihood of no more than 1:10 000. That’s the equivalent of flipping a coin and having it come up “heads” 14 times in a row.
Some climate change skeptics point to solar variability as the primary reason for climate changes on earth. The problem is, we’ve just observed two of the least active sunspot years in the last century in 2008 and 2009 during the current solar minimum. You would expect then that those two years would be cooler than average globally if the solar cycle theory is valid.
Instead, 2008 was the 8th warmest year in the global temperature record. And event though parts of the U.S. have been running cool in this year, globally 2009 is on pace to be the 6th warmest year on record. That pretty much shreds the solar variability only theory on global temperatures. Why did we observe two “top 10” warmest years during the lowest period of solar activity in nearly a century? Something else is at play here. Atmospheric changes are likely overcoming any natural solar variability.

Climate forcing graph shows solar variability as a much smaller climate change forcing component than greenhouse gasses.
(Sent to me by Kerry Emanuel MIT, based on Meehl et al. (2004) courtesy globalwarmingart.com)
This brings us to 2010, which is right around the corner. Several key elements appear to be in place that could produce one of the hottest years, if not the hottest year, in the modern global record.
1) The cooling effects of La Nina are gone in the Pacific Ocean. A moderate El Nino is gaining strength as we enter 2010. This may aid a rise in global temperatures in 2010.
2) The deepest solar minimum in nearly a century appears to be over. Sunspot 1029 formed rapidly this week and is the strongest this year. This could indicate the ramping up of solar cycle 24. Most astronomers expect a dramatic increase in solar activity in 2010.
If all these elements fall into place and the trend of recent decades continues, 2010 could be one of the hottest years on record.
Stay tuned.
PH


Comments (67)
Thanks for the nice article. I came across an article at www.icecap.us entitled “Comments on AP story: Statistics experts reject global cooling claims” that suggested the study overlooked something called the upper ocean heat content. What do you think about that?
Thanks.
Posted by Andy  | October 27, 2009  3:38 PM

Hi Andy:
 
Thanks for the comment.
I think the evidence is overwhelming that all measures of the planet are statistically much warmer than any sort of normal level in recent history would suggest.
Let’s see where we are after 2010 and in the next 5 to 10 years.
PH
Posted by Paul Huttner  | October 27, 2009  5:04 PM

Paul Huttner: A few things. You attribute the Climate Change Attribution graph to Kerry Emanuel, but it’s actually from Global Warming Art:
http://www.globalwarmingart.com/wiki/Image:Climate_Change_Attribution_png
 
The graph is obsolete. It relies on an outdated (1993) Hoyt and Schatten TSI reconstruction that was manufactured, in part, to explain part of the the rise in global temperature in the first half of the 20th Century. The current understanding of TSI variability shows little change in solar minimum:
http://i40.tinypic.com/zjb977.jpg
I discussed this in detail here:
http://bobtisdale.blogspot.com/2009/01/agw-proponents-are-two-faced-when-it.html
As you imply, global temperature variations are dictated by ENSO. This is confirmed by Knight et al (2009) “Do global temperature trends over the last decade falsify climate predictions?”:
http://www.metoffice.gov.uk/corporate/pressoffice/2009/global_temperatures_09.pdf
They write, “El Nino–Southern Oscillation is a strong driver of interannual global mean temperature variations. ENSO and non-ENSO contributions can be separated by the method of Thompson et al. (2008) (Fig. 2.8a). The trend in the ENSO-related component for 1999–2008 is +0.08 +/- 0.07 deg C decade–1, fully accounting for the overall observed trend. The trend after removing ENSO (the ‘ENSO-adjusted’ trend) is 0.00 +/- 0.05 deg C decade–1, implying much greater disagreement with anticipated global temperature rise.”
So there hasn’t been the anticipated rise in global temperature because, after you remove the effects of ENSO, the trend is zero. Therefore, if this year is a record year, it should be attributable to ENSO, not AGW.
Also note that Knight et al (2009) assume the relationship between ENSO and global temperature is linear.  It is not.
http://bobtisdale.blogspot.com/2009/09/relationship-between-enso-and-global.html
Have a nice day.
Posted by Bob Tisdale | October 29, 2009  4:32 AM

Here is a history of temperatures in Illinois. 2009 will likely be one of the coldest years on record. We could use a little warming so I hope you’re right. The state crop yield was horrible because of the cold.
 
http://www.isws.illinois.edu/atmos/statecli/Climate_change/iltren-temp.png
Posted by Windy City Kid  | October 29, 2009  8:23 PM

Could you give an example of why a comment would not be accepted on this site? Even from a fellow meteorologist?
 
Posted by Sera  | October 30, 2009  2:12 AM

Let’s parse that AP article:
 
“The statisticians, reviewing two sets of temperature data, found no trend of falling temperatures over time.“
Strawman. 2009 is warmer than 1979 and 1880. But the period between those two start points is not what skeptics have in mind by “over time.” They are referring to the most recent trend.
“And U.S. government figures show that the decade that ends in December will be the warmest in 130 years of record-keeping.”
Another technically correct pseudo-refutation. Since the first half of that period preceded heavy man-made CO2, and therefore warmed from another cause, it indicates there’s a non-anthropogenic component to the long-term warming trend—a component that could still be active. (I.e., the rebound from the LIA.)
“Global warming skeptics are basing their claims on an unusually hot year in 1998.”
Another strawman. Most skeptics (on WUWT, anyway) don’t choose 1998 as their starting point. Instead, they either claim it’s been cooling during the present century, or since 2002, or since 2004.
“They say that since then, temperatures have fallen — thus, a cooling trend. But it’s not that simple.”
A red herring (diversion). It IS that simple, because a short-term flattening and cooling trend falsifies the IPCC’s prediction for this decade, casting doubt on its models’ reliability; because it casts doubt on the implacability (and the urgency of the threat) of CO2’s alleged “forcing”; and because the PDO has flattened and turned negative at about the same time, which suggests that the PDO is the climate “forcer,” not CO2.
If a patient has a fever and the fever “breaks,” that breakage can’t be waved aside with the diversionary argument that the temperature decline hasn’t lasted long enough to be a long-term trend. No one is claiming it is a long-term trend–just that the fever (most likely PDO-driven) has broken.
Posted by Roger Knights  | October 30, 2009  2:14 AM

The Climate Change Attribution chart you show is now known to be incorrect.
1. The sun TSI figures are out of date.
http://www.leif.org/research/TSI-LEIF.pdf
2. Sulphate emissions were once thought to explain the cooling which took place between 1945-1975. It is now known that these emissions were very localised & were almost exclusively in the northern hemisphere and could not have had a global cooling effect.
This is a problem for the GHG hypothesis. How can one then explain the warming in the 1920’s & 1930’s, not dissimilar to the degree & pace of the 1980’s & 1990’s warming? How can we explain the cooling which followed the warming in the earlier period?
 
Posted by Geoff  | October 30, 2009  2:56 AM

I have read Anthony Watts comment which you rejected. It seems perfectly valid and reasonable to me. Perhaps you could explain why you decided to censor his comments.
 
Posted by Rodney Molyneux  | October 30, 2009  3:09 AM

Please explain why Bob Tisdale’s science-based comment was rejected.
 
Posted by Molon Labe  | October 30, 2009  3:23 AM

Paul,
 
I’ve read Bob Tisdale’s comment to your post and don’t understand why you haven’t replied to it. Here is the link to your colleague Anthony Watts’ WUWT blog site where it is posted so it can be compared to your post and evaluated:
http://wattsupwiththat.com/2009/10/29/minnesota-public-radio-cant-handle-comments-on-climate-change/
Your reply that “I think the evidence is overwhelming that all measures of the planet are statistically much warmer than any sort of normal level in recent history would suggest.” may be fine if you exclude the more reliable RSS and UAH satellite temp results showing near anomaly levels. But, why do that in favor of unreliable and openly selective GISS and NOAA results?
Posted by CO2isLIFE  | October 30, 2009  4:19 AM

Regarding the “Global Climate Change” graphic at the top of this page, what is really being measured? Does the Global Historic Climate Network (GHCN) measure the temperature of the earth or the temperature of the network?
 
The two are not the same.
The GHCN and its companion United States Historic Climate Network have a troubling problem with site quality. One that has been well documented by Climatologist Roger Pielke and Meteorologist Anthony Watts.
Many of us Minnesotans have seen the hilarious photo of an air conditioner in Detroit Lakes that exhausts hot air into a weather station sensor. However, few of us are aware of the more subtle problems – like with the station at Zumbrota where an asphalt parking lot has encroached on the USHCN station there.
No wonder NOAA and NASA claim that satellite data is “cooler”.  It lacks their station siting bias.
Posted by GregS  | October 30, 2009  5:24 AM

Paul,
 
RSS and UAH satellite temperature records exclude high latitude arctic regions which show the highest temperature anomalies. This is why they show a slightly lower warming tend the GISS record which includes these regions.
Regards,
Chris
Posted by Chris  | October 30, 2009  6:32 AM

Meteorologists are not having much success predicting annual or even seasonal temperatures of late. The UK Met Office similarly predicted that 2007 would be the hottest year ever (in Jan 2007) only for average temperatures to drop with the result that it was one of the coolest this century. And they have been wrong for about 6 winter / summer seasons in a row predicting mild winters and barbeque summers.
 
Predictions would be somewhat more believable if you could adequately explain why the IPCC2001 predictions have so far completely failed to materialise. Plotting actual temperatures against the predictions (p34 of the Summary report for policy makers) shows them underneath the ENTIRE RANGE. Why is that ?
Rgds
Imran
PS Its not cool to reject comments that are factual and scientific.
Posted by Imran  | October 30, 2009  6:46 AM

Chris,
 
What is the source of your assertion that RSS and UAH “exclude” high latitude arctic regions?
As for GISS including these regions, I would hardly credit a single thermometer in areas larger than Texas, as in inclusion.
On the other hand, to achieve the “hottest years ever” claim, NOAA has excluded the more accurate satellite data set of the oceans, and reverted to reliance on reports from buoys and tramp steamers.
It is all more the stuff of politics than science.
Posted by GregS  | October 30, 2009  6:52 AM

That last graph is awesome! The drop in sulfates and volcanics show an uncanny correlation to the rise in temperatures, far more than the CO2 does.
 
Volcanics have been proven and witnessed to have far more affect on climate than trace gasses.
Posted by Rick  | October 30, 2009  7:04 AM

I find it disturbing that an NPR related site would use outdated, politicized graphs and data, and then reject a comment from a highly qualified responder that merely attempts to update and de-politicize the graphs and data. Censorship of this type seems highly Nixonian, and contrary to the innate mores of NPR. Doesn’t TRUTH matter anymore?
 
Posted by Mike O’Kelly  | October 30, 2009  7:06 AM

I’m a bit curious as to why the second graph seems to end in the early 1990’s? Surely, we have some more current information. I’m also intrigued by the term, “greenhouse gases”. It’s a very prominent line, but quite a broad term, really.
 
Posted by BradH  | October 30, 2009  7:26 AM

Why is a government funded media outlet misrepresenting the facts so blatantly?
Journalism used to be about skeptical, tough looks.
Now, especially in the publicly funded media, the job is to sell the leftist view of any given issue.
That not one global warming prediction has been accurate will not change, no matter how much spin, misleading reporting, or suppression tax payer supported media engages in.
 
Posted by hunter  | October 30, 2009  7:29 AM

One thing in the statistical review article that seems curious to me is that their description of the data is not what anyone would use to describe the CO2 concentration over time (the Keeling curve). Given that CO2 increases are the foundation of the the Anthropogenic Global Warming theory, perhaps something other than CO2 is important.
 
Personally, I like Akasofu’s hypothesis that shows good correlation with a steady recovery from the Little Ice Age plus a 60 year periodic oscillation that fits the PDO.
See the full paper at http://people.iarc.uaf.edu/~sakasofu/pdf/two_natural_components_recent_climate_change.pdf
or comments and discussion at
http://wattsupwiththat.com/2009/03/20/dr-syun-akasofu-on-ipccs-forecast-accuracy/
[Aside – my url is down, apparently due to too many downloads.  It’ll be back Nov 1 or sooner if I throw money at the problem.]
Posted by Ric Werme | October 30, 2009  7:29 AM

I just read that you rejected a comment to this article from Bob Tisdale.   His comment can be found here:
 
http://bobtisdale.blogspot.com/2009/10/like-others-of-its-ilk-minnesota-public.html
I have read it, and it does not appear to contain anything that would cause a blog moderator to reject it.
Please post his comment so the readers here can see some well-documented information which is very pertinent to this article.
Thanks.
Posted by Fred C  | October 30, 2009  7:31 AM

Like Rodney Molyneux and other posters, I’d like to know how you can justify removing posts by those, such as Anthony Watts, who are capable of providing a coherent alternative to your arguments. What would be the public service in denying your readers the opportunity of the realization that you might be wrong? Or do the ends justify the means?
 
Best Regards,
Dr. Stritmatter
Posted by rstritmatter  | October 30, 2009  7:34 AM

Giss must be measuring another Arctic to the Danes.
 
Is there two of them?
http://ocean.dmi.dk/arctic/meant80n.uk.php
Posted by Ripper  | October 30, 2009  7:59 AM

As an Australian I am gobsmacked that censoring an esteemed person such as Bob Tisdale could happen in the so called land of the free.
 
Posted by Ripper  | October 30, 2009  8:01 AM

Silly article, silly “guess” at what will happen in 2010.  Yes, it’s warmer than it was in 1900, or 1901, or 1902 etc.
 
Warming trend since 1998 according to Nasa
http://www.woodfortrees.org/plot/gistemp/from:1998/plot/gistemp/from:1998/trend
Cooling trend since 1998 according to Hadley
http://www.woodfortrees.org/plot/hadcrut3vgl/from:1998/plot/hadcrut3vgl/from:1998/trend
Warming trend since 1999 Nasa
http://www.woodfortrees.org/plot/gistemp/from:1999/plot/gistemp/from:1999/trend
Cooling rend since 2001Nasa
http://www.woodfortrees.org/plot/gistemp/from:2001/plot/gistemp/from:2001/trend
Cooling trend since 2002 Nasa
http://www.woodfortrees.org/plot/gistemp/from:2002/plot/gistemp/from:2002/trend
Do they not teach math or statistics in University level meteorology??
Posted by Dan Robinson, PE  | October 30, 2009  8:42 AM

I warn you skeptics with great warning: Gaia will not be mocked! Cease all this endless caterwauling, or face her wrath!!! There are Three things, Three things you must do for Gaia: You must cease your mockery of the Faith, you must cease your vile consumption of meat, and you must make regular offerings to her prophets through the purchases of “carbon credits.” Oh, and you must dramatically reduce your industry and your emissions! Four, These Four things you Must do for Gaia, or She will smite you with great burning and endless woe and a really nasty heat rash!!!
 
Take heed, oh ye unbelievers!!!
Posted by The Goracle  | October 30, 2009  8:51 AM

Your rejection of the salient and respectful comments of Bob Tisdale concerning your post is very telling. A person of intellectual honesty and integrity would not do such a thing.
That implies you are not such a person.
 
Posted by Preston Calvert  | October 30, 2009  9:34 AM

As a native Minnesotan I am very disappointed in your lack of ethics in censoring a scientific comment. Pointing to the obvious propaganda piece by Seth Borenstein demonstrates a total lack of critical thinking. It is so blatantly cherry picked and unscientific.
 
Climate science is in its infancy. The warming claims are being made based on questionable data and simplistic computer models. Why do you think climate researchers are so much smarter than medical researchers that can’t cure the common cold (or cancers and hundreds of other diseases)? Yet, somehow in just a few years they’ve diagnosed the problem with a much more complex system called Earth and have a cure. More taxes. Why would anyone with an iota of common sense believe claims made about a poorly understood chaotic system when we already know that no scientist in any other field would make such bold claims with such limited knowledge?
Truly mind boggling.
Posted by Richard M  | October 30, 2009 10:00 AM

In the last 2 weeks the Pew Poll and the Harris Poll has indicated that the percentage of Americans who think man-made global warming is real has declined significantly, to about 36%.
 
This issue deserves fair and objective coverage. There is no shortage of knowledgeable people who can speak for the climate sceptics. Why is this side of the debate being ignored, on a public funded station?
Posted by r.wright  | October 30, 2009 10:11 AM

Do you really mean “hottest year ever”, or just the hottest year since the late 1800’s when we have some form of temperature record?
 
If reporters were honest with the facts then maybe the climate change discussions could be more reasonable. The fact that the earth has been much hotter in the past (and survived without any tipping points) is usually not mentioned and in fact hidden by headlines such as yours. Presenting the full facts to people might allow them to make sense of the discussions, rather than sensationalist headlines.
I wish the media would actual perform real journalism on climate change, where’s a good piece showing how the temperature anomaly graphs are created, the fact they use proxies, different number of temperature stations, how sparse the coverage is for a global temperature and how a global temperature is even calculated.
Posted by climatebeagle  | October 30, 2009 10:15 AM

“It’s not hard to hear consensus if you don’t hear any disagreement.”
 
Posted by vanderleun | October 30, 2009 12:15 PM

Where is the report written by these expert statisticians?
 
The only report I am aware of written by expert statisticians on climate controversies was the Wegman report, which confirmed Steve McIntyre’s criticism of the hockey stick picture.
Your headline is up there with those that claimed in 2007 that the arctic would be ice-free in 2008.
The AP article is full of utter nonsense, for example
“Since 1998, temperatures have dipped, soared,…”
There has been no soaring at all, in fact temperatures have levelled out since 1998 – even the head of the IPCC (Pachauri) has acknowledged this.
Posted by PaulM  | October 30, 2009 12:16 PM

On the other hand, although the silence from the author continues apace, it is indeed fortunate Watts noticed this item. Otherwise it would have the blog’s average comment stream: zero to two.
 
Posted by vanderleun | October 30, 2009 12:20 PM

One of the many problems with your report is that the GISS NASA data is dry-labbed. Hansen’s inscrutable algorithms massage (i.e. change) even recorded temperature data from the 1800’s to match his political beliefs. Add the urban heat island effect (which his data-changing algorithms exacerbate instead of mitigate), the unreliability of surface station data (caused by land use changes, for example paving a parking lot right next to the sensors, and moving sensors to be near or on top of buildings so that they can be automated), and the use of small numbers of measurements to cover vast unpopulated areas (thousands of square miles), and you find that GISS is just not trustworthy. It should never be used. Any time I see it used in an article, I disregard all of the author’s conclusions, because a reputable author who has done his homework would know the issues regarding it.
 
Posted by Scott  | October 30, 2009 12:39 PM

Minnesota Public Radio:
 
“Our Mission is to enrich the mind and nourish the spirit, thereby assisting our audiences to enhance their lives, expand perspectives and strengthen their communities.”
How can one enrich the mind and nourish the spirit if one refuses to listen to another point of view? There is more than one perspective to this global warming business and only honest, unbiased reporting by publicly funded media  – can deliver it.
Posted by Richard Just | October 30, 2009  1:05 PM

FYI:
 
I have not rejected Bob Tisdale’s or any other comments on the site. I (and MPR) accept all comments as long as they do not have profanity etc.
If a comment did not appear it was a techincal error. Please re-submit any comments.
You guys must be posting from Australia or something as many of the commetns came in the wee hours of the morning here. Don’t you guys sleep?
I was off duty at an appointment this morning through midday here Minnesota time. I do appreciate the comments and traffic!
More soon…
PH
Posted by Paul Huttner  | October 30, 2009  2:16 PM

//That last graph is awesome! The drop in sulfates and volcanics show an uncanny correlation to the rise in temperatures, far more than the CO2 does.
 
Volcanics have been proven and witnessed to have far more affect on climate than trace gasses.
Posted by Rick | October 30, 2009 7:04 AM //
Yes Rick, large volcanic eruptions have a significant temporary global cooling effect. Tambora and Pinatubo are great examples. They just don’t seem to occur often enough to play a role in long term climate.
PH
Posted by Paul Huttner  | October 30, 2009  2:47 PM

Is there a good reason why the graph shows an increasing solar influence when the current consensus is that over the long term (excluding the pseudo-11 year cycle) total Solar irradiance is near enough constant?
 
Posted by Sean Houlihane  | October 30, 2009  2:51 PM

Minnesota Public Radio:
 
“Our Mission is to enrich the mind and nourish the spirit, thereby assisting our audiences to enhance their lives, expand perspectives and strengthen their communities.”
//How can one enrich the mind and nourish the spirit if one refuses to listen to another point of view? There is more than one perspective to this global warming business and only honest, unbiased reporting by publicly funded media – can deliver it.
Posted by Richard Just | October 30, 2009 1:05 PM //
Richard: I think you can see many perspectives right here in these blog comments. And MPR is roughly 90% funded by our wonderful members’ contributions and underwriting.
People support us precisely because we give a fuller, deeper, more balanced approach to news than any other media outlet. That is why we are the clear number one rated radio station in this market.
PH
——————————————————————————–
Posted by Paul Huttner  | October 30, 2009  2:58 PM

//Do you really mean “hottest year ever”, or just the hottest year since the late 1800’s when we have some form of temperature record?
 
Posted by climatebeagle | October 30, 2009 10:15 AM //
Beagle:
Yes, a more accurate title might have been “Hottest Year on Record?”
I will change it.
Thanks..
PH
Posted by Paul Huttner  | October 30, 2009  3:03 PM

//As a native Minnesotan I am very disappointed in your lack of ethics in censoring a scientific comment. Pointing to the obvious propaganda piece by Seth Borenstein demonstrates a total lack of critical thinking. It is so blatantly cherry picked and unscientific.
 
Posted by Richard M | October 30, 2009 10:00 AM //
As I posted here, no comments have been censored. I have asked Mr. Tisdale to re-post his comment. If it did not make it thought it was purely a technical reason. It would be nice if people would check these things out before they claim “censorship.”
Clearly you can see all of the other posts made it through.
MPR does not censor commentary on blogs.
PH
Posted by Paul Huttner  | October 30, 2009  3:11 PM

I am SHOCKED that MPR would allow the above blog, but not the comments of comments of Bob Tisdale. As pointed out by Anthony Watts, many of what you call facts are outdated or just wrong.
 
For those who aren’t drunk on Al Gore’s cool-aid, and would like to become informed on this subject, Try going to Watts Up Wuth That.
Fewer Americans now believe in the Global Warming hoax than believe in Haunted Houses.  Eventualy the truth will prevail.
2009 Hottest year BALONEY.
Posted by Ronald Hansen  | October 30, 2009  3:15 PM

There are few things more disappointing than an NPR reporter refusing to accept valid criticism and then hiding behind the excuse of “technical error”.
 
The bottom line is Tisdale is correct and you made an error. A grotesque error. Not only in your analysis (which obscures the real science and does it a tremendous disservice), but in the intelligence and knowledge of your audience.
Posted by David Walton  | October 30, 2009  3:17 PM

Bob Tisdale:
 
Neither myself nor anybody at MPR rejected you comment. It must be a technical issue. Please re-submit your comment. As you can see all other comments have posted just fine.
It would be good to check with me personally before you post a claim that MPR “censors” comments. We do not. My contact information is easily available on the MPR site.
It is ironic that all your comments must be approved by the blog author.(you) All comments to Updraft post immediately, without my approval.
From your site: “Comment moderation has been enabled. All comments must be approved by the blog author.”
Amazing.
I attempted to post this on your blog but cannot as I do not have a Google account.
PH
Paul Huttner
Chief Meteoroloigst
MPR
phuttner@mpr.org
Posted by Paul Huttner  | October 30, 2009  3:30 PM

In the web address, it reads ‘publicradio’. Is it? Or does this website, like so many other media outlets that claim to be public, just another arm of ideologists that do not wish to see anything contradicting the AGW agenda.
 
Posted by David Alan  | October 30, 2009  3:52 PM

To All:
 
Thanks so much for the great posts.
With all respect to those who somehow find a way disagree with the fact that our planet is getting warmer, just try this.
Take out a coin and try to get “heads” on a flip 14 times in a row today.
From my Updraft post:
“It is remarkable statistically that the 13 warmest years in the modern record have all occurred since 1990. The fact that the 13 warmest years since 1880 could have occurred by accident after 1990 corresponds to a likelihood of no more than 1:10 000. That’s the equivalent of flipping a coin and having it come up “heads” 14 times in a row.”
This is perhaps the most compelling data that stack the deck in favor of AGW. How can anyone account for the fact that since 1984 we have not observed one year globally cooler than the 1961-1990 average?
http://www.globalwarmingart.com/wiki/File:Instrumental_Temperature_Record_png
You would expect that half the years since 1984 would have been below that average. And yet there is NOT ONE YEAR COOLER THAN AVERAGE since 1984?
Instead, we have seen the 13 warmest years since 1990? That’s a one in 10,000 shot folks.
Global warming “skeptics” are simply on the wrong side of the data. The only way you come to a conclusion that does not recognize global climate change is if you have a preset opinion.
I am always open to credible peer reviewed science that changes scientific theory and thinking. Say what you want, but the overwhelming scientific evidence is on the side of continued planetary warming in the coming decades.
Let’s see where we are after 2010, and beyond.
Enjoy the weekend.
PH
Posted by Paul Huttner  | October 30, 2009  3:54 PM


Richard: I think you can see many perspectives right here in these blog comments. And MPR is roughly 90% funded by our wonderful members’ contributions and underwriting.
 
Paul,
My point is that given the content of the comments it is apparent there is another perspective to the global warming issue. One supported by well-meaning, honest citizens and scientists that does not get published by MPR, NPR or other mainstream media.
MPR receives 64% operating revenue from “public” sources including grants from endowments, foundations, and businesses. About 20% of your budget comes from listener/member contributions (which is admirable.)
One might reasonably speculate that the larger percentage of grants from corporate and foundations may color your selection of climate science news. Which does not necessarily “expand perspectives and strengthen their [audience] communities.”
Posted by Richard Just  | October 30, 2009  3:55 PM

Paul,
 
Re: Instead, 2008 was the 8th warmest year in the global temperature record. And event though parts of the U.S. have been running cool in this year, globally 2009 is on pace to be the 6th warmest year on record. That pretty much shreds the solar variability only theory on global temperatures. Why did we observe two “top 10” warmest years during the lowest period of solar activity in nearly a century? Something else is at play here. Atmospheric changes are likely overcoming any natural solar variability.
As a meteorologist, you no doubt understand seasonal lag. On an oceanic level, there is also a lag to heat or cool it, and then realize the affect on temps, and that lag is much longer. I think it may be a bit premature to declare the solar / temp theory “shredded.” I’m interested to hear your thoughts regarding this.
Posted by Terry  | October 30, 2009  3:58 PM

Paul Huttner: A few things. You attribute the Climate Change Attribution graph to Kerry Emanuel, but it’s actually from Global Warming Art:
http://www.globalwarmingart.com/wiki/Image:Climate_Change_Attribution_png
 
The graph is obsolete. It relies on an outdated (1993) Hoyt and Schatten TSI reconstruction that was manufactured, in part, to explain the rise in global temperature in the first half of the 20th Century. The current understanding of TSI variability shows little change in solar minimum:
http://i40.tinypic.com/zjb977.jpg
I discussed this in detail here:
http://bobtisdale.blogspot.com/2009/01/agw-proponents-are-two-faced-when-it.html
As you imply, global temperature variations are dictated by ENSO. This is confirmed by Knight et al (2009) “Do global temperature trends over the last decade falsify climate predictions?”:
http://www.metoffice.gov.uk/corporate/pressoffice/2009/global_temperatures_09.pdf
They write, “El Nino–Southern Oscillation is a strong driver of interannual global mean temperature variations. ENSO and non-ENSO contributions can be separated by the method of Thompson et al. (2008) (Fig. 2.8a). The trend in the ENSO-related component for 1999–2008 is +0.08 +/- 0.07 deg C decade–1, fully accounting for the overall observed trend. The trend after removing ENSO (the “ENSO-adjusted” trend) is 0.00 +/- 0.05 deg C decade–1, implying much greater disagreement with anticipated global temperature rise.”
So there hasn’t been the anticipated rise in global temperature because, after you remove the effects of ENSO, the trend is zero. Therefore, if this year is a record year, it should be attributable to ENSO, not AGW.
Also note that Knight et al (2009) assume the relationship between ENSO and global temperature is linear. It is not.
http://bobtisdale.blogspot.com/2009/09/relationship-between-enso-and-global.html
Have a nice day.
Posted by Bob Tisdale | October 30, 2009  4:17 PM

You MPR scientific types are getting a reputation. Is it true your easy?
 
Posted by Fred J Harris  | October 30, 2009  4:30 PM

“Take out a coin and try to get “heads” on a flip 14 times in a row today.”
 
Paul, please don’t be so condesending. Many of the skeptics are hard core statisticians. No one is aurguing that the climate is Bernoulli process (like a coin toss process).
The correct question is: how likely is it to see the range of temperatures of the past few years, given the historical temperature range during this interglacial period?
To answer that question, you must know what the temperature variability has been over the past few thousand years.
Just because Google’s stock has traded high for the past month doesn’t mean that its in a run-away race condition and will never cycle back to lower prices.
Posted by mpaul  | October 30, 2009  4:39 PM

@Paul Huttner
 
” All comments to Updraft post immediately, without my approval.”
“Clearly you can see all of the other posts made it through.”
I posted a comment early this morning regarding the fact that GISS not only show more of a warming trend than RSS and UAH; but it also shows more of a warming trend than HadCRUT3. That comment elicited a reply that my comment was being held in moderation pending approval of the blog owner. The comment never made it through.
Posted by Dave Middleton  | October 30, 2009  4:42 PM

Paul Huttner: You wrote, “Neither myself nor anybody at MPR rejected you comment. It must be a technical issue. Please re-submit your comment. As you can see all other comments have posted just fine.”
 
I resubmitted my comment at ~5:15PM today and received the following reply page.
#####
Updraft
Minnesota Public Radio chief meteorologist Paul Huttner blogs about our region’s favorite conversation starter.
Thank you for commenting.
Your comment has been received and held for approval by the blog owner.
Return to the original entry.
####
Check your spam filter, Paul.
Posted by Bob Tisdale | October 30, 2009  4:43 PM


Paul,
 
My point is that given the content of the comments it is apparent there is another perspective to the global warming issue.
Posted by Richard Just | October 30, 2009 3:55 PM
//Hi Richard: “perspective” and “science” are two different animals. This is especially true in climate change discussions.//
PH
One might reasonably speculate that the larger percentage of grants from corporate and foundations may color your selection of climate science news. Which does not necessarily “expand perspectives and strengthen their [audience] communities.”
Posted by Richard Just | October 30, 2009 3:55 PM
Richard: I can assure you that NO ONE at MPR has or ever will tell me what to discuss or publish regarding climate change. If they did, I would walk immediately and make it public.
In fact MPR has the highest journalistic ethics of any news organization I have ever been fortunate enough to work for. And I have been fortunate enough to have worked at some of the best; including WCCO-TV in the Twin Cities and WGN-TV in Chicago.
My analysis and perspective on global climate change are my own, and are not dictated or influenced in any way by MPR or it’s supporters.
PH
Posted by Paul Huttner  | October 30, 2009  4:51 PM

Paul Huttner: You wrote, “Neither myself nor anybody at MPR rejected you comment. It must be a technical issue. Please re-submit your comment. As you can see all other comments have posted just fine.”
 
I resubmitted my comment at ~5:15PM today and received the following reply page.
#####
Updraft
Minnesota Public Radio chief meteorologist Paul Huttner blogs about our region’s favorite conversation starter.
Thank you for commenting.
Your comment has been received and held for approval by the blog owner.
Return to the original entry.
####
Check your spam filter, Paul.
Posted by Bob Tisdale | October 30, 2009 4:43 PM
Hi Bob:
I’m not an IT guy, so I don’t know why you would get that message. I will be happy to forward it to those who would know.
Obviously your comment above comment made it through…along with nearly all the 50+ others. Again, I do not personally approve any comments to Updraft.
I would be happy to respond to any data you can show me. But you are not being censored in any way.
Would you please remove or change your incorrect blog post headline below?
“Like Others Of Its Ilk, The Minnesota Public Radio Censors Comments On Its Climate Blog”
Again, you could have easily contacted me personally before inaccurately claiming to be “censored.”
PH
Posted by Paul Huttner  | October 30, 2009  5:04 PM

“Skeptics are on the wrong side of the data”
 
Simple right? Except exactly what data, what time frame (no cherry picking); do we take into account the recovery from the LIA?
Do we demand that temps continue to rise with increased CO2 or excuse the lack of increase the last decade or so?
Is it necessary for skeptics to prove CO2 does not raise temps much or do AGW fans have to prove it does?
Or is it just get in, shut up and hold on?
Posted by Ed  | October 30, 2009  5:21 PM

The missing comments were indeed being caught up in a spam filter. I’ve released the unpublished comments and they should be visible on the site now.
 
The comments that didn’t post had a large number of hyperlinks – suspect that’s why the filter didn’t like them.
Paul does not screen comments beforehand.
— Ken Paulman, managing editor for online news, MPR
Posted by Ken Paulman, MPR News | October 30, 2009  5:24 PM



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e925df52a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter German leaders seem to have a habit of driving their country into a wall.
This time it’s their desire to show environmental supremacy. Many really do believe they are powerful enough to control the climate and at the same time defy the laws of economics.
The European Institute For Climate And Energy (EIKE) has a piece written by Dr. Dietmar Ufer about an interview with economics professor Joachim Weimann on MDR public television.
Video of interview here at MDR:

Here’s the text of the interview:
MDR (0:08): With respect to the current price spiral for energy, is it the right approach?
Weimann: We’ve decided on an energy transition, and on a type of energy transition that is very very expensive. That means energy is going to be very expensive. It’s going to hit the poor very hard. That was to be expected- It was completel clear. That was easily predictable. It is indeed only the start of the price spiral. We have only begun to switch off the nuclear power plants and to start using renewable energy. It’s going to be very expensive.
MDR (0:57): You believe it’s going to impact a large spectrum of citizens.
Weimann: It has to be clear that the supply of energy that we want, one that is without nuclear energy, preferably without fossil fuel and mostly from renewable energy, is an extremely expensive way of producing energy and to save CO2
MDR (1:32): Are there alternatives available that could have avoided these extreme prices?
Weimann: Of course there are alternatives. Economists have been warning for years that subsidizing renewable energies just as we are doing is a bottomless pit that will have minimal effect. Just look at the fact that just for solar energy we have invested 100 billion euros without this technology having made any notable contribution to climate protection. This is going to cost everyone, especially gthe poor and also the working class – many are going to suffer immensely.
At the 2:24 mark, the Youtube clip then looks at veteran journalist Günter Ederer who 4 months ago in April warned of the exploding cost consequences of Germany’s shock energy transition. The pain is just beginning.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGerman climate blog Readers Edition here has been keeping an eye on the winter in South America. While much of the news has been buzzing about the “record heat wave” hitting the US last week (a whopping 0.4% of the stations reported record highs! /sarc), Europe and South America for example are being left out in the cold.  

No warming in sight. Chart source: http://wxmaps.org/pix/temp8.html
In South America, dozens of people have died from the bitter cold in 7 countries so far, just when cold snaps were supposed to be getting rarer and the heat waves more frequent.  The cold is repeat of last year’s brutal South American winter.
Readers Edition writes (paraphrased):
In southern Peru, temperatures in the higher elevations of the Andes fell to -23°C. Since the beginning of last week 112 people have died of hypothermia and flu.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Coldest winter in 10 years
In Argentina the lowest temperatures  in 10 years were measured – the temperature dropped to -14°C. At least 33 people died, some froze to death and some from poisonous gases emitted from faulty heaters.
Thousands of cattle freeze to death in fields.
It was unusually cold in neighbouring countries. In the tropical regions of Bolivia where temperatures rarely fall below 20°C (68°F), the temperature hovered near 0°C.  At least four people died because of the cold. Two homeless persons died in Uruguay. Thousands of cattle froze in the fields in Paraguay and Brazil.

Natural gas shortages
In some areas of Bolivia and Peru, school was cancelled for some kids at the end of the week. Emergency shelters were opened for the homeless in larger cities. In Argentina some provinces faced natural gas shortages.
Heavy snow in Chile
Unusually heavy snows have fallen in parts of Chile. States of emergency have been declared in 8 communities with some buried in  up to 3 meters of snow. In the south of the country about 170 people have become isolated from supply lines.
Share this...FacebookTwitter "
"Not since the depths of the financial crisis has panic set in on the scale experienced over the past week. Conversations on the bus, at work and in the pub arefocused on the prospect of a deadly disease spreading uncontrollably. Stock markets are falling as fear about the coronavirus heightens and the potential cost for the world economy becomes increasingly apparent. Wall Street has suffered the fastest reversal since 1933 during the depths of the Great Depression. The Dow lost more than 10% of its value in a week from record-breaking highs to the lowest point since 2016. More than $5tn (£3.9tn) has been wiped off the value of global markets. The FTSE 100 is not immune, plunging the most in a week since the 2008 crash. Markets are expected to fall further this week. In the battle to contain the coronavirus through travel bans, school closures and the cancellation of business conferences, the damage to companies, economic growth and living standards could be unprecedented. According to Capital Economics, world growth could collapse into recession for the first time since 2009. The safeguarding of life takes primary importance as the death toll around the world climbs, but the knock-on economic impacts will also have lasting repercussions.  Given the growing impacton lives and livelihoods, it is clear that another parallel with the 2008 chaos is needed: a coordinated international response to ease fears around the globe. Against a backdrop of nationalism and protectionism in Britain, the US and several other countries, however, such thinking may be wishful. Gordon Brown led the global response to the 2008 crisis by bringing together the world’s richest nations to tackle it together. Labour’s last prime minister played an instrumental role in convening the G20 to organise the fightback by expanding government spending. Until Brown stepped in, handling the fallout had been uncoordinated and shambolic. Even though his task was complicated by a relatively protectionist US president in George W Bush, a quick glance across the Atlantic now seems to render his difficulties benign compared with the current tensions in global relations. After the London meetings of the G20 in early 2009 with the banking system on its knees and riots on the streets, the world’s wealthiest nations agreed an unprecedented and concerted fiscal expansion, promising to create or save millions of jobs which would otherwise have been destroyed. The price tag amounted to $5tn. It is hard to imagine such an approach today. The collaboration to prevent the Great Recession from becoming a rerun of the Great Depression did not, however, last long. SA slide into austerity, populism and protectionism began soon after in many nations, leaving the door open to Brexit, Donald Trump, Alternative für Deutschland (AfD), Viktor Orbán in Hungary and Jair Bolsonaro in Brazil. The world today seems to lack figures who might orchestrate a fightback against the coronavirus. Boris Johnson is missing in action, apparently unable to travel as far as northern England, the West Country or Wales to show leadership following widespread floods. Donald Trump believes that all is fine in his election year, dismissing the biggest single-day crash in US stock market in history and labelling coronavirus as the Democrats’ latest hoax. The story of the outbreak speaks volumes about the constitution of the world economy and political landscape after decades of ever-closer integration. At this time of questioning and challenging the international order, it is the tale of late capitalism writ large. Despite only joining the world trade system two decades ago as a bit-part player, China, which is at the centre of the coronavirus outbreak, accounts for around a fifth of the world economy and is key to global supply chains, a factor that great amplifies the economic fallout. Rising numbers of middle-class citizens, cheap flights and better transport have boosted tourism numbers worldwide from about 500 million in 1995 to more than 1.3 billion in 2017. People are increasingly on the move, and with them, infectious diseases. Health scares are becoming more frequent, with increasingly evident links to global heating and extreme weather events. Rising flood waters, bush fires and hurricanes restrict access to safe water, and food and sanitary living conditions, and put pressure on healthcare systems. After a decade of austerity the fear is that the NHS might not have the capacity to respond, and the eminent academic Sir Michael Marmot confirmed last week that austerity has had a disproportional impact on the life expectancy of the poorest in Britain. Pandemics are more easily spread and have a greater impact as a result of globalisation, complex supply chains, travel and tourism. A world of 24-hour news and social media fuelled by rapid advances in technology – including the spread of fake news – fans the flames of panic. It is a situation where fear can spread faster than the pathogen, according to David Owen, chief European economist at the US Bank Jefferies, who said most economic losses from infectious disease outbreaks result from the actions of unaffected individuals. There are many lessons. Globalisation, technology and climate change make the spread of viral disease easier and incubate many other social and economic ills. Lurching headlong into a protectionist and luddite world will not provide adequate and lasting solutions. The scale of our collective problems demand international coordination."
"**A well-known rest stop on the A9 in the Highlands has closed temporarily after travel restrictions led to a marked fall in visitor numbers.**
The Ralia CafÃ© near Newtonmore is in Highland, a level one area.
But much of its custom comes from people travelling to and from parts of Scotland placed under the tighter rules of levels three and four.
Since April, visitor numbers have fallen by 63% compared with the same time last year.
In a ""normal"" year the cafÃ© owners expect about 150,000 customers, with an additional 100,000 just using its toilets.
The levels system and travel rules have been put in place to supress the spread of Covid-19.
The Ralia CafÃ© is located on the A9 close to Highland Council's boundary with Perth and Kinross - a level three area.
For travellers it is the road's last comfort break going south, after Newtonmore and its facilities, before leaving the Highland Council area, and the first heading north after leaving Perth and Kinross.
The cafÃ© opened in July 2005 on the site of a former tourist information centre and is run by Robin and Sheila Lambie who live in Kingussie.
Mr Lambie said: ""We closed the cafÃ© on Sunday until travel restrictions are lifted, and my team is furloughed yet again.""
The toilets have also been closed.
The cafÃ© could open again before the end of the year when, for a short time over the Christmas break, rules on travel are due to be relaxed.
But Mr Lambie said: ""It is really impossible to tell at this stage.""
He said that despite the ""fantastic location"" for anyone travelling north or south, revenue for 2020 was currently down 63% against the same period last year.
""Going to levels three and four immediately halved our turnover and legally restricting travel halved it again.""
Everyone living in level three or level four local authority areas must, by law, remain within their own council boundaries unless they have a ""reasonable excuse"" for doing so.
People in level four must also keep journeys within their own area to an absolute minimum.
Meanwhile, people in level one or level two areas must avoid any unnecessary travel to areas that are under level three or four restrictions and should minimise unnecessary journeys between areas in different levels."
"The world’s oceans are plagued with the problem of “dead zones”, areas of high nutrients (such as nitrogen and phosphorus) in which plankton blooms cause a major reduction of oxygen levels in the water. Sea creatures need oxygen to breathe just as we do, and if oxygen levels fall low enough marine animals can suffocate. This commonly happens around coastlines where fertilisers are washed from fields into rivers and the sea, but also mid-ocean, where currents trap waters in gyres (large systems of rotating ocean currents). To date most studies have shown that these dead zones have been growing with global warming. But a recent study published in Science by Curtis Deutsch and colleagues suggests that the ocean’s largest anoxic zone – where there has been a total depletion of oxygen – in the eastern tropical North Pacific, may in fact shrink due to weakening trade winds caused by global warming.  The trade winds drive water away from the coast, and the gap is filled by new cold and nutrient-rich waters that come up from the deep. These nutrients trigger algae and plankton blooms upon which larger animals feed, which builds up an accumulation of organic matter. As bacteria decompose this organic matter the oxygen in the water is depleted. This causes low oxygen areas, such as the oxygen minimum zones (OMZs) with very low oxygen content found at intermediate ocean depths. Weaker trade winds would mean less upwelling of these deep nutrient-rich waters, and consequently less plankton and less oxygen depletion. Deutsch and colleagues affirm that although initial oxygen content will be lower due to higher temperatures, oxygen demand will decrease as trade winds do. So, the result would be that low oxygen areas in the tropical north Pacific would shrink. Natural dead zones can be found worldwide, particularly near regions where strong upwelling occurs. These natural dead zones have typically had low oxygen levels over huge lengths of time, due to ocean circulation patterns that prevent mixing. Although these OMZs are natural, they can become larger and more intense due to human activities, such as prolonged and intensive use of fertilisers, changes in land use, deforestation, soil erosion, global warming, and waste waters from cities or industry. All these are well known to cause algal blooms and so drive the expansion of oxygen-depleted areas. In fact, dead zones caused by these human factors have increased over time. Naturally occurring OMZs have also been expanding as temperature rises, so the paper’s prediction that such oxygen minimum zones would shrink flies in the face of previous studies. Animals increase their respiration rates as temperature rises, so they need more oxygen to breathe at higher temperatures. Warmer water also dissolves less oxygen, so as climate change warms the oceans the amount of oxygen decreases, making the effects on marine life even more acute. Warming also encourages water stratification, where the water separates into layers based on temperature or salinity, creating a physical barrier that prevents oxygen reaching deeper waters. Previous studies have predicted a weakening of trade winds in tropical areas, but have also forecasted changes to low-pressure weather fronts over coastlines that would lead to stronger winds, sufficient to replace any upwelling effect lost by weaker trade winds.  It seems likely that, in the same way, greater water stratification will lead to a worsening and expansion of dead zones, counteracting any effect the weakening trade winds might have to halt the process of de-oxygenation, and the paper’s authors acknowledge that this is possible. Taking everything into account, it seems that the process of warming oceans under climate change will inexorably lead to larger areas of oxygen-poor ocean, with all the knock-on effects for marine life that entails."
"There seem to have been a dozen or so explanations for why the Earth’s surface has warmed at a slower rate over the past 15 years compared to earlier decades. This is perhaps not so surprising given the complexity of the climate system – the world’s best detectives will inevitably struggle to disentangle the factors which influence every lump and bump in the surface temperature record.  However, recent research implicates natural changes in the Pacific and Atlantic oceans as the prime culprits.  Just as the apparently random motions in a river’s flow can shift before our eyes from one minute to the next, the gradual sloshing about of our vast ocean waters can influence Earth’s climate from one year to the next and from one decade to the next. It is clear that natural variability has and always will influence the climate. In addition to chaotic ocean fluctuations, changes in the brightness of the sun and variations in the frequency and intensity of volcanic eruptions (which cool the planet temporarily with sunlight-reflecting aerosol particles) influence the surface temperature. The recent Intergovernmental Panel on Climate Change working report found that these natural factors have contributed toward the slowing rate of surface warming since 1998.  However, recent measurements of ocean temperature made by thousands of automated buoys and observations of Earth’s radiative energy budget by satellite instruments indicate that heating has continued at a rate equivalent to every person worldwide using about 20 kettles each to continuously boil the oceans. This is consistent with what is expected from the rising atmospheric concentrations of greenhouse gases due to human activity. If anything, Earth’s heating rate increased between the 1985-1999 and 2000-2012 periods, despite a slowing in the rate of surface warming.  So, how is it possible for increased heating to not directly correspond with surface warming? The Earth’s heating is caused by an imbalance between the amount of absorbed sunlight and the heat emitted back to space. This surplus of heat is primarily absorbed by the oceans since they command the lion’s share of storage capacity compared with other parts of the climate system such as the land, the atmosphere or the cryosphere (ice and snow). This large heat capacity of water is noticable from the amount of time it takes to heat up your pan of vegetables. And there is a lot of water in the oceans; nearly a fifth of a cubic kilometre of water for each person on the planet. Crucially, the temperature at the Earth’s surface depends upon where this heat is deposited in the oceans. If the upper levels warm, so too will the atmosphere above. However, if ocean circulations cause more heat to be drawn down to deeper depths (or less heat to be moved upward toward the sea surface) then surface temperatures will reflect this. Recent research has implicated our largest ocean, the Pacific, as the most likely mechanism for subducting heat to deeper levels. Indeed, atmospheric and ocean conditions in the Pacific have been unusual in the past decade and computer simulations show that decades of slow surface warming despite rising greenhouse gas concentrations are associated with increased heating below 300m depth. The mechanisms for heat absorption are less clear; the simulations show that similar patterns appearing to originate from the Pacific are associated with the draw-down of heat in the North Atlantic and Southern Ocean as well as the Pacific. New research published in Science now shifts the focus towards the Atlantic Ocean. Xianyao Chen and Ka-Kit Tung of the University of Washington show that heating from rising greenhouse gas concentrations has preferentially warmed the ocean’s 300-1500m layer since about 2000, thereby depriving the upper layers of this surplus heat and causing surface warming to slow.  The authors say these changes are part of a natural cycle of knock-on effects, involving ocean circulation responses to changes in how salty (and therefore dense) the upper Atlantic Ocean layers are. This cycle is thought to last around 30 years, contributing a sustained cooling effect then a warming influence on surface temperatures; when combined with steady heating from greenhouse gas increases this leads to a “staircase” effect of stable temperatures followed by rapid warming.  They argue the previous focus on the Pacific was based upon simulations that were unable to fully capture the intricacies of the Atlantic Ocean circulation. An observed decline in the North Atlantic Ocean circulation over recent years has also been identified as part of a longer-term shift based upon evidence from computer simulations. The changes in ocean circulation have also been shown to influence seasonal extremes and, based upon the proposed Atlantic mechanism, may persist for another decade before rapid warming is re-established. However, the nature of internal ocean fluctuations means it is difficult to pin down timings with any confidence.  While it is human nature to seek a single cause for notable events, in reality the complexity of the climate system means that it is unlikely there is one simple reason for any extreme weather event or a decade of unusual climatic conditions. Nevertheless, the recent hiatus in global surface warming has encouraged scientists to further scrutinise and learn in even finer detail than before the workings of our climate system."
"
No we aren’t talking pianos, but Grand Solar Minimums. Today a new milestone was reached. As you can see below, we’ve been leading up to it for a few years.
Above: plot of Cycle 23 to 24 sunspot numbers in an 11 year window 
(Update: based on comments, I’ve updated the graph above to show the 2004 solar max by sliding the view window to the left a bit compared to the previous graph. – Anthony)
A typical solar minimum lasts 485 days, based on an average of the last 10 solar minima. As of today we are at 638 spotless days in the current minimum. Also as of today, May 27th, 2009, there were no sunspots on 120 of this year’s (2009) 147 days to date (82%).
Paul Stanko writes:
Our spotless day count just reached 638.
What is so special about 638?  We  just overtook the original solar cycle, #1, so now the only cycles above  this are: cycles of the Maunder minimum, cycles 5 to 7 (Dalton  minimum), and cycles 10 + 12 to 15 (unnamed minimum).
Since the last  one is unnamed, I’ve nicknamed it the “Baby Grand Minimum”, in much  the same way that you can have a baby grand piano. We would now seem to  have reached the same stature for this minimum.  It will be interesting  to see just how much longer deep minimum goes on.
Of course it depends on what data you look at. Solar Influences Data Center and NOAA differ by a few days. As WUWT readers may recall, last year in August, the SIDC reversed an initial count that would have led to the first spotless month since 1913:
Sunspeck counts after all, debate rages…Sun DOES NOT have first spotless calendar month since June 1913
NOAA did not count the sunspot, so at the end of the month, one agency said “spotless month” and the other did not.
From Spaceweather.com in an April 1st 2009 article:
The mother of all spotless runs was of course the Maunder Minimum. This was a period from October 15, 1661 to August 2, 1671.
It totaled 3579 consecutive spotless days. That puts our current run at 17.5% of that of the Maunder Minimum.
By the standard of spotless days, the ongoing solar minimum is the deepest in a century: NASA report. In 2008, no sunspots were observed on 266 of the year’s 366 days (73%). To find a year with more blank suns, you have to go all the way back to 1913, which had 311 spotless days (85%):

The lack of sunspots in 2008, made it a century-level year in terms of solar quiet. Remarkably, sunspot counts for 2009 have dropped even lower.
We do indeed live in interesting times.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96016f4d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The last two weeks of July are normally the hottest of the year, so it’s no surprise that we’re being deluged with public‐​service announcements about the horrors of global warming. Radio and television stations are compelled to transmit these announcements at no charge because of a long‐​standing policy that they must provide “public good.” “Don’t Litter” and “Fasten Seat Belts” come to mind. Now the notion has been expanded to “Fight Global Warming.”



By defining it as something we all should fight, these announcements tell us warming must be bad — something no comprehensive treatise on the science and economics of climate change has ever demonstrated. 



Ogilvy and Mather, a prestigious public‐​relations firm whose for‐​profit clients include IBM and Motorola, produced the global‐​warming ads for free on behalf of Environmental Defense, a major environmental nonprofit that clearly advocates certain types of global‐​warming legislation.



Like their ads for Motorola, Ogilvy and Mather’s global‐​warming announcements are clearly targeted towards sullen youth — a brilliant idea, considering the appallingly low level of scientific knowledge our children have in comparison to their counterparts around the world. But scientific exploration requires critical skepticism, and these ads are full of unquestioned certainties. 



Perhaps the most egregious is a radio ad, called “The Gift.” It mentions dying coral reefs, rising sea levels, melting ice caps, devastating floods, and hurricanes, and accuses us of leaving them all to our children. 



The ads ignore facts that are widely accepted in the scientific community. Take hurricanes. The frequency of category 4 and 5 storms — the really destructive ones — has increased as the planet warmed. Good sound bite, with only one problem: It’s back to where it was in the 1940s and 1950s, long before human beings started warming things up.



In fact, as late as the 1970s, scientists were more concerned with planetary cooling, as revealed in the 1974 CIA report, “Potential implications of trends in world population, food production, and climate,” that presented cooling‐​related food shortages as a major strategic threat. The report first appeared in public in the _New York Times_ on May Day, 1976. Soon, global cooling abruptly reversed into global warming. Crop yields rose. 



The public‐​service announcements are all similarly big on melting polar ice caps and consequent rises in sea level. The Arctic cap loses ice in the summer, but no one bothers to mention that we only began collecting data on it in 1979, at the end of the second‐​coldest period in the Arctic in a century. The ice had to be abnormally expanded then. 



It’s also floating ice, and melting it and doesn’t change sea level at all. And, for all the headlines about loss of ice in Greenland, which does contribute to rising sea levels, the mean temperature there was much higher from 1910 through 1940. Between then and the late 1990s, temperatures in southern Greenland — the region where ice is melting — declined sharply. One has to presume that Environmental Defense knows this.



Around the world, in Antarctica, for the last few decades, average temperatures across the continent have been going _down_. Snowfall has increased, resulting in more continental ice. In fact, every modern computer simulation of 21st century climate has Antarctica continuing to accrete ice.



Ogilvy and Mather marketed their public‐​service announcements through the Ad Council, whose website says that “reversing the global warming trend is possible.”



This suggests that humans have the power to turn planetary warming into cooling — a scientific absurdity. We have neither the technology, the means, the money, nor the political will to do this. 



Consider the Kyoto Protocol, a “baby step” in the fight against global warming. It “requires” the U.S. to reduce its emissions of carbon dioxide to seven percent below 1990 levels by 2008–2012. Requirements vary by a percent or so for most other signatories such as Canada and the EU nations. Yet if every nation of the world met its Kyoto targets, the amount of warming that would be prevented is .07 degrees Celsius per half‐​century — an amount too small to even measure, as average surface temperatures fluctuate by about twice that much from year to year.



Neither the U.S. nor the EU nor virtually anyone else will be able to fulfill the Kyoto targets. EU emissions rose last year, while U.S. emissions remained unchanged. “Reversing” warming would require reducing carbon‐​dioxide emissions by 60–80 percent, which is simply impossible. The world economy would implode.



Ogilvy and Mather’s corporate website feature a quote from founder David Ogilvy: “We pursue knowledge the way a pig pursues truffles.” But what about knowledge on hurricanes, ice caps, and the real possibilities with respect to global warming?



The best course is one in which we continue to use our economic wherewithal to invest in successful companies, which are generally those that produce things efficiently or produce efficient things. Stating _that_ would be a public service. The ads you’re seeing and hearing are not.
"
"
Share this...FacebookTwitterWith the spate of bogus horror “climate papers” coming out recently, e.g. rapid sea level rise, oceans dying and spreading drought, one has to wonder if they weren’t timed to scare politicians into taking rash action at climate-rescue conferences. If they were, they have failed miserably so far.

EU Environment Council in Luxembourg on Tuesday. (Photo: President of the Council)
Firstly, the international climate talks in Bonn, which were aimed at forging a successor agreement to the Kyoto Protocol, which expires next year, broke down, failed and ended with no result. There won’t be a Kyoto-2 anytime in the foreseeable future. See Kyoto obituaries everywhere and Yvo de Boer: Kyoto is dead.
Yet, Europe still insists on being gung-ho about going it alone in rescuing the climate with its roughly 10% modest share of global emissions – to show the world the way. But perhaps reality is finally beginning to sink in in Europe too. The leftist and warmist Klimaretter here reports that talks yesterday in Luxembourg between the EU’s 27 environment ministers also collapsed and ended in failure. Klimaretter writes:
Uproar among the EU Minister Council in Luxemburg: The 27 ministers have postponed negotiations.
Without energetic efforts, the EU will fall way short of its long-term climate targets. Artur Runge-Metzger, responsible director at the General Climate direction of the EU-Commission, drew up a handout for the environment ministers that hardly could have better illustrated political failure:  instead of an 80% reduction, the production of climate gases will be reduced by only 40%.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Not that this difference would make any climatic difference globally to begin with. To put it in perspective, the EU is responsible for just over 10% of the world’s CO2 emissions. That means the difference between 40% and 80% reduction in Europe would make little climatic difference globally, yet would entail huge costs and sacrifice for European citizens.
Initially a press conference had been scheduled for the early afternoon to present details on the new plan. But by 6 pm there was still no press conference. It was becoming clear that no agreement could be reached. Klimaretter writes:
So there was little wonder that by 6 pm there still had not been a press conference: Hopelessly without agreement, the ministers psotponed the matter. ‘A black day for the leadership of the EU,’ said the British Minister for Energy and Climate Change, Chris Huhne, who put the blame on Poland: Poland refused to grant its approval for a step-by-step plan, and thus caused Runge-Metzger’s preparations to wind up in the dustbin.”
Also read here, http://www.reuters.com/article/2011/06/21/eu-climate-britain-idUSB5E7HK01M20110621 – hat-tip GWPF.
So how does one say “Thank You!” in Polish?
Dzieki Poland!
Share this...FacebookTwitter "
"
Share this...FacebookTwitterDer Spiegel here reports on Climategate 2.0, first reminding us that although Climategate 1.0 showed that a group of scientists had indeed stonewalled revealing the data and engaged in gatekeeping, they were cleared by several investigations of misleading and manipulating data.  Der Spiegel also admits Climategate 1.0 scarred the image of the science.
Now we have 2.0.
Der Spiegel writes that the UEA confirms the authenticity of the latest e-mails. Der Spiegel writes:
The chief of the Climatic Research Unit (CRU) of the University of East Anglia had to admit that he had deleted important e-mails on the documentation of his research. He also requested other scientists to destroy data in the same manner.”
Der Spiegel also brings up Ray Bradley’s “pathetic” e-mail, which slammed the 2003 paper by Michael Mann and Phil Jones. But Der Spiegel then quotes climate scientists saying that “such disputes are normal in science”. Maybe so, but the paper is still pathetic and was knowingly used to sway public policy.
Der Spiegel also reports on the “jubilation of the skeptics”, quoting Anthony Watts “What’s Up With That”:
Climate change skeptics cheered the e-mails. ‘They are authentic and spectacular!’ proclaims Watts Up With That?.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Here we see that Der Spiegel refuses to learn that we are not skeptics of climate change, but rather we are skeptics of climate catastrophe fairy tale in the future. It doesn’t matter how many times you tell them. Der Spiegel obviously is unaware that temperatures haven’t risen in almost 15 years, and likely think they are still rising. But we can we do? We (skeptics) are not special education teachers for media who have learning disabilities.
Finally Der Spiegel quotes the author of “pathetic” papers, Michael Mann, saying that Mann told the AP that it’s an organized “campaign by the oil and gas industry and criminal hacking of websites.” and that it “illustrates how desperate the skeptics are”.
At least give Der Spiegel credit for bringing up the story and for linking to the hacked e-mails – though not many Germans are going to read them.
What would it take for the media to take this widespread crime seriously? The lies, cover-ups, and deceit are right there in back and white. What more do you need? Tax authorities, for example, throw people in prison based on a lot less.
Thanks, Der Spiegel.
 
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterNASA photo of Irene
Irene is the first hurricane to hit the US mainland in almost 3 years since Ike hit back in September 2008. So much for global warming causing more hurricanes.
In fact, if no hurricanes had hit the United States this year, it would have been the longest lull between U.S. hurricane landfalls in recorded history, this according to data from the National Oceanic and Atmospheric Administration NOAA.
But that hasn’t stopped the disaster-craved German media from splashing huge block-letter headlines of Irene on the front pages, warning of an “unprecedented event”, a monster scale Armageddon, etc. Call them schadenfreude-junkies. They’ve been waiting 3 years for a fix.
Yesterday German media hopes of a huge hurricane disaster were once again boiling with life. Like the glory days of Katrina. What follows are just a few samples of German media headlines / excerpts (emphasis added).
Der Spiegel: Thousand-Kilometer Storm Threatens America’s Coast
President Obama warns of an ‘extremely dangerous storm’  and has interrupted his vacation: With 175 km/hr wind speeds Irene threatens a number of large cities on the US coast, tens of thousands of people are fleeing. Meteorologists are measuring the hurricane with air crafts. Their data are alarming.
The Big Apple itself is threatened with more than 100 billion dollars in damages; in low-lying areas like Manhattan hospitals and retirement homes have been evacuated.”
and
In the storm’s interior, inferno conditions: The wind near the eye is at 175 km/h; thus the hurricane has reached category 2 or 3 on the 5-category scale.”
Die Welt: In New York It’s Life And Death



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Irene is on course for New York: airports are closing, public transportation is shutting down. The mayor is preparing the the citizens for the worst.”
and
‘If you get an order to evacuate, please heed it’, the President added. The hurricane has to be “‘taken seriously’. All previous information say that it is going to be a “historic hurricane’.”
Bild: Fear of “Irene“! New York Fears Total Chaos
Hurricane Irene has put millions of people on the US coast in a state of fear  – destructive wind gusts and storm surges threaten. A state of emergency has been called in metropolitan New York. The Monster Storm could steer directly for the city and trigger a historic catastrophe!”
and
Hurricane expert Bernard expects that Irene will be scaled up to  category 4 during Friday. If the storm does not weaken again, then it would cause extreme damage to buildings with winds of 210 to 249 km/hr.”
FOCUS: Hurricane Irene Grows To The Size Of Europe 
Millions of Americans are fleeing: hurricane Irene is approaching the east coast. Already the storm has reached about the size of Europe. Due to the immense threat, New York has ordered an unprecedented mass evacuation.
Obama warns of a ‘historic hurricane’.”
Today, all media outlets are backpedalling big time, and look just a bit silly.
Share this...FacebookTwitter "
"As cities get bigger and cover more land, the need to make space for wildlife – including insects – in urban areas has become more pressing. Research has shown that cites may not be such a bad place for pollinating insects such as bumble bees, solitary bees and hoverflies. In fact, one UK study of ten cities and two large towns found a greater variety of species in urban areas than in rural areas, while another study showed some UK urban areas hosted stronger bumble bee colonies than those in rural areas.  But other research has found that cities only support the most common pollinators, such as the buff-tailed bumble bee (which is a flexible, generalist forager), and that many species decline in number as urbanisation increases.  One way to help pollinators in urban areas is to provide flowers for them to feed on, in an environment which is otherwise empty of flowering plant life. Flowers have been planted on roadsides around the UK for this very purpose.  Planting more flowers is a great idea – but it is difficult to predict which flowers different insects will use the most, and whether enough flowers are being provided for them. This is why, for our recent study, we teamed up with the National Botanic Garden of Wales to find out what some different bee species think of the wildflower strips planted by Bournemouth Borough Council. Gardeners and councils who want to plant the right flowers to attract bees usually choose them based on how easy they are to plant, and by watching which ones the insects already visit. Instead of doing this, we collected the pollen from bees who were visiting flower patches. Bees were caught and temporally held in a tube before release. The pollen that had fallen or rubbed off of the bee was used for DNA analysis to find out which flowers they had visited.  The technique we used is called DNA meta-barcoding. This allows us to look at a specific part of the plant genome and compare it to a database containing DNA barcodes for numerous British plants, created by the National Botanic Garden of Wales. This technique is relatively new and has previously been used to identify pollen in honey and pollen from the bodies of hoverflies to see which plants they had visited. By collecting the pollen from the bee’s body, we can find out the bee’s foraging history and get samples from places where you cannot follow the bee – like up in the trees or into people’s gardens. And because it is not destructive, there is the potential to collect from an individual more than once. But why use DNA techniques rather than simply looking at the pollen under a microscope? Well, it takes a long time to process and identify pollen grains with a microscope and DNA meta-barcoding can be done in a few days. In addition, accurately identifying pollen is very difficult even for those with a high level of expertise. The identification results from DNA meta-barcoding are also now comparable to or better than traditional pollen identification under a microscope. There are some limitations, however. In particular, DNA meta-barcoding cannot provide a count of each pollen type in a sample, only a relative proportion. Our results show that the bees are indeed using the floral patches put out for them in cities – but these areas alone are not enough. Some of the bees’ favourite flowers in the Bournemouth sample area were purple tansy (Phacelia), chrysanthemums (chrysanthemum), poppies (Papava), cornflowers (Centaurea) and viper’s bugloss (Echium). We also commonly found that they visit garden plants, for example lupins (Lupinus), hydrangeas (Hydrangea), buddleja (Buddleja) and privet (Ligustrum), and wild plants like brambles (Rubus), sow thistles (Sonchus) and wild lettuce (Lactuca). This shows that bees travel around the urban environment to find what they need, and don’t just rely on the small floral strips planted for them. After all, bees need high quality food and variety in their diet to stay healthy, just as humans do.  Our results also showed that different bees like different things depending on their size. For example, small solitary bees are restricted to using more open flowers like daisies, while bumble bees are less restricted because they have long tongues that can reach into deep flowers. So planters need to cater for all tastes if we hope to support bee diversity.  This study only covered a tiny percentage of the UK’s pollinator diversity and there are many other insects such as hoverflies, beetles and butterflies who rely on urban flowers, too. So while the research improves our knowledge on a small number bee species’ flower preferences, there is still lots of work to do in order to make cities friendly for a wide range of pollinators."
"**Chancellor Rishi Sunak is unveiling the government's spending plans for the coming year.**
The Spending Review will include details on public sector pay, NHS funding and money for the devolved administrations in Northern Ireland, Scotland and Wales.
Mr Sunak will also set out the extent of the damage done to the UK economy by the coronavirus pandemic.
A No 10 spokesman said the economic forecasts will be ""a sobering read"".
The government's Covid response has led to huge spending and borrowing rises.
The chancellor is expected to begin his statement at around 12:30 GMT following Prime Minister's Questions.
Some Spending Review announcements have already been trailed.
These include an extra Â£3bn for the NHS in England to help tackle the backlog of operations delayed due to Covid, an increase in defence spending and a Â£4.6bn package to help the unemployed back to work.
The government is expected to announce a cut in the UK's overseas aid budget to 0.5% of national income, down from the legally binding target of 0.7%.
There have also been reports that the chancellor is considering a pay freeze for all public sector workers except frontline NHS staff.
Plans to change the way big spending projects are analysed \- which the Treasury says is currently biased in favour of the south east of England - will be published alongside the Spending Review.
The chancellor may also choose to set aside money to tackle climate change and regional inequalities.
Devolved governments will receive money proportionate to any funding England gets in the Spending Review.
This is decided using the Barnett formula - devised by Lord Barnett, a Labour politician, in the 1970s.
Mr Sunak and Treasury Chief Secretary Stephen Barclay updated the Cabinet on Wednesday morning.
A Downing Street spokesman said: ""Cabinet was told the OBR forecasts will show the impact the coronavirus pandemic has had on our economy and they will make for a sobering read, showing the extent to which the economy has contracted and the scale of borrowing and debt levels.
""But - as the IMF (International Monetary Fund), OBR and others have pointed out - the costs would have been much higher had we not acted in the way we have done.""
""It's going to look horrible.""
The simple truth about the Spending Review according to a senior MP.
The chancellor will bang the drum for his plans to keep people in jobs, or help find new ones.
Rishi Sunak will take out the metaphorical megaphone to explain how he'll allocate billions of taxpayers' cash to spend on infrastructure in the coming months.
But the headlines of the Spending Review, when governments put their money where their mouths are, won't be in any rhetorical flourishes at the despatch box, nor likely in any surprise announcements kept back as goodies for the public.
The government had intended to use the Spending Review to set out its plans for the next three years, however this was reduced to just one year due to the economic turmoil caused by Covid.
The difficult financial backdrop will dominate this year's review with the economy projected to be 10% smaller than it was pre-virus.
Tax revenues have fallen as many businesses have been forced to close and government schemes to support furloughed workers have led to soaring levels of spending.
Public borrowing is expected to rise to Â£372bn - compared to the Â£55bn the government had originally expected to borrow.
The Spending Review will be accompanied by economic forecasts from the Office for Budget Responsibility - including predictions on how tax will be raised.
Labour's shadow chancellor Anneliese Dodds said the government's ""irresponsible choices"" during the pandemic had ""led to our country experiencing the worst downturn in the G7, and created a jobs crisis"".
""This prime minister and his government talk a good game but they haven't delivered on their promises - and regional inequality has got worse under their watch,"" she said.
""They clapped for key workers - but now they're freezing their pay, and looking to scrap planned minimum wage increases for the private sector.""
Unions called for Mr Sunak to maintain investment in the public sector, the TUC's deputy general secretary Paul Nowak telling BBC Breakfast ""now is not the time to make cuts to public services"".
And the SNP is calling for a huge stimulus package to support growth and jobs across the whole of the UK.
""The spending has to match the challenges we see in the economy,"" said its economic spokeswoman Alison Thewliss. ""At the moment interest rates are at a record low so the government should be borrowing."""
"
Share this...FacebookTwitterNot that is hasn’t been obvious. The Leibniz Institute for Ocean Sciences of the University of Kiel reports here that ocean chemist Dr. Christa Marandino and lead a group that will use new, innovative measuring techniques to directly measure the exchange of trace gases between oceans and atmosphere. 
Dr. Marandino wants to fill another gap in climate science and the IPCC models. (Photo credit: J. Steffen, IFM-GEOMAR)
Excerpts of the IFM GEOMAR press release are as follows, with my comments:
It sounds so little. Only 0.04% of the earth’s atmosphere consists of CO2. And yet it is this tiny amount of gas that provides for a greenhouse effect on the earth, makes life possible and with a small change can lead to considerable increasing temperatures on the planet.”
Co2 concentrations have gone up about 110 ppm over the last 150 years, yet the temperature is only up 0.8°C. Much of that temperature increase is traced back to solar activity and ocean cycles. So the above statement is certainly a load of BS. Note how they complete ignore acknowledging water vapour, aerosols, the sun and ocean cycles as climate factors.
Other trace gases include methane, dimethyl sulfide or also acetone. And like CO2, these gases are continuously exchanged between the oceans and the atmosphere. To which scale and speed the exchange occurs is an important factor for atmospheric chemistry, and thus for climate change. Unfortunately there are no measurements for many of these material flows at the boundary between water and air. Mathematical formulas that have been used up to now have proven to be inaccurate in getting values.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In other words: The models have failed. This is a clear admission that they don’t understand the climate mechanisms and that the science is filled with gaps.
Dr. Christa Marandino of the Leibniz-Institute for Ocean Sciences (IFM-GEOMAR) wants to close this gap using a new techniques. The method is called eddy correlation technique. ‘Simply said, one just measures the vertical wind speeds, the changes in gas concentrations and connect the two,’ explains the 35-year old scientist. For CO2 this method is already being used by some work groups worldwide. But for the most trace gases with tiny concentrations, the technical difficulties have been too large so far. The measurements have to be extremely fast and highly precise, and that on a bobbing measurement platform like a research ship.”
Don’t worry about it. I’m sure you’ll find the right correction factors and get the results you need. Anything goes in climate science. The money will always be there.
‘That’s the problem’, explains Dr. Maradino. She has already conducted the first promising attempts with newly developed equipment, an atmospheric pressure chemical ionization mass spectrometry“ (APCI-MS), at the University of California in Irvine. Dr. Marandino has been working in Kiel since 2008 and she wants to tweek the technology more here. The Helmholtz-Gemeinschaft Deutscher Forschungszentren (HGF) will support Dr. Marandino beginning January 2012 with the formation of a Helmholtz group. Over the coming 5 years she will receive  €250,000 for her research.”
The press release then ends with the usual “we are very glad to have this young scientist working here” blah blah blah.
I don’t mean to devalue her work here by any means. I’m just surpised to read that even though some insist the science is settled, we hear of yet another admission that it is indeed filled with gaping holes. And that means the IPCC models are discredited.
Share this...FacebookTwitter "
"
From the ARC Centre of Excellence for Coral Reef Studies James Cook University
“Evil twin” threatens world’s oceans, scientists warn
'Twins"" 1988 - Schwarzenegger and DaVito
The rise in human emissions of carbon dioxide is driving   fundamental and dangerous changes in the chemistry and ecosystems of the  world’s  oceans, international marine scientists warned today.
“Ocean conditions are already more extreme than  those experienced  by marine organisms and ecosystems for millions of years,”  the  researchers say in the latest issue of the journal Trends in Ecology and  Evolution (TREE).
“This emphasises the urgent need to adopt policies  that  drastically reduce CO2 emissions.”
Ocean acidification, which the researchers call the  ‘evil twin of  global warming’, is caused when the CO2 emitted by human  activity,  mainly burning fossil fuels, dissolves into the oceans. It is  happening  independently of, but in combination with, global warming.
“Evidence gathered by scientists around the world over  the last  few years suggests that ocean acidification could represent an equal –   or perhaps even greater threat – to the biology of our planet than  global  warming,” co-author Professor Ove Hoegh-Guldberg of the ARC  Centre of Excellence  for Coral Reef Studies and The University of  Queensland says.
More than 30% of the CO2 released from burning  fossil fuels,  cement production, deforestation and other human activities goes   straight into the oceans, turning them gradually more acidic.
“The resulting acidification will impact many forms  of sea life,  especially organisms whose shells or skeletons are made from  calcium  carbonate, like corals and shellfish. It may interfere with the   reproduction of plankton species which are a vital part of the food web  on  which fish and all other sea life depend,” he adds.
The scientists say there is now persuasive evidence that mass  extinctions  in past Earth history, like the “Great Dying” of 251  million years ago and  another wipeout 55 million years ago, were  accompanied by ocean acidification,  which may have delivered the  deathblow to many species that were unable to cope  with it.
“These past periods can serve as great lessons of what we can  expect in  the future, if we continue to push the acidity the ocean even  further” said  lead author, Dr. Carles Pelejero, from ICREA and the  Marine Science Institute  of CSIC in Barcelona, Spain.
“Given the impacts we see in the fossil record, there is no  question  about the need to immediately reduce the rate at which we are  emitting carbon  dioxide in the atmosphere,” he said further.
“Today, the surface waters of the oceans have  already acidified by  an average of 0.1 pH units from pre-industrial levels, and  we are  seeing signs of its impact even in the deep oceans”, said co-author Dr.   Eva Calvo, from the  Marine Science Institute of CSIC in Barcelona,    Spain.
“Future acidification depends on how much CO2  humans emit from  here on – but by the year 2100 various projections indicate  that the  oceans will have acidified by a further 0.3 to 0.4 pH units, which is   more than many organisms like corals can stand”, Prof. Hoegh-Guldberg  says.
“This will create conditions not seen on Earth  for at least 40  million years”.
“These changes are taking place at rates as much as  100 times  faster than they ever have over the last tens of millions of years”   Prof. Hoegh-Guldberg says.
Under such circumstances “Conditions are likely to  become very  hostile for calcifying species in the north Atlantic  and Pacific over  the next decade and in the Southern Ocean over the next few  decades,”  the researchers warn.
Besides directly impacting on the fishing  industry and its  contribution to the human food supply at a time when global  food demand  is doubling, a major die-off in the oceans would affect birds and  many  land species and change the biology of Earth as a whole profoundly,  Prof.  Hoegh-Guldberg adds.
Palaeo-perspectives on ocean acidification by Carles  Pelejero, Eva Calvo and Ove Hoegh-Guldberg is published in the latest  issue of the journal  Trends in Ecology and Evolution (TREE), number  1232.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8d484b3a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Cold event setups in atmospheric circulation patterns are aligning. Two days ago I brought to your attention that there was a strong downspike in the Arctic Oscillation Index and that the North Atlantic Oscillation Index was also negative. See The Arctic Oscillation Index goes strongly negative
Yesterday, Senior AccuWeather meteorologist Joe Bastardi let loose with this stunning prediction on the AccuWeather premium web site via Brett Anderson’s Global warming blog:

What is facing the major population centers of the northern hemisphere is unlike anything that we have seen since the global warming debate got to the absurd level it is now, which essentially has been there is no doubt about all this. For cold of a variety not seen in over 25 years in a large scale is about to engulf the major energy consuming areas of the northern Hemisphere. The first 15 days of the opening of the New Year will be the coldest, population weighted, north of 30 north world wide in over 25 years in my opinion.
The Climate Prediction Center discussion for their forecast also concurs with both of the above:
THE AO INDEX WHICH RECENTLY HAS BEEN VERY STRONGLY NEGATIVE IS FORECAST TO INCREASE SLIGHTLY IN VALUE BUT REMAIN STRONGLY NEGATIVE THROUGH DAY 14. TODAYS BLEND CHART INDICATES BELOW NORMAL HEIGHTS ACROSS ROUGHLY THE SOUTHEASTERN TWO-THIRDS OF THE CONUS, AND ABOVE NORMAL HEIGHTS OVER THE NORTHWESTERN THIRD OF THE CONUS, CONSISTENT WITH A STRONGLY NEGATIVE AO.
Here are two of the CPC forecast maps for the days covered by Bastardi’s forecast. It is fairly typical to see an above average temperature in the west when we get a cold deep jet stream in the east:


I was going to include some Met Office forecasts here but after trying to find something useful at their web site and failing to find anything, I gave up looking.
If you live in these areas: bundle up, stock up. Get ready.

Sponsored IT training links:
Get best quality 98-366  training material to prepare and pass 70-573 and 70-576 exam.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e905adfb0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterDr. Hermann Ott, German global warming mobber who not long ago called for a science pogrom aimed at skeptics of dubious global warming science, brings our attention here to a planned conference at the Bundestag in Berlin on June 10, 2011.The conference is designed to attack skepticism in science, and is hosted by the German Green Party faction.
It is dubbed: Strategies of the so-called climate sceptics and who is behind it. Here’s an excerpt of the conference description:
In the USA skepticism has had a long tradition, and with the recent congressional elections it has reached new dimensions. The ‘arguments’ of the climate change skeptics have found fertile ground there, and also in Germany. […] At our conference we wish to shine light on the background of the climate skeptic activities. What are the strategies of the so-called climate sceptics, who is behind them and who finances them?”
Yes, the Greens have to go back to the old worn out pages of its propaganda playbook and rehash all the old drivel about “climate change deniers”, evil industry funding, Fred Singer and Big Oil conspiracies as being behind the skeptic movement.
They’re stuck on stupid. They don’t have the science, so they keep using the old stories.
Unfortunately, Ott, Rahmstorf and the rest of the green supremacists have forgotten that all these questions concerning skepticism in Germany have been already answered, see see here in German and here in English.
Yet in response, many of us have questions of our own for the Greens, and we sent them a list. For example, the Free Democrats of the Friedrich Naumann Institute and other freedom and science-friendly organizations prepared a list of questions for the Greens to answer and sent it to the Green Parliamentarians – way back in November – more than 6 months ago.
Questions to the Greens still unanswered – 6 months and still waiting
The list of questions was even signed by thousands of taxpaying citizens with a copy posted in the Internet. After 6 months – still no sign of a reply. In case the Greens have forgotten, here is the list of questions once again, in short form.
1. Are the Greens aware of the 800 900 peer-reviewed papers that question AGW?
2. Are the Greens aware that climate science is a relatively new science that still entails lots of uncertainty, and that there is no consensus?
3. Will the Greens even send a representative to the 3rd 4th International Conference on Climate and Energy?
4. If the Greens think the question of climate change is already settled, then why spend billions more for financing of climate research work?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




5. Are the Greens aware of any other institution, except for the Pope and their own party, that claims to be infallible?
6. Are the Greens aware that scientific papers questioning man-made climate change were suppressed, and are they aware of Climate-gate, Himalaya-gate, Glacier-gate, Amazon-gate?
7. Are the Greens aware of the scientific achievemnts of Prof. Fred Singer’s distinguished scientific career?
8. Are the Greens aware that Prof. Dr. Judith Curry said: “Man made climate change is a theory and is highly uncertain”, and that distinguished professor Willaim Happer called it: “The dubious science of the climate crusaders”?
9. Is it the Greens’ view that serious, scientific work is only so if it supports the political aspirations of your party?
10. Why is it that no journalist has ever gotten the idea to check up on the scientific reputation of experts that are paid by the Greens?
11. What is the Greens’ position on the fact that your politics accompanies the profligate subsidies to solar and wind energy, bought by donations from, among others, IBC Solar AG, SMA Solar Technology AG, Ostwind, Umweltkontor Renewable Energy, EWO Energietechnologie GmbH, Conergy AG, Pro Vento, Nordex AG, Windpark G. W. Meerhof GmbH & Co. KG, Ersol AGder Windpark GmbH & Co. KG, Wind Project Development GmbH, Solarworld AG, SMA Technologie AG, Solon AG fir Solar Technology, AGU Energy and Electrotechnology GmbH?
12. Which renewable energy industries and to what extent have Parliamentarians of the Green Faction invested?
13. Do the Greens intend to continue their constant use of the expression “climate denier”?
14. Will the Greens continue to use public money to encourage and incite others to commit criminal acts such as vandalizing railways?
15. Do the Greens call protests at nuclear waste storage facilities only when they are in the opposition, and do the opposite when they are not?
16. Are you Greens aware of the point you’ve reached today, when you are asked such questions?
How about answers to our questions, Mr Ott? With us as taxpayers and you as a Parliamentarian, it is only fitting that you provide a reply. Why are you and your faction, along with Drs Rahmstorf and Schellnhuber, always hiding from open debate? Why do you and the scientists at PIK insist in remaining mired in the narrow-mindedness of dogmatism?
What are you afraid of?
Share this...FacebookTwitter "
"
The Liberal Party in Australia’s parliament has a new leader.

Herald Sun Blogger and Columnist, Andrew bolt writes to me in an email:
Anthony,
 This may be a first: a major political party has dumped a global warming believer as leader and replaced him with sceptic who last month called AGW “crap”. Tony Abbott has tempered his public pronouncements since, but has today become the new Liberal leader, toppling warmist Malcolm Turnbull, specifically because he was the only one of the three contenders today to promise to delay the Government’s emissions trading scheme.
Bolt adds some background:
Following up with excerpts from new Liberal leader Tony Abbott’s memoir Battlelines, released in July.
On page 171 he quotes, with approval,  Bjorn Lomborg:
“Natural science has undeniably shown us that global warming is man-made and real. But just as undeniable is the economic science, which makes it clear that a narrow focus on reducing carbon emissions could leave future generations lumbered with major costs, without major cuts in temperatures.”
Abbott then adds:
“Without binding universal arrangements, any effort by Australia (on emissions trading) could turn out to be a futile gesture, damaging local industry but making no appreciable dent in global emissions…. Another big problem with any Australian emissions reduction scheme is that it would not make a material difference to atmospheric carbon concentrations unless the big international polluters had similar schemes. Australia accounts for about 1 per cent of global carbon dioxide emissions. At recent rates of growth, China’s increase in emissions in about a year could match Australia’s entire carbon dioxide output. Without binding universal arrangements, any effort by Australia could turn out to be a futile gesture, damaging local industry but making no appreciable dent in global emissions.”
He also questions what climate alarmists truly want:
“It’s hard to take climate alarmists all that seriously, though, when they’re as ferociously against the one proven technology that could reduce electricity emissions to zero, nuclear power, as they are in favour of urgent reduction in emissions. For many, reducing emissions is a means to achieving a political objective they could not otherwise gain.”
======
Lest you think that Climategate had nothing to do with this political shift, please read what Bolt had to say about its impact in my previous post:
The Australian ETS vote: a political litmus test for cap and trade
Several MPs have indeed mentioned the emails in their party room speeches, and your correspondents miss the way MPs actually pick up things.
Andrew Bolt has one of the most read blogs and columns in Australia and is helping to educate both people and politicians alike on the true costs of climatic induced cap and trade, please visit his blog to show some support. – Anthony
http://blogs.news.com.au/heraldsun/andrewbolt/

Following up with  excerpts from new Liberal leader Tony Abbott’s memoir Battlelines, released in  July.
 
On page 171 he quotes, with approval,  Bjorn  Lomborg:“Natural science has undeniably  shown us that global warming is man-made and real. But just as undeniable is the  economic science, which makes it clear that a narrow focus on reducing carbon  emissions could leave future generations lumbered with major costs, without  major cuts in temperatures.”Abbott then adds: 
“Without binding universal arrangements, any effort  by  Australia (on emissions trading) could turn out to be a  futile gesture, damaging local industry but making no appreciable dent in global  emissions.… Another big problem with any  Australian emissions reduction scheme is that it would not make a material  difference to atmospheric carbon concentrations unless the big international  polluters had similar schemes.  Australia  accounts for about 1 per cent of global carbon dioxide emissions. At recent  rates of growth,  China’s increase  in emissions in about a year could match  Australia’s  entire carbon dioxide output. Without binding universal arrangements, any effort  by Australia  could turn out to be a futile gesture, damaging local industry but making no  appreciable dent in global emissions.”
He also questions what climate alarmists truly want:
“It’s hard to take climate  alarmists all that seriously, though, when they’re as ferociously against the  one proven technology that could reduce electricity emissions to zero, nuclear  power, as they are in favour of urgent reduction in emissions. For many,  reducing emissions is a means to achieving a political objective they could not  otherwise gain.”
 




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9089f1f2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI got the following as a reader comment. It’s a statement that appears to be direct from Janez Potočnik’s office. He is a European Commissioner for Environment and he refutes the story that they plan to mandate water saving measures in Europe.  That’s a relief – saved from another stupid idea, at least for now (Don’t worry though – they’ve got plenty of others, for sure).
 The statement was sent by a fellow named Joe Hennon.
——————————————————–
Statement by Janez Potočnik, European Commissioner for Environment
I am aware of allegations that appeared in the German press concerning future plans to restrict water use in the Member States and to impose regulations on landlords and households. These allegations are unfounded.
I wish to make it very clear that the Commission has no plans at present to make water-saving taps mandatory in any Member State, no plans to oblige Member States to reduce household water consumption, and no targets have been set in these areas.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The main aim of EU water policy has always been to ensure that good quality water is available throughout the EU in sufficient quantities. The Commission is committed to addressing water scarcity as part of a Blueprint for Water in 2012 and, as always, a number of studies are ongoing in this area.
Several policy options are being examined regarding the technical, environmental and economic feasibility of regulatory and non-regulatory options for water performance requirements for buildings, but no new decisions have been taken in this area, nor are they indeed pending.
The European law-making process is a highly consultative one. When legislation is proposed, proposals are accompanied by studies assessing the potential environmental, social and economic effects. Any targets or legislation in the area of water use would be accompanied by the appropriate impact assessment as a matter of course, taking account of the extent to which the situation in Europe is diverse, with a view to avoiding a “one size fits all” approach.
I regret that articles on sensitive questions such as Commission proposals for water use in Member States appear in the media before any details have been checked with the relevant staff at the European Commission
Joe Hennon
ec.europa.eu/commission_2010-2014/potocnik/index_…
joseph.hennon@ec.europa.eu
————————————————————-
(No mention if Joe Hennon is an aide to Potočnik, or what his function is.)
Share this...FacebookTwitter "
"

As of this writing, Tuesday, September 11, Hurricane Florence is threatening millions of folks from South Carolina to Delaware. It’s currently forecast to be near the threshold of the dreaded Category 5 by tomorrow afternoon. Current thinking is that its environment will become a bit less conducive as it nears the North Carolina coast on Thursday afternoon, but still hitting as a Major Hurricane (Category 3+). It’s also forecast to slow down or stall shortly thereafter, which means it will dump disastrous amounts of water in southeastern North Carolina. Isolated totals of over two feet may be common.   
  
  
At the same time that it makes landfall, there is going to be the celebrity‐​studded “Global Climate Action Summit” in San Francisco, and no doubt Florence will be the poster girl.   
  
  
There’s likely to be the usual hype about tropical cyclones (the generic term for hurricanes) getting worse because of global warming, even though their integrated energy and frequency, as published by Cato Adjunct Scholar Ryan Maue, show no warming‐​related trend whatsoever.   






_Maue’s Accumulated Cyclone Energy index shows no increase in global power or strength._   
  
  
Here is the prevailing consensus opinion of the National Oceanic and Atmospheric Administration’s Geophysical Fluid Dynamics Laboratory (NOAA GFDL): “In the Atlantic, it is premature to conclude that human activities–and particularly greenhouse gas emissions that cause global warming–have already had a detectable impact on hurricane activity.”   
  
  
We’ll also hear that associated rainfall is increasing along with oceanic heat content. Everything else being equal (dangerous words in science), that’s true. And if Florence does stall out, hey, we’ve got a climate change explanation for that, too! The jet stream is “weirding” because of atmospheric blocking induced by Arctic sea‐​ice depletion. This is a triple bank shot on the climate science billiards table. If that seems a stretch, it is, but climate models can be and are “parameterized” to give what the French Climatologist, Pierre Hourdin, recently called “an anticipated acceptable range” of results.   
  
  
The fact is that hurricanes are temperamental beasts. On September 11, 1984, Hurricane Diana, also a Category 4, took aim at pretty much the same spot that Florence is forecast to landfall—Wilmington, North Carolina. And then—34 years ago—it stalled and turned a tight loop for a day, upwelling the cold water that lies beneath the surface, and it rapidly withered into a Category 1 before finally moving inland. (Some recent model runs for Florence have it looping over the exact same place.) The point is that what is forecast to happen on Thursday night—a major category 3+ landfall—darned near happened over three decades earlier… and exactly 30‐​years before that, in 1954, Hurricane Hazel made a destructive Category 4 landfall just south of the NC/SC border. The shape of the Carolina coastlines and barrier islands make the two states very susceptible to destructive hits. Fortunately, this proclivity toward taking direct hits from hurricanes has also taught the locals to adapt—many homes are on stilts, and there is a resilience built into their infrastructure that is lacking further north.   
  
  
There’s long been a running research thread on how hurricanes may change in a warmer world. One thing that seems plausible is that the maximum potential power may shift a bit further north. What would that look like? Dozens of computers have cranked away thousands years of simulations and we have a mixture of results: but the consensus is that there will be slightly fewer but more intense hurricanes by the end of the 21st Century.   
  
  
We actually have an example of how far north a Category 4 can land, on August 27, 1667 in the tidewater region of southeast Virginia. It prompted the publication of a pamphlet in London called “Strange News from Virginia, being a true relation of the great tempest in Virginia.” The late, great weather historian David Ludlum published an excerpt:   




Having this opportunity, I cannot but acquaint you with the Relation of a very strange Tempest which hath been in these parts (with us called a Hurricane) which began on Aug. 27 and continued with such Violence that it overturned many houses, burying in the Ruines much Goods and many people, beating to the ground such as were in any ways employed in the fields, blowing many Cattle that were near the Sea or Rivers, into them, (!!- _eds_ ), whereby unknown numbers have perished, to the great affliction of all people, few escaped who have not suffered in their persons or estates, much Corn was blown away, and great quantities of Tobacco have been lost, to the great damage of many, and the utter undoing of others. Neither did it end here, but the Trees were torn up by their roots, and in many places the whole Woods blown down, so that they cannot go from plantation to plantation. The Sea (by the violence of the winds) swelled twelve Foot above its usual height, drowning the whole country before it, with many of the inhabitants, their Cattle and Goods, the rest being forced to save themselves in the Mountains nearest adjoining, where they were forced to remain many days in great want.



Ludlum also quotes from a letter from Thomas Ludwell to Virginia Governor Lord Berkeley about the great tempest:   




This poore Country…is now reduced to a very miserable condition by a continual course of misfortune…on the 27th of August followed the most dreadful Harry Cane that ever the colony groaned under. It lasted 24 hours, began at North East and went around to Northerly till it came to South East when it ceased. It was accompanied by a most violent raine, but no thunder. The night of it was the most dismal time I ever knew or heard of, for the wind and rain raised so confused a noise, mixed with the continual cracks of falling houses…the waves were impetuously beaten against the shores and by that violence forced and as it were crowded the creeks, rivers and bays to that prodigious height that it hazarded the drownding of many people who lived not in sight of the rivers, yet were then forced to climb to the top of their houses to keep themselves above water…But then the morning came and the sun risen it would have comforted us after such a night, hat it not lighted to us the ruins of our plantations, of which I think not one escaped. The nearest computation is at least 10,000 house blown down.



It is too bad that there were no anemometers at the time, but the damage and storm surge are certainly consistent with a Category 4 storm. And this was in 1667, at the nadir of the Little Ice Age.
"
"

Today it is not unusual to hear it suggested that the undeveloped world’s best hope lies in private property, the market economy, and the rule of law. But a short time ago, that suggestion would have scandalized many audiences. Peter Bauer is a major reason for that shift.



Lord Bauer, the son of a Budapest bookmaker, came to Britain in 1934 to study economics at Gonville and Caius College, Cambridge, where he later became a fellow. His pioneering work in development economics, which began with his study of the Southeast Asian rubber industry in the 1940s and his classic 1954 book, West African Trade, led him to question, and later overturn, many of the beliefs held by mainstream development experts. This work was carried out primarily from the London School of Economics and Political Science, where he taught from 1960 to 1983 and where he is currently emeritus professor of economics. In 1982, he was made a life peer and is a fellow of the British Academy.



Bauer’s work is characterized by careful observation of how countries move from subsistence to exchange economies, an application of simple economic principles, and a sound understanding of the role of non‐​economic variables in promoting material advance. As he noted in his book Dissent on Development, “Economic achievement depends primarily on people’s abilities and attitudes and also on their social and political institutions. Differences in these determinants or factors largely explain differences in levels of economic achievement and rates of material progress.”



What Bauer observed was that people in poor countries respond to price incentives just like people in rich countries. He also observed that when people have the freedom to own property and to trade, and when government is limited to the protection of those rights, they have a better chance of achieving prosperity.



The intellectual climate in the late 1950s was not hospitable to Bauer’s critique of state‐​led development policy. In 1956, Swedish economist Gunnar Myrdal, later a Nobel laureate, wrote, “The special advisers to underdeveloped countries who have taken the time and trouble to acquaint themselves with the problem … all recommend central planning as the first condition of progress.” That view persisted well into the 1960s and has only recently been supplanted by a more market‐​friendly view. It was not until after the collapse of communism in Eastern Europe and the Soviet Union that the World Bank admitted, in its 1997 development report, “State‐​led intervention emphasized market failures and accorded the state a central role in correcting them. But the institutional assumptions implicit in this world view were, as we all realize today, too simplistic.”



Bauer recognized, as noted in his book Reality and Rhetoric, that “the critics who propose replacing the market system by political decisions rarely address themselves to such crucial matters as the concentration of economic power in political hands, the implications of restriction of choice, the objectives of politicians and administrators, and the quality and extent of knowledge in a society and its methods of transmission.”



In observing economic reality and adhering to the logic of the price system, Bauer refuted key propositions of orthodox development economics, the most basic one being the idea of a “vicious circle of poverty.” Poor countries were said to be poor because people had low incomes and could not generate sufficient savings to allow for capital accumulation, one of the prerequisites for economic growth, as spelled out in mainstream growth models. Bauer observed that many people and many countries had moved from poverty to prosperity and that large‐​scale capital investment is neither necessary nor sufficient for material advance. His study of small holdings in the Malaya (now Malaysia) rubber industry and his observation of the importance of small‐​scale traders in West Africa convinced him that the reality of development was different from the rhetoric of development experts.



A corollary of the vicious circle is that poor countries cannot become rich without external aid from developed countries. However, the nations that have become rich had no access to foreign aid, while those that have received substantial external aid are for the most part still poor, as in Africa. So Bauer argued that foreign aid is more likely to perpetuate poverty than to alleviate it. And history has borne him out.



Bauer also strongly disagreed with the widely held view that population growth is a drag on development. In his essay “Population Growth: Disaster or Blessing?” he wrote, “Economic achievement and progress depend on people’s conduct not on their numbers.” Unlike many of the development experts who wanted to use government to “help the poor,” Bauer thought that poor people could lift themselves out of poverty through their own efforts, if only governments would safeguard both economic and personal freedom. When people are free to choose and bear the responsibility for their choices, as they do under a system of private property and free markets, they will be more able to improve themselves and provide for their families‐​as well as have stronger incentives to do so‐​than when they are dependent primarily on the state.



Bauer was one of the first economists to clearly see that state‐​led development policies and the quest for “social justice” would politicize economic life, impair individual freedom, and fail to achieve long‐​run prosperity for the majority of people. He also noted that those countries that had the fewest commercial contacts with the West were the least developed. Thus, he recognized the dynamic gains from free trade. In his most recent book, From Subsistence to Exchange and Other Essays, he wrote, “Contacts through traders and trade are prime agents in the spread of new ideas, modes of behavior, and methods of production. External commercial contacts often first suggest the very possibility of change, including economic improvement.” Certainly the experience of people in Japan, South Korea, Taiwan, China, and Hong Kong support that observation.



Bauer’s emphasis on individual merit, character, culture, property rights, and markets, and his distrust of big government, foreign aid, and the welfare state place him squarely in the classical‐​liberal tradition. His life’s work has been in the broad context of political economy, not in the narrow technical confines of modern development economics or the even narrower space of formal economic modeling. 



Bauer’s keen understanding of how individuals and nations grow rich comes from practical experience combined with plain economic theory and a deep knowledge of history. His work has stood the test of time. That is why he is now widely recognized as a hero of the revolution in development economics.
"
nan
"
Smearing around data or paint - the results are similar
Jeff Id of The Air Vent emailed me today inviting me to repost Ryan O’s latest work on statistical evaluation of the Steig et al “Antarctica is warming” paper ( Nature, Jan 22, 2009) I thought long and hard about the title, especially after reviewing the previous work from Ryan O we posted on WUWT where the paper was dealt a serious blow to “robustness”. After reading this latest statistical analysis, I think it is fair to conclude that the paper’s premise has been falsified.
Ryan O, in his conclusion, is a bit more gracious:
I am perfectly comfortable saying that Steig’s reconstruction is not a faithful representation of Antarctic temperatures over the past 50 years and that ours is closer to the mark.
Not only that, Ryan O did a more complete job of the reconstruction than Steig et al did, he mentions this in comments at The Air Vent:
Steig only used 42 stations to perform his reconstruction.  I used 98, since I included AWS stations.
The AWS stations have their problems, such as periods of warmer temperatures due to being buried in snow, but even when using this data, Ryan O’s analysis still comes out with less warming than the original Steig et al paper
Antarctica as a whole is not warming, the Antarctic peninsula is, which is signficantly removed climatically from the main continent.
Click for a larger image
It is my view that all Steig and Michael Mann have done with their application of RegEm to the station data is to smear the temperature around much like an artist would smear red and white paint on a pallete board to get a new color “pink” and then paint the entire continent with it.
It is a lot like “spin art” you see at the county fair. For example, look (at left) at the different tiles of colored temperature results for Antarctica you can get using Steig’s and Mann’s methodology. The only thing that changes are the starting parameters, the data remains the same, while the RegEm program smears it around based on those starting parameters. In the Steig et al case, PC and regpar were chosen by the authors to be a value of 3. Chosing any different numbers yields an entirely different result.
So the premise of the Steig et al paper paper boils down to an arbitrary choice of values that “looked good”.
I hope that Ryan O will write a rebuttal letter to Nature, and/or publish a paper. It is the only way the Team will back down on this. – Anthony
UPDATE: To further clarify, Ryan O writes in comments:
“Overall, Antarctica has warmed from 1957-2006. There is no debating that point. (However, other than the Peninsula, the warming is not statistically significant. ) 
The important difference is the location of the warming and the magnitude of the warming. Steig’s paper has the warming concentrated on the Ross Ice Shelf – which would lead you to entirely different conclusions than having a minimum on the ice shelf. As far as magnitude goes, the warming for the continent is half of what was reported by Steig (0.12 vs. 0.06 Deg C/Decade).
Additionally, Steig shows whole-continent warming from 1967-2006; this analysis shows that most of the continent has cooled from 1967-2006. Given that the 1940’s were significantly warmer in the Antarctic than 1957 (the 1957-1960 period was unusually cold in the Antarctic), focusing on 1957 can give a somewhat slanted picture of the temperature trends in the continent.”
Ryan O  adds later:  “I should have said that all reconstructions yield a positive trend, though in most cases the trend for the continent is not statistically significant.” 

Verification of the Improved High PC Reconstruction
Posted by Jeff Id on May 28, 2009
There is always something going on around here.
Up until now all the work which has been done on the antarctic reconstruction has been done without statistical verification. We believed that they are better from correlation vs distance plots, the visual comparison to station trends and of course the better approximation of simple area weighted reconstructions using surface station data.
The authors of Steig et al. have not been queried by myself or anyone else that I’m aware of regarding the quality of the higher PC reconstructions. And the team has largely ignored what has been going on over on the Air Vent. This post however demonstrates strongly improved verification statistics which should send chills down their collective backs. 
Ryan was generous in giving credit to others with his wording, he has put together this amazing piece of work himself using bits of code and knowledge gained from the numerous other posts by himself and others on the subject. He’s done a top notch job again, through a Herculean effort in code and debugging.
If you didn’t read Ryan’s other post which led to this work the link is:
Antarctic Coup de Grace
——————————————————————————–

Fig. 1: 1957-2006 trends; our reconstruction (left); Steig reconstruction (right)


HOW DO WE CHOOSE?


In order to choose which version of Antarctica is more likely to represent the real 50-year history, we need to calculate statistics with which to compare the reconstructions. For this post, we will examine r, r^2, R^2, RE, and CE for various conditions, including an analysis of the accuracy of the RegEM imputation. While Steig’s paper did provide verification statistics against the satellite data, the only verification statistics that related to ground data were provided by the restricted 15-predictor reconstruction, where the withheld ground stations were the verification target. We will perform a more comprehensive analysis of performance with respect to both RegEM and the ground data. Additionally, we will compare how our reconstruction performs against Steig’s reconstruction using the same methods used by Steig in his paper, along with a few more comprehensive tests.
To calculate what I would consider a healthy battery of verification statistics, we need to perform several reconstructions. The reason for this is to evaluate how well the method reproduces known data. Unless we know how well we can reproduce things we know, we cannot determine how likely the method is to estimate things we do not know. This requires that we perform a set of reconstructions by withholding certain information. The reconstructions we will perform are:
1. A 13-PC reconstruction using all manned and AWS stations, with ocean stations and Adelaide excluded. This is the main reconstruction.
2. An early calibration reconstruction using AVHRR data from 1982-1994.5. This will allow us to assess how well the method reproduces the withheld AVHRR data.
3. A late calibration reconstruction using AVHRR data from 1994.5-2006. Coupled with the early calibration, this provides comprehensive coverage of the entire satellite period.
4. A 13-PC reconstruction with the AWS stations withheld. The purpose of this reconstruction is to use the AWS stations as a verification target (i.e., see how well the reconstruction estimates the AWS data, and then compare the estimation against the real AWS data).
5. The same set of four reconstructions as above, but using 21 PCs in order to assess the stability of the reconstruction to included PCs.
6. A 3-PC reconstruction using Steig’s station complement to demonstrate replication of his process.
7. A 3-PC reconstruction using the 13-PC reconstruction model frame as input to demonstrate the inability of Steig’s process to properly resolve the geographical locations of the trends and trend magnitudes.
–
Using the above set of reconstructions, we will then calculate the following sets of verification statistics:
–
1. Performance vs. the AVHRR data (early and late calibration reconstructions)
2. Performance vs. the AVHRR data (full reconstruction model frame)
3. Comparison of the spliced and model reconstruction vs. the actual ground station data.
4. Comparison of the restricted (AWS data withheld) reconstruction vs. the actual AWS data.
5. Comparison of the RegEM imputation model frame for the ground stations vs. the actual ground station data.
–
The provided script performs all of the required reconstructions and makes all of the required verification calculations. I will not present them all here (because there are a lot of them). I will present the ones that I feel are the most telling and important. In fact, I have not yet plotted all the different results myself. So for those of you with R, there are plenty of things to plot.
Without further ado, let’s take a look at a few of those things.
Fig. 2: Split reconstruction verification for Steig reconstruction
You may remember the figure above; it represents the split reconstruction verification statistics for Steig’s reconstruction. Note the significant regions of negative CE values (which indicate that a simple average of observed temperatures explains more variance than the reconstruction). Of particular note, the region where Steig reports the highest trend – West Antarctica and the Ross Ice Shelf – shows the worst performance.
Let’s compare to our reconstruction:
Fig. 3: Split reconstruction verification for 13-PC reconstruction

There still are a few areas of negative RE (too small to see in this panel) and some areas of negative CE. However, unlike the Steig reconstruction, ours performs well in most of West Antarctica, the Peninsula, and the Ross Ice Shelf. All values are significantly higher than the Steig reconstruction, and we show much smaller regions with negative values.
As an aside, the r^2 plots are not corrected by the Monte Carlo analysis yet. However, as shown in the previous post concerning Steig’s verification statistics, the maximum r^2 values using AR(8) noise were only 0.019, which produces an indistinguishable change from Fig. 3.
Now that we know that our method provides a more faithful reproduction of the satellite data, it is time to see how faithfully our method reproduces the ground data. A simple way to compare ours against Steig’s is to look at scatterplots of reconstructed anomalies vs. ground station anomalies:
Your browser may not support display of this image.
Fig. 4: 13-PC scatterplot (left); Steig reconstruction (right)

The 13-PC reconstruction shows significantly improved performance in predicting ground temperatures as compared to the Steig reconstruction. This improved performance is also reflected in plots of correlation coefficient:
Fig. 5: Correlation coefficient by geographical location
As noted earlier, the performance in the Peninsula , West Antarctica, and the Ross Ice Shelf are noticeably better for our reconstruction. Examining the plots this way provides a good indication of the geographical performance of the two reconstructions. Another way to look at this – one that allows a bit more precision – is to plot the results as bar plots, sorted by location:
Fig. 6: Correlation coefficients for the 13-PC reconstruction

Fig. 7: Correlation coefficients for the Steig reconstruction

The difference is quite striking.
While a good performance with respect to correlation is nice, this alone does not mean we have a “good” reconstruction. One common problem is over-fitting during the calibration period (where the calibration period is defined as the periods over which actual data is present). This leads to fantastic verification statistics during calibration, but results in poor performance outside of that period.
This is the purpose of the restricted reconstruction, where we withhold all AWS data. We then compare the reconstruction values against the actual AWS data. If our method resulted in overfitting (or is simply a poor method), our verification performance will be correspondingly poor.
Since Steig did not use AWS stations for performing his TIR reconstruction, this allows us to do an apples-to-apples comparison between the two methods. We can use the AWS stations as a verification target for both reconstructions. We can then compare which reconstruction results in better performance from the standpoint of being able to predict the actual AWS data. This is nice because it prevents us from later being accused of holding the reconstructions to different standards.
Note that since all of the AWS data was withheld, RE is undefined. RE uses the calibration period mean, and there is no calibration period for the AWS stations because we did the reconstruction without including any AWS data. We could run a split test like we did with the satellite data, but that would require additional calculations and is an easier test to pass regardless. Besides, the reason we have to run a split test with the satellite data is that we cannot withhold all of the satellite data and still be able to do the reconstruction. With the AWS stations, however, we are not subject to the same restriction.
Fig. 8: Correlation coefficient, verification period, AWS stations withheld

With that, I think we can safely put to bed the possibility that our calibration performance was due to overfitting. The verification performance is quite good, with the exception of one station in West Antarctica (Siple). Some of you may be curious about Siple, so I decided to plot both the original data and the reconstructed data. The problem with Siple is clearly the short record length and strange temperature swings (in excess of 10 degrees), which may indicate problems with the measurements:
Fig. 9: Siple station data

While we should still be curious about Siple, we also would not be unjustified in considering it an outlier given the performance of our reconstruction at the remainder of the station locations.
Leaving Siple for the moment, let’s take a look at how Steig’s reconstruction performs.
Fig. 10: Correlation coefficient, verification period, AWS stations withheld, Steig reconstruction
Not too bad – but not as good as ours. Curiously, Siple does not look like an outlier in Steig’s reconstruction. In its place, however, seems to be the entire Peninsula. Overall, the correlation coefficients for the Steig reconstruction are poorer than ours. This allows us to conclude that our reconstruction more accurately calculated the temperature in the locations where we withheld real data.
Along with correlation coefficient, the other statistic we need to look at is CE. Of the three statistics used by Steig – r, RE, and CE – CE is the most difficult statistic to pass. This is another reason why we are not concerned about lack of RE in this case: RE is an easier test to pass.
Fig. 11: CE, verification period, AWS stations withheld
Your browser may not support display of this image.
Fig. 12: CE, verification period, AWS stations withheld, Steig reconstruction
The difference in performance between the two reconstructions is more apparent in the CE statistic. Steig’s reconstruction demonstrates negligible skill in the Peninsula, while our skill in the Peninsula is much higher. With the exception of Siple, our West Antarctic stations perform comparably. For the rest of the continent, our CE statistics are significantly higher than Steig’s – and we have no negative CE values.
So in a test of which method best reproduces withheld ground station data, our reconstruction shows significantly more skill than Steig’s.
The final set of statistics we will look at is the performance of RegEM. This is important because it will show us how faithful RegEM was to the original data. Steig did not perform any verification similar to this because PTTLS does not return the model frame. Unlike PTTLS, however, our version of RegEM (IPCA) does return the model frame. Since the model frame is accessible, it is incumbent upon us to look at it.
Note:    In order to have a comparison, we will run a Steig-type reconstruction using RegEM IPCA.
There are two key statistics for this: r and R^2. R^2 is called “average explained variance”. It is a similar statistic to RE and CE with the difference being that the original data comes from the calibration period instead of the verification period. In the case of RegEM, all of the original data is technically “calibration period”, which is why we do not calculate RE and CE. Those are verification period statistics.
Let’s look at how RegEM IPCA performed for our reconstruction vs. Steig’s.
Fig. 13: Correlation coefficient between RegEM model frame and actual ground data

As you can see, RegEM performed quite faithfully with respect to the original data. This is a double-edged sword; if RegEM performs too faithfully, you end up with overfitting problems. However, we already checked for overfitting using our restricted reconstruction (with the AWS stations as the verification target).
While we had used regpar settings of 9 (main reconstruction) and 6 (restricted reconstruction), Steig only used a regpar setting of 3. This leads us to question whether that setting was sufficient for RegEM to be able to faithfully represent the original data. The only way to tell is to look, and the next frame shows us that Steig’s performance was significantly less than ours.
Fig. 14: Correlation coefficient between RegEM model frame and actual ground data, Steig reconstruction
The performance using a regpar setting of 3 is noticeably worse, especially in East Antarctica. This would indicate that a setting of 3 does not provide enough degrees of freedom for the imputation to accurately represent the existing data. And if the imputation cannot accurately represent the existing data, then its representation of missing data is correspondingly suspect.
Another point I would like to note is the heavy weighting of Peninsula and open-ocean stations. Steig’s reconstruction relied on a total of 5 stations in West Antarctica, 4 of which are located on the eastern and southern edges of the continent at the Ross Ice Shelf. The resolution of West Antarctic trends based on the ground stations alone is rather poor.
Now that we’ve looked at correlation coefficients, let’s look at a more stringent statistic: average explained variance, or R^2.
Fig. 15: R^2 between RegEM model frame and actual ground data
Using a regpar setting of 9 also provides good R^2 statistics. The Peninsula is still a bit wanting. I checked the R^2 for the 21-PC reconstruction and the numbers were nearly identical. Without increasing the regpar setting and running the risk of overfitting, this seems to be about the limit of the imputation accuracy.
Fig. 16: R^2 between RegEM model frame and actual ground data, Steig reconstruction

Steig’s reconstruction, on the other hand, shows some fairly low values for R^2. The Peninsula is an odd mix of high and low values, West Antarctica and Ross are middling, while East Antarctica is poor overall. This fits with the qualitative observation that the Steig method seemed to spread the Peninsula warming all over the continent, including into East Antarctica – which by most other accounts is cooling slightly, not warming.
CONCLUSION
With the exception of the RegEM verification, all of the verification statistics listed above were performed exactly (split reconstruction) or analogously (restricted 15 predictor reconstruction) by Steig in the Nature paper. In all cases, our reconstruction shows significantly more skill than the Steig reconstruction. So if these are the metrics by which we are to judge this type of reconstruction, ours is objectively superior.
As before, I would qualify this by saying that not all of the errors and uncertainties have been quantified yet, so I’m not comfortable putting a ton of stock into any of these reconstructions. However, I am perfectly comfortable saying that Steig’s reconstruction is not a faithful representation of Antarctic temperatures over the past 50 years and that ours is closer to the mark.
NOTE ON THE SCRIPT
If you want to duplicate all of the figures above, I would recommend letting the entire script run. Be patient; it takes about 20 minutes. While this may seem long, remember that it is performing 11 different reconstructions and calculating a metric butt-ton of verification statistics.
There is a plotting section at the end that has examples of all of the above plots (to make it easier for you to understand how the custom plotting functions work) and it also contains indices and explanations for the reconstructions, variables, and statistics. As always, though, if you have any questions or find a feature that doesn’t work, let me know and I’ll do my best to help.
Lastly, once you get comfortable with the script, you can probably avoid running all the reconstructions. They take up a lot of memory, and if you let all of them run, you’ll have enough room for maybe 2 or 3 more before R refuses to comply. So if you want to play around with the different RegEM variants, numbers of included PCs, and regpar settings, I would recommend getting comfortable with the script and then loading up just the functions. That will give you plenty of memory for 15 or so reconstructions.
As a bonus, I included the reconstruction that takes the output of our reconstruction, uses it for input to the Steig method, and spits out this result:
Fig. 17: Steig reconstruction using the 13-PC reconstruction as input.

The name for the list containing all the information and trends is “r.3.test”.
—————————————————————-
Code is here Recon.R


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95dca824',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Vice President Gore continues to assault Gov. George W. Bush’s $1.5 trillion tax cut plan on the grounds that it would “spend all the budget surplus.” But Gore’s own federal spending promises are more costly than Bush’s tax cut, by a long shot.



Gore’s campaign proposals — for universal federal preschool funding, drug benefits to seniors, the Kyoto global warming treaty, anti‐​smoking programs, expanded Medicaid health coverage, and on ad infinitum — would add $1.6 trillion to the federal budget over the next 10 years, and that price tag could double in the following decade. Bush is right: No Democratic presidential candidate in the last 30 years — neither Dukakis nor Mondale nor McGovern — put forward such a high‐​priced menu of new federal initiatives. In fact, the cost of Gore’s spending schemes exceeds the proposals of Ralph Nader’s Green Party.



I have scoured through all of the spending proposals presented on the Gore 2000 web site and in the latest Clinton‐​Gore budget proposal presented to Congress. I have added to that the taxpayers’ tab for all the special‐​interest campaign promises Gore has made over the past several months. The biggest‐​ticket items are new entitlement programs. For example, Gore’s gold‐​plated prescription drug benefit program for seniors would cost $432 billion. His “Retirement Savings Plus” plan would dole out another $200 billion in tax dollars to low‐​income workers — many of whom cannot afford to save on their own because of the 12 percent Social Security tax.



Expanding government health coverage to uninsured families would, conservatively estimated, cost $146 billion. His plan to provide free or subsidized preschool for three and 4 year‐​olds carries a $115 billion price.



Gore’s blueprint also envisions beefing up the budgets of most of the federal regulatory agencies, including OSHA, the EPA, and the civil rights and antitrust snoops at the Justice Department. He wants $16 billion for teacher pay raises, a $200 million anti‐​smoking initiative, $45 million for curtailing violence at abortion clinics, $2 billion to combat suburban sprawl, several hundred million to develop solar energy and other alternatives to fossil fuels, $2 billion for a “livable cities” plan, at least $1 billion more for researching global climate change, and the ultimate in political correctness: a new Labor Department program to “train women for high‐​tech jobs” (no price tag listed).



The precise total comes to $1.64 trillion in new spending through 2010 — or almost $15,000 for every household in America. What is even more astonishing is that Gore has suggested virtually no offsetting budget cuts. All this new spending would be paid for by squandering the expected tax surpluses. Out of the several thousand federal programs in the 1,600-page federal budget, Al Gore, the man who invented reinventing government, hasn’t yet identified a single one in his presidential campaign that should be terminated. Regrettably, neither has Bush.



In the presidential debates and on the campaign trail, Gore has cultivated a fiscally moderate image. But the truth is that from the moment he first entered Congress more than 20 years ago, Gore has been a relentless advocate of nanny‐​state government expansionism. In 1989 and 1990, Gore won the National Taxpayers Union award for the biggest spender on Capitol Hill, on both occasions nudging out Ted Kennedy for this dubious honor. In 11 of 13 years, Gore received the lowest possible NTU grade on taxpayer issues.



If enacted, Gore’s new generation of federal welfare state entitlement programs would be ticking fiscal time bombs that will explode over the next decade, just when Baby Boomers are set to retire. Gore’s audacious $1.5 trillion agenda to nationalize day care, health care, education, crime fighting, transportation policy, health care, zoning and traffic patterns are brilliantly softened with conservative rhetoric about advancing “fiscal responsibility.”



Gore is not so much a man who wants to reinvent government as he is a man who wants to relegitimize it and expand it as much as possible. What is truly unseemly is how he pursues that objective by mendaciously employing the language of fiscal restraint. His proposed blitz of new spending is more expensive than any other presidential candidate has sought since Lyndon Johnson unveiled the Great Society. And just when we thought the era of big government was over.
"
"
Guest Post by Willis Eschenbach
This is an extension of the ideas I laid out as the Thunderstorm Thermostat Hypothesis on WUWT. For those who have not read it, I’ll wait here while you go there and read it … (dum de dum de dum) … (makes himself a cup of coffee) … OK, welcome back. Onwards.
The hypothesis in that paper is that clouds and thunderstorms, particularly in the tropics, control the earth’s temperature. In that paper, I showed that a falsifiable prediction of greater increase in clouds in the Eastern Pacific was supported by the satellite data. I got to thinking a couple of days ago about what other kinds of falsifiable predictions would flow from that hypothesis. I realized that one thing that should be true if my hypothesis were correct is that the climate sensitivity should be very low in the tropics.
I also figured out how I could calculate that sensitivity, by using the change in incoming solar energy (insolation) between summer and winter. The daily average top of atmosphere (TOA) insolation is shown in Figure 1.

Figure 1. Daily TOA insolation by latitude and day of the year. Phi (Φ) is the Latitude, and theta (Θ) is the day of the year expressed as an angle from zero to 360. Insolation is expressed in watts per square metre. SOURCE.
(As a side note, one thing that is not generally recognized is that the poles during summer get the highest daily average insolation of anywhere on earth. This is because, although they don’t get a lot of insolation even during the summer, they are getting it for 24 hours a day. This makes their daily average insolation much higher than other areas. But I digress …)
Now, the “climate sensitivity” is the relationship between an increase in what is called the “forcing” (the energy that heats the earth, in watts per square metre of earth surface) and the temperature of the earth in degrees Celsius. This is generally expressed as the amount of heating that would result from the forcing increase due to a doubling of CO2. A doubling of CO2 is estimated by the IPCC to increase the TOA forcing by 3.7 watts per metre squared (W/m2). The IPCC claims that the climate sensitivity is on the order of 3°C per doubling of CO2, with an error band from 2°C to 4.5°C.
My insight was that I could compare the winter insolation with the summer insolation. From that I could calculate how much the solar forcing increased from winter to summer. Then I could compare that with the change in temperature from winter to summer, and that would give me the climate sensitivity for each latitude band.
My new falsifiable predictions from my Thunderstorm Thermostat Hypothesis were as follows:
1 The climate sensitivity would be less near the equator than near the poles. This is because the almost-daily afternoon emergence of cumulus and thunderstorms is primarily a tropical phenomenon (although it also occurs in some temperate regions).
2 The sensitivity would be less in latitude bands which are mostly ocean. This is for three reasons. The first is because the ocean warms more slowly than the land, so a change in forcing will heat the land more. The second reason is that the presence of water reduces the effect of increasing forcing, due to energy going into evaporation rather than temperature change. Finally, where there is surface water more clouds and thunderstorms can form more easily.
3 Due to the temperature damping effect of the thunderstorms as explained in my Thunderstorm Thermostat Hypothesis, as well as the increase in cloud albedo from increasing temperatures, the climate sensitivity would be much, much lower than the canonical IPCC climate sensitivity of 3°C from a doubling of CO2.
4 Given the stability of the earth’s climate, the sensitivity would be quite small, with a global average not far from zero.
So those were my predictions. Figure 2 shows my results:


Figure 2. Climate sensitivity by latitude, in 20° bands. Blue bars show the sensitivity in each band. Yellow lines show the standard error in the measurement.
Note that all of my predictions based on my hypothesis have been confirmed. The sensitivity is greatest at the poles. The areas with the most ocean have lower sensitivity than the areas with lots of land. The sensitivity is much smaller than the IPCC value. And finally, the global average is not far from zero.
DISCUSSION
While my results are far below the canonical IPCC values, they are not without precedent in the scientific literature. In CO2-induced global warming: a skeptic’s view of potential climate change,  Sherwood Idso gives the results of eight “natural experiments”. These are measurements of changes in temperature and corresponding forcing in various areas of the earth’s surface. The results of his experiments was a sensitivity of 0.3°C per doubling. This is still larger than my result of 0.05°C per doubling, but is much smaller than the IPCC results.
Kerr et al.  argued that Idso’s results were incorrect because they failed to allow for the time that it takes the ocean to warm, viz:
A major failing, they say, is the omission of the ocean from Idso’s natural experiments, as he calls them. Those experiments extend over only a few months, while the surface layer of the ocean requires 6 to 8 years to respond significantly to a change in radiation.
I have always found this argument to be specious, for several reasons:
1 The only part of the ocean that is interacting with the atmosphere is the surface skin layer. The temperature of the lower layers is immaterial, as the evaporation, conduction and radiation from the ocean to the atmosphere are solely dependent on the skin layer.
2 The skin layer of the ocean, as well as the top ten metres or so of the ocean, responds quite quickly to increased forcing. It is much warmer in the summer than in the winter. More significantly, it is much warmer in the day than in the night, and in the afternoon than in the morning. It can heat and cool quite rapidly.
3 Heat does not mix downwards in the ocean very well. Warmer water rises to the surface, and cooler water sinks into the depths until it reaches a layer of equal temperature. As a result, waiting a while will not increase the warmth in the lower levels by much.
As a result, I would say that the difference between a year-long experiment such as the one I have done, and a six-year experiment, would be small. Perhaps it might as much as double my climate sensitivity values for the areas that are mostly ocean, or even triple them … but that makes no difference. Even tripled, the average global climate sensitivity would still be only on the order of 0.15°C per CO2 doubling, which is very, very small.
So, those are my results. I hold that they are derivable from my hypothesis that clouds and thunderstorms keep the earth’s temperature within a very narrow level. And I say that these results strongly support my hypothesis. Clouds, thunderstorms, and likely other as-yet unrecognized mechanisms hold the climate sensitivity to a value very near zero. And a corollary of that is that a doubling of CO2 would make a change in global temperature that is so small as to be unmeasurable.
In the Northern Hemisphere, for example, the hemispheric average temperature change winter to summer is about 5°C. This five degree change in temperature results from a winter to summer forcing change of no less than 155 watts/metre squared … and we’re supposed to worry about a forcing change of 3.7 W/m2 from a doubling of CO2???
The Southern Hemisphere shows the IPCC claim to be even more ridiculous. There, a winter to summer change in forcing of 182 W/m2 leads to a 2°C change in temperature … and we’re supposed to believe that a 3.7 W/m2 change in forcing will cause a 3° change in temperature? Even if my results were off by a factor of three, that’s still a cruel joke.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8d5f3754',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Leif Svalgaard writes in with a collection of points on the 10.7 cm solar radio flux. Being busy tonight, I’m happy to oblige posting them. – Anthony
Leif writes:
People often call out that F10.7 flux has now reached a new low, and that a  Grand minimum is imminent.
Perhaps this graph would calm nerves a bit:

The  blue curve is the current F10.7 flux [adjusted to 1 AU, of course] and the  red curve is F10.7 back at the 1954 minimum. The D spike (in 1954) was due to an old cycle [18] region.
There is always the problem of  how to align two such curves.. These two were aligned by eye to convey the  general nature of the flux over a minimum. The peaks labeled B and C and the  low part A were arbitrarily aligned, because peaks often influence the flux  for several weeks so would form natural points of correspondence. The  detailed similarity is, of course, of no significance. Note, however that  because of the 27-day recurrence one some peaks are aligned others will be  too. again, this has no further [deeper] significance. The next solar cycle  is predicted to be quite low and the cycle following the 1954 minimum  was one of the largest recorded. We will, of course, with excitement  watch how the blue curve will fare over the next year or so, to see how  the ‘ramp up’ will compare to the steep ramp up in 1955-1956.
Of course, as there was more activity before and after the minimum and even  during [as cycles overlap]. For the very year of the minimum apart from the  spike at D there is very little difference. The important issue [for me] is  the absolute level, because that is a measure of the density and temperature  of the lower corona, generated by the ‘network’ or background magnetic field,  which seems very constant from minimum to minimum, and certainly does not  portend an imminent Grand Minimum, which is not to say that such could not  come, just that a low F10.7 is not an indicator for it.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94ef3f1c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSince the “dramatic monster killer” tornadoes left a trail “of death and destruction” in the USA, the German media have once again began to blame abnormal human behaviour and SUV-witchcraft for the “man-made” disasters.
One example is here at the German leftist Rote Fahne News site. News is what they call it – others might term it rants and ravings of lunatics. Hat-tip DirkH. Note that this kind of reporting is not exclusive to kook sites like Rote Fahne, but is everywhere in the mainstream media. It really is so (Yesterday I heard the dubious claim on NDR public radio).
Here’s how the Rote Fahne explains the tornadoes and the US reaction to them.
That such conditions are becoming increasingly favourable because of global warming due to the burning of fossil fuels like oil, gas, and coal is being kept a secret. Huge amounts of carbon dioxide are being released. That acts ‘like a greenhouse glass roof over the planet. That leads to the greenhouse effect and takes the climate out of whack,’ says the climate program of the MLPD. US President Obama now expresses his deep sadness because of the victims and vast damage for citizens – but at the same time the destruction is being blamed on the natural environment!”
The nerve! Someone claiming the tornadoes are natural!
This of course reminds us of the 17th century witch hunts and trials held in both Europe and early colonial America. Superstitious people, claiming the authority of the latest science, claimed natural disasters and their ensuing misery, along with their own neurotic behavior, were caused by witches. Witch hunts then quickly became a method of eliminating opponents. Today, the climate science madness expressed in the text of Rote Fahne is precisely the same. Compare today’s global warming movement to the old witch trial times in the following Youtube video:
http://www.youtube.com/watch?v=NuSwx3d0R7w&feature=related


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Aren’t the parallels absolutely striking? There’s nary a difference.
Today’s version of what is driving the new witch hunts is only slightly modified (see the 0:45 mark of the above video). Today it could read as follows: “What is written in the Climate Bible is the Word of Gore; it is infallible and we have to live by it on a daily basis. And when you read the book of CO2, it is written: ‘Thou shalt not suffer a skeptic to live’.”
The paranoia and madness of 1692 Salem, thanks to modern media, has re-emerged and gone global today. It’s the same stuff. Today people like Hansen, Gore, and Schellnhuber are acting as the new Pope Gregory the 9ths or Innocent the 8ths.
Today they claim people are behaving “abnormally” and so terrible things are happening because of it. The natural disasters are all the proof they need. Today it is capitalist witches at work. How do we know? Rote Fahne delivers the proof, and writes:
The environmental crisis has developed concurrently as a by-product of capitalism by law.”
For them, that’s enough proof. Prepare the gallows! The author of this leftist excrement obviously has forgotten the environmental cesspool that communism and socialism produced in the former regimes of East Germany and the Soviet Union. Thanks to shutting down these inefficient systems, united Germany’s CO2 emissions dropped 15% overnight.
Make no mistake about it – eventually history will look back and view these “infallible scientists” as Pope Gregory the 9ths or Innocent the 8th-type crackpots and zealots who just lost their way in the darkness of their own ignorance and arrogance.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIt’s over. All that is left to die is that last flicker of European hope.Another European politician has long since seen the bold-letter writing on the wall. The following was written by European Parliamentarian Holger Krahmer and appears in German at the Achse des Guten here. What follows is an English translation (emphasis added).
=============================
Only Europe Still Believes In Rescuing The Climate
by Holger Krahmer (MdEP)
The climate policy of the European Union is now stuck for good in a dead end. Europe wanted to be the leader – showing the world the way. They especially wanted to export the “market-economic” instrument of emissions trading as a new standard of regulation. The climate summits in Copenhagen and in Cancun were supposed to herald in a successor treaty for the 1997 Kyoto Protocol, which expires in 2012. Both summits yielded zero results. Today it is clear: There is going to be no successor agreement. Also the option of simply extending the existing Kyoto Protocol was thrown overboard by the main countries at the last G8 summit.
The situation in global climate politics can be summarized in short: There isn’t any.
Especially the emerging economies of Asia are refusing to allow their possibilities for growth to be curbed by obligatory CO2 reductions. Everywhere globally, climate laws are being buried for good or put on ice. Especially the once ballyhooed instrument of emissions trading is obsolete. China, India and Australia are waving goodbye. In the USA the Chicago Climate Exchange was closed just after the last midterm Congressional elections. Just before that, the self-anointed climate pope Al Gore cashed in by selling his shares. “Climate politics is a dead project“ is the word in Washington today. Yet, the EU is still clinging to all measures and is even discussing making them even stricter. As a result, we are now left alone with the political costs of CO2 reduction. We are ignoring international reality with an amazing level of tenacity.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




As always, we continue to stick to the naive, worn-out argument: “Someone has to start the process.“ That start is a go-it-alone! And if we do not wake up to that, then we will ruin our market economy with one-sided massive costs.
In the end, that will do nothing for the climate. No matter what CO2 may do in the atmosphere, every reduction in European emissions will be offset very quickly by other emissions someplace else in the world. In addition to the political realities, questioning the scientific basis of climate change put out by the IPCC is being increasingly equated as a sort of blasphemy.
Now that the IPCC is completely discredited by scandals and political influence, the current CO2 hypotheses need to be re-evaluated. That climate change is complex was always known. That it is mainly driven by a trace gas in the atmosphere is unlikely when one soberly examines it closely with an open scientific mind and when one examines the long-term history of the climate.
Whoever has the power over the religion, also has the power over the people. That has always been the case for religions. Today the belief in manmade global warming appears to have become a sort of substitute religion.
=====================================================
Readers, please write a few words of support to Mr Krahmer!
holger.krahmer@europarl.europa.eu
http://www.holger-krahmer.de/
Share this...FacebookTwitter "
"Africa’s elephant population has plummeted from roughly a million in 1970 to around 400,000 today – a decline which is largely blamed on poaching for their ivory tusks. At its peak in 2011, poaching claimed 36,000 elephants a year, or one every 15 minutes.  Many of us are familiar with these statistics thanks to campaigns to end the ivory trade. But with our attention focused on poaching, an arguably greater threat to Africa’s elephants has emerged. In the time that Africa’s elephant population has crashed, its human population has boomed. The number of people living in Africa has doubled since 1982, reaching a billion in 2009, and is expected to double again by 2050.  To feed and house this growing population, natural habitats have been fragmented by roads and railways and entire swathes have been converted to farmland and settlements. As a result, Africa’s elephants have been squeezed into smaller and increasingly isolated pockets of land. It’s very possible that the future for all of Africa’s elephants will resemble what is currently seen in South Africa. Here, elephants are largely confined to small, fenced reserves separated by vast human-dominated landscapes. Elephants can’t disperse from these reserves, but their relative protection within them has seen their densities increase. So much so that in stark contrast to the “elephant extinction” narrative we’re used to, some consider South Africa’s reserves to have “too many elephants”. Elephants play a crucial role in Africa’s savanna ecosystems as seed dispersers. Their dung recycles valuable nutrients and by feeding on trees they maintain the savanna’s matrix of woodland and grassland and the biodiversity it supports.  However, over prolonged periods, high elephant densities can reduce tree cover, which shrinks woodland and expands grassland habitats. This may threaten browsing species, such as black rhinoceros and bushbuck, for which trees provide food and shelter. Managing elephants to prevent habitat change and preserve biodiversity has a long history. Culling programmes continued into the late 20th century and only ended in Kruger National Park in 1994. Culling remains a “last resort” for managing elephants in South Africa and there have been recent calls to resume culling operations in Botswana. Culls have now largely been replaced by non-lethal approaches, including translocating elephants to other areas and contraceptives to reduce birth rates.  However, all management interventions cause some degree of stress for elephants. There’s always a small risk with using anaesthetics and hormonal contraceptives can alter an elephant’s behaviour.  The fundamental question over the future of Africa’s elephants is whether we are happy to allow them to exist only where they are heavily managed. If so, then we’ll need more research to understand the most effective and ethical ways of managing elephants. If not, then securing more space for elephants alongside human communities could be the answer. This boils down to an old debate – to spare land or share it. Land sparing means separating pristine wildlife habitats from areas of human activity while land sharing involves maintaining biodiversity within landscapes shared by humans. But which is best for conservation? South Africa shows us what land sparing means for elephants – expensive, ongoing management in densely populated reserves. The alternative land sharing approach gives elephants greater access to Africa’s landscapes, but relies on coexistence between people and elephants. At present, land sharing systems outside of Africa’s national parks and reserves are fragile. Human-elephant interactions can threaten the lives of both parties but strategies exist to help coexistence. At the heart of all of them is an understanding that there must be clear benefits to humans in sharing space with elephants. The revenue from tourists paying to see elephants can provide direct employment but education programmes are also necessary to help people understand how elephants benefit the wider ecosystem. Livelihoods outside agriculture must be encouraged to reduce pressures on habitats and wildlife while providing stable incomes in the face of changing environments. Thoughtful land management and planning should ensure vital elephant habitats are protected. Groups across Africa are already working on solutions which can deliver this. Alongside tourism, projects have emerged which generate revenue from elephants without harming them or the environment, such as producing paper and gifts made from elephant dung. The charity Save the Elephants teaches local children about the benefits of living in harmony with elephants and organisations such as the Amboseli Ecosystem Trust have started working with conservationists, politicians and local communities to plan how coexistence can be achieved. Land sharing between humans and elephants will depend on this kind of collaboration between governments, conservation groups and local communities. If people want more for Africa’s elephants than confinement to heavily managed reserves, then everyone needs to be consulted. Only then can there be hope for peaceful coexistence between people and elephants."
"
Everyone see things in the clouds. People, animals, Christ on the cross,  UFO’s, angels, and even schizophrenically imagined chemical attacks by contrails. You name it, somebody has seen it. So when I was prodded with a news item that said “new cloud type defined” I was thinking “uh oh, here we go again”. It is a lot like cyclomania, as humans tend to assign patterns to randomly ordered observations of nature. Looking for meanings in the clouds isn’t much different than looking for meanings in the alignments of the stars and planets.
From ChattahBox and The UK Telegraph:
Click for a larger image
(ChattahBox)—Meteorologists around the world have taken notice of a new storm cloud on the horizon, literally. And if they have their way the dark and choppy cloud will take its rightful place among its more famous cousins, cumulus, cumulus, cirrus and nimbus.
Cloud gazing Meteorologists first noticed the stormy and billowy formation floating over the Scottish Highlands and above Snowdonia, Wales. The unique gray storm cloud was also spotted over Australia, the cornfields of Iowa and high above the Arctic Sea off the coast of Greenland.
A group in England dedicated to cloud watching, the Cloud Appreciation Society, became quite excited when viewing numerous photos of the new storm cloud floating in the atmosphere.
The Cloud appreciators describe the cloud as “…a bit like looking at the surface of a choppy sea from below,” said Gavin Pretor-Pinney, founder of the Cloud Appreciation Society, and the first man to identify the new cloud.
The Royal Meteorological Society has named the new cloud, “Asperatus,” the Latin word for rough, since the cloud has the appearance of a rough, choppy ocean.
The Royal meteorologists are now attempting to have Asperatus officially recognized by the UN’s World Meteorological Organization in Geneva to have it included in the International Cloud Atlas.
If the meteorologists are successful, this would mark the first time a new cloud  was officially recognized since 1953.
Source

I have seen clouds like this, but did not see them as being a new classification. Thus a little trouble with the idea of making an entirely new classification for this cloud, a sub classification perhaps would be more appropriate, especially since this cloud does not appear to inhabit the middle and higher levels of the atmosphere.
Here are the existing classifications:



Latin Root

Translation

Example


cumulus
stratus
cirrus
nimbus

heap
layer
curl of hair
rain

fair weather cumulus
altostratus
cirrus
cumulonimbus



Classifications 
 High-Level Clouds
Cloud types include:  cirrus and cirrostratus.
Mid-Level Clouds
Cloud types include: altocumulus, altostratus.
Low-Level Clouds
Cloud types include: nimbostratus and  stratocumulus.
Clouds with Vertical Development
Cloud types include: fair weather cumulus and  cumulonimbus.
Other Cloud Types
Cloud types include: contrails,  billow clouds,  mammatus, orographic and pileus clouds.
Source: http://ww2010.atmos.uiuc.edu/(Gh)/guides/mtr/cld/cldtyp/home.rxml

So for “asperatus” I could see maybe “stratoasperatus” but not “altoasperatus” since there is no evidence of them at the high altitudes, and clouds at that level tend not to be rough edged.
I actually hope WMO doesn’t accept this ploy for attention by the Cloud Appreciation Society, if they do, it could open an avalanche of new cloud classification applications, we may see pitches of the most absurd kind.
For example, here’s another one from the Cloud Appreciation Society:
This contrail formation has been sent in by several different cloudspotters, and has become known as the Dorset Doughnut. Over Dorset, U.K.
 
“Altostratus Obamus” perhaps?
People see all sorts of things in the sky, if this new one is accepted, the petitioning for WMO recognition of new cloud types would never end.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95824fcb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterClearing the way for a mountain top windpark. (Photo credit: Mountain Talk)
Just a few miles away from my hometown, Green Mountain Electric Power Company of Vermont and Quebec’s Gaz Metro are now building a monster mountain-top wind park that will be home (for 25 years) to twenty one 135-meter tall turbines that will go into operation by the end of next year.
To build the windpark they are now busily deforesting acre after acre and literally blasting off the top of the mountain, thus permanently disfiguring in a matter of weeks what took nature and the ice ages (climate change) hundreds of thousands of years to sculpture. Suddenly the environmentalists are fuming mad. Not this way, they insist.
Protesters even launched a website called Mountain Talk and are now making their presence known on the mountain.
Last week two protesters were arrested by the police for allegedly trespassing on the site. The two are reported to be students from nearby Sterling College, which according to its website is a “small, progressive, liberal arts college” that is committed “to grassroots sustainability”.
Were once enthusiastic supporters of renewable energy
What’s strange is that for years Vermont environmentalists railed against carbon based fuels and pressured legislators to produce “clean” energy. Never mind that over 90% of Vermont’s electric power was CO2-free to start with (hydro from Quebec and nuclear from Vermont Yankee). Vermont even elected Obama, giving him a whopping 68% of the vote. Let’s go green was the message. Now windparks are naturally getting installed on Vermont’s beautiful ridgelines. Doing that, though, isn’t easy. It involves massive deforestation to clear land for access roads and the windpark itself. Because Vermont mountains are ruggedly uneven, a good amount of dynamiting is part of it.
Suddenly environmentalists have woken up and are fuming mad about what they had for years enthusiastically endorsed. Many will deny this and claim that they never supported wind turbines. But looking at the website of Sterling College, where the two protesters attend, we see the following under Global Field Studies:


“Research environmental and cultural sustainability by comparing current ecological practices in Denmark, Iceland, Norway, and Sweden. Local scholars and experts serve as guides as we explore alternative energy sites, investigate eco-villages and industries featuring green technology, and study…”
 Either the protesters in Vermont are just plain hypocritical, or terribly confused. Sterling students even went to Japan (by plane) and saw this:




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Explore environmentally sustainable practices on the northern island of Hokkaido through visits to managed forests, organic farms, native Ainu communities, and  Zen gardens. Conversations with students and professors at Hokkaido and Obihiro  Universities lead to in-depth investigations into land use practices.”

That’s sure some amazing land-use in the photo above, wouldn’t you all say?
Protesters blame corporations and not the lawmakers for all the mountain mutilation
Okay, the protesters woke up awhile back and now realize that the green dream is in fact a nightmare. You’d think they’d vent their anger at the politicians who cleared the way for this energy transition. You’d be wrong. Throughout the Mountain Talk website, all you hear is a lot of whining and moaning aimed at…the corporations. At this point, I’d say they are confused.
If these protesters ever sat down down with energy managers, the first thing they’d hear is that there are lots of other cheaper, easier, cleaner ways of generating electricity than to blow up mountains and install monster turbines. Windmills and solar are hardly at the top of the power company’s wishlist for ways of generating electricity. They aren’t blowing the tops off mountains because they want to, but because they are being regulated to do so.
It’s too bad the protesters can’t get that through their skulls.
Protesters are turning to the politicians, begging them to stop the evil corporations. Pardon me, but it’s the politicians with the zany green ideas that need to be stopped. Mountain Talk quotes US Senator of Vermont Bernie Sanders here:
We are trying to save the world.”
In trying to “save the world” from a scientifically unsubstantiated doomsday, they are all ruining Vermont.
I truly do hope that the protesters succeed in stopping future mountain top windparks. If they pull it off, it’ll be what I like to call “One stupidity killing another.” Nowadays you have to take the victories anyway you can.
======================================
Hat-tip Benny Peiser: Wind-farms-are-useless-says-Prince-Philip
 

Share this...FacebookTwitter "
"The Obama administration has proposed several ad hoc multi-country economic agreements, and in doing so has abandoned de facto the World Trade Organization (WTO) as insufficiently malleable to its interests.  The two most important of these are the Trans-Pacific Partnership (TPP) and the more recent Transatlantic Trade and Investment Partnership (TTIP). Even as the latter was being negotiated by US and EU officials, the World Meteorological Organization (WMO) reported that the increase in greenhouse gases is more rapid than expected.  The organisation’s secretary-general warned that humankind is “running out of time” to reverse rising levels of carbon dioxide that drive climate change.   These two items, agreements to increase world trade and rapidly rising greenhouse gases, call for a bit of “linked up thinking”. The US trade and investment initiatives have come under considerable attack for handing too much power over public services to private corporations, for reducing employment rights and for harming national sovereignty.   Whatever the validity of these objections, there is a more fundamental problem.  The purpose of the TPP and the TTIP is to increase the volume of trade among countries, and that is inherently bad for humankind because of its environmental effects. I recently attended a meeting in London with environmental activists, including a well-known British climate scientist.  As a result of that meeting I realise that my past critiques of “free trade” have been far too timid and narrow.   The essential problem is not that these treaties foster US and EU corporate interests, though that is undesirable for the rest of us; the problem is international trade itself. The charts below show why.  The two countries with the most exports in 2012 are the US and China, with Germany and Japan considerably further back (both the US and China over US$2 trillion, Germany at just over 1.5). By no accident, China and the United States are at the top of the pollution list, with Japan fifth and Germany sixth. “But wait”, you say, these are also the largest economies in the world, so the issue is their domestic energy use, not whether what they produce is exported. World’s leading exporters 2012 (billions of US$) World’s Leading Polluters 2012 (millions of tons) Well, actually, no.  A moment’s reflection makes the fallacy clear (and I am ashamed to say that I only just realised something so obvious). When something is produced domestically, using it domestically is less polluting than exporting it.  Production represents only the first contribution of a product to the world’s pollution.  While only a fanatic would oppose the transport of bananas from the Caribbean to the UK, restricting the movement of cars between Japan and Europe would seem a no-brainer for the environment. Many studies show that the domestic production component of industrial pollution is least in Germany, followed by the United States, Japan, and China, by far the worst production polluter of five. Producing a product for export requires the company in question to transport it to the importer.  While ocean transport can claim a greener bill-of-health than aviation, that represents a classical example of “damning with faint praise”. Air pollution from ships is increasing, even as land-based emissions are gradually being reduced. As the campaigning group Transport & Environment points out:  If things are left as they are, by 2020 shipping will be the biggest single emitter of air pollution in Europe, even surpassing the emissions from all land-based sources together. When companies transport products over long distances the environment suffers. It really is that simple.   Over the past ten years the number of full sea-borne containers has more than doubled (that is, excluding “turn-around” of empty containers). And air freighting fresh fruit and vegetables puts shipping in the pollution-generating second division, the difference between an environmental misdemeanour and a felony.  One source estimates the 1% of the world’s food that travels to you and me by air contributes 11% of carbon emissions. All these transport-generated emissions occur before the pollution caused by the use and then disposal of the products themselves. Production, consumption and disposal apply to all products, of course, no matter their source and destination.  But exporting and importing make their own – and unnecessary – contribution to the destruction of the environment. The huge and growing amount of pollution due to international trade brings to mind the famous critique of free trade by John Maynard Keynes (infamous and blasphemous for mainstream economists): I was brought up … to respect free trade not only as an economic doctrine which a rational and instructed person could not doubt… As lately as 1923 I was writing that free trade was based on fundamental “truths” which … no one can dispute… [But now] I sympathise…with those who would minimise, rather than with those who would maximise, economic entanglement among nations. Ideas, knowledge, science, hospitality, travel – these are the things which should of their nature be international. But let goods be homespun whenever it is reasonably and conveniently possible, and, above all, let finance be primarily national. Were Keynes with us now (at age 151), he would have cause to omit or at least qualify his endorsement of “travel”.  “Flying Clean”, an organisation that seeks to reduce aviation pollution, reports “long-haul flights produce on average twice as much emissions per mile travelled per passenger than cars, and short-haul flights produce three times as much”.  These estimates let a Humvee pass as green. So let us join Keynes to imagine if we can a world in which goods are “homespun” and finance is “primarily national”.  If we cannot imagine such a world, there is little hope for the planet. The combination of climate deterioration and the ravages of international trade and finance will fulfil the prophecy of Hobbes, rendering life on Earth “poor, nasty, brutish and short”.  Less trade not more, to render life on Earth “prosperous, fraternal, peaceful and long”."
"**Waking up to the shock the UK economy's had this year, you'd be forgiven for wanting to pull the duvet back right back over your head.**
Rishi Sunak probably feels the same.
But on Wednesday in his Spending Review he'll have to talk us through just how bad things are. Then he'll spell out how he plans to tackle the next year.
It's about more than just tweaking numbers. It'll give us some idea of who might get a pay cut, what areas the government wants to invest in, such as schools, hospitals and roads.
It may also tell us where jobs will be created, and if - or maybe how soon - we could be facing higher taxes.
Here's what to look out for:
Melissa Aitchison, a student at Nottingham University, wants to know when the economy will get stronger.
She's financing her Master's year by waitressing in a restaurant, but even when it re-opens after lockdown she'll be on shorter hours and taking home less money.
""People aren't giving tips as much,"" she says, and she's worried it'll take a while for confidence to return.
""Even when we do have a vaccine and get busier I think it would take a long time because people are still wary of the [economic] consequences of corona and will be saving money,"" she says.
So the more Mr Sunak can do to boost general economic growth the better as far as she's concerned, even more so once she starts seriously job hunting. A lot of her friends who graduated last year are struggling to find jobs to fit their qualifications.
We're going to hear how much the economy has shrunk this year and it could be more than 10%. In normal years it grows at least a bit.
Anyone looking for work and relying on state benefits should listen out for what Rishi Sunak says about the extra Â£20 a week that was added to Universal Credit at the start of the pandemic. It is currently set to go next April.
Financial blogger Kara Gammell says if the chancellor wants to offer some good news ""this is the time he might announce an extension to that"".
But the chancellor's also under pressure to rein back the astronomical levels of spending triggered by the pandemic, so that may not happen.
There are other ways he may look to cut back too.
Sophie Wilkes, a 25-year-old primary school teacher, wants to know if Mr Sunak will go ahead with a plan to freeze pay for public sector workers like her.
Teachers and other public sector workers had their pay frozen for nearly a decade after the financial crisis. Once inflation is taken into account, holding pay flat amounts to a pay cut in terms of what it will buy you.
Given how much extra work school staff at her inner-city primary in Cardiff have put in his year, Sophie thinks the idea is ""a bit shocking"".
Police, fire and prison officers, local government workers, and members of the armed forces could be in the same boat. But NHS staff are expected to still get a pay rise.
While the coronavirus pandemic has upended the chancellor's plans for spending, 24-year-old beef farmer and cattle breeder Matt Rollason hopes Mr Sunak will have more to say on post-Brexit plans.
""The government needs to make sure that there is seamless trade of food and agricultural goods, not just to the European Union (EU), but elsewhere too. To do that, they need to commit money to things like customs checks, border checks and admin.""
Without this extra funding, there could be a ""major risk"" for the ""intricate and complicated"" system of supplying food in the UK, Matt says.
In the long-term, the Lancashire farmer hopes that the chancellor will also back up the switch from EU farming grants to a new English scheme with a ""serious amount of money"".
""If the government is serious about agriculture moving to net zero emissions and the climate challenge we're all facing, that has to be in [the Spending Review],"" he says.
Rishi Sunak could also hint at future tax rises to help pay for the pandemic spending - though specific changes are more likely to come in the Budget next spring.
Economists say cutting spending or raising taxes too soon risks pulling the rug from under the economy before it has had time to get back on its feet and getting the economy growing is a much better way to increase tax take.
If we are paying more tax or our pay is low we're less likely to go out and spend, points out Iona Bain who writes the Young Money blog. ""And that's what's going to be needed to kickstart the economy.""
Iona's expecting the chancellor to talk us through some infrastructure projects, especially where they could help ""level up"" the parts of the UK that have the deepest economic problems.
But she'd also like to see some more ambitious plans for improving the UK's digital networks and for housebuilding that could in the long run bring down house prices. That could give a boost to jobs in engineering, technology, transport, sustainable energy and construction.
""If they make good on infrastructure spending promises that could create opportunities for younger people. If for instance you are in a sector that has been really damaged by lockdown, there's a chance you can pivot, retrain.""
Entrepreneur and engineer, Roni Savage is hoping for a bit more clarity beyond the one-year scope of the Spending review.
What her firm, environmental engineering consultancy, Jomas Associates, needs is a sign from the chancellor that small and medium-sized businesses (SMEs) will benefit as well as the industry giants.
""He could acknowledge at this stage the importance of involving SMEs,"" she says. ""That would be a really big win for us.""
Any pledges to green the economy should benefit her firm too.
""Boris talked about building back better. We're looking for that not to just be words but action""."
nan
"
Share this...FacebookTwitterThe online German-language Die Presse reports here on new climate research out of Stanford. Did this get picked up in the US press? Too nutty probably.According to the new research, Christopher Columbus was likely the cause of the Little Ice Age.
How, you may ask? Columbus, a really dirty European, and his fellow colonizers sailed the ocean blue in 1492, just before the LIA began in earnest. He brought along with him some nasty diseases which quickly spread across the new world and wiped out the indigneous population. As so after a very short time, there was no one left to burn down trees, and so tree growth exploded, sucked up huge amounts of CO2 from the atmosphere in the process, which in turn lowered global CO2 concentrations, which led to global cooling, and thus the production of the Little Ice Age.
Hey, at least they admit now that the Medieval Warm Period and Little Ice Age weren’t just local North Atlantic phenomena.
Global temperatures began dropping after the end of the Medieval Warm Period, in the 14th century. Richard Nevle, geochemist at Stanford University, claims that the Maunder Minimum, a period of low solar activity from 1653 to 1715, had less to do with causing the Little Ice Age and thinks he has solved the mystery and found the real culprit.
The cooling was already in full swing by the time the Maunder Minimum began and so it had to be something else. Die Presse writes:
It was Columbus and the following colonists. They caused the not so sparsely populated New World to get practically wiped out in a very short time. Back then at the end of the 15th century as Columbus was on the way, between 40 and 80 million people were living mostly in Central and South America, and they cleared forest with fire. But as the Europeans arrived, most of them died of unknown diseases – measles, pox, diphtheria – 90% of them were wiped out, or one fifth of the global population. The forests suddenly stopped being cleared.”
I wonder if this brilliance will be taken up by the IPCC’s next assessment report. What conclusion shall we draw? It confirms how desperate the warmists are in their quest to remove the sun from the climate equation. Pretty damn desperate.
Finally, before you parents think about spending $250,000 to educate your child at Stanford, you may want to think again.
rnevle@stanford.edu
 
Share this...FacebookTwitter "
"By now, most of us have heard that the use of plastics is a big issue for the environment. Partly fuelled by the success of the BBC’s Blue Planet II series, people are more aware than ever before about the dangers to wildlife caused by plastic pollution – as well as the impact it can have on human health – with industries promising money to tackle the issue. Single use plastics are now high on the agenda – with many people trying to do their bit to reduce usage. But what if all of this just provides a convenient distraction from some of the more serious environmental issues? In our new article in the journal Marine Policy we argue plastic pollution – or more accurately the response of governments and industry to addressing plastic pollution – provides a “convenient truth” that distracts from addressing the real environmental threats such as climate change. Yes, we know plastic can entangle birds, fish and marine mammals – which can starve after filling their stomachs with plastics, and yet there are no conclusive studies on population level effects of plastic pollution. Studies on the toxicity effects, especially to humans are often overplayed. Research shows for example, that plastic is not as great a threat to oceans as climate change or over-fishing. 


      Read more:
      Plastics in oceans are mounting, but evidence on harm is surprisingly weak


 Taking a stand against plastic – by carrying reusable coffee cups, or eating in restaurant chains where only paper straws are provided – is the classic neoliberal response. Consumers drive markets, and consumer choices will therefore create change in the industry.  Alternative products can often have different, but equally severe environmental problems. And the benefits of these small-scale consumer driven changes are often minor. Take, for example, energy-efficient light bulbs – in practice, using these has been shown to have very little effect on a person’s overall carbon footprint.  But by making these small changes, plastic still appears to be an issue we can address. The Ocean Cleanup of plastic pollution – which aims to sieve plastic out of the sea – is a classic example. Despite many scientists’ misgivings about the project and its recent failed attempts to collect plastic the project is still attractive to many as it allows us to tackle the issue without having to make any major lifestyle changes.  That’s not to say plastic pollution isn’t a problem, rather there are much bigger problems facing the world we live in – specifically climate change.  In October last year the Intergovernmental Panel on Climate Change (IPCC) produced a report detailing drastic action needed to limit global warming to 1.5˚C. Much of the news focused on what individuals could do to reduce their carbon footprint – although some articles did also indicate the need for collective action.  Despite the importance of this message, environmental news has been dominated by the issues of plastic pollution. So it’s not surprising that so many people think ocean plastics are the most serious environmental threat to the planet. But this is not the case. In 2009 the concept of planetary boundaries was introduced to indicate safe operating limits for the Earth from a number of environmental threats.  Three boundaries were shown to be exceeded: biodiversity loss, nitrogen flows and climate change. Climate change and biodiversity loss are also considered core planetary boundaries meaning if they are exceeded for a prolonged time, they can shift the planet into new, less hospitable, stable states.  These “clear and present dangers” of climate change and biodiversity loss could undermine the capacity of our planet to support over seven billion people – with the loss of homes, food sources and livelihoods. It could lead to major disruptions of our ways of life – by making many areas uninhabitable due increased temperatures and rising sea levels. These changes could start to happen within the current century. This is not to distract from the fact that some significant steps have been taken to help the planet environmentally by reducing plastic waste. But it is important not to forget the need for large-scale systemic changes needed internationally to tackle all environmental concerns. This includes longer-term and more effective solutions to the plastic problem – but also extending to more radical large-scale initiatives to reduce consumption, decarbonise economies and move beyond materialism as the basis for our well-being.  The focus needs to be on making the way we live more sustainable by questioning our overly consumerist lifestyles that are at the root of major challenges such as climate change, rather than a narrower focus on sustainable consumer choices – such as buying our takeaway coffee in a reusable cup. We must reform the way we live rather than tweak the choices we make.  There is a narrow window of opportunity to address the critical challenge of, in particular, climate change. And failure to do so could lead to massive systemic impacts to the Earth’s capacity to support life – particularly the human race. Now is not the time to be distracted by the convenient truth of plastic pollution, as the relatively minor threats this poses are eclipsed by the global systemic threats of climate change."
"A proposal for radioactive waste to appear at a burial site nearby, would be likely to fill the great majority of the UK population with thoughts of danger, cancer – and falling house prices. This illustrates the huge problem of public misperception to overcome when disposing of radioactive waste. Britain’s nuclear reactors have generated low-carbon electricity since 1956, in doing so creating around 260,000m3 (about the size of 700 double-decker buses) of intermediate-level waste and 3,000m3 of highly radioactive high-level waste, as well as spent fuel, plutonium and uranium. The price for decommissioning past and existing nuclear power plant and disposing of that waste is around £70 billion – the single largest item of expenditure for the UK Department of Energy and Climate Change. What to do with radioactive waste is a problem that has so far proved to be intractable to successive generations of civil servants and ministers. In the mid-1970s, it was decided that deep burial would provide the optimum secure solution.  Here, radioactive waste would be packaged and contained for one million years, sealed by multiple chemical and physical barriers within a repository dug out around 500 metres below ground level. A serious attempt was made to investigate a site in West Cumbria close to Sellafield in the 1990’s, but that foundered on the complexity of the geology and flow of deep groundwater, making it difficult to predict how well sealed the waste would be into the far future. In 2003, the government set up an expert committee of social scientists and policy analysts which in 2006 affirmed recommendations to bury waste within a geological disposal facility as the best method for securing radioactive waste. Communities should be asked to volunteer themselves as potential disposal sites. However the only volunteers in 2010 were in West Cumbria – the same sites that had already been rejected in the 1990s. The government carried on, providing several million pounds for community engagement, but in January 2013 the process ground to a halt after local district councils voted in favour, only for the overarching county council to reject the proposition. Several issues of contention emerged. The right for the host community to withdraw was promised by the government, but never transcribed into any contract. A package of benefits to the hosting community was promised, but exactly what and when it would be paid was not stated. The definition of the host community, its boundary, and its relationship with the wider region remained vague. Exactly what waste would be buried was contested. And, as had previously been established, there was no confidence in the site’s geology. The government retired again to lick its wounds. Another review and public consultation was undertaken during 2013-14, from which emerged the White Paper “Implementing Geological Disposal” published in July 2014. The results show the government has done some serious listening, and it provides some distinctly new approaches.   First, a new body Radioactive Waste Management Ltd will be created to pursue a disposal site. The company will be wholly owned by the government and could propose more than one facility for different types of waste. This has been tried in the 1980s and 1990s with UK Nirex – a limited company wholly owned by UK government, which spent £400m investigating just one site. Can you spot any difference? So how this operates will be more important than the definition.   Second, the government states it is keen to “listen and respond to views and concerns”. Yet in the future this search will now become defined as a nationally significant infrastructure project, which means that many powers of local people to decide or influence could be restricted or removed. Specifically, the control and influence of local councils has been removed, combined with a statement that no tier of government will have the right to veto a project. So the responsibility of regional council authorities for managing this waste, and the associated road and rail and excavation infrastructure is also removed.  Third, the search for a site will become national, with a two-year period of geological survey and screening to identify suitable regions (not specific sites). Identifying secure regions may become difficult if the extensive fracking of England goes ahead for shale gas and oil, as, any effect on the underlying geology could affect a site’s long term secure storage potential.   Detailed geophysical surveys and borehole drilling will need planning permission to establish the suitability of a site, before the community opinion is consulted to make a binding test of acceptance, after which there is no withdrawal. For the first time, communities will be able to access independent expert advice and support. We can expect intensive education campaigns. But it is not stated who that “community” includes and how much support of what type is needed. As always, the agreement of the people is potentially the Achilles heel of the entire process.  Fourth – and most contentious – is the proposal that communities who volunteer will receive £1m per year for five years of exploration, followed by £2.5m per year for [up to 15 years](https://www.flickr.com/photos/deccgovuk/14705016331/in/set-72157645405696080/ of further investigations and design, which is likely to include additional surveys or drilling. On top of that potential £40m, there will be “substantial” benefits during construction to an accepting community.  The project will undoubtedly be extremely large.  Analogies with Crossrail are appropriate, which is estimated to cost more than £15 billion at today’s prices. Thousands of jobs will be created over 10-20 years of construction. But the facility will only require a handful of long term employees for day-to-day operation. Finally, as another major shift, the facility will be permanently sealed after 100 years of operation for even greater security. Potentially the most significant statement of all comes from the secretary of state for energy and climate change, Ed Davey, stating that arrangements for waste disposal have to be in place before planning consent will be given for new nuclear power stations. Does this mean that one or more sites need to be specifically identified before construction can start on the new nuclear reactors planned at Hinkley Point and elsewhere?  If formal discussions with new volunteers do not even start until 2016, and could conclude as late as 2030 – by which time Hinkley Point C should be generating power – that seems impossible. Perhaps ministers of the future be satisfied merely to know that the UK “has a plan”? If the past is any guide to the future, relying on such a plan didn’t help to find a nuclear mausoleum in 1978, 1996, or 2012."
"Climate change is arguably the single most important issue facing the planet over the next century. Today’s school students will be the ones who must reverse it, cope with it as best they can, or experience its consequences. We know students feel engaged and passionate about global issues like this, now adults have to work much harder to make school courses relevant and real. Challenging students to tackle “live” data and watch climate change in action is one way to approach this. To take one example, let’s look at the natural “cycles” of two key substances: water and carbon. Both are absolutely crucial to how the natural world works, and both have been badly disrupted by human activity. Understanding climate change is impossible without grasping what has happened to the water and carbon cycles, but unfortunately, as you may remember from school yourself, these are not the most thrilling topics. Globally, the two collide at the cellular level in tiny pores in the leaves of plants through which CO₂ enters before processing by photosynthesis. Importantly, while CO₂ enters the plant, water leaves in transpiration. As CO₂ levels rise, plants can choose whether to take advantage of this artificial, human-driven fertilisation and fix more carbon (grow faster), or reduce water loss and deal with drought more efficiently. In contrast, these naturally intermeshed carbon and water cycles do not collide in the English and Welsh school system. There, pupils are taught that carbon dioxide is taken into plants and feeds the carbon cycle while, entirely separately, in the water cycle, transpiration injects water into the atmosphere, affecting cloud cover and so the balance between cooling and warming. Climate change cannot be understood without an appreciation of how human actions are affecting all sorts of interlocking natural cycles, and colleagues and I hope that, as researchers, we can help school teachers to join these dots in the classroom. It’s true that talking about carbon and water cycles can seem abstract and disconnected from the exciting and forward-looking parts of the curriculum such as medical advances or molecular biology. But colleagues and I want to change that perception by engaging 15 and 16-year-old students in climate change research, enabling them to use real data to understand these cycles and how they relate to climate change. The materials are being rolled out this term in a pilot programme at a school linked to the University of Birmingham. These approaches are particularly timely. The new high school science courses in England and Wales (known as GCSEs) have a much greater emphasis on applying knowledge and ideas to unfamiliar scenarios as well as analysing and evaluating information than previous courses. One of the aims of the biology course, for instance, is to develop curiosity in the natural world and to encourage pupils to appreciate the relevance of biology to their everyday lives. These skills can be difficult to achieve in a classroom – especially as teachers are under pressure to just “get through” a seemingly ever-growing body of knowledge that pupils must understand and remember. Using materials we have developed, pupils have been able – in just one lesson – to make links between topics such as environmental change and the cycling of materials, or plant cell organisation and photosynthesis and transpiration. At the same time they are also getting to grips with large data sets and experimental design on a grand and unfamiliar scale. This data comes from Birmingham’s FACE project, which is fertilising a forest of mature oak trees with extra carbon dioxide on a huge scale. For at least the next decade, rings of 35m-high pylons will feed the trees extra CO₂ to see how such forests might respond to a carbon-rich atmosphere.  At this point, we have some information from lab studies but real experiments out in the forests are rare and have never looked at UK oak forests. What is clear is that the global consequences of climate change will be mediated through trees, as they balance the uptake of CO₂ with water loss and in turn directly affect any further global warming.  This is not an ivory tower experiment – the forest is wired and connected to the digital world, and pupils will be able to explore the experiment themselves online and monitor its progress in real time. Data sets can be downloaded and used in the classroom to directly address the water and carbon cycle aspect of the syllabus. Direct participation with this high tech research enriches both the school experience and the science itself in a two-way process. Trees get bigger as photosynthesis takes carbon from the atmosphere, but they shrink as water is lost through the open pores. So over months and years tree trunks get thicker, but they shrink and expand over a daily cycle. Here in a single organism the consequences of elevated CO₂ for carbon and water cycles are played out and captured in a single lesson plan. We hope that this sort of interactive, citizen science approach will revolutionise the study of ecosystems and climate change. This is an opportunity for students not just to learn about how science is progressing but to directly participate – ultimately, we aim not just to transmit knowledge, but to inspire action."
"
Share this...FacebookTwitterGerman flagship ARD public television broadcast a half hour show that was actually fully devoted to poking fun at the now religious climate movement in Germany.The show even ended with a warning that the movement may be getting out of hand. Okay, it was aired at 11.15 p.m., long after most Germans had gone to bed. Better than nothing, I guess.
The video at the above link is in German and I can’t post here because of copyright. But if you understand German, it is worth watching.
The show features Henryk Broder and Hamed Abdel-Samad journeying through Germany where they stop at various climate protection events and projects, etc. and at times speak to various climate protection leaders. Although the pair pokes fun at the people they interview, one gets a sense of just how radical and fanatic the movement has become.
In Germany, man-made climate change is long established and institutionalised. Germany is well into its mission to rescue the planet. The media, institutes, information centres, schools, government offices, tax laws, economic central planning programs, subsidies, etc. are all in place and geared at rescuing the climate. Everyone is being advised today to do their part to protect the climate.
At the 17:30 mark Broder and Abdel-Samad stop by and surprise German Green Party Chief Claudia Roth, who clearly doesn’t want to talk to Broder. Eventually she does answer Broder’s question after he presses her to tell us what she thinks sustainability means, about the eco-dictatorship and forcing everybody to live that way. Roth says:
“I don’t want someone going around telling everybody ‘you have to do this, do that, do this. I just want the citizens to have a “human literacy” so that they can decide for themselves. I don’t want to act with bans, but I can see limits are necessary, so if you’ll excuse me I really have to go.”
What she means by “human literacy” is that she wants people to believe the fairy tale and that they behave the way she wants them to. And with “limits” she means that if people go beyond them, then it’ll be very uncomfortable. The green person that Broder is left talking with after Roth is chauffeured)away in a luxury car reveals more about how the green vision looks:
And if we succeed to live well with another, cultural, ecological and technological standard, for example mobility will change, agriculture and nutrition will change, living, that is everything will change …the Great Transformation)’


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Broder replies:
In the end a big beautiful world?”
Broder’s and Abdel-Samad’s journey takes them to the Bavarian Ministry of Environment, where a director tells them about the virtues of separating your garbage, installing solar panels and turning off the lights.
Next stop is a school in Hamburg with a project to climatically brainwash its kids. For example a teacher tells us how children are taught not to open the oven while the pizza is baking because it’s bad for the climate. Throughout the journey, Broder and Abdel-Samad’s often pose the question: “What do we do with those that don’t cooperate?”. The answer is always in a nutshell (paraphrasing):
“We have to convince them to do so…otherwise we’ll make their lives hell.”
Even a young pupil is shown telling us that people who don’t go along have to be made to feel embarrassed and guilty about it. One zealously committed teacher proudly says they are all “acting as role models for the rest of the world”.
At the 25-minute mark, see how it all works in the classroom and how the kids are allled to think it’s all going to lead to utopia. Indeed kids are assigned to run around the school to check the heaters, lights, ventilation, windows, etc.  Eco-absurdity and climate protection madness that has indeed gripped Germany.
Broder appropriately ends with an ominous comment as a warning:
I really believe that in every idea to make a better world, lurks a hidden threat.”
Share this...FacebookTwitter "
"The Green New Deal has broadened imaginations worldwide on the subject of climate change, encouraging people to consider what action to tackle it could do for society. US congresswoman Alexandria Ocasio-Cortez announced the Green New Deal resolution in February 2019, calling for a rapid transition to net zero greenhouse gas emissions, a massive investment in infrastructure and financial redistribution.  While the project would attempt to halt further warming, it would also counter inequality and compensate losers from the energy transition, such as workers in carbon-intensive industries such as coal mining.  It’s already helped wrest the political agenda in the US from the regressive policies and scandals of the Trump administration, and has gained bipartisan support among US voters, despite right-wing pundits denouncing it as a communist plot. The Green New Deal borrows its name and ethos from the New Deal – introduced in the 1930s by then US president Franklin D. Roosevelt to kickstart an economy crippled by the Great Depression. But are strategies which echo the needs of the 1930s and 1940s – ending the Depression and defeating Nazism – suitable for the rapid transition from fossil fuels that defines our needs in the early 21st century?  Can any strategy which relies on historical analogies be adapted to the current climate emergency? The Green New Deal’s proposed investment in public infrastructure and focus on inequality mirrors the original aims of the New Deal, but economic transformation will look very different under a Green New Deal. Whereas Roosevelt’s New Deal aimed to grow the economy, its modern equivalent entails shrinking many economic activities currently central to the economy’s operations. Another way of looking at this is that the original New Deal spurred a massive increase in greenhouse gas emissions. By generating huge public investment in roads and power stations, as well as redistributing wealth through the emerging welfare state, it set the stage for what some call the “great acceleration” in greenhouse gas emissions during and after World War II.  In the US, military build-up was central to this early on, but then it was sustained by the expansion of consumption after the war – most directly by the shift to mass car ownership and urban sprawl that “locked in” high fossil energy use, not only in transport but in housing. The Green New Deal therefore contains a basic contradiction that anyone pursuing it will have to wrestle with as it develops. Many of the measures proposed – such as investing in infrastructure and spreading wealth more evenly – will intrinsically work in tension with efforts to decarbonise the economy.  They create dynamics that increase energy use at the same time as other parts of the Green New Deal are trying to reduce it. For example, building infrastructure such as new road networks will both create demand for carbon-intensive cement manufacture and opportunities for more people to travel by car.  To reach net zero emissions by sometime early in the second half of the 21st century, as the Paris Agreement and the IPCC Special Report on 1.5°C state we must, the global economy has to decarbonise by at least 3% per year. In rich countries such as the US, this needs to happen more rapidly so that poorer countries, which have contributed less overall to global warming, have more time to adapt.  


      Read more:
      Carbon emissions: our research shows a decade of steady decline across Europe and the US


 The targets in the Green New Deal are consistent with this sort of time-frame for decarbonising the global economy. But, even if wealthy countries like the US “only” have to achieve 3% cuts per year, as the economy grows by – say – 2%, then in effect the country has to cut emissions by around 5% per year relative to the growing size of the economy. To illustrate the scale of this challenge, historically, emissions have declined relative to GDP by only about 1% per year, in the aftermath of the 2008 recession. So the challenge is enormous. But of course, the effect of much of the Green New Deal – to invest in infrastructure, to redistribute income – will be to generate significant economic growth. Indeed, this is the point – to get the US economy out of its present stagnation. But it’s hard to see how this will be done without generating new sources of carbon emissions – more housing, more cars and more consumption generally. Herein lies the tension that will recur through the life of the Green New Deal, even if it gets through the immediate quagmire of US politics. Its supporters will have to manage this tension, even though the vast majority of the US left and environmental movement are behind it.  If one imperative is to build new infrastructure to get the US economy going, how much of this will really do more than pay lip service to the energy system transformation in practice? The “Green” in Green New Deal demands that all new infrastructure built is effectively carbon neutral. Even new transit infrastructure, for example, would have to be entirely electric, at the same time as that electricity system is supposed to rapidly abandon coal and then natural gas. It’s easy to imagine which will win when that tension works its way through the political process. It’s not that the Green New Deal isn’t worth pursuing – it’s an extremely promising development. It’s just important to remember Naomi Klein’s invocation that “this changes everything” – dealing with climate change is unlikely to lend itself to off-the-shelf solutions from an earlier age."
"
Share this...FacebookTwitterAwhile back Stefan Rahmstorf took a few readings off the coast of North Carolina and concluded sea levels were rising faster than at any time since Jesus had walked on water.
Well, Der Spiegel noticed it 63 years ago in 1948 and wrote a piece called “America will become less”. Obviously Rahmstorf had only recycled an old story from Der Spiegel when he wrote that paper. Needless to say his remix of that story was promptly debunked (J. R. Houston and R. G. Dean, Journal of Coastal Research 27 (4), 788-790 (2011).
In 1948, just after the 1910-1945 warm period, Mr. H. A. Marmer of the US Coast and Geodetic Survey of the North American Coastal Observation Bureau “reported that the sea level along the east coast of the USA was rising 6 mm/ year” and posed the question:
Is the country sinking or is sea level rising?  It is still not decided. The uniformity of change along the 2000-km coastline allows us to conclude that the sea is partly responsible.”
Der Spiegel then mentioned that sea level rise from sediment build-up from rivers discharging into the ocean adds 3-4 mm of sea level rise per century and writes:
A big part of sea level rise is the melting of ice. During the last ice age there was approximately 40 million cubic kilometers more ice on land than today. The melting of ice made the sea level rise about 100 meters over the the last 10-20 thousand years. And again a strengthened melting of glaciers and polar inland ice like a few decades ago could be responsible for the rising sea level along the American east coast.”
Der Spiegel also mentioned that Scandinavia was rising while the North German coast was sinking at a rate of 20 cm per century. Along North America Der Spiegel wrote that measurements had been taken since 1895, and that sea level in 1948 was accelerating:
…in the time until 1930 the coast sank at a rate that was only one seventh of the current rate. Then it began, from Florida in the south, to Maine in the north, a sinking of 6 mm per year. If that will continue, no one knows.”
As Der Spiegel was writing about melting polar caps and rising sea levels, it was only 26 years later in 1974 that they warned us of a new coming ice age.
Share this...FacebookTwitter "
"

Predictably, our European friends spent July 16 berating the United States for its refusal to go along with the infamous Kyoto Protocol on global warming. As most people know, Kyoto is an international agreement to reduce the emissions of greenhouse gases that, in reality, has no detectable influence on climate and costs a fortune. This reality notwithstanding, another semi‐​annual “Conference of the Parties” to Kyoto is taking place this week and next in Bonn, Germany. The last one was at The Hague, in the Netherlands, last November.



Chief among Monday’s berators was Juergen Trittin, Germany’s Environment Minister, who thundered that, “We cannot allow the country with the biggest emissions of greenhouse gases to escape responsibility.” That’s us, because we have the world’s biggest economy (which just happens to also be one of the most energy‐​efficient).



So who killed Kyoto? If any one person will be fingered by history, it will be Trittin himself. If any group of nations is to be singled out, it will be the EU, which has been out of step with the rest of the world on Kyoto since day one.



Kyoto’s last best chance at adoption was last November, when the same people who are now berating us in Bonn met at The Hague, two weeks after Election Day. The Clinton‐​Gore team, struggling to find some economically defensible way of meeting Kyoto’s totally unrealistic target–which would require a 33 percent reduction in total U.S. emissions (read: energy use)–proposed that we meet half of that target by planting trees, building up the organic content of our soils, and selling/​giving clean power production technology to polluting, poor (the two are highly correlated) nations.



Jurgen Trittin and the French Environment Minister, Dominique Voynet, said no. To them, speaking for the EU, the United States had to meet Kyoto by directly reducing energy use. Here they proved to even many radical American greens that Kyoto has nothing to do with climate and everything to do with hatred for the United States, very chic these days in Berlin, Paris and London.



So, the United States then proposed that it would only salt away 40 percent of its emissions in trees. No, said Trittin, Voynet and the EU. 30 percent? 20 percent? No. No. President Clinton gained the intercession of his friend, British PM Tony Blair. Voynet then turned on him, saying that he “had conceded too much to America.”



In disgust, the U.S. negotiation team packed its bags and left. As it later admitted to USA Today, the final proposals would have caused grave economic damage. On the way out, EU security guards sat on their hands, as green demonstrators assaulted U.S. negotiator Frank Loy with a pie in the face on world television.



Surely the EU knew that, despite the November turmoil, there was a pretty good chance George Bush was going to be the next president. And not long after this happened, National Security Advisor Condoleeza Rice announced, “Kyoto is dead.”



For that, we have been subject to incessant rants about the United States being a “pariah” and a “rogue state.” So who’s the pariah here? Kyoto doesn’t apply to China, the world’s most populous nation. Nor India, the second largest. Are people in Russia clamoring for its adoption? What about Indonesia, Pakistan, the Middle East? Africa has real fish to fry, like AIDS.



It is clear that the vast majority of the world’s citizens either aren’t bound by Kyoto or don’t care anyway. The United States is merely siding with the majority against a vocal and radical European minority that supports an ineffectual and expensive treaty, which they say can only be implemented in a fashion that will cause us (and, ultimately, the rest of the world) grave harm. There is no way the U.S. Senate will ratify it, anyway.



Kyoto always was sickly. At its inception, in December 1997, the Europeans pressed for impossibly large emission reductions, agreeing to a cut to 8 percent below 1990 levels for a five‐​year period centered around 2010. At Kyoto in 1997, as in The Hague in 2000, the EU proved incapable of standing up to its most radical green elements. Nor has the EU learned from these mistakes. On July 16 in Bonn the 15 EU leaders issued a joint declaration promising to fulfill their treaty commitments, adding one final farce to this tragic comedy. Why anyone would engage in a failed effort to do something that everyone knows wouldn’t even have a measurable effect on global climate remains a mystery.



So, who killed Kyoto? Not us. Bush was merely the coroner. Jurgen Trittin, now railing about holding the United States “responsible” for his own irresponsibility, was the perpetrator, and the EU, wildly out of step with the rest of the world, was the accomplice. But they’re Not Guilty, by reason of insanity.
"
"**A hot tub party host who told officers he ""didn't believe"" in Covid-19 laws has been ordered to appear in court, police said.**
Nottinghamshire Police were called to reports of a party in Poplar Grove, Forest Town, on Saturday.
When they arrived, they found people believed to be from up to five different households mixing.
The host was reported for summons to court and six people at the party were dispersed.
Police added the 32-year-old had refused to give his details and when they tried to explain the law, he told them he ""didn't believe in the Covid-19 legislation and continued to be obstructive"".
The force also broke up a ""large party"" involving 17 people at a house on Layton Avenue, Mansfield, on Monday.
The homeowner was fined and the guests were asked to leave.
England is currently in a national lockdown with strict rules on household mixing.
Latest NHS data showed Sherwood Forest Hospitals NHS Trust, which runs King's Mill hospital in the north of the county, had 92 Covid-positive patients in its beds last week, 12 of whom were on ventilators.
Assistant chief constable Steve Cooper said: ""I find it pretty astonishing that anyone would think it is OK to behave in this sort of way when there has been clear guidance given around not mixing households.
""We are still very much in a lockdown as a nation and we need to keep abiding by these laws.""
_Follow BBC East Midlands on_Facebook _,_Twitter _, or_Instagram _. Send your story ideas to_eastmidsnews@bbc.co.uk _._"
"
The sun has seen a resurgence of activity in December, with a number of cycle 24 sunspots being seen. The latest is group 1039 seen below:

2009 is ending with a flurry of sunspots. Indeed, if sunspot                      1039 holds together just one more day (prediction: it will),                      the month of December will accumulate a total of 22 spotted                      days and the final tally for the year will look like this:                   From Spaceweather.com

The dark line is a linear least-squares fit to the data.                      If the trend continues exactly as shown (prediction: it won’t),                      sunspots will become a non-stop daily occurance no later than                      February 2011. Blank suns would cease and solar minimum would                      be over.
If the past two years have taught us anything, however, it                      is that the sun can be tricky and unpredictable.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9042f07b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"In an experiment for a forthcoming National Geographic television show Mind Over Masses, a pavement was divided into lanes, separating those that are walking from those walking while talking on mobile phones.  This may be a breezy way of showing the irritation some feel about how others use mobile phones. But the proposed solution is really just another form of control, when what we need is not a test of people’s obedience, but a reassessment of how we interact with each other in public.  Civil society, to remain civil, requires an agreed framework for behaviour in public. You might call it etiquette, although that word has often aroused suspicions, with some seeing it as an infringement of their personal liberties. Justifiably so, if you look at traditional concepts such as deferring to rank, treating women as the “weaker sex”, bowing and scraping. But etiquette based on mutual recognition of fundamental human equality can make civic life more free, and more pleasant, for everyone.  Simple street etiquette could improve safety and comfort for all, and save millions in signage, policing, separation of traffic, and various other complex and expensive means of regulating public space. Imagine uncluttered public landscapes where people are comfortable and can move easily. This is possible only by returning to first principles of civil society, and not resorting to mechanisms of control. The common law, after all, is rooted in ground-up custom, not top-down control. The alternative is a society based on hierarchy, where those accustomed to projecting an air of entitlement claim the largest share of public space. Or one based upon control and obedience, where people are told unequivocally where to be and are punished for stepping out of bounds, or both. The clearest examples are societies run by organised crime such as the mafia. Gangsters opportunistically thrive on mistrust, on inflexible hierarchies, and on a breakdown of civil society.  Democracy is the practice of resisting these forms of hierarchy and of concentration of power, wealth and cronyism – and the violence that often accompanies them. Our own society and institutions are not entirely free of the tendency towards using force and coercion, and it’s important to watch for it and resist it when it arises. But this sort of power play happens on a miniature scale on many streets everywhere in the UK every day. In practice, people walk to the left or right as they will, with many who are able to exercise their dominance preferring the inside of the pavement away from traffic. Some try to keep left as a rule, some keep right as a rule and others duck and dive to find the path of least resistance. It is either the most aggressive pedestrian or the person who can project the greatest air of entitlement that wins.  All of this before even taking into account the oblivious “meanderthal” – of which the finest example is surely the semi-aware, distracted, walking mobile phone user. It is both rude and undemocratic to walk while on the phone; it forces others to take full responsibility for both their comfort and safety and that of others, while the screen-bound meanderthal acts as if they are above such responsibility. Mistrust and micro-aggressions in public space are symptomatic of instability in the larger society. One small and simple measure that could change the pedestrian experience would be if people agreed to keep left on footways as a general rule. This is already loosely called for within the Highway Code, which stipulates that one should “avoid being next to the kerb with your back to traffic”.  This makes good sense, but is impossible for pedestrians to heed when cities are filled with exceptions such as one-way streets, signals, and other contrary signage. A simple, blanket rule could be agreed while we work on re-evaluating the rest. Shared space schemes, proven to make streets safer, seek to remove street clutter and rely on eye contact, interaction, and etiquette.  It’s common in the design of cities to use evidence-based studies and computer models to try to predict behaviour so that it may be accommodated or controlled. This does us a disservice – what and who is our public realm for, who does it serve, if not for our democratic communities? We must first ask how our public landscapes serve our highest common ideals, and then work the rest out from there. "
"
Steve McIntyre published an update tonight showing the last 200 years of the Yamal tree ring data versus the archived CRU tree ring data used to make the famous hockey stick. For those just joining us, see the story here.
First here’s the before an after at millennial scale.
Steve McIntyre writes:
The next graphic compares the RCS chronologies from the two slightly different data sets: red – the RCS chronology calculated from the CRU archive (with the 12 picked cores); black – the RCS chronology calculated using the Schweingruber Yamal sample of living trees instead of the 12 picked trees used in the CRU archive. The difference is breathtaking.

Figure 2. A comparison of Yamal RCS chronologies. red – as archived with 12 picked cores; black – including Schweingruber’s Khadyta River, Yamal (russ035w) archive and excluding 12 picked cores. Both smoothed with 21-year gaussian smooth. y-axis is in dimensionless chronology units centered on 1 (as are subsequent graphs (but represent age-adjusted ring width).
Now lets have a look at the data for the last 200 years where that hockey stick lives (and dies):

Steve writes:
Here is a comparison of the Briffa chronology of the spaghetti graphs (red) versus the “SChweingruber” variation i.e. using russ035w instead of 12 recent of 252 CRU cores, leaving 240 unchanged. (The red curve here is the archived CRU chronology, which varies slightly from my emulation of the RCS chronology.)

Viva la difference!
Still broken.
h/t to Mosh


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e933c9833',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"The summer just finished was Australia’s second-hottest on record, with the temperature 1.88C above average, the Bureau of Meteorology says. The only hotter summer on record was the previous year, which was 2.14C above average. Temperatures this summer were above average across almost the entire country. Dr Blair Trewin, a senior climatologist with the bureau, said the hot summer, which was marked by the unprecedented bushfire crisis that devastated communities and wildlife in much of the country, was part of a long-term warming trend that had seen the country warm by 1.4C since 1910. Most of that warming has come since 1950. Trewin said: “That tells us the baseline is higher, and with that you have a higher risk of high extremes like we have seen in the past two summers. Our baseline expectation now is for warmer than average summers, and other seasons more generally.” The heat records are relative to the long-term average for the years between 1961 and 1990. The bureau’s summer report came as its officials told a Senate estimates hearing in Canberra that Australia is heating more rapidly than the global average. Karl Braganza, the head of climate monitoring, was asked how much Australia was projected to warm, given a scientific analysis involving the World Meteorological Organisation last year found average global temperatures were expected to rise between 2.9C and 3.4C by 2100 under commitments put forward by national governments as part of the Paris agreement. Braganza said the increase in Australia would be expected to be “closer to 4C” heating than the global average under that scenario, assuming countries did not do more than promised in Paris. Adam Bandt, the Greens leader, said the government was undertaking no planning for the possibility of 4C warming in Australia. “Australia saw this horrific bushfire season with just over one degree of warming. We’re hurtling towards 4C and it’s only going to get worse from here,” he said. This summer saw many more heat records broken than cool records. The bureau data shows 43 sites in NSW broke high temperature records but only five sites saw record lows. In Queensland, there were 10 heat records broken across different monitoring stations, but only one record for cool weather. The 2018-19 summer also saw below average rainfall across the country, with western NSW, south-western Queensland and the Top End particularly dry. Some areas, including Western Australia’s west coast, and parts of the east coast, saw above average rainfall. The second hottest summer comes after the bureau declared 2019  the hottest and driest year on record. Trewin said the summer could be characterised as a season “of two halves”. “We had December and the first few days of January, which was extremely hot almost nationwide, and extremely dry. Those heat extremes in December and early January were quite exceptional,” he said. That exceptional heat included 17 and 18 December, when on two consecutive days Australia recorded its hottest day on record. The national average temperature on 18 December was 41.9C – one whole degree hotter than the previous day. December 2019 also delivered the worst conditions for bushfires on a record going back to 1950. Several records were broken for warmest summer nights in NSW, Victoria and Queensland. On 1 February, nighttime temperatures in the rural area of Condobolin, west of Dubbo, NSW, did not drop below 34.7C. Aside from climate change, there were natural drivers of the heat in the early part of the summer. Trewin said in December and early January, the weather patterns were influenced by a strong Indian Ocean Dipole that had dragged moisture away from the continent and a “strongly negative Southern Annular Mode”. But as both these systems moved into a neutral phase, the rest of January and February was less extreme. Rainfall for January and February was slightly above average, Trewin said, but the extreme dry of December meant that across the three months rainfall was below average. But 10 February still delivered record rainfall for some parts of NSW. At Taralga, 100km west of Wollongong, the town had its wettest summer day on record with 197mm beating the previous record of 130mm, set in 1885. While the bureau’s report does not cover ocean temperatures, the hot summer has also seen the build-up of heat stress on the Great Barrier Reef, with scientists fearing a third mass coral bleaching event in the past five years if temperatures don’t fall in the next few days."
"
Share this...FacebookTwitterSimon Kuper of the Financial Times recently wrote a piece called Climate change: who cares any more?, which clearly reveals the frustration and growing resignation among warmists and how the climate cause is irrevocably ruined.
As the hoax of climate catastrophe becomes increasingly exposed, it is being taken far less seriously today than back at the peak of the scare in 2007. For example Kuper describes how he felt when leaving a climate conference in Britain in 2007:
I left feeling that if you were running a country like Britain in 2007, you probably thought climate change was the single overriding issue. Terrorism, immigration and even the economy were details by comparison.”
My how the mightiest of scares have fallen. If that doesn’t confirm that Gore’s movement is lost, then nothing will. In his opening sentence Kuper himself admits to having given up on protecting the climate, realising it is a senseless endeavour:
When someone offered me a trip to India, I said, “Definitely.” A couple of years ago I’d have fretted about the carbon emissions. But like almost everyone else, I have given up trying to prevent climate change.”
Kuper however blames the failure of the movement on the bad economy and human resignation and apathy, claiming people are more worried about their own prosperity. But the reality is that the science behind climate catastrophe has fallen apart, and this is being made known to the public. The public is realising that all the hype over climate change was mostly a hoax perpetuated by a few select greedy interest groups out to make a ton of money. That’s why the public is turned off over the climate issue. It just isn’t a real threat any more.
Now that the scientific data is getting analysed, people are realising it’s no longer necessary to worry about climate when deciding the direction of energy policy – because coal and other fossil fuels simply don’t have the destructive impact on the climate that was once hypothesized. The data simply doesn’t show it. Indeed while coal consumption globally increased 30% over the last 10 years, global temperatures have actually dropped.


The blue line shows skyrocketing global coal use, yet global temperatures have fallen.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: Review of World Energy
So why would any country do something as stupid as cut back on cheap coal during cooling times? Especially when it would only lead to lots of people freezing to death.
Kuper complains and appears baffled by the fact that the media have dramatically reduced their reporting on climate change. Come on Kuper, admit it: It’s not because people have resigned, it’s because people have woken up, and the phony climate catastrophe has since become a non-issue. Few are interested in it, and
Al Gore’s 24 Hours of Reality clearly demonstrated the folly of crying wolf for the ten thousandth time. Now it’s all falling on deaf ears. Fewer people than ever now believe all the climate catastrophe hogwash. Recall how everyone saw through the whole charade and all the bias behind the reporting of Hurricane Irene, which exposed the desperation of the movement. I ask: just how stupid do you think the public is, Mr. Kuper?
It is over.
A better question is: How stupid can one possibly be not to see the man-made climate change charade, and to continue believing the obvious hoax of global climate catastrophe?
No, it is not resignation by the public. It is an awakening. Rich and poor countries alike have opted to continue to do what they have been doing for thousands of years: ADAPT. As sea levels rise their usual 1 or 2 or 3 mm a year, people will simply take one or two or three steps back each century. That’s what they’ve always done as climate has always changed in the past.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterLubos Motl, I am pleased and honoured to say, has also written a piece of his own about Herr Masterplan Hans Schellnhuber. Boy, I thought I was being tough on the guy, Lubos takes it right to him. I found his post so enjoyable that I had to feature it here. He adds a lot more to what I’ve said, so do read it.
=======================================
Herr Schellnhuber has a master plan 
By Lubos Motl
Pierre Goselin is discussing a remarkable interview with the top German climate ideologue in Spiegel:
We Are Looting the Past and Future to Feed the Present (English)
Joachim Schellnhuber, a doomsday crackpot who calls himself a physicist (the inflation in using this term has been significant), starts with the assertion that nuclear power plants should be ready for infinitely strong earthquakes and economics and economy shouldn’t play any role because they’re “crazy logics”…
Continue reading…
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThis morning I found an e-mail from Vermont Senator Patrick Leahy in my mailbox. It was just a mass mailing asking readers to mobilize against the construction of a pipeline and warning of climate change. I’m a native of Vermont.
One last look at Vermont before the turbines come. (Photo credit: Creative Commons Attribution-Share Alike 3.0 Unported, 2.5 Generic, 2.0 Generic and 1.0 Generic license.
I remember visiting Washington back in 1982 and actually dropping by his Washington office out of the blue (back then you could do that) but he was out to lunch, or something.
Anyway I thought I’d send him a reply:
Dear Senator Leahy,
With all due respect, climate always changes. Just 12,000 years ago the Green Mountain State was buried under at least a mile of ice. Thanks to climate change, it isn’t so today.
Concerning today’s climate change, global temperature hasn’t risen in over 10 years, and many scientists say it won’t rise for another 30 or 40 years because of cyclic solar and oceanic activity. You haven’t heard?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




To understand climate, it first helps to know that it is a heck of a lot more complicated than just the straight line equation: CO2 regulates the climate and weather. To tell others that it is that simple is a disservice to the public. Why not talk to scientists from the other side? There are many. You haven’t done so yet? Seems to me it would be the responsible thing to do.
It really is time for you to retire, Mr. Leahy, and to let fresh ideas and open minds back into the Senate. Vermont thanks you for your service, but you’ve been in the Senate far too long.
Sincerely,
Pierre Gosselin
Native of Vermont
PS: I made a trip up to Crystal Lake last summer – nice wind turbines up there on the surrounding mountains. And I hear such a landscape-beautification is in the works for Lowell Mountain too.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGermany may be finally realising that there are much more pressing problems than the phony global warming scare, like the imminent financial collapse of western governments, to name one.Indeed the Earth has not warmed in 13 years, tropical storm activity is near record lows. Atlantic hurricanes hitting the US coast? Aint seen a real one in 4 years. Global temperature? A whopping twenty hundredths above the (cold period) 1960-1990 normal, and falling.
Germany’s Federal Environment Minister under Chancellor Angela Merkel, green wonder boy Norbert Röttgen (CDU) is also realising the world is not playing along with the Great Climate Hoax, and as a result has dumped cold water on expectations for the UN climate conference in Durban.
The warmist German-language CO2 Handel reports here:
‘Durban will produce less than what’s necessary,’ said Röttgen at a conference of the German Association of Industry (BDI) in Berlin on Tuesday.
Röttgen could hardly conceal his disdain for the Obama Adminstration’s efforts, calling the US “a stonewalling obstructionist”. Röttgen added:
«Us Europeans alone can’t save the world by ourselves, instead we need allies.”
In Germany blaming all the world’s ills on the USA is all that’s left in its political playbook. So all the talk is now about forming a” coalition of the willing” to rescue the climate.
Why do I get the feeling this is less about saving the world, and more about saving a movement and relevance? Let’s call it what it is: the coalition of the duped and desperate. Meanwhile BDI boss Markus Kerber trumpeted on behalf of the hapless Röttgen:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




German industry calls for a legally binding international treaty at the upcoming World Climate Conference in Durban. The debt crisis must not lead to investments in climate protection measures to go without financing”.
Yeah right, just start printing money. Now you know why Europe is in a financial mess. ..with advice like Kerber’s.
The whole must not be allowed to degenerate into a conference circus.”
Too late. This has been a $120 billion circus and it’s been going on for 2o years. The greatest circus in the history of man. Of course Kerber is joyfully singing the climate song because he knows this circus is about to end. You can be pretty sure that German industry bosses will be relieved once Durban is finished.
Saved again from another dumb treaty.
UPDATE – Die Welt here on German industry bosses:
Indeed from the data it is clear: Only 16% of the companies intend to reduce their emissions. And those that do plan to reduce are planning a reduction of a mere 1.3% – not per year, but in total!”
Someone needs to whisper this to Kerber.
 
Share this...FacebookTwitter "
"The Great Barrier Reef is still at risk of a widespread outbreak of coral bleaching despite a cyclone to the far west helping to temporarily cool stressed corals, according to US and Australian science agencies. Clearer skies, weak tides and above-average ocean temperatures are combining to create stressful conditions for corals along much of the world’s largest reef system.  Concerns are rising that southern parts of the reef that escaped major bleaching in 2016 and 2017 may be hit in the coming weeks unless weather conditions change. Australia’s marine science agency said on Thursday it was set to deploy a second underwater glider to monitor water temperatures in the central and southern areas of the reef. Townsville-based Dr William Skirving, of the US government’s National Oceanic and Atmospheric Administration’s (Nooa) Coral Reef Watch, told Guardian Australia: “Everything seems to be lining up. “The clouds are clearing and we can see the heat is still high and we know the tides will provide less and less mixing. Everything is falling in line for a short sharp bleaching event but the severity of it is difficult to predict.” Corals bleach when they sit in abnormally warm water for too long. The algae that live in the corals and provide much of its nutrients and colour leave their host, leaving a visible white skeleton behind. Corals can recover from mild bleaching. Coral reef systems have been long predicted to be susceptible to global warming. The United Nations science panel has said that even at global heating of 1.2C “most available evidence” showed that in the tropics “coral-dominated ecosystems will be non-existent at this temperature or higher”. Dr Katharina Fabricius, a senior principal research scientist at the Australian Institute of Marine Science, said: “The reef is still on a knife edge. The future of the reef now seems to depend on flukes of the weather. That is a big concern. “This is exactly what scientists have been predicting, with an increase in the frequency of bleaching events.” Tropical Cyclone Esther crossed the coast in the Gulf of Carpentaria on Monday morning and while the eye of the cyclone was hundreds of kilometres west, the weather system dragged clouds over the reef, helping to cool temperatures slightly. The Great Barrier Reef Marine Park Authority said late Thursday weather conditions over the next few weeks would “play a key role in determining outcomes for the Reef this summer”. Coral bleaching has been occurring on parts of the reef where thermal stress has accumulated the most over summer. The authority said in-shore reefs in the far north had already experienced “widespread bleaching affecting most coral types” but there had been minimal bleaching on outer reefs. So far, only patchy and isolated bleaching had been reported over large parts of the reef, the authority said. Skirving said: “The clouds helped because they cooled things off, but there’s still a lot of heat in the system. It’s hot enough for the corals to be accumulating stress.” He said the mass bleaching event of 1998 across the reef had taken place over the course of just one week. Weak tides for the next week meant there was less mixing of the warmer surface waters with deeper, cooler water. Skirving said if Cyclone Esther had not formed, “it would be already bleaching up and down the reef”. In updated analysis, compiled on Wednesday, Skirving wrote that Noaa’s tool for predicting heat stress on corals showed there was an 80% chance that most of the reef would experience some bleaching between 1 March and 8 March. For southern parts, the probability was even higher. Bleaching at Magnetic Island, inshore GBR, Feb 26th 2020.About 65% of colonies showing stress, with some quite unusual patterns in the bleaching at present.Next 2 weeks will be critical in determining mortality schedules. #coralbleaching2020 #GreatBarrierReef pic.twitter.com/Tz1vqfI40t Dr Sophie Dove, an associate professor at the University of Queensland’s Coral Reef Ecosystems Lab, said it was important to differentiate mild bleaching from severe events, where corals appeared starkly white. “If that whiteness persists then we are likely to see mortality,” she said, adding it was too early to say if the current conditions would have a serious impact on the reef. The Australian Institute of Marine Science (Aims) has been monitoring satellite data, their own weather stations and a network of more than 170 electronic temperature loggers to check temperatures. Craig Steinberg, an oceanographer at Aims, told Guardian Australia that two autonomous underwater gliders were being used to record water temperatures from the surface to a depth of 200 metres. He said after a brief cooling period, “we are now back into a warming phase” in the southern parts of the reef. Temperatures tend to peak on the reef in mid-March. Prof Terry Hughes, director of the ARC Centre of Excellence for Coral Reef Studies at James Cook University, said southern parts of the reef had a greater number of coral species that were more susceptible to bleaching. He said: “We are at the stage where there’s enough heat to cause widespread, mild bleaching now, and we have two weeks to go.” Fabricius said she had seen a bleaching event unfolding at Magnetic Island, near Townsville, over the past two weeks. At a snorkel trail at Geoffrey Bay, hard coral species were partially bleached, giant clams were turning white and some acropora corals were showing signs of death. “The forecast for Friday is for 35C with clear skies,” she added."
"

Just days before the Trans‐​Pacific Partnership is scheduled to be signed by its 12 member governments, an official expert from the UN Human Rights Council released a statement criticizing the agreement for being incompatible with the goals of the UN human rights regime. The criticism isn’t about the TPP in particular so much as the modern model of trade agreements as an inadequate vehicle for furthering wealth redistribution and massive regulatory intervention to pursue progressive goals. That is, it’s a complaint about what the TPP _doesn’t_ do.   
  
  
There are, of course, lots of things the TPP doesn’t do. Critics have complained that the TPP doesn’t prevent climate change, doesn’t eliminate human trafficking, and doesn’t reform repressive regimes in Vietnam and Brunei. But these are not things the TPP was ever supposed to do. It’s like complaining that Obamacare doesn’t end the drug war.   
  
  
There are legitimate criticisms to be leveled against the TPP—things it does but shouldn’t and things it doesn’t do as well as it should. There’s also a lot to like. But debates over trade agreements often get bogged down with unrelated controversies that are easier to argue about. Not one of the complaints the UN expert makes is explicitly about trade liberalization.   
  
  
The statement includes two specific criticisms of the TPP. One is the secrecy of the negotiations, and the other is investor‐​state dispute settlement. These are well‐​worn, standard complaints opponents of the TPP have been making for years. The persuasiveness of both arguments relies on reflexive fear of the unknown—opponents can hint at what horrible things might happen from the TPP rather than looking at specific, measurable impacts.   
  
  
These issues have become so controversial, in fact, that eliminating ISDS from future trade agreements and increasing transparency in negotiations would probably result in more free trade.   
  
  
The proliferation and prominence of non‐​trade arguments against trade agreements show that agreements like the TPP have strayed too far away from their core mission. Using “human rights” as an argument against trade agreements will be harder to do if they focus more on simply eliminating tariffs, quotas, and subsidies. A debate over the value of protectionism in promoting national and global welfare sounds very appealing and would surely lead to better policy.
"
"It sometimes feels as if environmental news is never good news, but that certainly isn’t true when it comes to the ozone layer.  The UN has announced that the ozone layer is showing “signs of recovery”. Evidence has pointed to recovery for some time, but researchers have waited until they were confident that the hole in the ozone layer was beginning to heal. It’s not yet restored to perfect health – that will take a few more decades – but a significant corner has been turned. That good news comes 30 years after governments around the world began to sign up to the Vienna Convention for the Protection of the Ozone Layer. Solving global environmental problems takes time, but the success of the Vienna convention, and the Montreal Protocol that puts the convention in to action, is proof that when the world works together, and keeps working together even when the going gets tough, it can deliver the solutions that we all need. Of course, having written “that we all need” begs an important question. Why does the ozone hole matter to me?  We have all seen those NASA images of the ozone hole over the Antarctic, but that’s a long way from where most of the planet’s population lives. It’s a little like that scene at the end of “Happy Feet” where the politicians challenged to respond to the plight of the penguins ask why they should “worry about a load of flightless birds”. So why should we worry whether or not there is a little more or less ozone, a tiny fraction of the gases in the atmosphere, than there might have been if we hadn’t all changed our fridges and under-arm deodorants? The most obvious answer is that the ozone layer protects us from ultraviolet (UV) light, and that being exposed to too much UV can eventually cause skin cancers.  OK, but just how many skin cancers have been prevented by protecting the ozone layer?   Until recently, it has been hard to answer that with any sort of numbers, but research has begun to model what the world would have been like if we had not protected the Earth’s ozone layer. These “world avoided” models are indicating that without the Montreal Protocol people around the world would already be exposed to increases in UV. Those increases would be enough to be causing skin damage that, over time, would mean more people developing skin cancers.   In fact, the most recent estimate of what would have happened without ozone protection suggests that by 2030 there would have been around 2m more cases of skin cancer a year worldwide.  That can’t be a precise figure, but even if we take as a “ball-park” estimate, that’s 2m people every year being saved from skin cancer because governments acted to protect the ozone layer. Looking over a longer timescale, do the maths. Two million fewer skin cancers a year, year on year on year soon generates some very large numbers. And those figures don’t take in to account the massive ozone depletion that would have occurred worldwide by the middle of this century. That collapse in global ozone is a consistent outcome of “world-avoided” research and would have increased UV levels around the world beyond anything that has ever been experienced since humans evolved.   Maybe we could have coped with that, but it would have been difficult. Yes, we can all reduce our exposure to UV by how we choose to behave, that’s probably the biggest factor affecting our risk of skin cancer in the world we actually live in. But what about in the world avoided? How much sun-cream would you have needed if without protection you would begin to sunburn in just a few minutes? What clothes would you send your children to school in?  Health-warning signs on the beaches? And even if you could cope, what about the damage to crops, to forests and to the oceans that would have resulted from run-away increases in UV, the scale of which we can’t yet really quantify. So yes, the news that the ozone layer is beginning to recover is a good reason to be cheerful. Be cheerful because we have protected the planet. Be cheerful because we have protected human health. Above all, perhaps, be cheerful because the success of the Vienna convention and the Montreal protocol shows that global governments can work together to solve major environmental problems.   When the Vienna convention was signed no one could be really sure exactly how ozone depletion might develop, but governments were brave enough to make tough decisions based on the best estimates of future risks.  30 years later, research allows us to confirm just how right those decisions were. Surely that’s good news not just for ozone, but also as we look ahead to the even tougher challenges of responding to climate change."
nan
"**The SNP's Westminster leader has apologised after being accused of bullying a photographer who he suggested may have broken Covid rules.**
Ian Blackford posted a tweet claiming that Ollie Taylor lives in the south of England, and questioned why he had taken a photograph of the Northern Lights in Caithness.
But Mr Taylor said he moved to Caithness earlier this year.
Mr Blackford has since deleted his tweet and apologised to Mr Taylor.
Speaking to the Press and Journal newspaper, Mr Taylor said he had tweeted a photograph of the Northern Lights that had been taken about five minutes away from his house in the Highlands.
He accused Mr Blackford of ""trying to stir up public hatred"" against him.
And he said the MP could have ""saved himself a bit of embarrassment"" by messaging him privately about his concerns rather than tweeting publicly.
Mr Blackford had tweeted in response to Mr Taylor's photograph: ""As you live in the south of England and travel to Scotland is only for permitted reasons I am sure there will be a valid reason as to why you are posting a photo from the north of Scotland last night?""
After facing a backlash from other Twitter users, some of whom accused him of attempting to orchestrate an online ""pile-on"", Mr Blackford deleted the tweet.
He also posted an apology to Mr Taylor, saying: ""As the local MP for Ross, Skye and Lochaber I know my constituents feel very strongly about the breaking of travel restrictions that we see across the Highlands and islands, which puts people's lives and our public services at risk.
""I will continue to stand up for my constituents who frequently raise these concerns with me but I recognise that it was wrong to query an individual on Twitter and I apologise to @OllieTPhoto for my earlier post, which I have deleted.""
Mr Blackford's constituency does not include Caithness.
Scottish Liberal Democrat leader Willie Rennie said Mr Blackford had ""picked on and bullied"" a private citizen by ""accusing him of breaking the travel restrictions when he had no evidence of him doing so"".
Mr Rennie added: ""Ian Blackford has form for his remarks about people from England who happen to be in Scotland.
""This behaviour could only add to the problems of anti-English sentiment in Scotland.""
Scottish Lib Dem MSP Alex Cole-Hamilton later asked First Minister Nicola Sturgeon whether she backed the ""vigilante action"" taken by Mr Blackford when he accused the photographer of breaking travel rules.
The first minister praised Mr Blackford's ""grace and dignity"" and said he had done the right thing in apologising for ""doing something he recognised he should not have done"".
Scottish Conservative MSP Annie Wells said Ms Sturgeon should have ""called out"" the SNP MP for ""harassing a private citizen"".
She said: ""Ian Blackford should be ashamed of himself. He purposely went after an individual who simply wanted to share a lovely photo on Twitter for people to enjoy.
""I'm afraid it is the same old story with the SNP - stoke up division, and when challenged, simply hold up their hands feigning innocence. It is pathetic."""
"
Guest post from Von Rudolf Kipp
Originally in German here, with some portions translated to English using the Google translator below. 
[update–translation provided by poster EWCZ ~ ctm]
Google translator is largely imperfect, but to read the Google translation in English go here.
If anyone wishes to do a personal translation for the entire article, please leave a note in comments and I will replace it. Of great interest is the global graphic below, which shows that the MWP is a worldwide event, not just limited to portions of the Northern Hemisphere.


 “ “Who controls the past controls the future: who controls the present controls the past.” – George Orwell, 1984
We live in an age of superlatives. When you turn on the TV nowadays, you get offered the choice of best films, the greatest hits or the dumbest opening lines of all time. And even with a detergent it is long ago not sufficient when it  washes whiter than white.  Again, the constant sale appeal to the consumer can be maintained only if the product is billed as “The best thing ever.”
Naturally, also the reporting on climate change must follow this trend. Therefore the upcoming conference in Copenhagen is optionally about  the salvation of mankind, of whole ecosystems, or for those who like it even more bombastic, the salvation of the planet. To achieve this, we continue to learn, enormous changes in our economic and financial system are needed. Production companies and countries should put on bureaucratic manacles to control their CO2 emissions. Best with the help of worldwide dedicated government-like organizations.
What is the purpose of all this? You suspect or know it already. We are experiencing a warming, which has not existed in the history of mankind, or even in the history of the earth. And as a result we will experience the greatest disasters of all time. Honestly!
Click for an interactive graphic that will expand each graph on mouseover
 Medieval Warm Period thesis contradicts the unprecedented warming
However, one must mention that, already the first half of the statement, that about the unprecedented warming, elicits significant question marks in many climate scientists and even at many historians. Wasn’t there something like the medieval warm period? And in the opinion of many scientists, wasn’t it warmer during this period than today?
The idea of a medieval warm period  was formulated for the first time in 1965 by the English climatologist Hubert H. Lamb [1].  Lamb, who founded the UK Climate Research Unit (CRU) in 1971, saw the peak of the warming period from 1000 to 1300, i.e. in the High Middle Ages. He estimated that temperatures then were 1-2 ° C above the normal period of  1931-1960. In the high North, it was even up to 4 degrees warmer. The regular voyages of the Vikings between Iceland and Greenland were rarely hindered by ice, and many burial places of the Vikings in Greenland still lie in the permafrost.
Glaciers were smaller than today
Also the global retreat of glaciers that occurred in the period between about 900 to 1300 [2] speaks for the existence of the Medieval Warm Period. An interesting detail is that many glaciers pulling back since 1850 reveal plant remnants from the Middle Ages, which is a clear proof that the extent of the glaciers at that time was lower than today [3].
Furthermore, historical traditions show evidence of unusual warmth at this time. Years around 1180 brought the warmest winter decade ever known. In January 1186/87, the trees were in bloom near Strasbourg. And even earlier you come across a longer heat phase, roughly between 1021 and 1040. The summer of 1130 was so dry that you could wade through the river Rhine. In 1135, the Danube flow was so low that people could cross it on foot. This fact has been exploited to create foundation stones for the bridge in  Regensburg this year [4].
Clear evidence of the warm phase of the Middle Ages can also be found in the limits of crop cultivation. The treeline in the Alps climbed to 2000 meters, higher than current levels are [5]. Winery was possible in Germany at the Rhine and Mosel up to 200 meters above the present limits, in Pomerania, East Prussia, England and southern Scotland, and in southern Norway, therefore, much farther north than is the case today [6]. On the basis of pollen record there is evidence that during the Middle Ages, right up to Trondheim in Norway, wheat was grown and until nearly the 70th parallel/latitude barley was cultivated[4]. In many parts of the UK arable land reached heights that were never reached again later.
Also in Asia historical sources report that the margin of cultivation of citrus fruits was never as far north as in  the 13th century. Accordingly, it must have been warmer at the time about 1 ° C than today [7].
Archeology and history confirm interglacial
Insects can also be used as historical markers for climate. The cold sensitive beetle Heterogaster urticae was detected during the Roman Optimum and during the Norman High Middle Age in York. Despite the warming of the 20th century, this beetle is found today only in sunny locations in the south of England [8].
During the medieval climate optimum, the population of Europe reached hitherto unknown highs. Many cities were founded at this very time with high-altitude valleys, high pastures and cultivated areas, which were at the beginning of the Little Ice Age again largely abandoned [9].
The Middle Ages was the era of high culture of the Vikings. In this period their expansion occurred into present-day Russia and the settlement of Iceland, Greenland and parts of Canada and Newfoundland. In Greenland even cereals were grown about this time.. With the end of the Medieval Warm Period the heyday of the Vikings ended. The settlements in Greenland had to be abandoned as well as in the home country of Norway, during this time, many northern communities located at higher altitudes [10]. The history of the Vikings also corresponds very well to the temperature reconstructions from Greenland, which were carried out using ice cores. According to the reconstructions, Greenland was  at the time of the Vikings at least one degree warmer than in the modern warming period [11].
Climate scientists want to eliminate contradictions
Until about the mid-90s of last century the Medieval Warm Period was for climate researchers an undisputed fact. Therefore in  the first progress report of the IPCC from 1990 on page 202, there was the graphics 7c [12], in which the Medieval Warm Period was portrayed as clearly warmer than the present. However, the existence of this warm period became quickly a thorn in the side for the scientists responsible. When in 12th century without human influence the climate has been even warmer than at the height of industrialization, why should the current warming have non-natural causes?

Thus, the Medieval Warm Period was soon declared an odious affair. Meanwhile, an e-mail is legendary, which was sent to a U.S. climate researcher David Deming [13] in 1995. This scientist  published an article in the prestigious journal Science in which he had presented research on climate change in North America based on cores [14].
With this publication, he was immediately known among climate researchers, and some of them obviously thought that he was toeing their line [13, 15]:
“With the publication of the article in Science, I gained significant credibility in the community of scientists working on climate change. They thought I would be one of them, someone who would pervert science in the service of social and political causes. So one of them dropped his guard. An important person working in the field of climate change and global warming sent me an astonishing email with the words: ‘We must get rid of the Medieval Warm Period’. ”
Meanwhile, the climate machinery for the eradication of the Medieval Warm Period has already started. In 1995, the English climatologist Keith Briffa published in the journal Nature a study with sensational results. According to his studies of tree rings in the Siberian Polar-Ural, there had never been a Medieval Warm Period and the 20th century, suddenly appeared as the warmest of the last 1000 years [16]. The real breakthrough was the thesis of 20th Century experience as the warmest of the millennium, but not until three years later, and that with the release of Michael Mann’s infamous Hockeystick [17, 18].
Warm period is extinguished
In this diagram that became the icon of human-induced global warming in the 3rd IPCC Assessment Report, the Medieval Warm Period has now been completely eradicated. However, this curve was quickly under attack, mainly because the Canadian mathematician Steven McIntyre had serious doubts about the correctness of the representation and those pursued with the meticulousness of an auditor [19]. McIntyre showed not only that Mann had used an algorithm that resulted in 90 percent of the cases to a hockey stick, but found also serious errors in the selection of the data and the location of places, as well as the use of incorrect data [20].

Of course, the Mann’s gang could not let these allegations unanswered. In response, Realclimate.com was founded, a name intended to suggest the truth, but somehow reminiscent of the Real Ghostbusters, a poorly made copy of the genuine, which in contrast to the original only pretends to be the right thing. This webpage was henceforth used for accusations and slanders against the non-“believers” [21]. It took also increasingly care not to call McIntyre, in the meantime identified as the main enemy, by his name.
Following the publication of Michel Mann’s hockey stick and the criticism, whole series of further studies was published to demonstrate that the results of Mann’s actually represented the real temperatures over the last 1000 years. The highpoint of the debate was the forced disclosure of the raw data from tree ring studies long held under lock and key, which served as one of the principal witnesses for the correctness of the thesis of the unusually warm 20th century. It turned out that clearly the data were selected intently to get the desired result [22].
Conflicting data
Regardless of the debate over the proper or improper use of proxy data like tree rings to determine the temperature history, mainstream climate researchers, however, are still struggling with a whole series of problems. What was with all the archaeological data, the records of weather events in church records and historical facts, which clearly documented that in the Middle Ages, there was an unusually warm period? Quite simply, the attempts to refute these arguments were made based on claims that all these phenomena indeed existed, but only as geographically limited events [23]. If the Middle Ages was warmer somewhere than today, then maybe it was only in England, the Alps, Greenland or North America. Globally, however, as shown in the many hockey stick charts, it has been colder than at the end of the 20th century.
If one, however, provides an overview of the literature on the subject of Medieval Warm Period, which has been published in recent years, there will be a completely different picture. There are now quite a number of studies from around the world, showing all one thing. And indeed, that the High Middle Ages were warmer than today. An excellent overview can be found on the website CO2 Science, which has set up a whole section for studies of this kind [24]. There are now  765 different scientists from 453 research institutes listed that have worked on the medieval warm period. A small portion of these studies is shown in the figure below [Click 25] (by the graph, you get a larger image where you can select individual work).
This survey shows one thing quite clearly. At the time of the Middle Ages, that is, from 1000 to 1300 it was almost everywhere in the world warmer than today. There have been periods of warming, that exceeded 0.6 degree Celsius rise in temperature in the 20th century and totally without the man-made increased emissions of the supposed “climate killer” of CO2. The statements, that there has not been any Medieval Warm Period, or it was merely a localized phenomenon, can safely be regarded as untenable.
It is therefore not surprising that there are influences on the climate, which can by far exceed the CO2 as a driver of climate variability. This hypothesis is massively supported by the observations made during the last 10 years. Finally, we have been experiencing no increase since 2002, the temperatures have dropped slightly [26]. And that even though the emissions of CO2 from fossil fuels in exactly the same period increased to previously unmatched dimensions.
Google translation in English of the full article is here.


Sponsored IT training links:
Best quality 70-293 study pack to help you pass 640-721 exam on first try. Download SK0-003 practice questions to test you knowledge before hand.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91ae218b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterRahmstorf writes to Overpeck and co-authors:
Dear Peck and IPCC coauthors,
– I know it’s Easter, but I’m having to deal with
Augusto Mangini, a German colleague who has just written an article
calling the IPCC paleo chapter “wrong”, claiming it has been warmer in the
Holocene than now, and stalagmites show much larger temperature
variations than tree rings but IPCC ignores them. What should I answer?
One of my points is that IPCC shows all published large-scale proxy
reconstructions but there simply is none using stalagmites – so please
tell me if this is true?!!
Continue reading…
 
Share this...FacebookTwitter "
"**Yeovil Town FC is considering selling its stadium to help ease the financial worries caused by Covid-19.**
South Somerset District Council is proposing to buy Huish Park then lease it back to the club.
Yeovil Town's chairman says the move is necessary to ""avoid serious financial distress"".
But the move could have ""catastrophic"" consequences according to one long-term fan.
Included in the plan is an option for the club to buy back the land when its finances have improved.
Like many clubs Yeovil, who play in the National League, have suffered due to not having fans in the stadium.
Chairman Scott Priestnall said the current season began after clubs were reassured by the league they would be compensated for lost revenue.
He said that despite that help, which included Â£10m from the National Lottery in October, Yeovil were still facing a financial crisis.
Mr Preistnall added that the club would be in ""deficit"" every months this season and the club ""has to take action"".
The government plans to allow a limited number of supporters back into stadiums, but Mr Priestnall said most income was generated at the start of the season through sponsorship and hospitality so the damage had already been done.
In a statement, South Somerset District Council said it helped many local businesses during the pandemic, giving out millions in grants, and the purchase of Huish Park would be in the same spirit.
The statement added the club ""makes a significant contribution"" to the local economy.
Yeovil fan Louis Purchase said the stadium sale proposal was something that should ""concern every Yeovil Town supporter.""
""If the club sells its main asset, it is unlikely Yeovil Town will ever own their own stadium and land again - and ties the club into paying rent every year.
""For a short-term financial boost, the long-term implications could be catastrophic.""
Huish Park is a registered community asset, which means Yeovil Town Supporters Ltd have to be offered the chance to make their own bid for the site, although the council warned this could delay the process by six months.
More details about the exact finance of the stadium purchase, which will be debated by the council on Thursday, 3 December, are due to be released later this week."
"
Share this...FacebookTwitterThe know-it-all German Greens want to create a new world. But the question is how ugly would their world be? I live in Germany, and I can tell you the windmills everywhere are a total eyesore and are doing nothing to cut back emissions. Here’s a good report on the German movement. H/t: DAmbler.

Green Is Ugly: Style Problems Plague Clean Energy Push
Germany is a world leader when it comes to green energy. But while its windmills and solar panels may be cleaning up the atmosphere, they’re also sullying the landscape.
Recent speeches and interviews by Green Party leaders offer insights into their plans for the future: speed limits, higher taxes and lots of regulations. Fans of fast cars will still be able to enjoy their rides, but not at the expense of the greater good of the people. Introduction of the planned speed limit would kill the one place where Germany is less regulated than the rest of the world, the Autobahn.
The Greens are hoping many people give up their cars altogether. Their re-education efforts – aimed at turning Germans into eternal bike riders – demonize the highway in order to glorify the bike path.
The intellectual cue givers for this planned policy shift leave no doubt about the drastic nature of the change. The government’s environmental advisory council demands nothing less than the reconstruction of civil society.
“The house of mankind is rotten and needs to be repaired urgently,” says climate scientist and advisory council member Hans Joachim Schellnhuber. “We need a sustainability revolution.”  Read more…
Share this...FacebookTwitter "
"

Issue 208, 2002 -click to enlarge
It’s worse than we thought! Now the IPCC has been citing magazine articles, like this one from Climbing Magazine, issue 208, shown at left. We’ve heard the title before, according to their index: “Canaries in a Coal Mine,” – Feature on global loss of glaciers. But wait there’s more! If you think that’s crazy, we also learn that IPCC Chairman Pachauri has penned a “smutty” romance novel! Bizarre, but true.
The Telegraph reports on the magazine issue:
The United Nations’ expert panel on climate change based claims about ice disappearing from the world’s mountain tops on a student’s dissertation and an article in a mountaineering magazine.
The revelation will cause fresh embarrassment for the The Intergovernmental    Panel on Climate Change (IPCC), which had to issue a humiliating apology    earlier this month over inaccurate statements about global warming.
The IPCC’s remit is to provide an authoritative assessment of scientific    evidence on climate change.
In its most recent report, it stated that observed reductions in mountain ice    in the Andes, Alps and Africa was being caused by global warming, citing two    papers as the source of the information.
However, it can be revealed that one of the sources quoted was a feature    article published in a popular magazine for climbers which was based on    anecdotal evidence from mountaineers about the changes they were witnessing    on the mountainsides around them.
The other was a dissertation written by a geography student, studying for the    equivalent of a master’s degree, at the University of Berne in Switzerland    that quoted interviews with mountain guides in the Alps.
The revelations, uncovered by The Sunday Telegraph, have raised fresh    questions about the quality of the information contained in the report,    which was published in 2007.
It comes after officials for the panel were forced earlier this month to    retract inaccurate claims in the IPCC’s report about the melting of    Himalayan glaciers.
Sceptics have seized upon the mistakes to cast doubt over the validity of the    IPCC and have called for the panel to be disbanded.
This week scientists from around the world leapt to the defence of the IPCC,    insisting that despite the errors, which they describe as minor, the    majority of the science presented in the IPCC report is sound and its    conclusions are unaffected.
But some researchers have expressed exasperation at the IPCC’s use of    unsubstantiated claims and sources outside of the scientific literature.
Professor Richard Tol, one of the report’s authors who is based at the    Economic and Social Research Institute in Dublin, Ireland, said: “These    are essentially a collection of anecdotes.
“Why did they do this? It is quite astounding. Although there have    probably been no policy decisions made on the basis of this, it is    illustrative of how sloppy Working Group Two (the panel of experts within    the IPCC responsible for drawing up this section of the report) has been.
“There is no way current climbers and mountain guides can give anecdotal    evidence back to the 1900s, so what they claim is complete nonsense.”
The IPCC report, which is published every six years, is used by government’s    worldwide to inform policy decisions that affect billions of people.
The claims about disappearing mountain ice were contained within a table    entitled “Selected observed effects due to changes in the cryosphere    produced by warming”.
It states that reductions in mountain ice have been observed from the loss of    ice climbs in the Andes, Alps and in Africa between 1900 and 2000.
The report also states that the section is intended to “assess studies    that have been published since the TAR (Third Assessment Report) of observed    changes and their effects”.
But neither the dissertation or the magazine article cited as sources for this    information were ever subject to the rigorous scientific review process that    research published in scientific journals must undergo.
The magazine article, which was written by Mark Bowen, a climber and author of    two books on climate change, appeared in Climbing magazine in 2002. It    quoted anecdotal evidence from climbers of retreating glaciers and the loss    of ice from climbs since the 1970s.
Mr Bowen said: “I am surprised that they have cited an article from a    climbing magazine, but there is no reason why anecdotal evidence from    climbers should be disregarded as they are spending a great deal of time in    places that other people rarely go and so notice the changes.”
The dissertation paper, written by professional mountain guide and climate    change campaigner Dario-Andri Schworer while he was studying for a geography    degree, quotes observations from interviews with around 80 mountain guides    in the Bernina region of the Swiss Alps.
read the complete article at the Telegraph


Sponsored IT training links:
Pass 642-642 exam fast using self paced 640-822 prep tools including 640-863 dumps and other study resources.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8eab22bb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the past two decades, New Zealand farmers have moved from an environment in which we were subsidized and the government dictated the type of agricultural goods produced to one in which we farm without subsidies. The clear focus for us now is on the consumer. That is the real reason we are in business: to serve the consumer, not the government.



One advantage for New Zealand is that we are a small country. We can be experimental and help other people understand our progress because of the smaller scale of the New Zealand economy.



 **“Fortress New Zealand”**



Let me take you back in history to the 1950s, when there were worldwide food shortages after World War II. New Zealand was an agricultural nation, feeding the United Kingdom, and had the second‐​highest per‐​capita income in the world. We were incredibly rich. The government was dominated by farmers. Almost all the cabinet ministers were farmers. Farmers had it made.



What did we do? We were our own worst enemy. We went to the government and said, “Prices fluctuate, the climate affects us, we think you should step in and help regulate what we earn, to take away fluctuations in our income.” And the government, being full of farmers, said, “Yep, that’s good. We’ll help you.” So we set up marketing boards and other structures that put constraints on the farming community. They either limited or expanded production, and they controlled the price depending on what the government and often farmer‐​controlled boards thought was best for farming.



Protection and control of the farming sector were bad enough, but New Zealand is also a small country, and the government even then did not believe the future of New Zealand was in agriculture. We needed to start creating industry in New Zealand that would employ people elsewhere. So the government started imposing import tariffs and quotas on nearly everything, including cars and televisions. The aim was to increase the prices of imports and make manufacturing in New Zealand competitive. New Zealand became “Fortress New Zealand.”



We locked out the foreign competition, we “protected” jobs, and we had virtually no unemployment in New Zealand right up to the 1970s. We were rich enough, because of the agricultural income that was coming in, to be able to sustain the jobs. But the trade barriers we imposed on ourselves during that time built in unnecessary costs within the New Zealand economy.



 **Shock, Crisis, and Reform**



Then we experienced two major economic shocks. The first was in 1973 when Britain entered the European Union. Suddenly, the market that used to take much of what we produced was no longer open to so many of our exports. The second was the oil shocks. Basically, we had this lovely little fortress in New Zealand, a top‐​down pricing mentality that ignored market forces, and–what do you know–inflation started to get out of control. But we also had a controlled society, in which interest rates and seemingly everything was controlled in New Zealand.



Finally, after 10 years, New Zealand experienced both fiscal and financial crises in 1984. There was no more money for the government to spend, and the government had run up huge deficits and borrowed a lot of money from overseas to try to keep the wheels of the New Zealand economy turning. Thankfully, the Federated Farmers of New Zealand realized that continually going to the government for more handouts wasn’t working. We were becoming increasingly uncompetitive in the world markets. We needed to change, so we said to the government, “Strip out our subsidies, but we want you to reform the rest of the New Zealand economy as well.”



The government made the reforms, but painfully for farmers it was a left‐​leaning government that made the transition. The government reformed agriculture first because the farmers had traditionally never voted for them anyway. The rest of the economy took six years to be fully liberalized. We now live in one of the most open and unregulated economies in the world.[1] Other than a few tariffs on shoes and some clothing, we are completely open at the border for everything. Before 1984, if you were well‐​off, you could avoid the tariffs. If you wanted to buy a car or a television, you simply took a holiday overseas and brought it back with you to New Zealand. Now you can buy anything anywhere in New Zealand from anywhere in the world. And it’s fantastic.



 **Satisfying the Consumer, Using Land Wisely**



The major development for the New Zealand agricultural sector was that, after enduring the pain of that long transition, responsibility for farming returned to the farmer. So the government was no longer involved in our lives. The key issue that we had to grapple with very quickly was that there was only one component important to farm income, and that was satisfying the consumer. We used to satisfy government officials in what they wanted. We had 70 million sheep and another 50 million lambs, of which the meat industry rendered, in one year, six million lambs into blood and bone (a fertilizer product) because nobody wanted them. In the worst days of the post‐​deregulation period, in a drought in the Canterbury region of New Zealand, farmers were getting only $6 a lamb. Now we actually focus on the consumer. Processors now pass on clear market signals and give farmers higher returns. The farmer now provides exactly what the consumer wants in the most cost‐​effective manner. Now, by meeting market specifications, we get between $60 and $100 a lamb depending on the time of year the lamb is supplied.



The other positive result of the reform process was that farmers began to better fit their agricultural production to the type of land that they farmed. If you don’t have a really good fit with the type of land you are on, your resources are used inefficiently, and then you can’t get your costs low enough to be competitive. New Zealand has reduced the number of sheep from 70 million down to 40 million,[2] but we produce roughly the same amount of sheep meat.[3] We increased our dairy herd from roughly 3 million cows to now more than 5 million in response to the market demand for protein products.[4] We’ve seen a huge diversification of land use in the last 20 years. In the good old days, people just would not have believed what farmers were capable of when everybody just wanted us to produce another lamb.



Farmers have continued to diversify, to change, and to accept that we not only need access to markets and consumers, but we also need an incredibly competitive internal economy. We need to be able to adjust our cost structures quickly to deal with the volatility of climate, product price, and, in a small country like New Zealand, the exchange rate. But we are happy to accept these risks and uncertainty because by facing these risks we make far better decisions. And we’re not blaming anyone else; we only have to rely on ourselves to get on with the job.



 **Rising Farm Productivity and Output**



The New Zealand farm sector before 1984 had a productivity increase of 1 percent a year. Since the reform, it’s been nearly 4 percent per year.[5] We’re performing better than any other part of the New Zealand economy. The agriculture sector in New Zealand has actually grown as a percentage of our GDP, from slightly more than 14 percent of GDP in 1986–87 to 16.6 percent in 1999–2000.[6] This is almost unheard of in any other developed country. So we’re actually playing our part and more for New Zealand.



Agricultural trade is incredibly important for New Zealand because it’s what we do well. We rely upon exports from our biological industries to pay for imported goods from around the world that we have no advantage in producing ourselves.



However, we face lots of criticism. Our competitors criticize the size of our trade in dairy products compared with that of the U.S. or European dairy industries. On the other hand, New Zealand looks at the bigger picture. We look at the total trade between countries, as opposed to sector by sector.



Since the transition period, New Zealand’s move toward a completely open economy has created nearly 450,000 jobs on net.[7] For a small country of 4 million people, that’s a lot of jobs, and we’re still creating more. Many myths abound about trade being bad for jobs, but we’ve found the exact opposite to be true. The reality is that trade is good for jobs.



A liberal trading environment not only encourages investment, but it also enables countries to produce to their comparative advantage and trade to their strengths. This leads to a more efficient use of resources and creates more wealth and jobs.



 **Overcoming the “Fear Factor”**



The New Zealand experience brings two key messages to the world trading environment. The first is that producers must focus on the consumer. The reason we want liberalization in trade is so that we can talk directly to consumers in individual countries. We want to be able to tailor our products specifically to the market in each country. It’s not just about world trade in the abstract; it’s about reaching out to each country’s individual consumers and supplying them with what they want.



I recently attended the World Farmers Congress of the International Federation of Agricultural Producers. A continual theme I heard was “Exports are good, imports are bad,” and that “When you liberalize world trade, it’s a race to the bottom.” Well, in New Zealand we are in a race to the top. But every time we race to the top, a bureaucrat somewhere else in the world tries to squash us. The classic example is Europe, where we exported “spreadable” butter. Because it didn’t meet the specifications of the regulators as butter, they prevented it from entering Europe, even though the demand from the consumer was strong. We were adding value, creating a better product, and meeting what the consumer wanted, but a bureaucrat said, “Ah, but it’s not butter, because it is too soft.” These are the types of issues we have to deal with in global negotiations.



The second message from the New Zealand experience is that we use our resources well. We’ve put our resources where they are most efficiently used. For sustainability, we want the world to use its resources well. Agriculture plays a huge part in that process. It makes no sense for farmers in Europe and the United States to produce sugar at three or four times the world price when you can more efficiently produce it in many tropical countries around the world.[8] The tropical countries can produce sugar at a fraction of the cost and with much more efficient use of world resources. Our vision is for a dual outcome–to liberalize trade and to make better use of the world’s resources as each country produces to its comparative advantage.



In New Zealand, we went through the same fear factors that I hear from other farmers in the developed countries. People said, “We can’t do it. We’re not going to produce any more milk. We’re all going to go broke. The government hates us.” Once you get through all that–and I firmly believe that the developed countries’ farmers can–farmers will start to focus on what they can do. They will focus on what’s good in their region. American and European farmers have a wonderful advantage that we don’t have in New Zealand: their customers are in their backyards. Their customers are not 12,000 miles away from a little country in the middle of the South Pacific. They do not have to transport everything great distances before they reach a consumer.



Producers must move beyond thinking that they can’t produce for the wide range of consumer demands. They must look for the opportunities that exist both in their own backyards as well as beyond their borders. Fear of change often runs rampant, but once you are farming in accord with your comparative advantage, it becomes possible to adapt your production system to suit consumer needs and adjust production costs accordingly.



A good example of this is the New Zealand wine industry. New Zealanders are quite good at producing wine, but we still import more wine than we export. New Zealand consumers like to have choice, and we are willing to give our consumers that choice. We believe it is better for the farming community, better for the New Zealand economy, and ultimately better for the world.



 **Conclusion**



I recognize that our vision is bold, but we’d like the Americans and others to be ambitious in the Doha Development Round of WTO trade negotiations. We’re practical. We accept that there is probably going to be a need for a reasonably long transition period as people move toward an open market, but this is an opportunity that the world really needs to seize. We need to be forward thinking on trade liberalization. Based on the experience of the Uruguay Round and even this round, negotiating these agreements takes a long time. The opportunities don’t come very often. I hope we can turn that opportunity into reality.



[1] New Zealand’s economy now ranks as the third‐​freest in the world, behind only Hong Kong and Singapore, according to James Gwartney and Robert Lawson, _Economic Freedom of the World: 2004 Annual Report_ (Vancouver, B.C.: Fraser Institute, 2004), p. 11.



[2] New Zealand farmers owned 39.6 million sheep as of June 30, 2002, according to the New Zealand Ministry of Agriculture and Forestry, “Agriculture Production Survey,” June 30, 2004,  www​.maf​.govt​.nz/​s​t​a​t​i​s​t​i​c​s​/​p​r​i​m​a​r​y​i​n​d​u​s​t​r​i​e​s​/​l​i​v​e​s​t​o​c​k​/​s​h​e​e​p​/​0​2​-​s​h​e​e​p​-​a​g​e​-​f​s.xls. There were 69.7 million sheep as of June 30, 1984, according to the Policy Information Group, New Zealand’s Ministry of Agriculture and Forestry,  www​.maf​.govt​.nz/​s​t​a​t​i​s​t​i​c​s​/​p​r​i​m​a​r​y​i​n​d​u​s​t​r​i​e​s​/​l​i​v​e​s​t​o​c​k​/​s​h​e​e​p​/​s​h​e​e​p.htm.



[3] New Zealand produced 113,000 tons, bone‐​in (2002–2003), compared to 130,000 tons, bone‐​in (1985–1986), according to the New Zealand Ministry of Agriculture and Forestry, Game Industry Board, The Economic Service, available through Meat and Wool Innovation Ltd., www​.wool​pro​.co​.nz/​e​c​o​n​o​m​i​c​s​e​r​v​i​c​e​/​q​u​i​c​k​s​t​a​t​s​.html.



[4]New Zealand farmers owned 5.1 million dairy cattle as of June 30, 2003, compared to 3.2 million dairy cattle as of June 30, 1984, according to the Policy Information Group, Ministry of Agriculture and Forestry,  www​.maf​.govt​.nz/​s​t​a​t​i​s​t​i​c​s​/​p​r​i​m​a​r​y​i​n​d​u​s​t​r​i​e​s​/​l​i​v​e​s​t​o​c​k​/​d​a​i​r​y​/​d​a​i​r​y.htm.



[5] See Roger Kerr, “A Vision For Agriculture,” Address, New Zealand Business Roundtable, Wairarapa Agricultural Seminar, July 19, 2001; and Federated Farmers of New Zealand, “Life After Subsidies: The New Zealand Farming Experience 15 Years Later,” August 2002, pp. 1–2. Productivity estimates are from Lawrence and Diewert’s Treasury Working Paper 99/5, “Measuring New Zealand’s Productivity,” March 1995, www.treasury.govt.nz/workingpapers/1999/99–5.asp.



[6] This statistic, measuring both agricultural production and downstream processing, is from the Federated Farmers of New Zealand, “Life After Subsidies: The New Zealand Farming Experience 15 Years Later,” August 2002, p. 1, and Rob Davison, “Agricultural Productivity,” _Vetscript_ , May 2003. When downstream activities are not included in a measure of the agriculture sector, agriculture comprised approximately 4 percent of New Zealand GDP in 1985, and just over 7 percent in 2001. In “OECD National Accounts Volume 2,” the OECD calculates New Zealand’s agriculture sector as 6.4% of total GDP in 1986 and 8.3% of total GDP in 2000 based on current prices, and 7.3% of GDP in 1986 compared to 8.3% of GDP in 2000 based on constant prices. See also New Zealand Treasury, “New Zealand Economic & Financial Overview 2002,” and Robert A. Buckle, David Haugh, and Peter Thomson, “Treasury Working Paper 01/33: Calm after the Storm?: Supply‐​side contributions to New Zealand’s GDP volatility decline,” 2001, Figure 5. 



[7] Phil Briggs, “Looking at the numbers: a view of New Zealand’s economic history,” The New Zealand Institute of Economic Research, www​.nzi​er​.org​.nz/​S​I​T​E​_​D​e​f​a​u​l​t​/​S​I​T​E​_​P​u​b​l​i​c​a​t​i​o​n​s​/​r​e​s​e​a​r​c​h​_​m​o​n​o​g​r​a​p​h​s​.​asp#2. And, Statistics New Zealand, “Household Labour Force Survey,” March 2003, June 2003, September 2003, and December 2003.



[8] Oxfam International, “Dumping on the World: How EU Sugar Policies Hurt Poor Countries,” Oxfam Briefing Paper 61, March 2004, p. 7.


"
"
Gore 2.0, now with Pacific Hurricanes, coming to a book store near you. Gore plans to hawk it on David Letterman next Tuesday night. One more reason not to watch Dave anymore. One can always hope though. Maybe he’ll feature Gore as a “stupid human trick”.

Al doesn’t seem to learn when it comes to visuals. Or maybe he just thinks that he’s obligated to put a picture of a hurricane on the front cover to keep the theme of AIT going. Either way. Any imagined link between hurricanes and global warming has evaporated.

My prediction:  sales will be a fraction of AIT, and it probably won’t make the NYT bestsellers list. People are tired of the yap, as indicated by recent polls.
Here’s what he has to say about it on his blog (note: he doesn’t take comments)
From the Press Release:
Today Vice President Gore announced that his next book, Our Choice, will be published by Rodale in the US and by other publishers internationally on November 3, 2009. Picking up where An Inconvenient Truth left off, Our Choice utilizes Mr. Gore’s forty years of experience as a student, policymaker, author, filmmaker, entrepreneur and activist to comprehensively describe the real solutions to global warming. A co-recipient of the Nobel Peace prize in 2007 for his environmental work, Mr. Gore continues to make sense of the pressing issues we face and Our Choice will unquestionably inspire and rally those ready to fight for solutions that were deemed impossible only a short time ago.
Said Vice President Gore, “An Inconvenient Truth reached millions of people with the message that the climate crisis is threatening the future of human civilization and that it must and can be solved. Now that the need for urgent action is even clearer with the alarming new findings of the last three years, it is time for a comprehensive global plan that actually solves the climate crisis. Our Choice will answer that call.”
Since the publication of the New York Times bestseller An Inconvenient Truth and the release of the Academy Award® winning film of the same title, Mr. Gore has led more than thirty “Solutions Summits” with top scientists, engineers and policy experts to examine every solution to the climate crisis in depth and detail. Our Choice draws on conclusions developed through those summits as well as on extensive independent research, describing how the bold choices necessary to save the earth’s climate should also be the foundations of policies worldwide to create new jobs and stimulate sustainable economic progress.
As they did with An Inconvenient Truth, former Vice President Gore and Mrs. Tipper Gore will donate 100% of the proceeds of the book to the Alliance for Climate Protection, a non-profit, non-partisan group dedicated to spreading awareness about the climate crisis and how to solve it.
Our Choice will feature 100% recycled paper, locally produced and sourced editions, low VOC inks, and will be carbon neutral.
“Rodale is honored to continue our relationship with Vice President Al Gore,” said Rodale Inc. President and CEO Steven Pleshette Murphy. “We were proud to publish An Inconvenient Truth and very much look forward to bringing Our Choice to the growing audience of committed citizens who are seeking solutions to the climate change crisis. In the spirit of our longtime mission, we are dedicated to creating the greatest possible platform for Vice President Gore’s work and message.”
Simon & Schuster Audio, who published the Grammy Award winning audio version of Al Gore’s An Inconvenient Truth, will publish the audio edition of the book on CD and digital download simultaneous with the hardcover publication.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e927261e8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
On June 25th the Competitive Enterprise Institute (CEI) released a draft copy of the suppressed EPA report by EPA employee Alan Carlin critical of the EPA’s position on Carbon Dioxide saying:
The released report is a draft version, prepared under EPA’s unusually short internal review schedule, and thus may contain inaccuracies which were corrected in the final report.
While we hoped that EPA would release the final report, we’re tired of waiting for this agency to become transparent, even though its Administrator has been talking transparency since she took office. So we are releasing a draft version of the report ourselves, today,” said CEI General Counsel Sam Kazman.
CEI notes that: Internal EPA email messages, released by CEI earlier in the week, indicate that the report was kept under wraps and its author silenced because of pressure to support the Administration’s agenda of regulating carbon dioxide.
I’m pleased to say that we have the final report exclusively available here, courtesy of our verified contact at the EPA, who shall remain anonymous. For some background on this contact, developed with the help of Tom Fuller at the San Francisco Environmental Policy Examiner, please read the WUWT story below. The download link is also below.
Source inside EPA confirms claims of science being ignored, suppressed, by top EPA management
The title page of the final report from Alan Carlin of the EPA reads:
Comments on Draft Technical Support Document for Endangerment Analysis for Greenhouse Gas Emissions under the Clean Air Act
By Alan Carlin
NCEE/OPEI
Based on TSD Draft of March 9, 2009
March 16, 2009
Alan prepared an update to this document which is on page 3, I’m reproducing it here for our readers:

Important Note on the Origins of These Comments
These comments were prepared during the week of March 9-16, 2009 and are based on the March 9 version of the draft EPA Technical Support document for the endangerment analysis for Greenhouse Gases under the Clean Air Act. On March 17, the Director of the National Center for Environmental Economics (NCEE) in the EPA Office of Policy, Economics, and Innovation communicated his decision not to forward these comments along the chain-of-command that would have resulted in their transmission to the Office of Air and Radiation, the authors of the draft TSD.
These comments (dated March 16) represent the last version prepared prior to the close of the internal EPA comment period as modified on June 27 to correct some of the non-substantive problems that could not be corrected at the time. No substantive change has been made from the version actually submitted on March 16. The following example illustrates the type of changes made on June 27. Prior to March 16 the draft comments were prepared as draft comments by NCEE with Alan Carlin and John Davidson listed as authors. In response to internal NCEE comments this was changed on March 16 to single author comments with assistance acknowledged by John Davidson. There was insufficient time, however, because of deadlines imposed by the Office of Air and Radiation, to make the corresponding change in the use of the word “we” to “I” implicit in the change in listed authorship. This change has been made in this version.
It is very important that readers of these comments understand that these comments were prepared under severe time constraints. The actual time available was approximately 4-5 working days. It was therefore impossible to observe normal scholarly standards or even to carefully proofread the comments. As a result there are undoubtedly numerous unresolved inconsistencies and other problems that would normally have been resolved with more normal deadlines. No effort has been made to resolve any possible substantive issues; only a few of the more evident non-substantive ones have been resolved in this version.
It should be noted, of course, that these comments represent the views of the author and not those of the US Environmental Protection Agency or the NCEE.
Alan Carlin
June 27, 2009

UPDATE: Before downloading, please read the paragraph above from Alan Carlin to get some perspective. Certainly, this document is not perfect. How could it be? The EPA gave an internal comment period of 1 week on the most far reaching “finding” the agency has ever dealt with. This short window was unprecedented. So ask yourself, could you produce a paper like this, covering many disciplines outside of your own, that is “perfect” on 5 working days notice?
The EPA’s procedure here is the culprit.
Download the final report from Alan Carlin here, link:  Endangerment comments v7b1 (PDF 4MB)

Sponsored IT training links:
Get guaranteed success in 1Y0-A11 exam using best quality 000-200 prep tools including 642-611 dumps and other study resources.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9546c601',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The parents of a 10-year-old boy who died after contracting Covid-19 have spoken of their ""indescribable"" pain.**
Fehzan Jamil, from Bradford, had a number of underlying health issues and is believed to be one of the youngest victims of the pandemic in the UK.
His mother and father, Tayyaba and Mohammed Jamil, said: ""There were four of us, now there are only three. The pain is indescribable.""
Fehzan, who was described as a ""brave fighter"", was laid to rest on Monday.
Speaking to Channel 4 News on Tuesday, his mother said: ""I just can't describe our loss. Everything feels empty now.""
Fehzan had been treated by staff at Bradford Royal Infirmary for several years due to a number of health issues, including epilepsy.
His family had tried to shield him during the pandemic, knowing that his health issues made him vulnerable to Covid-19.
He was kept at home as much as possible, with anybody coming into the house required to wear a mask.
""We tried our best to keep him safe but somehow Covid got to him,"" Mr Jamil said.
Mrs Jamil paid tribute to hospital staff who had treated her son, saying: ""All of the staff were very good to us. They have known Fehzan for many years now and have always looked after him.
""They let us be beside him when he died. It meant a lot.""
On Tuesday, the government recorded another 608 UK deaths within 28 days of a positive Covid test.
This is highest figure since May 12, when 614 deaths were reported, and brings the UK total to 55,838.
In June, a 13-day-old baby, thought to have no underlying health conditions, died with Covid-19.
_Follow BBC Yorkshire on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to_ yorkslincs.news@bbc.co.uk _or_send video here _._"
"
Both WUWT and Climate Audit had posts regarding the ridiculous WaPo story about snowfall being a result of climate change.
This is a follow up to those posts done by guest contributor Steven Goddard.

One of the  NWF claims about global warming is that snow in the Colorado mountains is diminishing and has become very erratic, as seen in the NWF graphic at left.
click for a larger image
In this article I will show that the claim is incorrect – Colorado snowfall has been generally increasing for the last hundred years and that year over year variability has always been extremely high.
Fortunately, there are excellent long term records of snowfall available from  NOAA’s Western Regional Climate Center. I chose the  Crested Butte, Colorado station because it is centrally located in the mountains (so is representative of a wide region) and has the most complete and continuous snow record of every month for the past 100 years.   I have randomly sampled quite a few other stations in Colorado.  None seem to have as a complete a record as Crested Butte, and the pattern described for Crested Butte seems to be fairly consistent in the mountainous regions of the state.
Below are graphs showing  annual and monthly snowfall totals (in inches) for Crested Butte since 1909.  The trend lines were generated using Google Spreadsheet’s linest() function. Note that every month is trending upwards in snowfall and the standard deviation is very high.  Also note that there were several very dry years early in the 20th century with very little snow – and the last few decades have seen more consistent snowfall.  Since 1981, every year has received more than 100 inches of snow.  Prior to 1930, it was not uncommon to have snow years with less than 100 inches of snow.  Prior to 1930, the average annual snowfall was 177 inches.  Since 1930, the average annual snowfall has been 200 inches – a 10% increase.
Note – the raw data is incorrect for 1910, 1919, and 1924 due to a significant number of missing measurements, so I substituted a calculated annual value based on the trend line. This probably overestimates the snowfall for 1919 and 1924, and is thus conservative.
Click images below for full-sized ones.

Standard deviation = 67  Mean = 195  Trend = +7.7 inches per decade

Mean = 23.4  Standard Deviation = 15.1

Standard deviation = 25.9  Mean = 33.5

Standard deviation = 27.9  Mean = 38.4

Standard deviation = 19.3  Mean = 33.5

Standard deviation = 18.2  Mean = 31.0

Standard deviation = 13.1  Mean = 16.9
In summary, snowfall is increasing annually and we see upward trends in the months of “snowfall season” in Colorado.  Year over year variability has always been very high and may actually be lower in recent years. And, the Colorado mountains no longer have extremely low snow years like they did 80 years ago. By the data, it seems the NWF claims are unfounded.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8ec3ab98',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The number of unemployed people in the UK is expected to surge to 2.6 million by mid-2021, Rishi Sunak has warned.**
In his Spending Review, the chancellor said the ""economic emergency"" caused by Covid-19 had ""only just begun"".
The government expected to borrow Â£394bn this year - the ""highest"" level ""in our peacetime history"" - he added.
The latest figures show 1.62 million people are unemployed, a number which has risen by more than 300,000 since last year.
In the House of Commons, Mr Sunak said the government would spend Â£280bn this year ""to get our country through coronavirus"".
He also announced that most public sector workers would have their pay frozen, with only the lowest paid, as well as nurses, doctors and other NHS staff, getting a salary rise.
And the chancellor said spending on overseas aid, as a proportion of national income, would be 0.5% in 2021-2 - down from the 0.7% currently set in law.
The document accompanying Mr Sunak's statement makes no mention of extending the temporary Â£20 uplift in Universal Credit beyond next April, but this is expected to be reviewed in the new year.
The last time the UK unemployment figure was as high as 2.6 million was in May to July 2012.
The number exceeded three million from 1983 to 1987 and for a few months in early 1993.
Mr Sunak told MPs the economy was predicted to contract by 11.3% this year - ""the largest fall in output for more than 300 years"" - and grow by 5.5% next year and 6.6% in 2022.
He added: ""Even with growth returning, our economic output is not expected to return to pre-crisis levels until the fourth quarter of 2022. And the economic damage is likely to be lasting.""
The government's Covid response, including furlough, has led to huge spending rises, at a time when its income from taxation is down.
Mr Sunak said the UK was expected to borrow Â£394bn this year, which was predicted to fall to Â£164bn next year and Â£105bn in 2022-3.
Some other Spending Review announcements were trailed before his statement, including:
The chancellor had intended - as usual - to set out plans for the next three years, but this was reduced to just one year due to the economic turmoil caused by the pandemic.
For Labour, shadow chancellor Anneliese Dodds said a longer-term spending review was needed soon ""to build a future for our country as the best place in the world to grow up in and the best place to grow old in"".
She criticised Mr Sunak for not mentioning Brexit in his speech, with the UK set to leave the EU single market and customs area at the end of the year.
Ms Dodds added: ""There's still no trade deal. So does the chancellor truly believe that his government is prepared and that he's done enough to help those businesses that will be heavily affected?""
The Office for Budget Responsibility said that if no deal was reached, and the UK and EU had to trade under World Trade Organization rules - including tariffs - this could ""reduce real GDP"" by 2% in 2021, on top of the damage caused by coronavirus.
The economic shock of the ""various temporary disruptions to cross-border trade and the knock-on impacts"" would continue for years, it predicted.
But a Treasury spokesman insisted the government was confident about the future of the UK, whatever the outcome of negotiations with Brussels.
He said the chancellor was focussed on Covid, which he described as the ""core economic challenge"" and ""the one that matters today to people's jobs"".
Dave Prentis, general secretary of the union Unison, called the pay freeze for most public sector workers ""austerity, plain and simple"" and a ""bitter pill"" for those affected.
He added: ""A decade of spending cuts left public services exposed when Covid came calling. The government is making the same disastrous mistake again."""
"
Share this...FacebookTwitterThe Swiss online NZZ has a commentary written by Social Sciences Prof. Ulrike Ackermann, Director of the John Stuart Mill Institute for Liberty Research in Heidelberg, read here (in German).It’s no secret that the German Greens have been on the rise and that their movement has spearheaded the drive against the use of fossil fuels and atomic power in Germany and Europe. Ackermann writes: “For the Greens, nature is good, humans are bad” and thus must be always kept under the watchful eye of a powerful, better-knowing state. This means more central (by amateurs) planning, equalization and social uniformity.
It means the destruction of individual responsibility, and thus the individual.
Ackermann reminds us: “History shows us that this is precisely what never has put us on the path to democracy, freedom and prosperity.”
The German Greens advocate massive state intervention and deep interference in our private lives with the aim of squashing individual liberty and independence. The Greens in Berlin, for example, aim to require all do-it-yourself home improvement projects to be subjected to state permitting. This, they say, would ensure cheap and affordable housing. Installing hardwood floors or other similar improvements only serve to make living quarters more luxurious, and thus less affordable to poorer people.
The same massive state intervention is also called for when it comes to transportation. People need to be herded into public transport systems and other forms of “healthy transport modes”…like bicycles, buses, or walking. Airport expansions, on the other hand, are to be stopped at any cost. Ackermann writes:
On the path to this noble target, adult citizens are being treated like children. The program is in ‘easy language’ that is especially designed for people with ‘learning difficulties, reading impairments and speech problems’, and so verily pushes infantilism to a new high.”
To me this indicates that people who do not have the faculties to care for themselves as responsible adults are particularly attracted to Green promises. Green sympathizers probably can be classified in 3 primary categories: 1) people who want to nanny and boss everyone around, 2) people who want to be nannied and bossed around, and 3) the many gullible who actually believe the climate catastrophe scam.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Green movement of course needs scientific authority to sustain it. This is provided by Hans Schellnhuber’s WBGU, which in English stands for: Scientific Council of the German Government for Global Transformation. Global transformation by Germans?
Here once again we have a few arrogant Germans with an insatiable desire for world domination. One function of the WBGU is, as Ackermann writes:
With a new contract for society, the Council desires to implement the Great Ecological Transformation to a nuclear-free and carbon-free global economy. A powerful, ruling government shall provide for this, and will care for the ‘societal problemtic’ of ‘non-sustainable living styles’. Climate protection will be one of the fundamental targets of the state. Our current parliamentary democratic legal process is to be supplemented by a ‘Future Council’ which will be expressly superordinate over Parliament, party squabbling and conflicting interests, and will be assembled by draw.”
This means a select unelected elitist group of European white guys, would have the final say on global decision making. Clearly the green movement is well on its way to something of quite another color. Ackermann asks:
Haven’t we seen something like this before?”
Ackermann summarizes by writing we do not need a paternalistic, dictatorial state that decides everything for us, thus destroying the individual. Instead, she writes, we need a state that promotes all individuals, their independence, and thus their pride and self responsibility. Only a society that  can produce such individuals can expect to see prosperity, justice and equality. The Green Movement will deliver none of that.
Kudos to the NZZ for publishing Prof. Ackermann’s commentary.
Share this...FacebookTwitter "
"People learn by making mistakes. The same is true for firms and society – success depends on being able to internalise lessons and behave differently in future, to avoid repeating the same errors. Firms tend to review their organisational structures and routine practises to flag problems before they occur, or respond quickly to unexpected problems to minimise their impact. This is apparently not the case with Vale, however – the fifth largest mining company in the world. Vale is the world’s biggest producer of iron ore and nickel and is also responsible for what may be the largest environmental disaster in Brazil’s history, after one of its tailings dams – an embankment which is supposed to hold back a vast reservoir of toxic mining byproducts – collapsed on January 25, 2019 at the Corrego do Feijao mine in south-eastern Brazil.  Following the collapse, 186 people were confirmed dead and 122 are still missing. Official data from the Brazilian Environmental Agency says that the mudflow destroyed 270 hectares, of which more than half was native vegetation or protected forest. The swathe of natural habitat destroyed is equivalent to 300 football pitches. Tragically, this happened only three years after a similar accident on another of Vale’s dams in the southern Brazilian state of Minas Gerais, near the city of Mariana, which killed 19 people. By any measure of business sustainability, Brumadinho has been a heavy blow to Vale’s performance and reputation. Since then, the company has had its credit rating downgraded, not to mention suffered crippling damage to its public image. Vale appointed Fabio Schvartsman as CEO after the Mariana disaster in November 2015. Schvartsman took the job and announced to shareholders, employees and the Brazilian people a strong slogan: “Mariana, never again”. He failed miserably.  Considering Vale’s recent history and the magnitude of these disasters, the corporate response to Brumadinho’s tragedy seems ludicrous. Schvartsman said to the Brazilian parliament, in a session which assessed the condition of other mining dams in Brazil after the collapse at Brumadinho: Vale is a Brazilian jewel that cannot be condemned for an accident that took place in one of their dams as much, as it was [considered] a tragedy. Despite his confidence, questions remain unanswered. Why didn’t the company move a canteen that was below the dam level and in a high risk area, according to the assessment of reports dated October 3, 2018? The precautionary principle states that if an operation has a risk that might cause severe or irreversible harm to the public or to the environment, that operation must be stopped – even if the likelihood of it happening is low. So why wasn’t this applied? Since standards clearly hadn’t improved over the last three years, how can Brazilians now trust the safety of other dams? Considering the normal course of an accident, why did sirens allegedly fail to work when the dam collapsed to alert employees and the local community to evacuate?  Vale must account for its operational failure with the same gravity as is the standard in developed countries. The Canadian mining company Imperial Metals is still suffering from the environmental liabilities of the Mount Polley mine disaster in Canada.  There is precedent for CEOs and executives to be forced to resign after serious errors, such as the former CEO of BP, Tony Hayward, who resigned after the Deepwater Horizon disaster that killed 11 people in 2010. Schvartsman’s temporary resignation sends an unclear message about the company’s commitments to the lives of its employees and the communities it operates in.    Vale’s next CEO will need to go beyond Schvartsman’s rhetoric and consider what stricter operating procedures might be necessary. Otherwise, it could only be a matter of time before the next “accident”. There are more than 50 similar dams still functioning under Vale’s operations in Minas Gerais state alone that could be another tragedy waiting to happen. Vale cannot operate at the expense of lives and environmental destruction. Brumadinho can still be a turning point in the history of the company and show it is able to learn to better avoid accidents and tragedies. It may still transform itself into a more responsible company. For that, however, it will need to urgently embrace change as never before."
"Light is among the fastest growing human-made pollutants of the natural environment. Numbers of outdoor lights are growing rapidly across the world, far outpacing general population growth. We know light pollution costs too much, wastes energy and spoils our view of the night sky.  But these are all human concerns; what does light pollution mean for plants and animals? Even academics ignored what light pollution does to the ecosystem until the turn of the century. A Google Scholar search for scientific articles concerning “light pollution” and “ecology” prior to the year 2000 returns no relevant search results. The study of “Ecological light pollution” didn’t really take off until a 2004 paper of that name by urban wildlife experts Travis Longcore and Catherine Rich. But, ten years on, major gaps still remain in our knowledge about the impact of light on most species. From the limited science we have, it appears that artificial lighting has the potential to disrupt the behaviour and physiology of a huge number of species and even entire ecosystems. For example, by disorienting sea turtle hatchlings, affecting the choice of nest and breeding success of birds, or detritivore activity in freshwater streams. But in truth, we just do not fully know – yet. Our lack of knowledge is confounded by other factors such as the wide variety of different lights, each with their different spectral properties and intensities. Older lights tend to emit a narrow band of orange light whereas the widely hailed “light of the future”, the Light-Emitting Diode (LED), has a broad spectrum of white light. These differences make generalisations between different studies particularly hard.  Similarly, light influences behaviour in lots of different ways. Some animals are most active during daylight, others are nocturnal or crepuscular (active at twilight) – the same light that annoys a diurnal animal trying to get some rest may also ruin the dinner plans of a nocturnal forager. Even age can make a difference, thus the way an organism will respond to light is often difficult to generalise across a single species, let alone a genus or family. To date, 86 studies examine the impact of artificial light on organisms, 70 of which have been published in the past four years. Most focus on the impact of light on different species of plants or animals – birds, invertebrates and mammals (mostly bats) are particularly popular – with the remaining quarter looking at ecosystems and ecological impacts in a more general tone.  My research has examined the impact of artificial light at night on the behaviour of an economically and environmentally valuable freshwater fish, the Atlantic salmon (Salmo salar). For fish, light is thought to alter the natural pattern of light and dark in their ecosystem, affecting their daily behaviour, and that of their prey.  Research such as this is important, as light pollution is exploding and so little is known about what this will mean for ecosystems and organisms. Lighting itself is changing to be more energy and cost efficient, a move that could increase the impact on animals. And then of course there is global climate change, a variable that, to date, has not been accounted for.  Given these changes that are looming on the horizon, it becomes all the more pertinent that we begin to fully engage with the subject of ecological light pollution. The enormous gaps in our knowledge that can only be filled through investment of time and money in researching the impact of lights old and new, on species big and small. It might be annoying for us to see so much energy wasted on illuminating the night sky, ruining our view of the stars. But for some animals, this same pollution could be life-threatening."
"Australia’s greenhouse gas emissions have dipped slightly on the back of new clean energy and a sharp fall from agriculture due to the drought, but the decline was almost entirely wiped out by surging industrial pollution. Official data released on Monday revealed national emissions were down 0.3% in the year to September.  Emissions from electricity generation fell 2% (3.6m tonnes of carbon dioxide) as the now-filled renewable energy target boosted invested in solar and wind power. Pollution from agriculture fell even further – 4.1m tonnes, or 5.8% – due to the impact on livestock of the historic eastern seaboard drought and north Queensland floods. But fugitive emissions, released during coal and gas extraction, were estimated to have jumped 6% (3.3m tonnes). Pollution from stationary energy – which includes the manufacturing, construction and commercial sectors – was up 2.6% (2.6m tonnes). The biggest leap was in the surging liquefied natural gas export industry, now worth $50bn after a wave of investment, particularly in Western Australia. LNG emissions were up 16.9% (6.3m tonnes). As Guardian Australia has reported, the rise in industrial pollution has occurred as the government has allowed big polluters to increase emissions limits under a scheme known as the “safeguard mechanism”, which was promised in 2015 to prevent rises above business-as-usual levels. National emissions across the year were estimated to be 530.8m tonnes, nearly 1% lower than they were in 2000. The government’s 2020 target over that timeframe is a 5% cut. Emissions are down about 1.3% since the Coalition was elected in 2013, but just 0.4% since 2014, the year the national carbon price was repealed and replaced by then prime minister Tony Abbott with the emissions reduction fund. Climate pollution fell about 14% when Labor was in power under Kevin Rudd and Julia Gillard. The data was released in the wake of the Labor leader, Anthony Albanese, recommitting the party to a target of Australia having net zero emissions by 2050. It has not yet explained how it would get there. Every Australian state, the Business Council of Australia and more than 70 countries have supported a goal of net zero emissions by 2050. Asked on Monday if the federal government would make the same commitment, the minister for emissions reduction, Angus Taylor, said the world had agreed “to get to net zero emissions in the second half the century” under the Paris climate agreement but the Coalition would not adopt a target that was unfunded or not backed by a plan. The government is expected to soon release a “technology investment roadmap”, but has confirmed few details. It has also received a report from a panel led by the businessman Grant King, which was last year asked to advise on new ways to cut emissions. “We’ve got to do our bit, and that’s why we’ll be focusing on technology,” Taylor said on Monday. In a statement on the new emissions data, Taylor emphasised emissions were down 13% since 2005, the year against which Australia’s Paris target for 2030 (a 26-28% cut) is measured. Labor’s climate spokesman, Mark Butler, said the fall in emissions was “little more than a rounding error”, and the government’s policies were “hopelessly inadequate”. “There was no reduction in emissions in the quarter to September 2019 and annual emissions only reduced by a pitiful 0.3%,” he said. “Scott Morrison is failing to protect Australians from the dangerous realities of climate change.” Taylor said the Australian economy was “steadily decarbonising” but growth in exports was putting upward pressure on emissions. In particular, he said, LNG exports were reducing global emissions “by displacing more emissions-intensive fuels overseas”. Guardian Australia last year asked for the evidence that Australian LNG was displacing coal-fired power in Asia to the extent the government has claimed. It was not made available. A government report at the time suggested gas would be increasingly competing with zero-emissions nuclear and renewable energy in Japan, Australia’s biggest LNG market. Taylor said emissions from export industries had increased 54% since 2005 levels, but per capita pollution and the emissions intensity of the economy – the amount released relative to production – were at the lowest levels in nearly three decades. “The value of our exports has increased by $83bn since September 2013, reflecting the Coalition government’s good economic management,” Taylor said."
"**Some dental practices could close as a result of the pandemic, the British Dental Association (BDA) Scotland and the Royal College of Surgeons of Edinburgh (RCSEd) has warned.**
The number of patients dentists can treat has been cut by rules to stop the virus from spreading.
The RCSEd has warned that for NHS work the government was ""providing very little towards treatment"".
But the Scottish government said the sector has had ""unprecedented support"".
Prof Philip Taylor, of the Faculty of Dental Surgery at RCSEd, said that for NHS work the government was ""providing very little towards treatment"".
Fees are complex and can vary with each patient, but a dentist who fits a new metal crown for a back tooth on the NHS may receive a fee set by government of Â£80 to Â£100 - with the patient paying most of that.
However, if the work was done privately, the RCSEd said the dentist could charge Â£300 to Â£400.
Dentists whose businesses rely on NHS patients have warned that they are struggling to stay afloat with so few patients coming through the door.
Under strict Covid protocols, treatment rooms must be thoroughly cleaned between patients and need to be left empty for a period of time after some procedures.
This has cut the number of patients dentists can see, and those who rely on slim margins from NHS patients have warned that they may close their practices.
Prof Taylor warned that practices closing down would create an access problem for patients.
Many of those that closed were likely to be the ones most dependant on NHS fees - often in poorer parts of big cities - he added.
One dentist told BBC Scotland the number of patients his practice could see in a day had been cut from 100 to 20.
Fewer patients, he said, meant less income.
""I think we are in a dental health crisis,"" said David McColl, of BDA Scotland.
""You might see more practices saying we can't afford to run a service like this with NHS funding and turn private.
""Then you'd have a two-tier system where if you can afford it you can get the treatment, if you can't, you won't.""
He added: ""We want to get back to treating our patients and have a fair system that is not target-driven.
""None of us wants this two-tier system - we need universal access for all"".
But the Scottish government has said it is committed to ensuring all NHS patients who want to access NHS dental services continue to receive treatment.
Scotland's chief dental officer, Tom Ferris, said that from 1 November all NHS treatments had been allowed again.
He said: ""If an NHS patient has been offered or provided with private dental treatment then their dentist must be able to confirm that all available and appropriate NHS alternative treatments have been discussed with the patient and informed consent obtained.""
Mr Ferris added that where a dentist had decided to reduce or altogether withdraw their NHS dental service, then suitable alternative arrangements would be put in place for patients.
On the funding the government had made available to dentists during the pandemic, he said: ""The government is continuing to provide an unprecedented amount of financial support to ensure the continuity of NHS dental services.""
He said payments worth up to Â£12m per month were being made to support the incomes of NHS dental practices.
""As well as deploying the Scottish government budget for NHS dental services, we are investing an additional Â£2.75m per month.""
He added that dentists had been receiving a guaranteed income of 80% of gross fees during the pandemic, which had been increased to 85% from 1 November.
PPE for dentists providing NHS care has also been provided through public money, where they previously provided this themselves.
Mr Ferris added that in December a 2.8% pay rise for dentists would be backdated to April.
''We are working to ensure NHS dental services emerge from this crisis well placed to care for the oral health of the population and will continue to work collaboratively with the dental sector in developing a new model of care,"" he added."
"On a global scale the science is settled: human emissions of greenhouse gases have already led to a rise in global temperature of more than 1°C, and the consequences are visible around the world. Already, in 2019, Australia has sweltered in record-breaking heat, while the US Midwest was hit by freezing conditions colder than Antarctica.  In the UK meanwhile, winter temperatures soared past 20°C for the first time ever. Britain is not particularly known for intense heatwaves, vicious hurricanes or snowstorms. But, in a world that is getting warmer, climate change will mean the country experiences more and more extreme weather.  To be identified as “extreme”, a weather event must significantly differ from normal patterns, be associated with severe impacts and be historically infrequent. In the UK such events include floods, heatwaves and droughts. The UK’s national weather service, the Met Office, has examined historical trends in extreme weather events and has projected how the climate may change over the 21st century. If emissions continue to increase, there is an increased chance of milder, wetter winters and hotter, drier summers. But at the same time, the country may see an increase in the frequency and intensity of hot days and heavy rainfall events.  This does not mean that every season will be warmer than the previous one. Daily and annual changes in weather patterns will continue to produce some unusually cold summers and winters, even as the climate warms.  As global temperatures continue to climb, by 2050 heatwaves similar to that witnessed in 2018 could occur every other year. By 2100, heatwaves will likely become more intense and longer lasting, with the possibility of temperatures exceeding 40°C and heatwaves lasting 50 days. The laws of physics tell us that a warmer atmosphere can hold more moisture, increasing the frequency and strength of extreme rainfall events. From year to year, rainfall patterns across the UK will still vary – but when it does rain it may fall in more intense bursts. Recent research has found that there is a one-in-three chance of record-breaking rainfall hitting parts of England and Wales each winter. That means in the future, wet winters like 2015/16 could become more common. While summers are expected to become drier overall, climate change means downpours could become heavier. Researchers at the Met Office, the UK’s national weather service, modelled the future of UK precipitation in very fine detail (down to squares of just 1.5km) and found that intense rainfall associated with severe flash flooding could become almost five times more frequent by the end of this century. The emerging field of “event attribution” is enabling scientists to better understand the drivers of extreme weather. New and stronger evidence confirms that abnormally high temperatures and associated extreme weather are indeed related to human activities. For example, the very latest attribution study found that human-caused climate change had at least doubled the likelihood of the remarkable northern Europe heatwave in 2018. Rainfall is harder to predict, as it depends more on relatively local factors such as atmospheric circulation and the availability of moisture. Many of these processes are not adequately represented in observational data or climate models. So, while a warmer world is associated with an increase in atmospheric moisture, what that means for extreme rainfall events in the UK will vary substantially from region to region. Having said that, we can use past observations and future climate model experiments to infer whether the intensity and likelihood of such events are a result of climate change. In December 2015 Storm Desmond travelled across the North Atlantic, leaving in its wave an “atmospheric river” of super-moist air. Record-breaking rainfall caused major flooding across Ireland and northern England. Researchers at Oxford University and the Royal Netherlands Meteorological Institute have shown climate change increased the frequency of a rainfall event like Storm Desmond by 59%. This isn’t the first time that extreme UK rainfall has been linked to a changing climate. By analysing the extremely wet winter in 2013/14, scientists found that climate change had made such a season 25% more likely. While there are many uncertainties in attribution studies, researchers have shown that human-caused climate change is nudging the temperatures up and increasing the odds of new extremes in heat and rainfall."
"Milk, a precious resource in many parts of the world, has become a throwaway commodity in wealthy countries. For example, in the UK, an estimated 4.2m tonnes of foodstuffs wasted per year are wasted, of which milk is in the top three. In 2012, the country disposed of 420,000 tonnes of avoidable dairy and egg waste, costing £780m; perhaps no surprise as supermarkets retail milk for as little as 44p per litre. Bottled water can be two to three times the price. Such extreme market forces lead to vanishing profit margins, so the dairy industry has had to become super-efficient: fewer, larger herds typically with several hundred, high-yielding Holstein cows capable of producing 10,000 litres per annual lactation cycle, milked by a single dairyman.  These remarkable cattle are the result of highly selective breeding over many generations using a very small pool of elite bulls capable of producing over a million offspring by artificial insemination. A marvellous exemplar of sustainable intensification and food security though application of modern science and technology … perhaps? From another perspective, the industry has boxed itself into a tight and uncomfortable corner. Modern Holstein dairy cows only last for two to three lactations, rather than the five to eight (or more) of more traditional systems.  These animals carry a heavy burden of nutritional and metabolic diseases and poor fertility, often with adverse consequences for welfare that require routine treatment with antibiotics and hormones – all justifiably of concern to the consumer. An average of 37% of Holstein cattle suffer from painful lameness, significantly more so than other breeds. The Holstein cow is arguably the world’s least fertile farm animal. Around 60% require hormonal treatments for successful pregnancy, an obvious prerequisite to the annual calving and lactation cycle. These treatments may not be harmful to consumers, but routine use of hormones for growth promotion in farm animals was banned in the EU in 1988, and consumers are ill-informed about the risks involved. The prodigious milk yield of Holstein cows involves consumption of energy and protein far beyond the levels available from pasture. They must be fed a grain-rich diet they are ill-equipped to digest, consuming in a single lactation more than their own body weight of cereals.  Feeding cereals to multi-stomached ruminants such as cattle negates much of their evolutionary advantage, namely their ability to digest fibrous plant material such as forage, green waste and by-products that are of low nutritional value to  species such as pigs, poultry or indeed humans. Importantly, cereals are potential human food and are generally produced using polluting artificial fertiliser. In addition, digestive disorders such as displaced abomasum (one of the cow’s four stomachs) were a relative rarity a generation ago but are now commonplace. In the UK, a minority of dairy farmers use alternative breeds, such as the British Friesian, Ayrshire, or the Montbéliarde. They yield up to 8,000 litres per lactation, but these cows are more robust and are fed primarily off grass or preserved forage in winter, with a modest level of concentrate supplements at peak lactation. Lameness, mastitis, metabolic disease and infertility are far less frequent than in intensively managed Holsteins.  Welfare is less of an issue and antibiotics are rarely necessary, if used at all. Many of these breeds are dual purpose, so their male calves are suitable for rearing for beef, unlike Holsteins in which males are generally disposed of at birth. Dairy cows fed in pasture also require less inorganic fertiliser for cereal production, with less associated environmental pollution.  A change to a less intensive dairy production system would be in keeping with a broader vision, laying down a number of the basic principles for sustainable livestock. One of the central tenets is reduction in consumption of livestock products by humans, with consumption focussed on quality rather than quantity. It is worth noting that milk and dairy products from grass-fed cattle are higher in N-3 fatty acids, and conjugated linoleic acids. Finally, much attention has been placed on cattle as a source of methane, accounting for the majority share of the 14.5% of man-made greenhouse gas attributed to livestock. It is difficult to predict the value of managerial change to a less intensive dairy system, but there could be other immediate environmental benefits, such as reduction in artificial fertiliser use. For example, current analysis suggests the overall environmental costs of inorganic nitrogen use in Europe (estimated at €70–€320 billion per year) outweighs its direct economic benefits to agriculture. The Pareto principle (80:20 rule) is arguably at work here: with the Ayrshire and other less extreme dairy breeds you get 80% of the yield for perhaps only 20% of the welfare cost, and maybe just 20% of the environmental costs too. Given that today’s overweight consumers perceive milk as low-value and currently throw much of it away, having only 80% of today’s supply might not be too high a price for a sustainable future with healthier, happier cows."
"

America’s goal of stopping nuclear non‐​proliferation has suffered two serious setbacks in recent years. Both North Korea and Iran appear to be pursuing ambitious nuclear weapons programs. What U.S. officials have not recognized is that such actions are a logical, perhaps even inevitable, response to the foreign policy the United States has pursued since the end of the Cold War. Washington may have tried to shape the international system with the best of motives, believing that taking action against unsavory states would both enhance America’s security and advance the goals of peace and justice in the world. But as generations of realist scholars have shown, other nations may not concede that the motives of an activist power are benign. What might seem to U.S. policymakers justifiable, even noble, behavior may seem threatening to nations that have a less than cordial relationship with the United States. 



Consider the extent of U.S. military action since the opening of the Berlin Wall. The United States has engaged in nine major military operations during that period. Moreover, in his 2002 State of the Union address, President Bush explicitly linked both North Korea and Iran to Iraq (a country with which the United States was clearly headed to war) in an “axis of evil.” In the wake of Bush’s decision to engage in pre‐​emptive regime change in Iraq, it is hardly surprising that Pyongyang and Tehran concluded that they might be next on Washington’s hit list unless they could effectively deter an attack. Yet, neither country could hope to match the conventional military capabilities of a superpower. The most reliable deterrent–maybe the only reliable deterrent–is to have nuclear weapons. In other words, U.S. behavior may have inadvertently created a powerful incentive for the proliferation of nuclear weapons–the last thing Washington wanted.



Of course, the United States must always be prepared to use military force to defend its vital interests. With regard to elective wars, however, U.S. leaders need to become more aware of the range of possible outcomes–including the risk of unintended side effects. In the security environment of the 21st century, Washington should adopt a security strategy that is both more cautious and more flexible.



North Korean and Iranian leaders noticed that the United States treats nations that possess nuclear weapons quite differently than those that do not. That is not a new phenomenon. Just six years after China began to develop nuclear arms, the United States sought to normalize relations–reversing a policy of isolation that had lasted more than two decades. U.S. leaders show a nuclear‐​armed Russia a fair amount of respect, even though that country has become a second‐​rate military power and a third‐​rate economic power. Washington has treated Pakistan and India with far greater respect since those countries barged into the global nuclear‐​weapons club in 1998.



Contrast those actions with Washington’s conduct toward non‐​nuclear powers such as Iraq and Serbia. The lesson that North Korea and Iran learned (and other countries may be learning as well) is that possessing a nuclear arsenal is the way to compel the United States to exhibit caution and respect. This is especially true if the country has an adversarial relationship with the United States. After the Iraq War started, North Korea’s Foreign Ministry declared: “The Iraqi war shows that to allow disarming through inspection does not help avert a war but rather sparks it. [Only] tremendous military deterrent force powerful enough to decisively beat back an attack … can avert a war and protect the security of the country.” Those who cheered U.S. military interventions, conservatives and liberals alike, need to ask themselves whether increasing the incentives for nuclear proliferation was a price worth paying–because greater proliferation is the price we are now paying.



And it is not only “rogue states”, but even long‐​established signatories to the Nuclear Non‐​Proliferation Treaty (NPT) that are considering whether acquiring a nuclear deterrent in the current international climate might be in their interests. That development has provoked a predictable yet unhealthy reaction in the United States. Members of the arms control community have devoted at least as much time and energy to the possibility that stable, democratic, status quo powers such as Germany, Japan, Sweden and South Korea might decide to abandon the NPT and develop nuclear deterrents as they have to the prospect that unstable or aggressive states might do so.



That misguided concern is not confined to the traditional arms control community, often considered to be more liberal. As the North Korean nuclear crisis evolved, some of the most hawkish members of the U.S. foreign policy community became terrified at the prospect that democratic U.S. allies in East Asia might build their own nuclear deterrents to offset Pyongyang. Robert Kagan and William Kristol regarded that prospect with horror: “The possibility that Japan, and perhaps even Taiwan, might respond to North Korea’s actions by producing their own nuclear weapons, thus spurring an East Asian nuclear arms race … should send chills up the spine of any sensible American strategist.”



This attitude misconstrues the problem. A threat to the peace may exist if an aggressive and erratic regime gets nuclear weapons and then is able to intimidate or blackmail its neighbors. But nuclear arsenals in the hands of stable, democratic, status quo powers are not an inherent threat to peace and stability. Kagan and Kristol–and others who share their hostility toward such countries having nuclear weapons–are guilty of embracing a moral equivalence between a potential aggressor and its potential victims.



America’s current one‐​size‐​fits‐​all non‐​proliferation policy is the international equivalent of domestic gun control laws–and exhibits the same faulty logic. Gun control laws have done little to prevent criminal elements from acquiring weapons. Instead, they disarm honest citizens and make them more vulnerable to armed predators. The non‐​proliferation system is having a similar effect. States like Iran and North Korea are well along the path to becoming nuclear powers, while their more peaceful neighbors are hamstrung by the NPT from countering those moves. The United States must therefore extend its nuclear umbrella, placing America at greater risk, to guarantee the security of allies and clients. Washington’s own non‐​proliferation efforts should focus on delaying rogue states in their quest for nuclear weapons, not on causing problems for peaceful states that want to become nuclear powers to deter unfriendly actors in their neighborhoods.



True, such a changed attitude on the part of the United States might well lead to greater proliferation in some regions. That prospect, however, should not be extrapolated to the nightmare vision of a spiraling arms race and endless proliferation. Nations will make their decisions about whether to become nuclear powers based on a host of factors, and it is not a cost‐ or risk‐​free decision–financially, politically or diplomatically.



U.S. policymakers must rid themselves of the notion that all forms of proliferation are equally bad. The United States should concentrate on making it difficult for aggressive or unstable regimes to acquire the technology and fissile material needed to develop nuclear weapons. Even then, American leaders should keep in mind that, at best, U.S. actions will likely only delay, not prevent, such states from joining the nuclear club.



Still, delay can provide important benefits. A delay of only a few years may significantly reduce the likelihood that an aggressive power with a new nuclear weapons capability will have a regional nuclear monopoly and thus the ability to blackmail neighbors. In some cases, the knowledge that achieving a regional nuclear monopoly is impossible may discourage a would‐​be expansionist power from making the effort to begin with. At the very least, it could cause such a power to configure its new arsenal purely for deterrence rather than for aggression. Although one cannot be certain that the nuclear equilibrium that the United States and Soviet Union achieved on a global basis during the Cold War will be replicated on a regional level, the chances of such a stable environment emerging from a balance of power between regional nuclear states are better than they are in a situation where an aggressive, revisionist state enjoys a nuclear monopoly. Indeed, the recent behavior of India and Pakistan provides some cautious evidence that stable regional nuclear balances may be possible.



So while in the general sense it might be true that fewer nuclear weapons in the world (and fewer countries with nuclear weapons) would be a good thing, such logic is not necessarily absolute. Instead of assuming that all proliferation of nuclear weapons is an inherent danger that must be prevented, policymakers should analyze proliferation and assess its consequences on a case‐​by‐​case basis.



There is, of course, one area in which the United States must have a proactive policy: making it clear to new nuclear powers that transferring nuclear technology or weapons to non‐​state actors is utterly unacceptable because such groups are probably not deterrable. And there are three principal states of concern that fall under the heading of potential malignant proliferators: North Korea, Iran and Pakistan.



The ability of the United States to tolerate a nuclear‐​armed North Korea is predicated on North Korea not becoming the global supermarket of nuclear technology. An especially acute danger is that Pyongyang may provide either a nuclear weapon or fissile material to Al‐​Qaeda or other terrorist organizations. North Korea’s record on missile proliferation does not offer much encouragement that it will be restrained when it comes to nuclear materials. Perhaps most troubling of all, Pyongyang has shown a willingness to sell anything that will raise revenue for its financially pressed regime, as evidenced by the recent discovery of its involvement in the heroin trade.



Washington should communicate to North Korea that selling nuclear material–much less an assembled nuclear weapon–to terrorist organizations or hostile governments will be regarded as a threat to America’s vital security interests. Indeed, U.S. leaders should treat such a transaction as the equivalent of a threatened attack on America by North Korea. Such a threat would warrant putting all options on the table, including military action to remove the North Korean regime. It might even include using nuclear weapons in retaliation for any terrorist nuclear attack. Pyongyang must be told in no uncertain terms that trafficking in nuclear materials is a bright red line that it dare not cross.



Iran’s nuclear weapons program is a concern because of that country’s ties to terrorist groups. It is no secret that Iran provides funding, safe haven, training and weapons to anti‐​Israeli groups. But Iran has not supplied terrorist groups with chemical or biological weapons to use against Israel, so it is not clear what incentive Iran would have to give nuclear weapons to terrorists. Indeed, Israel’s nuclear arsenal (believed to consist of up to 200 warheads) serves as a powerful deterrent against Iran taking such action.



Iran’s terrorist ties were also cited by the 9/11 Commission, which implicated Iran in the 1996 Khobar Towers bombing in Saudi Arabia and cited “strong evidence” that Iran facilitated the transit of several Al‐​Qaeda members before 9/11. The potential Iran‐​Al‐​Qaeda connection is a serious one that deserves further investigation. But without clear evidence that the regime in Tehran was involved in 9/11 or is otherwise supporting or harboring Al‐​Qaeda, the United States cannot afford another unnecessary war and military occupation like Iraq. But, as with North Korea, it should be made clear to Tehran that transfer of such weapons, material or technology to terrorist groups will be justification for regime change. This policy must be strictly (and swiftly) enforced, not just with Iran but with any other country that aspires to nuclear status.



There is another point that needs to be raised. It is especially troubling that the United States has no direct ties with Iran or North Korea, the two nations that may be the next members of the nuclear weapons club. That is an unhealthy and dangerous situation. In contrast, throughout the Cold War, when the United States and Soviet Union had thousands of nuclear warheads aimed at each other and a nuclear conflict would have resulted in certain destruction of both societies, the two adversaries did not cut off relations. Despite a hostile relationship, the United States maintained an embassy in Moscow and engaged in normal diplomatic relations without ever conceding the fundamental legitimacy of the Soviet system. Eventually, the two powers developed a crisis hotline and adopted other confidence‐​building measures. By creating normalized relationships with nations with nuclear aspirations, Washington would reduce the danger of miscalculation.



Another benefit of normal diplomatic relations with rogue states is the increased likelihood of better intelligence about a country’s nuclear program. Iraq is an important example. The United States had virtually no intelligence assets on the ground to provide first‐​hand information on the status of Iraq’s nuclear program. Instead, decisions were based on second‐​hand information–primarily from Iraqi exile groups. But the reliability of that information was always uncertain. And–whether for the right reasons or wrong ones–information provided by UN weapons inspectors was not deemed accurate or reliable, and therefore was not trusted.



So, however unsavory it might seem, a more productive approach to U.S. non‐​proliferation efforts would be to develop better and closer relations with the very regimes that are cause for concern–rather than isolating them, as is currently the case. There is, after all, something to be said for the maxim attributed to Sun Tzu: Keep your friends close but your enemies closer.



Pakistan also demonstrates the limitations of current non‐​proliferation thinking. Although the current regime is considered an ally in the War on Terror and has helped capture some important Al‐​Qaeda operatives, the prospect of that country’s nuclear weapons falling into the hands of radical Islamists must be planned for. Pakistan is also a concern because so many nuclear efforts in other countries (such as North Korea, Iran and Libya) were tied to a nuclear bazaar created by Pakistani scientist A. Q. Kahn, who has been hailed as a national hero by Pakistani President Pervez Musharraf. Unfortunately, neither the traditional non‐​proliferation approach nor pre‐​emptive war is a real solution to this problem. Instead, U.S. efforts should focus on creating better security and command and control over Pakistan’s nuclear weapons to prevent them from being used by terrorists. Continuous pressure must also be exerted on Pakistan to make sure that leakage of weapons and materials does not occur. In that vein, there may be lessons to learn from the Nunn‐​Lugar cooperative threat‐​reduction program to safely secure Russian “loose nukes” that could be transferred to the Pakistani nuclear arsenal.



U.S. policymakers must think beyond traditional non‐​proliferation policy. That policy may have served us reasonably well in the past, but a rapidly changing global security environment is rendering it obsolete and potentially counter‐​productive. We can no longer cling to the NPT and all it symbolizes as the answer to all the varied problems of nuclear proliferation. Instead, we need a large policy toolbox with a variety of tools. That involves a tacit admission that the NPT has probably outlived its usefulness–at least in its current form. No policy lasts forever. In its nearly four decades of existence, the NPT has achieved some significant results, most notably in reversing South Africa’s decision to become a nuclear power and delaying proliferation in a number of regions. But the emergence of Israel, India and Pakistan as full‐​blown nuclear powers and of North Korea and Iran as threshold nuclear states shows that the traditional non‐​proliferation regime centered around the NPT is a waning asset.



Some aspects of existing U.S. nuclear policy remain viable. We can continue to rely on the ability of America’s vast nuclear arsenal to deter attacks on the American homeland by other nuclear powers. Nation‐​states have fixed addresses and leaders of those countries understand that an attack on the United States would be met with certain retaliation. America deterred the Soviet Union under Josef Stalin and China under Mao Zedong. We can deter new nuclear adversaries as well.



Although proliferation of nuclear weapons and the need to deter new nuclear powers are not welcome prospects, we must be realistic and recognize the likelihood that the number of nuclear powers in the international system will increase in the coming decades and that many of those new members of the global nuclear club will be unsavory regimes. Washington’s non‐​proliferation efforts should concentrate on delaying rogue states in their quest for nuclear weapons, not on badgering peaceful states that may want to become nuclear powers for legitimate security reasons. The problems confronting a focused non‐​proliferation policy are daunting enough without continuing the vain effort to prevent all forms of proliferation. 
"
"Many parts of the world are likely to experience above-average temperatures over the next few months, even without a natural El Niño effect, according to weather experts. The UN’s World Meteorological Organisation (WMO) said the signal from human-induced climate change was now as powerful as the natural phenomenon, which drives warmer temperatures.  It said there was a 60% chance of a neutral situation without an El Niño or its opposite, La Niña, between March and May. There was a 35% chance of an El Niño developing and 5% for a La Niña. The El Niño southern oscillation (Enso) is a naturally occurring phenomenon in the Pacific with a warming influence on global temperatures. It is also linked to heavy rain, flooding and drought. Despite the expected absence of an El Niño, the WMO forecasts there will be above-average sea surface temperatures in many parts of the world, which will lead to higher than normal land temperatures. Climate change would contribute to these conditions, the WMO said. The WMO’s secretary general, Petteri Taalas, said: “Even Enso-neutral months are warmer than in the past, as air and sea surface temperatures as well as ocean heat have increased due to climate change. “With more than 90% of the energy trapped by greenhouse gases going into the ocean, ocean heat content is at record levels. Thus, 2016 was the warmest year on record as a result of a combination of a strong El Niño and human-induced global warming. 2019 was the second-warmest year on record, even though there was no strong El Niño. “We have just had the warmest January on record. The signal from human-induced climate change is now as powerful as that from a major natural force of nature.”"
"Policies matter. Good policies lead to good outcomes, while bad policies can lead to disaster. But what about where there is no policy, or a policy that is incohesive and incomplete? We only need to look at the state of science research policy in Australia to find out. Scientific research in Australia has always suffered from political influence, because research in Australia is heavily dependent on federal government funding. But political interference in scientific research has been weaponised during the past decade of Coalition governments.  The most obvious and destructive manifestation of this political interference on the nation’s scientific research effort is the lack of a comprehensive science policy. It can be argued that some consequences of this meddling in the research effort have been this summer’s bushfire emergency and the widespread environmental destruction, mostly initiated via climate change. The scientific community and the whole of rational Australia were stunned by the decision of the Abbott government not to appoint a minister for science in 2013. This was coincident with the defunding of research into climate change and the environment. There was no science policy put forward by the Abbott government. The Turnbull government did go some way to redress this lack of policy through the national innovation and science agenda in 2015, which promised to set “a focus on science, research and innovation as long-term drivers of economic prosperity, jobs and growth”. What they came up with was Australia’s national science statement in 2017 which, in turn, gave Australia a grab bag of science policies, programs and projects. This is the closest the Coalition government has come to providing a science policy. But this is not a science policy in either scope or execution. It’s a political agenda favouring a few scientific research programs that resonate with the government’s economic agenda. There is a strong emphasis on projects that may provide an economic return in the near future and there is no scope for funding research into areas that the government does not favour. This political agenda has been carefully crafted to make it look like the government is supporting scientific research, while in reality it neither understands what science has to offer or has the desire to fund any more research than it has to. The centrepiece of the current government’s science agenda is its statement on science, technology, engineering and mathematics (Stem). It is quite clear that this agenda is closely linked to economic outcomes and returns. This reduces the Australian scientific research effort to a cookie jar of favoured projects that can make a financial return for the canny investor. It turns Australia’s scientists into the lapdogs of industry. Perhaps the saddest part of this political game is that the favoured issues and projects do genuinely deserve support and funding but they have been turned into sock-puppets aimed at distracting public attention from areas of research that have been excluded from such support. Issues such as advancing women in Stem and research into emerging technologies such as nanotechnology, robotics, quantum computing and space industries are all worthy of public money, but so too are issues of climate and environmental research. Not that Coalition governments are all that interested in research anyway. In 2018 research funding for Australian universities was cut to the lowest levels in 40 years. In that year our research effort plunged well below the OECD average. A minor bounce in the 2019 budget for research funding has not redressed this loss. If we are to judge this government by its actions, it is not interested in research and never has been. It is not hard to see why the current and former Coalition governments want to play favourites with scientific research. There has been a consistent theme of climate denial and environmental inaction from the federal government since 2013. All the while there has been a clarion call for action on climate and environmental issues led by scientists working in those areas. Seemingly immune to facts and reason (one Coalition MP recently vowed not to let his opinions be informed by evidence), why would you want to fund the nagging voice of science? No need to shoot the messenger when you can simply starve them to death. So what have been the consequences of this lack of a proper science policy for Australia? The weakened voice of our climate and environmental research scientists has been easier to ignore. With minimal monitoring and reporting on the environment and next to no effective research into the effects of climate change in Australia, the early troubling signs of the unfolding catastrophes were largely unobserved and unreported. We drifted into the unprecedented drought, the drying of the Murray-Darling, the catastrophic bushfires and the ferocious flooding events, untroubled by any warnings from a research community that had been effectively silenced. A wise government would have a broad-based science policy backed up by funding support that is at least on par with the world average. Since we have neither of these, we can only question the wisdom of the current federal government. • Associate Professor Paul Willis is adjunct in palaeontology at Flinders University. Former director of the Royal Institution of Australia and long-time science presenter with the ABC"
"

Recently, a World Wildlife Fund press release was picked up by Reuters. “Himalayan glaciers are among the fastest‐​retreating glaciers globally due to the effects of global warming,” the advocacy group announced.



WWF timed its press release for a two‐​day Energy and Environmental Ministerial conference in London, where the United States was (predictably) criticized because it won’t commit economic suicide by adopting the Kyoto Protocol on global warming.



This is one of those repeating news stories, like “Strife in Haiti” or “Irish unrest.” It goes like this. “The (glaciers, polar bears, butterflies) of (anywhere) are in dramatic decline because of global warming. Unless the (U.S., U.S., U.S.) signs on to the Kyoto Protocol, their continued decline is assured.” 



Here’s another broken record. “It appears that the (U.N., World Wildlife Fund, New York Times) forgot to check the temperature histories where the (ice, polar bears, butterflies) are in decline, and the (U.S., U.S., U.S.) isn’t going along with counterfactual nonsense produced by agenda‐​driven environmentalists.”



We offer this evidence. WWF is especially interested in the Gangotri glacier, in the Indian Himalayas. The glacier is retreating an average 75 feet yearly.



Glaciers are in steady state when the annual snowfall and summer melting rate are roughly in balance. Actually, this is rare. When glaciers melt too much in the summer, they retreat. And if it snows more in the winter than normal, they advance.



The United Nations Intergovernmental Panel on Climate Change (IPCC) publishes historical temperature records around the planet. They are averages for 5 X 5 degree latitude/​longitude rectangles. They used these somewhat large areas so that, in general, many local records are averaged up to form a reliable regional picture. The Gangotri Glacier, which feeds the Ganges River, is in the 30–35N, 75–80E box.



High‐​altitude glaciers melt during the summer. The IPCC has June‐​August temperatures for the Gangotri region back 1875. The net decline in temperature over the last 130 years is striking. In fact, at 1.2 degrees (C), it is one of the largest summer coolings on Earth. That’s right: cooling. In contrast, the temperature for the Northern Hemisphere as a whole increased 0.8 degrees during the same period.



Still, no one doubts the Gangotri glacier is receding. It was expanded far beyond where it is today when the cooling was first noted more than a century ago. Temperatures reached their low in 1990 and have popped up a bit, to the long‐​term average for the last 130 years. Perhaps this has something to do with Gangotri’s recent more rapid retreat.



But that it has been in such a decline as overall century‐​scale temperatures have cooled tells us much about the long‐​term fate of glaciers away from polar regions: They are relics of the Ice Age, destined to melt.



Another place with an ice history that resembles Gangotri is our own Glacier National Park in Montana. There were 147 glaciers in the park 150 years ago, near the start of the Gangotri temperature record. Today there are only 37. What happened to summer temperatures? Unlike Gangotri, they didn’t cool. But temperatures remained fairly constant, with no significant warming since records began in 1895.



Most scientists think the mid‐​19th century marks the end of a multicentury period known as the “Little Ice Age,” though a small but vocal core of skeptics maintain a view known as the “Hockey Stick” history — one in which temperatures do not change for nearly a millennium and then shoot up in the last 100 years, producing a graph that indeed resembles a hockey stick. This view has been pretty much marginalized in a number of papers in scientific literature over the last year.



Indeed, glaciers went into retreat at the end of this cold period. Gangotri is even more tenuous, receding even as local temperatures continued declining.



Incidentally, the Northern Hemisphere’s largest ice mass — the Greenland icecap — is in retreat in the southern part of the island, where temperatures also show a substantial net cooling for the last 75 years.



All this leads to an obvious conclusion. Southern Greenland, Glacier National Park and the Himalayan glaciers are on their way out, with little or no nudging needed from people. They’re relics of the Big Ice Age that ended 11,000 years ago. It’s too bad, though, that in the fight to hype global warming, the truth is also rapidly becoming another relic.
"
"

Global dimming and brightening: A review
Martin Wild
Institute for  Atmospheric and Climate Science, ETH Zurich, Zurich, Switzerland
There is  increasing evidence that the amount of solar radiation incident at the  Earth’s surface is not stable over the years but undergoes significant  decadal variations. Here I review the evidence for these changes, their  magnitude, their possible causes, their representation in climate models, and  their potential implications for climate change. The various studies  analyzing long-term records of surface radiation measurements suggest a  widespread decrease in surface solar radiation between the 1950s and 1980s  (“global dimming”), with a partial recovery more recently at many  locations (“brightening”). There are also some indications for an  “early brightening” in the first part of the 20th century. These  variations are in line with independent long-term observations of  sunshine duration, diurnal temperature range, pan evaporation, and,  more recently, satellite-derived estimates, which add credibility to  the existence of these changes and their larger-scale  significance.
Current climate models, in general, tend to simulate these  decadal variations to a much lesser degree. The origins of these  variations are internal to the Earth’s atmosphere and not externally forced  by the Sun. Variations are not only found under cloudy but also  under cloud-free atmospheres, indicative of an anthropogenic  contribution through changes in aerosol emissions governed by economic  developments and air pollution regulations. The relative importance of  aerosols, clouds, and aerosol-cloud interactions may differ depending on  region and pollution level. Highlighted are further potential implications  of dimming and brightening for climate change, which may affect  global warming, the components and intensity of the hydrological cycle,  the carbon cycle, and the cryosphere among other climate  elements.
Received 14 November 2008; accepted 10 March 2009; published 27  June 2009.
Citation: Wild, M. (2009), Global dimming and brightening: A  review,
J. Geophys. Res., 114, D00D16, doi:10.1029/2008JD011470.
I found this passage that parallels a lot of what I’ve been saying about data quality:
The assessment of the magnitude of these SSR (surface solar radiation) variations faces a number of challenges. One is related to data quality. Surface radiation networks with well-calibrated instrumentation and quality standards as those defined in BSRN [Ohmura et al., 1998] need to be maintained on a long-term basis and if possible expanded into underrepresented regions (see Figure 1b).
However in this figure, citing CRU surface temperature, he likely doesn’t understand what data quality issue might have contributed to the trend from 1960-2000

One of the effects of urbanization is the compression of the diurnal temperature variation. I recently was able to demonstrate this between two stations in Honolulu. One is in the middle of the Airport and had a sensor problem, the other was in a more “rural” setting about 4 miles away. Note how the ASOS station at the airport has an elevated temperature overall, but that the biggest difference occurs in the overnight lows, even when the ASOS sensor giving new record highs was “fixed”:

Urbanization affects Tmin more than Tmax. For example, here’s the nighttime UHI signature of Reno, NV that I drove as a measurement transect using a temperature datalogger:

Click for larger image
Even several hours after sunset, at 11:15PM, the UHI signature remained. The net result of  urbanization is that it increases Tmin more than Tmax, and thus minimizes the diurnal range, which we see in Wild’s diurnal range graph.
Even the IPCC misses it:

Wild probably has no idea of this type of issue in the CRU data, but again it speaks to data quality which he seems to be keen on. He’s looking for a global solar signature in temperature data, something Basil Copeland and I have done, to the tune of much criticism. The signature is there, but small. But, when diurnal temperature variation is looked at, any solar signature is likely swamped by the urbanization signal. I’m not saying there is no solar component to what Wild is looking at, but it seems fairly clear that UHI/urbanization/land use change plays a significant role also.
Even rural stations can be affected by our modern society, as Dr. John Christy demonstrated in California’s central valley:
A two-year study of San Joaquin Valley nights found that summer nighttime low temperatures in six counties of California’s Central Valley climbed about 5.5 degrees Fahrenheit (approximately 3.0 C) between 1910 and 2003. The study’s results will be published in the “Journal of Climate.”
The study area included six California counties: Kings, Tulare, Fresno, Madera, Merced and Mariposa.
While nighttime temperatures have risen, there has been no change in summer nighttime temperatures in the adjacent Sierra Nevada mountains. Summer daytime temperatures in the six county area have actually cooled slightly since 1910. Those discrepancies, says Christy, might best be explained by looking at the effects of widespread irrigation.
Wild’s study is a very interesting  and informative paper, I highly recommend reading the entire paper here (PDF 1.4 mb)
h/t and sincere thanks to Leif Svalgaard for bringing this paper to my attention.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e955ba37c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The Government of Jersey has been given the power by the States Assembly to make wearing masks in shops mandatory.**
Members also approved laws to limit the size of gatherings, as part of updated Covid-19 regulations.
Currently the wearing of masks is not legally enforceable, with the government continuing to promote them in guidance.
The regulations have not created new rules, rather they have set the terms of possible restrictions.
The maximum penalty for individuals breaching any mask or gathering law would be set at Â£1,000.
Children under 12 years old would not have to wear a mask, along with people exempt for health or disability reasons, according to the regulations.
If a law requiring wearing of face coverings is introduced, it would apply to specific workplaces where a member of the public is present as a customer.
The regulations also allow orders to oblige businesses to collect personal data to aid contact tracing and to refuse service to those not wearing masks.
Any restrictions on the size of gatherings will only apply to groups of 10 or more people.
Visiting people's homes in Jersey was banned during the first wave of the pandemic before the ban was lifted in May, although public health guidance has discouraged meeting indoors since."
"
Share this...FacebookTwitterThe eruption of Grímsvötn on May 20, 2011 has produced a cloud of volcanic ash that shot up over 50,000 ft and has drifted over parts of Europe closing a number of major airports and creating air traffic havoc.
Now the online Der Spiegel reports today that another volcano, Hekla, is on the verge of exploding as well. Satellite altimetry measurements show that the mountain has swollen – more than it did right before it exploded the last time in 2000. Der Spiegel writes what scientists have found:
On the Hekla volcano they have discovered a 20 km wide swelling. Magma has risen up under the ground and is pushing the ground up, reports a group around Benedikt Ofeigsson of the University of Iceland in Reykjavik in the magazine ‘Journal of Geophysical Research‘. An eruption soon is ‘very likely,’ confirms vulcanologist Birger-Gottfried Lühr of the PotsdamGeosciences Research Centre.”
Hekla is right now under extreme pressure.
Is Hekla next? ‘If it keeps its rhythm of the last decades, then it is now due,’ says Lühr.
Instruments on the mountain show that Hekla has swollen up more than it’s last eruptions in 2000 and 1991.”
According to Wikipedia, during the Middle Ages, Icelanders called the volcano the “Gateway to Hell.” In January 2010 there were reports of patches near to the summit not covered with snow. Hekla had massive eruptions in 5050 BC, 3900 BC, 2310 BC and 950 BC, which threw about 7.3 km of volcanic rock into the atmosphere, placing its Volcanic Explosivity Index (VEI) at 5. This would have cooled temperatures in the northern parts of the globe for a few years afterwards.
After being dormant for 250 years, Hekla erupted again in 1104 AD with os VEI of 5. Hekla has also erupted every 10 years since 1970. Some eruptions had a VEI of 3, which sent ash 15 km into the atmosphere. If the scientists today are right, it could be a disruptive year for European air travellers.
Share this...FacebookTwitter "
"Our mild winter may have helped the UK’s legal problems with nitrogen oxides. In 2016, Department for Transport scientists made a chance finding when testing 38 diesel cars following the Volkswagen scandal. They found that exhaust emissions were far worse when vehicles were tested outside in the cold rather than indoors in the warm, as required in the legal tests.  Drive your diesel car on a cold day and the exhaust clean-up system may be turned down or even off. Manufacturers argued that this protects components from damage, but technical experts disagreed. Last year data was published from snapshot measurements on 200,000 cars as they travelled through light beams placed along UK roads. The average diesel car produced nearly three times more nitrogen oxide pollution at 0C than at 25C and this depended on the clean-up technology used. BMW was the best and General Motors was the worst. Future diesel cars will have to pass on-road rather than just laboratory tests and early evidence shows that new diesels emit less pollution than the old ones that they replace. The loss of public confidence in diesel means that more petrol cars are being sold. These produce little nitrogen oxides whatever the weather. These two factors suggest that nitrogen dioxide pollution should improve."
"One of China’s major genetically modified food projects is now to all intents and purposes dead and buried. The expiry on August 17 of the biosafety certificates issued to strains of GM rice developed in the labs of Huazhong Agricultural University, Wuhan, signals a major blow to the fight to establish GM food in China. The contrast with the rest of the world could hardly be starker. In the UK, for instance, the country’s first genetically modified crops are almost ready for harvesting following a landmark trial. The production of a unique crop of “false flax” camelina – one spliced with genes capable of producing omega-3 fatty acids normally found only in fish – has been hailed as a milestone in the country’s journey towards the creation of GM food.  For China, it didn’t have to be this way. For more than two decades, with government support, Chinese scientists have been frontrunners in researching and developing genetically modified organisms (GMOs). The country has become a global leader in agro-biotechnology, driven by the inescapable need to feed a population of 1.3 billion and by a concomitant determination to increase yields, improve nutrition, ensure food security and tackle the problems caused by pests, diseases and pollution. Now, the head start it enjoyed over many nations has been eroded. Where once there was significant momentum, progress is slowing to an agonised crawl. Scientists who were formerly championed as trailblazers are now habitually condemned as agents of a foreign power. How did this happen? China was actually more cautious than most in embracing genetically modified foods. It issued its first regulations on genetic engineering in 1993, and three years later it followed up with detailed measures on the implementation of biosafety in researching, testing and commercialising agricultural GMOs. In 2001 a high-level State Council regulation established an administrative office for agricultural GMOs, which draws policymakers and experts from across ten government agencies. GM crops have to pass a five-stage process leading to biosafety certification. This requires evaluation in a laboratory setting and during various phases of field testing. Crops have to undergo internationally accepted risk-assessment procedures and are also benchmarked against their non-GM counterparts. It is a commendably thorough protocol – one intended to balance advances in research with broader concerns of both scientists and others. Five years ago, with considerable fanfare, the process culminated in the Chinese government granting biosafety certification to GM rice – at least for research purposes. Unlike the US and Iran, which stopped short of commercialisation, China was widely expected to soon put GM rice on the country’s dining tables. Those very same biosafety certificates have now expired. It will be interesting to see what – if anything – happens now, not least in light of the agriculture ministry’s recent pledge that the illegal growth or sale of any GM grains will be met with “zero tolerance” and “harsh punishments”. In any event, what is becoming ever clearer is that China’s enthusiasm for GM food, in direct contradiction to that of many of its economic rivals, continues to wane. Given their respective trajectories in this regard, it is tempting to ask what a nation like the UK has that China doesn’t. It would be fanciful in the extreme to suggest the regulatory environment is conspicuously slacker; it would also be unrealistic to claim the UK’s needs are somehow more pressing. We would perhaps be better served by reversing the question and asking what China has that the UK doesn’t; and the answer is an anti-GM movement whose power and influence are more than matched by its fervour and sheer, undiluted paranoia. The debate over GM food has been aired around the world. Often it has been one of science versus emotion and even politics. But nowhere have the arguments against been more outrageous – or, almost incredibly, more successful – than in China, where the gainsaying has now even descended into a very public slanging match between the agriculture ministry and the army. In the eyes of its Chinese opponents – whose ranks include Major General Peng Guangqian, who also happens to be deputy secretary general of the National Security Policy Committee – GM food is not merely a cause of cancer and a source of infertility. It is also a grand Western scheme. It is a monumental, supremely devious plot to annihilate the Chinese and other people of colour. It has been created by Monsanto, an American firm, with the backing of the Pentagon and leading private foundations in the US, to control the global food chain. Some critics seek to extend this extraordinary warped logic. They contend that many of the Chinese scientists who have worked on GM rice are American-educated; that they must therefore be in league with the architects of the aforementioned plot; that they must therefore be dedicated to undermining China’s national interests and security; and that they must therefore be – to quote one enraged blogger – “traitors and lackeys”. Thus a recent China Central Television investigation screened late last month saw fit to implicate Professor Zhang Qifa, of Huazhong Agricultural University, in an alleged conspiracy to spread the strains of GM rice developed in his laboratory. It might be ludicrous, but it is effective. Irrational pronouncements have been deemed to carry more weight than empirical evidence. Anti-Western sentiment has been judged more convincing than a raft of studies endorsing the merits of agro-biotechnology. Government support for GM food is dwindling fast, and it seems safe to say that the opportunity to commercialise GM rice – and with it the chance to help address some of China’s most urgent problems – is all but gone. You do not have to be a scientist to see that reason has suffered a crushing defeat. "
"
Share this...FacebookTwitterOur friend Rudolf Kipp at the German Science Skeptical site here has a shocking report on a callous Der Spiegel piece that appeared yesterday. I’ve translated Rudolf’s report in English with his permission.
===================================================
Organic Foods Are Killing – This Time in Africa
by Rudolf Kipp
Children are dying in Africa – just so that German organic food shops can keep their store shelves well-stocked. This is what author Laura Koch writes in Der Spiegel Online in a story titled “How the Malaria Wonder-Weapon Drives Farmers Into Poverty”.
In the Spiegel report’s introduction, the author writes:
Malaria transmitted by mosquitoes kills hundreds of people in Uganda daily – that’s why the government there uses the insecticide DDT. But the use of the pesticide has grave consequences for people living out in the countryside: Suppliers of organic foods are no longer able to sell their products, and now they are threatened by abject poverty.”
These introductory words alone bring up 2 fundamental questions. Firstly: Is the planting of organic foods the only possibility that Ugandan farmers have in providing for their livelihoods? Secondly: Since hundreds of people can be saved from death by DDT daily, how many Ugandans are we willing to sacrifice in order to allow a few farmers to produce crops that meet the directives of some European and US-American organic food associations? Just one note on the side: Half of the malaria-caused deaths are small children.
The eco-movement’s downfall
Within enlightened circles, the ban of DDT pushed by environmental groups and government bodies since the early 1960s has become known as the eco-movement’s downfall. Already in the early 1970s it was clear that the horror stories connected to the use of DDT were scientifically unfounded. Nonetheless, efforts were made to ban the substance globally. Eventually bans were enacted through various instruments involving political and economic pressure.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




One can rightly criticize the massive agricultural use of DDT that took place in the 1960s. From this time it was possible to detect traces in the fat tissue of animals in the Arctic and Antarctic. But these times are long gone. Substitute substances have been found for use in agriculture and are much more effective, and they break down and dissipate much more quickly.
Wonder weapon DDT
When combating the anopheles mosquito, the main transmitter of malaria, the case is different though. Here DDT remains by far the most effective and the most economical weapon against the disease. And only very small amounts are needed compared to the amounts used for crop protection. Here it is already enough to spray the walls of homes located in risk areas with a trace amount of DDT only twice a year. Mosquitoes that remain on the wall die promptly.
Of course there also exist alternatives to DDT when combating malaria. But none are as effective, and, what is particularly crucial in the impoverished countries of Africa, none is as cost-effective. Mosquito nets, which are always propagated by aid organisations and environmental groups, function poorly and only when one sleeps under one. Anyone who goes outside during twilight hours still gets exposed to the lethal infection. Carbamates are also as effective as DDT, but are 4 to 6 times more expensive and must be sprayed many times more often. Organo-phosphates cannot be sprayed inside homes and apartments because of their hazard. And the often-mentioned wonder weapon of synthetic pyrethroide against malaria has been shown to be considerably less effective.
Eco-imperialism 
Let’s emphasize yet one more time: When using DDT for combating malaria, the substance is no longer sprayed over large land areas. Rather, it is used in small amounts in a targeted manner. In all countries that have used DDT, the number of of people falling ill or dying from malaria has decreased significantly. Many countries that have bent to the will of aid organisations and governments of western countries have once again experienced explosions in the number of those who have fallen ill or died.
Taking all this into account, it is especially reprehensible to call for a ban of DDT just so that western countries can eat organic food that does not contain DDT also in the ultra-trace amounts. Sadly in our prosperity some among us are obviously prepared to accept the otherwise avoidable death of millions of people – all in the name of protecting ourselves from an extremely hypothetical risk. That is eco-imperialism in its purest and worst form.
Rudolph Kipp
==========================================
Share this...FacebookTwitter "
"New Zealand prime minister Jacinda Ardern won’t raise climate change when she meets with Scott Morrison in Sydney but will push for a change in Canberra’s position on forced deportations. Ardern was scheduled to hold talks with Morrison in the harbour city on Friday after briefly meeting with New South Wales premier Gladys Berejiklian in the morning. The New Zealand prime minister speaks with Morrison more than any other leader to the point where her officials often joke they don’t need to do any work “because we often just resolve things directly”. But that doesn’t mean there aren’t issues of contention, Ardern acknowledged ahead of Thursday’s flight to Australia from Fiji. New Zealand is held up as a great ally on climate issues in the Pacific while Australia is criticised for promoting fossil fuels. The divide is such that Ardern – who has enshrined into law a pathway to net-zero carbon emissions by 2050 – said she would not bother to raise the topic in Sydney. She pledged to discuss the “corrosive” issue of deportations which has seen hundreds of New Zealanders, some with limited links to their country of birth, deported from Australia after committing serious crimes. One of the most high-profile cases involves AFL star Dustin Martin’s father who was deported to New Zealand after living in Australia for 20 years because of his links to the Rebels bikie club. Canberra has been unrelenting when it comes to the policy and New Zealand is realistic as to whether any concessions may be forthcoming. Overall, however, Ardern knows the special bond between the regional neighbours means “we’re countries that lean on one another in times of need”."
"**Councils in Lancashire have written to the prime minister's interim chief of staff to criticise ""unfair"" levels of financial support received during lockdown.**
In the letter to Sir Edward Lister, seen by BBC News, leaders accused the government of ""breaching"" an agreement reached when the area entered the top level of restrictions in October.
They also said northern councils had received less than the south, despite a longer period of restrictions.
BBC News has approached the government for comment.
Lancashire Councils say an agreement was reached that Â£30m of additional support for businesses would be made available by government, when the area entered the highest level of coronavirus-related restrictions under the previous tiered system in mid-October.
In the letter to Sir Edward, the 15 council leaders said the agreed funding was intended to cover a ""28 day surge period"", during which the area would be under heightened restrictions.
But on 31 October, Boris Johnson announced the whole of England would be entering a second period of wider lockdown measures from 5 November to 2 December - with the same level of financial support extended to all English councils as a one-off grant.
Lancashire's leaders said it was ""unfair"" and a ""breach"" of the agreement, as areas facing four weeks of restrictions under the lockdown received the same amount as they did for seven weeks.
And they are now calling for a further Â£20m from central government to cover the additional period.
The leaders wrote: ""We have been advised that Lancashire will receive no additional funds, despite the fact that we are now essentially subject to seven weeks of restrictions.
""This is inherently unfair and divisive - it is likely that London and other areas, predominantly in the south, will receive the same compensation for 4 weeks of restrictions.""
They added: ""This is an unacceptable position for us and the people of Lancashire and a breach of the agreement we made with the government through the negotiations.""
The government's been clear that this time there'll be no regional negotiations over which restrictions apply where.
No showdowns with local leaders of the type we saw play out in Greater Manchester.
But that doesn't mean there won't be regional pushback.
Some Tory MPs are already warning about higher tiers being applied to large areas, regardless of some local spots being low risk.
And then there's the financial fight too.
Local authorities across the board have long warned of budget shortfalls, despite several billions of pounds of extra funding the government's put in during the pandemic.
In particular councils in the areas hardest-hit by this virus, many in the midlands and north of England, say funding packages aren't enough to help them support communities which have been living under restrictions for some time.
Ministers have pointed to a plethora of support schemes for individuals and businesses, and there will be more cash for councils in the highest tiers.
But this could still be a flashpoint when the regional rules are announced, and a government that's promised to address regional inequality can't afford to leave any area feeling hard done by.
Some parts of Lancashire, such as Blackburn with Darwen, have been under extra restrictions since August.
Geoff Driver, the Conservative leader of Lancashire County Council, said extra funding was ""vital"" to businesses in his area.
""Lancashire's 15 leaders have set aside political differences to make the case to government to provide us with this support,"" he said.
""It is really disappointing that we haven't had this, not least because Lancashire has been ready, willing and able to work with central government to support our people and businesses throughout this pandemic - and we remain so.
""Looking forward, we don't know yet what tier or tiers Lancashire will be in, but it is clear that some if not all of the county will be facing significant restrictions and our businesses will continue to require support.""
Mohammed Iqbal, the Labour leader of Pendle Borough Council in Lancashire, said: ""While I recognise we entered national lockdown before the end of the 28 day period, Lancashire was under additional restrictions without additional support for more than two weeks.
""To date, we haven't received a penny.
""The government has rowed back on its promise to fund tier three areas and it beggars belief that ministers think it fair that councils in Lancashire under seven weeks of restrictions are not entitled to more support than those areas that are under four weeks of restrictions.""
Ministers have put aside an additional Â£900 million to support councils in tier three of the new tiered restrictions system to come into effect on 2 December.
Previous business support grants for councils in tier three or under national lockdown restrictions amounted to Â£1.1 billion.
An announcement is expected Thursday on which areas will be under the highest level of restrictions once the current lockdown ends."
"**London is a city of contrasts containing leafy suburbs, urban high rises and sprawling high-streets. So should the capital be split into more localised regions with different lockdown restrictions?**
Several MPs have called on the government not to consider the capital as one area when it comes to coronavirus.
Instead, it should be split up by inner city and outskirts or borough-by-borough to better reflect local infection rates.
They argue areas with low-infection rates shouldn't be ""punished unnecessarily"" by going into a higher tier designed for suppressing large outbreaks of coronavirus.
Regions will find out which tier they are in on Thursday but the prime minster has argued against splitting up the capital.
As a whole London has an infection rate of 180.5 cases per 100,000, well below average the England national average of 217.6 cases per 100,000.
But between London's boroughs there is huge variation.
Havering, London's worst affected borough, had a weekly case rate of 360 cases per 100,000 up to 20 November.
This is more than three times higher than Camden's 97 cases per 100,000 and Richmond's 108.6.
""London is a wonderful, diverse city - it's not just one unit,"" Conservative MP for Wimbledon, Stephen Hammond said.
""We ought to look at London on a borough-by-borough or sub-regional basis,"" Mr Hammond said.
""I don't see why Londoners should suffer when the rest of the country is being looked at on a more localised basis.""
A borough-by-borough basis is also supported by Bob Blackman, Conservative MP for Harrow East.
""What we need to avoid is a situation where the borough's with the lowest infection rates are placed under the same restrictions as those with the highest infection rates and therefore punished unnecessarily,"" Mr Blackman said.
""In the outskirts of London there are only a few boroughs which have high infection rates.""
But not everyone agrees.
Despite representing one of London's least infected boroughs Sarah Olney doesn't want her Richmond constituency to potentially end up in a lower tier.
The Liberal Democrat MP said: ""I want Richmond to remain as part of an overall London tier, to discourage people from travelling here and increasing the risk of infections spreading.""
The government has not released the exact formula it will use to decide what restrictions will be applied to which region.
In London the rate of infection appears to be slowing but the latest figures show the number of weekly deaths is starting to creep up.
Deaths linked to coronavirus in London have dropped drastically from a peak in early April.
In the latest week with complete data, the week ending 13 November, the number of people who died after contracting coronavirus in the capital jumped by 42% from 87 to 124.
Prime Minister Boris Johnson has said the three-tiered regional measures will return from 2 December, but added each tier would be toughened.
Mr Johnson argued against splitting London into smaller regions with different tiers.
In the House of Commons he said despite London's diversity ""it is held together by a very dense mass transit system"" making it ""difficult to separate one bit of London from another"".
In this he's supported by the Mayor of London, Sadiq Khan, who has said smaller regions will make enforcing the rules harder.
The mayor's office argues that setting different rules for high streets, pubs and restaurants in neighbouring boroughs could push footfall between areas ""with unintended consequences"".
A spokesman for the mayor said the Met Police ""had a centrally coordinated approach"" to enforcing lockdown restrictions.
""Any difference between boroughs would not help clarity of message or rules,"" he said."
"
Share this...FacebookTwitter???
===========================================
UPDATE: Oh wait! Here’s something: http://www.focus.de/wissen/wissenschaft/wissenschafts-dossiers/tid-23443/neue-studie-bestaerkt-klimaskeptiker-die-sonne-unter-verdacht_aid_658937.html
I need to be more patient with Germans. They have a habit of thinking about things from different angles before shooting off. That’s good of course. Will write more about the FOCUS report later today.
Share this...FacebookTwitter "
"**Animals have been ""a lifesaver"" for people struggling during Covid lockdown, according to retailer Pets at Home which has seen sales rise sharply.**
Chief executive Peter Pritchard said pets had played ""an incredibly important role"" through a period of ""social loneliness"".
He added that during the early days of lockdown one of the few reasons people could go out ""was to walk your dog"".
In the six months to 8 October, Pets at Home saw revenues rise by 5.1%.
Mr Pritchard told the BBC's Today programme: ""The pet care market has been incredibly strong throughout and I think that tells you an awful lot about people's relationships with their pets and the roles that pets play in people's lives.
""It has been a lifesaver for many through this incredibly challenging period for everybody in the country.""
He added that the change in more people working from home had allowed them to get a dog or a cat. ""More people have considered having a pet because their lives have changed and they are at home more often,"" he said.
The company sells some small animals and fish but does not sell cats or dogs. It said, however, that membership of its Puppy & Kitten Club for new owners had risen by 25% during the six-month period.
Pets at Home said the first half of its financial year, which runs between April and 8 October, reflected the entire period since the week after national lockdown was implemented. Restrictions on households weighed on trade in the first quarter before a 12.7% jump in like-for-like sales in the second three months.
Pets at Home is classed as an essential retailer and has been allowed to stay open during lockdown. Total sales over the six months rose to Â£574.4m while pre-tax profit grew by more than 14% to Â£38.9m.
The company did not place any employees on furlough and said that it has actually been recruiting more staff.
However, Pets at Home's share price dropped by 7.3% to 388p in early trading.
The company warned of uncertainty because of the pandemic and said: ""At this stage, absent any escalation of restrictions, or other significant disruption to our operations, we now anticipate full-year underlying pre-tax profit to be in line with the prior year.""
Julie Palmer, partner at professional services firm Begbies Traynor, said: ""Looking ahead, with the value of the resilient UK pet market set to hit Â£7bn next year and the nation's love affair with pet ownership showing little sign of abating, chief executive Peter Prichard's optimism for the future appears well justified.""
However, she added that the retailer would be ""mindful of the dampening effect of social distancing measures in store, which may impact margins over the all-important Christmas period and into the first quarter of 2021""."
"
I’ve been very critical of statements made by Dr. Mark Serreze of the National Snow and Ice Data Center. It seems that I’m not the only one critical of his statements to the press. – Anthony
Excerpts from The Times, UK story:
Exaggerated claims undermine drive to cut emissions, scientists warn
Mark Henderson, Science Editor
Images from 2001, top, and 2007 from Philip's Universal Atlas of the World indicated a big decline in Arctic ice, used as proof of climate change
Exaggerated and inaccurate claims about the threat from global warming risk  undermining efforts to cut greenhouse gas emissions and contain climate  change, senior scientists have told The Times.
Environmental lobbyists, politicians, researchers and journalists who distort  climate science to support an agenda erode public understanding and play  into the hands of sceptics, according to experts including a former  government chief scientist.
Excessive statements about the decline of Arctic sea ice, severe weather  events and the probability of extreme warming in the next century detract  from the credibility of robust findings about climate change, they said.
Such claims can easily be rebutted by critics of global warming science to  cast doubt on the whole field. They also confuse the public about what has  been established as fact, and what is conjecture.
The experts all believe that global warming is a real phenomenon with serious  consequences, and that action to curb emissions is urgently needed.
They fear, however, that the contribution of natural climate variations  towards events such as storms, melting ice and heatwaves is too often  overlooked, and that possible scenarios about future warming are  misleadingly presented as fact.
…
“When people overstate happenings that aren’t necessarily climate  change-related, or set up as almost certainties things that are difficult to  establish scientifically, it distracts from the science we do understand.  The danger is they can be accused of scaremongering. Also, we can all become  described as kind of left-wing greens.”
Vicky Pope, head of climate change advice at the Met Office, said: “It isn’t  helpful to anybody to exaggerate the situation. It’s scary enough as it is.”
She was particularly critical of claims made by scientists and environmental  groups two years ago, when observations showed that Arctic sea ice had  declined to the lowest extent on record, 39 per cent below the average  between 1979 and 2001. This led Mark Serreze, of the US National Snow and  Ice Data Centre, to say that Arctic ice was “in a downward spiral and may  have passed the point of no return”.
Dr Pope said that while climate change was a factor, normal variations also  played a part, and it was always likely that ice would recover a little in  subsequent years, as had happened. It was the long-term downward trend that  mattered, rather than the figures for any one year, she added.
“The problem with saying that we’ve reached a tipping point is that when the  extent starts to increase again — as it has — the sceptics will come along  and say, ‘Well, it’s stopped’,” she said. “This is why it’s important we’re  as objective as we can be, and use all the available evidence to make clear  what’s actually happening, because neither of those claims is right.”
…
“In 1998, people thought the world was going to end, temperatures were going  up so much,” Dr Pope said. “People pick up whatever makes their argument,  but this works both ways. It’s the long-term trend that counts, which is  continuing and inexorable.”
Read the entire article here at The Times
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92194b10',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Going on safari in Africa offers tourists the opportunity to see some of the most spectacular wildlife on Earth – including African elephants (Loxodonta africana). Known for their complex social systems, long memory and high intelligence, this species is also threatened by poaching and shrinking habitats, so further disturbance to their precarious existence could have serious consequences. Wildlife tourism can help protect these animals and their habitat by generating income for conservation and providing stable work in local economies. Countries such as South Africa and Kenya receive two to five million visitors to protected areas each year, generating receipts of up to USD$90m. But as it becomes more popular worldwide, it’s worth remembering that we often don’t know how tourism affects the animals we observe. In Madikwe Game Reserve in South Africa, tourists stay in lodges within the park and go on safari twice a day in large, open vehicles driven by professional field guides. Over 15 months in Madikwe, we recorded how often elephants performed stress-related, vigilant or aggressive behaviours to find out whether they increased during months when there were more tourists. Vigilant behaviour could be an elephant extending its trunk into the air to smell. Stress-related behaviour included elephants bunching together or fleetingly touching their faces with their trunks – a response akin to a nervous tic in humans. Aggression was noted, for example, if an elephant charged at another, spread its ears to appear larger or hit another elephant with its tusk. We also watched the movements of elephant herds to see if they stuck around or moved away from tourist vehicles. We found that elephants were more likely to be aggressive towards other elephants in months when tourist numbers in the park were high. Elephant herds were also more likely to move away from tourist vehicles when there were more vehicles present. So, it appears that tourism does have some impact on elephant welfare – but this may not be entirely bad news. We didn’t observe an increase in stressed or vigilant behaviour in response to higher numbers of tourists, and the effect of increased aggression was small. Hunting can have much greater effects on elephants, even among those who aren’t attacked by humans. Studies which measured levels of stress hormones in elephants after they witnessed hunts or were nearby have found they increase significantly. Humans riding on the backs of elephants is also much worse for elephant welfare than observation tours. Wildlife watching, without physical contact, seems to be the better mode of tourism for elephant welfare, but it’s not without its concerns. Although these results were interesting, they are only from a single population in South Africa where driving regulations were enforced. We don’t know how elephants are affected in areas where tourists drive their private vehicles on safari unaccompanied by professional guides. We also don’t know what exactly was causing the changes in behaviour. More tourists per month meant there were more vehicles on the roads, but also more air traffic, more diverse smells and sounds and who knows what else. Parks could create refuge areas where safari tours are restricted and contact with wildlife minimised, perhaps in areas where there are fewer roads already. Tour companies could strictly enforce a no off-roading rule here and prohibit guided walks by tourists. Such refuge areas have previously been shown to have great potential in reducing pressure on elephants during times of increased stress, such as following large wildfires. Tourism can be a great conservation tool as long as it is monitored closely, and measures are taken to alleviate the potential pressures it can put on animals. If you’re ever lucky enough to find yourself on a safari, think twice about getting up close and personal with that iconic species. Instead, keep your distance and the welfare of the animals in mind."
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom._   
  
  
From the authors of a new paper just‐​published in the journal _Nature Geoscience_ comes this surprising finding:   




Tropical forests are less likely to lose biomass – plants and plant material – in response to greenhouse gas emissions over the twenty‐​first century than may previously have been thought.



A rare “not as bad as we thought” admission about the impacts of manmade global warming!   
  
  
Not only that, but based on recent findings that the true climate sensitivity is much lower than climate models emulate—findings not incorporated in new study—the results are probably still even more “not as bad as they thought” than they thought!   
  
  
Chris Huntingford from the U.K’s Centre for Ecology & Hydrology and colleagues coupled climate model projections to a land surface/​vegetation model to see how the tropical forests in the Americas, Africa, and Asia respond to changes in atmospheric conditions. Their vegetation model includes interactions between terrestrial plants and influences such as temperature, precipitation, and the carbon dioxide concentration of the atmosphere (a plant fertilizer).   
  
  
Unlike other studies which used a very limited selection of climate models and less sophisticated vegetation models, the Huntingford team found that in virtually all future simulations that the biomass of tropical forests increases over the course of the 21st century. This is a significantly different result than many previous which suggested that anthropogenic climate change would lead to, as Huntingford et al. put it, “catastrophic losses of forest cover and biomass.”   






Perhaps most interestingly, the major driver for the biomass increase is the projected growth in atmospheric carbon dioxide concentration (thanks to our use of fossil fuels). The model projected changes in precipitation had little impact on the biomass predictions and the projected increase in temperature acted to decrease the biomass (although not as much as additional carbon dioxide acted to increase it).   
  
  
Which is why the results probably get even better if there is less warming associated with carbon dioxide emissions than current generation climate models predict (new research suggest that climate models together produce about 50% more warming than they should).   
  
  
The authors are quick to mention that uncertainty abounds, as our level of understanding of forest response to changing environmental conditions is not all that high. But even given these uncertainties, the authors are confident that their results of increasing biomass are robust. Here is how Huntingford described the situation in a press release:   




The big surprise in our analysis is that uncertainties in ecological models of the rainforest are significantly larger than uncertainties from differences in climate projections. Despite this we conclude that based on current knowledge of expected climate change and ecological response, there is evidence of forest resilience for the Americas (Amazonia and Central America), Africa and Asia.



Resilience. A refreshingly honest assessment of an ecosystem response to climate change. And one that is probably a much more apt descriptor of natural systems than “delicate,” “sensitive,” or “fragile.”   
  
  
Now if only the folks in charge of assembling national and international climate impact assessments would realize (or probably more accurately, admit to) this.   
  
  
We are hard at work trying to focus their attention as we are vigorously reviewing the latest draft “National Assessment” of climate change. We will leak out particularly juicy snippets in these pages when the time seems right.   
  
  
**Reference:**   
  
  
Huntingford, C. et al., 2013. Simulated resilience of tropical rainforests to CO2‐​induced climate change, _Nature Geoscience_ , 10.1038/NGEO1741.
"
"
Bob Tisdale writes in with:
What Do You Suppose They’ve Been Doing At The National Hurricane Center This Summer?
 http://i27.tinypic.com/im1m2r.gif
Source: http://www.nhc.noaa.gov/
Bocce maybe?  Horseshoes?
UPDATE – Ryan Maue of Florida State University writes in comments:
Global (Northern Hemisphere) tropical cyclone ACE for the months May – June – July is the lowest in at least the past 30-years or more.
I, for one, am not surprised.  Continued inactivity should persist for the next few weeks until the atmosphere catches up with the radiative warming of the tropical oceans due to the season called summer.
2007 was a dud.   2008 was saved from being a record year by 2007.  2009 is behind the pace of both years.  Amazing how natural variability affects tropical cyclone formation, tracks, and intensity.  Who would have thought?
Ryan’s Tropical web page at Florida State University has this graph that shows accumulated cyclone energy (ACE) :
click for larger image
Sorted monthly data: Text File 
Note where 2009 is in the scheme of things.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e945cd97a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe European Institute for Climate and Energy (EIKE) has a story on the German Weather Service, and temperature trends for Germany, which are a good indicator for Central Europe.
Cooling in Germany has been accelerating. Source: EIKE
A few days ago the Deutscher Wetterdienst, DWD, (German Weather Service) in a press release warned that Germany’s temperature were likely to rise 2 to 4°C by the year 2100 and that action was necessary. Like the Potsdam Institute for Climate Impact Research (PIK) the DWD too has been transformed into a propaganda mouthpiece for Germany’s powerful government-driven global warming movement.
But there’s a small problem at the DWD. Like the outlier sea level projections made by the PIK, the temperature projections made by the DWD just don’t match observations. For example Germany’s annual temperature over the last 11 years has shown COOLING, and not warming, see chart above produced by EIKE.
This has led scientists at EIKE to comment as follows:
This casts the DWD’s credibility into question.”
Sure many warmists will point to the latest press release issued by the DWD, which claims that the first 6 months of 2011 in Germany have been the second warmest on record.
But if the 2nd half continues on the same path as the July trend, then the DWD, may soon start find itself comparing quarter years, or even months to milk out any warming news from its statistics.
And in the meantime, in the real world, Germans will have to wait until 2050 or even 2100 for any real warming. The DWD should take a close look at what happened to the Met Office in England back when it tried to get into the global warming gig and so began issuing stupid press releases filled with fantasies and not meteorology.
Share this...FacebookTwitter "
"The Sydney Opera House is proof that Danish architects like to do things differently. As do some Danish zoos, who kill giraffes for conservation. And now there is  a Danish proposal to create a zoo there without cages for the animals – a Zootopia where the animals will roam free and humans will observe them from various enclosures.  This in itself is nothing new or radical, but finding the right way to engage zoo visitors is vital for global conservation efforts so it’s worth examining how zoos can manage that. Bars on animal cages are controversial because they remind humans of prisons. This is anthropocentric, however – to monkeys, the bars represent climbing structures. Some barred animal enclosures, such as the gorilla housing at Howletts Zoo, are ugly to the human eye but highly functional for the occupants.  But we can question what message the human visitor takes home. It is interesting to reflect that some zoos referred to their animals as the inmates until the 1960s. Modern zoos justify their existence based on their contributions to animal conservation, research and public education. Arguably the most important role of zoos is in public education – 10% of the world’s population visits a zoo every year and no other type of institution has such potential to globally educate people about the importance of conservation.   Informal zoo education is just as important as the more formal education visitors get from listening to talks and reading signs on their visit. The impressions that a visitor leaves a zoo with are highly important. A significant factor affecting this is enclosure design, this is why good modern zoo designers pay great attention to visitor psychology. The original barless zoo was developed in 1907 by Carl Hagenbeck at  Tierpark in Hamburg. Hagenbeck, an animal collector, loved the natural landscapes where he captured his zoo animals and wished to show the animals to the public in a natural manner. The animals were kept in their enclosures by hidden moats. His zoo was a radical change from small barred enclosures and gave the public an appreciation of the animals in their wild context. It was the birth of Safari Parks in the 1960s which really brought the zoo visitor close to the animals. Here the attraction continues to be the visitors’ closeness to a lion or giraffe. Rather like a rollercoaster, the thrill is a perceived one with your car window acting as an imperceptible barrier. But the problem is in conveying an effective education message when people are driving their own car. Plus, encouraging car use goes against the overall conservation message animal institutions should be telling. Florida is home to Monkey Jungle a zoo where the humans are caged and the animals run free. It makes for an oppressive experience as a human. While it gave me an insight into what it must be like for an animal to live in a small cage, I felt it must be difficult to get over an educational message about conservation in such an environment. The Netherlands have Apenheul Primate Park, a zoo where neither animals nor their human visitors are caged, but instead they share the same woodland space. I remember my visit to this zoo as very enjoyable – it was great fun to see woolly monkeys running across the heads of the visitors and squirrel monkeys playing with children in their pushchairs. But research shows that when humans have direct contact with wild animals they tend to see them as a child and this distracts from the important conservation message the zoo is trying to impart. In the UK there is Trentham Monkey Forest, which of the three I prefer.  Here visitors and monkeys share the same space, but there are human guides to prevent direction human-monkey interactions and to educate the public about the animals. I think this institution has got it right – the visitor has the wow factor of being close to animals in a natural environment and receives good quality information about conservation. All this raises the question, what is novel about the Danish proposal for Zootopia? There are two things. One is the integration of the public’s animal viewing areas into the landscape – to make them invisible to the animals.  The second is a series of futurist mirrored globes, which will allow visitors to float down a river to see the wildlife or be carried overhead on a cable car system. I have some reservations about this concept. The mirrored globes remind me of a cult 1960s TV series that was ironically called The Prisoner. To me this concept sounds like the visitors are voyeurs and does not acknowledge their part in the natural world. Given the looming wildlife extinction crisis, it is more important than ever that zoos get the right message over to their visiting public. This requires balancing how we can learn about animals formally, as well as through the informal messages transferred through the experience of viewing animals. "
"Language, music, and art often vary between adjacent groups of people, and help us identify not only ourselves but also others. And in recent years rich debates have emerged and spawned research into culture in non-human animals. Scientists first observed chimpanzees using tools more than half a century ago. As this complex behaviour appeared to differ across different populations, researchers concluded that tool use in apes was socially learned and therefore a cultural behaviour. This was the beginning of exploring what behaviours in other species might be considered cultural as well. Killer whale pods and dolphins exhibit different dialects and use tools differently, for instance. Scientists have mostly focused on primates, however. Capuchin monkeys of Central and South America exhibit 13 variants of social customs, to take one example, while different orangutan populations vary their callsand the use of tools, nests or other objects. But no species has garnered more discussion on the presence, importance, and evolution of culture than chimpanzees. Examples of chimpanzee culture range from social customs, such as the way they grasp their hands during grooming, to how males sexually display, to the type of tools used for cracking nuts or ant-dipping. An early study argued that there are as many as 39 different behaviours that are candidates for cultural variation. This set off an eager debate about whether animals have culture or not and how we would be able to detect it.  As in humans, cultural behaviours in chimpanzees are likely critical for individuals to demonstrate community membership. If a young chimpanzee in the Tai forest in the Ivory Coast wants to signal to a peer that they would like to play around, then they build a small, rudimentary ground nest and sit in it. In most other chimpanzee groups, ground nests are mainly used for resting. But chimpanzees now face the daunting task of surviving in a habitat increasingly infested and assaulted by humans. And as their populations decline, so does their behavioural variation. In short, humans are causing chimpanzee cultural collapse.  Two of us (Alexander and Fiona) were involved in a new study which integrated data from 144 chimpanzee communities across Africa, and found the more that humans had disturbed an area, the less behavioural variants are exhibited by nearby chimpanzees. The results are published in the journal Science. The actual mechanism behind this is not entirely known. The most obvious explanation is that increased human disturbance means there are fewer chimpanzees overall. Even those that remain have to be more inconspicuous in order to survive in areas where their food and nesting sites are threatened by logging operations, their water sources are polluted by miners, and they risk being hunted for bushmeat by poachers brought into their forests by newly-built roads.  All this forces the chimpanzees to forage in smaller groups and use less long distance communication like pan hoots and drumming on tree trunks. This likely leads to a decrease in the spread of cultural behaviours, as associating in smaller group sizes lowers the chance of learning socially from one another.  Chimpanzees have also been observed to adapt to human disturbance by inventing new coping mechanisms such as eating human crops. But despite these rare adaptations, overall human activity is vastly erasing the rich behavioural diversity that now characterises chimpanzees. But, if the species is gradually merging into a single cultural entity that stretches all the way from Senegal to Tanzania – why does this matter? After all, monocultural species are not inherently problematic. There is no direct relationship between cultural diversity and species distribution, for example. Flies, rats and crocodiles are all disseminated across a vast area, and yet have not yet been described as cultural. Losing chimpanzee behavioural diversity doesn’t itself threaten the species survival.  Losing diversity could be representative of larger issues, however, not least that the species is on the decline, which is the worst scenario. For example, we don’t yet know how adaptive these behaviours are. A loss of behavioural diversity could represent compromises in how animals respond to selection pressures like changes in food availability and how they adapt to climate change. The risk is that we humans are irreversibly endangering a unique chance to discover the full extent of cultural diversity in our closest living relatives. When scientists discover a new group of wild chimpanzees it often exhibits unique behaviours that have never been observed previously, and it is hard to know what would be eradicated before we know about it.  If things continue as they are, the opportunity to study common evolutionary roots with our own species might soon be forever lost. Making protection of cultural diversity a conservation priority, which extend to numerous other species, would help to ensure the survival of our extraordinary primate heritage."
"

Back in December 2000, President Clinton and Vice President Gore were busy fellows — what with dishes to pack, furniture to ship and an election to contest. So busy were they that they neglected to read some of the fine print in a cascade of administration‐​ending paperwork. One of these was an obscure item called the “Federal Data Quality Act” (FDQA), which was dutifully signed by the president.



Put simply, the FDQA prohibits the use of junky science in the promulgation of federal regulations and laws. And, now that the new hats are in town, it shouldn’t be much of a surprise that the FDQA is being turned against the “science” of the Clinton‐​Gore team, particularly concerning the global environment.



Specifically, it has been turned against the “U.S. National Assessment of the Potential Consequences of Climate Variability and Change” (USNA), a document that breaks the cardinal rule of science: If a hypothesis doesn’t work, throw it out. The Assessment can’t pass the simplest of scientific tests.



The Assessment began with a 1997 letter from Gore to all the federal agencies, and was published 10 days before the 2000 election. If the Office of Management and Budget chooses to apply FDQA, the Assessment will be redacted down the Memory Hole soon.



And none too soon. The power of the USNA’s bad science can be seen in recent drafts of Sen. Tom Daschle’s (D-S. Dak.) energy bill, where the USNA provides the findings necessary to induce new fuel economy measures and prohibit drilling for domestic oil — all in the name of global warming and its pernicious effects on America.



In fact, that it serves as the basis for legislation is the reason that the USNA has run afoul of the law. The FDQA requires scientific objectivity and normal reproducibility of positive results in any simulation or scientific experiment that underpins prospective regulations. The Assessment has neither.



The Assessment purports to project the consequences of United States warming, produced by two computer models. One is from Canada and the other from the United Kingdom. Both models are extreme outliers. Unlike the consensus of the dozens of available models, the Canadian model produces an exponentially increasing heating. The result is a ridiculous rise of 8.1ºF in projected U.S. temperatures this century. The UK model predicts greater precipitation changes than any other model the USNA looked at.



A horde of peer reviewers–some from federal laboratories that have a track record of global warming doomsaying–told the USNA that the use of these two models was wrong. Even the greens at the United Nations agree that these models can’t be used to make local and regional climate projections with any reliability.



How does even the rankest climate amateur know the Canadian model is a joke when applied to the United States? Because it “predicts” that U.S. temperatures should have changed 300 percent more than they did in the last 100 years. In fact, neither the Canadian model nor the British can beat a table or random numbers when it comes to predicting U.S. temperature for the last century.



A climate model is nothing but a statement of scientific hypothesis: What we “think” should happen based upon currently fashionable theory. When a hypothesis doesn’t work (i.e., performs worse than a bunch of darts thrown at the Dow Jones), the ethic of science requires that it be thrown out. In this case, it means that the USNA should have used better models, or, absent a defensible model, it should have used none. If a computer simulation of climate can’t beat a table of random numbers over the United States, it borders on scientific malpractice to continue to apply it.



It wasn’t that the politically chosen leaders of the USNA didn’t know there was a problem. In fact, the USNA’s politically handpicked steering committee was so disturbed about the finding of the peer‐​reviewers that it commissioned its own study. Guess what? The USNA’s own scientists verified that the temperature models didn’t work over the United States. And yet the report went forward, now serving as the basis for the most sweeping energy legislation ever introduced in this nation’s history.



Well, anyway, all of these shenanigans are precisely what the Federal Data Quality Act was designed to prevent. The irony is that the obscure piece of legislation that slipped through when Clinton and Gore weren’t minding the store is about to throw the USNA and its global warming hysteria into its well‐​deserved dustbin.
"
"

Al Gore’s defense of global‐​warming hysteria in Sunday’s _New York Times_ has many flaws, but I’ll focus on just one whopper — where the “Inconvenient Truth” man states the opposite of scientific fact.



Gore says, “The heavy snowfalls this month have been used as fodder for ridicule by those who argue that global warming is a myth, yet scientists have long pointed out that warmer global temperatures have been increasing the rate of evaporation from the oceans, putting significantly more moisture into the atmosphere — thus causing heavier downfalls of both rain and snow in particular regions, including the Northeastern United States.”



It’s an interesting theory, but where are the facts?



According to “State of the Climate” from the National Oceanic and Atmospheric Administration, “Global precipitation in 2009 was near the 1961–1990 average.” And there was certainly no pattern of increasing rain and snow on America’s East Coast during the post‐​1976 years, when NOAA says the globe began to heat up.



So what was it, exactly, that Gore’s nameless scientists “have long pointed out”? A 2008 report from the Intergovernmental Panel on Climate Change, “Climate Change and Water,” says climate models “project precipitation increases in high latitudes and part of the tropics.” In other areas, the IPCC reports only “substantial uncertainty in precipitation forecasts.”



In other words, the IPCC said that its models predicted some increases in rain or snow — not observed them. And only in high latitudes or the tropics, which hardly describes New York or Washington, DC.



In fact, recent research actually contra dicts Gore’s claims about “significantly more water moisture in the atmosphere.”



In late January, _Scientific American_ reported: “A mysterious drop in water vapor in the lower stratosphere might be slowing climate change,” and noted that “an apparent increase in water vapor in this region in the 1980s and 1990s exacerbated global warming.”



The new study came from a group of scientists, mainly from the NOAA lab in Boulder. The scientists found: “Stratospheric water‐​vapor concentrations decreased by about 10 percent after the year 2000 … This acted to slow the rate of increase in global surface temperature over 2000 to 2009 by about 25 percent.”



Specifically, the study found that water vapor rising from the tropics has been reduced, because it has gotten cooler there (another inconvenient truth). A _Wall Street Journal_ headline summed it up: “Slowdown in Warming Linked to Water Vapor.”



Moisture in the lower stratosphere (about 8 miles above the earth’s surface) has been going down, not up.



Aside from clouds, water vapor accounts for as much as two‐​thirds of the earth’s greenhouse‐​gas effect. Water vapor traps heat from escaping the atmosphere — but clouds have the opposite effect (called “albedo”) by reflecting the sun’s energy back into space. And snow on the ground from the IPCC’s predicted precipitation in high latitudes would have the same cooling effect as clouds.



What the new research suggests is that changes in water vapor may well trump the effect of carbon dioxide (only a fraction of which is man‐​made) and methane (which has mysteriously slowed since about 1990).



This raises an intriguing question: Since the Environmental Protection Agency declared that it has the authority to regulate carbon emissions because of their presumed effect on the global climate, why hasn’t the EPA also attempted to regulate mist and fog?
"
nan
"

On July 25, at a hearing of the House Oversight and Investigations Subcommittee (the same folks who grilled WorldCom last month), the nation found out how little real science there is about global warming. The hearing was prompted by the discovery that federal scientists were using computer models that they knew could not replicate U.S. temperatures. They appeared in two landmark documents that have served as the basis for very expensive and intrusive energy legislation. 



What came out of the hearing has people asking if the same problems affecting Enron, WorldCom, Global Crossing, etc…are now troubling environmental science.



Puffy federal documents create consequences. In this case, look no further than a recent California law requiring the first statewide reductions on global warming emissions from cars. It cites many findings from the “U.S. National Assessment” of global warming, one of the documents that provoked the House Oversight hearing.



Science operates by a hard and fast ethic. Theory must conform to reality and be tested by reality. In the case of climate science, our “theories” are huge computer models that project various amounts of warming for the next 100 years. 



In my review of the Assessment, I discovered two very disturbing facts. The Assessment team considered several computer models, but chose the two that predicted the most extreme changes in temperature and rainfall over the United States. That’s prima facie evidence for some type of bias, which isn’t surprising. The team was vetted through four Clinton administration committees, one of which was headed by Al Gore. Worse, these models couldn’t beat a table of random numbers, or two dice on a crap table, when it came to predicting U.S. temperatures.



The Assessment team was required to publicly comment on the science reviews, and swept this criticism under the rug. Behind the scenes they were much more fearful, and replicated my experiment. They found out that the computer models they were using couldn’t even simulate 25‐​year temperature averages over the United States as the greenhouse effect changed in the last 100 years. 



How, then, could these models be used to assess climate change in the next 100 years?



It gets worse. The temperature and precipitation data these models spit out are then input to other sectors of the U.S. economy, such as agriculture, forestry, and water supplies.



In science, random numbers are garbage, and that’s what went in. Refuse in science is what we call “transitive.” Start with it and you end with it. Garbage out.



Tom Karl, who heads the National Climatic Data Center in Asheville, North Carolina, and co‐​chaired the production of the Assessment, was the lucky person who had to come to Washington to defend these shenanigans. His dancing was about as painful as you would expect when a noted scientist has to defend something so wrong.



The core defense of the federal establishment was that they were not making “predictions” of future U.S. climate with these models. Instead they were “plausible projections.” Are the most extreme estimates of U.S. temperature and rainfall changes “plausible”? 



Can someone explain to a reporter, seeing these dire results spit out by extremist computer models, that there’s a lick of difference between a “prediction” and a “projection”? Does anyone believe that the political impact of either, if based upon bad models, isn’t the same, i.e., bad legislation? 



This rings the same bell that Bill Clinton did when he said, “That depends on what the definition of the word ‘is’ is.” Predictions, projections, forecasts…they’re all the same to any reporter, or, for that matter, to any professor. And when they’re based upon computer models that can’t beat dice dancing in Atlantic City, they’re junk.



This story isn’t going away. It surfaced on MSNBC’s ” Hardball” last week, and it will to bubble up every time someone comes up with a new law on climate change in the United States.



It’s no accident that all of this wound up in front of the Oversight and Investigations Subcommittee. Recently it targeted corporations that cook books and hide things from shareholders. 



Forecasting something as important as future U.S. climate based upon models that do not work is as deceptive as stating assets that a company does not have. In this case it is the federal science establishment and we, citizens of the United States, who are the investors. 



It’s not just CEO’s and CFO’s who inflate results to jack up their currency. The corruption has now spread to science. 
"
"The National Trust has ditched plastic for the annual membership card it sends out to 5 million members, it has announced. The new card will be made from a type of strong and durable paper featuring a tough water-based coating, with the paper certified by the Forest Stewardship Council. They will be produced in a mill powered by its own biomass.  The trust said the new cards would avoid the use of 12.5 tonnes of plastic a year. The new cards will be entirely recyclable and compostable, as well as coming in at a fraction of the cost of the old cards, which were made of plastic and chalk, a byproduct of the mining industry. The National Trust said the move was part of a range of measures it was bringing in to protect the environment and tackle the climate emergency, after a survey showed it was backed by the majority of its members. Mel Nursaw, from the trust’s membership team, said: “Replacing our membership cards is a great step towards helping to reduce our impact on the environment, which we know is an important issue for so many of our supporters.” In total, the National Trust now has almost 6 million members, including children and life members who do not receive an annual card. The trust is also exploring how to transfer its physical cards to digital ones. Elsewhere, the charity is looking at removing plastic from most of its greeting cards and wrapping paper, looking at alternatives to plastic tree guards and trialling drink dispensers to reduce sales of bottled drinks."
"

Since the June House vote on the Waxman‐​Markey “cap‐​and‐​trade” bill, lawmakers from both chambers have backed significantly away from the legislation. The first raucous “town hall” meetings occurred during the July 4 recess, before health care. Voters in swing districts were mad as heck then, and they’re even more angry now. Had the energy bill not all but disappeared from the Democrats’ fall agenda, imagine the decibel level if members were called to defend it and Obamacare.   
  
  
But none of this has dissuaded the editorial boards of the _The New York Times_ and _Washington Post_. Both newspapers featured uncharacteristically shrill editorials today demanding climate change legislation at any cost.   
  
  
The _Post_ , at least, notes the political realities facing cap‐​and‐​trade and resignedly confesses its favored approach to the warming menace: “Yes, we’re talking about a carbon tax.” The paper—motto: “If you don’t get it, you don’t get it”—argues that in contrast to the Boolean ball of twine that is cap‐​and‐​trade, a straight carbon tax will be less complicated to enforce, and that the cost to individuals and businesses “could be rebated…in a number of ways.”   
  
  
Get it? While ostensibly tackling the all‐​encompassing peril of global warming, bureaucrats could rig the tax code in other ways to achieve a zero net loss in economic productivity or jobs. Right. Anyone who makes more than 50K, or any family at 100K who thinks they will get all their money back, please raise you hands.   
  
  
The prescription offered by the _Times_, meanwhile, is chilling in its cynicism and extremity. It embraces the fringe—and heavily discredited—idea of “warning that global warming poses a serious threat to national security.” It bullies lawmakers with the threat that warming could induce resource shortages that would “unleash regional conflicts and draw in America’s armed forces.”   
  
  
(Note to the Gray Lady: This is why we have markets. Not everyone produces everything, especially agriculturally. For example, it’s too cold in Canada to produce corn, so they buy it from us. They export their wheat to other places with different climates. Prices, supply, and demand change with weather, and will change with climate, too. Markets are always more efficient than Marines, and will doubtless work with or without climate change.)   
  
  
Appallingly, the piece admits that “[t]his line of argument could also be pretty good politics — especially on Capitol Hill, where many politicians will do anything for the Pentagon. … One can only hope that these arguments turn the tide in the Senate.” In other words: the set of circumstances posited by the national‐​security strategy are not an object reality, but merely a winning political gambit.   
  
  
There’s no way that people who see through cap‐​and‐​trade are going to buy the military card, but one must admire the _Times_ ’ stratagem for durability. Militarization of domestic issues is often the last refuge of the desperate. How many lives has this cost throughout history?   
  
  
Nevertheless, one must wonder at the sudden and inexplicable urgency that underpins the positions of both these esteemed newspapers. Global surface temperatures haven’t budged significantly for 12 years, and it’s becoming obvious that the vaunted gloom‐​and‐​doom climate models are simply predicting too much warming.   
  
  
Still, one must admire the _Post_ and _Times_ for their altruism. The economic distress caused by a carbon tax, militarization, or any other radical climatic policy certainly won’t be good for their already shaky finances, unless, of course, the price of their support is a bailout by the Obama Administration.   
  
  
Now that’s cynical.
"
"

The Dakota Access Pipeline (DAPL) is more than just a pipeline. It is a political hot potato. There are a lot of issues underlying the DAPL conversation, including indigenous peoples’ access to ancestral lands, environmental concerns of a potential spill, water rights, and social justice.



One false assumption is that rejecting the DAPL would result in fossil fuels staying in the ground. The lack of a pipeline has not stopped growth in oil production from the Bakken shale formation yet. This oil is currently transported by rail or road, where it has a significantly higher chance of spillage, explosion or tragedy. Higher prices will be passed on to consumers, a regressive policy that inordinately affects the poorest citizens.



Climate change activists have also entered the fray. Anti‐​fossil fuels advocate Bill McKibben said the pipeline couldn’t pass “a climate test” and the Center for Biological Diversity has made DAPL a touchstone of its aggressive climate campaign. Wouldn’t it be great to see the numbers behind the rhetoric?





The environment is important, but not as important to environmentalists as a large coalition.



There are several calculators that use the EPA’s own model — the Model for the Assessment of Greenhouse Gas‐​Induced Climate Change (MAGICC) — to determine the effects of various policy proposals. The model requires we know how much additional oil DAPL will transmit, which is rather difficult to predict. The pipeline’s capacity is currently rated at 470,000 barrels per day from the Bakken shale formation. This is about half of the Bakken’s current production.



Moving this oil by pipeline is about $7/​barrel cheaper than current transport by truck and rail. For the sake of argument, assume that an additional 470,000 barrels come out of the Bakken as a result of the pipeline. (Note: In reality, production changes are determined by a host of unrelated market conditions).



Plugging that figure into the MAGICC model will tell you how much that oil will raise global temperatures by the year 2100 — just 0.006 degrees Celsius. A rise of 0.006°C is roughly the temperature change we experience every few seconds, even in a thermostatically controlled environment.



It’s also too small to measure on a global scale. In addition, the MAGICC model assumes, along with the EPA, the “sensitivity” of global surface temperature to a doubling of atmospheric carbon dioxide is around 3.0°C. There are dozens of reality‐​based experiments in the recent peer‐​reviewed literature showing that assumption to be a serious overestimate.



A large reason for the prominence of the #NoDAPL movement is due to a concerted effort by environmental groups to include minorities and the economically disadvantaged in their association. By melding environmentalism and social justice into “environmental justice,” environmentalists are able to swell their ranks.



They have incorporated issues plaguing minority groups, like systematic racism and poverty, with the specter of apocalyptic climate change. These priorities were on full display in the November defeat of Initiative 732 in Washington — a carbon tax many environmentalists rallied _against_ due to its lack of wealth redistribution. The environment is important, but not as important to environmentalists as a large coalition.



We should note that, come January 20th, Donald Trump will be president. The Army Corps of Engineers’ process will find itself under the guidance of a new executive — one boisterously behind domestic energy production. We find it difficult to believe President Trump won’t address the DAPL.



Trump has publicly voiced his support for the project. This reroute of the project will merely delay the inevitable — oil flowing from the Bakken shale to consumers with no detectable effect on earth’s climate.
"
"
A Myth About The Surface Temperature Record Analyses Perpetuated On Dot Earth By Andy Revkin
By Dr. Roger Pielke Sr.


On the weblog Dot Earth today, there is text from Michael Schlesinger, a climatologist at the University of Illinois, that presents analyses of long term surface  temperature trends from NASA, NCDC and Japan as if these are from independent sets of data from the analysis of CRU.  Andy Revkin is perpetuating this myth in this write-up by not presenting the real fact that these analyses draw from the same  original raw data.  While they may use only a subset of this raw data, the overlap has been estimated as about 90-95%.
The unresolved problems with this surface data (which, of course, applies to all four locations) is reported in the peer reviewed paper
Pielke Sr., R.A., C. Davey, D. Niyogi, S. Fall, J. Steinweg-Woods, K. Hubbard, X. Lin, M. Cai, Y.-K. Lim, H. Li, J. Nielsen-Gammon, K. Gallo, R. Hale, R. Mahmood, S. Foster, R.T. McNider, and P. Blanken, 2007: Unresolved issues with the assessment of multi-decadal global land surface temperature trends. J. Geophys. Res., 112, D24S08, doi:10.1029/2006JD008229.
I discuss this issue in my recent post
Further Comment On The Surface Temperature Data Used In The CRU, GISS And NCDC Analyses
where I document that even the CCSP 1.1. report acknowledged this lack of independence.
Andy Revkin’s post on the surface temperature record data sets is not journalistically accurate.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90ceedad',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I don’ t know what sort of world NYT reporters live in, but I am now convinced that some like Paul Krugman have no clue about the real world people live in elsewhere.

‘This Week” with George Stephanopoulos debates ClimateGate – more here
Noel Sheppard over at Newsbusters provides some video and transcript of a debate between Paul Krugman of the NYT and Washington Post columnist  George Will.
KRUGMAN: There is tremendously more money in being a skeptic than there is in being a supporter. ... They get almost equal time in the media.
When I read what Paul Krugman said, I laughed out loud. He’s truly clueless.

Here’s the context:

WILL: Speaking of the marketplace, the biggest industry in the world right now may be fighting climate change. There are billions, trillions of dollars on the table, and when you say, well, they are academics and they are scientists and they talk in funny ways — academics are human beings, and the enormous incentive to get on the bandwagon on global warming, the financial incentive, the market driving this, is huge.
KRUGMAN: There is tremendously more money in being a skeptic than there is in being a supporter.
WILL: Hardly.
KRUGMAN: It’s so much easier, come on. You got the energy industry’s behind it. There are 20 times as many believers as there are skeptics in the scientific community. They get almost equal time in the media.
(CROSSTALK)
WILL: Is there a larger venture capital firm in this country than the Energy Department of this government, which right now is sending out billions and billions of dollars in speculation on green energy?
Noel Sheppard writes:
Skeptics get almost equal time in the media? Yeah, that’s why this appears to be the first time ABC addressed this ClimateGate issue.
As for there being more money in being a skeptic than there is in supporting this myth, the facts say otherwise.
The Science and Public Policy Institute issued a report on the money involved in funding the global warming debate in August concluding, “Over the last two decades, US taxpayers have subsidized the American climate change industry to the tune of $79 billion.”
By contrast, the same study found that the media bogeyman “Exxon Mobil gave a mere $23 million, spread over ten years, to climate sceptics.”
See the video and transcript at Newsbusters
UPDATE: Professor Don Easterbrook left this comment on the ABC news site:
I’ve spent 4 decades studying global climate change and as a scientist I am appalled at Krugman’s cavalier shrugging off the Hadley email scandal as ‘just the way scientists talk among themselves.’ That’s like saying it’s alright for politicians to be corrupt because that’s the way they are. Legitimate scientists do not doctor data, delete data they don’t like, hide data they don’t want seen, hijack the peer review process, personally attack other scientists whose views differ from theirs, send fraudulent data to the IPCC that is used to perpetuate the greatest hoax in the history science, provide false data to further legislation on climate change that will result in huge profits for corrupt lobbyists and politicians, and tell outright lies about scientific data.
Posted by: Don Easterbrook | Nov 29, 2009 1:57:05 PM


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9161a078',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I’m a news junkie. I’ve bookmarked Drudge, and on Election Day, I kept going to his site to check the results of the morning exit polls that were commissioned by the major news agencies. When the polls showed Kerry way ahead in every battleground except Iowa, like many others, I wondered whether I should even bother to vote. Something similar happened in Florida in 2000, when thousands of Central time zone voters in the Florida Panhandle (largely Republican) walked away from the polls because CNN had already called the state for Gore, based upon (much more Democratic) votes from the closed polls in the Eastern time zone.



My quandary shows how authority and science are used to influence the political process. 



We expect scientists (and pollsters can be considered scientists because they’re applied mathematicians) to be both correct and savvy. So, when I read that the morning sample was based upon a sex ratio of roughly 60:40 (female/​male), I automatically figured that whomever did the polling simply adjusted the results to the expected sex ratio of the electorate, multiplying each male vote by approximately 1.1 and each female one by 0.9 (assuming a 50–50 electorate). In other words, I trusted the “experts” to know what they were doing, and to report it to the public in a clear manner. Most other people do the same thing. For instance, when they hear about some forecast of dire climatic changes made at some prestigious university or federal laboratory, they assume that the people who put the thing together are so smart that they would have compensated for any systematic problems with their methods. 



Here’s an example from global warming: As a matter of convention, most of our computer models for our climatic future assume that carbon dioxide — the main global warming gas — is increasing at a rate of 1 percent per year. The concentration of carbon dioxide in today’s atmosphere is roughly 375 parts per million (ppm). An increase of 1 percent in a computer model for next year’s climate would raise that concentration to 378.8 ppm, and to 382.5 the following year. 



But in reality, that’s not what’s happening. In the last three decades, the percent change per year has averaged 0.39, 0.41, and 0.51 per cent, respectively. What the computer does is more than double the rate of increase that is actually occurring. 



The amount of warming produced by those models is directly proportional to the rate of carbon dioxide increase. In other words, the models are compelled to calculate twice as much carbon dioxide‐​related warming as could possibly occur in coming decades. 



Back during the Clinton presidency, climate scientists produced a so‐​called “national assessment” of the effects of climate warming for the 21st century. They used models that did exactly what I describe above, and the report showed dramatic (and false) results. The Bush administration has since used the false material from the earlier report for its own study of climate change. The Bush document, in turn, served as the basis for climate change legislation by John McCain (R‐​Ariz.) limiting our net emissions of carbon dioxide. This can’t be accomplished without actively discouraging energy consumption, i.e., dramatically raising the price of gas. 



McCain is a consummate political animal, positioning himself for a 2008 presidential run. He authored that legislation for one simple reason: He sees political advantage in claiming to care about global warming. After all, most of his Republican competitors are going to be on the other side, against regulation. 



Politically, it is profoundly easy to demagogue any climate anomaly into global warming. Remember September’s hurricanes? A coalition of scientists — “Scientists and Engineers for Change” — exploited those disasters by plastering central Florida with billboards claiming that re‐​electing President Bush would make hurricanes worse because he’s not doing enough about global warming. Their scientific basis? The same computer models used by McCain, with the wrong increase in carbon dioxide. 



In this case, they failed. They were not able to persuade enough voters to eke Florida into the Kerry column. And, for that matter, neither did the exit polls produce a large enough effect to turn the nation. 



Matt Drudge is a sharp guy. So are the people at Slate Magazine, who kept the erroneous exit poll numbers up all afternoon. (Drudge took them down for a while.) The poll results were leaked with the full knowledge of their political effect — just as scientists know that a computer model that must overestimate global warming will also stir things up. 
"
"
One of the common themes seen with the surfacestations.org project has been the proximity of BBQ grills to official NOAA thermometers used in the United States Historical Climate Network (USHCN). Despite now having surveyed over 77% of the 1221 station network, some truths continue to be self evident.
USHCN climate station of record, Hartington, NE
This station was photographed by our prolific volunteer, Eric Gamberg. The proximity to the concrete patio earns this station a CRN4 rating, it may be a CRN5 when they wheel out the BBQ away from the house. But who knows? The grilling schedule is not part of the metadata.
But fear not, NASA GISS adjusts for such problems of concrete and BBQ grills. Consider the following blink comparator:
Notice how the past is adjusted cooler, increasing the trend
Source: NASA GISS
USHCN RAW:
http://data.giss.nasa.gov/cgi-bin/gistemp/gistemp_station.py?id=425744450020&data_set=1&num_neighbors=1
GISS Homogenized:
http://data.giss.nasa.gov/cgi-bin/gistemp/gistemp_station.py?id=425744450020&data_set=2&num_neighbors=1
I’m not sure why the hinge point is 1978, perhaps that’s when the homeowner acquired the BBQ? Sure, that is an absurd claim, but certainly no more absurd than the GISS homogenization adjustment itself. Adjusting the past increases the overall positive slope of the temperature trend.
For those new to the whole concept of USHCN stations, the NOAA thermometer is the white slatted object on the post in the center of the photo. It is known as an MMTS thermometer and a cable goes from it into the home where the volunteer observer will write down the high and low into the B91 logbook and send in the report once a month to the National Climatic Data Center (NCDC).There are more photos of this station which you can see in my online station database.
The Gallery of photos can be seen here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95f1fddf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The critical links between water, sanitation, and our global consumption of energy – the “energy-water nexus” are more obvious than ever before. But how many of us will take direct action at the most basic level of all? It turns out that the way we use the toilet has a profound impact not only on our water resources, but is implicated in global energy security and perhaps the future of industrial agriculture as we know it. Flushing a standard WC accounts for around 30% of daily domestic water consumption in developed countries. This water must then be decontaminated before it is released back to the environment – an increasingly vital process of recycling as water stress grows globally. Urine is usually nearly sterile and high in nitrogen, phosphorus and potassium – the triad of nutrients known as “NPK” on which intensive farming is based. Faeces are rich in organic matter and carry lots of diseases. Both are recyclable, but the modern practice of mixing them together in the waste stream necessitates expensive, energy-consuming tertiary treatment processes.  One problem is the high nutrient levels from the urine, which can lead to serious ecological impacts on receiving waters if not removed. Although some wastewater treatment plants generate energy from anaerobic digestion of the sewage itself to offset these costs, there is a strong argument for reducing or avoiding the need for the extra energy altogether. So separating the waste at source has substantial merit. The pathogen-free urine can be reused more or less directly as a high-quality, if potentially rather smelly, fertiliser. Without the urine, treatment of the solid waste is easier, less expensive and less energy-hungry and the reduced frequency of flushing lowers stress on sewer infrastructure as well as our demand for water at source. Another key factor is phosphorus. As the “P” in NPK fertiliser, phosphorus supports at least 30-50% of global agricultural output but it is a finite and increasingly vulnerable mineral resource. Once mined and applied to agricultural land, what doesn’t leach into watercourses is harvested and transported around the world to our supermarkets and restaurants. A significant fraction goes to landfill as food waste, while the rest goes, via our digestive tracts, straight to sewage. Recent estimates indicate that primary stocks of P will last at least a century. But the growing demand for food, the vulnerability of mineral commodities to market forces, and the geopolitical implications of reserves concentrated in places such as Morocco and China mean that “closing the loop” on our phosphorus use is critical to sustaining and securing food supplies into the next few decades.  Simplifying wastewater treatment by separating and capturing it before it enters the sewer will play an important role in making this possible. So why aren’t we all doing it already? The devil is in the detail, as studies by major European institutes, and increasingly public responses, have shown. Initial opinion towards “urine-diversion” (UD) toilets – which contain two pans, one for liquids, one for solids – is often strongly positive, but this changes as users have to live with the devices.  In numerous settings where UD toilets have been installed in large-scale developments and public buildings, experiences are negative. Children, used to a single pan, aren’t sure “where to go”. Men in many countries don’t like sitting down to pee, and women can find it difficult to aim.  All of this compromises the entire premise of the device, increases cleaning and maintenance costs, but most importantly makes people less comfortable about going to the loo.  UD toilets are common in small-scale eco-houses, often in combination with no-flush composting systems and complete independence from mains sewerage. But the users in these cases have already modified their lifestyles to accept what many others see as undesirable “hardships”. Recent reports describe an entire eco-development in China retrofitted with standard toilets after large-scale failure of a Swedish-designed UD-composting system. The single-pan, flushing WC ranks as one of the greatest inventions of all time, just above the internal combustion engine according to a 2010 poll. It has turned the most private activity of billions from a noisome necessity to what many regard as a sanitary, peaceful few moments of calm. Like the car, the WC has shaped our modern collective psyche. But the wider negative consequences may prove to be just as profound.  Separating number ones and number twos makes sense in a sustainable vision of the future, but it may need a change of domestic design psychology to take us there."
"

The fat police have tried to frighten us for so long they’ve used up most of their stock of scary images. Yet the media still run with every ‘The Fat End is Nigh’ story, no matter how absurd. 



Exhibit A is today’s World Cancer Research Fund (WCRF) report, _Policy and Action for Cancer Prevention — Food, Nutrition, Physical Activity, and the Prevention of Cancer: A Global Perspective_ , which warns of a global catastrophe from obesity‐​induced cancer. 



As the _Observer_ ’s David Smith breathlessly previewed the WCRF report last weekend, ‘Cancer cases are now rising at such a rate in Britain and the rest of the world that the disease poses a threat to humanity comparable to climate change’. Not to be outdone, the _Mail on Sunday_ ’s David Derbyshire wrote that the ‘obesity epidemic will double the number of cancer deaths within 40 years… At least 13,000 cases of cancer are caused by obesity in Britain each year’. 



The new WCRF report is largely based on a report published two years ago by the same group that claimed that a third of cancers were caused by diet and lack of exercise. That controversial report advised people to be as thin as possible, and to avoid red and preserved meat and alcohol. The problem with this latest effort from the WCRF is that it is as blatantly and foolishly wrong as its 2007 version. This is especially evident in four areas. 



**Are cancer and obesity linked?**



The report’s headline‐​grabbing claim about the link between obesity and cancer is not supported by the majority of the WCRF’s own data from the 2007 study, or by other, more recent British and American studies. Once you dig beyond the bold claim, you find that what is being claimed is that overweight and obesity can increase your risk for six cancers (cancers of the oesophagus, pancreas, colon‐​rectum, breast, endometrium and kidney). But even this more limited claim has little scientific support. 



Take pancreatic cancer, for example. The 2007 report cites 20 case control studies, but only three show a statistically significant association between obesity and pancreatic cancer. Similarly, of 42 cohort studies on colorectal cancer, only 13 show a link with obesity. Again, with breast cancer and obesity, of 16 studies only three are statistically significant, while eight show a decreased risk between breast cancer and obesity. 



Even for oesophageal cancer, the increased risk was generally found in the morbidly obese (ie, those with a body mass index, or BMI, of at least 40 — a very small percentage of the population). And with endometrial and kidney cancers the relative risks were below two. According to the National Cancer Institute, relative risks below two (that is, two times the risk compared to a control group) are so small that they may be due to ‘chance, statistical bias or the effects of confounding factors’. 



These claims about a link between obesity and cancer are further called into question by the UK’s recent Million Women Study, which examined the link between 17 of the most common cancers and BMI. In that study, the incidence of 10 of the cancers does not show a statistically significant association with either higher levels of overweight or obesity. Of the remaining seven cancers, the association between overweight and cancer is non‐​significant in four, and where the results are significant the relative risks (except for endometrial and oesophageal cancer) are never stronger than two, except in the obese. 



The supposed link between cancer and overweight and obesity is again called into question by a study from the US National Cancer Institute and the Centers for Disease Control published in 2007 in the _Journal of the American Medical_ Association. This study found that being overweight was not associated with increase mortality from cancers considered obesity‐​related, and further noted that ‘little or no association with excess all‐​cancer mortality with any of the BMI categories’. In other words, the overall risk of dying from cancer was not related to body weight. 



Indeed, the study suggested that being overweight might be protective against cancer. For example, in individuals aged 25–59, obesity appeared to be protective against death from cancer. Even for those individuals aged 70 and over, BMIs in excess of 35 were not significantly linked with a higher risk of dying from cancer. 



**Will ‘five a day’ keep cancer away?**



The WCRF report claims that there is a link between a certain kind of diet, obesity and cancer, and that fruits and vegetables, which fat people tend to eat less of, can protect against cancer. Once again, the scientific evidence contradicts such claims. 



For example, of the 17 cancers discussed in the report, virtually all have statistically non‐​significant associations with every type of food, which means that they provide no evidence of a link between a particular food and a particular cancer. For example, of the 17 studies cited that looked at a link between colon cancer and processed meat, 13 are not statistically significant. 



Despite the advice to avoid red meat, the report itself concludes that ‘there is limited evidence…suggesting that red meat is a cause of oesophageal cancer’. Or, again, ‘there is limited, inconsistent evidence…that grilled or barbecued animal foods are causes of stomach cancer’. 



If the evidence is so limited and inconsistent, how can the advice to entire populations to reduce red meat consumption or avoid it entirely be so dogmatic? 



What of the extraordinary claim that, since fat people tend to eat less fruit and vegetables, they are more likely to get certain cancers? This claim is contradicted by the largest and most expensive randomised controlled studies of the effect of eating certain foods and weight on the risk of getting breast cancer, colon cancer, heart disease, and stroke, that is, the Women’s Health Initiative Dietary Modification Trial. 



Almost 49,000 American women were followed over an eight‐​year period in terms of eating, weight, and disease. The women in the intervention group ate ‘healthy’ diets that were low fat and high fibre. The results? There were no statistically significant differences between the intervention and the control group in the incidence of breast cancer, colon cancer, strokes, or heart attacks. 



In fact, the women following the healthy diet didn’t even weigh less than they did at the beginning of the study, or less than the group that continued to eat as they always had. So much for the claim that there is a link between eating certain foods and avoiding cancer. 



**Are fat children more likely to get cancer in later life?**



The WCRF’s Martin Wiseman told the _Mail on Sunday_ that ‘the increase in the number of overweight children is deeply troubling because the more overweight a child is, the more likely they are to be overweight as an adult. And the more overweight the population becomes, the more cases of cancer we are storing up for the future.’ 



But if Wiseman had simply looked at the official figures on childhood overweight and obesity in the most recent Health Survey for England, he would have found that since the previous survey in 2006, there was a decrease in obese girls aged 2–15, from 18 per cent to 15 per cent. Among boys aged 2–10, the prevalence of overweight declined from 16 per cent to 12 per cent. Indeed, according to the Health Survey, amongst boys and girls aged between 2 and 15, overweight and obesity has been declining since 2004. 



Nor is Wiseman’s claim about fat kids becoming overweight adults true. Only morbidly obese children — that is, a very small minority of all children — are at risk for adult obesity. As Charlotte Wright in her Thousand Families Study in Newcastle found, there is ‘little tracking from childhood overweight to adulthood obesity’. 



**Does exercise protect against cancer?**



The final problem with the WCRF report is its assumption that there is a scientifically established link between physical activity, obesity, and cancer prevention. To put it charitably, this assumption is open to significant question. 



Most of the summaries of the relevant scientific literature on this supposed connection are either non‐​committal or highly skeptical, despite the common sense claim that exercise is good for one. As one reviewer wrote, ‘It is important to emphasis at the outset that most of what can be written on this topic remains speculative. No study exists which has recorded adequate birth‐​to‐​death information relating physical activity to health’. 



Moreover, the evidence of a specific cancer‐​physical activity link is difficult to establish. Commenting on the supposed association between breast cancer and physical activity, Rissanen and Fogelholm wrote that due to the lack of evidence one could not make a public health recommendation for women to exercise to reduce their risk of breast cancer. 



A recently published meta‐​analysis on 52 studies that looked at the association between colon cancer and physical activity reported that 44 were not statistically significant. So the supposed, obvious link between physical activity and preventing cancer dissolves upon closer examination. 



Once you get beyond the scary headlines, the WCRF’s claims about diet, weight, and cancer turn out to be, at best, dubious and, at worst, simply untrue. 
"
"
Contiguous U.S. GISTEMP Linear Trends:  Before and After
Guest post by Bob Tisdale
Many of us have seen gif animations and blink comparators of the older version of Contiguous U.S. GISTEMP data versus the newer version, and here’s yet another one. The presentation is clearer than most.
 http://i44.tinypic.com/29dwsj7.gif
It is based on the John Daly archived data:
http://www.john-daly.com/usatemps.006
and the current Contiguous U.S. surface temperature anomaly data from GISS:
http://data.giss.nasa.gov/gistemp/graphs/Fig.D.txt
In their presentations, most people have been concerned with which decade had the highest U.S. surface temperature anomaly: the 1940s or the 1990s. But I couldn’t recall having ever seen a trend comparison, so I snipped off the last 9 years from current data and let EXCEL plot the trends:

 http://i44.tinypic.com/295sp37.gif
Before the post-1999 GISS adjustments to the Contiguous U.S. GISTEMP data, the linear trend for the period of 1880 to 1999 was 0.035 deg C/decade. After the adjustments, the linear trend rose to 0.044 deg C/decade.
Thanks to Anthony Watts who provided the link to the older GISTEMP data archived at John Daly’s website in his post here:
http://wattsupwiththat.com/2009/06/28/an-australian-look-at-ushcn-20th-century-trend-is-largely-if-not-entirely-an-artefact-arising-from-the-%e2%80%9ccorrections%e2%80%9d/
NOTE: Bob, The credit really should go to Michael Hammer, who wrote that post, but I’m happy to have a role as facilitator. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9515c18e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Gyms and non-essential shops in all parts of England will be allowed to reopen when lockdown ends next month, the prime minister has announced.**
Boris Johnson told the Commons that the three-tiered regional measures will return from 2 December, but he added that each tier will be toughened.
Spectators will be allowed to return to some sporting events, and weddings and collective worship will resume.
Regions will not find out which tier they are in until Thursday.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
The PM, who is self-isolating after meeting an MP who later tested positive for coronavirus, told MPs via video link he expected ""more regions will fall - at least temporarily - into higher levels than before"".
He said he was ""very sorry"" for the ""hardship"" that such restrictions would cause business owners.
Speaking later at a Downing Street briefing, Mr Johnson added that ""things will look and feel very different"" after Easter, with a vaccine and mass testing.
He warned the months ahead ""will be hard, they will be cold"" - but added that with a ""favourable wind"" the majority of people most in need of a vaccination might be able to get one by Easter.
Until then, the PM said, there would be a three-pronged approach of ""tough tiering, mass community testing, and [the] roll-out of vaccines"".
Describing how the tiers had become tougher, the PM said:
Where pubs and restaurants are allowed to open, last orders will now be at 10pm, with drinkers allowed a further hour to finish their drinks.
Indoor performances - such as those at the theatre - will also return in the lower two tiers, although with reduced capacity.
In terms of households mixing, in tier one a maximum of six people can meet indoors or outdoors; in tier two, there is no mixing of households indoors, and a maximum of six people can meet outdoors; and in tier three - the toughest tier - household mixing is not allowed indoors, or in most outdoor places.
In all tiers, exceptions apply for support bubbles. From 2 December, parents with babies under the age of one can form a support bubble with another household.
Mr Johnson said the tiers would now be a uniform set of rules, with no negotiations on additional measures for any particular region.
Measures in Scotland, Wales and Northern Ireland continue to be decided by the devolved administrations, but a joint approach to Christmas, involving all four nations, will be set out later in the week.
The prime minister said: ""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
For the third week running we have had some positive vaccine news, but the announcement about the toughened tiers is a reminder, if we needed any, that the next few months will be tough.
Ministers and advisers have been hinting for the past week that the tiers will be toughened - and that is exactly what has happened.
Attention will now naturally turn to which areas will be in which tiers.
Deciding that is a complex equation that will take into account whether the cases are going up or down, the percentage of tests that are positive, hospital pressures and infection rates among older age groups.
To give a flavour of how complex this is places in the North West and Yorkshire have some of the highest rates but they are falling the fastest.
London and the South East have lower rates and more hospital capacity but cases are going up.
Fine judgements will have to be made. We will find out on Thursday.
Mr Johnson also announced changes to sport for both spectators and participants.
While elite sport has continued behind closed doors during the lockdown, grassroots and amateur sport has been halted since 5 November.
From 2 December, outdoor sports can resume, while spectators will be allowed to return in limited numbers. Some organised indoor sports can also resume.
In the lowest risk areas, a maximum of 50% occupancy of a stadium, or 4,000 fans - whichever is smaller - will be allowed to return. In tier two, that drops to 2,000 fans or 50% capacity, whichever is smaller.
In tier three, fans will continue to be barred from grounds.
In tiers one and two, business events can also resume inside and outside with tight capacity limits and social distancing, as can indoor performances in theatres and concert halls, the government's plan says.
Labour leader Sir Keir Starmer described the government's return to the regional system as ""risky... because the previous three-tier system didn't work"".
He added that decisions on which areas will belong to each tier must be taken without delay - ""I just can't emphasise how important it is that these decisions are taken very quickly and very clearly so everybody can plan.
""That is obviously particularly important for the millions who were in restrictions before the national lockdown, because the message to them today seems to be 'you will almost certainly be back where you were before the national lockdown - probably in even stricter restrictions'.""
Helen Dickinson, of the British Retail Consortium, said shops would be ""relieved"" at the decision to allow them to reopen.
""Sage data has always highlighted that retail is a safe environment, and firms have spent hundreds of millions on safety measures including Perspex screens, additional cleaning, and social distancing and will continue to follow all safety guidance,"" she said.
But the UK hospitality industry warned the new rules ""are killing Christmas and beyond"" and said pubs, restaurants and hotels faced going bust.
Meanwhile, a further 15,450 positive coronavirus cases were recorded across the UK on Monday. There have also been a further 206 deaths within 28 days of a positive test. Figures can be lower on a Monday, due to a lag in reporting.
Earlier, it was announced that daily coronavirus tests will be offered to close contacts of people who have tested positive in England, as a way to reduce the current 14-day quarantine period.
Mr Johnson said people will be offered tests every day for a week - and they will not need to isolate unless they test positive.
He also said rapid tests will allow every care home resident to have up to two visitors tested twice a week."
"Whülü Thurr is a staunch believer in ancient farming traditions. “There is an old adage,” she says, “which goes ‘even a single stalk of millet can revive a dying man’.” The 65-year-old farmer, from New Phor village in Nagaland state, north-east India, is a devotee of the ancient grain millet, and is well versed in its nutritional benefit. She is one of the few farmers here who has stayed with the traditional crop over the decades. Many other farmers in Nagaland, the majority of whom are women, have stopped growing it due to a lack of demand.  White rice, easily available both in markets and via the government public distribution system – which sells it at a subsidised rate – is preferred by consumers across the state. But growing rice requires large quantities of water, which makes it vulnerable to the effects of the climate crisis. Faced with a changing climate, Thurr and other farmers are now working to bring millet back to the region and encourage more people to eat it. Thurr joined the Millet Farmers Group, created by the Nagaland chapter of North East Network (NEN), a women’s rights non-profit organisation. Through the network, 200 millet farmers from 11 villages in the district of Phek are sharing knowledge, increasing their output, and accessing new markets. Thurr says she has seen her yields increase since she joined the group. NEN Nagaland also provides interest-free loans of between 1,500 and 2,000 rupees (£16 and £21) to millet farmers. And it is trying to build demand for the grain, including producing a cookbook featuring baked goods, fried snacks, sweets and pancakes – all made from millet flour – aimed at a younger urban audience. “Before joining NEN and the Millet Farmers Group, the millet I grew was only for my family. With the increased production, I now have a surplus which I’m able to sell,” says Thurr. “This money is very helpful towards meeting our household expenses as well as for our medical needs.” Wekoweu Tsuhah, the Nagaland state director of the network, says: “Rice farmers here were already affected by climate change. We’ve had erratic rainfall, rising temperatures, new pests and new crop diseases. So, in 2010–11, we began focusing on millet farming as a way to ensure food security and build climate resilience. “Millet needs very little water, grows well on poor soil, is fast growing and suffers from very few diseases. Once harvested, it stores well for years. We see growing millet as a tool to empower local women farmers in the face of climate change.” Foxtail is the most commonly cultivated type of millet in the region, but other varieties such as sorghum, proso and pearl are also being grown to ensure diversity. Many farmers grow crops to feed their families but some of the bigger producers also sell their output. As well as trying to improve access to markets, NEN Nagaland has helped some of the women to buy dehusking machines. Without access to these machines farmers had to dehusk the grain manually, which put many off millet as a crop. “For years after I resumed millet farming in 2013, I did not have a dehusking machine. I had to manually pound the grain to dehusk it. It took up a lot of my time and energy as I had no help – these activities are considered to be a woman’s job. But I persisted. It was important to me to cultivate this traditional crop,” says Povelu Shijo, a farmer from Phuhgi village. In 2018, with the support of NEN Nagaland, Shijo was able to buy a dehusking machine at a subsidised rate from the government. Inspired by her enthusiasm, Povelu’s entire village is now growing millet. In Akhegwo village, Chutso and her Millet Farmers Group are allowed to cultivate on village land, rent-free. “We already knew of the nutritional benefits of millet,” she says. “But now that NEN has made us aware of its ecological value, our farmer’s group is focusing on seed production.” Last year, the group harvested between 130 to 150kg of seeds, which were distributed to the community. “This year, millet farming will happen on a larger scale in our village. We will start sowing soon,” adds Chutso. In Sumi, the village council rewards the farmer with the highest millet production with a cash prize of 2,000 rupees. Thurr says she is happy that young farmers in her village have started to grow millet again. Not only is it generating income, it is reclaiming its cultural significance. “I received millet as a wedding present from my parents and I’ve gifted each of my four daughters 10 baskets of millet at their weddings.” Wekoweu adds: “By reviving millet farming, we are not only preserving our traditional knowledge, we are also preparing for an uncertain future.”"
nan
"

On Friday, Aug. 9, the Federal Register posted an announcement calling for public comments on the use of the “social cost of carbon” in DOE rulemaking. The members of the House of Representatives have already presented their opinions on social cost of carbon by passing a bill just prior to recess prohibiting its use by the EPA without consent of Congress. It is unclear whether the Senate will take up the issue, although the prohibition would almost certainly face a presidential veto. But without good cause.



The social cost of carbon is a poor concept from the start. It is an ill‐​conceived, one‐​sided supposed measure of the damages associated with climate change resulting from human emissions of carbon‐​containing greenhouse gases (such as carbon dioxide and methane). Or, rather, it is a measure of the damages predicted to occur by a collection of computer models — computer models which themselves largely fail at capturing the climate evolution during recent decades.



Under normal circumstances, little attention would be paid to the esoteric squabbling of economists arguing about how to place a largely theoretical value on a measure which is imprecise and ever‐​changing by its very nature. However, the social cost of carbon has been elevated to the limelight by the Obama administration which has introduced it into the cost‐​benefit analysis that must be performed for new rules and regulations.





But in its haste to find a way to regulate greenhouse gas emissions, the administration has turned its back on both standing federal guidelines as well as sound science.



The social cost of carbon — or its converse, the alleged benefits conferred by reducing carbon dioxide emissions — has become one the administration’s favorite tools for counteracting the high costs associated with an ever‐​growing string of actual and proposed new rules governing everything from microwave oven efficiency to coal‐​killing power plant emissions standards.



The administration is so empowered by the social cost of carbon, that, realizing still untapped potential, it recently upped its initial estimates of the social cost of carbon by about 50 percent. By assigning a central damage estimate (cost) of $35 for each ton of emitted carbon dioxide rather than $21 per ton, more and costlier regulations can be neutralized by the purported benefits of greenhouse gas reductions.



But in its haste to find a way to regulate greenhouse gas emissions, the administration has turned its back on both standing federal guidelines as well as sound science.



For example, the administration dismisses federal guidelines which require an analysis of the cost of regulations from a domestic perspective. Rather than focusing only on costs expected to occur in the U.S., the administration determines the social cost of carbon from a consideration of perceived global impacts. Since the U.S. is much better positioned to respond to and adapt to climate changes than many other countries, the domestic costs are only a fraction of the total global costs. So what the administration is essentially doing is claiming ill‐​defined foreign benefits to justify the costs of U.S. regulations.



More egregiously, the administration turns its back on science. There is growing realization among climate scientists that the projections of climate change resulting from human greenhouse gas emissions have been overestimated. This realization stems from evidence published in the peer‐​reviewed scientific literature over the course of the past several years suggesting that the warming potential from greenhouse gas emissions is 40 percent lower than that which is currently encapsulated in climate models. Even while admitting that the climate sensitivity to greenhouse gas emissions is a key parameter in its calculations, the administration ignores these new findings and instead increased its estimate of the social cost of carbon in the face of the best science which demands that they should have decreased it.



The social cost of carbon is a concept which is easily gamed to fit the desires of the user — a characteristic emphasized in a recent paper by M.I.T. economist Robert Pindyck where he wrote that the models used to determine the SCC “suggests a level of knowledge and precision that is nonexistent, and allows the modeler to obtain almost any desired result because key inputs can be chosen arbitrarily.”



In this case, the user, the Obama administration, desires to limit greenhouse gas emissions in an attempt to mitigate climate change (an endeavor in which it will ultimately fail as the future course of climate change lies not with the U.S., but with the large, developing nations of the world). Unsurprisingly, the social cost of carbon was determined to be high and has gotten even higher just in time for the new round of regulations and executive actions making up the president’s recently announced Climate Action Plan.



Unbeknownst to most of us, the social cost of carbon is a playing an increasing role in our personal lives as our government uses it to justify making things more expensive — from cars to electricity. To do so, it lays science and best practices by the wayside.
"
nan
nan
"Clothing in Britain is increasingly characterised by a high volume/low value approach to business. Judging by past precedent, consumers will discard some 680m items of clothing when they spring clean their wardrobes this year. Replacements are cheap: dresses can be bought for as little as £5 from online retailers (indeed, a mere £3.75, reduced 25%, in Boohoo’s current sale). Cheap prices are praised for providing wider access to consumers. Fashion retailers argue that they are a sign of efficiency. But there is a dark side. A new report from the Environmental Audit Committee enquiry into sustainable fashion reveals how consumers are only benefiting from cheap clothes at considerable cost to the environment and through exploitation of poor and vulnerable garment workers. The environmental impact of fashion is well known. Cotton production uses large amounts of pesticides and water, while synthetic fibres such as polyester are derived from finite oil supplies. Bamboo, increasingly used as a cotton replacement, sounds pleasingly natural but is a semi-synthetic fibre, the production process of which involves the use of chemicals such as caustic soda. And while British consumers with an environmental conscience may feel less guilty as they take their unwanted garments to a charity store or vintage shop, many of these end up in landfill or incinerated because they cannot attract buyers – domestically or overseas. The social impact of fashion similarly raises concern. Evidence suggests that fashion companies do not yet monitor supply chains with such diligence that consumers can be sure that their purchases have not involved exploitation of the workforce. In Britain, many garment workers in Leicester are apparently being paid less than the minimum wage. Abroad, slave labour, child labour and poor working conditions persist, more than five years after the collapse of the Rana Plaza complex in Bangladesh left more than 1,100 garment workers dead. In recent years, the Waste and Resources Action Programme (known as WRAP), which is supported largely by public funds and works closely with the fashion industry, has done an excellent job in promoting clothing sustainability. Its report Valuing our Clothes provided a strong evidence base for action. It has warned government and industry of the environmental impact of a throwaway culture, producing guidelines for designers and a protocol for companies wanting to produce longer-lasting clothing. But designing garments for longevity is pointless if they are discarded prematurely and merely add to the vast tonnage of clothing waste generated each year. Every garment that is produced contributes to the industry’s environmental impact. In a sustainable fashion culture, far fewer garments would be produced and, when no longer wearable, the materials would either be reused – for example, through upcycling, where unwanted clothes are redesigned into new items – or recycled.  It is a vision that still appears a long way off. The Environmental Audit Committee’s report, however, offers hope. Vast amounts of clothing is discarded and, currently, barely 1% is recycled. A recent study highlighted the many obstacles to recycling that need to be overcome. The UK government’s Resources and Waste Strategy promised to review and consult on textile waste – but only by 2025, giving a strong indication that it does not regard the issue as a priority. By contrast, the Environmental Audit Committee’s report proposes a “producer responsibility” scheme in which producers would pay a 1p charge per garment to improve clothing collection and recycling in order to address textile waste. Such a strategy is long overdue and, like all tax threats, inevitably attracted the most media headlines. But the report proposes several other initiatives that could prove even more significant. For example, noting that Sweden has reduced VAT on clothing repair services, the committee recommends a more general reform of taxes, suggesting that “the chancellor should use the tax system to shift the balance of incentives in favour of reuse, repair and recycling to support responsible companies”. And it concludes that the voluntary approach represented by WRAP’s Sustainable Clothing Action Plan, which requires supporting companies to reduce their water, waste and carbon footprints, has had its day. Instead, it argues, such targets should be mandatory for all retailers with a turnover exceeding £36m. A revival in lessons on designing, creating and repairing clothes in the school curriculum is also proposed: a much-needed return to what was once taught as home economics. Whether the government will have the courage to accept this challenge at the expense of STEM subjects will be interesting to see. Economic and educational measures are needed, because recycling does not address the fundamental problem of unsustainable levels of production and consumption in the clothing sector. WRAP’s evidence to the committee revealed that improved production efficiency has reduced environmental impacts – but that these gains were more than offset by increased consumption. The situation is especially bad in Britain. The Textile Recycling Association reported in 2018 that British consumers purchase far more clothes than consumers in other European countries: more than 26kg each year, compared with  17.6kg in Germany, 14.5kg in Italy and 12.6kg in Sweden.  In short, companies produce too much and consumers buy too much. The sector is based on an outmoded and unsustainable business model and relies on insatiable consumer demand.  Thus the fashion industry produces more garments than retailers are able to sell, while the secondhand market is unbalanced – with supply far exceeding demand. Meanwhile, wardrobes in Britain hold a vast surplus stock of garments, much of it unworn because we lack repair and alteration skills. The recent report contains many welcome proposals, but we need to go further. In Britain we consumed more than 1.13m tonnes of clothing in 2016, a significant increase compared with 2012. A target to halve this by 2030 would be an appropriate goal to focus people’s minds."
"**The first public Covid screening centre at an airport in Scotland has opened at Edinburgh Airport.**
Passengers and staff at the airport, as well as members of the public, will be able to pay for PCR swab tests before receiving their results the next day.
Airport managers said the move could help the aviation industry ""drive Scotland's recovery"".
But they urged people to continue to follow their local coronavirus restrictions.
The tests are being carried out at the ExpressTest site in the FastPark car park area, in front of the terminal.
Airport chief executive Gordon Dewar said: ""Protecting and mitigating risk to public health and providing reassurance and confidence to people who need and want to travel is incredibly important if aviation and all of the industries that rely on it are to recover.
""We have acted with ExpressTest to ensure we are in as strong a position as possible to allow aviation as a facilitator industry to drive Scotland's recovery.
""Until then, people must continue to adhere to local regulations and ensure they understand and follow government guidance to protect themselves and others.""
Test results will typically be emailed or sent by text the next day, with airline passengers advised to schedule a test between two and five days before their departure as a precaution.
Anyone receiving a negative result will be emailed a Fit to Fly certificate authorised by a doctor.
But passengers are advised before booking a test at the airport to check if their travel provider will accept the document.
ExpressTest founder Nick Markham described the development as ""hugely exciting"".
He added: ""The ambition is for us to have 30 locations up and running across the UK in the next few months, and this landmark facility in Edinburgh is just the start.""
Passengers and staff at the airport will be charged a subsidised rate of Â£80 and Â£60 respectively to use the service, which will also be available to the general public for Â£99.
When asked about airport testing during her daily media briefing the first minister said the Scottish government was working with the airports in a bid to get to a position where testing is, if not a complete alternative to quarantine, then a ""partial alternative"".
Nicola Sturgeon said the UK government has already announced its plans to introduce a different system from 15 December but she is continuing to seek clarity on how it would work.
Ms Sturgeon added: ""We have some concerns about the accuracy and the reliability of private testing, that's not to say we rule that out, but we are continuing to take a very careful way through coming to a considered decision here.
""I know how important this is for the airports, but it's also important that we get these decisions right so that we're not increasing the risk that travel is posing to our efforts to control the virus."""
"
It appears that all is not well with the idea of “climate forecasting” as the Ministry of Defence pulls the financial rug out from under the Met Office climate program. Now how will they pay for the electricity to run “deep black”, the 1.2 megawatt supercomputer they just purchased?
Phil Jones may have drain the moat and sell access to his climate data lists and code to pay the bills.

From the Register, UK
Weather soothsayers lose £4.3m
By Austin Modine
Story excerpts:
The Met Office, home of UK weather soothsaying, is getting its climate research budget chopped by a quarter after the Ministry of Defence ended financial support to focus on “current operations.”
A loss of £4.3m ($7m) funding will hit the Met Office Hadley Centre for Climate Change, according to the science journal Nature. The research institute provides the government with bleeding-edge computer models indicating which parts of the UK should stockpile sunscreen and floaties for the coming Thermageddon.
Please support net journalism. Read the entire story at the Register -> here
Read a couple of interesting articles about the shenanigans of the Hadley Climate Reasearch Unit at Climate Audit:
The UK Met Office Deepens The Moat
Phil Jones: the Secret Agent in Hawaii




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9528ac31',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"As part of one of the largest environmental protests ever seen, over a million young people went on strike on Friday March 15 2019, calling for more ambitious action on climate change. Inspired by Greta Thunberg, a Swedish school girl who protested outside the Swedish parliament every Friday throughout 2018, young people in over 100 countries left their classrooms and took to the streets. The previous #YouthStrike4Climate on February 15 2019 mobilised over 10,000 young people in over 40 locations in the UK alone. Their marches, chants and signs captured attention and prompted debates regarding the motivations and methods of young strikers. Many were criticised by those in the government and the media for simply wanting an opportunity to miss school. My PhD research explores youth participation in climate change governance, focusing on the UN climate negotiations. Between 2015 and 2018 I closely studied the Youth Climate Coalition (UKYCC) – a UK based, voluntary, youth-led group of 18 to 29 year olds – which attends the international negotiations and coordinates local and national climate change campaigns. My research shows that young people are mobilised by concern for people and wildlife, fears for the future and anger that climate action is neither sufficiently rapid nor ambitious. Young people need to feel as though they are “doing something” about climate change while politicians dither and scientists release increasingly alarming projections of future climate conditions.  The strikes have helped young activists find like-minded peers and new opportunities to engage. They articulate a collective youth voice, wielding the moral power of young people – a group which society agrees it is supposed to protect. All the same, there are threats to sustaining the movement’s momentum which need to be recognised now. The paternalism that gives youth a moral platform is a double-edged sword. Patronising responses from adults in positions of authority, from head teachers to the prime minister, dismiss their scientifically informed concerns and attack the messenger, rather than dealing with the message itself. You’re too young to understand the complexity of this.  You’ll grow out of these beliefs. You just want to skip school. Stay in school and wait your turn to make a difference. Striking may hurt your future job prospects. The list goes on … This frightens some children and young people into silence, but doesn’t address the factors which mobilised them in the first place. These threats are also largely unfounded.  


      Read more:
      Climate change: a climate scientist answers questions from teenagers


 To any young person reading this, I want to reassure you, as a university educator, that critical thinking, proactivity and an interest in current affairs are qualities that universities encourage. Over 200 academics signed this open letter – myself included – showing our support for the school strikes. Growing up is inevitable, but it can cause problems for youth movements. As young people gain experience of climate action and expand their professional networks, they “grow out of” being able to represent youth, often getting jobs to advocate for other groups or causes. While this can be positive for individuals, institutional memory is lost when experienced advocates move on to do other things. This puts youth at a disadvantage in relation to other groups who are better resourced and don’t have a “time limit” in how long they can represent their cause.  Well-established youth organisations, such as Guides and Scouts, whom I have worked with in the past, can use their large networks and professional experience to sustain youth advocacy on climate change, though they lack the resources to do so alone. It would also help for other campaigners to show solidarity with the young strikers, and to recognise youth as an important group in climate change debates. This will give people more opportunity to keep supporting the youth climate movement as they get older. Researching the same group of young people for three years, I have identified a shift in their attitudes over time. As young participants become more involved in the movement, they encounter different types of injustices voiced by other groups. They hear activists sharing stories of the devastating climate impacts already experienced by communities, in places where sea level rise is inundating homes and droughts are killing livestock and causing starvation.  The climate justice movement emphasises how climate change exacerbates racial and economic inequality but frequently overlooks the ways these inequalities intersect with age-based disadvantages.
Forgetting that frontline communities contain young people, youth movements in developed countries like the UK begin to question the validity of their intergenerational injustice claims.  Many feel ashamed for having claimed vulnerability, given their relatively privileged position. Over time, they lose faith in their right to be heard. It would strengthen the entire climate movement if other climate justice campaigners more vocally acknowledged young people as a vulnerable group and shared their platform so that these important voices could better amplify one another.   With my own platform, I would like to say this to the thousands who went on strike. You matter. You have a right to be heard and you shouldn’t be embarrassed to speak out. Have confidence in your message, engage with others but stay true to your principles. Stick together and remember that even when you leave school and enter work – you’re never too old to be a youth advocate. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterInstead of utopia and paradise, Germany’s green dreams are delivering an ecological nightmare, turning the country into a toxic cesspool of lethal man-made biocides: botulism and now E-coli bacteria. Millions of consumers are now at risk.
A few days ago I wrote about how biogas plants are producing and distributing deadly botulism throughout Germany and how a thousand farms and thousands of heads of cattle have been infected, along with wildlife. Even humans have been infected.
Worse, the media and authorities are choosing to ignore the problem completely – and thus are putting thousands of lives at risk. Veterinary officials even refuse to ban the slaughter of botulism-infected cattle (to avoid the compensation of farmers by the state), and so the meat of the botulism-infected cattle is being marketed to consumers!
Now E-coli bacteria – likely from organic farms
Now there’s a new deadly bacteria released that is rapidly spreading rampantly all over Germany – E-coli bacteria. So far it has infected and sickened hundreds of Germans and has claimed the lives of 2 people. The online Bild daily here reports that on Saturday an 83-year old woman died of the “Killer-Bacteria E-coli” and that a 25-year old woman died yesterday in Bremen – showing the symptoms (tests must still be done to confirm the E-coli). Bild writes:
The E-coli bacteria are spreading at a rapid speed in Germany. About 300 people have been infected thus far, over 40 patients are suffering from the life-threatening hemolytic-uremic syndrome (HUS), which is caused by the intestinal bacteria. It leads to kidney failure, damage to blood vessels and anemia.
and
A spokesman at the Hamburg Health Dept.: ‘The situation is serious!’ “
The news of the E-coli epidemic is making headlines everywhere; authorities are in a state of alarm.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The bacteria cause bloody diarrhea and intestinal bleeding, and can lead to death if not treated in time.
Origin of the deadly bacteria
Officials are searching feverishly for the origin. Media reports and experts say the source is very likely from raw, unwashed vegetables. I immediately wondered if all that healthy organic food grown in nature without industrial processing could be the source. My suspicions seem to be well-founded. Bild writes:
The horrible suspicion: Contaminated manure, often used as fertilizer for organic vegetables could be the source!”
The Federal Institute for Hazard Assessment warned already back in January: ‘Through contact with feces, for example manure fertilizers, food plants could be burdened with E-coli.’ Martin Hofstätter, Greenpeace agriculture expert: ‘The manure is spread by the wind and lands on also on fruit and vegetables of neighbouring fields.’
It is quite possible that the contaminated manure is still finding its way onto fields.”
Someone ought to tell that Greenpeace “expert” that industrial food at least gets washed and cleanly processed according to strict quality standards, and not picked and handled by soiled hands and sold directly to consumers as “fresh and healthy”.
So much for the green paradise. Add this contamination to the list, which now includes mercury from  energy saving lights, botulism from biogas plants, and cadmium in solar panels. And let’s not forget the bird-shredding windmills that litter the landscape.
Also read here: http://www.thelocal.de/national/20110524-35217.html
Share this...FacebookTwitter "
"

Much has been made of a paper published on Jan. 8 in the journal _Nature_ by Chris Thomas and 18 co‐​authors, claiming that global warming will cause a massive extinction of the earth’s biota. Thomas told _The Washington Post_ “we’re talking about 1.25 million species. It’s a massive number.”



It turns out that there are a massive number of glaring problems with their study that clearly eluded the peer review process. This is evinced by the rapid turnaround for the manuscript, with acceptance in final form a mere five weeks after original submission. No one can clear revisions through 19 authors in that time unless there weren’t many revisions suggested, or, if there were, they were ignored by the journal’s editors in a rush to publication. 



In fact, acrimonious debates about what should or should not be published about global warming are the rule rather than the exception, simply because papers are being published — on many sides of the issue — that can be shredded after only a cursory review. Unfortunately, the debate may have started with _Nature_ itself. In 1996, conveniently a day before the U.N. conference that gave birth to the Kyoto Protocol, _Nature_ published a paper purporting to match observed temperature with computer models of disastrous warming. It used weather balloon data from 1963 through 1987. The actual record, however, extended (then) from 1958 through 1995, and, when all the data were used, the troubling numbers disappeared. Since that famous incident, people have been very leery of what major scientific journals publish on global warming. The Thomas extinction paper only throws more fuel on an already roaring inferno.



Thomas et al.‘s work is an interesting exercise in computer modeling that shows again that what comes out of a computer is a product of the assumptions that go in. The scientists examined the distribution of more than 1,000 plants and animal species, calculated their current climatic range, and then used a climate model to determine whether the amount of land the species could occupy in the future would shrink or expand. If there was a likely shrinkage, the researchers expected an increased chance of extinction. 



Fair enough. But this assumes that climate change is the sole driver of changes in biodiversity, which is hardly true. Consider the effects on an ecosystem of the mutation of some previously harmless bacterium, a clearly non‐​climatic cause of extinction. The plethora of factors that influence ecosystems, besides climate, determine the composition of the community. In fact, placing all the onus for extinction on climate also calls the entire dramatic result into question.



Their lowest scenario for warming is bounded at 0.8°C in the next 50 years, and produces an extinction of roughly 20 percent of the sampled species. 



There’s a convenient reality check available. That’s because surface temperatures indeed _have_ risen this amount in the last 100 years. But there is absolutely NO evidence for massive climate‐​related extinctions. (One would think the reviewers at Nature would have picked that up!) 



There are several other major problems: 









Obviously, there is a lot to criticize in this paper. What is surprising is that something with so many inconsistencies and unrealistic assumptions made it unscathed through the review process in such a prestigious journal as _Nature._ The politicization of scientific papers on global warming and the tendency of science journals to rush to judgment have to end. 
"
"Across the open heather moors of upland Britain, last-minute preparations are being put in place for the start of the red grouse shooting season on August 12. On average about 200,000 grouse are shot every year in England and Wales. Yet the management that makes such large numbers of grouse available for the guns in autumn is becoming increasingly contentious. The reason is that there is a growing and convincing body of evidence that suggests that birds of prey, or raptors, are being illegally killed by those who manage grouse stocks. These accusations of illegal killings of birds such as harriers, falcons and eagles has given rise to a clash between those on both sides of the debate. While the main conservation and game shooting organisations claim to be keen to see an end to this illegal activity, they favour different approaches. The conservationists tend to back the strengthening and enforcement of policy. For example, the RSPB demands that grouse shooting be licensed, so that a licence may be revoked should illegal activity be detected. Mark Avery, the former head of conservation science at the RSPB, has gone further and demanded that grouse shooting be banned. There is a day of protest set for August 10, and Marks & Spencer recently found themselves in the protesters’ sights for their plans to sell grouse in their stores. In contrast, supporters of red grouse shooting claim that the land management associated with it has benefited a range of species, as well as bringing income and jobs into remote rural areas. They wish to see some form of legal management of birds of prey. Feelings run high; many question why it seems to be a taboo to discuss the management of raptors and why the desire to increase the number of these birds should come before jobs and communities. The hen harrier finds itself at the epicentre of this conflict. The species has virtually disappeared as a breeding bird on intensively managed grouse moors – killed because of its efficient hunting of grouse. One of the features of the harrier is that it is not particularly territorial, so on some grouse moors it can breed at levels that lead to significant financial losses for those involved. So what is the best way of resolving this problem? A shooting ban would certainly lead to a change in how large swathes of our uplands are managed. However, the costs and benefits of this change for the communities, predators and other species would depend on what it is replaced with. Such an approach would also inevitably anger many of those involved in land management and possibly damage the long-term relationships between hunters and conservation organisations.  Could the management of harriers provide a solution? A forthcoming population modelling study due to be published in the Journal of Applied Ecology explored one approach. The study highlighted the densities at which harriers could co-exist with driven grouse shooting through a quota or brood management scheme.  The idea is simple: once numbers go beyond some agreed level the chicks would be moved into captivity until ready to fly, when they would be released to rejoin the wild population. It may sound an unusual way of managing a wild species, but there is a precedent for it. In continental Europe, harriers breed in agricultural crops and are often killed if they aren’t flying before the combines harvest the crop. Instead, harrier chicks are taken into captivity before harvesting and subsequently released. This approach could allow the co-existence of harriers and grouse without recourse to illegal killing, but it would need to be tested in the field.  The problem is that the conservation movement are nervous because of the precedent this approach sets for active management of birds of prey – even if it ultimately led to more harriers. They argue that any discussion around techniques that would affect birds of prey should only take place once harriers are allowed to breed freely on grouse moors. Unsurprisingly, grouse managers will only trial such a scheme if there is an agreed quota, and an exit strategy that they can work towards.   We are at a crossroads, but it is not yet clear which way we are going to go: enforcement or management? While science has provided evidence, in the end the decision will – as is so often the way – depend as much on political, moral, social and economic arguments as it does on science."
"The government is set to introduce E10 fuel containing 10% ethanol as a new form of “cleaner” petrol aimed at cutting carbon dioxide emissions. Grant Shapps, the transport secretary said the government was consulting on plans to make it the standard grade at British filling stations from 2021.  The new petrol, he said, had the potential to reduce CO2 emissions by about 750,000 tonnes per year or the equivalent of 350,000 fewer cars on the road. Petrol grades in the UK currently contain up to 5% bioethanol, known as E5. The E10 blend is already used in countries including Germany, France, Belgium and Finland. Shapps said the proposed new fuel was a steps towards “a net zero future”. “Before electric cars become the norm we want to take advantage of reduced CO2 emissions today,” Shapps said."
"
Share this...FacebookTwitterWhat follows are just a couple of examples from James A. Marusek’s A Chronological Listing of Early Weather Events (may take a few minutes to load).
This is an oustanding work that the warmists’s will not want you to see. (Hat tip reader Ron de Haan).
585 A.D. “Western Europe was so rainy, that it could be confused with
winter. The bulk of the rains this year caused rivers to overflow their banks and flood the fields and meadows. These floods seriously compromised the crop yields.”
994 A.D. “A destructive storm struck London, England, blowing down fifteen hundred buildings and killing several hundred persons. The summers in the years 994 and 995 in Europe produced very high temperatures and a very persistent heat wave. Historians reported that the drought was so terrible that the fish died in the ponds, the trees caught fire, and the fruit and the flax harvest were destroyed. In 995 the greater part of Europe’s rivers were so shallow that you could wade through them. In 994 in Western Europe, the dearth of rain caused the rivers to dry up. It killed the fish in most lakes. It dried up thousands of trees and burned grassland and crops.”
1186 A.D. “In Germany the winter was warmer than had known for a long time. The vegetation was very advanced. The harvest took place in May and the grape harvest in August. In France, the trees were blooming in the middle of winter.”
Recall how alarmists would love have us believe that before man began emitting CO2 the weather was tame and friendly to all of the earth’s inhabitants, and that terrible storms and extremes began only after man embarked on industrialization and the burning of fossil fuels.
Well breadandbutter.com reminds us that extreme weather extremes occurred just as often in the past. Indeed today’s attempt to have us believe that every occuring weather extreme is a sign of man-made climate change just shows how bankrupt the AGW science has become.   
http://www.breadandbutterscience.com/Weather.pdf.
Share this...FacebookTwitter "
"The equivalent of 1,145 truckloads of oil is stolen in Mexico per day from PEMEX – the state-owned petroleum company. That’s 146 billion Mexican Pesos (USD$7.4 billion) in lost revenue since 2016 – a significant hit for a country where 3.8% of GDP comes from oil exports. President Andrés Manuel López Obrador has introduced reforms to tackle the problem, including defining oil theft as a serious felony and releasing a new national strategy for oil production. The new strategy includes shutting off several major pipelines and working at reduced capacity until appropriate measures can be taken to protect them. While flow through the pipelines is halted, oil remains inside at a constant pressure. Theft causes pressure differences that actually help pipeline operators to detect where thieves are taking oil, but the stationary fuel has tempted more and more people to take risks.    A pipeline exploded recently after it was tapped by people trying to fill containers with oil. A spark ignited the fuel and killed more than 70 people. A few days later, on January 28, another pipeline exploded although, thankfully, without casualties. The government has since enlisted the military to patrol pipelines. Fuel shortages have gripped several areas of the country, but illegal tapping by ordinary people can only be blamed for about 20% of total theft. Corruption within the oil industry and organised crime make up the majority of the problem. PEMEX has already fired 100 workers for complicity in fuel theft schemes and many more are under investigation for their ties to organised crime and for sharing information about the location of pipelines.  A study from EnergeA, a consultancy hired by the Mexican Energy Regulation Commission, looked at the hydrocarbon sector’s security and found that drug cartels, militias, petrol stations, PEMEX staff and police have all been implicated as profiteers from bribes or the direct illegal sale of oil.  Many sell the oil to petrol stations at lower prices than official distributors. Their activities go beyond tapping and include the theft of barrels at PEMEX facilities, before distributing it for sale to the black market. People in Mexico have been generally supportive of the president’s measures to prevent theft, despite widespread shortages and explosions at pipelines where oil has stopped and tapping has occurred. However, political adversaries have tried to take advantage of the situation, such as former presidents Felipe Calderon and Vicente Fox, by demanding López Obrador change course. The president’s enduring public approval ratings of 57% suggest many citizens support a hard line on halting the supply of oil and making oil theft a felony. But to tackle thieves throughout the oil supply chain, Mexican authorities can learn from India, where theft has been substantially reduced.  Pipelines in India are routinely patrolled to monitor unusual activities and new technologies have been developed which allow engineers to detect even small signs of theft. New pipelines are also routed near highways or railway tracks to keep pipelines under close vigilance and ensure emergencies can be quickly addressed.    One gadget used in India is a handheld system which identifies leaks and illicit tapping to pinpoint the location of a theft, allowing pipeline operators to coordinate with local police to catch thieves in the act. Portable data loggers can be deployed and installed on pipelines where leaks or thefts are suspected.  Thieves learn to counter traps quickly, so innovative approaches which combine technology and an understanding of why and how people are stealing oil could bring permanent solutions in Mexico. Given the scale of the problem and how ingrained the “shadow supply” of oil is within Mexican society, focusing on social change is necessary too. The president has appealed to businesses, oil distributors and citizens to boycott the black market in stolen fuel, but transforming social practices will be challenging.  Young people could be educated about the negative effects of oil theft for society and the economy. These include fluctuations in the price of fuel, the operational costs for a public utility and the significant personal risk to thieves.  All of this will need support from strong, credible and transparent institutions, as corruption currently limits the authority of state organisations and the police and excuses the participation of private businesses and citizens, such as petrol station owners. There are many fixes for plugging leaks in the supply chain but stopping oil theft won’t happen without reliable governmental bodies. This includes regulators responsible for monitoring fuel production and organisations enforcing the legal supply of fuels to service stations. Without transparency and accountability in organisations such as PEMEX, their role as enablers of a shadow supply chain will continue. This demands rethinking corruption as a systemic problem within Mexico, one that is costing lives and goes far beyond oil. Introducing mechanisms which can help identify and report corruption could be a step towards broader social change."
"
  From the BBC: ‘Iceberg the size of Luxembourg threat‘ – click image for video and watch the collision of two giant ice masses. Of course 50 years ago, such things would likely go unnoticed without satellite imagery.

They write:
A vast iceberg that broke off eastern Antarctic earlier  this month could disrupt marine life in the region, scientists have  warned.
They say the iceberg, which is 78km long and up to 39km  wide, could make it harder for the area’s colonies of Emperor Penguins  to find food.
But British and Australian scientists disagree on  whether it could also cause major problems to our own weather patterns.
Well so far, nobody at the BCC is blaming the collision on Global Warming:
BBC tells the truth – shock horror! – iceberg not caused by global  warming
But I don’t think Joe Romm has weighed in on it yet. There’s still time. At least it’s not a bridge in Minnesota.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8dec0e71',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Businesses must improve how they disclose their impact on the environment or risk failing to meet climate targets, the Bank of England governor, Mark Carney, warned the City on Thursday. Without disclosure rules that allow investors to compare how businesses are meeting the climate challenge, the world risks missing targets to be carbon neutral by 2050, Carney said.  Speaking at a climate conference at the Guildhall in London, Carney said he wanted to pull together a set of rules ahead of November’s Cop26 climate summit in Glasgow to spur a switch of investment funds away from polluting industries in time to prevent a global temperature rise of about 1.5C. “Given the scale of the climate challenge and the rising expectations of our citizens, 2020 must be a year of climate action where everybody’s in, and that includes the world’s leading financial centre,” Carney said. “To identify the largest opportunities and to manage the associated risks, disclosures of climate risk must become comprehensive, climate risk management must be transformed, and investing for a net-zero world must go mainstream.” He was joined by Christine Lagarde, the president of the European Central Bank, who said inconsistent reporting by the 26 biggest eurozone banks and insurers meant “we still have some way to go”. Lagarde, who until last year ran the International Monetary Fund, said: “Only five out of the 26 partially disclose the impact of their financial assets, and none of them provide full disclosure.”  Last year, Carney said firms that failed to move towards zero-carbon emissions would be punished by investors and go bankrupt. Carney is due to take over as the UN special envoy on climate finance next month after stepping down as governor. He has also been appointed by Boris Johnson to advise the UK government in the run-up to the Glasgow conference. The G20 group of nations sponsored the development in 2015 of a new rulebook for companies to declare their climate impact. Carney was a sponsor of the project as head of the G20 financial stability board. He said the framework developed by the Task Force on Climate-related Financial Disclosures (TCFD) was supported by more than than 1,000 companies from 54 countries, with a market value of $16.7tn (£12.8tn). “The objective is that every professional financial decision will need to take climate change into account,” he said. The UK government is under pressure to commit to the TCFD rules and force all UK companies to disclose the impact of their activities on the environment. The EU has already said it is prepared to impose rules within the next few years if an agreement at the G20 group of countries, which sponsored the TCFD, cannot be reached. It is likely the EU and UK will need to go it alone following a reluctance of Donald Trump’s US administration to join global efforts to limit global warming. Philipp Hildebrand, the vice chair of BlackRock, told the conference it would help investors play their part in tackling the climate emergency if governments forced companies to comply with newly drafted global disclosure rules. “It would make life easier,” he said responding to concerns that the adoption of rules was being held back by companies and countries reluctant to recognise the climate emergency. Hildebrand, the former head of the Swiss central bank, said: “Let’s face it, finance has had a horrible decade. The way we ignored leverage [ahead of the 2008 financial crisis] is the way we are ignoring climate change.” BlackRock, which manages more than $7tn of private investor funds, has offered its support for the TCFD, but has yet to show how this will affect its relationship with companies that it invests in. David Schwimmer, the head of the London Stock Exchange, said companies needed to conform with the TCFD rules before they were forced to by governments. “We think mandatory disclosure is the direction of travel,” he said. The newly-appointed business secretary, Alok Sharma, whom Johnson named as president of the summit this month after sacking a former energy minister from the role, echoed Carney’s call for the City, London’s financial centre, to embrace disclosure rules. “We are calling on action from everyone – businesses, civil society and each part of the global financial system – to meet the Paris agreement goals,” Sharma said. • Sign up to the daily Business Today email here or follow Guardian Business on Twitter at @BusinessDesk."
"
Share this...FacebookTwitterLook how cozy Revkin is in bed with alarmists like Rahmstorf. And they really detest skeptics, don’t they? Rahmstorf writes to Andy:
Hi Andy,
from over here, it is hard to see this kind of Inhofe speach as anything else than an irrelevant piece of absurd theatre. It doesn’t even bother me any more – he’s simply lost it.
Cheers, Stefan”
And Andy replies to Steffi (emphasis added):
I know. but he still speaks to and for a big chunk of America — people whose understanding of science and engagement with such issues is so slight that they happily sit in pre-conceived positions.”
Read it all here! Looks like I’m going to be up for awhile tonight!
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAnd she used to be a big proponent of the IPCC! My how things are changing.
I’m glad to see that Donna Laframboise’s new book is taking off well. I’ve ordered a copy myself and look forward to reading it. Sounds like it’s a real IPCC slammer. First off, hats off to Donna for taking the time to do all that digging into an organisation which sorely needs transparency.
Judith Curry at Climate Etc here does a good job reviewing the book. She gave it 4.5 stars (professors rarely give out A+s), which means two solid thumbs up. I was particularly amused by her comment:
Does the problems with the IPCC mean that WG1 science is incorrect?  Not necessarily, but I agree that a “new trial” is needed.  WG2 and WG3 reports pretty much belong in the dustbin, as far as I can tell.”
There it is. Now how much money and resources went into producing WG2 and WG3? Paper in the dustbin is what we get for it?
Worse, the IPCC refuses to learn anything from all this. Recently we’ve heard they are setting up shadowy back rooms in order to skirt FOIA. I’m starting to wonder if it would be safe to turn on the light in a room full of these characters.
Donna appears to have set something into motion. The books are well-priced and I suggest readers buy extra copies and give them away as birthday or Christmas presents – or give them to your political representatives. Ask your pol if they can do better with our money.
ORDER INFO HERE: http://nofrakkingconsensus.com/2011/10/13/a-book-is-born/
PS: Donna Laframboise is the midwife of NoTricksZone. I was one of her “citizen auditors” which she mentions in her book and she invited me to WordPress. The rest of the story you know.
Share this...FacebookTwitter "
"Journalist and environmentalist George Monbiot recently wrote a powerful polemic against the concepts of ecosystem services and natural capital, arguing that they were leading us on a neoliberal “road to ruin”. In many cases nature is ignored or trumped by other economic or social priorities, or seen as a barrier to growth to be overcome. Ecosystem services and natural capital help re-frame nature as an asset to society that delivers many benefits. Monbiot’s attack, therefore, is in danger of throwing out this natural asset baby with the “dirty” neoliberal bathwater. The power of ecosystem services and natural capital concepts is that they break down and clarify what nature provides in economic terms. Value is revealed with respect to the benefits it provides to society. So, for example, city parks are more than just attractive green spaces. They improve air quality and help to minimise the heat island effect, they provide a natural health service for people to walk and relax. Their presence actually raises nearby house prices. If they are well designed they can also provide flood protection and improve biodiversity. All these factors contribute to the productive economy and, and so this provides, in theory, an economic incentive to protect them. Now, I am a planning academic who champions the use of ecosystem thinking to help improve the way nature is treated in decision-making. While I share concerns about the descriptive shortfall of sterile economic terminology such as “ecosystem services”, I believe the framework offers useful ways to address its current neglect. The 2011 UK National Ecosystem Assessment (NEA) was the first comprehensive assessment of the state and worth of Britain’s natural environment. It concluded that nature’s services were in significant decline across most habitats – in part because nature is consistently undervalued in decision-making. The NEA  provided a valuable evidence base from which to assess the impact of different future scenarios on key ecosystem services. Most controversially, it calculated the financial worth of the benefits that different habitats provide to society. For example, the  benefits that inland wetlands bring to water quality are worth up to £1.5 billion per year to the UK.  The main problem with economic values is that they fail to account for the intrinsic values of nature. Ecosystem services form only one of the 12 strategic principles of the ecosystem approach. This aims to integrate the management of land, water and living resources with the objectives of biodiversity conservation, sustainable use, and an equitable sharing of the benefits from natural resources. However, most people focus solely on the ecosystem services aspect, cherry picking those services that best fit their agendas, instead of using the 12 principles to inform policy decisions. As one of the principal investigators within the NEA follow-on project between 2012-2014, I sought to address this problem by translating ecosystem science and jargon into practical guidance. This included adapting assessment tools to better take into account the value of nature, directly challenging the view that nature is a barrier. The project highlights the need for decision-makers to fully assess a wider range of policy alternatives from the outset, and to target policies where environmental benefits are maximised.    The key lesson is to work with nature rather than dominate it. If we continue to ignore the value of nature and selectively value only the ecosystem services we want, we risk making poor decisions that will cost UK PLC more in the long term. Plans to engineer multi-million pound flood defence schemes as opposed to implementing much cheaper upland management of our moorlands and better land use management practices is a case in point. Thinking about nature helps us all benefit. There is some evidence of this thinking being incorporated into planning policy. For example the value of ecosystem services has been recognised in planning guidance (under the new National Planning Policy Framework). However, its potential is limited by the alienating language. Hence our work tried to address this by embedding the value of nature within more familiar concepts and tools used in day-to-day work. Planning represents the frontline where decisions are made about building and development that may affect the environment. Yet the level of environmental knowledge in decison-making is often inadequate. We need to find a common language to better communicate the many benefits nature provides us, but we must also avoid simplistic assessments that overlook and poorly account for nature.  The Treasury Green book, for example, uses a method of converting future costs and benefits to a present day equivalent to make them comparable (a discount rate). HM Treasury recommends applying a discount rate of 3.5% for periods of up to 30 years, compared to Germany’s Federal Environment Agency which recommends a discount rate of 1.5% – a significant under-representation of their value. Similarly, biodiversity offsetting schemes have been proposed as a means to account for the loss of ancient woodland, by planting new trees. But this fails to recognise the irreplaceable value of such resources and is little more than an environmental con.  We must no longer see nature as a “bolt-on”, or separate to the economies of cities, countries, and the planet. And we can no longer expect economies to thrive if the health of the environment is compromised. "
"**Up to three households will be able to meet up during a five-day Christmas period of 23 to 27 December, leaders of the four UK nations have agreed.**
People can mix in homes, places of worship and outdoor spaces, and travel restrictions will also be eased.
But a formed ""Christmas bubble"" must be ""exclusive"" and would not be able to visit pubs or restaurants together.
The leaders urged people to ""think carefully about what they do"" to keep the risk of increased transmission low.
They added 2020 ""cannot be a normal Christmas"" but family and friends will be able to see each other in a ""limited and cautious"" way.
However, some scientists have warned that the relaxation of Covid restrictions over the festive period could spark another wave of infections and further deaths.
The measures will see travel restrictions across the four nations, and between tiers and levels, lifted to allow people to visit families in other parts of the UK.
Anyone travelling to or from Northern Ireland may travel on the 22 and 28 December, but otherwise travel to and from bubbles should be done between the 23 and 27.
People will not be able to get together with others from more than two other households, and once a bubble is formed, it must not be changed or be extended further.
The guidance says a bubble of three households would be able to stay overnight at each other's home but would not be able to visit hospitality, theatres or retail settings.
However, existing local restrictions will still be in place mean many pubs and restaurants - such as those in England's tier three or Scotland's level four - will remain closed during the festive period.
The leaders of England, Scotland, Wales and Northern Ireland reached the agreement at a meeting on Tuesday afternoon.
In a joint statement, they said: ""Even where it is within the rules, meeting with friends and family over Christmas will be a personal judgement for individuals to take, mindful of the risks to themselves and others, particularly those who are vulnerable.
""Before deciding to come together over the festive period we urge the consideration of alternative approaches such as the use of technology or meeting outside.""
Published guidance for England gives further details of the rules for 23 to 27 December:
Scientists say a typical Christmas gathering at home is the type of environment where infections can spread.
The guidance also advises people to take precautions when meeting their Christmas bubble such as washing hands frequently and opening windows to clear potential virus particles.
In a video message from Downing Street, the prime minister described the agreement as a ""special, time-limited dispensation"", saying: ""This year means Christmas will be different.""
Boris Johnson said people must make a ""personal judgment"" about the risk of who they form a bubble with or if they visit elderly relatives., adding: ""Many of us are longing to spend time with family and friends... And yet we can't afford to throw caution to the wind.""
Wales' First Minister Mark Drakeford said it was ""not an instruction to travel, it's not an instruction to meet with other people. People should still use a sense of responsibility"".
Scotland's First Minister Nicola Sturgeon added: ""The virus is not going to be taking Christmas off, so although we want to give a little bit of flexibility for Christmas we are still urging people to be very cautious and to use this flexibility responsibly and only if you think it is necessary.""
Northern Ireland's First Minister Arlene Foster said she hoped people would have space to plan, adding: ""We of course recognise how important Christmas time is for so many people.""
Deputy First Minister Michelle O'Neill urged people to ""be responsible"", saying while they wanted to mark Christmas after such a ""desperate"" year the relaxations would increase opportunities for the virus to spread.
She added it was hoped that an alignment with rules in the Irish Republic could be achieved.
What to do about Christmas divides opinion.
Increased mixing indoors will certainly mean there is greater transmission of the virus.
But, as chief medical adviser Prof Chris Whitty said on Monday, there is a balance to be struck between the harm the virus can cause and the societal and economic impacts of trying to control it.
He called for a ""public-spirited approach"".
By that he means adhering to the restrictions in the lead-up to Christmas, being responsible with the opportunity the relaxation gives people, and then immediately switching back to compliance.
If that happens, any impact could be minimised - and, of course, it will be up to individuals to decide just how much they mix within the rules.
These are very fine judgement calls by ministers.
They hope Christmas will provide respite and help steel the public for what is clearly going to be a long, hard winter.
They also feel they have little choice, believing large numbers of people would ignore pleas not to mix - and this way they can provide advice on how to enjoy Christmas as safely as possible.
But there is also the risk by sanctioning it there will be more mixing than there would have otherwise been.
Transport Secretary Grant Shapps earlier said Christmas travellers should plan journeys carefully and prepare for restrictions on passenger numbers to allow for social distancing.
Meanwhile, the government has recorded another 608 UK deaths within 28 days of a positive Covid test. There have also been a further 11,299 cases of people testing positive for coronavirus.
Gyms and non-essential shops in all parts of England will be allowed to reopen from 2 December under a strengthened three-tiered system.
Areas will not find out which tier they are in until Thursday - and the decision will be based on a number of factors including case numbers, the reproduction rate - or R number - and pressure on local NHS services.
Prof Andrew Hayward, director of the UCL Institute of Epidemiology and Health Care, and a member of the government's Sage committee, told BBC Newsnight that allowing families to meet up over Christmas amounted to ""throwing fuel on the Covid fire"".
He said it would ""definitely lead to increase[d] transmission and likely lead to third wave of infections with hospitals being overrun, and more unnecessary deaths.""
Prof Hayward said while you cannot ban Christmas, he called for clearer messaging to families about the ""dangers"" of socialising and inter-generational mixing.
And Paul Hunter, professor of medicine at the University of East Anglia, suggested the relaxation of restrictions at Christmas will ""almost inevitably"" lead to an increase in transmission.
But he said: ""Providing that the new tier system is better managed than in October, any increase in cases could be relatively short-lived.
""After Christmas we will still have to live through a few more months of restrictions at least.""
Jillian Evans, the director of health intelligence at NHS Grampian, said the easing of restrictions over Christmas would cost lives.
""We've got winter weather, we know that people are more susceptible to infection over the colder period, and we've got a festive period where people will be socialising,"" she said.
""Those are facts, and I would rather be honest and tell you that those are the facts, and be truthful about it so people can understand the risks that they're taking.""
Kate Nicholls, chief executive of the UKHospitality lobby group, said there was ""muddled thinking"" over the Christmas rules and they would cause the sector more economic harm.
She said: ""Hospitality venues should be considered part of the solution for providing people a well-deserved safe and enjoyable Christmas, especially given that allowing multiple households to mix in the confines of private homes presents an exponentially greater risk."""
"BP is to sever links with three US-based trade associations, including the country’s main refining lobby, because of disagreements over their climate-related policies and activities. The decision comes after the UK oil corporation’s new chief executive, Bernard Looney, set an ambitious target to shrink its carbon footprint to net zero by 2050. To achieve this it will have to cut more greenhouse gas emissions every year than the amount produced by the whole of the UK. BP said it would pull out of the American Fuel and Petrochemical Manufacturers (AFPM), following in the footsteps of Shell and France’s Total, which left the lobby group last April. It will also quit the Western States Petroleum Association (WSPA) and the Western Energy Alliance (WEA). Over the past six months, BP has conducted a review of how its climate crisis-related policies and activities compare with those of 30 trade associations. BP cited material differences in its views on carbon pricing in relation to the positions adopted by AFPM and WSPA. It will not renew its WEA membership because of significant differences around the federal regulation of methane. Looney said: “BP will pursue opportunities to work with organisations who share our ambitious and progressive approach to the energy transition. And when differences arise we will be transparent. But if our views cannot be reconciled, we will be prepared to part company. “My hope is that in the coming years we can add climate to the long list of areas where, as an industry, we work together for a greater good.” The AFPM president and chief executive, Chet Thompson, said: “We are certainly disappointed with BP’s decision. We do not believe that BP’s trade association report accurately reflects AFPM’s position and commitment to finding solutions that address climate change. “As an active member of our executive committee, BP knows full well that AFPM recognises that climate change is real and that we are committed to engaging on and developing policies that enable our members to provide the fuels and petrochemicals that humanity needs to thrive in a sustainable way.” BP has identified a further five organisations with which it is only partially aligned on the climate crisis, and has told them about these differences. They are the American Petroleum Institute, Australian Institute of Petroleum, Canadian Association of Petroleum Producers, National Association of Manufacturers and the US Chamber of Commerce. The American Petroleum Institute has lobbied on behalf of oil companies to weaken the landmark US environmental laws enshrined in the the 50-year-old National Environmental Policy Act, which green groups fear will increase greenhouse gas emissions and accelerate the climate crisis. BP also successfully lobbied US policymakers directly for the regulation to be weakened. A Guardian investigation revealed last year that five of the world’s largest oil and gas companies – ExxonMobil, Chevron, Shell, BP and Total – spend nearly $200m (£153m) every year on lobbying governments to block or water down climate policies. BP spent $53m, including a $13m campaign to successfully derail a proposed carbon tax in Washington state.  Mel Evans, a climate campaigner for Greenpeace UK, was not impressed. “Judge a company by the company they keep. BP are sticking with the American Petroleum Institute, the lobby group who wrecked Obama’s methane restrictions,” she said. “When BP remains part of a group whose position they claim to oppose, it’s easier to get clarity on what they really think by looking at where their money is going – more oil, more gas, more climate change. “This report appears to be tokenistic, inadequate and hypocritical – like all of BP’s climate plans that we’ve seen so far.”"
"
In case you are just joining us, here is some background on the story below. I know the identity of the mole. The ball is now in CRU’s court. Steve McIntyre reports below and throws down the gauntlet.

Met Office/CRU Finds the Mole
 
by Steve McIntyre on July 28th, 2009
More news on the Met Office/CRU molehunt.
Late yesterday (Eastern time), I learned that the Met Office/CRU had identified the mole. They are now aware that there has in fact been a breach of security. They have confirmed that I am in fact in possession of CRU temperature data, data so sensitive that, according to the UK Met Office, my being in possession of this data would, “damage the trust that scientists have in those scientists who happen to be employed in the public sector”, interfere with the “effective conduct of international relations”, “hamper the ability to protect and promote United Kingdom interests through international relations” and “seriously affect the relationship between the United Kingdom and other Countries and Institutions.”
Although they have confirmed the breach of security, neither the Met Office nor CRU have issued a statement warning the public of the newCRU_tar leak. Nor, it seems, have they notified the various parties to the alleged confidentiality agreements that there has been a breach in those confidentiality agreements, so that the opposite parties can take appropriate counter-measures to cope with the breach of security by UK institutions. Thus far, the only actions by either the Met Office or CRU appear to have been a concerted and prompt effort to cover up the breach of security by attempting to eradicate all traces of the mole’s activities. My guess is that they will not make the slightest effort to discipline the mole.
Nor have either the Met Office or CRU contacted me asking me not to further disseminate the sensitive data nor to destroy the data that I have in my possession.
By not doing so, they are surely opening themselves up to further charges of negligence for the following reasons. Their stated position is that, as a “non-academic”, my possession of the data would be wrongful (a position with which I do not agree, by the way). Now that they are aware that I am in possession of the data (and they are aware, don’t kid yourselves), any prudent lawyer would advise them to immediately to notify me that I am not entitled to be in possession of the data and to ask/instruct me to destroy the data that I have in my possession and not to further disseminate the sensitive data. You send out that sort of letter even if you think that the letter is going to fall on deaf ears.
Since I am always eager to help climate scientists with these conundrums, I’ll help them out a little here. If, prior to midnight Eastern time on Thursday, a senior executive of the Met Office or the University of East Anglia notifies me that I am in wrongful possession of the data and directly requests me to destroy my copies of the CRU station data in question and thereby do my part in the avoidance of newCRU_tar proliferation, I will do so.
I will, of course, continue my FOI requests since I do not believe, for a minute, that their excuses have any validity nor am I convinced that the alleged confidentiality agreements actually exist nor, if they exist, am I convinced that they prohibit the provision of the data to me.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94ab520d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Our future depends on a transition away from fossil fuels. To map out a path, we need to get to grips with how, and why, the use of coal, gas and oil has risen to unsustainable levels. Most fossil fuels are consumed not by individuals, but by and through large technological systems, such as electricity networks, urban transport systems, built environments, and industrial and agricultural systems. While the media offers plenty of advice on how individuals can cut consumption, how to transform or supersede these technological systems is much less obvious. These unsustainable systems are deeply embedded in day-to-day life. For example, fossil-fuel-driven power stations on average use roughly three units of energy to produce one unit as electricity, while further energy is lost in transmission networks. Steel and cement are also produced in energy-inefficient ways, and used to construct heat-hungry buildings. To engineers, these are all huge opportunities for energy conservation. Car-based urban transport systems could hardly be more fuel-inefficient. That is why Atlanta in the US, a spread-out city dominated by suburban housing and car transport (including many SUVs), has 11 times the greenhouse gas emissions per head of Barcelona, Spain, which has a similar number of people, with similar income levels, but is more compact, with better public transport and a relatively car-free centre. The best way to interpret the growth in fuel consumption is by starting with the evolution of these technological systems, and the way they are embedded in social and economic systems. As I argued in my recent book Burning Up: A Global History of Fossil Fuel Consumption, such an approach can help us through a dizzying array of statistics, which themselves reflect a range of political views of consumption. Here is a guide to the most often-used ones: Consumption-per-head statistics measure a country’s total energy use, and divide it by the number of people in the country. Developing nations use these figures at international climate talks, to underline the historic inequality of consumption. For example, in 2014 the US consumed 31 times more energy products per head than Bangladesh; three decades earlier in 1984, it was 71 times more.  But no US citizen consumes those quantities of energy products directly. His or her share is mostly swallowed by the technological systems. Even those car drivers in Atlanta do not control their own consumption: it’s difficult to live there without a car, except in hardship. These numbers also hide inequality within nations, such as between an extravagant SUV driver and an unemployed cyclist. The prominent economist Thomas Piketty and his colleague Lucas Chancel tried to correct for that anomaly. Using wealth statistics, they estimated individuals’ fuel use, from the super-rich to the poorest, and found even more eye-watering contrasts. But such approaches still do not account for the technological systems that consume most fuels. Consumption-based emissions statistics filter out the effect of one aspect of international inequality. They count greenhouse gases emitted in manufacture according to the countries where products are used, instead of where they are made. So emissions “embedded” in a steel bar, produced in a carbon dioxide-belching coal-fired furnace in China and exported to the US, are counted as American. These numbers underline that, even today, the vast bulk of fossil fuel use is in, or for, the global north. Attributing emissions to fuel-producing companies helps to highlight the role of corporations. The Climate Accountability Institute’s brilliant research shows that nearly two thirds of carbon dioxide emitted since the 1750s can be traced to the outputs of the 90 largest fossil fuel and cement producers. Some headlines proclaim that these corporations are therefore “responsible” for climate change. But that’s only half the story. They produce fuel, others consume it. A list of the companies that do so – electricity producers, metals and engineering consortia, car makers, construction companies, petrochemicals and agriculture giants – would be longer and more complex, because fossil fuel use is so integral to all types of economic activity. So we need sector-by-sector breakdowns, and the International Energy Agency (IEA) publishes those. Flow charts can help visualise things, and materials flow researchers, such as the authors of Sustainable Materials With Both Eyes Open, do those. Then the numbers need interpreting; the bulky Global Energy Assessment had a shot at that. Companies and governments may be hiding things of course. There are tell-tale signs in IEA statistics: more than three times the amount of fuel used for global aviation is used on “other energy industry own use and losses” – that is, fuel the energy companies have either lost, or lost track of. And IEA reports on energy efficiency, which rely on companies to detail improvements, are full of complaints that crucial information is withheld. Military use is largely hidden. The US Department of Defense was in the 2000s the world’s biggest single consumer of commercial energy, wolfing down more than Nigeria. And at least we have that information: many countries simply don’t report military fuel use. Tracing fossil fuel use is not simple. The focus needs to shift from individual consumption to the big technological systems by and through which most fossil fuels are used, and the social and economic factors that make them work the way they do. A harsh light needs to shine on companies that consume the fuels, as well as the producers. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

We now have a large tax cut, but little tax reform. The primary objective of the $1.35 trillion cut passed Memorial Day weekend seems to have been to maximize revenue loss rather than to minimize tax distortions and disincentives. Not only did we end up with a package containing little bang for a lot of buck, but one that disappears into the sunset after 2010. Congress will have no choice but to revisit the issue, and it had better not wait until 2009 when anxiety about future tax rules will become unsettling.



The trouble with the tax law of 2001 — like those of 1990 and 1993 — is that Congress had no thought of its ultimate destination. It rushed into the debate with far too many political objectives and far too few economic principles. Specifically, it avoided discussion of the two most serious questions in tax reform — what should be taxed and how. That is, what should be the tax base and the tax rate?



When it comes to the tax base, a large and growing majority of public‐​finance economists agree that tax policy should move toward taxing income devoted to consumption. That means ending, or at least minimizing, today’s multiple taxation of personal and corporate savings. This would include taxes on profits, dividends, interest, capital gains and estates.



Economists of all stripes also agree that high marginal tax rates are the main source of tax distortions that suffocate economic growth. On tax rates, Congress at least got the direction right, if not the magnitude and pace.



Yet most of the plan fell short. For example, suppose we counted the liberalization of tax‐​deferred pensions and lightened taxation of estates as steps toward alleviating overtaxation of savings. Even then, only $187.6 billion of the $1.35 trillion tax bill, or 13.9 percent, could be counted as progress toward a consumption tax base. What about marginal rates? Reduction in the highest four tax rates meant a static revenue loss of $420.6 billion over 10 years. With only 14 percent of the tax cut devoted to treating savers more fairly, and only 31 percent devoted to reducing significant marginal rates, that leaves an amazing 45 percent of the new tax cut in some miscellaneous category — disconnected from the two fundamental goals of tax reform.



If George W. Bush truly cares about reform, he must ensure that the next phase of the tax debate is focused on enacting those few changes that would make a big improvement in the tax climate for entrepreneurs and savers while costing the Treasury little or nothing:



Estate and gift taxes: The new law on estate and gift taxation inspires no long‐​term confidence. Instead of reducing the tax rate to 20 percent by 2008, Congress opted to keep the tax at 45 percent through 2009, while more than tripling the exemption. The result is an inefficient tax, one that does the most damage for the least loot. Congress also opted to keep the gift tax forever, a superfluous move if it is serious about killing the estate tax. The estate tax is repealed in 2010, but only for one year. In that same year, assets would begin to be inherited at their purchase price rather than market value (carryover basis), so heirs would inherit old capital‐​gains tax liabilities. The bookkeeping burden alone would be horrendous. If carryover basis were maintained after 2010, when the estate tax is automatically reinstated, then heirs could end up brutally taxed on both the value of inherited assets and old gains on those assets.



Corporate income: There has been no attention paid to the corporate income tax since 1986. Treasury Secretary Paul O’Neill says he’d like to abolish the corporate tax — heresy among politicians but not among economists. But taxing corporations as we tax partnerships would be a tough sell. In the spirit of moving close to a consumption tax base, the administration should instead push for an end to the complicated and capricious depreciation of plants and equipment. Congress has wisely allowed small businesses to write off increasing amounts for capital investments as soon as the expense is incurred. That would be equally appropriate for capital outlays by all businesses, big or small. Expenses are expenses — not income. Expensing would lose little revenue over the longer‐ run, because writing off more today means less in the future, and because improved investment means a larger economy and tax base.



Capital gains: Needless complexities and distortions arise from imposing a penalty rate of 35 percent on short‐ term gains. There is no justification for abusing tax policy to distort the timing of asset sales. There would be no revenue loss from taxing short‐ and long‐​term gains equally. Smart investors realize short‐​term gains only when they have offsetting losses. But the special penalty on realizing short‐ term gains seriously depresses turnover and liquidity, and therefore potential tax collection on trades that don’t happen.



In this year’s initial tax skirmish, the Bush White House willingly closed its eyes to some of the worst abuses of good tax policy. The Senate was almost invited to dump the brunt of its alleged revenue savings on “rich” families in the 31 percent tax bracket, whose marginal rate will now dip to 28 percent rather than to 25 percent. Since the White House voiced no strong concern about reducing estate tax rates, it became vulnerable to the symbolism of a one‐​year repeal in 2010 at the expense of high tax rates, retention of the gift tax, and a new tax on inherited capital gains.



Serious and lasting improvement in tax policy must first begin with a clear discussion and explanation of the ideal destination. Serious and lasting political leadership must refuse changes that do not move the country toward that destination, and that have no durability.
"
"
Share this...FacebookTwitterSometimes timing is everything. Yesterday we looked at how a climate scientist is busy making “masterplans” for transforming our society instead of studying the climate.Some people were not happy about my reminder of where this sort of dubious activity can lead. After all, planning societies and claiming absolute truth is not the job of  scientists, especially those as dogmatised as Schellnhuber and those at his Potsdam Institute For Climate Impact Research.
Organising society is best left to democracy, where everyone’s vote is equal, and must be so – no matter how ignorant Schellnhuber thinks the population is. It’s not perfect, but it’s the best system we’ve got. “Masterplanners” in history have invariably led to disasters. And, as much as people do not want to be reminded of it, a not so little concentration camp in Poland is an example in the worst extreme.
Today, the worst part is that all these “masterplans” are based on flaky, often times fraudulent and alarmist science – all designed to promote panic rather than reason. This has already led to disastrous results (biofuel induced global hunger or landscape desecration by windmills to name two) and will surely lead to even greater disasters.
Another example of a so-called masterplan that has just come to our attention is the latest European proposal to ban all fossil-fuel-powered cars from cities by 2050 – again all this with little or no thought about the potential conseqeunces this junk-science-based regulatory hyper-zealousness could have. The UK Telegraph here writes:
Cars will be banned from London and all other cities across Europe under a draconian EU masterplan to cut CO2 emissions by 60 per cent over the next 40 years.
These masterplans are designed to regulate and control our lives, and have nothing to do with saving the planet – all confirmed by Siim Kallas, the EU transport commission, who said of the masterplan:
Action will follow, legislation, real action to change behaviour.”
The Association of British Drivers reacted harshly to the proposed restriction on mobility, and rightly so. Hugh Bladon, a spokesman for the BDA said:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I suggest that he goes and finds himself a space in the local mental asylum. If he wants to bring everywhere to a grinding halt and to plunge us into a new dark age, he is on the right track. We have to keep things moving. The man is off his rocker.”
This can be said about all the social engineering master planners out there who have taken it upon themselves to tell the rest of society how to behave.
But it doesn’t stop there. The transportation masterplan also includes air transport. The plan provides for the end of cheap flights and has the target of forcing more than 50% of all journeys above 300 km to be done by rail.
Air travel is also a big target of the enviro-zealots. So it should not come as a surprise that a new study is just out claiming that air travel is “even worse for the climate than they previously thought” , this reports the Austrian Der Standard, which leads with:
Scientists calculate the impacts of air traffic in the sky and come to unexpected results. Vapour trails from aircraft and their impacts apparently have a far greater impact on climate than previously thought.”
The scientists who claim this are Ulrike Burkhardt, of the German Centre For Aviation And Aeronautics, and Bernd Kärcher – in a paper published yesterday in Nature titled: Global radiative forcing from contrail cirrus. In the abstract the authors claim:
We show that the radiative forcing associated with contrail cirrus as a whole is about nine times larger than that from line-shaped contrails alone. We also find that contrail cirrus cause a significant decrease in natural cloudiness, which partly offsets their warming effect. Nevertheless, net radiative forcing due to contrail cirrus remains the largest single radiative-forcing component associated with aviation.”
The timing of this contrail paper and the EU masterplan for transportation just couldn’t be more convenient.
Share this...FacebookTwitter "
"

Former Vice President Al Gore is apparently on the hunt for votes for his prospective presidential campaign. He criticized the Bush administration on just about every ground at a dinner hosted by the Congressional Black Caucus Saturday night. But his greatest moment of unintended hilarity came when he charged that Attorney General John Ashcroft “is not respectful of civil liberties.”



That’s actually true. And Gore was right when he argued: “I believe one of the test of our nation is whether in times of grave challenge, we have the courage to be true to our deepest principles.” 



But while these are legitimate sentiments‐​whether you agree with them or not — it’s a bit curious to see them come from someone in the Clinton‐​Gore administration. After all, William Jefferson & Company perfected the practice of jackboot liberalism. 



Start with Attorney General Janet Reno, apparently defeated in her quest for the Democratic nomination for governor of Florida. On her watch the federal government burned the children in order to save them in its assault on the Branch Davidians in Waco, Texas. 



The same Justice Department supported Draconian restrictions on abortion protesters, including prohibiting the display of any “images” that were “observable” from abortion clinics. In the same vein, the Defense Department attempted to gag military chaplains, preventing them from discussing the Catholic Church’s Life Postcard Campaign regarding the President’s veto of legislation banning partial‐​birth abortion. Clinton, Gore & friends politicized the FBI, using it to justify the White House Travel Office purge. Presidential aides snooped through FBI files on potential administration opponents. 



The IRS audited a suspiciously large number of conservative foundations and groups. Proof that this reflected a conscious campaign was scarce, but no liberal groups reported undergoing similar reviews. The White House pressured the Treasury Department over the latter’s probe of Madison Guaranty, which financed the Clintons’ dubious Whitewater investment. 



The Department of Housing and Urban Development used intimidated opponents of federally subsidized housing projects. HUD launched dozens of investigations against local activists and groups; subpoenaed copies of organization membership lists and financial information, people’s diaries, and other records; demanded cessation of public criticism; and threatened protestors with prosecution for speaking out.



Similarly, in 1995 the U.S. Commission on Civil Rights issued subpoenas to two leaders of anti‐​immigration groups. The commission, whose chairman and staff director were appointed by the Clinton‐​Gore administration, wanted computer printouts, internal documents, reports, and other information from the organizations which were, of course, engaged in First Amendment political activities. The commission retreated, but only under congressional pressure.



Intimidation has been an administration hallmark. In 1994 President Clinton expressed outrage that radio talk‐​show host Rush Limbaugh could get on the air and “have three hours to say whatever he wants. And I won’t have an opportunity to respond.” White House Communications Director Mark Gearan called for radio talk shows to put on opposition — meaning administration — guests. Senior adviser George Stephanopoulos suggested resurrecting the misnamed “Fairness Doctrine,” to be enforced by Clinton‐​Gore appointees on the Federal Communications Commission, to regulate political broadcasts. 



Then there was the Department of Energy’s press‐​rating system. Reporters were judged on their coverage; sources were rank‐​ordered based on their opinion of the department. Department press secretary Barbara Semedo explained that a low rating “meant we weren’t getting our message across, that we needed to work on this person a little.” But, of course, getting the message meant spouting the department’s line.



The Food and Drug Administration’s grab for control over the tobacco industry was amazing: The FDA sought to prohibit even the use of brand names on non‐​tobacco products (such as lighters and t‐​shirts) and the use of non‐​tobacco brand names on tobacco products. (Alas, state attorneys general and the trial bar later achieved the same end through extortionate litigation.) The administration supported labeling restrictions on the alcohol industry, unsuccessfully urging the Supreme Court to void the firms’ First Amendment rights. The Clinton‐​Gore administration also backed FCC Chairman Reed Hundt’s abortive campaign to bar the advertising of distilled spirits on television. The same administration supported the Communications Decency Act, which would have attempted to ban the transmission of “indecent” materials over the Internet. Though well intentioned, the law, voided by the Supreme Court, inevitably meant heavy‐​handed federal censorship of the most free communication medium today. 



Although President Clinton spoke of reforming affirmative action, his administration promoted it instead. Perhaps the ugliest episode was his Justice Department’s support for the Piscataway, New Jersey school district that fired a teacher because she was white. Justice eventually flip‐ flopped in the case, but left its support for the government’s vast system of racial spoils otherwise undisturbed. The Education Department responded to California’s passage of Proposition 209 by threatening to prosecute the university system. 



Within the Clinton‐​Gore administration “diversity” became a code word for preferential treatment of politically advantaged groups. HUD required that employees not only implement federal diversity policy, but demonstrate “interest” and “personal commitment” to diversity, be active in “minority, feminist or other cultural organizations,” and participate in “cultural diversity activities outside of HUD.” The Department of Agriculture reassigned an employee for criticizing, on his own time, the department’s policy of offering spousal benefits to same‐​sex partners. 



There were also haphazard bureaucratic witch‐​hunts. The State Department fired Timothy Hunter, a retired Army counterintelligence officer who served in a number of government agencies before joining the State Department in 1990, for raising questions about agency administrative practices, discriminatory hiring and firing policies, make‐​work foreign‐​service jobs, and the State Department’s failure to defend the religious freedom of Americans working in Saudi Arabia.



But the harshest examples of jackboot liberalism have come from the Justice Department and federal law enforcement agencies. The Branch Davidian case continues to stand as an example of government run amok, persecuting people who wanted little more than to be left alone. Yet the Clinton‐​Gore administration steadfastly resisted attempts to hold anyone accountable in either Waco or Ruby Ridge, Idaho, where federal agents earlier killed the wife, son, and dog of loner Randy Weaver in order to arrest him in a case verging on entrapment.



The administration did, however, use the Oklahoma City bombing as an excuse to propose sweeping new federal powers — such as restricting the right of habeas corpus and expanding use of wiretaps — even though proponents were unable to point to a single example where civil‐​liberties protections prevented the police from deterring terrorism. Several of its proposals were turned into law. 



Clinton, Gore & Company, who constituted the most wiretap‐​friendly administration in U.S. history, essentially sought to eliminate the requirement of a warrant for searches from the Fourth Amendment. The president claimed to possess “inherent authority to conduct warrant‐​less searches for foreign intelligence purposes.” The administration required public‐​housing residents to sign away their constitutional right that authorities procure a warrant to search their dwellings and personal property. The Justice Department backed warrant‐​less (indeed, suspicion‐​less) drug tests for high‐​school athletes. The administration requested greater FBI authority to conduct “roving wiretaps,” without a court order. In the same way, Clinton‐​Gore officials pushed the Communications Assistance Act, which required telephone companies to retrofit their systems to ease police surveillance, supported restrictions on the sale of Internet encryption technology, and requested legislation forcing firms to give the government the “keys” to such technology. 



The administration was tougher than its predecessor on drugs. Marijuana arrests were up 50 percent over Bush‐​41 years and the Clinton‐​Gore administration consistently sought to frustrate state voters who approved measures to allow the desperately ill — victims of AIDS and cancer, in particular — from using marijuana to ease their nausea and pain. Administration appointees even threatened to prosecute any physician who provided a prescription for medical use of marijuana as allowed by state law. When asked about the criticism that sellers of crack were being punished far more severely than those who peddled cocaine, the president responded that penalties for the latter — which already ensured that minor drug dealers spend more time in jail than do many armed robbers, rapists, and murderers — should be raised. (As was his wont for shifting with the political winds, he later proposed moving modestly in the other direction, cutting the disparity from 100 to ten‐​to‐​one.) No more squishy, compassionate liberalism. The Clinton‐​Gore administration was enthusiastic about throwing people in prison. 



The administration also jailed people for resisting federal designation of their (very dry) property as “wetlands,” and committing other environmental offenses. In 1994 the Justice Department relaxed its control of environmental prosecutions in order to allow individual U.S. attorneys great latitude in prosecuting business. But the Justice Department retained the right to proceed if a local U.S. attorney refused to bring charges. 



Interior Secretary Bruce Babbitt attacked energy companies for criticizing administration scare mongering about global warming. He charged the firms with attempting “to distort the facts and to mislead,” adding: “I think that the energy companies need to be called to account, because what they are doing is un‐​American in the most basic sense.” He left unsaid how he would call “un‐​American” businesses “to account,” but climate scientists have long reported that the administration uses its control of research funding to reward researchers who tow the party line and punish those who express skepticism of climatic Chicken Littles. 



And the Clinton‐​Gore administration advanced additional thuggish policies and proposals — curfews for kids, random drug tests for welfare recipients and kids seeking drivers licenses, attacks on the requirement of a jury trial, ex post facto tax hikes, attempts to gain court sanction for uncompensated property takings, prosecutions implicating the double‐​jeopardy clause, pretentious claims of federal criminal jurisdiction, infringements of the Second Amendment right to possess a firearm, et al. 



Tim Lynch, assistant director of the Cato Institute’s Center for Constitutional Studies, covered these and more in his devastating study, Dereliction of Duty: The Constitutional Record of President Clinton. He observed that “Although President Clinton has expressed support for an ‘expansive’ view of the Constitution and the Bill of Rights, he has actually weakened a number of fundamental guarantees.” 



Perhaps any particular decision could be defended on one ground or another, but Wired magazine’s John Heilemann accurately called the Clinton‐​Gore civil‐​liberties record “breathtaking in both the breadth and the depth of its awfulness.” 



Former Vice President Al Gore says he is worried about our civil liberties. How quaint. Too bad he didn’t evidence a similar concern when the administration of which he was a key member was routinely putting power before liberty. There’s no reason to believe that a Gore administration would be any different than a Clinton‐​Gore administration: We all would almost certainly be paying for more jackboot liberalism with our freedoms.
"
"Transportation continues to generate a large proportion of emissions worldwide, even as emissions from other areas of the economy fall. In the EU, transport accounts for around 30% of CO2 emissions, and is rising. It’s the transport sector that is set to derail the EU’s overall emission reduction objectives.  Globally, the number of cars is expected to double by 2035, and the air travel industry is expecting its passenger volumes to triple by 2050, yet there has been little political acknowledgement of this issue. In the meantime, the airline and automobile industries go to great lengths to convince politicians and the public that technology alone can solve this problem, while the weight of scientific evidence suggests technology cannot rein in transport emissions sufficiently. There’s growing evidence to suggest we need tougher regulation on planes and cars, but there’s no political willingness to introduce restrictive policies. Our research suggests policies that would support sustainable transport have been largely ignored by European policymakers because of a number of “transport taboos”. These are issues that constitute a fundamental barrier to implementing any significant transport-related climate policy, ignored because of their political risk. If politicians violate a norm by grappling with one of these hot potatoes – even if the science clearly supports it – they can be punished by powerful lobby groups, by peers, or at the ballot box. In our paper, published in the Journal of Transport Geography, we identify a series of transport taboos. Aircraft and cars are the most important from an emissions perspective. One example is from Germany: even though opinion polls are in favour of a speed limit on the autobahn, and the importance of speed limits for reducing carbon emissions is well documented, no party is willing to touch the issue because of the outrage that would ensue from car associations, manufacturers and some drivers. Another taboo is the matter of who contributes to the volume of transport on our roads and in our skies. This is skewed heavily towards a small number of people, mostly from higher income classes, who are responsible for a large share of the overall distances travelled. This is particularly evident in the context of air travel. The travel patterns of the highly mobile need addressing, yet those from the political classes in power tend themselves to be included in this hypermobile group. Paradoxically the most environmentally aware are also among the most mobile, yet there is a distinct unwillingness among this section of society to fly less.  A further taboo is that most measures to reduce transport emissions in the EU are market-based, and so will disproportionally affect the less wealthy. For instance, car taxes are based on the CO2 performance of individual models, but this does not take account of income inequalities. A SUV might use twice the amount of fuel as a small car and be taxed twice as much, but its driver is likely to earn several times the average income. Lower income groups will shoulder a heavier relative burden. Tackling this taboo carries the same kind of political risk as increasing income tax rates in the higher tax bands.  Similar issues apply in the context of flying, where taxes disproportionally affect lower income groups, yet are not high enough to seriously impede the mobility patterns of frequent-flying elite. These continue to enjoy the effects of market distortions, where their flights are subsidised through the exemption of international air travel from VAT. And so the costs of flying, one the most environmentally harmful modes of transport, remain largely externalised. The airline industry and its lobbyists work hard to instil the idea that “mobility is freedom”, and that to restrict such mobility through regulation is nothing short of an infringement of that liberty; another taboo. If we are to have any chance of slowing the rise of transport emissions in the EU and worldwide, these and many more transport taboos need to be confronted and overcome. We need more research on these taboos and how they operate, so that strong supporting evidence can be put before political leaders. Even then, any change will need to be publicly palatable, and building that support will be hard. After all, for a great number of people this will still be an inconvenient truth."
"**With over a million new Covid-19 cases nationwide in the past week, according to Johns Hopkins University, public health officials are cautioning people not to gather in large groups this holiday season.**
Next week's Thanksgiving weekend poses a particular concern. Americans traditionally travel home to be with loved ones, taking part in meals, parades and shopping sprees.
The Centers for Disease Control and Prevention (CDC) strongly recommended that this year, Americans stay home and celebrate only with those they live with.
The nation's top infectious disease expert Dr Anthony Fauci urged Americans to ""think twice"" about holiday travel plans, adding that even ""innocent home gatherings"" with family and friends could result in several outbreaks.
As hard-hit regions have reimposed pandemic restrictions, even the Bidens and Trumps have made changes this year. President-elect Joe Biden revealed he and his wife, Dr Jill Biden, will have just one guest at their Thanksgiving dinner, while outgoing President Donald Trump and First Lady Melania Trump will remain at the White House for the weekend.
We asked people from around the country what changes they had made to their Thanksgiving plans and how they felt about 2020's pared-down holiday season.
_Daniela is a college student in Virginia, but is flying to meet family in two different states._
**How are you spending Thanksgiving?**
I'm visiting my dad's brothers who live in Atlanta and I'm trying to stay as safe as I can. We'll be here for a week, then next week I go back to Louisiana to be with my immediate family. I was a little worried because the last time I flew on a plane was June or July and there was not much separation between people.
I felt a lot more comfortable this time because there were seats between people and that helped out a lot. I've been wearing my mask, staying socially distanced from people I don't typically see very often and, on the plane, I made sure to wear a bit of a higher grade mask. I was debating whether to come home for Thanksgiving or for Christmas, and we decided it would be best to do Thanksgiving.
**What's the most difficult thing about Thanksgiving this year?**
Not being able to see my family as much as I would like. Keeping distance and making sure that I don't spread the virus is going to be a little hard. I was the one in the airport, so if we get Covid, it would probably be my fault. That added level of stress is going to be very difficult for me, but I'll just have to push through it.
**How are you feeling about the holidays this year?**
It's been rough and has put a damper on my life. On top of that, we had to deal with the election too, so it's been a lot. But it's getting better, and I'm feeling very happy now that I'm here with my family and get to see everyone.
_Rab is a Bangladeshi-born palliative care specialist raised in New Jersey._
**How are you spending Thanksgiving?**
We initially planned to see both sets of parents in New Jersey. Given that they are in their mid-to-late 70s, we have opted to stay in the Ohio area. I have two brothers and their families are there too.
We are masking up and limiting the number of people we are in contact with. If we do meet people, it's always socially distanced. If I walk with a friend outside, I do it masked up. And I expect lots of Zoom and FaceTime calls to friends and family.
**What's the most difficult thing about Thanksgiving this year?**
We are used to family gatherings. Not being able to spend holidays with family - especially not visiting parents - is hard, but it's the right and responsible thing to do. We want them around.
**How are you feeling about the holidays this year?**
I'm sure many people will be lonely without the physical contact of loved ones. It's sad, frustrating, lonely and exhausting at times. This should have been under better control sooner.
_Eliana is a professional dancer._
**How are you spending Thanksgiving?**
Nothing has changed too much. My family and I are meeting at one of my brother's homes and all of us are going to stay overnight because he's got enough room for us. If we do holidays at someone's house that's far away, we generally get a hotel, but because of Covid, we would rather stay together.
**What's the most difficult thing about Thanksgiving this year?**
There's nothing difficult in particular. Things just get a little more sad over the years, meeting together and seeing all the kids grow up.
**How are you feeling about the holidays this year?**
Generally, I look back at where I was this time last year, hoping I'm in a more progressive place in my life. This year, I feel pretty neutral because I'd love to have moved further in my career but that was taken away by the pandemic - an uncontrollable variable - but I'm just grateful to be here. So I'd say there's more positive feelings than negative, because at least we're here and we get to celebrate the holidays with people we care about.
_Meredith is an administrative assistant at a university._
**How are you spending Thanksgiving?**
We normally travel to my parents' house in Raleigh, North Carolina or to an extended family event with my husband's family on Long Island in New York. Our families have been understanding about our desire not to travel this year. My husband and I will have just one friend over, who has been part of our 'bubble' all year.
**What's the most difficult thing about Thanksgiving this year?**
Watching all of the people who are still planning big gatherings despite everything that's going on, and justifying it as ""but my [grandparent/parent] is old, maybe this is the last holiday we'll have with them!""
It very well could be, if you bring Covid to the table to share.
**How are you feeling about the holidays this year?**
Exhausted. Friends are putting up Christmas trees and lights already, using the excuse that it's been a slog of a year and everyone could use a little cheering up.
Seeing decorations in stores and aggressive holiday shopping campaigns earlier than normal definitely is not helping with my already-distorted sense of time's passage.
_Ryan is a graduate student at the University of Wyoming._
**How are you spending Thanksgiving?**
I am staying home and avoiding any unnecessary travel or contact with other people. Normally we travel to spend the Thanksgiving holiday with our family. We had planned to make the 12-hour road trip after self quarantining for two weeks. But even with making plans to never interact with other people on the drive, it just isn't fair to create the additional risk of a car accident which would put unnecessary pressure on the doctors and nurses at our hospitals that are already overwhelmed.
At this point the only respectful and responsible thing to do is to self isolate. It sucks, but in the end it is a small sacrifice.
**What's the most difficult thing about Thanksgiving this year?**
Hearing the disappointment in our parents' voices when we said we would not be coming home. I'm sad I will not be able to see my grandma. I miss being in their presence.
**How are you feeling about the holidays this year?**
Not well. This is a sombre time and I am feeling like the holiday season this year needs to be one of remembrance and reflection. In a sane world we would be laser focused on reducing transmission of this plague, making sacrifices now for the long term collective good.
We would have compassion and empathy for our fellow humans, honouring the heroes fighting this disease in the hospitals around the country."
"
Share this...FacebookTwitterBy Ed Caryl
There have been multiple studies of solar influence on global and regional temperature changes. Just of few of them are here, here, here, and here. This author made a contribution to the question here. Many of these studies show a some correlation of the sun’s output versus temperature, but most researchers think that this relationship is a weak one, contributing from one third to one half of the observed warming. So, from where is the remainder coming?
Figure 1: Heat map of Buffalo, New York, USA, NASA image.
Pierre and this author have written about urban warming here and here. It is clear from the infrared satellite photos by NASA that urban heat islands are both more wide spread and warmer that previously known. Particularly in the eastern half of the U. S., the heat islands are blending together, raising the temperature of the whole region. How much?
No one (to my knowledge) has researched what part of the global temperature rise is due to energy use. All energy use ultimately goes to heat. This is what causes the heat islands. Much of energy usage is immediately wasted as heat: cooling towers at power plants, automobile radiators, heat loss through home insulation, heat loss up the chimney, electric motor heat loss, heat from electric lights, are just a few examples of heat losses. Even energy used to transport things is ultimately lost as heat. Just moving something through the air, heats the air. For this reason, we can convert all the energy used into watts and calculate the temperature rise. In these calculations, the energy used will be considered over particular land areas.
The first area considered is the U.S.A. There are figures for the energy consumption in the U. S. in 2005, 29 Pwh (Petawatt hours, a PetaWatt is 1015 watts. That is 1 with 15 zeros. The area of the contiguous U. S. (the lower 48 states) is 8,080,464 km2. If we divide the energy used by the area, we get 3,589 Wh/m2. Divide that by 8766 hours in a year we get 0.409 W/m2, or 9.826 Wh/m2/day, as the average energy dumped into the environment in the U. S. in 2005.
The sun provides about 4.5 kW/m2/day on a horizontal, flat, black, surface at the average latitude of the U.S. The average albedo of the earth’s surface is 0.3, which means that on average, the surface will absorb 70% of the insolation (solar energy) that strikes it. This means that the effective heating will be 70% of 4.5 kW/m2/day, or 3,150 W/m2/day. The energy dumped into the environment by every American’s energy use is 0.312% of the sun’s energy. This will raise the temperature by 0.312%. The average temperature in the U. S is 11.6°C or 284.75°K. The temperature rise will be about 0.89°C.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




As you can see on the temperature chart below from NOAA, this will neatly take care of the temperature rise seen in the last 25 years.
Figure 2. Source: http://www.noaanews.noaa.gov/stories2009/images/1208natltemp.png
What about the global picture? Figures are available for global energy consumption for 1988 through 2006. As most of this consumption is in the northern hemisphere, and that is where we see the most warming, the calculation uses the northern hemisphere land area, 100,228,500 km2. The same average insolation value will be used as in the U. S. example above, 3150 W/m2/day. Figure 3 is a chart of global and hemispheric temperature trends and the calculated temperature rise in the land area of the northern hemisphere due to energy usage. The temperature anomaly data comes from GISS/NASA here. The chart may underemphasize the temperature rise due to energy use because energy use is localized to a limited local areas in few countries: the US, Europe, including Russia, and China. The temperature data may also be overemphasized because the surface temperature measuring sites are at airports and other urban settings that are even warmer than the average location.
Figure 3. NH, SH, and Global temperature rise, and the rise caused by energy use (dark blue line).
Dr. Richard C. Wilson states that 50% of the temperature anomaly is due to total solar irradiance changes (TSI). Drs. Judith Lean and David Rind make predictions based on TSI along with ocean cycles. Neither mentions any influence from energy usage.
Here are two maps. The first is the lower troposphere temperature rise over the period 1978 to 2006, much the same period as the chart above.
Figure 4. Source: University of Alabama Huntsville.
The second map is of energy usage by country.
Figure 5. Source: Energy Information Administration, U.S. Department of Energy, 2006.
Though the energy usage map is very coarse, one can see that the maximum energy usage area roughly coincides with the same areas as the northern hemisphere temperature rise.
The planet has been heating in the last two hundred years. Some of that change in temperature comes from ocean cycles, some from the sun and its various influences, and some from man. Much of the anthropogenic (man caused) portion is simply energy use that has dramatically increased in the last fifty years. A thorough, honest, investigation needs to be done before we blame it all on CO2.
Share this...FacebookTwitter "
"
A Guest Post by Basil Copeland



Like many of Anthony’s readers here on WUWT, I’ve been riveted by all the revelations and ongoing discussion and analysis of the CRUtape Letters™ (with appropriate props to WUWT’s “ctm”). It might be hard to imagine that anyone could add to what has already been said, but I am going to try. It might also come as a surprise, to those who reckon me for a skeptic, that I do not think that anything was revealed that suggests that the global temperature data set maintained by CRU was irreparably damaged by these revelations. We’ve known all along that the data may be biased by poor siting issues, handling of station dropout, or inadequate treatment of UHI effects. But nothing was revealed that suggests that the global temperature data sets are completely bogus, or unreliable.
I will return to the figure at the top of this post below, but I want to introduce another figure to illustrate the previous assertion:

This figure plots smoothed seasonal differences (year to year differences in monthly anomalies) for the four major global temperature data sets: HadCRUT, GISS, UAH and RSS. With the exception of the starting months of the satellite era (UAH and RSS), and to a lesser degree the starting months of GISS, there is remarkable agreement between the four data sets – where they overlap – especially with respect to the cyclical pattern of natural climate variation. This coherence gives me confidence that while there may be problems with the land-sea data sets, they accurately reflect the general course of natural climate variation over the period for which we have instrumental data. While we need to continue to insist upon open access to the data and methods used to chronicle global and regional climate variation, and refine the process to remove the biases which may be present from trying to make the data fit the narrative of CO2 induced global warming, it would be wrong to conclude that the “CRUtape Letters” prove that global warming does not exist. That has never really been the issue. The issue has been the extent of warming (have the data been distorted in a way that would overstate the degree of warming?), the extent to which it is the result of natural climate variation (as opposed to human influences), and the extent to which it owes to human influences other than the burning of fossil fuels (such as land use/land cover changes, urban heat islands, etc.). And flowing from this, the issue has been whether we really know enough to justify the kind of massive government programs said to be necessary to forestall climate catastrophe.
Figure 2 plots the composite smooth against the backdrop of the monthly seasonal differences of the four global temperature data sets:

Many readers may recognize the familiar episodes of warming and cooling associated with ENSO and volcanic activity in the preceding figure. With a little more smoothing, we get a pattern like that depicted in Figure 3, which other readers may notice looks a lot like the cycles that Anthony and I have attributed to lunar and solar influences (they are the same):

In either case, the thing to note is that over time climate goes through repetitive episodes of warming and cooling. You have to look closely on Figures 2 and 3 – it is much clearer in Figure 1 – but episodes of warming exist when the smooth is above zero, and cooling episodes exist when the smooth is below zero. Remember, by design, the smooth is not a plot of the temperature itself, but of the trend in the temperature, i.e. the year to year change in monthly temperatures. The intent is to demonstrate and delineate the range of natural climate variation in global temperatures. It shows, in effect, the trend in the trend – up and down over time, with natural regularity, while perhaps also trending generally upward over time.
Which brings us to Figure 1. Here we are focusing in on the last 30 years, and a forecast to 2050 derived by a simple linear regression through the (composite) smooth of Figure 3. (Standard errors have been adjusted for serial correlation.) There has been an upward trend in the global temperature trend, and when this is projected out to 2050, the average is 0.114°C per decade ± 0.440°C per decade. Yes, you read that right: ± 0.440°C per decade. Broad enough to include both the worst imaginations of the IPCC and the CRU crowd, as well as negative growth rates, i.e. global cooling. Because if the truth be told, natural climate variation is so – well, variable – that no one can say with any kind of certainty what the future holds with respect to climate change. Be skeptical of any statistical claims to the contrary.
I think we can say, however, with reasonable certainty, that earth’s climate will remain variable, and that this will frustrate the effort to blame climate change on CO2 induced AGW. Noted on the image at the top of this post is a quote from Kevin Trenberth from the CRUtape Letters™: “The fact is that we cannot account for the lack of warmth at the moment, and it is a travesty that we can’t.” Trenberth betrays a subtle bias here – he cannot acknowledge the recent period of global cooling. It is, rather, “a lack of warmth.” But he is right that it is a “travesty” that we cannot fully account for the ebb and flow of earth’s energy balance, and ultimately, climate change. I think Trenberth just sees it as a lack of monitoring methods or devices. But I think there still remains a considerable lack of knowledge, or understanding, about the mechanics of natural climate variation. If you look carefully at Figure 1, you will notice that there seem to be upper and lower limits to the range of natural climate variability. On the scale depicted in Figure 1 (the scale is different with other degrees of smoothing), when warming reaches a limit of approximately 0.08-0.10°C per year, the warming slows down, and eventually a period of cooling takes place, always with the space of just a few years. Homeostasis, anyone? While phenomenon like ENSO are the effect of this regularity in natural climate variation, they are not the cause of it.
In my opinion, what is the real travesty of the global warming ideology is the hijacking of climate science in the service of a research agenda that has prevented science from investigating the full range of natural climate variation, because that would be an inconvenient truth. We see this, quite clearly, in the CRUtape Letters™ where the Medieval Warm Period is just “putative,” and a rather inconvenient truth that needs to be suppressed. Or the “1940’s blip” that implies that global temperatures increased just as rapidly in the early part of the 20th Century, as they did at the end of the 20th Century, an inconvenient truth at odds with the narrative preferred by the IPCC. 
It is a truism that “climate varies on all time scales.” With respect to the variability demonstrated here, I’m convinced that someday it will be acknowledged that variability on this scale is dominated by lunar and solar influences. On longer scales, such as the ebb and flow from the Medieval Warm Period, through the Little Ice Age, and now into the “Modern Warm Period,” I do not think climate science yet has any real understanding of the underlying causes of such climate change. If we are, as seems possible, on the verge of a Dalton or Maunder type minimum in solar activity, we may eventually have an answer to whether solar activity can account for centennial scale changes in earth’s climate. And I do think it is reasonable to conclude, at the margin, that human activity has had some influence. It is hard to imagine population growing from one to six billion over the past one and a half centuries without some effect. Most likely, the effect is on local and regional scales, but this might add up to a discernible impact on global temperature. But until all of the forces that determine the full range of natural climate variability are understood better than they are now, there is no scientific justification for the massive overhaul of economic and government structures being promoted under the guise of climate change, or global warming.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90e487d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The _Washington Times_ ’ columnist Don Lambro recently wrote that some supply‐​siders (not me) were wary of Arnold Schwarzenegger because adviser Warren Buffett said it makes no sense to pay more property taxes on his $500,000 house in Omaha than on his $4 million house in Laguna Beach. 



The Buffett remark bothered many but just struck me as naive. Having lived in California for 27 years, I assumed a half‐​million bucks in Omaha probably buys a much nicer place than $4 million in a California beach town. Mr. Buffett has owned that California house for decades, so he didn’t pay much for it. 



Ever since the 1978 Proposition 13, California’s constitution requires that property taxes be initially based on what you paid for a house (not on what you might sell it for) and annual increases are limited to 2 percent. Since all houses eventually are sold, this works out surprisingly well. The assessed value of California property rose 4.8 percent a year from 1990 to 2002, virtually identical to the 4.9 percent annual growth of personal income. Mr. Schwarzenegger obviously understands this, though Mr. Buffett did not. 



What made no sense was for Mr. Buffett to even hint about a policy change that would require an unlikely change in the state constitution. There are much easier and quicker chores to attend to. Mr. Buffett’s blunder demonstrated it made no sense for Mr. Schwarzanegger to go all the way to Nebraska in search of economic advice from a noneconomist. The tens of thousands who flee California taxes each week are not heading for Nebraska, which the Tax Foundation rates as having the 45th worst tax climate in the nation (California is 49th). 



If Mr. Schwarzenegger were looking for economic advice from a Democrat, he could have called Robert Hall at Stanford University, co‐​architect of the Hall‐​Rabushka flat tax. If he would settle for a Republican, he could have asked Mike Boskin, David Henderson, Art Laffer or Ben Zycher. There’s plenty of local talent, and not just among such older giants as George Shultz. Indeed, Mr. Schwarzenegger himself seems fundamentally sound on the critical tax and spending issues, and so do rivals Bill Simon and Tom McClintock. 



The trouble with California taxes is not that property taxes are too low, but that income and sales taxes are way too high. The personal income tax rates are high enough to provoke an exodus of skilled people — a “brain drain.” Corporate tax rates are high enough to provoke an exodus of business capital — “capital flight.” Sales tax rates are high enough to provoke wholesale tax avoidance. 



California’s tax revenue per capita ranks as “only” the sixth‐​highest in the nation, but comparing tax revenues understates the damage. The state’s uncompetitive tax rates ultimately yield more economic distress and less revenue because they erode the tax base. Profitable companies leave the state, while companies with chronic losses stay. People in top tax brackets leave the state, while nontaxpayers who expect to gather tax‐​financed benefits arrive in droves. 



In 1991, as State Sen. Tom McClintock recently recalled, “An 18 percent increase in the sales tax and a 15 percent increase in upper brackets of the income tax were supposed to produce a net of $7 billion of new revenues. But they didn’t. … We didn’t take in $7 billion more — we took in $1 billion less. We lost another $1 billion the next year.” 



Indeed, real per capita personal income fell 5.6 percent during the three years following the 1991 tax increase, even though the national economy was recovering. Californians now arguing for another tax increase — notably Lt. Gov. Cruz Bustamante — are conveniently forgetting what happened after that was tried in 1991. 



There is obviously plenty of room for spending cuts, since California’s budget grew by 44 percent in the past four years. The appearance of tiny spending cuts in the new July 29 budget was a fraud. The state’s legislative analyst’s office reports, “Most of this increase can be explained by four factors: the [vehicle licensing fee] rate increase … new federal funds, borrowing to cover the state’s 2003–2004 pension obligations and the MedicCal accounting shift from an accrual to cash basis.” 



There was predictable whining after fees at the California State University and University of California were finally raised a bit after remaining unchanged for eight years. California has long been overtaxing some residents so people like me could get a valuable college education almost for free. Once that education begins to raise our future income, however, many of us leave the state to avoid steep taxes. 



The 9.3 percent income tax also applies to capital gains (though Bill Simon wants to cut it to 5 percent), which is one reason aging investors like me could never be enticed to bring our capital back to California. Sell stock, and the state will hit you with a big tax — don’t sell, and they’ll get you with an estate tax. But that only works if intended victims don’t know how to vote with their feet. 



California’s personal income tax is 9.3 percent on income above $38,291 — a definition of “rich” that could only make sense to Hollywood liberals. That brutal tax rate makes it difficult for companies to recruit and retain skilled people without paying higher salaries. That, in turn, raises the cost of doing business for skill‐​intensive industries and makes them uncompetitive until they move jobs out of California. 



California’s corporate tax is above 8.8 percent — much higher than any nearby state. The corporate tax in Washington and Nevada is zero. Companies with chronic losses don’t mind staying in California, but many profitable firms become much more profitable (after taxes) by relocating. 



The sales tax varies locally from 7.25 to 8.5 percent, which is high enough to foster significant tax avoidance (e.g., shopping in Oregon, which has no sales tax, or ordering from another state). States with lower sales tax rates than California collect more money from this tax. They also collect more income taxes because a lower sales tax helps in the building of profitable retail businesses, who hire more people. 



California never will be able to compete well for new businesses and the skilled people those firms require until its tax system becomes reasonably competitive. At a minimum, that means sales and income tax rates no higher than 6 percent. 



A state with no sales tax like Oregon can get away with a higher income tax. A state with no income tax like Washington can get away with a higher sales tax. But Sacramento’s politicians have been foolishly rapacious with both income and sales taxes. That is killing the state economy, and it has to stop. 



There are two or three gubernatorial candidates who seem to get it. I suspect or at least hope Arnold Schwarzenegger may be one of them. 
"
"The most recent 12% of time on Earth is a striking anomaly when compared with the great bulk of our planet’s evolution. After 3 billion years or more of Earth as a microbial world, cells found ways to grow and assemble in their millions to build eyes, guts, muscles, nerve systems with brains, skeletons and the rest of the complex structures that form the mobile, sentient, marvellously various animals that fill our now familiar world.  The speed of this transition, seen by geologists in the sudden appearance of complex fossils such as trilobites in layers of rock, seemed so shockingly abrupt that it amazed and worried even Charles Darwin. As this flowering of complex multicellular life is used to mark the beginning of the Cambrian Period of Earth time, 541m years ago, it has long been called the “Cambrian explosion”. We now know it was not quite so abrupt. The evolution actually took place in distinct stages over more than 30m years – the pioneering geologist Preston Cloud called it “the Cambrian eruption”. The period has, somewhat mysteriously, left behind more than its fair share of amazing fossil localities called “Lagerstätten”, where not only hard skeletons but soft and delicate tissues are preserved, to give a much clearer window on the totality of life than the usual fragments of shell and bone. The classic in this respect was long the Burgess Shale, high up on Mount Stephen in British Columbia, Canada, and mined for its wondrous fossils for more than a century. In the past three decades it has been joined by China’s Chengjiang deposits.  Together these have given a detailed picture from near the dawn of animal life in the marine realm (the land, then, was still largely barren): a wealth of arthropods, worms, lamp shells, sponges, chordates, and even some animals that still defy biological assignment. Now another Cambrian Lagerstätte has turned up – and the first results, just published in Science, suggest that it may rival those of the Burgess Shale and Chengjiang. It is also in China, in Qingjiang in Hubei province, and is about the same age as the Chengjiang, some 518m years old, and so in the early part of the Cambrian Period. Indeed, it formed on the same general stretch of continental shelf sea as the Chengjiang fossils, about 1,000km away, and seemingly in somewhat deeper waters. What is surprising is that not only are the fossils just as exquisite, but they represent quite different animal communities. The Cambrian seas were clearly more diverse than thought, even in those early days. Although the Qingjiang fossils do include many worms, arthropods and sponges, they also are prolific in animals possessing the most delicate and translucent of tissues, such as jellyfish, here preserved with mouths and tentacles, and their distant relatives “comb jellies”. These are rare in the Burgess Shale and Chengjiang deposits, but among the Qingjiang fossils there are many astounding examples, flattened on the shale surfaces and preserving fine details of these softest of anatomies.  The comb jellies may be the earliest form of animal – a title that they currently contest with the sponges. Both these kinds of animal are preserved here, in numbers and fidelity that may help resolve the dispute. Other strange and wonderful animals appear in the Qingjiang strata, and represent curious and perhaps profound trends in evolution. There are kinorhynchs, for example – these are the “mud dragons”, animals that are today obscure because they are part of the “meiofauna”, those tiny creatures that live between grains of sediment on the sea floor.  Here, three new fossil forms have been found, some up to 4cm long, rather than the contemporary sub-millimetre size. These Cambrian giants suggest that some of today’s meiofauna started off “normally” sized, and then became miniaturised – for good. It is a true cornucopia. How did it form? The same kind of quickfire preservation process is mooted as for the Burgess Shale and the Chengjiang fossils: the animals were caught up in mud slurries, and carried down to deep, oxygen-starved parts of the sea floor to be rapidly buried in the stifling mud. It makes sense – but then such conditions and processes persisted long after the Cambrian, and yet were rarely associated with such bonanza fossil finds. There is much that remains mysterious about the dawn of life as we know it – but the Qingjiang fossils, as we study them more, will slowly shed light on these enigmas."
"
From the “weather is not climate department” another report of ice further south than has been recently experienced. Here’s a picture from this China Daily news story:

From Maritime Global.net
CHINA PORTS FREAK WEATHER ALERT
By David Hughes
Published: Tue, 26 January 2010
Freak weather conditions and/or abnormal weather patterns have been  reported in several parts of the world during recent months warns the  American P&I Club. One of the latest examples is a significant  build-up of sea ice in some major northern Chinese ports, the volume  exceeding, it says, anything experienced in more than 30 years.
In an alert to its members, the club says the problem is centred around  Bohai on the northern Yellow Sea coast, affecting ports such as Bayuquan  and Dalian. At Bayuquan, patches of ice 500-600mm thick have formed in  some places, while lesser patches have been seen in the immediate  vicinity of the port.
Three icebreakers are working to avoid delays to ships, while the local  Maritime Safety Authority is strictly supervising inbound and outbound  vessel traffic.
Other northern ports – such as Jingtang, Caofeidian and Xingang – are  said to be not so seriously affected. On January 17, the Chinese  National Sea Weather Forecast Station reported that floating ice around  Liaodong Gulf extended as far as some 60 nautical miles from shore, at  Bohai Gulf around 22 miles, Northern Yellow Sea around 14 miles, and  Laizhou Gulf around 33 miles.
However, with more cold weather fronts expected later, ice coverage  around the Bohai coast could expand, according to the club’s  correspondents in China, Huatai Agency & Consultant Services Ltd.
The club advises that vessels scheduled to call at northern ports,  especially Bayuquan, should be ready for extreme temperatures and ensure  Port State Control requirements are strictly followed to avoid  unnecessary delay.
===============
Here’s another news story from AsiaOne News:
Bohai bay turns into block of ice 
h/t to Ron De Haan (Note: please fix your email!!)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8ef70049',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWe’ve seen that kind of thinking before. Authoritarianism.
Forget democracy, human freedom and free markets. These concepts, which have made today’s human prosperity and long lifespans a reality for humans, wherever and whenever they have been given the chance to work, are upsetting a small but very elitist group of individuals who view these concepts as a threat to their world view.
German Hans Joachim Schellnhuber has a “Master Plan” for society. Source: http://nachhaltigkeit2009.commerzbank.de/reports/commerzbank/annual/2009/nb/German/3010/_innovations-chancen-nutzen_.html
 
The English edition of Der Spiegel has a recent interview with Prof Hans Joachim Schellnhuber, Director of the Potsdam Institute for Climate Impact Research. In this interview Schellnhuber announced that he would unveil his “Master Plan” for transforming society – one no doubt that suits his world view. In Schellhuber’s view, human society needs to be scaled back and managed by an elite group of “wise men” who know what is best for the rest.
Schellnhuber’s contempt for today’s organisation of western soctety is illustrated by his statements. For example Der Spiegel stumps Schellnhuber with a simple question:  “Why is it that your messages haven’t been all that well received until now?” Schellnhuber responds:
I’m neither a psychologist nor a sociologist. But my life experiences have shown that the love of convenience and ignorance are man’s biggest character flaws. It’s a potentially deadly mixture.”
Oh the contempt. So we are all too comfortable and ignorant. It’s time for “wiser men” to think for the rest of us.  Your message, Mr Schellnhuber, has not been well received because it is anti-democratic and authoritarian. Your kind of thinking is a threat to the principles of Germany’s Constitution and so belongs under government observation.
Schellnhuber indeed has a very poor understanding of history. History teaches us a different lesson – that it is not the combination of love for convenience and ignorance that is man’s greatest flaw, rather it is the combination of towering arrogance and ignorance that is man’s greatest flaw. That is the real threat to society. Schellnhuber also denies climate history and concocts his own fraudulent version to warrant an authoritative intervention.
The belief that the planet needs to be dictated by a higher authority is what makes people like Schellnhuber so dangerous to democracy. Their impatience and frustration with democracy, and their claim to have superior knowledge, remind us of others who have led us down very dark paths back in the 20th century.
These people are convinced they are all-knowing. Der Spiegel asks: “Do you feel that the government’s abrupt change of course in relation to its energy policy is adequate?” Schellnhuber replies:
No. It can only be the beginning of a deep-seated shift. The German Advisory Council on Global Change, which I chair, will soon unveil a master plan for a transformation of society. Precisely because of Fukushima, we believe that a new basis of our coexistence is needed.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A master plan for transforming society drafted by a climate scientist, a person who admits having no background in sociology. This plan no doubt will be asserted by authority, and not democracy. Schellnhuber and others have indicated many times that democracy is flawed and is a nuisance. It’s getting in the way of solving the “world’s crises” (like zero temperature growth over the last 15 years).
Well, here is a little history lesson for the all-knowing Herr Schellnhuber and his bunch, who are frustrated with democratic principles and individual freedom, on where this type of contempt can take us if left unchecked:

This is where people were turned into numbers. Into this pond were flushed the ashes of 4 million people. And that was not done by gas. It was done by arrogance. It was done by dogma. It was done by ignorance. When people believe they have absolute knowledge with no test in reality, this is how they behave. This is what men do when they aspire to the knowledge of gods…Every judgement in science stands on the edge of error, and is personal.”
=================================================
I’ve brought up this Ascent of Man clip before, and am bringing it up again, and will continue to bring it up in the future. Watch the entire following series Ascent of Man, Knowledge or Certainty:
Ascent of Man – Knowledge or Certainty
Other links:
EU master plan to ban cars from cities by 2050.
ALERT-German Climate Advisor proposes creation of a CO2 budget
Share this...FacebookTwitter "
"
And yet…it was claimed to be a “success”. Note to organizers: the media is laughing at you.
Snowstorm squelches climate change protest
By Judy Fahys, The Salt Lake Tribune Updated: 12/30/2009 06:36:58 PM MST
From KSL-TV - click for video and full report
A downtown [Salt Lake City] protest of the climate change talks in Copenhagen became a victim of Wednesday’s snowstorm.
“Not many people showed up because of the blizzard conditions,” said organizer Clea Major, an international studies student at the University of Utah.
It didn’t take long for the six friends to pack up a bullhorn and posters they’d planned to use for their “scream-in,” an outlet for their frustration about the failure of the Copenhagen climate talks earlier this month to curb the pollution blamed for climate change.
Still, they chatted with a few passers-by during the commuter-hour protest near the Gateway, and explained that, blizzard aside, climate change is expected to bring chaos to the global climate, said Major.
She called Wednesday evening’s effort a success and possibly the first in a series. As for the snow, it’s not entirely new; a protest she attended last year in Washington, D.C., suffered a similar fate.
“There is always the irony element,” Major said.
###
============================================
Here’s the original announcement from KCPW in Salt Lake:
Protesters Scream for Climate Change
12.30.2009 by Elizabeth Ziegler
(KCPW News) Climate change activists will stage a “scream-in” today at the Gateway Mall in downtown Salt Lake City to vent their frustrations about the Copenhagen Accord adopted by global leaders two weeks ago. University of Utah student Cléa Major says the demonstration is intended to call attention to the fact that the accord doesn’t require countries to reduce greenhouse gas emissions.
“Basically we want your average shopper to go home tonight after work and say, ‘Man, you know, I was returning my Christmas gift and there were these people screaming on the sidewalk, you know, what’s that all about?’” Major says. “We wanted to make an impression on people and we wanted to maybe put it in a location where it wasn’t necessarily expected to get people out of the post-Christmas haze to just sort of listen.”
Major sees the Copenhagen Accord as a failure, akin to the cap and trade legislation, The American Clean Energy and Security Act, that activists had hoped would set the tone in Copenhagen. The bill has stalled in the Senate after garnering a narrow margin of support in the House of Representatives this summer. All three of Utah’s Congressmen voted against it.
Major says the scream-in will be cathartic for those participating, who see the Copenhagen Accord as a missed opportunity to reverse climate change.
“We just all felt so helpless, we felt betrayed by this,” Major says. “We felt helpless and we felt furious because it was like we had just been looking to this to be the big thing that could turn it around or at least be a major jumping off point. And it just kind of stalled and failed and now there’s kind of this feeling of we don’t really know where to go next.”
The scream-in takes place at 5:30 p.m. on the northwest corner of 400 West and 200 South at the Gateway Mall.
﻿


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f576028',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterProf. Claes Johnson’s blogsite here reports on how an IPCC Vice Chair recently used his UN position to suppress scientific dissent and discourse. Hat-tip: Hans Labohm.Johnson, professor of applied mathematics at the Royal Institute of Technology in Stockholm, had been asked by the Société Européenne des Ingénieurs et Industriels (European Society of Engineers and Industrialists) to participate (along with Fred Singer) in discussions on climate science in Brussels Sept 1-2.
Spooked, an IPCC higher-up intervened and cancelled the discussion – obviously too much scientific dissent over a science that can no longer take it.
Johnson got the following letter from the SEII dated August 20 signaling a coming invitation:
SEII (Société Européenne des Ingénieurs et Industriels, Prof Henri Masson) organizes a conference for Fred Singer and Claes Johnson at the Fondation Universitaire in Brussels on September 1, at 18.00 h. Official invitation from SEII follows by E-mail.
The next day 2 September there will be a workshop with some of our Think Tank. Our preliminary programme looks as follows:
– 18.15 S. Fred Singer: What is new in climate change?
– 19.00 Claes Johnson: Blackbody radiation and Climate Thermodynamics
– 19.45 to 20.30: Questions and Answers”
But for Johnson, the invitation never came. Instead a letter (written in French) was sent August 22 by IPCC Vice Chair Jean-Pascal van Ypersele, who is also a member of the Belgium Royal Academy, was sent to the Fondation Universitaire.
The effect of the letter: The SEII/Fondation Universitaire seminar was cancelled.
Yes, that cancellation of open scientific debate was brought about by a “forceful intervention” by IPCC vice president Jean-Pascal van Ypersele, who wrote in his letter to the Fondation Universitaire:
[…] You should know that Fred Singer is a person who leaves very little to be desired when it comes to scientific honesty. His activities of disinformation are financed by the fossil fuel lobbies (see XXXXXXXXXXXXXX), and it is scandalous that such a person could be remotely or closely associated with the SEII and to the Fondation Universitaire.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some eiminent colleagues have also written me that M. Johnson is no better. One of his recent textbooks, where he spoke up against climate change, published by the Royal Institute of Technology (KTH, Sweden), should have been retracted because it contained errors.
I thank you in advance if you’ll rapidly inform me of the measures the SEII intends to take in order to distance itself from this ‘event’?  […]
Cordially, Prof. Jean-Pascal van Ypersele
There you have it – a Vice Chair of the IPCC reacting to scientific dissent like Superman reacts to kryptonite. They haven’t learned that science is all about taking a hypothesis and putting it on the test-stand of rigorous scrutiny. Not that this has never been done in climate science – indeed it has been done many times. Therein lies the problem! The AGW hypothesis holds up about as well has a sugar cube left out in a hurricane. The only dissent that the IPCC accepts is dissent that agrees with their science.
“…these people will go to any length to suppress scientific dissent”
Here’s S. Fred Singer’s reaction, who got news of the suppression soon after:
– Why am I not surprised by this disreputable action of this IPCC officer? After all, we know from Climategate emails that these people will go to any length to suppress scientific dissent. Even to libel and to use bald-faced lies.
– Of course, I am not supported by fossil-fuel industry. That is complete nonsense and invention.
– My Europe visit is paid by the Ettore Majorana Foundation — to give an invited talk at a climate conference in Erice. I am using the occasion to accept additional invitations to speak (without lecture fees) at the Univ of Hamburg, Imperial College, Univ of Paris – Jussieu, and of course at the KNMI in De Bilt. By happenstance I was also invited to address 100+ engineers in Zurich.
– Our IPCC colleague van Yp also questions my honesty. Well now —  the IPCC has been using me as a scientific reviewer, I publish regularly in peer-reviewed journals and am an elected Fellow of several scientific societies. So there must be some who disagree with van Yp.”
The more the IPCC suppresses and tries to shut down dissent and debate, the more suspicious other scientists will get. Hardly a good way to build trust and respect.
UPDATE 1: Also read Russell Cook’s piece here.
=====================================
PS: You can contact the SEII and asked them why they refuse to have professors Singer and Johnson.
Tel.: Fondation Universitaire: 32 (0)2 545 04 00 (ask for SEII)
Fax: 32 (0)2 502 98 31
Email: info@seii.org or seii@tiscali.be
Share this...FacebookTwitter "
"**Mink at all fur farms should be routinely screened for coronavirus, according to a leading scientist.**
A mandatory surveillance programme is urgently needed, with Denmark, which is culling all mink, acting as a warning, said Prof Marion Koopmans.
She pointed to a ""major concern"" that the virus could spread to wildlife via escaped mink.
And there were questions over whether mink played a role in the origins of Sars-CoV-2, she said.
Writing in The Lancet journal, Prof Koopmans, who has been leading investigations into cases in mink in the Netherlands, highlighted the risk of escaped mink transmitting the virus to other wildlife.
Speaking to BBC News, the head of the Erasmus MC Department of Viroscience said mandatory early warning screening for mink was already in place in The Netherlands, which should be made mandatory worldwide.
While human cases seen in mink farmers ""are not a major public health risk"", it is crucial to learn lessons from the pandemic, Prof Koopmans added.
""Animals and animal farms are an important source of food and income for many, but there are risks associated with large scale animal production and the increasing demand does require reflection,"" she said.
""This is not to point fingers to the animal sector, this is a joint responsibility for public health and citizens. There is no large-scale farming without large scale consumer demand.
""This is part of a much larger sustainability agenda. I really hope that is what we will retain from this pandemic: the need to seriously look at more sustainable production systems for the future. ""
According to the European Centre for Disease Prevention and Control in Solna, Sweden, Europe has an estimated 2,750 mink farms and produces more than 27 million pelts per year.
Denmark is culling an estimated 17 million mink, over fears the virus is mutating.
Infections have also been detected in mink in France, Spain, Sweden, Italy, the US and Greece, as well as the Netherlands, which will now ban fur farming by March 2021.
On Tuesday, the first coronavirus cases at mink farms in Poland were detected in the north of the country, according to the Medical University of Gdansk.
Last week, Ireland said it would cull mink at its three remaining mink farms, citing concerns over the mutated form of the virus detected in mink on a farm in Denmark. Tests found no cases in the mink, but culling was recommended as a precaution.
Mink appear particularly susceptible to Sars-CoV-2, which can spread rapidly in farms.
Sars-CoV-2 has the potential to infect a range of farmed and wild mammals, with opportunities for the virus to mutate, said Prof Christine Kreuder Johnson of the school of veterinary medicine at the University of California.
She told BBC News: ""A great deal (of) vigilance and monitoring of animal populations will be needed to understand genetic mutations and implications this could have for human vaccines.
""This is just another indication that we have lots of work to do to keep Covid-19 at bay for the long term.""
Follow Helen on Twitter."
"

Chairman McCrery, Ranking Member Levin and members of the subcommittee, I thank you for giving me the opportunity to express my views on the reform of our Social Security system. Eleven years ago, on October 4, 1994, I had the opportunity to speak before this same Committee and in my written testimony I offered:



As both a son and a father, I am interested that the elderly are well cared for, and that the young have the opportunity to build sufficient assets so that they, too, can retire in dignity. Social Security, as presently structured, ultimately will achieve neither objective. Although compassionate in its original intent, it is flawed in design.



The system’s financial structure is fundamentally unsound. Legislation of 1977 and 1983 attempted to address this by raising taxes and cutting benefits; Social Security was to be on sound financial footing well in to the 21st century. And now, just a few years later, The 1994 Board of Trustees’ report suggests that the system will run out of money seven years earlier than it projected just one year ago. Legislative initiatives to reduce benefits further and raise taxes are again on the drawing board. This did not work in 1977 or 1983; it will not work now. Social Security’s financial integrity requires an entirely different approach. I offer this testimony in the spirit of the starting point for an alternative: a concept of privatization wherein Americans benefit from the engine of a free economy and free choice. With privatization properly structured, today’s elderly will be protected, the young will retire with higher incomes, and our political leaders will have offered, once and for all, a lasting solution for which all voters will be thankful.



Since that testimony our nation has had a vigorous and open discussion. Many new ideas have been offered, ideas not developed prior to 1994. The climate of opinion has changed; more Americans are now aware of the issue, more Americans want the option to save and invest for their own future. We are getting closer to the point where the “rubber meets the road,” when you, as Members of Congress, will have to vote. Your decision is more important than perhaps you know. It has been eleven years since my first testimony on this issue and in many ways, but certainly not all, little has changed politically; we’re still talking about raising taxes and reducing benefits. We have wasted precious time.



 **A Collision Course**



Like other nations we face an unprecedented challenge of how to deal with a reality that mankind has never confronted before and one that most people are unaware of. How we and other governments respond will affect each American citizen, our families, businesses across the land, indeed our very way of life. The reality is not only unprecedented, it is unyielding.



Dr. Karl Otto Pohl, former president of the German central bank, the Bundesbank, stated it this way: “In a relatively short period, we must adapt our domestic institutions, international relationships, and even our individual life plans to a new, and powerful reality.”



What he was speaking of, and what confronts each of us here, is the fact that there are two powerful forces on a collision course. The first is the aging of society, the reality that the elderly population is increasing more rapidly then the population as a whole. In America, but even more so in other countries, the elderly rely on Social Security to survive financially. Should Social Security falter, many elderly will be destitute.



The second force is that most Social Security systems, including ours, are, in fact, failing. They are financially unstable, and not sustainable as they are presently structured.



The challenge is to avoid the collision of these two forces. In my view, the risks are high that we will not. But should we prevail by structuring a lasting solution, the rewards will be as unprecedented as the challenge itself.



 **The Early Years: Social Security’s Roots**



Social Security was enacted in 1935 during the Great Depression. During the first half of the 1930s real GDP fell by about 25 percent, unemployment jumped to 22 percent and the stock market virtually imploded and fell about 70 percent. Our nation was on her economic knees. President Roosevelt had to do something, something big, but large government programs were anathema to the frontier spirit of our young nation. In order to achieve his goals he needed unprecedented authority. To grasp that authority he went before the nation on March 4, 1933 in his first Inaugural Address and asked for authority “… as great as the power that would be given me if we were in fact invaded by a foreign foe.” He achieved his goal and ushered in Social Security, the flagship program of the New Deal.



Much like other Social Security programs that preceded ours, the first being Germany’s in 1889, benefits paid to the elderly were financed by payroll taxes. In our case, during the Great Depression, people who had jobs were considered the wealthy. It wasn’t like today wherein Americans have portfolios of stock and bonds, real estate, defined benefit and contribution plans and the like; you were considered wealthy if you had a job. And needs were so urgent that the “payroll wealth” was taxed. A saving and investment structure would not have worked at that time because it takes time to compound investment returns to accumulate wealth, and time was short.



 **Today: A Fundamentally Flawed Program**



Over the decades, however, this sort of urgent safety net has turned into the rough equivalent of a defined benefit plan. Yet its financial structure has not advanced. The Old‐​Age and Survivors Insurance part of Social Security, as its finances are presently structured, is inefficient, financially unsound and fundamentally flawed.



Because benefits are paid by taxing payroll, benefits can increase by no more than payroll increases, assuming that the tax rate on payroll is held constant. Over the last four decades or so, payroll has increased by about 1.5 percent per annum in real terms. That is roughly equivalent to saving and investing and receiving a rate of return of 1.5 percent. To put this into perspective, if one were to save $1,000 each year for a 45‐​year working career and earn 1.5 percent, the saving would accumulate to about $64,000. During the last 79 years a mixed portfolio of 90/10 percent large/​small company stocks earned an inflation‐​adjusted average annual return of 9.7 percent. One thousand dollars invested annually for 45 years earning that return would accumulate to about $650,000. And a conservative portfolio of 60/40 percent stocks and bonds, respectively, would accumulate to about $288,000. These different values give a glimpse of the lost opportunity that our citizens incur by being required to finance their retirement through payroll taxes.



But it is worse. For any particular age group it matters how many workers pay taxes relative to the number of retirees who receive benefits. The change in this ratio is largely determined by the change in national wealth, or GDP per capita. As national wealth rises, life spans also rise. We observe this not only here but across all parts of the globe. When Social Security was enacted life expectancy at birth was 61 years of age; it is now about 78. In the post‐​war period global life expectancy has increased from 45 to 65 years of age, a greater increase in the last 50 years or so than in the previous 5,000 years. This is all new. We didn’t expect it. But now we think it will continue.



Also, as nations become more wealthy birth rates fall. In many countries they have fallen below the population replacement rate of 2.1. The combination of rising life expectancy and falling birth rates causes havoc with pay‐​as‐​you‐​go financed Social Security systems. In the United States there were 16 workers per beneficiary in 1950; today there are about 3.3. It is expected that there will be only two in just 35 years. Therein lies an interesting paradox: as countries become more wealthy their Social Security systems become more poor. The oddity is driven by the causal relationship between increasing wealth‐​and increasing life expectancy as well as decreasing birth rates‐​all wrapped around pay‐​asyou‐ go financing.



Birth rates have fallen to such low levels in Europe-France-1.9, Germany-1.4, Italy-1.3, Spain-1.3-that “there is now no longer a single country in Europe where people are having enough children to replace themselves when they die.”



 **The Global Political Response: Raise Taxes**



The political responses to the changing demographics that squeeze Social Security’s finances are frequently the same across the world. Governments and politicians tend to see the problem in the narrowest of lights: merely a solvency issue‐​too many benefits paid, too few taxes received. This near‐​sighted analysis is further compounded by the focus on just today’s solvency and not tomorrow’s.



But from this myopic perspective the options are clear; raise taxes, cut benefits. Of the two, governments tend toward raising taxes first. This makes sense for at least a couple of reasons. There are more workers to tax than there are retirees from whom to cut benefits. Therefore, if the choice were only one or the other, raising taxes inflicts a lesser per capita burden. The second reason is that workers are younger than retirees, therefore, they have more time to adjust to a tax increase than retirees have to adjust to a benefit cut.



The short‐​sighted strategy of raising taxes has been employed world‐​wide. In the United States, for example, in 1950 when there were 16 workers per beneficiary, the maximum Social Security tax any American worker paid was $90 a year. At that time the tax rate was 3 percent on only $3,000 of wage income. As the glacial force of demographics slowly and unrelentingly squeezed the system, the $90 tax rose and squeezed the worker. Now, the tax, just for the retirement portion of Social Security, is 10.6 percent of $90,000, or $9,540. After adjusting for inflation over the last 55 years, that tax has increased almost 2,000 percent. In all likelihood, the reason that we stood for this is that the tax rose slowly; the increase was never really noticeable in any one year, but over time it has become more of a burden than the income tax for about three quarters of American workers.



Our friends in Europe, however, would consider us lucky. The payroll tax in France is about 50 percent and in Germany, Italy and Spain it ranges roughly between 38 and 42 percent. It is true that these countries’ systems provide more services than ours, but this is not a plus. Europeans are dependent on more of their needs from government programs that are not sustainable.



As many European nations have raised their payroll taxes to prohibitive levels they have choked individual economic freedom and incentive. Economic growth is stagnant, and unemployment rates hover around 10 percent, even 12 percent in Germany. Civil unrest is now more common in Germany and France as governments tell their people that benefits are no longer affordable and will have to be cut, while at the same time they extol the virtues of the welfare state. We are on the same path, but for the moment we trail far behind.



 **Then, Cut Benefits**



At some point, the strategy of raising taxes approaches a political wall. People sense that maybe, just maybe, they could achieve more with their hard‐​earned wages than they get from Social Security. Politicians sense this and move to the lesser desirable option of cutting benefits. Such blunt language, however, is not commonly uttered. Code is employed: progressive price indexing, longevity indexing, adding a third bend point, reducing bend point factors, increasing the NRA, decreasing the PIA, and it goes on and on. It’s all code for cutting benefits.



 **Fundamental Reform: Retarded by the Claim of Insurance**



Eventually, after cutting benefits hits its political wall, the thinking shifts to fundamental reform, saving and investing in wealth‐​producing assets for all workers. This idea of market‐​based financing for retirement income is not new, in fact it is old and well established in the private sector, but it is viewed with some disdain from advocates of the status quo. They object to the notion that Social Security should be an investment structure and defend their objection by claiming that it is insurance. This claim had some merit decades ago. Not now. In fact, Social Security’s finances are in trouble largely because they are inappropriately based on the insurance model.



Insurance works well when many people are subject to an event that has little chance of happening to any single individual. A good example is homeowners’ fire insurance. Many people buy fire insurance to protect their homes and yet few homes burn. Because the number of homes insured is many times the number of homes that burn, the annual insurance premium is very low relative to the replacement cost of one’s house. Insurance companies are simply the medium through which individual uncertainty of loss is transferred to, and financed by, the group.



The insurance model does not work well when the group is subject to an event that the entire group experiences. For example, if it were certain that everybody’s house would burn down, say, when the owner reached age 65, then insurance companies would have to charge annual premiums the future value of which would be the cost of rebuilding all the houses. This premium would be a large multiple of the premium charged for the uncertain case. Central to the insurance model is that the ratio of the annual premium to the dollar value of what it protects is negatively correlated to the uncertainty of individual loss.



Social Security is frequently heralded as insurance, more precisely social insurance. The ‘social’ part of the term merely means that the government plays the role of the insurance company. Other than that, it remains the insurance model. When Social Security was enacted in 1935, life expectancy was 61 but benefits weren’t payable until age 65. Now benefits are payable at age 62 and life expectancy is 78. The element of uncertainty has kind of flipped upside down. Because of this, the retirement component of Social Security isn’t insurance; once born, reaching age 62 and needing retirement income is almost certain. As a result, there is very little risk, or uncertainty, to transfer to the group,resulting in the fact that annual premiums must be enough to accumulate to a sum, including interest, that will finance retirement income.



Under these conditions, social insurance cannot provide such income at a lower cost than saving and investing for retirement. Unfortunately, however, it can and does provide it at a higher cost because it is financed through the payroll tax and is subject to unyielding demographic forces. In a perverse way Social Security’s finances and its adherence to the insurance model are caught in a kind of time warp; in the age of the iPod Social Security is a 78 RPM, wind‐​up phonograph. Unless protected by the power of the state, it can neither compete nor survive.



 **The State Monopoly Faces Competition**



Being protected by the power of the state really means that for 10.6 percent of their wage income American workers are not free to choose among alternatives for their retirement. Bad as that is, the 10.6 percent doesn’t buy much relative to reasonable and available alternatives. This is why Social Security is mandatory; few would participate if they had the freedom not to. Competition, as always, is a threat to the status quo. For workers, however, competition is their hope.



Competition would allow all workers to invest part of their payroll tax in capital markets around the world, in professionally managed portfolios that are highly diversified across asset classes, national borders and time. Such an opportunity would allow one to accumulate enough wealth to replace the pay‐​as‐​you‐​go benefit entirely.



For an average wage worker retiring this year at age 65, Social Security’s scheduled benefits are projected to replace about 42 percent of his last year’s wage. But for workers retiring in the future full benefits won’t be paid until age 67. For those future retirees, should they choose to retire at age 65, benefits will replace only 36 percent of their last wage. The worker’s cost for these scheduled benefits, which are in excess of what is affordable based on present law, is the 10.6 percent payroll tax.



 **The Market‐​Based Alternative**



The market‐​based alternative is significantly more attractive. Over the last 79 years a conservative portfolio of 60/40 percent stocks and bonds, respectively, earned a real return of just a little over 7 percent. Investing just half of the retirement payroll tax, 5.3 percent, each year for 45 years would provide a retirement benefit equal to 97 percent of one’s last year’s wage. This assumes that there is no interruption in saving each year, that the market return is as stated and falls by 2 percent during the distribution phase, and that life expectancy upon retirement is 20 years. Each of these assumptions can be changed. Work may be interrupted. Markets may do worse or better. Life expectancy may be more or less than 20 years once retired.



To take a conservative path, if the market return were only 5.5 percent and if life expectancy were 30 years at the onset of retirement‐​about 10 years more than assumed by Social Security‐​then under these conditions the replacement rate would 39 percent, higher than Social Security’s scheduled benefits at age 65 and significantly higher than payable benefits.



 **Americans Understand the Tradeoffs**



Our citizens sense these tradeoffs, risks, uncertainties, and the fundamental differences in providing retirement income from a tax system versus a saving and investment system. This is why, but only part of why, they want the option, the freedom to choose.



If they could acquire this freedom they also would have personal property rights over their accumulated wealth. They have no such rights to Social Security benefits. They also could bequeath some or all of their retirement assets. They cannot under Social Security. They would benefit from the direct relationship between effort, their saving, and reward, their accumulating wealth. They would have the dignity that comes with being personally responsible for their future. They would no longer be tethered to the government. They would no longer be subject to politicians’ preferences over when they can retire, how much they can get, how their spouses are treated, how much they’re going to pay, and all of the rules and regulations that have evolved to the point of being incomprehensible. They would be free.



It’s been eleven years since I had the opportunity to speak before this Committee. Although much of what I am saying today is what I said then, I hope that we are closer to fundamental reform. If we are not, and the two powerful forces that I mentioned above in fact collide, we will edge closer to the wrenching difficulties that Europe is now facing.



 **You, Congress, are the Hope**



But should we grasp the extraordinary opportunity that this challenge offers, we will forever strengthen our nation, our economy, our freedoms, and our ability to finance the many needs that the future will undoubtedly require. It is a matter of will and political leadership in seeing the benefits of greater personal freedom and acting to ensure them. You, as Members of Congress, have the unique opportunity to offer, once and for all, a lasting solution for which all Americans will be forever thankful.



Thank you,



William G. Shipman
"
"
Share this...FacebookTwitterUPDATE 1: Roger Pielke Jr:
Here is another good example why I have come to view parts of the climate
science research enterprise with a considerable degree of distrust.”
===============================================
With the IPCC 5th report due to come out in the near future, the order books at hockey stick factories are full.
The Potsdam Institute for Climate Impact Research (PIK) offers a complete line…whether you’re looking for sea-ice sticks, temperature sticks, storm frequency sticks, tornado sticks, drought sticks, flood sticks, glacier sticks….you name it, they can craft it.
And all now come with even sharper than ever blade angles. Here’s the latest junk science from the PNAS:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Increase of extreme events in a warming world
Stefan Rahmstorf and Dim Coumou, Potsdam Institute for Climate Impact Research, PO Box 601203, 14412 Potsdam, Germany
Edited by William C. Clark, Harvard University, Cambridge, MA, and approved September 27, 2011 (received for review February 2, 2011)
Abstract
We develop a theoretical approach to quantify the effect of long-term trends on the expected number of extremes in generic time series, using analytical solutions and Monte Carlo simulations. We apply our method to study the effect of warming trends on heat records. We find that the number of record-breaking events increases approximately in proportion to the ratio of warming trend to short-term standard deviation. Short-term variability thus decreases the number of heat extremes, whereas a climatic warming increases it. For extremes exceeding a predefined threshold, the dependence on the warming trend is highly nonlinear. We further find that the sum of warm plus cold extremes increases with any climate change, whether warming or cooling…continue abstract here.
By the way, the authors conclude that man is responsible for the heat (and not nature or the sun).
Share this...FacebookTwitter "
"

Here’s a new list of bloggers who are citing, discussing and writing about Cato commentary and analysis: 



If you’re blogging about Cato, let us know on Twitter (@catoinstitute) or email cmoody@​cato.​org.
"
"
Share this...FacebookTwitter30 years ago Waldsterben (forest dieback) was probably Germany’s first post-war environmental hysteria to grip the country. Today we see that all the prophecies of doom were completely wrong.
The excellent Michael Miersch brings our attention to this oustanding arte Franco-German documentary called “The Forests are Dying Again“ (in German, and here in French), which takes a look back at one of the greatest environmental hysterias ever to grip a population: Waldsterben (forest dieback), a.k.a. acid rain.
The documentary also exposes the dirty tricks the media used to keep the hysteria alive (see 24-min. mark). There are so many parallels to today’s modern climate hysteria.
Again, back then there was “consensus”, all the scientists agreed, there was no denying the catastrophe, and politicians called it a grave threat that required immediate action. Fear gripped Germany. Environmentalists, union leaders, church leaders, citizens, politicians, etc. marched on the streets and demanded the government take action. The culprit was clear: emissions from industry and man were producing acid-rain that was chemically searing forests. At the 1:29 mark of the documentary:
The early 1980s, thousands of people took to the streets, an entire country is in panic, the German forest is dying. That’s for sure. But we alone are at fault due to our unbridled efforts to attain prosperity and progress. We treated nature like crap, and now there is nothing left to do but take it to the grave.”
Der Spiegel triggers the hysteria
The scare was first set into motion by Der Spiegel’s November, 1981 front page story called: “The Forests are Dying. Acid Rain Over Germany“. Soon all other media outlets fell over themselves to see who could produce the most sensational stories.
Der Spiegel wrote that the forest had only 5 years left. Stern, not to be outdone,followed with: “Acid Death” and claimed that the forest had only 3 years left.  Waldsterben remained the hottest story for years in the German press. The scare even served as one of the major springboards that launched Germany’s Green Party.
At the 3:08 mark, the documentary cuts back to present-day 2011 Allgäu, 30 years later, where we see the forests look completely healthy. “How can that be?” the documentary asks.
Rudy Holzbergercollected 150 media clippings about the tree-dieback hysteria, and has gone back and analyzed them. While some media outlets like Stern claimed the forest would die in as little as 3 years, all agreed on one thing, Holzberger says:
All of them said the forest would be dead at the latest by the year 2000.”
Holzberger then goes on to explain that the science behind the scare was flaky and thin. Sound familiar? Most of the forest dieback junk-science is traced back to University of Göttingen professor Bernhard Ulrich, who says at the 6:36 mark:
“There’s no doubt for those who are involved in the science the cause is air pollution, acid rain, and everything that comes with it.
We have to expect that after a warm and dry year it will lead to widespread forest damage and death.”
According to Professor Ulrich, German forests would soon appear as dead as those shown at the 7.50 mark of the documentary.
Later in the documentary, tree rings reveal that an even more widespread tree die-off occurred in 1947, and that the tree die-off in the early 1980s was nothing unusual and part of the natural cycle. The 1980s episode, however, showed how the media for the first time could drive an entire nation into mass panic.
Happening faster than anyone expected
The panic eventually spread into France (but to a lesser degree) thanks to assertions made by Professor Josef Reichelt, who claimed that French trees were dying off as well. But the French press ignored the story as a whole. Yet, there were still some kooks like Richard Kletty who claimed:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“It’s happening unbelievably fast. We know the resistance that trees have, and so it really surprised us how fast the damage is taking place and the trees are dying.”
Joschka Fischer
Today we hear the same about sea ice melt. Yet, the scare never took off in France as it did in Germany, where the topic was emotionalized rather than being based on science and reason. At the 14:20 mark the documentary tells us how the German Greens made the jump into the German Parliament, with a young Joschka Fischer (looks like him, anyway) marching in carrying a dead tree.
The all-knowing, bearded Greens protested the inauguration of Helmut Kohl, claiming he was dealing with the problem irresponsibly. The political payoff for the greens was handsome. As the forests appeared to be dying, Germany embarked on the path of turning “green”.
The forests then recovered, but the media ignored it
At the 23-minute mark, the documentary tells us that eventually by 1993 the trees, which go through natural cycles of losing needles and greening again, depending on rainfall, were back in a state of ruddy health and that there was no longer any danger of the once feared massive forest die-off. How did the media react to this news? At the 23:55 mark, Helmut Schulz says:
We made an analysis of the press to see how they reported on this. Of 54 daily newspapers, only 4 reported on the positive news. All the others, 50 newspapers, reported negatively.”
The media had no interest in an improving forest health – they wanted to remain stuck on Armageddon. Instead they rolled out more apocalyptic headlines. At the 23:42 mark, Holzberger shows some of the headlines: Stern in 1994: The Death Struggle of the Trees which included words like: “If Trees Could Scream, which described the death of trees in human terms.“It was complete nonsense”, says Holzberger.
Tree-dieback deniers got smeared
It was also a difficult period for scientists who did not share the apocalyptic views of mass forest die-off. In 1996 Professor Heinrich Spieker published a scientific assessment of European forests commissioned by a Finnish forestry institute. The report was called: Growth Trends of European Forests, which reached the conclusion: “The forest in Europe is growing faster and they are healthier”. This is not what the media wanted to hear. It contradicted prevailing dogma. The reaction from the media was harsh.
Here were some of the claims made by the media (see 26.46 mark), the Süddeutsche Zeintung:
EFI study is superficial and fundamentally flawed.”
and called the deniers:
Witch doctors and charlatans”
and one German activist group wrote:
Half of the financing came from the Finnish government, and that Spieker was married to a Finnish woman.”
Today, it’s clear that Heinrich Spieker was right, and that it is the slimy media who have egg on their faces. Indeed German forests are expanding 170 sq km annually. And again today in climate science, the very same newspapers and groups are at it again.
Ironically, today’s forest die-off is due to the green biofuels craze
At the 27-minute mark, the documentary focuses on today’s claims that climate change is threatening yet another forest die off. But as the documentary shows, forests are adapting as they always have, and that the Sahara is getting greener. The science shows that the warming temps over the last 30 years (likely due to ocean cycles) is making the planet greener, and not browner.
In the last segment of the documentary we see the real threat to forests – especially tropical forests. It is deforestation to make way for green bio-crop plantations – to grow crops that allegedly will save forests from climate change.
Share this...FacebookTwitter "
"**A man who asked police officers about coronavirus and what would happen if coughed on them, then sneezed and spluttered in the back of their car.**
Police said Thomas Coates, 21, had deliberately sneezed at the officers after he was arrested on 13 June.
Coates, from Wellesbourne, Warwickshire, has been jailed for five months after admitting two counts of assaulting an emergency worker.
Police said he put officers and their families' health at risk.
There was ""no indication the defendant had Covid"", Warwickshire Police said.
The incident happened after Coates, of Hotchkiss Close, was arrested over a separate offence. He asked the officers their view on coronavirus and what would happen if he coughed on them.
He then, in the car, ""deliberately sneezed and coughed"", Warwickshire Police said.
Coates had not been displaying symptoms and neither was anyone at his home, so in line with government guidance, officers were advised to look out for symptoms in themselves and self-isolate if they materialised, the force said.
At Warwick Crown Court Coates was given five months each for the two counts to run concurrently.
Head of local policing Ch Supt Ben Smith said: ""At a time when police officers are at the front line of keeping the public safe during the pandemic it is shocking that people think it acceptable to act in this way.
""Coates's actions not only put the officers' health at risk but the health of their colleagues and families.""
_Follow BBC West Midlands on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to:_newsonline.westmidlands@bbc.co.uk"
"
From the University of California, San Diego Press Release
The previously unknown eruption in 1809 was larger than the Mt. Pinatubo eruption. Credit: USGS
A team of chemists from the U.S. and France has found compelling evidence of a previously undocumented large volcanic eruption that occurred exactly 200 years ago, in 1809.
The discovery,  published online this week in the scientific journal Geophysical Research Letters, offers an explanation as to why the decade from 1810 to 1819 is regarded by scientists as the coldest on record for the past 500 years.
“We’ve never seen any evidence of this eruption in Greenland that corresponds to a simultaneous explosion recorded in Antarctica before in the glacial record,” said Mark Thiemens, Dean of the Division of Physical Sciences at UC San Diego and one of the co-authors of the study. “But if you look at the size of the signal we found in the ice cores, it had to be huge. It was bigger than the 1991 eruption of Mount Pinatubo in the Philippines, which killed hundreds of people and affected climate around the world.”






Jihong Cole-Dai of South Dakota State U. headed the research team. Credit: South Dakota State U.



Led by a chemist from South Dakota State University, the team of scientists made its discovery after analyzing chemicals in ice samples from Antarctica and Greenland in the Arctic, where the scientists visited and drilled ice cores three years ago. The year-by-year accumulation of snow in the polar ice sheets records what is going on in the atmosphere.
“We found large amount of volcanic sulfuric acid in the snow layers of 1809 and 1810 in both Greenland and Antarctica,” said professor Jihong Cole-Dai of SDSU’s Department of Chemistry and Biochemistry, who was the lead author of the paper.
Joël Savarino of the Laboratoire de Glaciologie et Géophysique de l’Environment in Grenoble, France, and a former postdoctoral fellow at UC San Diego, was also part of the team.
Cole-Dai said climate records show that not only were 1816 — the so-called “year without a summer”— and the following years very cold, the entire decade from 1810 to 1819 was probably the coldest for at least the past 500 years.






The team drilled ice cores in Greenland’s ice sheet. Credit: Mark Thiemens, UCSD



Scientists have long been aware that the massive and violent eruption in 1815 of an Indonesian volcano called Tambora, which killed more than 88,000 people in Indonesia, had caused the worldwide cold weather in 1816 and after. Volcanic eruptions have a cooling effect on the planet because they release sulfur gases into the atmosphere that form sulfuric acid aerosols that block sunlight. But the cold temperatures in the early part of the decade, before that eruption, suggest Tambora alone could not have caused the climatic changes of the decade.
“Our new evidence is that the volcanic sulfuric acid came down at the opposite poles at precisely the same time, and this means that the sulfate is from a single  large eruption of a volcano in 1809,” Cole-Dai said. “The Tambora eruption and the undocumented 1809 eruption are together responsible for the unusually cold decade.”
Cole-Dai said the Tambora eruption was immense, sending about 100 million tons of sulfur gas into the atmosphere, but the ice core samples suggests the 1809 eruption was also very large — perhaps half the size of Tambora — and would also have cooled the earth for a few years. The researchers reason that, because the sulfuric acid is found in the ice from both polar regions, the eruption probably occurred in the tropics, as Tambora did, where wind patterns could carry volcanic material to the entire world, including both poles.






UCSD’s Mark Thiemens (upper left) pulls a cylinder in Greenland containing an ice core. Credit: UCSD



Cole-Dai said the research specifically looked for and found a special indicator of sulfuric acid produced from the volcanic sulfur gas in the stratosphere.
The special indicator is an unusual make-up of sulfur isotopes in the volcanic sulfuric acid. Isotopes are different types of atoms of the same chemical element, each having a different number of neutrons (but the same number of protons). The unique sulfur isotope composition is like a fingerprint of volcanic material that has reached the stratosphere, said Cole-Dai.
The stratosphere is the second major layer of the Earth’s atmosphere, reaching from about six miles to about 30 miles above the Earth’s surface at moderate latitudes. To impact global climate, rather than local weather, the sulfur gas of a volcanic eruption has to reach up into the stratosphere and once there, be spread around the globe.
“The way in which that these volcanoes affected the average temperatures of our planet gives us a better idea of how particulates in the atmosphere can affect our climate,” said Thiemens. “People talk about the possibility of geo-engineering our climate, but the question is how? In this case, nature has done an experiment for us.”
Other members of the research team were South Dakota State post-doctoral researcher David Ferris and graduate student Alyson Lanciki; and Mélanie Baroni of CEREGE (Le Centre Européen de Recherche et d’Enseignement des Géosciences de l’Environnement) at L’Université Paul Cézanne in Aix-en-Provence, France.
The researchers were funded by the National Science Foundation, French Polar Institute (IPEV) and the Institut National des Sciences del’Univers (INSU).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91f10ef8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterScott Portman
No, I’m not becoming a warmist or a tree-hugger. But I am pleased to give environmentally concerned citizens the microphone here. 
The following is from Scott Portman of Atlanta, who politely asked to have his essay published here. It’s about the EPA and regulating mercury. I’m in favour of reducing mercury emissions, as long as the benefits outweigh the costs. I think Scott here will very much appreciate your comments.
========================================
The Debate On Mercury Emission Standards
by Scott Portmann
The United States’ Environmental Protection Agency has recently proposed the first-ever national standards for mercury and other air pollutants. The agency’s proposed standards are meant to regulate coal fired power plants in the US. They currently believe that with the new regulations, public health will be dramatically improved on a global level; in the US alone, they project that 17,000 premature deaths from lung diseases, such as mesothelioma, will be prevented. The standards, in addition, will prevent a whopping 11,000 heart attacks and 120,000 cases of childhood asthma.
The administrator of the US EPA, Lisa Jackson, confirmed her belief on the subject, claiming in a statement:
With the help of existing technologies, we will be able to take reasonable steps that will provide dramatic protections to our children and loved ones, preventing premature deaths, heart attacks, and asthma attacks”
It is possible that these new standards might be a result of the recent pressure the EPA has come under, due to a lot of pushback from the Republican Party. They hold the belief that the EPA is hurting the global economy with their rigid regulations. In an attempt to try to reign in the EPA, Republican lawmakers have targeted the agency’s climate rules. In their attempt, these new mercury standards have come under fire, too.
Currently, the technology exists to make this environmental goal a reality. Just by installing the regulating systems, power plants could effectively lower a slew of harmful emissions. US President Barack Obama has even issued an executive order that mandates the EPA to make sure their regulations are cost effective and not overly burdensome to industry. In response to that, the EPA has claimed that their standards are so cost-effective that for every $1 spent, the public will see $13 in benefits.
As to be expected, there are opponents to the EPA’s mercury regulations. Some believe that the standards would impose major economic burdens to manufacturing companies, costing many people their jobs. In the current economic climate, they raise a legitimate concern. They believe that the expenses will be passed on to consumers, who will face higher electricity bills.
On the flipside, if the standards pass the public will most definitely see increased health benefits. Thousands will live longer, and even more will breathe easier. Perhaps most importantly, the environment will be safeguarded and it will be a step towards preventing climate change. Toxic mercury will be reduced from bodies of water, and as a result, fish will be safer to eat. With fewer illnesses, there will be fewer expenses due to hospital and doctor visits as well. The money saved from collateral costs will most likely outweigh any additional electricity costs. It just seems that the positive aspects of the mercury standards far outweigh the negatives.
==================================================
A health, safety, and political advocate with a passion for economics, Scott Portman is an aspiring journalist who currently resides in the South East United States
Share this...FacebookTwitter "
"Picture the scene: a dinner for MEPs organised by leading fossil fuel firms to explain the lengths to which their industries have gone to combat climate emergency. On the guest list, the environment minister of Croatia, current holders of the EU’s rotating presidency, and Guido Bortoni, an adviser in the European commission’s energy directorate. Nobody at all from civil society or the NGO sector. In other words a perfect Brussels lobbying event. Billed as “Oil and Gas and the Green Deal” this dinner took place on 17 February, just a fortnight before the unveiling of the EU’s first ever climate law. The meal was sponsored by the International Association of Oil and Gas Producers (IOGP), which represents 29 of Europe’s main fossil fuel operators, including Total, Shell, BP and ExxonMobil. According to the EU transparency register the IOGP spent €350,160 in 2018 lobbying in Brussels, so it can afford to take a few MEPs out to dinner. The European Energy Forum (EEF) is the perfect organiser: it is headed by Jerzy Buzek, an MEP for the European People’s party (EPP), a former prime minister of Poland, a former president of the European parliament and currently chair of its industry research and energy committee. The forum boasts 82 associate members, firms that pay “at least” €7,000 a year in membership fees. Predictably they are all in the oil and gas sector. The EEF politely turned down a media request to attend an earlier gathering on 26 November entitled “Gas: Driving the Energy Transition”. “We do not accept journalists at our events,” was the response. The event was sponsored by Eurogas, the gas industry’s main lobbying organisation, and two German firms, Uniper and Wintershall. An official from the European commissions DG Energy also attended. The European Green Deal, the commission’s €1tn plan to prepare the European economy for confronting the climate crisis, has prompted a round of intensive lobbying activity. “Since 2010, the five main oil and gas corporations and their fossil fuel lobby groups have spent at least a quarter of a billion euros buying influence at the heart of European decision-making,” claims a report produced by a coalition of climate NGOs (Corporate Europe Observatory, Food & Water Europe, Friends of the Earth Europe and Greenpeace). Such firms, their business founded on extracting fossil fuels, have an interest in delaying or undermining policies designed to mitigate the effects of the climate crisis. They make much use of Brussels-based thinktanks such as the Centre for European Policy Studies (CEPS). A week before the newly sworn-in commission presented its climate policies, CEPS hosted on 4 December, a “by invitation-only” event on the Green Deal funded by Equinor and Fortum, the energy market leaders in Norway and Finland, respectively.  Firms are welcome to join CEPS as corporate members. ExxonMobil, for example, pays €15,000 a year for this access. The thinktank has organised members-only breakfast meetings with guests such as Frans Timmermans, now EU commissioner in charge of the Green Deal.  Members of commission president Ursula von der Leyen’s team have been the focus of relentless attention from energy industry and particularly gas lobbyists for months. In the last 10 days of January alone, Eurogas representatives met six commissioners and officials from departments for the environment, trade, budget, agriculture and economic affairs. Each time the declared business of the meeting was the Green Deal. The energy commissioner, Kadri Simson, will be the keynote speaker at this year’s Eurogas conference on 19 March in Brussels. Six members of the association’s staff are accredited for access to EU parliament buildings. Eurogas spent €800,000 on lobbying in 2018 alone. “DG Energy is very permeable to the gas lobby,” says Pascal Canfin, a Renew Europe MEP and chair of the parliament’s environment committee. The IOGP has been busy too. On 17 December it met Ditte Juul Jorgensen, the head of DG Energy, then in January two other European commission members, in different directorates. The topic was of course the Green Deal. Similarly employers’ group Business Europe has shown great interest in the commission’s new plans for the environment. Speakers at the federation’s annual gathering on 4 March – the day the EU’s new climate law is being unveiled – include Von der Leyen in person, two of her vice-presidents and three commissioners. Another means of influencing European policymakers is to use lobbying firms. FTI Consulting is one of the most effective. Transparency International ranked it second for setting up meetings with the EU executive in 2018. With 38 staff members accredited for access to the European parliament FTI billed Eurogas, then a new customer, between €25,000 and €50,000 in 2018. Earnings from ExxonMobil, its second largest account totalled €600,000 to €700,000. The same year it carried out confidential research for Gas Infrastructure Europe. Measures for a Sustainable Gas Storage Market, the resulting study, was subsequently posted and is still on the European commission’s website. The firm has made good use of the many revolving doors available in Brussels. In 2016-18, for instance, Philip Lowe, after heading DG Energy (2010-14), was a senior adviser to FTI on public affairs. Jean-Arnold Vinois is another example of the revolving doors in operation. He is an energy policy adviser at the Jacques Delors Institute, a well known thinktank, but also an honorary director for energy at the commission and a consultant at FleishmanHillard, another big lobbying outfit in Brussels. This firm counts the European Chemical Industry Council (Cefic) – earning it almost €1m in 2018 – the gas lobby Gas Naturally (€25,000 to €50,000) and Fuels Europe, another oil and gas federation, among its customers. Carbon capture and storage (CCS) is the gas industry’s latest hobby-horse, seen as a means of justifying its fossil fuel activities. The snag is that CCS technology is far from being ready for widespread use and even if research succeeds in perfecting the techniques, a drastic reduction in emissions will still be needed over the next 20 years. The gas industry nevertheless persists in promoting it as a solution. Eurogas is working hand-in-hand with the Global CCS Institute, the key lobby in this field. On 18 February, in partnership with the United Kingdom Mission to the EU, it held a conference on the topic in Brussels. It is particularly difficult to pin down the role of EU governments in advancing the interests of their firms. There is no equivalent to the EU transparency register to keep track of the national governments or their missions. Industry is by no means alone in attempting to influence EU lawmakers on climate issues. Powerful environmental thinktanks and NGOs – such as the World Wildlife Fund (WWF), Transport & Environment, the Climate Action Network or the European Climate Foundation (ECF) – are hard at work too. Their financial means are substantial but less than their opponents. In 2018 the ECF reported expenditure of between €500,000 and €600,000, while WWF spent €2.2m to €2.4m over the same period. Aude Massiot writes for Libération on environment issues. This article is part of a This is Europe collaboration with Libération and Tageszeitung."
nan
"Plans for a third runway at Heathrow airport have been ruled illegal by the court of appeal because ministers did not adequately take into account the government’s commitments to tackle the climate crisis. The ruling is a major blow to the project at a time when public concern about the climate emergency is rising fast and the government has set a target in law of net zero emissions by 2050. The prime minister, Boris Johnson, could use the ruling to abandon the project, or the government could draw up a new policy document to approve the runway. The government is considering its next steps but will not appeal against the verdict. The transport secretary, Grant Shapps, said: “Our manifesto makes clear any Heathrow expansion will be industry-led. Airport expansion is core to boosting global connectivity and levelling up across the UK. We also take seriously our commitment to the environment.” Johnson has opposed the runway, saying in 2015 that he would “lie down in front of those bulldozers and stop the construction”. Heathrow is already one the busiest airports in the world, with 80 million passengers a year. The £14bn third runway could be built by 2028 and would bring 700 more planes per day and a big rise in carbon emissions. Johnson is thought to have been looking for a pretext to withdraw support for the extra runway and could make the argument for Birmingham to provide increased airport capacity for London given that train journey times will be reduced by HS2. The court’s ruling is the first major ruling in the world to be based on the Paris climate agreement and may have an impact both in the UK and around the globe by inspiring challenges against other high-carbon projects. Lord Justice Lindblom said: “The Paris agreement ought to have been taken into account by the secretary of state. The national planning statement was not produced as the law requires.”  What just happened? For the first time judges have said that plans for a major infrastructure project are illegal because they breach the UK's commitments to reduce greenhouse gas emissions to tackle the climate crisis. This is a groundbreaking legal decision that could effect future infrastructure developments and puts the UK’s commitment to cut emission to net zero by 2050 at the forefront of future policymaking. What will happen next? The government has been told by the court of appeal to declare its decision to allow Heathrow airport expansion - contained in its airline national policy statement - illegal. Ministers have two choices now. They can withdraw the whole policy statement or try to amend it to make it compatible with the UK’s commitments to reduce greenhouse gas emissions to net zero by 2050.  Will the runway be built? If the government can prove that expanding Heathrow is compatible with its commitments under the Paris agreement to very radically reduce greenhouse gas emissions, the runway may go ahead. But the prime minister has always been against the third runway, and the government has told the court it will not be appealing against its decision on Thursday.  There now hangs a very big question mark over whether the bulldozers will ever start work on the runway. “It’s now clear that our governments can’t keep claiming commitment to the Paris agreement, while simultaneously taking actions that blatantly contradict it” said Tim Crosland, at legal charity Plan B, which brought the challenge. “The bell is tolling on the carbon economy loud and clear.” Plan B’s intervention was one of a number of legal challenges against the government’s national policy statement, which gave the go-ahead for the new runway in 2018 after MPs backed it by a large majority. Others were brought by local residents, councils, the mayor of London, and environmental groups including Friends of the Earth and Greenpeace. The challenges were dismissed in the high court in May 2019 but the complainants took their cases to the court of appeal, which delivered its verdicts on Thursday. Plan B argued that the Paris agreement target, which the government had ratified, was an essential part of government climate policy and that ministers had failed to assess how a third runway could be consistent with the Paris target of keeping global temperature rise as close to 1.5C as possible. “This is an opportunity for Boris Johnson to put Heathrow expansion to bed and focus on the most important diplomatic event of his premiership, the UN climate summit in Glasgow in November,” said Lord Randall, a former Conservative MP and climate adviser to the former prime minister Theresa May. “It’s his chance to shine on the world stage.”  The court of appeal did not overturn the high court’s dismissal of the other challenges, which related to air and noise pollution, traffic, and the multibillion pound cost of the runway. But the Paris agreement ruling is far-reaching, according to Margaretha Wewerinke-Singh, an international public law expert at Leiden University, in the Netherlands. “Its implications are global,” she said. “For the first time, a court has confirmed that the Paris agreement temperature goal has binding effect. This goal was based on overwhelming evidence about the catastrophic risk of exceeding 1.5C of warming. Yet some have argued that the goal is aspirational only, leaving governments free to ignore it in practice.” Prof Corinne Le Quéré, at the University of East Anglia, said: “Government needs to put climate targets at the heart all big decisions, or risk missing their own net zero objectives with devastating consequences for climate and stability. I am relieved this is finally recognised in law.” Climate campaigner Greta Thunberg said: “Imagine when we all start taking the Paris agreement into account.” Heathrow and proponents of the third runway say it would provide an economic boost and is important for international business, particularly after Brexit. “The court of appeal dismissed all appeals against the government – including on ‘noise’ and ‘air quality’ – apart from one, [i.e. climate change] which is eminently fixable,” said a spokeswoman for Heathrow. “We will appeal [as an interested party] to the supreme court on this one issue and are confident that we will be successful. Expanding Heathrow, Britain’s biggest port and only hub, is essential to achieving the prime minister’s vision of global Britain. We will get it done the right way.” Mike Cherry, at the Federation of Small Businesses, said: “The verdict is a blow to small firms who need greater regional and global connectivity, as well as more opportunities to export.” However, most flights are taken for pleasure and just 20% of the UK population take more than two-thirds of international flights. Critics say the economic benefits are illusory given, for example, the estimated £10bn of taxpayers’ money needed to alter road and rail links to the airport, and would draw investment towards the south-east. “No amount of spin from Heathrow’s PR machine can obscure the carbon logic of a new runway,” said John Sauven, at Greenpeace UK. “Their plans would pollute as much as a small country.” Geraldine Nicholson, from local campaign group Stop Heathrow Expansion, said: “This is the final nail in the coffin for Heathrow expansion. We now need to make sure the threat of a third runway does not come back.” At a separate event on Thursday, Alok Sharma, the business secretary and president of November’s UN COP26 climate summit, said: “The only economy which can avoid the worst effects of climate change, and thus continue to deliver growth, is a decarbonised economy. Our choices will make or break the zero-carbon economy.” • This article was amended on February 28 2020. An earlier version had mistakenly called the business secretary Ashok Sharma, rather than Alok Sharma. This has been corrected. "
"

California’s newly released regulatory initiative to reduce greenhouse gas emissions from new cars sold in that state represents the triumph of symbolism over substance. It’s an ill‐​considered gesture that ought to annoy partisans on both sides of the global warming fence.



How much will these new emission rules help in the fight against global warming? “Not much” would be a charitable answer.



Back‐​of‐​the envelope calculations derived from computer simulations performed by climatologist Tom Wigley (who, by the way, supports aggressive action to address the threat of global warming) suggest that even if every state in the union adopted California’s new program, global temperatures would drop by something less (actually, probably far less) than one‐​tenth of 1 degree Fahrenheit by 2050. What everyone in the scientific community understands but few want to discuss publicly is that stopping global warming — or even slowing it down appreciably — requires the near total abandonment of fossil fuels.



Suggesting otherwise only encourages the public to believe that they can meaningfully address climate change without sacrifice.



Because I’m against economic sacrifice, that’s fine with me. While the planet is indeed warming — probably due in no small part to industrial greenhouse gas emissions — the warming has been modest, benign, and largely confined to northern latitudes during winter nights. There are good reasons to expect that warming pattern to continue. And that warming pattern does not threaten to usher in the convulsive climatic events we are warned about in the press or in the movie theaters. In fact, some scientists and economists can make a pretty good case that global warming will prove a net plus to both the economy and the global environment.



So perhaps I should applaud California’s regulations. Unfortunately, this empty gesture isn’t completely cost‐​free. 



The California Air Resources Board estimates that their plan will add about $1,000 to the cost of a new car by 2015. Question: How many people would be willing to pay a $1,000 tax each time they buy a new car to reduce global temperatures 46 years from now by a number too small to measure? How many economists could you find who’d accept that such a tax passes any sane cost‐​benefit test? Not many, I’ll bet.



When pressed on this, environmentalists counter that anything that begins the long regulatory journey they envision before us is valuable in and of itself. But if the first baby steps necessary to initiate this journey are so expensive and so obviously incapable of passing any reasonable cost‐​benefit test, how expensive do you think future steps might be that environmentalists dare not forward at the moment?



Then there are the hidden costs. Because there are no technologies available to remove greenhouse gas emissions from automobile tailpipes, the only way to comply with the California mandate is to improve auto fuel efficiency. Yet improving fuel efficiency reduces the marginal cost of driving one’s car, and economists have demonstrated that people respond to those lower driving costs by actually driving more. 



What does more driving mean? More pollution, that’s what. Economist Andrew Kleit of Pennsylvania State University calculates that a 50 percent increase in the fuel efficiency of the automobile fleet — essentially what California is requiring through these regulations — will increase net automobile emissions of volatile organic compounds by 1.9 percent, nitrogen oxides by 3.4 percent, and carbon monoxide by 4.6 percent. In other words, environmentalists are asking us to trade off an infinitesimal reduction in global temperature for more smog than we might experience otherwise.



This wouldn’t be the case if environmentalists were more honest with the public about the costs associated with their policy prescriptions. The fact is that the only way to reduce greenhouse gas emissions is to increase the cost of fossil fuels via an implicit or explicit tax. But imposing a fuel tax makes the cost of these policies transparent and, environmentalists fear, difficult to defend politically.



They’re right.
"
"

Many long‐​held dreams were finally realized on Saturday. The enlargement of the European Union saw eight countries from Central and Eastern Europe, along with Malta and Cyprus, return home after a long and eventful journey, reuniting the continent after 50 years of division. The enlargement has been hailed by some as the “end of history.” But while this is a historic occasion that offers a good opportunity to remember heroes of the struggle for freedom on the both sides of the Iron Curtain, the history of Europe is certainly not ending. In fact it is just beginning.



The effects of EU enlargement will go deeper than most people are predicting, changing Europe beyond recognition. From the core bureaucratic structure of the union itself, to fiscal policy and ideological identity, it will not be possible for the union to continue as before. The current structure will be unmanageable with 25 member states and will have to undergo a radical overhaul. The Common Agricultural Policy will have to be reformed, as there simply isn’t enough money to extend it to new member states in its present form.



Meanwhile, the enlarged EU will have to reconcile the low‐​tax, low‐​regulation economies of the incoming members with the high‐​tax, high‐​regulation policies in much of the existing union. This will be the great tension within the union over the next decade.



Nearly all the new member states — even those more inclined to the left — have introduced far‐​reaching and necessary economic reforms. They have undergone the most radical fiscal reforms and abolished most of their subsidies. They have privatized their economies and opened them to foreign investment. Social‐​security networks and pension systems have been overhauled. The role of agriculture in GDP has been dramatically decreased. The business climate in new member states is open, the labor market is not overregulated, the tax burden is low.



But if “old Europe” is to compete effectively with “new Europe,” it will have to lower taxes and rethink the social‐​welfare systems that high taxation supports. Ten years ago Estonia became the first country in Europe to introduce flat rate proportional personal income tax, a policy designed to energize our people and stimulate growth. It was a huge success. Latvia and Lithuania followed, then Russia, Ukraine and now Slovakia. We can only sit back and wait for the next dominoes to fall. It looks quite possible that within five years the whole of Central and Eastern Europe will move to flat‐​rate income taxes.



Such developments put pressure on old Europe, pushing down taxes in countries neighboring new member states, and so creating more room for investment and development. But this has also made old Europe nervous. Enthusiasts of social welfare see enlargement as a serious threat to European civilization, akin to the barbarian invasions at the end of the Roman Empire.



The welfare state is considered a core part of European identity, despite its negative impact on European competitiveness, and its long‐​term unsustainability. Swedish Prime Minister Goran Persson and German Chancellor Gerhard Schroeder have complained that the rich are not taxed heavily enough in new member states and are seeking to extend the EU’s power into areas of taxation and take away the national veto in a number of areas of regulation. Both of these would have a negative impact on the open economies of new member states.



The danger is that such moves will crystallize the problems inherent in the economies of current members and export them to EU level, interfering negatively with those economies that have increased competitiveness through radical reforms — reforms that have been suggested by every EU panel and expert group over the past decade, but which have been consistently rejected by European leaders for domestic political reasons.



Enlargement should be the catalyst that at last forces their hand. Europe’s economic malaise must be confronted if it is to compete with its global rivals. The Continent needs a clear vision and a new agenda for the 21st century. The enlargement should provide the impetus to work out this agenda and regain the momentum for reform. New member states are poor today and still bear the burden of their communist heritage, but if they stiffen their resolve and maintain their liberal approach and open economies they may succeed not only in improving their own countries and economies, but also in injecting all Europe with a new dynamism and momentum for reform.



It is essential that new members, along with Britain, resist attempts to introduce tax harmonization and increase the burden of regulation in the enlarged EU, sucking vibrant new members into the old stagnant Europe — a Europe that is slowly but steadily losing its importance in the world, and consistently lagging behind its global competitors economically. This is the moment when Europe can move in a new direction. If everybody dreaming of a new Europe works together, then they can do miracles. They can make all Europe new again.
"
"

_Politico_ asks, “Was he convincing?”   
  
  
My response:   
  
  
In Copenhagen this morning, President Obama convinced only those who want to believe — of which, regrettably, there is no shortage. Notice how he began, utterly without doubt: “You would not be here unless you, like me, were convinced that this danger is real. This is not fiction, this is science.” The implicit certitude is no part of real science, of course. But then the president, like the environmental zealots cheering him in Copenhagen, is not really interested in real science. Theirs, ultimately, is a political agenda. How else to explain the corruption of science that the East Anglia Climate Research email scandal has brought to light, and the efforts, presently, to dismiss the scandal as having no bearing on the evidence of climate change? If that were so, then why these efforts, or the earlier suppression of contrary or mitigating evidence that is the heart of the scandal?   
  
  
We find such an effort in this morning’s _Washington Post_, by one of those at the center of the scandal, Penn State’s Professor Michael E. Mann. Set aside his opening gambit — “I cannot condone some things that colleagues of mine wrote or requested” — this author of the famous, now infamous, “hockey stick” article seems not to recognize himself in Climategate. That he then goes after Sarah Palin as his critic suggests only that on a witness stand, confronted by his real critics, he’d be reduced to tears by even a mediocre lawyer. One such real critic is my colleague, climatologist Patrick J. Michaels, who documents the scandal and its implications for science in exquisite detail in this morning’s _Wall Street Journal_.   
  
  
But to return to the president and his speech, having uncritically subscribed to the science of global warming, Mr. Obama then lays out an ambitious policy agenda for the nation. We will meet our responsibility, he says, by phasing out fossil fuel subsidies (which pale in comparison to the renewable energy subsidies that alone make them economically feasible), we will put our people to work increasing efficiency in our homes and buildings, and we will pursue “comprehensive legislation to transform to a clean energy economy.”   
  
  
Mark that word “legislation,” because at the end of his speech the president said: “America has made our choice. We have charted our course, we have made our commitments, and we will do what we say.” But we haven’t made “our choice” — cap and trade, to take just one example, has gone nowhere in the Senate — even if Obama has made “our commitments.” And that brings us to a fundamental question: Can the president, with no input from a recalcitrant Congress, commit the nation to the radical economic conversion he promises?   
  
  
Environmental zealots say he can. Look at the report released last week by the Climate Law Institute’s Center for Biological Diversity, “ _Yes He Can_ : President Obama’s Power to Make an International Climate Commitment Without Waiting for Congress,” which argues that in Copenhagen Obama has all the power he needs under current law, quite apart from the will of Congress or the American people, to make a legally binding international commitment. Unfortunately, under current law, the report is right. I discuss that report and the larger constitutional implications of the modern “executive state” in this morning’s _National Review Online_.   
  
  
There is enough ambiguity in the president’s remarks this morning to suggest that he may not be prepared to exercise the full measure of his powers. But there is also enough in play to suggest that it is not only the corruption of science but the corruption of our Constitution that is at stake.
"
"
Since we’ve been talking about snow quite a bit recently, this seems fitting. WUWT reader Tom in Texas tips us to this image:

A composite of archival Hubble data taken with the Wide Field Planetary Camera    2 and the Advanced Camera for Surveys. Like a whirl of shiny flakes    sparkling in a snow globe, Hubble caught this glimpse of stars in  the    globular cluster M13. The cluster is home to over 100,000 stars, packed    closely together in a ball approximately 150 light-years across, and is    located at a distance of 25,000 light-years. Picture: AFP / NASA / ESA 
Click    here to see a high-res version of the Hubble snow globe I wonder what the sky would look like from a world in the center of that cluster? Would some of the stars look like bright marbles in the sky?
If you really want to see some interesting things from the HST, have a look at this gallery:
Hubble Space Telescope Advent Calendar 2009
Like the photo above, it gives some perspective about our place and scale in the universe. 





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8fb5ccdc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Exactly a year ago, the world was wrestling with the possibility of another US-led military assault on an Arab state, following the horrific gas attacks in Damascus, Syria.  When US military action did come in early August this year, it was in northern Iraq against the Islamic State (IS) which evolved out of the Syrian civil war. In the context of the spiralling humanitarian crisis, swift and co-ordinated IS advances, and single acts of astonishing barbarity, ongoing US attacks have become focused on control of a dam. It is the latest and most visible chapter in the world’s growing water crisis and confirmation of water’s central role in conflicts. The Mosul Dam blocks the Tigris River south of the Turkish border, forming a reservoir 11 billion cubic metres in volume – the fourth largest in the Middle East. Much of the military rhetoric has focused on the potential for deliberate destruction of the structure, releasing catastrophic flood waves reaching 4.6m high as far downstream as Baghdad, 350km away. But politically and economically it is the control of the dam’s hydroelectricity which gives it priority. Engineers, meanwhile, noting the reservoir’s unorthodox setting (on water-soluble karstic geology ) fear an accidental breach of the dam if vital geotechnical work, including continuous injection of impermeable grout, is not properly maintained. Strategically, the use of the dam to determine water levels and supplies to large parts of the country makes it the largest prize in what security analysts describe as a “battle for control of water” which many observers see as defining IS’s aims in Iraq.  This plan was evident as early as June this year, following extensive flooding caused by the deliberate closure of the captured Nuaimiyah Dam west of Baghdad.  But this is not the first time water has been used as a weapon in the “Fertile Crescent” at the confluence of the Tigris and Euphrates rivers. Saddam Hussein targeted water resources during the Iran-Iraq War and his oppression of the Marsh Arabs in southern Iraq during the 1990s centred on the drainage of 6,000 km2 of wetlands, destroying a subsistence economy perhaps 10,000 years old. This was a “war by other means”, according to engineer Azzam Alwash, who won the 2013 Goldman Environmental prize for his post-2003 work to re-establish the marshlands. The tactical use of water supplies in war dates back almost as far as civilisation itself. Limiting and depleting water supplies has been used as a siege weapon throughout history. The “Dambusters” are even part of the UK’s popular cultural memory of World War Two. But is the current zeitgeist – that this century will be marked by wars dominated by water – representative of a real or imagined threat? The UN was widely seen to endorsed this thesis in its 2009 World Water Development Report. Shortly after, an opinion article in the journal Nature roundly rejected it, claiming instead that “inequitable access to water resources is a result of…broader conflict and power dynamics: it does not itself cause war” and concluding that wars over water are a myth which distract from a globally progressive approach to co-operation in water management. So which position is correct? Mark Zeitoun, an expert on Middle East water politics, has developed a theory of “hydro-hegemony” in which control over water supplies is an intrinsic component of unequal power relationships. This is perhaps nowhere better illustrated than in relations between Israel and its neighbours which shift constantly and all-too-visibly from armed to unarmed conflicts, encompassing unilateral annexation of both land and water resources as well as uneasy bilateral agreements. In this view, water is an integral component of all kinds of conflict, from cultural antagonism to military aggression. It follows that as global demand for water grows and areas already experiencing water stress suffer further under predicted climate change, then the importance of water in tensions at all scales will grow proportionally.  Water is at the heart of many conflicts worldwide, whether between nations such as Egypt and Ethiopia, where diplomatic tensions are high regarding the construction of the massive Grand Renaissance Dam on the Nile; between developing world communities and multinational corporations, for example Coca-cola in India; or between regions within countries, such as in the western US where various states are in legal battles over the Rio Grande. We should remain confident that the strong frameworks of national and international law will continue to confine many of these conflicts to council chambers and diplomatic conferences. However, where these mechanisms break down then a shift on the spectrum of conflict towards violent confrontations, shaped by our fundamental human need for water, does seem possible if not inevitable. In the past months in northern Iraq, from an escalating Syrian crisis in which water stress likely played a destabilising part, we may have seen the first shots fired."
"

 **S** o the great energy fight of ’01 is on. Conservatives are doggedly rallying around the newly released Bush energy plan while liberals are attacking it with relish. Unfortunately, the fight is nine parts political theater to one part policy — and that “one part” of policy is so pregnant with economic mischief and counterproductive rhetoric that it’s beyond me why conservatives are so determined to play the role the administration is casting for them.



First of all, why are free‐​market types cheering the introduction of a “comprehensive national energy strategy”? After all, conservatives didn’t cheer a “comprehensive national Internet strategy” when one was proposed by Al Gore, a “comprehensive national industrial strategy” when one was proposed by Clinton Labor Secretary Robert Reich, or a “comprehensive national health‐​care strategy” when one was proposed by Hillary Clinton. Conservatives as a general matter believe that the government ought to leave markets alone and that “comprehensive national economic strategies” are things that old Soviet commissars and young French socialists are in the business of promoting, not free‐​market American presidents.



“Energy’s different,” they say. Excessive regulation and environmental opposition have shut down energy production and delivered us into a mess that only a “comprehensive national energy strategy” can sort out. Really?



Without the guidance of a “comprehensive national energy strategy,” investors are currently pouring billions into the energy sector. For instance, we’re currently in the midst of a power‐​plant construction boom, with some 90,000 megawatts of new electricity capacity scheduled to come on line by 2002 and a staggering 150,000–200,000 megawatts by 2004. This will not only burst the electricity‐​price bubble but will probably produce an electricity glut in the near future. Similarly, so many billions are flooding into the natural‐​gas market today that futures contracts are being made at half the price of today’s wholesale spot price. And high gasoline profit margins are inducing foreign refineries to enter the American market for the first time in decades and bringing new investment in domestic refining capacity as well. Barring some unforeseen supply disruption in the refining sector, gasoline prices will actually begin to decline slowly but steadily as the summer wears on.



What about all those “Not‐​In‐​My‐​Back‐​Yard” activists supposedly blocking the new wires and pipelines necessary to get energy from producers to consumers? You can certainly make an argument that the real problems in the energy sector are delivery problems, not production problems, but it’s unclear whether NIMBY is at the root of it.



Incumbent utilities have little incentive to build new transmission lines that would make it easier for ratepayers to buy cheaper power from competitors in neighboring service territories. Nor do utilities have an incentive to invest in new power lines when the profits allowed them by the Federal Energy Regulatory Commission are too low to make those investments particularly worthwhile. And with transmission rules still up in the air and unsettled at both the federal and state level, regulatory uncertainty is likewise dampening investment.



Similarly, there is little evidence that investors have been inhibited from increasing pipeline capacity when profit opportunities present themselves. The Energy Information Administration notes that pipeline capacity “has grown with end‐​use demand, and as new supplies have developed, new pipelines have been built to bring this gas to markets.” The Gas Research Institute likewise concludes that “growth in pipeline capacity is not a constraint on growth in gas supply. If supply is available, history has demonstrated that the pipelines will be built as needed. It is simply an investment and engineering issue.”



If government’s not in the way, why then did energy prices shoot up in the first place? Well, energy markets, like most commodity markets, are subject to boom and bust cycles. Energy prices after adjusting for inflation have been plummeting more or less for 15 years. Investors took money out of production and exploration budgets because profits were hard to come by. The bust suddenly ended last year, catching almost everyone by surprise, and the boom is now on. Investors are scrambling to expand supply, but capital investments take time. Let me make this simple: High prices = high profits = increased investment = price declines. It might take some time to get from here to there, but government’s record in speeding up the process is abysmal.



Regardless of President Bush’s gloomy rhetoric, this isn’t the beginning of America’s descent into the long dark economic night unless the feds can somehow come to the rescue. There’s plenty of energy around for suppliers to get their hands on and plenty of reason for people to conserve in the face of high prices. Rather, we’re experiencing the economic equivalent of a low‐​pressure front that will soon pass (indeed, already is passing) as long as government doesn’t do anything foolish in the meantime.



So how does the administration’s plan rank on that front? While there are literally about a hundred proposals that call for the government to “consider” this, “examine” that, and “investigate the possibility” of the other thing, the concrete proposals within the plan’s 170 pages are few and far between. Let’s look at the highlights:



Drilling on Federal Lands. Even if you’re happy digging up the tundra, there’s little reason to think that drilling in ANWR will do much to bring down energy prices. Industry’s best estimate is that ANWR could produce about 1 million barrels of oil per day at its peak. That’s a 1.25 percent increase in global production that, all things being equal, would reduce world oil prices by about 10 percent, from $25 per barrel to $22.50. While that’s nice, it wouldn’t do much of anything to deliver America from the power of the OPEC cartel, particularly since OPEC’s likely reaction would be to cut its own production to maintain world crude prices at today’s levels.



The administration would also like to increase industry access to various fields on federal lands in the lower‐​48 and in the Gulf of Mexico, primarily to get at natural gas deposits. That’s fine, but again, it’s not as if, without those new fields, existing gas wells would run dry.



None of this is to say that the federal government shouldn’t be a more reasonable economic steward of public lands. But it is to say that the administration is overselling the benefits that those policies will deliver to energy consumers.



Federal Eminent Domain Power For Electricity Transmission. As noted above, there are lots of reasons why utilities aren’t investing much in new transmission. NIMBY is only one of those reasons — and perhaps not even the most important. Having the feds step in and force private property owners to cut deals they don’t want to make with power companies seems antithetical to an administration that likes to talk about its commitment to private property rights.



The administration’s energy plan calls for the feds to adopt incentive‐​based rate making for transmission investments in lieu of the present rate‐​of‐​return regulatory regime. This ought to help some, but a better idea is to remove rate caps on transmission charges entirely. Still, let’s wait to pull out the federal guns on private landowners at least until we know they’re absolutely needed.



Tax Incentives for New Energy Production. If you’ve got an energy lobbyist in Washington, has Dick Cheney got a sack of money for you! Everyone’s a winner: oil; gas; hydrogen; hybrid and fuel‐​cell vehicles; superconductors; landfill methane; coal (make that “clean” coal, the adjective that is de rigueur whenever the word “coal” is used by this administration); ethanol; nuclear fission; nuclear fusion; solar; wind; bus, truck and automobile engine manufacturing; fuel cells; biomass; industrial cogeneration plants; and producers of energy efficient this and that. With this blizzard of new federal research and development initiatives, accelerated depreciation allowances, production tax credits, consumption tax credits, and subsidies for energy businesses competing in foreign markets, don’t expect the tax code to get any more comprehensible or your tax burden to get any lighter anytime soon.



It’s unclear why we need to bribe investors with tax money to take advantage of profit opportunities, and government’s track record at turning dubious ideas that don’t attract private investment into wonderful new economic toys is pretty bad (remember synfuels?). But hey, corporate welfare is what makes the political world go around.



Energy Welfare. If you’re poor, might soon be poor, or live in the Northeast, the Bush administration feels you pain. More tax money to the notoriously wasteful Low Income Energy Assistance Program; more tax money for the Weatherization Assistance Program; and more money for the Northeast Heating Oil Reserve. For this we elect Republicans?



Regulatory Fine‐​Tuning. This is a story of the good, the bad and the ugly. The good: repeal of the antiquated Public Utilities Holding Company Act (PUHCA), which dictates both the organizational structure and permissible service territories of electric power companies; and expedited renewal of permits for construction of the Trans‐​Alaskan natural gas pipeline. The bad: rumblings about increasing Corporate Average Fuel Efficiency (CAFÉ) standards for automobiles, standards which are simply back‐​door taxes on big cars, SUVs and mini‐​vans; further federal prohibitions on energy “inefficient” (read “cheap”) appliances; and a directive to federal agencies to pursue international agreements to address global climate change (haven’t we had enough of that for a while?). The ugly: tighter regulation of power plant emissions of sulfur dioxide, nitrogen oxide and mercury. Is more environmental regulation of power plants really necessary or really helpful at the moment?



So what exactly have we got here? The political equivalent of a sugar pill. The Bush energy plan won’t do much good, but it won’t do too much harm either. And that’s pretty good for government work.



On balance, we’d be better off if the administration had not opened this can of political worms. Had the administration simply focused on tinkering with regulations where necessary and revising federal land‐​use rules where politically possible, we would not need to put up with so much chaff for so little wheat. Instead, we’re now in the midst of an empty but still white‐​hot argument about whether we should more heavily subsidize this rather than that. We’re also subjected to a bizarre debate about whether “the nation” (as if we have some command‐​and‐​control, Soviet‐​style economy) should invest more heavily in supply or whether “the nation” should invest more heavily in conservation.



And to make its case, the administration is finding it useful to dredge up ludicrous arguments, such as the horrific implications of importing oil or the apocalyptic consequences of having investors go about their business without some detailed, comprehensive federal energy planning document to guide them.



Free‐​market types have no business in this intellectual ghetto. Nor do they have any business promoting most of this interventionist, corporate‐​welfare agenda.
"
"

Following the dubious example set recently by U.S. legislators, French politicians have informally proposed slapping punitive tariffs on goods from countries who refuse to curb greenhouse gas emissions. The German State Secretary for the Environment has, quite rightly, called foul: 



There are two problems — the WTO (World Trade Organization), and the signal would be that this is a new form of eco‐​imperialism,” Machnig said.   
  
  
“We are closing our markets for their products, and I don’t think this is a very helpful signal for the international negotiations.”



I have a paper forthcoming on the carbon tariff issue, but in the meantime here’s a recent op‐​ed (written jointly with Pat Michaels) on climate change policy mis‐​steps.
"
"
Share this...FacebookTwitterLooks like Thanksgiving is early this year. Here’s a feast for kings. 🙂
http://foia2011.org/index.php?id=2
Update: The real doozies are marked in bold print. having gone through them, I have to say sorry, but these scientists are nothing but liars. If the governments don’t act now on this, then we are truly living in anarchy. It will mean that all has turned into a free for all.
/// The IPCC Process ///
Thorne/MetO:
Observations do not show rising temperatures throughout the tropical
 troposphere unless you accept one single study and approach and discount a
wealth of others. This is just downright dangerous. We need to communicate the
uncertainty and be honest. Phil, hopefully we can find time to discuss these
further if necessary […]
Thorne:
I also think the science is being manipulated to put a political spin on it
which for all our sakes might not be too clever in the long run.
Carter:
It seems that a few people have a very strong say, and no matter how much
talking goes on beforehand, the big decisions are made at the eleventh hour by
a select core group.
Wigley:
Mike, The Figure you sent is very deceptive […] there have been a number of
 dishonest presentations of model results by individual authors and by IPCC […]
Overpeck:
The trick may be to decide on the main message and use that to guid[e] what’s
included and what is left out.
Overpeck:
I agree w/ Susan [Solomon] that we should try to put more in the bullet about
“Subsequent evidence” […] Need to convince readers that there really has been
an increase in knowledge – more evidence. What is it?
Wanner/NCCR:
In my [IPCC-TAR] review […] I crit[i]cized […] the Mann hockey[s]tick […]
My review was classified “unsignificant” even I inquired several times. Now the
internationally well known newspaper SPIEGEL got the information about these
early statements because I expressed my opinion in several talks, mainly in
Germany, in 2002 and 2003. I just refused to give an exclusive interview to
SPIEGEL because I will not cause damage for climate science.
Coe:
Hence the AR4 Section 2.7.1.1.2 dismissal of the ACRIM composite to be
instrumental rather than solar in origin is a bit controversial. Similarly IPCC
in their discussion on solar RF since the Maunder Minimum are very dependent on
the paper by Wang et al (which I have been unable to access) in the decision to
reduce the solar RF significantly despite the many papers to the contrary in
the ISSI workshop. All this leaves the IPCC almost entirely dependent on CO2
for the explanation of current global temperatures as in Fig 2.23. since
methane CFCs and aerosols are not increasing.
Briffa:
I find myself in the strange position of being very skeptical of the quality of
all present reconstructions, yet sounding like a pro greenhouse zealot here!
Jones:
I too don’t see why the schemes should be symmetrical. The temperature ones
certainly will not as we’re choosing the periods to show warming.

Trenberth:
[…] opposing some things said by people like Chris Landsea who has said all the
stuff going on is natural variability. In addition to the 4 hurricanes hitting
Florida, there has been a record number hit Japan 10?? and I saw a report
saying Japanese scientists had linked this to global warming. […] I am leaning
toward the idea of getting a box on changes in hurricanes, perhaps written by a
Japanese.
Jones:
We can put a note in that something will be there in the next draft, or Kevin
or I will write something – it depends on whether and what we get from Japan.
Jones:
Kevin, Seems that this potential Nature paper may be worth citing, if it does
say that GW is having an effect on TC activity.
Jones:
Getting people we know and trust [into IPCC] is vital – hence my comment about
the tornadoes group.
Jones:
Useful ones [for IPCC] might be Baldwin, Benestad (written on the solar/cloud
issue – on the right side, i.e anti-Svensmark), Bohm, Brown, Christy (will be
have to involve him ?)
Stott/MetO:
My most immediate concern is to whether to leave this statement [“probably the
warmest of the last millennium”] in or whether I should remove it in the
anticipation that by the time of the 4th Assessment Report we’ll have withdrawn
this statement – Chris Folland at least seems to think this is possible.
/// Communicating Climate Change ///
Humphrey/DEFRA:
I can’t overstate the HUGE amount of political interest in the project as a
message that the Government can give on climate change to help them tell their
story. They want the story to be a very strong one and don’t want to be made
to look foolish.
Fox/Environment Agency:
if we loose the chance to make climate change a reality to people in the
regions we will have missed a major trick in REGIS.
Adams:
Somehow we have to leave the[m] thinking OK, climate change is extremely
complicated, BUT I accept the dominant view that people are affecting it, and
that impacts produces risk that needs careful and urgent attention.
Lorenzoni:
I agree with the importance of extreme events as foci for public and
governmental opinion […] ‘climate change’ needs to be present in people’s
daily lives. They should be reminded that it is a continuously occurring and
evolving phenomenon
Jones:
We don’t really want the bullshit and optimistic stuff that Michael has written
[…] We’ll have to cut out some of his stuff.
Mann:
the important thing is to make sure they’re loosing the PR battle. That’s what
 the site [Real Climate] is about.
Ashton/co2.org:
Having established scale and urgency, the political challenge is then to turn
 this from an argument about the cost of cutting emissions – bad politics – to
 one about the value of a stable climate – much better politics. […] the most
 valuable thing to do is to tell the story about abrupt change as vividly as
 possible
Kelly:
the current commitments, even with some strengthening, are little different
from what would have happened without a climate treaty.
[…] the way to pitch the analysis is to argue that precautionary action must be
taken now to protect reserves etc against the inevitable
Singer/WWF:
we as an NGO working on climate policy need such a document pretty soon for the
public and for informed decision makers in order to get a) a debate started and
b) in order to get into the media the context between climate
 extremes/desasters/costs and finally the link between weather extremes and
 energy
Torok/CSIRO:
[…] idea of looking at the implications of climate change for what he termed
“global icons” […] One of these suggested icons was the Great Barrier Reef […]
It also became apparent that there was always a local “reason” for the
destruction – cyclones, starfish, fertilizers […] A perception of an
“unchanging” environment leads people to generate local explanations for coral
loss based on transient phenomena, while not acknowledging the possibility of
systematic damage from long-term climatic/environmental change […] Such a
project could do a lot to raise awareness of threats to the reef from climate
change
Minns/Tyndall Centre:
In my experience, global warming freezing is already a bit of a public
 relations problem with the media
Kjellen:
I agree with Nick that climate change might be a better labelling than global
 warming
Pierrehumbert:
What kind of circulation change could lock Europe into deadly summer heat waves
like that of last summer? That’s the sort of thing we need to think about.
/// The Medieval Warm Period ///
Pollack:
But it will be very difficult to make the MWP go away in Greenland.
Rahmstorf:
You chose to depict the one based on C14 solar data, which kind of stands out
in Medieval times. It would be much nicer to show the version driven by Be10
solar forcing
Cook:
A growing body of evidence clearly shows [2008] that hydroclimatic variability
during the putative MWP (more appropriately and inclusively called the
“Medieval Climate Anomaly” or MCA period) was more regionally extreme (mainly
in terms of the frequency and duration of megadroughts) than anything we have
seen in the 20th century, except perhaps for the Sahel. So in certain ways the
 MCA period may have been more climatically extreme than in modern times.
/// The Settled Science ///
Warren:
The results for 400 ppm stabilization look odd in many cases […] As it stands
we’ll have to delete the results from the paper if it is to be published.
Wils:
[2007] What if climate change appears to be just mainly a multidecadal natural
 fluctuation? They’ll kill us probably […]
Wilson:
Although I agree that GHGs are important in the 19th/20th century (especially
 since the 1970s), if the weighting of solar forcing was stronger in the models,
 surely this would diminish the significance of GHGs.
[…] it seems to me that by weighting the solar irradiance more strongly in the
 models, then much of the 19th to mid 20th century warming can be explained from
 the sun alone.
Hoskins:
If the tropical near surface specific humidity over tropical land has not gone
up (Fig 5) presumably that could explain why the expected amplification of the
warming in the tropics with height has not really been detected.
Jenkins/MetO:
would you agree that there is no convincing evidence for kilimanjaro glacier
melt being due to recent warming (let alone man-made warming)?
Jones:
[tropical glaciers] There is a small problem though with their retreat. They
have retreated a lot in the last 20 years yet the MSU2LT data would suggest
that temperatures haven’t increased at these levels.
Jones:
There shouldn’t be someone else at UEA with different views [from “recent
 extreme weather is due to global warming”] – at least not a climatologist.
Crowley:
I am not convinced that the “truth” is always worth reaching if it is at the
 cost of damaged personal relationships
Briffa:
Also there is much published evidence for Europe (and France in particular) of
increasing net primary productivity in natural and managed woodlands that may
be associated either with nitrogen or increasing CO2 or both. Contrast this
with the still controversial question of large-scale acid-rain-related forest
decline? To what extent is this issue now generally considered urgent, or even
real?
Crowley:
Phil, thanks for your thoughts – guarantee there will be no dirty laundry in
 the open.
Steig:
He’s skeptical that the warming is as great as we show in East Antarctica — he
 thinks the “right” answer is more like our detrended results in the
 supplementary text. I cannot argue he is wrong.
Jones:
This will reduce the 1940-1970 cooling in NH temps. Explaining the cooling with
 sulphates won’t be quite as necessary.
Haimberger:
It is interesting to see the lower tropospheric warming minimum in the tropics
 in all three plots, which I cannot explain. I believe it is spurious but it is
 remarkably robust against my adjustment efforts.
Klein/LLNL:
Does anybody have an explanation why there is a relative minimum (and some
 negative trends) between 500 and 700 hPa? No models with significant surface
 warming do this
Osborn:
This is an excellent idea, Mike, IN PRINCIPLE at least. In practise, however,
it raises some interesting results […] the analysis will not likely lie near to
the middle of the cloud of published series and explaining the reasons behind
this etc. will obscure the message of a short EOS piece.
Norwegian Meteorological Institute:
In Norway and Spitsbergen, it is possible to explain most of the warming after
 the 1960s by changes in the atmospheric circulation. The warming prior to 1940
cannot be explained in this way.
/// The Urban Heat Effect ///
Jenkins/MetO:
By coincidence I also got recently a paper from Rob which says “London’s UHI
has indeed become more intense since the 1960s esp during spring and summer”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Jones:
I think the urban-related warming should be smaller than this, but I can’t
think of a good way to argue this. I am hopeful of finding something in the
data that makes by their Figure 3.
Rean:
[…] we found the [urban warming] effect is pretty big in the areas we analyzed.
This is a little different from the result you obtained in 1990.
[…] We have published a few of papers on this topic in Chinese. Unfortunately,
when we sent our comments to the IPCC AR4, they were mostly rejected.
Wigley:
there are some nitpicky jerks who have criticized the Jones et al. data sets —
we don’t want one of those [EPRI/California Energy Commission meeting].
Jones:
The jerk you mention was called Good(e)rich who found urban warming at
all Californian sites.
Jones:
I think China is one of the few places that are affected [urban heat]. The
paper shows that London and Vienna (and also New York) are not affected in the
20th century.
Jones:
[…] every effort has been made to use data that are either rural and/or where
the urbanization effect has been removed as well as possible by statistical
means. There are 3 groups that have done this independently (CRU, NOAA and
GISS), and they end up with essentially the same results.
[…] Furthermore, the oceans have warmed at a rate consistent with the land.
There is no urban effect there.
/// Temperature Reconstructions ///
Wilson:
any method that incorporates all forms of uncertainty and error will
undoubtedly result in reconstructions with wider error bars than we currently
have. These many be more honest, but may not be too helpful for model
comparison attribution studies. We need to be careful with the wording I think.
Jones:
what he [Zwiers] has done comes to a different conclusion than Caspar and Gene!
I reckon this can be saved by careful wording.
Mitchell/MetO
Is the PCA approach robust? Are the results statistically significant? It seems
 to me that in the case of MBH the answer in each is no
Wilson:
I thought I’d play around with some randomly generated time-series and see if I
could ‘reconstruct’ northern hemisphere temperatures.
[…] The reconstructions clearly show a ‘hockey-stick’ trend. I guess this is
precisely the phenomenon that Macintyre has been going on about.
Bradley:
I’m sure you agree–the Mann/Jones GRL paper was truly pathetic and should
never have been published. I don’t want to be associated with that 2000 year
“reconstruction”.
Osborn:
Because how can we be critical of Crowley for throwing out 40-years in the
middle of his calibration, when we’re throwing out all post-1960 data ‘cos the
MXD has a non-temperature signal in it, and also all pre-1881 or pre-1871 data
‘cos the temperature data may have a non-temperature signal in it!
Esper:
Now, you Keith complain about the way we introduced our result, while saying it
is an important one. […] the IPCC curve needs to be improved according to
missing long-term declining trends/signals, which were removed (by
dendrochronologists!) before Mann merged the local records together. So, why
 don’t you want to let the result into science?
Cook:
I am afraid that Mike is defending something that increasingly can not be
 defended. He is investing too much personal stuff in this and not letting the
 science move ahead.
Cook:
One problem is that he [Mann] will be using the RegEM method, which provides no
better diagnostics (e.g. betas) than his original method. So we will still not
know where his estimates are coming from.
/// Science and Religion ///
Wigley:
I heard that Zichichi has links with the Vatican. A number of other greenhouse
 skeptics have extreme religious views.
Houghton [MetO, IPCC co-chair]
[…] we dont take seriously enough our God-given responsibility to care for the
 Earth […] 500 million people are expected to watch The Day After Tomorrow. We
must pray that they pick up that message.
Hulme:
My work is as Director of the national centre for climate change research, a
job which requires me to translate my Christian belief about stewardship of
 God’s planet into research and action.
Hulme:
He [another Met scientist] is a Christian and would talk authoritatively about
the state of climate science from the sort of standpoint you are wanting.
/// Climate Models ///
Watson/UEA:
I’d agree probably 10 years away to go from weather forecasting to ~ annual
scale. But the “big climate picture” includes ocean feedbacks on all time
 scales, carbon and other elemental cycles, etc. and it has to be several
decades before that is sorted out I would think. So I would guess that it will
not be models or theory, but observation that will provide the answer to the
question of how the climate will change in many decades time.
Shukla/IGES:
[“Future of the IPCC”, 2008] It is inconceivable that policymakers will be
willing to make billion-and trillion-dollar decisions for adaptation to the
projected regional climate change based on models that do not even describe and
simulate the processes that are the building blocks of climate variability.
Lanzante/NOAA:
While perhaps one could designate some subset of models as being poorer in a
lot of areas, there probably never will be a single universally superior model
or set of models. We should keep in mind that the climate system is complex, so
that it is difficult, if not impossible to define a metric that captures the
breath of physical processes relevant to even a narrow area of focus.
Santer:
there is no individual model that does well in all of the SST and water vapor
 tests we’ve applied.
Barnett:
[IPCC AR5 models] clearly, some tuning or very good luck involved. I doubt the
 modeling world will be able to get away with this much longer
Hegerl:
[IPCC AR5 models]
So using the 20th c for tuning is just doing what some people have long
 suspected us of doing […] and what the nonpublished diagram from NCAR showing
correlation between aerosol forcing and sensitivity also suggested.
Jones:
Basic problem is that all models are wrong – not got enough middle and low
 level clouds.
Jones:
GKSS is just one model and it is a model, so there is no need for it to be
correct.
/// The Cause ///
Mann:
By the way, when is Tom C going to formally publish his roughly 1500 year
reconstruction??? It would help the cause to be able to refer to that
reconstruction as confirming Mann and Jones, etc.
Mann:
They will (see below) allow us to provide some discussion of the synthetic
example, referring to the J. Cimate paper (which should be finally accepted
upon submission of the revised final draft), so that should help the cause a
 bit.
Mann:
I gave up on Judith Curry a while ago. I don’t know what she think’s she’s
 doing, but its not helping the cause
Berger:
Phil,
Many thanks for your paper and congratulations for reviving the global warming.
Jones:
[on temperature data adjustments] Upshot is that their trend will increase
Jones:
[to Hansen] Keep up the good work! […] Even though it’s been a mild winter in
the UK, much of the rest of the world seems coolish – expected though given the
La Nina. Roll on the next El Nino!
Schneider:
Even though I am virtually certain we shall lose on McCain-Lieberman, they are
forcing Senators to go on record for for against sensible climate policy
/// Freedom of Information ///
Jones:
I’ve been told that IPCC is above national FOI Acts. One way to cover yourself
 and all those working in AR5 would be to delete all emails at the end of the
 process
Briffa:
UEA does not hold the very vast majority of mine [potentially FOIable emails]
anyway which I copied onto private storage after the completion of the IPCC
task.
Osborn:
Keith and I have just searched through our emails for anything containing
“David Holland”. Everything we found was cc’d to you and/or Dave Palmer, which
you’ll already have.
McGarvie/UEA Director of Faculty Administration:
As we are testing EIR with the other climate audit org request relating to
communications with other academic colleagues, I think that we would weaken
that case if we supplied the information in this case. So I would suggest that
we decline this one (at the very end of the time period)
Jones:
[FOI, temperature data]
Any work we have done in the past is done on the back of the research grants we
 get – and has to be well hidden. I’ve discussed this with the main funder (US
 Dept of Energy) in the past and they are happy about not releasing the original
 station data.
/// FOIA 2011 — Background and Context ///
“Over 2.5 billion people live on less than $2 a day.””Every day nearly 16.000 children die from hunger and related causes.”
“One dollar can save a life” — the opposite must also be true.
“Poverty is a death sentence.”
“Nations must invest $37 trillion in energy technologies by 2030 to stabilize
greenhouse gas emissions at sustainable levels.”
Today’s decisions should be based on all the information we can get, not on
hiding the decline.
This archive contains some 5.000 emails picked from keyword searches. A few
remarks and redactions are marked with triple brackets.
The rest, some 220.000, are encrypted for various reasons. We are not planning
to publicly release the passphrase. We could not read every one, but tried to cover the most relevant topics such
as…
 
 
Share this...FacebookTwitter "
"New cars sold in the UK produce more carbon dioxide than older models, according to new research that suggests the industry is going backwards in tackling the climate crisis. Cars that reach the latest standards of emissions use cleaner internal combustion engine technology to combat air pollution, but the relentless rise in demand for bigger, heavier models meant that average emissions of the greenhouse gas rose, according to the consumer group Which? The latest generation of cars produced 7% more emissions than those manufactured to earlier standards, testing of 292 models released in the UK since 2017 found. Cars account for just over 18% of UK emissions, according to government figures, and reining back pollution from the sector is seen as crucial to efforts to cut the country’s carbon emissions to net zero by 2050. Lisa Barber, editor of Which? magazine, said: “It is shocking to see our tests uncover increasing levels of carbon dioxide emissions for the latest cars that are being built and sold to UK consumers. “Manufacturers must ensure that they are doing everything in their power to create cleaner vehicles that are fitter for our planet and its future.” Overall, cars that met the latest emissions regulations (standards known as Euro 6d and Euro 6d-temp) produced 162.1g of CO2 per kilometre, 10.5g more than those in the previous generation (Euro 6b and Euro 6c). That was far above the 95g target carmakers must meet across all EU sales in order to avoid steep fines. Manufacturers across Europe are racing to make and sell new electric models in order to meet the rules, although many are relying on hybrid models that combine internal combustion with battery power. Mike Hawes, chief executive of the Society of Motor Manufacturers and Traders (SMMT), the industry lobby group, said: “We can’t comment on the results of non-official tests by commercial organisations where the methodology is unclear. “Only the official, Europe-wide WLTP [Worldwide Harmonised Light Vehicle Test Procedure] test – the toughest and most comprehensive in the world – can be relied upon by consumers to accurately compare vehicles on a like-for-like and repeatable basis. This shows that new cars emit, on average, some 29.3% less CO2 than models produced in 2000, the effect of which drivers can see at the pump.” However, the new findings tally with SMMT data which found that the average CO2 output of cars sold in the UK has risen for the past three years. Cars sold in the UK in 2019 produced average emissions of 127.9g of CO2 per kilometre. The Which? analysis found that carbon emissions were rising across every segment of the car market, from smaller city cars through to SUVs, as manufacturers packed more technology into their cars. Emissions rose fastest in the hybrid segment, up by 31% between generations, in part because of the weight of two different power sources. Newer cars performed significantly better on air quality issues, with the latest models slashing emissions of carbon monoxide and nitrogen oxides, both of which directly harm human health. The tests also found that carbon emissions were higher than official readings carried out by EU regulators, which do not measure extended use at motorway speeds or take into account a car full of people using the air conditioning and the radio. Doug Parr, chief scientist at Greenpeace, said the figures showed that the government should ban the sale of new petrol and diesel cars from 2030, earlier than current plans to ban internal combustion engines from 2035. “It’s clear that we can’t simply rely on the car industry’s good will to make progress,” he said. “We need decisive intervention from government, starting with enforcing existing rules on car emissions and bringing in a firm ban on sales of new diesel and petrol vehicles by 2030.” • This article was amended on 28 February 2020 because an earlier version misnamed the Society of Motor Manufacturers and Traders, as the Society for Motor Manufacturers and Traders. This has been corrected."
"Octopuses grow quickly, have lots of tasty flesh and are found all over the world. As the world’s supply of fish diminishes while the number of humans keeps increasing, it seems these creatures would make an ideal mass-produced food for our hungry mouths. So where are all the octopus farms? The main thing that prevents octopus farming at large scale is that the common octopus – Octopus vulgaris – is tough to feed in captivity, especially when first born. After hatching, octopuses first exist as tiny organisms known as paralarvae, drifting around the upper ocean in among clouds of plankton which they feed on. It is this stage – before they become fully fledged young adults and descend further into the sea – which is hardest to replicate in aquaculture.  Feeding octopuses adequately during their first two months of life is a challenge. In this period, octopuses have highly selective feeding habits, and acceptable survival rates are hard to achieve. At an industrial scale, the only possible solution is to take wild juveniles caught at sea and grow them in floating sea cages. Fishermen start with individuals of about 800 grams and grow them until they are more than 2-3kg, supplying them with crustaceans and low value fish over a period of three or four months.  Fishermen cooperatives in north-western Spain grow octopus in sea-cages. They sell them at high season – Christmas and summer – where large creatures can reach €10-12 per kilo, double the usual price. So far, research has allowed small-scale production by artisanal producers in Vigo, Galicia, reaching a production of just ten metric tons per year. But this system is highly dependent on the success of the initial catches; without a good crop of smaller octopuses to grow in the cages, end results will always be limited. This is why farmed-raised octopus hasn’t had commercial success yet. Over the past 15 years the Spanish Institute of Oceanography (IEO) in Vigo has carried out important and successful research to overcome the problems with octopus cultivation, and the institute is now focused on rearing octopus across a full life cycle – from hatch to catch. In fact they managed to complete full cultivation across the life cycle of several octopuses for the first time ever in 2001. This experiment was achieved after using live crustacean larvae known as zoeae as prey along with the commonly used artemia, a brine shrimp. However, it is very difficult to obtain these zoeae in large quantities, making large-scale production prohibitively expensive. So researchers are now focusing on analysing the biochemical composition of the larvae to find out what makes them tick – and what makes octopuses find them so tasty (or at least edible). Once identified, the idea is to ensure the cultured enriched artemia has the same features. But once the octopus is up to a certain size, there is another step to be solved: the transition between paralarvae and juvenile. This stage is another mortality peak in octopus farms.  Working with other species that don’t have a paralarvae phase would help, such as the Mexican Four-Eyed Octopus – known scientifically as Octopus maya. Like cuttlefish, these octopuses hatch ready for the deep seas, with all the same features as their adult selves. There is still a transition phase of sorts though, when they still need commercial feed pellets to grow adequately. Octopus maya cultivation represents the most advanced attempts at commercially sound cephalopod aquaculture. But even with this species, it has been necessary to rely on targeting a specialised gourmet-level market. In conclusion, the best octopus farms cannot yet compete directly with the common wild-captured product.  Developments in feeding products and techniques over the next few years will be key. Once small octopuses can be fed in large numbers, the development of a full undersea farming industry will be much easier."
"The Morrison government will on Friday signal plans to shift investment from wind and solar to hydrogen, carbon capture and storage, lithium and advanced livestock feed supplements, as part of a “bottom up” strategy to reduce emissions by 2050. Angus Taylor will use a speech to an economic thinktank to put some flesh on the bones of the Coalition’s much-vaunted technology roadmap. The emissions reduction minister will also declare Australia will take a technology-based long-term emissions reduction “strategy” to the United Nations-led climate talks in Glasgow at the end of this year. While not ruling out adopting a specific emissions reduction target, Taylor will contend the “top down” approach of countries proposing emissions reduction targets in the global climate framework has “failed” because countries are not delivering on their commitments. According to a speech extract circulated in advance, Taylor will say the government intends to roll out “a series of detailed pieces of work” between now and the United Nations climate change conference in Glasgow, known at COP26, in November. Taylor will say Australia wants “to lead the world” on a new approach to laying out domestic abatement plans. As well as the roadmap, the government is reviewing its much-criticised emissions reduction fund and the operation of the safeguard mechanism, and is working on an electric vehicles strategy, despite blasting Labor during last year’s election, claiming measures to drive the takeup of EVs were a “war on the weekend”. Taylor will say the looming technology investment roadmap will form the cornerstone of the government’s 2050 emissions reduction strategy, providing guidance to the public and the private sector on “what future energy and emissions-reduction technologies the government will prioritise”. He will say the government has already invested $10.4bn into clean technology projects with a value of $35bn, but declare “we are coming to an end of the value of these investments”. “Wind and solar are economic as a source of pure energy at least, and the government should not crowd out private sector investment,” the minister will say. “We must move our investments to the next challenges – hydrogen, carbon capture and storage, lithium and advanced livestock feed supplements to name a few”. Taylor will say the roadmap will include “measurable economic goals for technology that will allow us to assess the cost curve for technologies and to give a clear signal for when a technology is commercial”. The government will be a player in investing in emerging technology “as both a market signal and leader” but the objective will be to generate significant private sector investment. “To be successful from both a portfolio and from a technology perspective we must track how much private sector and other investment in R and D and early deployment follows our own investment,” Taylor will say. “To measure the success of the overall portfolio I think we should be aiming for a four or five time multiplier. That is for every dollar invested I want to see four or five dollars from the private sector following over the course of our investments.” It is unclear from the speech extract what policy mechanism will drive the new investments and who will administer the research and development, and it does not address why the government is looking at technology such as carbon capture and storage, or CCS. Most low emissions scenarios for the future consider CCS a potentially important technology if the world is to keep global heating to 1.5C about pre-industrial levels, but only when used with biomass, not fossil fuels. The technology has not proved commercially viable with fossil fuel power generation despite being promised billions in taxpayer support through initiatives such as the CCS flagships program. There are fewer than 20 CCS projects across the globe in the industrial sector. Chevron last year announced a $2.5bn CCS project at one of the country’s largest liquefied natural gas developments, in the Pilbara, had finally begun injecting carbon dioxide underground after three years of delays. Taylor’s speech flags a consultation paper to inform the design of the roadmap. The tone of the speech extract suggests the government is reluctant to adopt a target, having blasted Labor since the opposition adopted a net zero target by 2050 in its first major climate policy announcement since losing the 2019 election. But it is not definitive. On Thursday, the Liberal MP Trent Zimmerman continued his call for the leadership to embrace net zero. “I think it is a target we need to look at,” the member for North Sydney told the ABC. “We have a good target for 2030 but we need to look beyond that as we head into Glasgow.” Taylor will say on Friday: “If I could stand up today – announce a target and see the CO2 reduce – then I would. “If setting a target today would lower emissions – then today would be a short speech. I wouldn’t have to outline our plan and I wouldn’t have to outline all the work that has led to today.” Guardian Australia revealed on Thursday new analysis by ClimateWorks Australia suggesting Australia can achieve a transition to net zero emissions by 2050 with known technologies, but the deployment of low emissions options will need to be accelerated significantly."
"
Share this...FacebookTwitterGermany will completely abandon nuclear energy and technology by 2022, this decided by Angela Merkel’s government. Merkel and German activist leaders think they can competitively power the country with windmills, biogas, solar, and blind faith. The last nuclear reactors so will go offline by the year 2022. Read here.China, seeing a golden opportunity, aims to capitalise on Merkel’s hasty, panicked decision and now hopes to lure German engineers and nuclear scientists to China in order to accelerate its own use of nuclear energy (hat-tip: The Liberale Institute here), so writes Der Spiegel here:
The People’s Republic wants to profit from Merkel’s nuclear power stop. Peking wants to attract researchers and employees from German power plants. The country has embarked without any hesitation on a path to nuclear energy – the Chinese all but exclude a disaster like that in Fukushima.”
China is not worried about nuclear accidents and safety issues. Gee I wonder why? I wonder if an unemotional look at the following graphic might have something to do with that:

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
China’s leaders are bewildered by Germany’s hysterical move. Der Spiegel quotes a CNEA deputy:
It is false that a country with so few resources of its own would abandon nuclear power, Deputy General Secretary of the Chinese Atomic Agency CNEA, Xu Yuming, told the Frankfurter Allgemeine Zeitung. His criticism also included an offer to the German nuclear specialists: ‘We invite the German specialists to research for us in China and to work. The German nuclear plants are mong the best in the world, the engineers and scientists have a great reputation’.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




China has ambitious nuclear energy plans for the future as it gets set to put the country on a rapid development course. According to the World Nuclear Association, 62 reactors are currently under construction worldwide, 27 in China alone. Here’s what the rest of the planet, outside of the Berlin Enviro-Wall, is doing.
Plants under construction (a few selected countries)
China: 27 reactors
Russia: 10
India: 5
Germany: 0
German nuclear engineers and scientists won’t have have any trouble finding work, that’s for sure. Many more reactors are being planned – 158 in total.
Reactors planned:
China: 50 reactors
India: 18
Japan: 12
Russia 14
USA: 9
Germany: 0
And many more are being proposed (324 reactors in all):
China: 110 reactors
India: 40
Italy 10
Russia 30
Ukraine: 20
Japan: 12
Russia 14
USA: 23
UAE: 10
Vietnam: 12
Turkey: 4
Poland: 6
Germany: 0
Not everyone is happy about Germany cutting and running on nuclear energy. Especially industry that relies on cheap and reliable energy are not pleased about it. For example, Daimler CEO Dieter Zetsche is not happy and is quoted in Die Welt here, calling it a “risky and emotional decision” that makes Germany a less attractive location for industry.
China has got to be laughing.
Share this...FacebookTwitter "
"

Suppose you think the federal government is a bloated monstrosity in need of a stomach‐​stapling, extreme makeover. What should be done? Small‐​government types — free‐​market conservatives and libertarians — are increasingly at odds on this question.



Tax hawks like Grover Norquist, of Americans for Tax Reform, maintain that we should “starve the beast”: create pressure on Congress to reduce spending by cutting the government’s intake of taxes and running up deficits. This is the approach prescribed last year by Milton Friedman and Gary Becker, both Nobel Prize‐​winning free‐​market economists, in separate Wall Street Journal op‐​eds. Friedman predicts that “deficits will be an effective… restraint on the spending propensities of the executive branch and the legislature. The public reaction will make that restraint effective.”



However, economist William Niskanen, chairman of the Cato Institute (also my employer), has presented econometric evidence that federal spending tends to increase when tax revenues decline, flatly contradicting the starve‐​the‐​beast theory. Furthermore, according to William Gale and Brennan Kelly of the Brookings Institution, members of Congress who signed the President’s “No New Taxes” pledge were more, not less, likely to vote for spending increases, which is hard to square with the starve‐​the‐​beast theory.



“Starve the beast” is really a conjecture about the psychology of voters and legislators. The idea embodied in Friedman’s statement is that mounting deficits will spur voters to choose representatives who will impose fiscal discipline. But why would voters react that way? Will they be worried about deficits causing rising interest rates, or about the prospect that their children will be stuck with a huge bill?



It seems just as likely that current voters would prefer to have their kids and grandkids foot the bill. In the long run, we’re all dead, and the dead don’t pay taxes. If the doctor gives you a month to live, why not run up the Visa?



Niskanen’s analysis suggests that when current spending is financed by current taxes, voters see it as their money being spent, and so are more motivated to be frugal. But when current spending is financed by debt, voters see it as future voters’ money being spent. If voters prefer to benefit now and have some one else pay later, there is no good reason to think legislators will see deficits as a reason to restrain themselves.



Starve‐​the‐​beast advocates might retort that the theory has yet to be tried. Sure, we’re running record deficits. Sure, we’ve had tax cuts. Sure, most Republicans in Congress nevertheless voted for plush increases in education, defense, Medicare and more. And sure, President Bush has never seen a spending bill he wouldn’t sign. The reason “starve the beast” has yet to kick in, they say, is that things aren’t bad enough yet.



But if the deficit reaches crisis proportions — and it will, quickly, if it continues to grow at the current rate — we should not imagine that the government will rush to contain the crisis by rapidly cutting the fat from government. As George Mason University economist Alex Tabarrok recently argued, “The combination of changing demographics and current tax cuts is seeding our economy for a fiscal ‘perfect storm.’ When the storm hits, there will be a crisis, and… small government rarely does well in a crisis.”



So, given our monumentally huge deficits, and the unsustainability of current policy, should small‐​government folks give up on further tax cuts, at least for now? That’s a harder question than you might think.



For many libertarians and conservatives, cutting taxes is about more than efficiency; it’s about morality. We have a moral claim to the fruits of our labor. Every cent the government takes from us beyond what is strictly necessary to secure our basic rights is a token of injustice. Cutting excess taxes is rectification, a way of making abused taxpayers whole. Therefore, for many proponents of smaller government, passing up a chance at a tax cut, or, worse, defending a tax increase, is a willing perpetuation of injustice.



However, if further tax cuts would _accelerate_ deficit spending, justice would be threatened. Under present conditions, further tax cuts would largely be tax shifts, moving the burden of government spending to future generations. And there is nothing notably moral about raising taxes on the future to subsidize the present.



By promulgating the idea that given a tax cut, spending will take care of itself, advocates of the “starve the beast” theory have helped produce a political climate in which principled vigilance about spending seems unnecessary.



But we need principled political discipline now more than ever. It is not enough to cut out the pork. According to economists Jagadeesh Gokhale of the Cato Institute and Kent Smetters, cutting the entire discretionary budget forever would still not be enough. The real fiscal beasts are Social Security and Medicare. Unless they are tamed by serious reform, they will grow out of control and devour almost all future federal revenues.



A sustainable and just America requires the principled will to eliminate the unconstitutional, the inessential and the ineffective, and the courage to reform Social Security and Medicare today so that future generations will inherit a world at least as well‐​off as our own. 
"
"
UPDATE: Some readers took exception to my title, and I can see why now. I regret my choice of wording for the title. “Regulate its escape into the atmosphere” is where I was going. “Regulate” from my perspective in engineering things and making things work is different than what others might think. I wasn’t implying legislation. Recycling and recovery systems is what was in my mind.  Gas regulator valves and all that. This passage from the story below was my focus: “Since we already know how to capture methane from animals, landfills, and sewage treatment plants at fairly low cost, targeting methane makes sense,”.
I’ve amended the title [in brackets] -Anthony
…
According to the 2007 IPCC AR4  Methane has a “global warming potential” of 25 times that of CO2 over 100 years. Here’s a CH4 budget pie chart. Note that there are several sources where we can manage methane without affecting energy creation. Starting on Methane, rather than CO2, is an idea that I could get behind because it can be recycled and used for many things.



A new paper from Drew Shindell from NASA JPL prompted Roger Pielke Jr. to write:


For years my father has been arguing that:
. . . attempts to “control” the climate system, and to prevent a “dangerous intervention” into the climate system by humans that focuses just on CO2 and a few other greenhouse gases will necessarily be significantly incomplete, unless all of the other first order climate forcings are considered.
His views are now being robustly vindicated as a quiet revolution is occurring in climate science.  Here is how PhysOrg reports on a study out today in Science by NASA’s Drew Shindell and others:
According to Shindell, the new findings underscore the importance of devising multi-pronged strategies to address climate change rather than focusing exclusively on carbon dioxide. “Our calculations suggest that all the non-carbon dioxide greenhouse gases together have a net impact that rivals the warming caused by carbon dioxide.”
In particular, the study reinforces the idea that proposals to reduce methane may be an easier place for policy makers to start climate change agreements. “Since we already know how to capture methane from animals, landfills, and sewage treatment plants at fairly low cost, targeting methane makes sense,” said Michael MacCracken, chief scientist for the Climate Institute in Washington, D.C.
This research also provides regulators insight into how certain pollution mitigation strategies might simultaneously affect climate and air quality. Reductions of carbon monoxide, for example, would have positive effects for both climate and the public’s health, while reducing nitrogen oxide could have a positive impact on health but a negative impact on the climate.
“The bottom line is that the chemistry of the atmosphere can get hideously complicated,” said Schmidt. “Sorting out what affects climate and what affects air quality isn’t simple, but we’re making progress.”
Of note, Shindell et al. cautiously suggest that the entire framework of international climate policy may be based on an overly-simplistic view of the human effect on climate, by focusing on carbon dioxide equivalencies in radiative forcing (i.e.,g “global warming potential” or GWP), from their Science paper out today (emphasis added):
There are many limitations to the GWP concept (25). It includes only physical properties, and its definition is equivalent to an unrealistic economic scenario of no discounting through the selected time horizon followed by discounting to zero value thereafter. The 100-year time horizon conventionally chosen strongly reduces the influence of species that are short-lived relative to CO2. Additionally, GWPs assume that integrated global mean RF is a useful indicator of climate change. Although this is generally reasonable at the global scale, GWP does not take into account the rate of change, and it neglects that the surface temperature response to regionally distributed forcings depends on the location of the RF (26) and that precipitation and circulation responses may be even more sensitive to RF location (27). Along with their dependence on emission timing and location, this makes GWPs particularly ill-suited to very short-lived species such as NOx, SO2, or ammonia, although they are more reasonable for longer-lived CO. Inclusion of short-lived species in agreements alongside long-lived greenhouse gases is thus problematic (28, 29).


Read his complete commentary here


Here’s the press release from NASA/JPL with comments from Drew Shindel also.


Interactions with Aerosols Boost Warming Potential of Some Gases



Surface Methane - Credit NASA Goddard
This map shows the distribution of methane at the surface. New research shows that methane has an elevated warming effect due to its interactions with other substances in the atmosphere.   For decades, climate scientists have worked to identify and measure key substances — notably greenhouse gases and aerosol particles — that affect Earth’s climate. And they’ve been aided by ever more sophisticated computer models that make estimating the relative impact of each type of pollutant more reliable.
Yet the complexity of nature — and the models used to quantify it — continues to serve up surprises. The most recent? Certain gases that cause warming are so closely linked with the production of aerosols that the emissions of one type of pollutant can indirectly affect the quantity of the other. And for two key gases that cause warming, these so-called “gas-aerosol interactions” can amplify their impact.
“We’ve known for years that methane and carbon monoxide have a warming effect,” said Drew Shindell, a climate scientist at the NASA Goddard Institute for Space Studies (GISS) in New York and lead author of a study published this week in Science. “But our new findings suggest these gases have a significantly more powerful warming impact than previously thought.”
Mixing a Chemical Soup
When vehicles, factories, landfills, and livestock emit methane and carbon monoxide into the atmosphere, they are doing more than just increasing their atmospheric concentrations. The release of these gases also have indirect effects on a variety of other atmospheric constituents, including reducing the production of particles called aerosols that can influence both the climate and the  air quality. These two gases, as well as others, are part of a complicated cascade of chemical reactions that features competition with aerosols for highly reactive molecules that cleanse the air of pollutants.

“Emissions-based” estimates highlight the indirect effects that emissions of certain gases can have on the climate via aerosols, methane, ozone, and other substances in the atmosphere. Credit: NASA/GISS › Larger image
Aerosols can have either a warming or cooling effect, depending on their composition, but the two aerosol types that Shindell modeled — sulfates and nitrates — scatter incoming light and affect clouds in ways that cool Earth. They are also related to the formation of acid rain and can cause respiratory distress and other health problems for those who breathe them.
Human activity is a major source of sulfate aerosols, but smokestacks don’t emit sulfate particles directly. Rather, coal power production and other industrial processes release sulfur dioxide — the same gas that billows from volcanoes — that later reacts with atmospheric molecules called hydroxyl radicals to produce sulfates as a byproduct. Hydroxyl is so reactive scientists consider it an atmospheric “detergent” or “scrubber” because it cleanses the atmosphere of many types of pollution.
In the chemical soup of the lower atmosphere, however, sulfur dioxide isn’t the only substance interacting with hydroxyl. Similar reactions influence the creation of nitrate aerosols. And hydroxyls drive long chains of reactions involving other common gases, including ozone.
Methane and carbon monoxide use up hydroxyl that would otherwise produce sulfate, thereby reducing the concentration of sulfate aerosols. It’s a seemingly minor change, but it makes a difference to the climate. “More methane means less hydroxyl, less sulfate, and more warming,” Shindell explained.
 Many atmospheric pollutants compete for access to hydroxyl radicals (OH), highly reactive molecules that “scrub” the atmosphere of pollutants. This diagram illustrates hydroxyl converting methane (CH4) into carbon dioxide (CO2) and sulfur dioxide (SO2) into sulfate aerosols. Credit: NASA/GISS › Larger image
His team’s modeling experiment, one of the first to rigorously quantify the impact of gas-aerosol interactions on both climate and air quality, showed that increases in global methane emissions have caused a 26 percent decrease in hydroxyl and an 11 percent decrease in the number concentration of sulfate particles. Reducing sulfate unmasks methane’s warming by 20 to 40 percent over current estimates, but also helps reduce negative health effects from sulfate aerosols.
In comparison, the model calculated that global carbon monoxide emissions have caused a 13 percent reduction in hydroxyl and 9 percent reduction in sulfate aerosols.
Nitrogen oxides — pollutants produced largely by power plants, trucks, and cars — led to overall cooling when their effects on aerosol particles are included, said Nadine Unger, another coauthor on the paper and a climate scientist at GISS. That’s noteworthy because nitrogen oxides have primarily been associated with ozone formation and warming in the past.
A New Approach
To determine the climate impact of particular greenhouse gases, scientists have traditionally relied on surface stations and satellites to measure the concentration of each gas in the air. Then, they have extrapolated such measurements to arrive at a global estimate.
The drawback to that “abundance-based approach,” explained Gavin Schmidt, another GISS climate scientist and coauthor of the study, is that it doesn’t account for the constant interactions that occur between various atmospheric constituents. Nor is it easy to parse out whether pollutants have human or natural origins.
Natural sources of methane include wetlands, termites, decomposing organic materials in ocean and fresh water, and a type of ice called methane hydrate. Man-made methane sources include livestock, rice paddies, biomass burning, landfills, coal mining, and gas production. Credit: U.S Dept. of Energy Technology Laboratory
› Larger image “You get a much more accurate picture of how human emissions are impacting the climate — and how policy makers might effectively counteract climate change — if you look at what’s emitted at the surface rather than what ends up in the atmosphere,” said Shindell, who used this “emissions-based” approach as the groundwork for this modeling project.
However, the abundance-based approach serves as the foundation of key international climate treaties, such as the Kyoto Protocol or the carbon dioxide cap-and-trade plans being discussed among policymakers. Such treaties underestimate the contributions of methane and carbon monoxide to global warming, Shindell said.
Unpacking the Implications
According to Shindell, the new findings underscore the importance of devising multi-pronged strategies to address climate change rather than focusing exclusively on carbon dioxide. “Our calculations suggest that all the non-carbon dioxide greenhouse gases together have a net impact that rivals the warming caused by carbon dioxide.”
In particular, the study reinforces the idea that proposals to reduce methane may be an easier place for policy makers to start climate change agreements. “Since we already know how to capture methane from animals, landfills, and sewage treatment plants at fairly low cost, targeting methane makes sense,” said Michael MacCracken, chief scientist for the Climate Institute in Washington, D.C.
This research also provides regulators insight into how certain pollution mitigation strategies might simultaneously affect climate and air quality. Reductions of carbon monoxide, for example, would have positive effects for both climate and the public’s health, while reducing nitrogen oxide could have a positive impact on health but a negative impact on the climate.
“The bottom line is that the chemistry of the atmosphere can get hideously complicated,” said Schmidt. “Sorting out what affects climate and what affects air quality isn’t simple, but we’re making progress.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91dd7f8d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Argentina’s miseries now cry out in the headlines: riots and violence, a farcical procession of presidents‐​for‐​a‐​day, and the gathering doom of default and devaluation. But behind the headlines lurk deeper ills that gnaw away at the foundations of the country’s political and economic life. Those ills helped to bring about the current crisis, and they will persist long after the media spotlight now on Argentina fades away.



Argentina’s woes are many, but underlying them all is the dilapidated state of its political and legal institutions. According to an annual index of corruption levels published by Transparency International and based on surveys of business people, academics and risk analysts around the world, in 2001 Argentina ranked a dismal 57th out of 91 countries. Worse, in other words, than Botswana, Namibia, Peru, Brazil, Bulgaria, and Colombia, and on par with notoriously corrupt China. 



The same results came through in the 2000 Global Competitiveness Report, coproduced by Harvard University and the World Economic Forum, which surveyed business leaders from 4,022 firms in 59 countries on their perceptions of business conditions. Again, Argentina languished near the bottom: 40th for the frequency of irregular payments to government officials; 54th in the independence of the judiciary; 55th in litigation costs; 45th for corruption in the legal system; and 54th in the reliability of police protection. 



It wasn’t always this way. The disrepair of Argentina’s institutional infrastructure is a legacy of its Perónist past. Look, for example, at the crucial question of judicial independence. Prior to the descent into statism, justices of Argentina’s Supreme Court enjoyed long tenures undisturbed by political interference. At the beginning of Juan Perón’s first administration in 1946, Supreme Court justices averaged 12 years on the bench. 



It’s been downhill since then. Since 1960, the average tenure has dropped below four years. After Perón (he left the presidency for the second time in 1974), five of 17 presidents named every member of the court during their term, a distinction that had previously been limited to Bartolomé Mitre, the country’s first constitutional president (1862–1868). And so, while before Perón, it was typical for a majority of the court to have been appointed by presidents from the political opposition, that was no longer the case. The Supreme Court, the supposed bulwark of the rule of law, was reduced to a puppet of executive power. 



The pro‐​market reforms of the early 1990s brought little improvement. President Carlos Menem, who deserves credit for stabilizing the currency and privatizing industries, nonetheless persisted in traducing the integrity of the country’s institutions. Faced with a politically hostile Supreme Court, Mr. Menem responded with a court‐​packing scheme — he expanded the court from five to nine members and filled the new slots with political supporters. 



His transgressions did not stop there: Allegations of corruption swirled throughout his two terms in office. Those charges finally caught up with him in June of last year, when the former president was arrested for his alleged role in an illegal arms‐​shipments deal. But after five months of house arrest, Mr. Menem was set free by his hand‐​picked Supreme Court. 



Corruption in Argentina extends far beyond Buenos Aires. To get a first‐​hand look at the problem, I visited the northwestern province of Tucumán earlier this year. During the “dirty war” of the 1970s, Tucumán served as a refuge for pro‐​Castro guerillas and was roiled by bloody fighting. Today it is better known as home to the world’s largest producer of lemons, as well as a now‐​declining sugar industry, and its problems are more prosaic: bloated and corrupt bureaucracy, and a backward and unreliable legal system. 



The public sector in Tucumán, for example, serves primarily to enrich politicians and fund patronage jobs. Out of a formal work force of some 400,000, there are nearly 80,000 provincial and municipal government employees and another 10,000 federal government workers. Elected officials siphon off small fortunes for themselves: The annual salary for provincial legislators is roughly $300,000. 



Tucumán is by no means noteworthy for such abuses. In the impoverished province of Formosa on the country’s northern border, about half of all formally employed workers are on the government payroll, and many show up only once a month — to collect their paychecks. 



Such profligacy lies at the root of Argentina’s present financial crisis. Government spending as a percentage of gross domestic product climbed to 21% in 2000 from 9.4% in 1989 despite the fact that sweeping privatizations were alleviating significant fiscal burdens. 



And while the country’s mess may begin in the capital, free‐​spending provincial officials bear much of the blame as well. Operating expenses at the provincial level rose 25% from 1995 to 2000 even though inflation was nonexistent. The spending binge was financed by an unsustainable runup of external debt — the reckoning for which has now arrived. 



Meanwhile, as the public sector ballooned uncontrollably, vital government responsibilities went unfulfilled, among them the provision of a legal system that promptly and reliably vindicates the rights of the citizenry. As a result, the acute financial traumas that now beset Argentina are compounded by a business environment that is profoundly hostile to investment, dynamism, and growth. 



In San Miguel de Tucumán, the capital of Tucumán province, I spoke with Ignacio Colombres Garmendia, the head of a major law firm in town. “The legal system is absolutely vital for our region’s economic development,” he noted, “but the politicians are blind to it. It’s hard to see what doesn’t happen because of a bad legal climate, and so nobody knows about it. But every day I see deals collapse — I see potential investors who decide not to come to Tucumán — because of the legal risks. They call and ask me about this or that legal issue, and I have to tell them, and they say ‘Thank you very much’ and that’s the end of it. ‘The world is a big place,’ a client told me once, ‘and we don’t need Tucumán.’ ” 



It takes an average of five years to foreclose on a commercial mortgage in Tucumán. And given the punishingly high interest rates that prevail now in Argentina, delays like that can render even excellent collateral insufficient to cover the amount ultimately due. In a vicious circle, the risks caused by delay and uncertainty serve to drive interest rates up even higher. And, lo and behold, the net effect of a system that leaves investors and creditors so badly exposed is simple: less investment, less financing, and less growth and opportunity. 



It is fashionable now to blame Argentina’s problems on the free market. The country’s latest president, old‐​school Perónist and unabashed protectionist Eduardo Duhalde, has joined the anti‐​market chorus by vowing to break with the “failed economic model” of the past decade. But Argentina’s tragic crack‐​up occurred not because pro‐​market reforms went too far, but because they did not go nearly far enough.



A healthy market economy requires not just the absence of statist controls; it requires the presence of sound institutions. And although the reforms of the Menem era made strides toward meeting the former requirement, they ignored the latter altogether. Today Argentina is suffering grievously from that oversight. Until it is corrected and the country’s ramshackle political and legal systems are overhauled, there is little hope that a stable and prosperous Argentina can emerge from the wreckage. 
"
"
Share this...FacebookTwitterPortuguese website Ecotretas here informs us that the European carbon market, the EU Emissions Allowances, has crashed over 20% last week alone, see the following chart.
Carbon price per ton in euros over the last one year. Graphic source: http://www.eex.com
Read more about what is behind the crash at Ecotretas
In the scheme, carbon emission allowances, called EU Allowances (EUAs), are allocated to specific industrial sectors and cap the total level of emissions at levels which reduce over time. There are just over two billion allowances on issue, which are traded between emitters and other market participants on exchanges and via brokers.
The European Union started the market in carbon dioxide emissions in 2005.
The EU trading acheme applies to 7300 companies and 11,500 installations in sectors with high carbon dioxide emissions across the 27 nations of the EU. These include: energy utilities, oil refineries, iron and steel producers, the pulp and paper industry as well as producers of cement, glass, lime, brick and ceramics. Aviation. The scheme is regulated by the European Commission (EC).
First the science crumbles, then the costs explode, the illusions melt away, and finally the markets crash. But don’t expect Europe to change course. In socialist Europe it’s: “Let no economic suicide go unfinished!”
No wonder China doesn’t want any part of this scam, and so refuses to buy any Airbus planes from bossy Europe.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitter
Parts of southern Europe are experiencing drought conditions. So what does the European Union want to do about it? They want the wet European countries to use less water too.

Okay, it’s not a solution, but I guess it’s a way of showing solidarity with dry countries, or something. Call it Europe’s next folly.
The online Die Welt has a report here called “Billions For New Faucets“. Now that energy saving lights have been decreed by the EU Burgermeister-Meisterburger, now comes water-saving faucets. Die Welt writes:
In the coming years additional tens of billions of euros in extra costs may be levied on homeowners and tenants. The EU Commission wants to increase the efficiency of buildings with respect to water consumption by reducing it 30% using a new directive in member states. It is being considered to obligate homeowners and landlords to replace shower heads, faucets and toilets with new ones that have have considerably less consumption.”
This is for real. Got to hand it to the EU Commission – they really know how to come up with ways to harass and infuriate its citizens, and to interfere with their lives. For Americans and non-EU citizens, it will soon be coming to you too.
All of this is designed to benefit a few select companies, primarily manufacturers of high-end household fixtures who are having difficulty selling their high-priced wares due to the economic crisis in Europe and USA. This is going to be expensive for normal citizens. Die Welt writes:
Using a conservative figure of 400 euros per living unit, owners of the more than 25 million homes and apartments in Germany will have to fork out over 10 billion euros.”
If you do the math for all of Europe, you can estimate about €50 billion! And for what? For the luxury of having less water of course. Well, didn’t you know? Everyone dreams of having less water. Die Welt adds:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




With the planned regulation, Slovenian EU-Environment Kommissar Janez Potocnik wants to mainly fight the water shortage in southern Europe.”
Can someone tell me how using less water in rainy Britain is going to make things wetter in Romania? Die Welt takes a look at some water statistics in Germany and what a 30% reduction in consumption would mean.
‘Of the annually available 188 billion cubic meters of water in Germany, only 2.7 percent gets used by public water works,’ says Martin Weyland, head managing director of the Federal Association of Energy and Water Management (BdEW).”
Eventually, that 2.7% ends up going right back into the water cycle. And a 30% reduction would mean that Germany would use only 1.9% instead of 2.7% of the water it has available. The result: that little, meaningless dip in the statistics would cost €10 billion. Yes folks, the EU masterminds are indeed again at work.
Die Welt also writes that already many public wastewater utilities say their sewage systems are having problems because NOT ENOUGH water is being fed into them by households, and so the utilities themselves have to flush their sewers with fresh water from time to time. Weyland says:
This is the only way to prevent foul odors and damage to sewage lines from deposits.”
Die Welt also brings up that it is highly questionable whether replacing faucets, shower heads and toilets would lead to any savings at all. People will simply take longer showers, or flush several times. Moreover, wastewater utility companies would have to flush their lines out with fresh water more often.
Any other brilliant ideas Herr Kommissar?
Although the Die Welt piece has lots more interesting points, I don’t need to write on more about this; you all get the picture. Enough lunacy for today.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSteffen Hentrich at the  “Thinking For Freedom”, blog of the (Classic) Liberal Institute Friedrich Naumann Foundation For Freedom brings our attention to this Youtube Video by Ronald Bailey (Reason Magazine) und Julian Morris (International Policy Network) which features the Top 5 Environmental Disasters That Never Happened.

Enviro-leftists have a talent for concocting scare stories about mass environmental disaster, and thus stampeding the public into destrcutive policy-making that always ends up killing millions of poor people. Eventually, the scare story gets exposed as a hoax, and the media look like a bunch of thoughtless dupes.
You’d think these enviro-madmen would eventually learn something from their terrible mistakes. Well, you’d be dead wrong. In fact, they seem to get a kick out it, a ghoulish delight, and so they simply go on and concoct new schemes to stampede people into self-destruction.
The latest of course is catastrophic man-made climate change precipitated by the burning of fossil fuels. And now as man attempts to scale back use of fossil fuels, the poor once again are getting hammered the hardest.
I really believe too many of these enviro-crackpots are evil and diabolical, especially when you listen to how they talk about human population – referring to people as parasites, scourges – a disease to the planet. One could arguably call them “green genocidists”. Yes, I’m getting myself worked up. Here are Reason Magazine’s The top 5 diasters that never happened:
No. 5: Frankenfoods
Nonsense upon stilts.”
Think about golden rice, and all the children who have to hungry, or blind, or die prematurely because agricultural progress is irrationally impeded.
No. 4:  The end of biodiversity
70 to 80% of all animal species would be extinct by the year 1995.”
“Not based at all on evidence.”
Today, there are no signs of biodiversity shrinking.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




No. 3. The energy crisis
By the year 2000, if present trends continue, we will be using crude oil at such a rate that there won’t be any more crude oil left.”
“And of course what happened is that our government did all kinds of idiotic things under President Richard Nixon.”
Sound familiar? And who gets hit the hardest when energy prices go up? The poor of course. It ends up causing more hunger, disease, and ultimately premature death.
No. 2: “A Silent Spring” The Banning of DDT
There was a really apacolyptic vision that was frankly based on a flimsy and inaacurate representation of arginal science.”
“And those bans have led to some unpleasant and unintended consequences.”
“The result, well, now a million people a year are dying of malaria.”
No. 1: Malthusian famine
The battle to feed all of humanity is over. In the 1970s hundreds of millions of people are going to starve to death in spite of any crash programs embarked upon now.”
“Ehrlich was spectacularly wrong.”
“If we had followed his policies, we would have created the famines that he claimed were inevitable.”
“More people are being better fed today than at any time in human history.”
Wvery one of these problems was an iminent threat to life on the planet, promoted by “leading experts” who were certain, and insisted action had to be taken quickly. Many world leaders are now behaving like Nixon, and implementing idiotic policies.
The global warming scare is nothing new and will also end up in the graveyard of past scare stories as well. But in the meantime it indeed has the potential of “creating the famines that are claimed to be inevitable.” These professors like Hansen, Ehrlich, Schellnhuber, Mann, Jones, etc. are indeed dangerous and need to be reviewed by rational thinkers.
================================================
Here’s another example of a crackpot prediction: http://thegwpf.org/science-news/2861-un-predictions-a-the-eco-refugees-scare.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe German online Die Zeit here takes a look at the series of tornadoes that have ravaged the USA and conducted an interview with US meteorologist and Mississippi State University professor Grady Dixon.
Meteorology professor Grady Dixon: ""Terrible mistake"" to relate tornado up-tick to climate change. (Photo source: Mississippi State University)
Die Zeit asks the question: “Herr Dixon, is the number of such lethal storms rising in the USA?” Dixon replies:
No, to the contrary. Over the long term the number of deadly tornadoes has even dropped dramatically. […] However, we have to expect that more people will be hit by tornadoes in the future. Not because there are more storms, but because the population is growing and suburbs and cities are expanding. In any case, 2011 is an unusually violent tornado year and it is just a fluke.”
Dixon is also asked if climate change favors the creation of more tornadoes. Dixon answers:
Research results are mixed on this. […] But all indications show that it does not necessarily mean that tornadoes will be increasing in frequency.”
On the frequency of tornadoes, Dixon is also quoted by the English-language France 24 here:
‘It’s having to do with better (weather tracking) technology, more population, the fact that the population is better educated and more aware. So we’re seeing them more often,’ Dixon said.
But he said it would be ‘a terrible mistake’ to relate the up-tick to climate change.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




France 24 also quotes a FEMA official:
Craig Fugate, administrator of the Federal Emergency Management Agency (FEMA), also dismissed Thursday climate change as a factor in the deadly tornadoes: ‘Actually what we’re seeing is springtime,’ he said.
‘Many people think of Oklahoma as ‘Tornado Alley and forget that the southeast United States actually has a history of longer and more powerful tornadoes that stay on the ground longer’.”
Many weeks back I recall Joe Bastardi predicting a humdinger of a tornado season, and of course we now see that his warnings were spot on. This spike in tornadoes is not due to warmth, but to cooling brought on by La Nina, with cold northern air smashing into warm, moist southern air.
David Imy from the NOAA Storm Prediction Center in Norman, Oklahoma adds:
We knew it was going to be a big tornado year. But the key to that tip-off was unrelated to climate change: It is related to the natural fluctuations of the planet.”
A rare moment of sanity coming from the NOAA? Sorry Romm, but on this one you’re a lone fool (again) out in the desert.
========================================================
Unrelated: Benny Peiser brings or attention to news that a Global Climate Treaty Is DOA. Looks like Europe will be joining Romm out in the desert.
Share this...FacebookTwitter "
"

Thank you very much for this opportunity to testify on mandatory retirement age regulations in the United States. I am very honored by it.1



Ideally, the decision about when to retire should be made voluntarily by workers in response to labor market conditions. Mandatory retirement age rules have been eliminated in most private sector jobs as a result of anti‐​age‐​discrimination laws that were introduced beginning in the 1960s. Were they allowed, however, private sector employers would likely incorporate them in employment contracts designed for ensuring and improving worker‐​efficiency. Instead, private firms structure long‐​term incentive contracts including features of defined benefit pension plans, other non‐​wage benefits, and severance packages to induce early job terminations. Retirement incentives incorporated in such long‐​term incentive contracts appear to have spurred the trend toward earlier retirement in the United States.



Mandatory retirement age rules still prevail in some private and public‐​sector occupations: State and local police (55–60) and firefighters (55–60); federal firefighters (57); federal law enforcement and corrections officers (57); and air traffic controllers (56, if hired after 1972); and commercial airline pilots (60). These are “earlier‐​than‐​normal” retirement ages compared to the vast majority of other occupations.



Mandatory retirement age restrictions were introduced in these occupations several decades ago, primarily for ensuring their safe and effective conduct. Under today’s conditions, however, these retirement age rules appear to be outdated. The need to revise these rules appears urgent due to impending worker shortages. It appears desirable to introduce long‐​term incentive compensation structures in these jobs similar to those in the private sector. However, the manner in which they should be introduced and whether they would deliver retirement choices to workers while simultaneously ensuring safe and effective job performance requires further examination.



My testimony is comprised of several parts. First I report findings on private sector compensation structures that are designed to elicit worker efficiency and loyalty, and yet induce timely retirements. The findings suggest that these contracts involve divorcing current productivity from current compensation–by postponing compensation from workers’ early‐ to late‐​career stages. Such contracts would not be profitable if workers stayed on the job to collect wages in excess of their productivity for too long. Thus, it appears that private firms could put mandatory retirement age rules to good use.



However, these “incentive” contracts would become infeasible if employers terminated workers too early to avoid paying them seniority rents. This makes the case for anti‐​age discrimination rules. As explained below, it turns out that prohibitions against age discrimination are more useful than mandatory retirement age rules in making such contracts feasible. Evidence on U.S. firms suggests the prevalence of long‐​term incentive contracts. Thus, in the United States, firms induce workers to retire early by appropriately structuring non‐​wage elements of compensation, while anti‐​age discrimination laws provide an external commitment mechanism against premature discharges–thereby inducing workers to accept long‐​term incentive contracts.



Employment contracts are structured differently in the academic sphere. The recent removal (in 1994) of the age‐​70 mandatory retirement rule has resulted in many older and highly compensated faculty members remaining on university payrolls. Universities have not since been successful in inducing earlier retirements via pension plan and other incentives because these are costly to offer to faculty who are already close to retirement. However, universities may over time adopt long‐​term incentive contracts for younger employees similar to those prevalent in large private companies.



Second, health and longevity of the U.S population generally has been improving. This may mean that jobs that could not be conducted effectively by workers after their late fifties, now can be. A historical comparison of mortality rates suggests that those aged in their early sixties today are as healthy as were those in their mid fifties a few decades ago–when the mandatory retirement age rules were first imposed in the occupations under consideration.



Existing mandatory retirement age rules appear unfair for some categories of workers–such as pilots and air‐​traffic controllers that are subject to such rules. Because they spend their careers in jobs requiring specific, non‐​transferable skills, early job separation now results in longer spells of unemployment or forced retirement despite possessing the ability to conduct their jobs competently. Evidence suggests that independently of their tenure in earlier jobs older workers have greater difficulty in finding jobs in the private sector.



Third, improvements in technology imply that, other things equal, federal police, firefighting, and air‐​traffic‐​control jobs may have become physically easier to conduct. Evidence also suggests that health is now much less important as a consideration for the decision to retire than was the case several decades ago because of both, lower physical job demands and improvements in the treatment of chronic conditions.



Fourth, better technology and equipment increase the need for trained and experienced personnel to operate and coordinate activities. Hence, although better technology makes the jobs easier to perform, it could require more rather than fewer skilled workers.



Finally, as the baby boomers exit the workforce, the burden on working generations to support a larger and longer lived retiree population will increase. Baby boomer retirements will likely create skilled‐ and experienced‐​worker shortages in many occupations. The shortages are likely to become more acute in occupations that impose relatively early mandatory retirement age restrictions.



Overall, impending worker shortages in occupations with mandatory retirement age restrictions motivate the revision of these rules. However, as experience in the U.S. academic sphere indicates, addressing the shortages by immediately removing these restrictions may lead to other problems–such as a disproportionately older workforce. Introducing long‐​term incentive contracts to improve worker efficiency and yet provide flexibility in retirement decisions appears to be desirable. However, such employment contracts “pay off” only in the long‐​term and should be restricted to younger workers and new hires. Worker shortages in the near term could be addressed by revising mandatory retirement age rules upward. The decision to elimate these rules may never become necessary if younger worker and new hires are offered long‐​term incentive contracts. As these contracts become more widespread they may improve worker efficiency and induce timely retirements. Mandatory retirement age rules will automatically fall by the wayside as workers who remain subject to them leave the workforce.



 **Section 1: Mandatory Retirement Age Rules vs. Anti‐​Age‐​Discrimination Laws in the Private Sector**



Anti age discrimination laws and mandatory age retirement rules are polar opposites. Would private employers enforce mandatory retirement age rules in the absence of anti‐​age discrimination rules? The answer to this question _appears_ to be in the affirmative for those jobs and occupations requiring firm‐​specific skills–that is, full knowledge of company policies, operating rules, personnel, technology, on‐​going innovations and specific features of the work environment. These job requirements arise in managerial positions where staff must learn the nature of the business over several years. These requirements also apply in occupations requiring special on‐​the‐​job training–coordinating activities on a construction site, scheduling to run a factory work‐​shops etc. On‐​the‐​job acquisition of specific skills is also needed in varying degrees from a safety and job‐​effectiveness perspective, as is the case with air‐​traffic‐​controllers, pilots, law enforcement officers etc.



Whenever workers are required to possess “specific human capital,” it is in the employers’ interest to ensure that workers don’t quit immediately after acquiring those skills. However, because slavery is illegal, firms must induce workers to stay on the job by incorporating appropriate incentives in employment contracts, which may involve _implicit_ agreements on some elements. For example, workers may be paid less than their productivity during the early part of their tenure in exchange for (the implicit promise of) being paid more than their productivity during the later periods of their tenure with the firm. This implies the creation of “seniority rents.”



Several studies have found evidence consistent with the existence of long‐​term incentive contracting in the private sector. Overall, the evidence generally points against the “spot” market explanation of how compensation generally varys with age. In particular, the evidence suggests that compensation exceeds productivity at older ages providing a basis for employers to treat older workers differently.



Thus, in occupations requiring firm‐​specific skills, workers’ compensations may rarely, if ever, match their current productivity. Instead, the employers seek to match prospective productivity with prospective compensation over the workers entire tenure with the firm.



Apart from wages and salaries, compensation includes pensions, health insurance and other benefits. Employers generally use non‐​wage elements of compensation to design work and retirement incentives. Such incentives address multiple firm objectives: Ensuring worker bonding with the firm over long periods; ensuring that workers do not shirk on the job; inducing older workers to leave the firm at the “right” time, and so on.



Pension vesting and benefit accrual patterns of defined benefit pension plans can be designed to achieve all of these objectives. Vesting rules in DB pension plans generally require workers to be with the firm for 5 or 10 years before pension benefits begin to accrue. Pension accruals–the annual additions to the present value of pension benefits from additional years of work–are also designed to provide early retirement incentives.



Pension accrual patterns produce age‐​profiles of compensation that are initially steeper than workers’ age‐​productivity profiles. Pension accrual begins upon vesting and increases sharply at the early retirement age–usually age 55. The steep increase in pension accrual at age 55 arises because the eligibility to retire early and collect benefits immediately is associated with a smaller than fair reduction in benefits–compared to retiring at age 65 with full benefits. Moreover, delaying retirement beyond age 55 reduces pension accruals sharply‐​possibly making accruals negative.



These features of pension accurals create incentives for workers to retire early‐​keeping them from collecting seniority rents by staying on the job for too long. Firms can fine tune their compensation structures with other non‐​wage compensation elements, including severance packages. Thus, jobs involving the acquisition of significant firm‐​specific skills may exhibit productivity and compensation patterns such as those shown in Figure 1.





The productivity and compensation profiles of Figure 1 are estimated for male managers based on employment and compensation data from a Fortune 500 firm with over 300,000 employees. It shows several features:



Not all workers may be compensated under long‐​term incentive contracts. Routine office workers, support staff, sales agents, and so on appear to be compensated on a “spot” basis rather than under long‐​term incentive contracts. For example, we estimated annual productivity and compensation to be aligned more closely for salesmen in the above firm–as shown in Figure 2.





It should be noted that the adoption of long‐​term incentive contracts does not obviate the need to monitor worker performance. Indeed, the threat of being caught shirking and discharged is an integral part of the incentive structure.



Workers may also prefer contracts with rising compensation by age–if they can obtain them. One possible reason is their inability or lack of discipline to save — as is suggested by some psychological studies of saving behavior. They may prefer to receive lower compensation in the near term and higher compensation in the future as a forced saving mechanism.2



For long‐​term incentive contracts to be feasible, however, workers must also be convinced that employers will not arbitrarily discharge them as soon as they begin accruing seniority rents.



One conjecture is that firms’ incentives to “cheat” in this manner are reduced by the need to maintain their reputations–in order to continue hiring workers. However, purely reputational effects need to operate extremely strongly to effectively police against mid‐​career job terminations by employers. In addition, evidence suggests that early worker terminations can occur through other mechanisms — for example, after hostile takeovers of corporations. In one of my studies I find that post‐​hostile takeover managements discharge older workers, implying little or no negative reputational consequences.



The weakness of reputational effects in preventing premature worker terminations provides a possible rationale for anti age discrimination laws. A law prohibiting terminations purely on the basis of age can serve as external, and therefore more credible, commitment mechanism‐​an external check against the temptation to discharge workers prematurely–and makes long‐​term incentive contracts easier to implement.



How effective are long‐​term incentive contracts in inducing early retirements? Table 1 contains an answer based on data from the firm mentioned earlier. It shows retirement “hazard” rates–that is, the fraction of those employed at the beginning of the year that leave the firm within the year. The rates are shown by age and tenure with the firm.





The table suggests that because of the inducement to retire early (at age 55) provided through the pension accrual pattern, retirement rates step up to the 10–12 percent range between ages 55 and 59. Without the retirement incentives, they would remain at about the 3 percent level that prevails prior to age 55. Note that those aged 55–61 who are not yet fully vested in the pension plan (that is, those with less than 10 years of service) exhibit separation rates around 3 percent annually. Job separation rates at 10–12 percent per year rather than 3 percent per year can have substantial cumulative effects on overall labor force participation between the ages of 55–61. It is noteworthy that job separation rates increase even more dramatically at age of 62 and 65. These increases in retirement hazards probably occur as workers not subject to long‐​term incentive contracts respond to the retirement incentives provided by the Social Security program at these ages.



Prior to the 1980s, defined benefit (DB) plans covered two‐​thirds of workers and defined contribution (DC) plans covered about one‐​third. As is well known, defined benefit pension plan coverage has been declining and defined contribution plan coverage has been growing during the last two decades. Evidence shows that DB plans’ usefulness has declined for both employers and employees in an environment of rapid technological and structural changes in the economy. Under such conditions, the desirability of long‐​duration employer‐​employee matches has declined. Employers attempting to adapt to new technologies may require greater flexibility in workforce composition. Employees may prefer greater portability of pension assets if expected job‐​durations are shorter.3



However, the surge in DC pension plans since the early 1980s has not extinguished the use of DB plans: About one‐​third of the workforce continues to be covered under DB plans. Moreover, because early retirement incentives are not incorporated into DC plans, retirement rates among those in their early sixties have declined–a reversal in the trend established over several decades.



In the current context, offerring efficiency enhancing compensation structures to federal and state and local workers similar to those adopted in the private sector appears to be desirable–to the extent such incentive contracts are not offerred today. This recommendation is motivated by the need to elicit worker efficiency, and is independent of the fact that public operations are not driven by a profit maximizing motive. Moreover, such incentive compensation structures would provide greater retirement flexibility and help achieve employers’ objectives of safety and effectiveness in job performance.



However, any revision of public sector employment contracts along these lines would require a careful examination of whether such compensation structures are feasible, the manner in which they should be introduced, and how effective they could be as substitutes for the mandatory retirement age rules currently in force.



 **Section 2: Mandatory Retirement Ages, Occupational Development, and Personnel Abilities**



For the occupations under consideration, adopting a single mandatory retirement age is not necessarily better or cheaper than adopting flexible compensation‐​based incentives to retire.



A fixed retirement age potentially introduces two types of errors from the perspective of retaining qualified workers. First, those who are less qualified than others would remain on the work force because they are younger than the mandatory retirement age. Second, those who are better qualified than others are forced to retire because they are older. The mandatory retirement age could be set to minimize the sum of both types of errors. For example, if the mandatory retirement age were set at 40, we would force many qualified workers to retire prematurely. Similarly, if the retirement age were set at 80 many workers who are no longer competent would be retained. These errors would be minimized by setting the mandatory retirement age between these extremes.



The mandatory retirement age rules in the occupations under consideration were set several decades ago. Assuming that they were initially set optimally to minimize the sum of the two types of errors‐​they are probably obsolete today for a number of reasons.



 _Improving Health and Fitness_



The significant progress achieved in medical innovation and health care have increased the longevity of the U.S. population in general.4 People in their early sixties today enjoy similar health and lifestyles today with greater frequency as did those in their mid fifties several decades ago. One indication of the better health of today’s workers is the downward trend in mortality rates.



For example, mortality information from the Social Security Adminstration suggests that 55‐​year‐​old men in 1960 faced the same likelihood of dying within the year as do 62 year‐​old men today. And today’s 66 year old men have the same chance of dying as did 60 year old men in 1960. For women, mortality improvements are somewhat smaller. Women aged 55 in 1960 experienced the same average mortality as do 60 year old women today. And the same fraction of 60 years old women died in 1960 as do 64 year old women today.



Evidence that health among 50–60 year olds is improving can also be gleaned from surveys of self‐​reported health. Figures 3 and 4 show calculations based on the Panel Survey of Income Dynamics (PSID).5 The calculations reported below are based on weighting each household to convert the survey’s sample into a representative U.S. household population.



Figure 3 shows that by 1997, a sizable majority of men and women were in good or better health. Over a 14 year period between 1985 and 1997, the fraction of men aged 56 through 65 who reported being in good or better health increased by almost 5 percentage points to 71 percent. Figure 4 shows that for women, the share of those in good or better health increased by about 7 percentage points to 67.4 percent.







Although these data are based on self‐​reported health by survey respondents and spouses, a study (based on a different survery) shows that such responses are representative of the type of information used in professional evaluations of health and disability status.6



These data go back only through 1985. Projecting them further back in time would presumably reveal even more substantial gains in the health and fitness of those in their mid‐​fifties and early sixties. Indeed, other studies have indicated that health is now much less important as a consideration for the decision to retire than was the case several decades ago.7 This is because of both, lower physical job demands and improvments in the treatment of chronic conditions.



Finally, the data indicate sizable gains in longevity and health for the _general_ U.S. population. I do not have direct evidence of similar health gains for the subset of the population that forms the base for recruitment into the occupations under consideration. To the extent that such gains have occurred, revising mandatory retirement ages upward by a few years may be feasible.



 _Technology Induced Demand and Projected Worker Shortages_



The technology used in executing jobs in many of the occupations under consideration is much better today compared to 3 or 4 decades ago. The largest improvement has occurred in communications and information technology, and in all occupations; firemen have better heat resistant and fire‐​retardant materials; pilots have planes that are easier to fly an land; police officers have better investigative, forensic, and interdiction techiques, better body armour, DNA analysis, computerized laboratories etc. Not only does newer technology allow jobs to be executed faster and more efficiently, they can be executed with lesser exertion of effort.



They also call for a workforce with a wider range of skills–which implies that the availability of new technology is not necessarily labor‐​saving overall. It requires more training to operate and maintain newer equipment and requires more experienced personnel to coordinate job activities. On the other hand, as the baby‐​boomer generation retires, many jobs requiring trained and experienced workers will begin to experience shortages. Those jobs where retirements are mandatory at younger ages will experience acute shortages earlier.



Personnel shortages in key jobs that must be executed on time provoke the imposition of mandatory overtime. However, forced overtime over long periods imposes additional burdens and is likely to lower worker morale. A high‐​stress atmosphere is likely to induce additional accelerated retirements and make worker shortages even more acute. Hence, worker shortages in crucial occupations can be self‐​reinforcing unless dealt with in a timely manner. The prospect of increased shortages because of the impending surge in retirements along with an increasing demand for security and aviation efficieny in a new post‐​9/​11 world makes it necessary to revisit mandatory retirement age rules in the occupations that currently enforce them.



 **Conclusion**



Private sector enterprises get by without the imposition of mandatory retirement age rules: They successfully hire high skilled workers under long‐​term incentive contracts. Indeed, anti age‐​discrimination laws–the polar opposite of mandatory retirement age rules — appears more important for making such contracts feasible. An important element of private long‐​term incentive contracts are defined benefit pension plans and other non‐​wage compensation to achieve firms’ objectives of eliciting worker efficiency and ensuring timely retirements. Similar compensation arrangements could be usefully considered in the public sector as well, despite the lack of a profit‐​maximizing objective.



Mandatory retirement age rules in certain private, federal, and state and local occupations have been in place for several decades. Their revision appears worthy of consideration for several reasons.



The improvement in general health and abilities of those aged between 55 and 65 in general may imply that mandatory retirement at these ages is unfair for a growing number of workers who retain the ability to execute their jobs competently but cannot transfer their skills to other occupations–such as pilots and air‐​traffic controllers. Evidence suggests that older displaced workers find it much harder to find jobs compared to younger workers and suffer larger wage declines upon re‐​employment. 8



Better technology makes the conduct of these jobs physically less taxing. Moreover, newer technologies are likely to require a larger workforce with a broader set of skills to fill these positions. And, the onset of baby‐​boomer retirements is likely to create acute shortages of experienced personnel, especially in occupations with mandatory retirement set at younger ages.



Summarily eliminating mandatory retirement age rules to prepare for upcoming worker shortages may not, however, be the correct policy response. Doing so may create other problems as in the U.S. academic institutions where retirement rates have plummetted after mandatory retirement at age 70 was abrogated. That has led to slower turnover of teaching staff and aging faculties. Because long‐​term incentive contracts pay off only when introduced at the beginning of worker careers, implementing such contracts for older workers becomes expensive.



Hence, existing mandatory retirement age rules should be revised in two steps. To deal with impending shortages, existing mandatory retirement ages could be advanced by a few years. Long‐​term incentive contracting should be introduced for younger workers and new hires. The workforce subject to long‐​term contracting should be designed to both provide retirement choices to workers and satisfy employers’ objectives of work‐​safety and efficiency. The revised mandatory retirement age rules will be automatically phased out over time as the workers to which they apply retire over time.



 **Notes**



1. I am Jagadeesh Gokhale, Senior Fellow at the Cato Institute in Washington D.C. I have conducted studies on labor market contracting in the private sector and the effects of long‐​term employment contracts and worker tenure on the market for corporate control. I have also written on demographic and retirement issues relating to the sustainability of the federal budget.



2. R. H. Frank and R. M. Hutchens, 1993, _Economic Journal,_ Vol. 21.



3. See Friedberg and Owyang, National Bureau of Economic Research, Working Paper No. 10714.



4. Frank Lichtenberg “Sources of U.S. Longevity Increase: 1960–1997,” National Bureau of Economic Research, Working Paper No. 8755, February, 2002.



5. The PSID is conducted by theUniversity of Michigan’s Survey Research Center. This survey’s sample contained just over 10 thousand U.S. households in 1985 and it attrited to about 6,700 households by 1997.



6. Hugo Benitez‐​Silva, Moshe Buchinsky, Hiu Man Chan, Sofia Cheidvasser, and John Rust, National Bureau Economic Research Working Paper No. 7526.



7. Costa (1994), NBER Working Paper No. 4929.



8. See David Shapiro and Steven L. Sandell, 1985, _Southern Economic Journal,_ Vol. 52.
"
"**With much of the UK under lockdown, many shops, pubs and restaurants are shut, and large parts of the economy are effectively closed.**
The government has spent hundreds of billions on measures to support businesses and jobs, and fight the pandemic. But how will it pay for these?
We won't know how big the final bill will be until after the crisis is over. But the government will certainly have to borrow enormous amounts of money because it is spending more than it is taking in from tax.
On 25 November, the Office for Budget Responsibility (OBR), which keeps tabs on government spending, estimated that borrowing would be Â£394bn for the current financial year (April 2020 to April 2021).
That's the highest figure ever seen outside wartime.
To put that into context: before the crisis, the government was expecting to borrow about Â£55bn for the whole financial year.
This year the government is spending a staggering Â£280bn on measures to fight Covid-19 and its impact on the economy.
That includes Â£73bn for measures to support jobs such as the furlough scheme, where the government pays most of the wages of employees who cannot work.
The NHS and other public services have been given Â£127bn extra to combat the pandemic, and Â£66bn will be spent on grants and loans to support businesses.
The government will also raise Â£100bn less tax than it hoped because of the crisis. Unemployed or furloughed workers pay less income tax, businesses pay less tax if their profits are lower, and shoppers pay less VAT if they buy fewer things.
With more money going out and less coming in the government has only one option - to borrow.
Even if the pandemic ends quickly, there will still be higher costs and lower tax receipts in future years too, all of which means more borrowing.
**Will I have to pay more tax?**
Some economists argue that all the costs of the crisis could be covered by borrowing alone, but many disagree.
If the government wants to get borrowing down, it will have to cut spending or raise taxes, or most likely, both.
Raising taxes would be politically awkward, because the Conservative 2019 manifesto promised not to raise the three biggest taxes. These are income tax, national insurance and VAT - and together they bring in more than half of government revenue.
Increasing taxes means people have less money to spend, which could slow the economy down further. However, the respected Institute for Fiscal Studies think-tank has warned that tax rises of more than Â£40bn a year are ""all but inevitable"".
Cutting spending will also be difficult. There have been big cuts over the past decade, and many of the easy savings have already been made.
Some areas have long been protected, such as the NHS - and it would be difficult to reduce health spending after a big pandemic.
State pensions, another big spending item, are protected by a so-called ""triple lock"", which guarantees they rise with wages, prices, or 2.5% every year, whichever is highest. The manifesto promised to keep this, too.
In Wednesday's Spending Review the chancellor, Rishi Sunak, did make some cuts such as freezing pay for many public sector workers.
He also cut the amount the UK will spend on overseas aid, despite a manifesto pledge to keep it at 0.7% of national income.
The chancellor could say the pandemic also makes other manifesto promises impossible to keep. But difficult choices will have to be made.
If taxes go up, people will soon realise they have less money to spend.
Likewise, people will notice if lower public spending results in worse public services, such as longer waiting times in hospitals or fewer police on the streets.
Public sector workers whose wages are frozen will feel that impact very keenly.
And if pensions or benefits are cut, or even just increased less rapidly, that will be felt directly by anyone who depends on them.
At first the government will raise money by borrowing from investors.
They could be individuals, companies, pension funds, or foreign governments who lend money to the UK government by buying bonds.
A bond is a promise to pay the money back in the future, and pay interest on the loan in the meantime.
The Bank of England is buying huge amounts of bonds, to support the economy by encouraging more spending and investment, in a process called ""quantitative easing"".
This year the Bank is buying Â£450bn worth of bonds, which makes it much easier for the government to borrow money.
In recent years, the government has been able to borrow easily at very low interest rates, which makes its debt more affordable.
At the moment it pays just 0.32% interest to borrow for ten years.
There is a limit to how much the government can borrow, before interest payments become so great it can't afford them. No-one knows quite where that limit is.
But those interest payments will still weigh on future generations until the debt is paid off, and will mean there is less money available to spend on public services, or tax cuts."
nan
"**Here are five things you need to know about the coronavirus pandemic this Tuesday morning. We'll have another update for you at 18:00 GMT.**
Travellers who arrive in England from high-risk countries will soon be able to reduce their quarantine period by more than half if they pay for a Covid test after five days - and of course, if that test is negative. The new system will begin on 15 December and will cost between Â£65 and Â£120 per person. The travel industry welcomed the policy but described it as long overdue. The transport secretary said it would allow people to see loved ones and give a boost to business. Read more on the current quarantine rules.
We're expecting restrictions to be eased over Christmas, but discussions between ministers in the four nations are continuing over very practical concerns - the length of any relaxation, the impact on public transport, and what exactly constitutes a ""household"". Overall, the tone has been cautious - the PM called it the season to be ""jolly careful"". On Monday, he announced the new ""toughened"" tier system for England \- the BBC's Laura Kuenssberg says the real political test will come on Thursday when it becomes clear how many regions are in the highest level.
Every household in Northern Ireland is to be given a voucher worth about Â£200 to spend on the High Street, as part of the devolved government's economic support package. The country re-enters a strict lockdown for two weeks from Friday. Other measures include money to help older and disabled people with their heating bills, and cash for drink-only pubs. Read more on the new restrictions coming for Northern Ireland. Meanwhile, concerns have been raised about rising rates in certain parts of England, especially Kent, despite the ongoing lockdown.
Tomorrow, Chancellor Rishi Sunak begins setting out plans for the economy beyond Covid-19 - although such is the uncertainty that his lookahead, or Spending Review, has been limited to the next 12 months, rather than the usual three or four years. The economic shock of the pandemic has left the UK a poorer country, so what might Mr Sunak do? Our BBC Business colleagues take a look. The overseas aid budget certainly seems on course for a cut - find out more.
One thing you can start planning for Christmas is your TV viewing, and this morning the BBC has revealed its festive schedule. Staples such Doctor Who, Call the Midwife and Mrs Brown's Boys are all on the list, along with some new entries, including dramas Black Narcissus and The Serpent. BBC chief content officer Charlotte Moore said it had been ""a real struggle"" to make the usual range of festive shows during the pandemic, but the stars and crews ""pulled out all the stops"".
Get a longer news briefing from the BBC in your inbox, each weekday morning, by signing up here.
Find more information, advice and guides on our coronavirus page.
Plus, we now know four Covid vaccines have shown very promising results in final-stage trials, so how and when might a programme of jabs begin? Our health reporter Philippa Roxby explains.
**What questions do you have about coronavirus?**
_ **In some cases, your question will be published, displaying your name, age and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read our**_terms & conditions _ **and**_privacy policy.
Use this form to ask your question:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or send them via email to YourQuestions@bbc.co.uk. Please include your name, age and location with any question you send in."
"

In last week’s budget standoff, the headlines had President Obama repeatedly “summoning” the speaker and the Senate majority leader to the White House. “Why,” my colleague David Boaz asked, “doesn’t the speaker ‘summon’ the president to what I think is the real seat of government?”



It’s a good point: Whether or not you score the budget deal as a win for the GOP, the casual way the media describes the president’s role shows how dangerously far we’ve drifted toward one‐​branch rule.



Congressmen of old “would have received as a personal affront” any message from the president calling on them to change their position, Massachusetts Sen. George Hoar wrote in his 1903 memoirs: “If they visited the White House, it was to give, not to receive advice.” “In a republican government,” the Federalist explains, “the legislative authority necessarily predominates,” and is therefore most to be feared.





In the shell game of modern American governance, we’ve let ourselves become easy marks.



Today, not so much. Consider the controversial “policy riders” that almost sank the budget deal. The measure defunding Planned Parenthood got most of the coverage, overshadowing important provisions aimed at restricting the Environmental Protection Agency’s power to regulate greenhouse gases.



Seizing on the Supreme Court’s 2007 ruling that the Clean Air Act’s definition of “pollutant” was broad enough to encompass CO2 — a gas essential to life on Earth — the Obama administration has begun to “legislate” global warming policy in an end‐​run around Congress.



Whatever your views on climate change, you ought to find it unsettling that, here and elsewhere, most of the actual “law” in this country is crafted by unelected executive‐​branch bureaucrats.



But that’s where we are. A few months back, the _New York Times_ reported that some 230 health regulators had descended on Bethesda — paying double rent for office space so they could immediately begin drafting more than 300 rules implementing Obamacare. Thanks to “mega‐​bills passed by Congress,” the _Times_ explained, regulators are issuing “hundreds of sweeping financial and health care regulations that will ultimately affect most Americans.”



As at home, so too abroad: having ceded its constitutional power, Congress sits on the sidelines and carps while the president wages war.



Two weeks ago — nine days after we started bombing Libya — President Obama got around to explaining why. His televised address ran more than 3,000 words, but “Constitution” never appears — and “Congress” occurs only once: a passing reference to “consulting the bipartisan leadership of Congress.”



The president’s supposed to do more than “consult”; the real “decider” is Congress. Our Constitution grants the legislature sufficient power to make talk of “co‐​equal branches” a misnomer.



The constitutional scholar Charles Black once commented, “My classes think I am trying to be funny when I say that, by simple majorities,” Congress could shrink the White House staff to one secretary, and that, with a two‐​thirds vote, “Congress could put the White House up at auction.” (I sometimes find myself wishing they would.)



But Professor Black wasn’t trying to be funny: it’s in Congress’ power to do that. And if Congress can sell the White House, surely it can defund an illegal war and rein in a runaway bureaucracy.



If they don’t, it’s because they like the current system. And why wouldn’t they? It lets them take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the bureaucracy when it imposes costs in the course of deciding what those statutes mean.



But it’s our fault as well. In the shell game of modern American governance, we’ve let ourselves become easy marks. Unless and until voters wise up and demand accountability, Congress will continue to take our money and shirk its duty.
"
"
Share this...FacebookTwitterDie Welt, one of Germany’s flagship dailies, wrote up a comprehensive assessment of solar energy as a supply of energy to meet demand titled: The Great Solar Swindle.
The authors of the piece, Daniel Wetzel and Reto Klar, as suggested by the title, conclude that solar energy is a well-executed swindle that was and is promoted by slick industry lobbyists. Today solar panels and systems, massively subsidized by the government, are enjoying a boom in Germany. Solar energy is the number one desired form of renewable energy by Germans, according to a recent survey. The industry, needless to say, is making money hand over fist.
And because the solar energy boom is riding a tsunami of political populism, leaders are afraid of a political backlash should they try to put the brakes on the runaway gravy train.
Indeed behind all the solar energy brightness, the very dark clouds of economic and technical reality are gathering quickly. Despite all the billions in investment, solar energy in Germany today still only contributes a measly 3% of the country’s energy needs. It is many times more expensive than the conventional coal or nuclear power, and thus is causing rates for consumers to rise rapidly. And as Die Welt writes, it is not creating thousands of green jobs, except in Asia that is, where 70% of the solar modules now installed in Germany are made.
Expert council advises the German government to scale back solar installations
One big problem is that solar energy does not work well in often gray and overcast Germany, and so it makes little economic sense to add more solar capacity. Die Welt writes that an…
Expert council for environmental issues, a high level advisory commission for the federal government, recommends no longer forcing the expansion of photovoltaic, but rather to restrict them to very tight limits. Flensburg environment scientist Olav Hohmeyer, a member of the expert council, requests that the current rate of solar expansion be scaled back by at least 85% to only 500 to 1000 megawatts annually.”
Clearly the government is beginning to see that the power supply is becoming vulnerable and is at risk. When the power supply is at risk, then so are the consumers and industry who need a steady supply that can be relied on. Costs and feasibility are now under hefty criticism. Die Welt writes:
The Rhine Westphalia Institute for Economic Research (RWI) says that because of the ‘hype surrounding photovoltaic’, a growing cost tsunami will hit Germany.
and later quotes Thomas Bareiß, energy policy coordinator of Merkel’s CDU/CSU faction:
What is taking place here makes no economic sense and is socio-politically irresponsible.”
The authors list 10 reasons why solar energy needs to be re-evaluated:
1. It is not cheap.
Just for the modules built by the end of 2010, the German consumer will be saddled with pure subsidy costs, or so-called ‘solar debt’, to the tune of € 81.5 billion, which will have to be paid over a period of 20 years.
According to calculations by the RWI, German consumers will incur another €42 billion in costs by 2020.”
Add necessary grid expansion to the cost calculation and costs explode conservatively beyond $200 billion!
2. Nuclear power is not expensive
Cheap energies like coal and nuclear are being  forced out of the market and replaced by expensive alternative energies. The cost of the additional infrastructure needed by wind and solar will make nuclear and coal power look like a real bargain in the long run. And then consider plans by the EU to go ahead with Desertec, which is estimated today at €400 billion, and whose price tag will surely skyrocket over time. Worse, it will be located in the Sahara, i.e. in countries that are hardly stable.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




3. There’s no demand for it.
“The solar lobby wants to install 70 gigawatts by 2020.” The problem is that there will be little power generation in the wintertime, and all the surplus generation in the summer time will have to be given away to foreign countries.
4. Energy independence is a fairy tale
At Germany’s latitudes, peak production capacity of solar modules are only possible 875 hours of the 8760 hours in a year. The rest of the time is night, bad weather or winter. That means solar panel owners are forced to draw power from the rest of the grid 9 of every 10 hours, like everyone else has to.”
5. Local communities are being weakened
Energy independence and local production is supposed to strengthen the community, but the opposite is true. For example communities with hydro-power do not need solar or wind power, and so cannot accept it.”
But now public utilities have to invest millions of euros in additional equipment just so that a few dozen people can earn a few thousand euros with solar power.
6. The problem of storing power
Solar panels often produce energy when you don’t need it, or produce nothing when you need it the most. If only there was a cheap way to store the energy. Boston Consulting Group (BCG) says Germany’s current storage capacity using pump reservoirs is 7 gigawatts only.
To be able to supply enough energy for one week without wind or solar and without conventional power plants, 1260 pump-reservoir power plants of the type used in Goldisthal would be needed. ‘Ignoring that Germany has no suitable locations for such facilities’, say Bode and Groscurth and write ‘the costs per kilowatt hour would be astronomical because most of these systems are operated only for a few hours each year’.”
7. Hightech industry – but not in Germany
Global market share of German-made solar cells dropped last year alone from 15.4 to 9.7%. During the same short time period, China’s global market share climbed 25% to 48% overall. Promoted by cheap credits from the Chinese state banks, Chinese manufacturers such as Yingli, Suntech or JA Solar are snapping up complete production lines that use the latest western technology.”
8. Job engine solar industry is a myth
Even using the rosy figures from the German solar industry, the number of jobs by 2020 will not only not grow, but will even shrink.”
9. The solar boom has been a big success – for a few lobbyists.
The rest will have to pay through the nose.
10. Contribution to climate protection is insignificant, and costly.
Photovoltaic is the most expensive way of climate protection. Scientists of the International Energy Agency (IEA) or the RWI have calculated how much it costs to prevent one ton of greenhouse gas CO2 from be emitted from a fossil fuel plant by other energy sources. The result: The CO2 prevention cost for photovoltaic was a record high of €648 per ton.”
Share this...FacebookTwitter "
"Curious Kids is a series by The Conversation, which gives children of all ages the chance to have their questions about the world answered by experts. All questions are welcome: you or an adult can send them – along with your name, age and town or city where you live – to curiouskids@theconversation.com. We won’t be able to answer every question, but we’ll do our best. How do creatures living in the deep sea stay alive with the pressure? – Torben, age eight, Sussex, UK. Hi Torben,  This is a great question – thank you so much for asking it. The deep sea is a very difficult place to live. There is no light, it’s cold, there’s not much oxygen and little food – and, as you rightly point out, the creatures that live there have to deal with the enormous pressure of the water above.  In the deepest part of the Atlantic, the pressure can be 840 bars – that’s about 840 times the pressure we experience at sea level. At Challenger Deep in the Mariana Trench – the very deepest part of all the world’s oceans – the pressure may be 1,000 bars or more.  But the creatures that live in the deepest parts of the ocean have special features, which help them deal with these tough conditions – including the crushing pressure.  When you dive to the bottom of a deep swimming pool, you might start getting a painful or unpleasant feeling in your ears and sinuses. This is because they contain air: that feeling comes from the air sacs in your body being squashed by the pressure of the water.  Fish living closer to the surface of the ocean may have a swim bladder – that’s a large organ with air in it, which helps them float up or sink down in the water. Deep sea fish don’t have these air sacs in their bodies, which means they don’t get crushed.  The deepest dwelling species of fish, called the hadal snailfish, can be found at depths of about 8,200m.  But having a body with no air cavities will only get you so far, since high pressure can also destroy the very structure of molecules – the tiny building blocks that make up all matter.  


      Read more:
      Curious Kids: is everything really made of molecules?


 To help with this, deep sea creatures have “piezolytes” – small, organic molecules which have only recently been discovered. These piezolytes stop the other molecules in the creatures’ bodies, such as membranes and proteins, from being crushed by the pressure (though we’re not exactly sure how, yet).  Another interesting thing about piezolytes is that they give fish their “fishy” smell. Shallow water species have piezolytes too, but deep sea species have many more – so deep water species would smell much more fishy.  This molecule is only effective up to certain depths though; as the water gets deeper, the pressure becomes too much, even for snailfish. Tiny organisms called microbes have been recovered from the very bottom of the Mariana Trench, and they have peizolytes to help protect them, too.   While some animals live full time in the deep sea, others just visit. Species such as Cuvier’s beaked whale commute between the surface of the water, to breathe, and depths of over 2,000m, to feed.  These whales breathe air, but their lungs are collapsible, so they don’t get crushed when the whales dive into the deep sea for almost two hours at a time.  When diving, these whales store the oxygen from the air they breathe in their blood and muscles. They can do this because they have higher levels of haemoglobin and myoglobin molecules – which are used to store oxygen – than other whale species. Cuvier’s beaked whales can also reduce their heart rate and temporarily stop the blood flowing to certain parts of the body, which helps the oxygen to last longer.  So, there are a few different ways creatures can survive in the deep sea, depending on whether they are just visiting, or live there all the time.  There’s one last thing to think about: it’s very difficult for scientists to study deep sea animals, as they tend to die when they are brought to the surface – so there might be many other remarkable features we don’t yet know about.   More Curious Kids articles, written by academic experts: How does heat travel through space if space is a vacuum? – Katerina, age ten, Norwich, UK. What makes a shooting star fall? - Katelyn, age seven, Adelaide, Australia. What causes the northern lights? – Ffion, age 6.75, Pembrokeshire, UK."
"
Share this...FacebookTwitterHistoric October Snowstorm Still Crushing New England
A historic October snowstorm is still crushing New England with heavy snow and howling winds before cruising away into Atlantic Canada.
Snow amounts have already topped two feet across portions of New England, while record-shattering snow hammered the major Northeast cities from Washington, D.C., and Philadelphia to New York City and Hartford, Ct…Keep reading…
Share this...FacebookTwitter "
"**The number of patients treated within 18 weeks of referral has fallen by more than half compared with the same time last year, figures show.**
Public Health Scotland (PHS) data from September shows fewer than 30,000 patients were seen within 18 weeks, down from about 73,000 a year ago.
However, the figure has been growing since July.
The Scottish government said NHS services were now being resumed with a focus on those who needed urgent care.
Health boards were asked to suspend all non-urgent elective treatment in mid-March as the NHS was put on an ""emergency footing"" to cope with the pandemic.
It has led to a backlog of patients awaiting treatment in Scotland.
The latest PHS figures cover the three months until the end of September 2020. The statistics cover all health boards apart from NHS Borders, NHS Grampian and NHS Lothian, which have been unable to provide figures.
The figures show that the number of people waiting more than six weeks for key tests to diagnose conditions including cancer is three times higher than the same time last year, rising from 15,509 to 47,968.
However, this figure has also been falling from a high of 66,726 at the end of May this year.
Scottish Labour said the waiting times for conditions such as cancer were ""unacceptable"" and risked a ""tidal wave of future health problems"".
The party's health and social care spokeswoman, Monica Lennon, said: ""SNP ministers have been warned for months to bring in routine testing for all healthcare workers and put measures in place to maintain Covid-safe spaces in our hospitals.
""Scotland is facing a tidal wave of health problems and the SNP government must urgently start giving our NHS the support it needs in order to prevent unnecessary deaths and chronic illness.""
The Scottish government said its ""guiding principle"" during the pandemic had been to keep as many people safe as possible.
A spokesperson said: ""Services are now being resumed, with a focus on patients who need urgent care, including cancer treatment, to ensure they are treated as quickly and safely as possible.
""To support this, the health secretary published a Clinical Prioritisation Framework last week to help boards prioritise care for those patients with the greatest need.""
More than 4,500 outpatients have now been seen at NHS Louisa Jordan, with a wider range of non-Covid healthcare being considered at the temporary hospital in Glasgow, the spokesperson added."
"
Michael Ronayne writes:
To the right of the burned out pixel, a second Sunspot group, with two spots, is forming which can be seen in this image:

The burned out pixel between the two groups is a fairly common issue with SOHO, and they routinely “bake” the sensor to get rid of them. Sometimes people mistakenly interpret them as sunspots in this new age of counting sunspecks.
The way to determine if it is a burned out pixel or not is to look for other off-colr pixels immediately arround it. If the pixel stands by itself, it is a burned out pixel.
So far these have not been assigned a number. They are just barely what one would call sunspots and my bet is that much as we’ve seen before from SC24 specks, they will be short lived, probably 48 hours or less.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95926819',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"

Yesterday, 215 scientists released a petition in Bali — site of a global confab to talk about whether and how we should talk about a future treaty to reduce greenhouse gas emissions — that “begs” the world to cut greenhouse gas emissions in half by 2050. I was quoted in an AP story on the matter to the effect that scientists are in no position to intelligently dictate such a policy. And as expected, some in the blogosphere howled.   
  
  
I do not believe in leaving public policy to “guys in white coats” — in any discipline. And that’s not necessarily a proposition that vitiates against environmentally‐​friendly public policy. Climate scientists do not have the training to tell us whether the costs associated with reducing greenhouse gas emissions are less than, equal to, or greater than the costs of business as usual. And that’s something you would want to know before signing off on greenhouse gas emission reductions. When climate economists have explored that matter, they find little to support such emission reductions even if we accept the prognostications about the future coming out of the IPCC.   
  
  
Likewise, economic calculations about the same are heavily predicated on how you feel about future costs and benefits. If you believe in valuing dollars and lives in, say, 2150 as much as you value dollars and lives today, then it’s hard to accept IPCC reports and not conclude that GHG emission cuts pass a cost‐​benefit test. If you apply a discount rate of, say, 3, 5, or 7 percent, then it’s hard to accept IPCC reports and not conclude that GHG emission cuts don’t pass a cost‐​benefit test. But how you value the future is subjective, and economists have no objectively “better” preference regarding that matter than you or I.   
  
  
Many have argued that we should value our great grandchildren’s lives and money as much as we value our own. Fine — there is nothing objectively wrong with that belief. But if you do, hand in your Rawlsian membership card. That’s because you’re endorsing a policy that will transfer wealth and well‐​being from the relatively poor (us) to the very rich (them). That is, even if the Stern Review is correct about the economic costs of climate change, real per capita income in developing countries will be higher than that of the developed world today by 2100. Moreover, if you value the future every bit as much as you value the present — and thus embrace, say, a 0.1% discount rate — then simple math suggests you ought to be saving just about everything you earn.   
  
  
I do not believe that “the experts” in any field should be dictating climate policy because there are plenty of important value judgments built in to those policies and experts however defined have no objectively better values than you or I. I do believe, however, that any serious reflection on the ethics of reducing greenhouse gas emission will find that the case for such a policy is harder to make than you might think, even if you accept what the IPCC is telling us.
"
"

This week President Barack Obama will open the doors to Hu Jintao, President of China. The visit will include an arrival ceremony, a joint press conference, and a glitzy state dinner. In addition to highlighting the importance of the relationship between the world’s two largest economic powers, which have become so central to growth of the global economy and the stability of the international system, President Hu’s meetings with President Obama will provide an opportunity to press the reset button on Sino‐​American ties.



The relationship has been strained since President Obama’s visit to China in November 2009: the tensions during the UN Climate Change Conference in Copenhagen, and in the aftermath of Obama’s meeting with the Dalai Lama, his decision to authorize $6.4 billion in military sales to Taiwan. Then there have been continuing American complaints over allegations that China was manipulating its currency, Chinese concerns over U.S. naval and air military exercises with South Korean forces in the Yellow Sea, and American opposition to China’s newly assertive claims to disputed waters in the East and South China Seas.



The meeting also comes as the two nations are recovering from the global economic crisis and reassessing their geo‐​strategic interests in East Asia and elsewhere. In Washington, Beijing, and other world capitals, there is a recognition that China, with its huge surpluses, and the U.S., with its gigantic deficits, need to cooperate in order to rebalance the global financial system and avoid another meltdown. At the same time, traditional U.S. allies in East Asia have been expressing support for continuing American military presence as a way of counter‐​balancing China’s growing assertiveness.



So much has been said and written about the evolving strategic and economic ties between China and the U.S., with pundits drawing a variety of historical analogies to apply to what has been described the “world’s most important relationship.” It could determine whether the first part of the twenty‐​first century will see the continuing expansion of the economic globalization and international peace, or whether Beijing and Washington are doomed to relive the Cold War that had existed between the U.S. and the former Soviet Union – or worse, that the economic and military tensions between two global powers will resemble those between Germany and Britain on the eve of World War I: a rising power (Germany then; China now) that is perceived to be challenging the existing pro‐​status quo power (Britain then; the U.S. now).



No one expects one visit to help provide clear answers to these and similar questions, and it is quite possible that – contrary to the conventional wisdom, preoccupied with the notion that an insecure “declining” America and an overconfident “emerging” China are bound to come to blows in the near future – neither Washington nor Beijing are expecting dramatic changes in their relationship anytime soon. In fact, Americans are trying to recover from the Great Recession and to end military quagmires in the broader Middle East, and the Chinese are managing the consequences of their dramatic and yet risky economic growth; both sides may be interested in steadying their relationship instead of rocking the boat.



The public statements made by Presidents Obama and Hu suggest that the two are hoping to stabilize the relationship and take steps to avert potential crises over issues like China’s overvalued currency or Taiwan. But on both sides there are political and bureaucratic forces that do want to pick a fight with the other side. Chinese nationalists believe that the U.S. is trying to contain their country’s rise to economic and military power by pressing Beijing to re‐​value the yuan and to resolve their territorial conflicts with their neighbors.



Mirror imaging these fears are the suspicions of those Americans who believe that China is using a mercantilist strategy aimed destroying the American economy while building up their military as part of an effort to get U.S. forces out of Asia. And without a clear sign that the leaders in Beijing and Washington are in control in managing the bilateral relationship, these mutual fears could force both sides into a widening insecurity trap and ignite more confrontation in the future.



This danger was on display this week on the day when – just as U.S. Defense Secretary Robert Gates was meeting with Chinese President Hu in Beijing – China’s new J-20 stealth fighter took what was believed to be its maiden test flight. Reports suggested that that was an attempt by the Chinese military leaders to demonstrate Chinese might to Gates, displeasing President Hu and his political leaders, who were interested in patching things up with the U.S. and avoiding any tensions before Hu’s State visit to Washington.



Similarly, while President Obama and his top aides have resisted pressure from protectionists and China bashers on Capitol Hill to punish the Chinese for refusing to revalue their currency, there is little doubt that if the U.S. unemployment rate remains high and the American economic recovery slows down, Congress will end up embracing a more confrontational approach towards China and demand that the administration impose economic sanctions on it. And that could result in an all‐​out trade war between the two countries. But the last thing that the Obama Administration officials need now as the U.S. economic recovery is gaining some momentum is new economic tensions with Beijing.



It is unlikely that the U.S. and Chinese leaders will be able to come up with detailed plans to rebalance the financial flows between the two economies. That will require politicians on both sides to make difficult decisions; they must change current patterns of spending and savings, making it possible to slash U.S. debt and increase Chinese domestic consumption. More likely, the visit will focus on Chinese willingness to remove existing barriers to American exports and provide stronger protection for intellectual property rights.



The U.S. and China will likely provide more specific details on how they plan to implement the 2009 Joint Statement issued by the two sides during Obama’s visit to China, which committed the two governments to expand bilaterally in areas such as science and technology, clean energy, civil aviation, agriculture, public health, space science, and cultural and educational exchanges. Some progress in these areas was made in the Joint Commission on Commerce and Trade last year.



Each leader needs to come out of the summit able to demonstrate to an anxious public that the relationship between the two countries is advancing each nation’s economic interests – helping to accelerate U.S. economic recovery and create more American jobs while providing more momentum for China’s economic growth – and showing that the relationship does not amount to a zero‐​sum‐​game. That will most effectively diminish the influence of both the China‐​bashing crowd in Washington and the anti‐​American nationalists in Beijing.
"
"
Earlier I wrote about the Arctic Oscillation Index going strongly negative in December and what new cold to expect in January. From NASA’s Earth observatory, we have a high resolution temperature anomaly map that provides visualization of the effects. This image was taken while the Copenhagen Climate Conference was in progress.


Deadly Cold Across Europe and Russia



Click image above to enlarge or download large image (3 MB, JPEG) 		acquired December 11 –  18, 2009



It will be interesting to see how the NASA imagery compares with the anomaly maps of GISS and HadCRUT for December when they are made available. More images are available at links below.
A wave of frigid air spilled down over Europe and Russia from the Arctic in mid-December, creating a deadly cold snap. According to BBC.com, at least 90 people had died in Europe, including 79 people, mostly homeless, in Poland. In places, the bitter cold was accompanied by heavy snow, which halted rail and air traffic for several days during the week of Christmas.
This image shows the impact of the cold snap on land surface temperatures across the region from December 11–18, 2009, compared to the 2000–2008 average. The measurements were made by the Moderate Resolution Imaging Spectroradiometer (MODIS) on NASA’s Terra satellite. Places where temperatures were up to 20 degrees Celsius below average are blue, locations where temperatures were average are cream-colored, and places where temperatures were above average are red. Light gray patches show where clouds were so persistent during the week that MODIS could not make measurements of the land surface temperature. The biggest anomalies were in northern Russia, but a swath of below-average temperatures stretched across the countries around the Baltic Sea as well.
See also:

Daily, 8-day, and monthly land surface temperature anomaly maps
Animation of monthly global land surface temperature anomalies

h/t to WUWT reader “JT”



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e900dc066',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
\---   
  
Poof, it was gone.   
  
Just like that, the human fingerprints on a century-long warming trend in Northwestern United States were erased and replaced instead by the telltale signs of natural variability.   
  
That is the conclusion of new research published last week by a pair of scientists from the University of Washington. James Johnstone and Nathan Mantua published their paper titled “Atmospheric controls on northeast Pacific temperature variability and change 1900-2012” in the _Proceeding of the National Academy of Sciences_ (PNAS).   
  
So as not to be accused of putting words in their mouth, here, in full, are the contents of a box labeled “Significance” from their paper:   




Northeast Pacific coastal warming since 1900 is often ascribed to anthropogenic greenhouse forcing, whereas multidecadal temperature changes are widely interpreted in the framework of the Pacific Decadal Oscillation (PDO), which responds to regional atmospheric dynamics. This study uses several independent data sources to demonstrate that century-long warming around the northeast Pacific margins, like multidecadal variability, can be primarily attributed to changes in atmospheric circulation. It presents a significant reinterpretation of the region’s recent climate change origins, showing that atmospheric conditions have changed substantially over the last century, that these changes are not likely related to historical anthropogenic and natural radiative forcing, and that dynamical mechanisms of interannual and multidecadal temperature variability can also apply to observed century-long trends.



  
  
Translation: Natural variability in the atmosphere/ocean dynamics of the northern Pacific Ocean rather than human-caused global warming can largely explain the century-long rise in temperature in the Pacific Northwest.   
  
And the authors have the figures to prove it.



The left-hand panel of Figure 1 (below) illustrates the observed trends over the period 1900 _–_ 2012 for weather observing stations in the contiguous United States located west of longitude 116°W—this includes all of Washington and Oregon, most of California, and parts of Idaho and Nevada. Notice all the red, upward-pointing arrows indicating an overall temperature rise since 1900 across the entire region. The right-hand panel shows the trend in the same stations once the natural variability identified by Johnstone and Mantua has been removed. If global warming were having an influence, it would be evident in both panels—more strongly so in the right-hand one. Instead, in that panel, we have a mixed bag of up and down trends of very low magnitude—no strong signal of any kind, no sign of global warming.   






  
  
_Figure 1. (left)_ _Northeast Pacific annual temperature trends, 1901_ _–_ _2012\. Map of trends (°C/century) of annual (July_ _–_ _June) mean surface air temperature for observing stations west of 116°W. Triangles mark statistically significant (_ _p_ _< __0.05) trends (red, upward pointing: positive; blue, downward pointing: negative). Gray dots mark locations of insignificant trends. (Right) Map of residual surface air temperature trends after removal of natural variability temperature signals (adapted from Johnstone and Mantua, 2014)._   
  
Now, we’ll be among the first to admit that _PNAS_ does not have the reputation of publishing the most robust of studies, but, still, this result is intriguing. We hope that it will be verified by publication in a more rigorous peer-reviewed journal in the near future.   
  
In the meantime, we can only wonder what the authors of the recent U.S. National Climate Assessment (NCA) are thinking. After all, they have an entire chapter of their new report (a report basically crafted to support President Obama’s Climate Action Plan) dedicated to present and future climate change in the Northwest. While the NCA authors coyly admit that the region’s “climate trends include contributions from both human influences (chiefly heat-trapping gas emissions) and natural climate variability” they are quick to add “[t]hey are also consistent with expected changes due to human activities.”   
  
Hmm. While the observed trends may be “consistent with” the NCA authors’ “expected changes due to human activities,” in actuality they are, in fact, _not_ caused by human activities.   
  
This is a good lesson that “consistent with” does not equate with “a result of.” Which means that all the other regional changes in the Northwest noted in the NCA—wildfires, insect outbreaks, changes in the timing of stream flow, etc.—are also largely a result of influences other than human-caused climate change. And, it means that all the projections of future climate changes forwarded by the NCA are also nonrobust.   
  
Somehow, we doubt a correction will be forthcoming.   
  
After all, the NCA and the president are not particularly interested in what the science actually says about climate change and its causes and effects, but rather how it can be molded to support the climate change activism that will shape the legacy of the current administration. Inconvenient truths are too readily brushed aside.   
  
Reference:   
  
Johnstone, J. A., and N. J. Mantua, 2014. Atmospheric controls on northeast Pacific temperature variability and change, 1900-2012. _Proceedings of the National Academy of Sciences_ , doi:10.1073/pnas.1318371111.


"
"
Readers may recall yesterday where I posted this stunning image of cold for Europe and Russia for mid December 2009 from the NASA NEO MODIS satellite imager.


Deadly Cold Across Europe and Russia


Click image above to enlarge or download large image (3 MB, JPEG) 		acquired December 11 –  18, 2009
In that story were links to additional images, and I’d planned to return to them for a comparison. Inspired by my posting, METSUL’s Alexandre Aguiar saved me the trouble. There’s an interesting comparison here between the surface anomaly done by weather stations (NASA GISS) and that of satellite measurement (NASA NEO MODIS) – Anthony

Guest post by Alexandre Aguiar, METSUL, Brazil
COMPARE THE TWO MAPS

NASA GISS on the left, NASA MODIS on the right
Here’s the same images but larger – click either image for full size:



South America: The vast majority of the continent is near average or below average in the NEO map, but according to GISS only the southern tip of the region is colder. The most striking difference is Northeast Brazil: colder in the NEO map and warmer at the GISS.
Africa: Most of the continent is colder than average in the NEO map, but in the GISS most of Africa is warmer than average.
Australia: The Western part of the country is colder than average in the NEO map, but the entire country is warmer in the GISS map.
Russia: Most of the country is colder than average in the NEO map, a much larger area of colder anomalies that presented in the GISS map.
India: Colder than average at NASA’s NEO website and warmer at NASA’s GISS map.
Middle East: Huge areas of the region (Israel, Jordan, Turkey, Iraq, Syria) are colder than average in the NEO map and average/warmer in the GISS map.
Europe: Near average or slightly above average in the NEO map and much above average in the GISS map.
Greenland: Entire region colder than average at NEO and much of the area warmer at GISS.
Same source (NASA), but very different maps !!!
Why:
At NEO, land surface maps show where Earth’s surface was warmer or cooler in the daytime than the average temperatures for the same week or month from 2000-2008. So, a land surface temperature anomaly map for November 2009 shows how that month’s average temperature was different from the average temperature for all Novembers between 2000 and 2008.
Conclusion
Despite being very warm compared to the long term averages (GISS, UAH, etc), November 2009 was colder in large areas of the planet if compared to this decade average.
See PDF here. December should be very interesting in the northern hemisphere.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f434e61',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Of all the world’s great natural predators, hyenas are surely among the most maligned. They are often seen as good for nothing scavengers, the bullies of the African plains, laughing as they gang up to steal hard won meals from their more majestic competitors.  Footage of spotted hyenas (the largest and most familiar of the four hyena species) seen on nature documentaries often strengthens this notion, as does their portrayal in stories such as Disney’s Lion King. But where does such a negative view come from? And is it justified?  The origins of the hyena’s reputation may lie in the role it has played in African folklore. In Tanzania, witch doctors often kept spotted hyenas in cages and were said to ride on their backs at night. There is also an old superstition in that country that if a child is born at night while a hyena is crying, he or she will grow up to become a thief.  Then there is the hyena’s association with scavenging, and a widely held traditional belief that hyenas are there to clean up rotting carcasses – including human ones. Indeed, for the Masai of Kenya something was believed to be wrong with a person if their corpse was not consumed by a hyena. As a result they used to cover human corpses in blood and fat to encourage consumption – and avoid social disgrace.  Today, the label of “scavanger” is portrayed as being a negative trait. But the idea that hyenas are purely scavengers that profit from the hard work of other (more popular) carnivores such as lions or cheetahs is incorrect. It is a myth often perpetuated by nature documentaries which show large groups of hyenas mobbing lions after a kill.  The truth is that spotted hyena are actually excellent hunters in their own right. Indeed, the majority of all the prey they consume comes from their own hunting efforts. Given the opportunity to scavenge or steal prey from another carnivore they will take it – but so would any other carnivore on the African plains.  Such kleptoparasitism (parasitism by theft) makes perfect sense from an energy conserving point of view. The nutritional gain provided by a carcass can be obtained without any risk of being injured during a hunt or the energy expenditure involved in a chase. Of course, the act of stealing a carcass from a hungry lion is not without its risks, and individuals can be killed in the attempt. But more often the species with the larger numbers in their ranks will prevail.  For some reason though, we tend to ignore the hunting skills of hyenas, while admiring the efforts of their rivals. We marvel at the power of a leopard dragging its prey up a tree, at the speed of a cheetah coursing a gazelle, and the team work of lions as they pursue large and dangerous animals. Yet we fail to notice that hyenas are just as impressive and efficient hunters.  They typically hunt alone or in groups of up five individuals – the size of potential prey increases with the hunting group size. One adult has been observed taking down a fully grown wildebeest, a testimony to the hyena’s awesome strength.  They can also run up to 40-50km per hour over several kilometres, with one pursuit being observed over an astonishing distance of 24km. This combination of strength and speed make them formidable hunters and one of the top predators in the African savannas.  Living in female dominated clans, hyenas are also one of the most social of all carnivores. Cubs are reared communally (although females only suckle their own offspring) with the group providing safety in numbers, improved vigilance of adults, and an effective defence of territory and food. As is common in many social carnivores, the spotted hyena has a wide repertoire of vocalisations to aid communication. The high pitched cackling heard at kills or when competing for carcasses is widely referred to as a “laugh”. But the laugh is actually a submissive call, showing that the individual making the sound is not a threat.  The other call spotted hyenas are well known for is a “whoop”, a long-distance call used to communicate to other clan members, and one of the iconic sounds of the African bush.  So perhaps it is time to cast aside Disney stereotypes and “baddy” bit parts in nature documentaries. The hyena is a wonderful predator in its own right. It is just as important and impressive as any of Africa’s great carnivores, and an animal with attributes that humans admire in other species, as well as in ourselves.  Yes, they steal prey from other carnivores, but so do lions when they get a chance. They may appear like an angry mob at times, but what they are actually displaying is superb team work."
"
How to properly place your outdoor thermometer
04:52 PM PDT on Wednesday, July 29, 2009
  By TRAVIS PITTMAN / KING5.com 

excerpts:
SEATTLE – With temperatures in the Puget Sound region breaking records this week, many people are playing a watching and waiting game – waiting to see when the thermometer outside their home will reach triple digits.
…
Below is a photo of a thermometer sent to KING 5 News Tuesday afternoon by a viewer in Oso, east of Arlington. It clearly shows the temperature reading 116 degrees. You can also clearly tell the sun is reflecting off it and it’s mounted right next to a building.

source: KING 5 Viewer…
The National Weather Service says this is where you need to place your thermometer to get an accurate reading:
– It must be in a shaded, well-ventilated and open area, 5 feet above        ground, give or take a foot.
– Away from sprinkler systems
– No closer than four times the height of any obstruction. For example, if a building is 10 feet tall, it needs to be no closer than 40 feet from that building.
– Located over natural ground such as grass, dirt or sod.
– At least 100 feet from road or concrete.
…
The picture they provide is what the surfacestations project is all about. Note the 100 foot distance from asphalt.
 
source: National Weather Service
Here is a diagram of how to properly place your thermometer to get an accurate reading.
Full article here h/t to WUWT reader “Ed”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94296fff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**A care home put residents at ""serious risk"" by allowing a staff member who tested positive for Covid-19 to work, inspectors have found.**
Preston's Aadamson House Care Home also let a second person with symptoms work a shift, a Care Quality Commission (CQC) inspection in October found.
The CQC said it had ""consistently"" failed to protect people from ""catching and spreading infections"".
The care home said it had ""taken steps to rectify the concerns highlighted"".
Rating the care inadequate and placing the home, which opened in July 2019, in special measures, the CQC said the inspection had followed concerns raised about the management of coronavirus and staffing levels.
Its report said people at the premises on Peel Hall Street were ""exposed to the risk of harm"" because the provider ""failed to ensure a staff member who tested positive for Covid-19 isolated"" and ""another member of staff worked one shift with symptoms"".
Inspectors said it ""failed to consistently protect people... from catching and spreading infections"" and had not enforced social distancing in communal areas.
The CQC also found staff did not consistently wear personal protective equipment and staff had not received training in how to deal with Covid-19.
Breaches were identified by inspectors in a number of other areas and in one case, the CQC found a person who was hurt in a fall was not been adequately assessed and the risk of further falls was not mitigated.
The report also said there were not enough staff trained in basic life support, moving and handling, falls awareness and night-time emergency evacuation procedures and residents had not been protected from the risk of fire, with inspectors finding one fire escape blocked.
Aadamson House Care Home, which was caring for 13 people at the time of the inspection, said it had appointed a new manager and compliance consultancy.
It said it ""speedily put in place new systems and proactive steps to prevent any further failings and to rectify those already highlighted"", adding: ""We will work tirelessly... to ensure the residents' safety remains at the centre of the service.""
_Why not follow BBC North West on_Facebook _,_Twitter _and_Instagram _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
nan
"Boris Johnson should publicly declare climate deniers as wrong in order to secure the UK’s standing in vital UN climate talks this year, campaigners have urged, as climate deniers with links to the Tory party prepare for a new battle. As the UK leaves the EU, and its emphatic environmental commitments including the European green deal, those who want to see less action on the climate crisis are hoping Johnson’s government will be more amenable to delaying and watering down green measures. The EU on Wednesday sets out its first ever climate law, the long-awaited centrepiece of the European green deal, which is designed to prepare the EU economy for dealing with the climate emergency. The new law would enshrine a 2050 net zero emissions target in legislation and empower Brussels to take governments to court if they fail to comply. But the UK prime minister has yet to articulate a strategy for meeting Britain’s target of net zero by 2050, giving the advocates of delay and distraction everything to play for. Climate denial is taking new forms, some experts say, moving from an outright rejection of science to covert attacks on green policies and spending on efforts to cut carbon. The EU’s green deal has prompted frantic lobbying in Brussels by powerful fossil fuel interests, as a Guardian collaboration with other European media organisations this week reveals. But the Tory party retains links with prominent individuals who are climate deniers, or who are connected to organisations that either deny the climate crisis or actively work to undermine climate policies, such as the Global Warming Policy Foundation, the thinktank set up by the former Conservative chancellor Lord Lawson. “The GWPF have shown themselves to be tremendous opportunists,” said Bob Ward, policy director at the Grantham Research Institute on climate change at the London School of Economics. Ward claims the thinktank has been fundraising heavily in the US – as well as spreading its views to ministers and across Whitehall – to raise money for a major campaign to influence the Johnson government. “I expect that as the economy continues to take a hit from Brexit that the GWPF will attempt to mislead policymakers into believing the cause is climate policy.” Sir Michael Hintze, a hedge fund billionaire who is a long-time major donor to the Tory party, is understood to be a funder of the GWPF, which is not required to disclose its funding, and is also understood to have provided support to Priti Patel, Dominic Raab and Andrea Leadsom, among other Tory MPs. Matt Hancock has accepted donations from Ian Taylor, the chairman of oil trader Vitol, while Michael Gove and Liz Truss have been linked to the the American Enterprise Institute, which lobbies political, business and public opinion against action on the climate crisis. There is no suggestion of wrongdoing on the part of the Tory MPs. However, in the current climate crisis, experts have raised questions about the Tories accepting funds from individuals with fossil fuel interests or linked to climate denial organisations. “Well-connected climate sceptics must be called out by ministers, from the prime minister down,” said Shaun Spiers, the executive director of the Green Alliance thinktank. “We have seen the damage that well-funded rightwing campaigns against climate action have done in the US, Australia and Brazil. Conservative environmentalism is stronger than ever, with the party officially recognising the seriousness of the climate emergency. It should disassociate itself from the deniers.” This is Europe is a new stream of Guardian journalism that investigates the big challenges that transcend national boundaries, and seeks out the solutions that could benefit us all. These are testing times, and crises are not limited by national borders. But then neither are we.  This year’s UN talks on the climate are the most important since the Paris agreement in 2015, as the world is now far adrift of the Paris goals. The Cop26 summit set for Glasgow in November is seen as a last chance to get back on track to avoid climate breakdown, but the UK will face an uphill struggle to bring other countries on board with strong new carbon-cutting commitments. That will not be helped by any perception that the Tory party is sympathetic to, or funded by, climate sceptics and deniers, and fossil fuel interests, experts told the Guardian. Johnson should claim the mantle of Margaret Thatcher, advised Mohamed Adow, the director of developing country thinktank Power Shift Africa, and a longtime observer of the UN climate talks. “[Cop26] is an opportunity for Johnson to show that he’s broken with those dinosaurs of the past, and is instead showing that tackling climate change transcends tribal politics. His predecessor Margaret Thatcher was the first western leader to give a speech on climate change at the UN, so he won’t be the first Tory to show leadership on this issue,” he said. Other countries would be watching closely, warned Adow: “It is a problem that the Conservatives are so closely linked to discredited organisations like the GWPF. If Britain is going to be taken seriously by the rest of the world, [the Conservative party] should distance itself from these shady groups who try to undermine efforts to address the climate crisis.” As well as disavowal of denialist arguments, Johnson must set out clear measures to fulfil the UK’s own climate goals, added Alasdair Cameron of Friends of the Earth. “The voices of denial or delay are obviously scrabbling to stay relevant, but in truth are fading fast,” he said. “However, it will not be enough for government to simply accept that there is a problem. The UK has the opportunity to be a leader in building a positive zero-carbon future, but it will require urgent and concerted action at home and abroad, and right across government.” Doug Parr, the chief scientist at Greenpeace, said that although the government was espousing green policies, it was inconsistent. “Inside the Boris Johnson government, climate denial seems to have morphed into something else – a form of cognitive dissonance. It’s the contrast between acknowledging that there’s a climate emergency and keeping expanding fossil fuel exploration, funding oil and gas projects overseas, and spending billions on new roads. This way of thinking is ultimately on a collision course with reality.” Johnson has made only one public appearance on Cop26 so far this year, attending the launch in front of a group of schoolchildren and more than 100 dignitaries. But the event was overshadowed by criticism of Johnson by Claire O’Neill, the former energy minister who was to be Cop26 president but was abruptly sacked a few days before. She accused Johnson of a lack of knowledge and commitment on the climate crisis. She was replaced last month by Alok Sharma, the new business secretary. Some green campaigners are concerned that his dual role will create a conflict of interest and prevent him from standing up to vested interests reluctant to move urgently to a low-carbon economy. Hintze said in a statement from his spokesman: “I believe that the increase in concentration of carbon dioxide is in part due to human activity over the past century and that it has been a cause of global warming. But the sole focus on CO2 is too narrow and there are many contributors that need to be considered. All sides must be heard to reach the right conclusion for society as a whole.” Benny Peiser, the director of the GWPF, said the organisation was always fundraising, but did not disclose its donors. He said Cop26 was unlikely to make much progress: “The international community is deeply split on this and I do not expect anything more than a continuation of the deadlock.” A Conservative party spokesperson said: “Tackling climate change is one of the Conservative party’s top priorities, and this government is working tirelessly day in, day out to achieve our ambitious targets.”"
"“Ugly” or “wonky” veg were blamed for up to 40% of wasted fruit and vegetables in 2013, as produce was discarded for failing to meet retailer appearance standards. About 1.3 billion tonnes of food is wasted worldwide every year and, of this, fruit and vegetables have the highest wastage rates of any food type. But just how much of that is due to “ugly veg” being tossed by farms and supermarkets? The biggest culprit for food waste may be closer to home than we’d like to admit. “Ugliness” is just one reason among many for why food is wasted at some point from farm to fork – there’s also overproduction, improper storage and disease. But the problem of “wonky veg” caught the public’s attention. A report published in 2017 suggested that sales of “wonky veg” have risen in recent years as retailers have acknowledged the problem with wasting edible food, but it’s estimated that up to 25% of apples, 20% of onions and 13% of potatoes grown in the UK are still wasted on cosmetic grounds. Morrisons reported that consumers had begun to buy more misshapen food, whereas Sainsbury’s and Tesco both report including “wonky veg” in their recipe boxes, juices, smoothies and soups. Not all ugly veg is wasted at the retail point of the supply chain however. WRAP, a charity who have been working with governments on food waste since 2000, have investigated food waste on farms and their initial findings suggest a major cause of fruit waste is due to produce failing aesthetic standards. For example, strawberries are often discarded if they’re the wrong size for supermarkets. The National Farmers Union also reported in 2014 that around 20% of Gala apples were being wasted prior to leaving the farm gate as they weren’t at least 50% red in colour. Attitudes seem to be changing on “ugly veg” at least. Morissons ran a campaign to promote its “ugly veg” produce aisle, and other supermarkets are stocking similar items. Despite this, household waste remains the biggest culprit for food waste in the UK. Just under 5m tonnes of food wasted in the UK occurs in households – a staggering 70% of all post-farm gate food waste. A further million tonnes is wasted in the hospitality sector, with the latest government report blaming overly generous portion sizes. This suggests that perhaps – despite the best effort of campaigns such as Love Food Hate Waste – farms and retailers have been unfairly targeted by the “wonky veg” campaigns at the expense of focusing on where food waste really hits home. The 2013 Global Food Security Report put the figure for household and hospitality waste at 50% of total UK food waste. There are some signs we’re getting better at least. WRAP’s 2015 research showed that, at the household level, people now waste 1m tonnes of food per year less than they did in 2007. This is a staggering £3.4 billion per year saved simply by throwing less edible produce away. As climate change and its influence on extreme weather intensifies, reducing waste from precious food harvests will only become more important. Knowing exactly where the majority of waste occurs, rather than focusing too much on “wonky veg” in farms and supermarkets, is an important step towards making sure everyone has enough affordable and nutritious food to live on. During the UK’s “Dig for Victory” campaign in World War II, a large proportion of the population had to grow their own fruit and vegetables. Now the majority of people live in cities and towns – typically detached from primary food production. In the UK, the MYHarvest project has started to uncover how much “own-growing” contributes to the national diet and it seems demand for land to grow-your-own is increasing.  Research in Italy and Germany found that people who grow their own food waste the least. One way to fight food waste at home then – whether for “wonky” fruit and vegetables or otherwise – may be to replace the farm-to-fork supply chain with a garden-to-plate approach."
"
Share this...FacebookTwitter Just call it bazaar science.
The projection of sea levels has become quite the political football. So much hinges on the projections. Der Spiegel in an article titled IPCC haggles over data for sea level rise writes that 146 million people live in areas 1 meter or less above sea level. Tens of billions of dollars would be needed to expand dikes to keep the waters back, or to relocate citizens should seas rise too much. So the numbers are hotly contested.
To make things complex, there are hundreds of studies that offer a huge range of projections, up to 5 meters sea level rise by 2100. The job of deciding which sea level rise the IPCC should bank on in its next IPCC report rests on 18 scientists from 10 countries.
In the past each successive IPCC report lowered the sea level rise that is expected to occur by 2100. Critics pounced on the IPCC’s downward corrections, and so fears of rising seas diminished along with the IPCC’s credibility. Now the IPCC faces a dilemma (and irrelevance): Will it go back to alarmism? That may be real tough to do. Der Spiegel writes (emphasis added):
Now for the next IPCC report [due in 2013] the UN experts have to examine hundreds of reports – but indeed the selection is tougher than ever. The haggling over the results is like dealing at a bazaar: On one hand scientists have published alarming sea level prognoses, which surpass those given by the last IPCC Report. And on the other hand the actual sea level measurements indicate no detectable extreme increase.
4000 experts recently met at the IUGG Conference recently in Melbourne and Der Spiegel writes that the motto was: “Who bids the most!” NASA alarmist junkie James Hansen appears to have been the highest bidder at 5 meters. Currently sea levels are rising about 3 mm per year, which is just 1/17 of what Hansen projects.
Jim Houston and Bob Dean have a recent paper saying  there has been no detectable acceleration, while Stefan Rahmstorf says there is (though measurements don’t show it). Der Spiegel then cites other experts:
Simon Holgate, sea level researcher at the National Oceanogrphy Centre in Liverpool: Likely the irregularities in data arising from the changeover in measurement instruments are responsible for the differences [in the recent results].



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




 I believe that it is improbable that the sea level increase accelerated in the same year that satellites were put into service.”
Guy Wöppelmann of La Rochelle in France, Der Spiegel writes:
The increase sea level rise since 1993 is nothing unusual, as the sea level during the 20th century accelerated before, only then to decelerate.”
Eduardo Zorita:
The sea level rise rate has slowed down during the last 8 years. What happens in the future is unknown.”
Of course there are also a number of alarmist scientists who insist that sea level is accelerating and that Greenland and Antarctica pose a serious risk. But so far data measurements don’t show it.
Obviously the risk is all in the modeling (and not the actual measurements).
===================================
UPDATE 1: Schellnhuber now offering 70 m! Does anyone offer 80m? http://stevengoddard
h/t: DirkH
Share this...FacebookTwitter "
nan
"

In a world where many adults receive their science education from newspapers and television, a great deal of misinformation about global warming exists. The media is quite skilled at making highly untenable predictions of greenhouse doom and gloom appear credible‐​foretelling drastically rising sea levels, the increased fury of hurricanes, and even plagues of locusts. The inaccuracies about how humans inadvertently warm earth’s tenuous atmosphere so pervade popular culture that the actual science behind this notion is hardly given a second thought. 



So how do we separate the global warming wheat from the greenhouse chaff? As a meteorologist practicing my trade for nearly 20 years, I recommend a read of the latest work by Patrick Michaels, a professor at the University of Virginia and senior fellow in environmental studies at the Cato Institute. The book, _Meltdown_ , is the latest in Michaels’ sequence of books on the topic of global warming. 



_Meltdown_ presents the flip side of what most people have heard about global warming, a cogent counterpoint to the view that the introduction of anthropogenic carbon dioxide is pushing the fluid systems of this planet into hyper‐​overdrive. The book presents a vast body of highly credible and growing knowledge that has been largely ignored. It includes scientific information that does not get reported in the papers or in government reports, because this information threatens to undermine the great doom and gloom establishment. The basic thesis of _Meltdown_ is that, yes, there has been a recent upward trend in the temperature of the atmosphere. But the increase is small and unlikely to mushroom into something truly catastrophic; the public, policy, and scientific distortions that have emerged are way off the mark. The book is steeped in scientific fact, with no fewer than 100 references to journal literature, but Michaels distills, synthesizes, and cuts through the morass like a beacon. His coverage is broad, and the distortions he uncovers are organized into topics dealing with ecosystems, drought and flood, severe storms, diseases, and the cryosphere.



The book is also rewarding because Michaels’ writing is a pleasure to read. His style is conversational, nontechnical, and often quite humorous. In fact, we get the sense that Michaels enjoys agitating the beehive, poking and jabbing at the current paradigm of greenhouse disaster. His tone conveys an often blunt, common sense way of thinking about the problem. If meteorology had its own weekly news show, a _60 Minutes_ on climate change, Michaels would be the perfect Andy Rooney.



 _Meltdown_ provides a global warming education that is unlikely to be gleaned from textbooks or a formal introductory college course on meteorology. It also may clear up common misconceptions picked up over the years. Right from the start Michaels’ explains that anthropogenic global warming and the greenhouse effect are not one and the same process. In fact, the greenhouse effect has been operating steadily for millennia, actually making the earth habitable. Global warming has many causes, and the statistics can be misleading. For instance, an anomalously warm year can be cloaked in the form of a strong El Niño; a virulent tornado outbreak in the Midwest is not a sure signal of humaninduced greenhouse enhancement.



Michaels operates like a pathologist, sifting through the minutiae of datasets, time series, and statistics to construct a convincing argument against the imminent melting of glaciers and hurricanes spinning rapidly out of control. Michaels applies rigorous tests to the data, pointing out where the datasets frequently break down or are stretched beyond the point of credibility. _Meltdown_ provides a badly needed balance, throwing some cold water on those waiting for the planet’s thermometer to boil over.



Those interested in climate science owe it to themselves to complete their education by reading _Meltdown_ and Michaels’ earlier works. In a room that has grown quite stuffy during the past few years, this book throws open the windows and lets in some fresh air.
"
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"Pupils from around the world are going on strike from school to demand urgent action from the world’s leaders on climate change. Here, a scientist answers  teenagers’ questions about climate change, gathered by the Priestley International Centre for Climate at a previous strike in February. You can find more Q&As like this on the centre’s website. If you’re a teen, you can have your questions about the world answered by academic experts – find out how at the end of this article. The “12 years” date you’ve heard comes from a special report requested by the United Nations, which looks at the impacts of global warming at 1.5°C above pre-industrial levels. At the moment, the world is 1°C warmer than in the late 19th century: the earliest period for which we have reliable temperature measurements and just before the Industrial Revolution got into full swing.  To avoid global warming above 1.5°C, humanity needs to cut its carbon dioxide (CO₂) emissions to about half of today’s levels by 2030, and to zero by 2050. The 2030 date – 12 years from when the report was released in October, 2018 – got a lot of media attention. Missing the 2030 deadline would make it very difficult to keep global warming under 1.5°C. That temperature is not necessarily safe, but the damage caused by climate change will quickly get worse with higher levels of warming.  At today’s 1°C of warming, there have already been increases in extreme weather events (such as heat waves and flooding), as well as food shortages and effects on food production. Entire species are already going extinct, for reasons related to climate change. At 2°C of warming or above, rising sea levels, more frequent extreme weather and damaging effects on food and water supplies will make some parts of the world very hard to live in. As a result, it’s predicted that many people will need to leave their homes and become climate refugees, while many millions more people worldwide will be exposed to poverty. What’s more, many species will be lost and virtually all corals will die.  Unfortunately, we are not on track to keep warming below 1.5°C, or even 2°C. If countries hit their existing targets, temperatures will rise by around 3°C – or more than that, if emissions continue to grow. The planet itself will survive man-made climate change. In fact, it has been warmer, millions of years ago, although the world looked very different back then. Humans are not expected to go extinct – but we will have to learn to cope with a warmer world, and all its challenges. This means cooperating and providing support and resources to vulnerable people. No single policy will end climate change, but a very effective strategy would be to quickly phase out fossil fuels such as coal and petrol, which are used to create electricity and power transport. There are many different ways to achieve this goal, and it’s important that leaders choose policies that create good jobs and strengthen communities.  For example, governments need to put money towards safe, reliable, efficient and affordable trains and buses, so people can get around without using cars. Towns and cities should be designed to be more friendly to walking, cycling and public transport. Homes should have good transport links, and be built or modified to be more energy efficient, so that they’re easier to keep cool in the summer and warm in the winter.  International air travel is also responsible for a growing share of global emissions and governments around the world need to work together to come up with a response.  Farming – especially meat and dairy production – also creates a surprisingly large amount of emissions. So, governments should encourage farmers to use sustainable approaches. Agriculture can also lead to deforestation. Since trees remove carbon dioxide from the atmosphere, forests must be protected, and new trees planted. First, you can find out what your environmental footprint is using this questionnaire from the World Wide Fund for Nature (WWF). The survey gives advice to help you and your family reduce your impact. Research has also highlighted the biggest changes a person can make to help the climate. They are:  Smaller actions in your daily life can also help. Turning down the heating or air conditioning at home and only heating or cooling the rooms you’re using will save money and reduce carbon emissions. Try to buy less clothing, plastics and gadgets, since it takes resources and energy to make these items.  Make, borrow, swap, buy secondhand or find things for free, and recycle as much as possible that can’t be reused. When you’re old enough, you can also choose to put your money in an ethical bank account, and get electricity from 100% renewables. Individual changes will only go so far, but remember that your actions can inspire others. Use your voice! Talking about climate change with your friends, family and classmates really helps to raise awareness and drive further action."
"
Guest Post by David Archibald
NASA’s David Hathaway has adjusted his expectations of Solar Cycle 24 downwards.  He is quoted in the New York Times here Specifically, he said:
” Still, something like the Dalton Minimum — two solar cycles in the early 1800s that peaked at about an average of 50 sunspots — lies in the realm of the possible.”
NASA has caught up with my prediction in early 2006 of a Dalton Minimum repeat, so for a brief, shining moment of three years, I have had a better track record in predicting solar activity than NASA.

The graphic above is modified from a paper I published in March, 2006.  Even based on our understanding of solar – climate relationship at the time, it was evident the range of Solar Cycle 24 amplitude predictions would result in a 2°C range in temperature.  The climate science community was oblivious to this, despite billions being spent.  To borrow a term from the leftist lexicon, the predictions above Badalyan are now discredited elements.
Let’s now examine another successful prediction of mine.  In March, 2008 at the first Heartland climate conference in New York, I predicted that Solar Cycle 24 would mean that it would not be a good time to be a Canadian wheat farmer.  Lo and behold, the Canadian wheat crop is down 20% this year due to a cold spring and dry fields. Story here.
The oceans are losing heat, so the Canadian wheat belt will just get colder and drier as Solar Cycle 24 progresses.  As Mark Steyn recently said, anyone under the age of 29 has not experienced global warming.  A Dalton Minimum repeat will mean that they will have to wait to the age of 54 odd to experience a warming trend.
Where to now?  The F 10.7 flux continues to flatline.  All the volatility has gone out of it.  In terms of picking the month of minimum for the Solar Cycle 23/24 transition, I think the solar community will put it in the middle of the F 10.7 quiet period due to the lack of sunspots.  We won’t know how long that quiet period is until solar activity ramps up again.  So picking the month of minimum at the moment may just be guessing.
Dr Hathaway says that we are not in for a Maunder Minimum, and I agree with him.  I have been contacted by a gentleman from the lower 48 who has a very good solar activity model.  It hindcasts the 20th century almost perfectly, so I have a lot of faith in what it is predicting for the 21st century, which is a couple of very weak cycles and then back to normal as we have known it.  I consider his model to be a major advance in solar science.
What I am now examining is the possibility that there will not be a solar magnetic reversal at the Solar Cycle 24 maximum.

Sponsored IT training links:
Achieve guaranteed success using up to date 646-230 dumps and 642-426 study guide prepared by 642-661 certified experts.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e947c662e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
In the “Steig et al – falsified” thread, since we have been discussing geothermal activity along the Antarctic peninsula, I thought I’d pass along these images that show other parts of the planet where geothermal heat seems capable of melting ice and making it all the way to the surface. Lake Baikal is quite deep, over 5000′ feet in places, so this demonstrates that even in deep water, the melting of ice from that geothermal heat is a real possibility. Hat tip to WUWT commenter “Mark”  – Anthony
By Betsy Mason, Wired News 
Click for a larger image - photo from NASA - ISS
Astronauts aboard the International Space Station noticed two mysterious dark circles in the ice of Russia’s Lake Baikal in April. Though the cause is more likely aqueous than alien, some aspects of the odd blemishes defy explanation.
The two circles are the focal points for ice break-up and may be caused by upwelling of warmer water in the lake. The dark color of the circles is due to thinning of the ice, which usually hangs around into June.
Upwelling wouldn’t be strange in some relatively shallow areas of the lake where hydrothermal activity has been detected, such as where the circle near the center of the lake (pictured below) is located.
Circles have been seen in that area before in 1985 and 1994, though they weren’t nearly as pronounced. But the location of the circle near the southern tip of the lake (pictured above) where water is relatively deep and cold is puzzling.
The lake itself is an oddity. It is the largest by volume and the deepest (5370 feet at its deepest point), as well as one of the oldest at around 25 million years. The photo above was taken by an astronaut from the ISS.
The photo below was taken by NASA’s MODIS satellite imager.
Click for a larger image - photo from NASA - MODIS
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95c9c39f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWhat happens when you take the work of energy production and management away from the experts and power companies, and transfer it to government bureaucrats and environmental ideologues who surround themselves with clueless media cheerleaders?
Guenter Keil tells us here at Science Skeptical.
In a nutshell, it’s like taking a country’s central bank and putting it into the hands of communists – i.e. it gets run into the ground in short order.
Late last October in the German state of Brandenburg (East Germany), German television and print media were all present and enthusiastically cheered as State Minister Matthias Platzeck put “the world’s first” wind-driven hydrogen hybrid power plant officially into operation. Hooray, everyone cheered and patted each other on the back – we’re rescuing the planet!
“…the entire big problem of renewable energy sources is solved,” proclaimed German daily Süddeutsche Zeitung!
Werner Diwald Chairman of operating company Enertrag AG, which no doubt is getting subsidized up the wazoo for this, proclaimed:
It’s more than a project; it’s the cornerstone of the energy transformation.”
Brandenburg’s Minister Matthias Platzeck (Socialist Party) added that environmentally the plant is “a huge step forward”.
So just what is this marvel of German green engineering: the hydrogen-hybrid plant? How does it work?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Science Skeptical tells us. Electricity produced by wind parks is used to produce hydrogen gas whenever the wind power is not needed. This hydrogen is then stored and used later when needed to drive a gas turbine, which powers an electric generator, which in turn produces electricity. What a clever and elegant solution! At least that’s what the media, politicians and some environmentalists think.
19% efficiency
Now let’s return to reality. It only takes a freshman engineering student to explain the utter folly of this system. Breaking it down in steps we have: wind > wind turbine rotor > wind generator > AC electricity > DC electricity > electrolysis > hydrogen > pump/storage vessel > gas turbine > generator  > AC electricity > transmission lines > consumer. That’s a long chain of energy transformation steps. Unfortunately though, each step involves LOSSES.
Of course, none of the media, politcians or greens even bothered to calculate the efficiency and cost of this system. But thankfully Keil has, and reached a grand efficiency of 19%, meaning 81% of the energy is lost as heat into the atmosphere. According to Keil with such a system electricity will end up costing the consumer over $1.00 per kwh. And if the hydrid system uses solar panels to provide the primary energy, then the cost rises to over $1.20/kwh.
Conclusion: when bureaucrats and other energy engineering morons take over the management of energy, you get a system that is hardly more efficient than the Greek government.
Finally the Süddeutsche Zeitung quotes GDR physicist and now current Chancellor of Germany Angela Merkel, who led the project’s groundbreaking ceremony in 2009:
We are going to learn something from this power plant.”
Indeed we will.
====================================
For those of you who may have forgotten, here’s yet another government-run energy engineering marvel (again from East Germany): click here!
 
Share this...FacebookTwitter "
"**Churches in NI are to be allowed to stay open for individual prayer during the two-week coronavirus lockdown.**
Stormont ministers met on Tuesday morning and agreed the clarification to the regulations.
It followed calls by church leaders across NI for the change.
The executive had initially agreed that places of worship should close for all but weddings, civil partnerships and funerals from Friday until 11 December.
They will also be allowed to carry out drive-in services.
Outdoor visitor attractions will have to close as well, while the executive has confirmed that self-catering accommodation will only be permitted to operate on a ""restricted basis"", in line with arrangements already in place for hotels and B&Bs.
On Tuesday, the Department of Health also confirmed that 11 more people have died after contracting coronavirus, taking the total to 947 in Northern Ireland.
As of Tuesday, there were 445 people in hospital, of those 37 people are in intensive care, 29 are on ventilation.
NI's chief medical officer Dr Michael McBride also confirmed the R number had not yet dropped to below one.
Dr McBride said they hoped to get the R number below one with the next two weeks of interventions.
Speaking during a media briefing Dr McBride said there had not been a significant reduction in the number of hospital admissions or the number of patients in ICU.
He said there was an opportunity to ""redouble efforts"" to make difference and make the restrictions over the next two weeks ""count"".
Non-essential retail shops obliged to close later this week for the so-called circuit-breaker will be permitted to operate click-and-collect services.
First Minister Arlene Foster welcomed the executive's decision to allow places of worship to remain open.
""This weekend sees the beginning of Advent and we have agreed that it is important that all places of worship can remain open in a limited way,"" she said.
She added that the small adjustments would help to maintain well-being and reduce pressures.
Deputy First Minister Michelle O'Neill said the executive had listened to views on a range of issues.
""These two weeks offer us the best chance of pushing community transmission as low as possible to allow a safer Christmas for everyone,"" she said.
In a tweet, Justice Minister Naomi Long said that it was ""important"" that smaller retailers could operate on this basis.
AodhÃ¡n Connolly, director of the NI Retail Consortium, said the decision would be a ""lifeline"" for retailers coming up to Christmas.
""We need public support to make this work. Please abide by the time that you are given to collect your goods,"" he added.
""Leave extra time for your shopping and be kind to staff and fellow shoppers. This is in no way a normal Christmas but we can make it better we work together.""
The two-week lockdown was agreed by Stormont ministers last Thursday, in a bid to reduce the transmission of Covid-19 in Northern Ireland.
It is much tougher than previous periods of restrictions imposed by the executive, but schools will be allowed to stay open.
Last week, the Moderator of the Presbyterian Church had described the closure of places of worship as a cause of ""significant regret and concern"".
In a statement, Rt Rev Dr David Bruce said the Presbyterian Church continued to make representations regarding public worship to the Northern Ireland Executive in Belfast and to the Irish Government in Dublin.
The Roman Catholic Archbishop of Armagh and Primate of All Ireland Eamon Martin had questioned why off-licences could remain open but churches had to close.
He had urged the executive to accept that for many people a ""meaningful Christmas"" is about more than shopping, eating and drinking and that spiritual preparation is essential.
Earlier this year, the leaders of NI's four main Christian Churches asked parishioners to wear face coverings during services."
"Whenever I tell people I work with solar cells I am asked the same two questions: are they ever going to be really cheap? And can you get me some? While the answer to the second question is no, the answer to the first is a lot more positive. Year on year, solar panels have been plunging in price and improving in the efficiency with which they can convert light into energy.  At the same time fossil fuel costs continue to rise, and in the next few years we will reach the point where the costs overlap – some figures suggest this may have already happened. The question is not whether solar energy can supplant fossil fuels as the cheapest means to produce energy, but rather when.  While this has provided an enormous boost to the solar industry, the main excitement in the solar sector today is due to a new type of material called perovskite. Combining some of the best qualities of more mainstream materials, it has proved incredibly flexible – to the point that University of Sheffield researchers have manufactured perovskite solar cells as a spray-on liquid. So what is perovskite, and what’s the buzz around it? Solar cells, the component of solar panels that reacts to light, are built from what are known as photovoltaic materials. When light hits these materials electrons are freed to move through the material. With careful design of the structure of these solar cells, these electrons can be collected into a flow of electrical current. This is the process that provides the somewhat magical property of solar panels – sunlight in and electricity out. Broadly speaking, solar cells can be divided into two distinct groups – those based on inorganic photovoltaic materials, such as silicon or cadmium telluride, and those based on specific organic compounds, such as PCDTBT. Both have their own respective advantages and disadvantages.  The inorganic materials are already industrially well-established, capable of converting light to electricity at greater than 20% efficiency and create solar panels with more than 25-year lifespans. The downside is that the raw materials required, particularly with silicon, can be expensive. Organic solar cells are based on potentially low-cost materials and can even be manufactured from a liquid solution, which makes them very fast and cheap to produce. However, even on a laboratory-scale, organic solar cells struggle to achieve efficiencies of more than 10%. Even more crucially, the organic compounds gradually decompose under light, often reducing panel lifetime to the order of months or weeks rather than years. Consequently these organic materials have rarely been used to produce solar panels, as no one likes the idea of having to climb on their roof to replace them every six months. Ideally we want a solar cell with the performance and long term stability of inorganic materials with the ultra-low cost of organic materials. In the past few years solar energy research has witnessed the emergence of a remarkable new class of materials known as perovskites. This is a hybrid organic-inorganic material, essentially an organic compound with an inorganic element attached. Perovskite refers to the specific type of crystal structure, which occurs naturally in certain minerals. These hybrid compounds have this crystal structure but are also a complex combination of organic ammonia and methyl groups with inorganic lead iodide or lead chloride molecules attached.  The reason for the excitement surrounding these materials is the frankly staggering rate at which they have developed. Previously whenever a new material was discovered it had taken some 10-20 years of research to reach an efficiency rate of even 10%. Perovskite solar cells only emerged in 2012, but have already clocked up conversions of more than 19% efficiency. This blistering rate of development is unprecedented in solar research. As a hybrid material, as well as boasting good efficiencies as with inorganic materials, perovskites can also take advantage of organic solar materials’ capacity to be applied as a liquid solution. This is what Professor David Lidzey’s group at the University of Sheffield has taken advantage of, spraying the perovskite as a liquid coating onto a substrate material. This allows solar cells to be manufactured at high volumes and low cost. Does this mean that all future solar cells will be based on perovskites? It’s far too early to say. Although they have many benefits there are still a number of key challenges to be overcome.  There are some questions regarding the potential environmental impact of the lead content of the material (although work is ongoing to remove the requirement for lead) and how easily production can be up-scaled to a useful commercial size. As with organic solar cells, their long term stability is also highly questionable and they are particularly sensitive to moisture – a few drops of water can completely destroy the material. So building a perovskite solar panel module capable of surviving for decades outdoors is most likely still some way off – in fact there’s no guarantee it’s even possible. But what is for certain is that the potential of perovskite solar cells is staggering, and if the material’s promise can be realised it could completely revolutionise the capabilities of solar energy. "
"Public bikesharing schemes are sprouting up in towns and cities worldwide. The bikes are generally provided without helmets, and this has led to concerns regarding the risk of serious head injuries.  It has been shown that the users of these bike hire schemes are less likely to wear helmets, high-visibility clothing or specialist cycling Lycra than people riding their own bikes. We’ve argued this is a good thing, as it helps normalise the image of cycling away from a specialist past-time, reducing the perception that riding a bicycle is a risky activity or only for super-sporty people.  But a recent study by Janessa Graves and colleagues published in the American Journal of Public Health concluded that there was a link between the introduction of bikesharing schemes in North American cities and the risk of bicycle-related head injuries.  So it was argued that helmets should be incorporated into the schemes as standard from the outset. We suggest that these concerns are misplaced, and agree with the many other commentators who have argued that the study’s data don’t justify the authors’ conclusions. In fact, the paper’s data could be reasonably interpreted to argue the opposite – that the take-up of bikesharing schemes leads to lower risk of injury.  The authors compared the number of injuries (head and non-head) in five cities with bikesharing schemes, and five without, over a 12 month period. Their result is the percentage of cyclist injuries that are head injuries, and this does significantly increase in cities with bikesharing schemes, from about 42% pre-bikeshare scheme to 50% afterwards. In the control cities, the figure dipped from 38% to 36%. But the result does not represent the risk of head injuries; that is, they did not divide the number of head injuries recorded by a measure of the total amount of cycling being done in the city, to get the “risk per trip” or “risk per kilometre cycled”.  As the study’s data don’t include the amount of cycling trips in each city, we’re also unable to calculate the risk per kilometre cycled. But we can examine what happens to the absolute numbers: the number of head injuries falls in absolute terms in cities with bikesharing schemes relative to the baseline. Crucially, it also falls relative to the control cities. The only reason the percentage of head injuries rises is that the number of non-head injuries fall even faster, leaving head injuries as a greater proportion – but not greater in absolute numbers.  This can be represented as the change over time in the likelihood of injury in cities with a bikesharing scheme versus those without. On this measure of relative change (technically, an odds ratio) all values are less than 1, which indicates that things have got relatively safer in the bikesharing cities.  So a reasonable interpretation would be that cities with bikesharing schemes experienced decreases in all types of injury, including a trend towards a decrease in head injuries, relative to those cities without. In other words, the data suggest injury rates seem to have gone down in bikesharing cities. It’s quite plausible that this actually underestimates the true decrease in risk, since introducing bikesharing schemes is likely to have led to an increase in total cycling. There are other reasons to be sceptical of the study’s conclusions.  One of the surprising findings in the study is that the percentage of head injuries rises fastest among children. This is precisely the opposite of what one would expect if bikesharing schemes were to blame, since children are less likely to use bikesharing schemes – the bikes are adult-sized, and riders typically need a credit card. So blaming a rise in head injuries among children on a lack of helmets among bike hire users doesn’t seem consistent, as researcher Kay Teschke has pointed out. The study’s conclusions are also not supported by other evidence pointing to lower injury rates on bikeshare bikes than on personal bikes. The Graves study doesn’t record who is being injured – that is, whether or not they are helmet wearers and whether or not they are bikeshare users. Our recent modelling study of the London bikesharing scheme did have these details, and found that injury risks were lower among bikeshare users compared with the average for cyclists in the same area. This was despite the fact that those hiring bikes were much less likely to wear helmets or hi-viz than other cyclists. Initial reports from New York also suggest that risk of injury among bikesharing users is lower than expected. It’s important to be cautious before attributing either more or fewer injuries to bikesharing schemes. For a scheme to be responsible for the 41% reduction of non-head injuries reported by the Graves paper, the scheme would have needed to have replaced a substantial proportion of existing cycling trips and carry a very low risk of non-head injuries among users (for example, equivalent to a 41% replacement of existing cycling combined with a zero risk of non-head injuries among users).  Such replacement of existing cycling levels are just implausible, because hire bikes are usually only available in a relatively small, downtown part of a city, and represent only a small proportion of cycle trips even in those areas. Our study in London also suggested the majority of trips are new cycling trips. As an example, bike hire trips in London make up only around 4% of all cycle trips. The study’s authors do not estimate the proportion of bikesharing trips in their cities, but do show that the average number of shared bicycles is three per 1000 population, which is only around 2.5 times higher than in London. So a plausible estimate of the amount of cycling due to bikesharing (2.5 x 4%) is around 10% of cycling journeys.  In other words, far smaller than the 41% replacement of existing cycling needed  to be responsible for the change in injuries observed. It’s more likely that the introduction of a bikeshare scheme was one of a number of policies and changes that, taken together, led to safer cycling in these cities. So a broad ecological study such as the paper from Graves and her co-authors tells us little about the effect bikesharing schemes have on injury rates amongst users. To truly understand this we need direct evidence and individual-level statistics about users and non-users and injury rates. To understand how cycle hire may affect risks for all cyclists we are going to need evidence from different sources, such as qualitative data, policy analysis, and systems models.  What this study does tell us fits with the possibility that injury risks may actually be lower when using hire bikes, and that the introduction of these schemes may go hand-in-hand with a general lowering of risk. So for now, calls for bikesharing schemes (or all cyclists) to require helmets are not supported by the evidence available. Hard Evidence is a series of articles in which academics use research evidence to tackle the trickiest public policy questions."
"**France will begin to ease its strict coronavirus restrictions this weekend, allowing non-essential shops to reopen, President Emmanuel Macron has said.**
People will also be able to share ""moments with the family"" over the Christmas period, Mr Macron announced.
But he said bars and restaurants would have to remain closed until 20 January.
France has reported more than 2.2 million cases and more than 50,000 confirmed coronavirus-related deaths since the start of the pandemic.
In a televised address on Tuesday evening, Mr Macron said the country had passed the peak of the second wave of virus infections.
He said that the bulk of lockdown restrictions would be eased from 15 December for the festive period, with cinemas reopening and general travel restrictions lifted, as long as new infections were at 5,000 a day or less.
On Monday, France reported 4,452 daily Covid-19 infections - its lowest tally since 28 September.
The latest seven-day rolling average for new infections in France is reported to be 21,918. That figure peaked at 54,440 on 7 November.
Mr Macron said the recent news of successful vaccine trials offered ""a glimmer of hope"" and that France would aim to begin vaccinations against Covid-19 ""at the end of December or at the beginning of January"", starting with the elderly and most vulnerable.
The French president said the situation would be reviewed on 20 January, and if infections remained low, bars and restaurants would then be permitted to reopen. Universities would also be able to accept students again.
However, if the situation had worsened, he said he would look at options to avoid triggering a third wave.
""We must do everything to avoid a third wave, do everything to avoid a third lockdown,"" Mr Macron said.
He later tweeted to say that all businesses forced to remain closed during the restrictions, such as restaurants, bars and sports halls, would have the choice of receiving up to â¬10,000 (Â£8,900) from a ""solidarity fund"" or the payment of 20% of their turnover.
He said that France's ski resorts may have to remain closed until next year because the current risks associated with the virus made it difficult for such sports to resume.
However, he said he would discuss the issue with other European leaders and provide an update in the coming days.
Ski resorts were responsible for numerous outbreaks of Covid-19 cases across Europe in the early days of the pandemic.
Mr Macron said the lockdown would be replaced by a nationwide curfew between 21:00 and 07:00, except on Christmas Eve and New Year's Eve.
France has been under a second national lockdown since the beginning of November. People have only been permitted to leave home to go to work, buy essential goods, seek medical help or exercise for one hour a day. Anyone going outside must carry a written statement justifying their journey.
While all non-essential shops, restaurants and bars have been shut, schools and crÃ¨ches have remained open. Social gatherings have been banned.
Measures to deal with coronavirus outbreaks remain in place across Europe, but a reduction in daily reported cases in some areas - coupled with the reported success of a number of vaccines - has led countries to revisit their restrictions. Some of the latest developments include:"
"
Share this...FacebookTwitter
German “intellectual” online daily Die Zeit here interviews four (state-funded) alarmists (some more, some less) just to reassure its readers that the world is indeed coming to a catastrophic end.
Here it’s interesting to compare the remarks of Hans von Storch and Stefan Rahmstorf on the subject of sea level rise of the North Sea, as measured by tide gauges.
Rahmstorf:
Especially important for us here in Germany was a study on North Sea sea level. Scientists at the University of Siegen showed what consequences climate change is already having on our own coastlines. The evaluation of the data from 13 gauge stations over the last one and a half centuries confirm a sea level rise – and an acceleration over the last few decades.”
Hans von Storch:
Through an anylsis of synchronous fluctuations at many gauges, a rise in sea level of about 20 cm per century could be now be estimated.”
That’s a whole 2 mm per year, which is one seventh the rate of what Ramstorf envisions in his tamer wet dreams. Moreover, the link that Rahmstorf provides has a chart which clearly shows the rate of rise during the end of the 19th century was greater than what has occurred over the last few decades. He forgot to mention that.
One thing is clear: sea level measurement is fraught with complexity, and thus it would be easy for any shifty scientist to get the results he wants. But not to worry, we all know that they are, to quote Mark Antony,  “ambitious and honorable men”.
As far as the other content in the DIE ZEIT piece, it’s just the usual, alarmist pre-Durban crap. (Don’t waste your time)
Share this...FacebookTwitter "
"

From the abstract of the lead paper by Martin Wild: Recent evidence suggests that solar radiation reaching the Earth’s surface has not been constant over time but has undergone substantial variations on decadal timescales. The available observations suggest a widespread decrease in surface solar radiation between the 1950s and 1980s (popularly referred to as “global dimming”), with some more recent evidence for a partial recovery (“brightening”).
From ETH Zurich News
“Global dimming and brightening” –  The role of solar radiation in climate change
A special volume of the “Journal of Geophysical Research” reviews the growing research field of “global dimming” and “global brightening” in over 20 articles. These phenomena, supposedly human-induced, control solar radiation incident at the Earth’s surface and thus influence climate.

Clouds and aerosols influence the solar radiation on the earth’s surface and therefore the climate. (Photo: flickr/Schrottie)<!– (mehr Bilder) –>


Special instruments have been recording the solar radiation that reaches the Earth’s surface since 1923. However, it wasn’t until the International Geophysical Year in 1957/58 that a global measurement network began to take shape. The data thus obtained reveal that the energy provided by the sun at the Earth’s surface has undergone considerable variations over the past decades, with associated impacts on climate.
Research focus at ETH Zurich
Investigating which factors reduce or intensify solar radiation and thus cause “global dimming” or “global brightening” is still very much a nascent field of research in which especially scientists from ETH Zurich became renowned. The American Geophysical Union (AGU) has now published a special volume on the subject which presents the current state of knowledge in detail and makes a considerable contribution to climate science. “Only now, especially with the help of this volume, is research in this field really taking off”, stresses Martin Wild, senior scientist at the Institute for Atmospheric and Climate Science of ETH Zurich, who is a specialist on the subject.
Decrease in solar radiation discovered
The initial findings, which revealed that solar radiation at the Earth’s surface is not constant over time but rather varies considerably over decades, were published in the late 1980s and early 1990s for specific regions of the Earth. Atsumu Ohmura, emeritus professor at ETH Zurich, for example, discovered at the time that the amount of solar radiation over Europe decreased considerably between the 1950s and the 1980s. It wasn’t until 1998 that the first global study was conducted for larger areas, like the continents Africa, Asia, North America and Europe for instance. The results showed that on average the surface solar radiation decreased by two percent per decade between the 1950s and 1990.
In analyzing more recently compiled data, however, Wild and his team discovered that solar radiation has gradually been increasing again since 1985. In a paper published in “Science” in 2005, they coined the phrase “global brightening” to describe this new trend and to oppose to the term “global dimming” used since 2001 for the previously established decrease in solar radiation.
Only recently, an article in the journal “Nature”, which Wild was also involved in, brought additional attention to the topic of global dimming/brightening.
Air pollution favors photosynthesis
In this study, for the first time, the scientists examined the connection between global dimming/brightening and the carbon cycle. They demonstrated that more scattered light is present during periods of global dimming due to the increased aerosol- and cloud-amounts, enabling plants to absorb CO2 more efficiently than when the air is cleaner and thus clearer. According to the scientists, this is because scattered light penetrates deeper into the vegetation canopy than direct sunlight, which means the plants can use the light more effectively for photosynthesis. Consequently, there was around 10 percent more carbon stored in the terrestrial biosphere between 1960 and 1999.
The special volume, which appears in the AGU’s renowned “Journal of Geophysical Research”, provides an overview of the current state of knowledge. Almost half of the publications in the volume were either completely or partially written by ETH Zurich scientists. Wild is the guest editor, and author or co-author of ten of these articles.
The articles provide the first indication of the magnitude of these effects, how they vary in terms of time and space and what the possible consequences might be for climate change. They also discuss in detail the underlying causes and mechanisms, which are still under debate.
Many questions left open
It is particularly unclear as to whether it is the clouds or the aerosols that trigger global dimming/brightening, or even interactions between clouds and aerosols, as aerosols can influence the “brightness” and lifetime of the clouds. The investigation of these relations is complicated by the fact that insufficient – if any – observational data are available on how clouds and aerosol loadings have been changing over the past decades. The recently launched satellite measurement programs should help to close this gap for the future from space, however.
“There is still an enormous amount of research to be done as many questions are still open”, explains Wild. This includes the magnitude of the dimming and brightening effects on a global level and how greatly the effects differ between urban and rural areas, where fewer aerosols are released into the atmosphere. Another unresolved question is what happens over the oceans, as barely any measurement data are available from these areas.
A further challenge for the researchers is to incorporate the effects of global dimming/brightening more effectively in climate models, to understand their impact on climate change better. After all, studies indicate that global dimming masked the actual temperature rise – and therefore climate change – until well into the 1980s. Moreover, the studies published also show that the models used in the Intergovernmental Panel on Climate Change’s (IPCC) fourth Assessment Report do not reproduce global dimming/brightening adequately: neither the dimming nor the subsequent brightening is simulated realistically by the models. According to the scientists, this is probably due to the fact that the processes causing global dimming/brightening were not taken into account adequately and that the historical anthropogenic emissions used as model input are afflicted with considerable uncertainties.
“This is why at ETH Zurich we are working with a research version of a global climate model, which contains much more detailed aerosol and cloud microphysics and can reproduce global dimming/brightening more effectively”, says Wild. For him, the studies so far constitute “initial” estimates that need to be followed up with further research.
Link to these papers in JGR here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93ed862e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Even as Secretary of Defense Donald Rumsfeld and Secretary of State Colin Powell visited Canberra for the annual AUSMIN (Australia-U.S. Ministerial) consultations, talking of expanded security relations, mutterings of disappointment were heard. Peter Hartcher of the Australian Financial Review wrote of “a climate of deflated expectations.” But it is time for not just Australia, but U.S. allies around the world, to expect less of the United States and do more themselves.



American security obligations are breathtaking. The United States maintains 100,000 troops in Western Europe to defend against phantom security threats, and is talking about further expanding NATO. Washington helps garrison the Balkans, an area never of strategic interest to the United States. Washington routinely attempts to dictate affairs in Central America and occasionally blunders into Africa. Washington’s support for Israel and demand for oil have led to multiple interventions in Lebanon, war against Iraq, and permanent garrisons in Kuwait and Saudi Arabia. An additional 100,000 troops are stationed in East Asia. Treaties, informal security guarantees and bases litter the region. U.S. and Australian officials suggested creating an informal JANZUS security group of America, Australia, Japan and maybe South Korea. It was one thing to carry such a global burden during the Cold War, when the Soviet Union and its many surrogates threatened friendly nations that had been wrecked or weakened by World War II. But the threats have dissipated and the Western states are far stronger than their potential antagonists. Which means that Washington need not do as much.



Even changing its policy is considered by others to be arrogant and “unilateral” — the term of opprobrium, that seems to have replaced isolationist. In fact, it is amazing how seldom the United States acts unilaterally. Washington subsidizes every international aid agency, participates in almost every international organization, ratifies most international treaties and dominates all of the leading military alliances. Other countries routinely act unilaterally when they believe it to be in their interest. So, too, should the United States.



Ultimately, Washington owes loyalty to the citizens of the United States, not those of Japan, Germany, Uzbekistan, Kosovo or Australia. Although the United States cannot pursue its own interest without restraint, it is under no obligation to put the interest of other states first — especially when they are able to help themselves. So it is with AUSMIN.



Washington should act unilaterally and transform the alliance into a much looser cooperative relationship. For what purpose does AUSMIN exist? The former Red Navy is rusting in port. Japan is a most unlikely repeat aggressor. China is years away from possessing a serious offensive capability, let alone one able to threaten Australia. Vietnam and Malaysia make unlikely invaders. There is only Indonesia, which threatens a flotilla of refugees, not soldiers. What Australia most needs, then, is not a superpower alliance, but a more robust military and stronger regional ties.



ASEAN is better situated than the United States to handle a messy breakdown in Indonesia. Cooperative relationships with India and Japan could substitute for reliance on the United States in forging a naval force to deter a future aggressive China from interfering with international navigation. No doubt, Canberra might prefer to rely on the United States than to spend the money and take the effort to develop regional forces and relationships.



But a preference to be subsidized by Uncle Sam is no reason for Uncle Sam to do so. After all, having defended most of the known world for the last half century, Americans might suggest that it was time for, say, the British to man Alaskan radar stations, Japanese to patrol United States sea lanes, and Australians to maintain U.S. bases in Hawaii, Wake Island and Guam. Such requests likely would not meet with much favor anywhere, including in Canberra.



This latest AUSMIN consultation was filled with the usual platitudes and terms of endearment. But the United States and Australia have divergent interests. Canberra’s concerns are largely local. Instability in Indonesia, Fiji and Papua‐​New Guinea could spill over. Hostility among ASEAN states could greet Australian activism. None of these issues matter much to Washington, which is more concerned about relations with past and potential future superpowers: Russia, China and India.



The United States cannot accept a rival hegemonic domination of Eurasia; little else poses significant worry. The relationship between the United States and Australia should be based on their many shared cultural, economic and political interests. Harmonizing trade relations is perhaps the most important issue now before them. Military ties should be left to informal cooperation, such as intelligence sharing. Then the United States won’t be promising to defend yet another distant dependent. And Australia will be focusing on building the kind of regional relationships that will provide it with the greatest long‐​term security.
"
"**Gyms and non-essential shops in all parts of England will be allowed to reopen when lockdown ends next month, the prime minister has announced.**
Boris Johnson told the Commons that the three-tiered regional measures will return from 2 December, but he added that each tier will be toughened.
Spectators will be allowed to return to some sporting events, and weddings and collective worship will resume.
Regions will not find out which tier they are in until Thursday.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
The PM, who is self-isolating after meeting an MP who later tested positive for coronavirus, told MPs via video link he expected ""more regions will fall - at least temporarily - into higher levels than before"".
He said he was ""very sorry"" for the ""hardship"" that such restrictions would cause business owners.
Speaking later at a Downing Street briefing, Mr Johnson added that ""things will look and feel very different"" after Easter, with a vaccine and mass testing.
He warned the months ahead ""will be hard, they will be cold"" - but added that with a ""favourable wind"" the majority of people most in need of a vaccination might be able to get one by Easter.
Until then, the PM said, there would be a three-pronged approach of ""tough tiering, mass community testing, and [the] roll-out of vaccines"".
Describing how the tiers had become tougher, the PM said:
Where pubs and restaurants are allowed to open, last orders will now be at 10pm, with drinkers allowed a further hour to finish their drinks.
Indoor performances - such as those at the theatre - will also return in the lower two tiers, although with reduced capacity.
In terms of households mixing, in tier one a maximum of six people can meet indoors or outdoors; in tier two, there is no mixing of households indoors, and a maximum of six people can meet outdoors; and in tier three - the toughest tier - household mixing is not allowed indoors, or in most outdoor places.
In all tiers, exceptions apply for support bubbles. From 2 December, parents with babies under the age of one can form a support bubble with another household.
Mr Johnson said the tiers would now be a uniform set of rules, with no negotiations on additional measures for any particular region.
Measures in Scotland, Wales and Northern Ireland continue to be decided by the devolved administrations, but a joint approach to Christmas, involving all four nations, will be set out later in the week.
The prime minister said: ""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
For the third week running we have had some positive vaccine news, but the announcement about the toughened tiers is a reminder, if we needed any, that the next few months will be tough.
Ministers and advisers have been hinting for the past week that the tiers will be toughened - and that is exactly what has happened.
Attention will now naturally turn to which areas will be in which tiers.
Deciding that is a complex equation that will take into account whether the cases are going up or down, the percentage of tests that are positive, hospital pressures and infection rates among older age groups.
To give a flavour of how complex this is places in the North West and Yorkshire have some of the highest rates but they are falling the fastest.
London and the South East have lower rates and more hospital capacity but cases are going up.
Fine judgements will have to be made. We will find out on Thursday.
Mr Johnson also announced changes to sport for both spectators and participants.
While elite sport has continued behind closed doors during the lockdown, grassroots and amateur sport has been halted since 5 November.
From 2 December, outdoor sports can resume, while spectators will be allowed to return in limited numbers. Some organised indoor sports can also resume.
In the lowest risk areas, a maximum of 50% occupancy of a stadium, or 4,000 fans - whichever is smaller - will be allowed to return. In tier two, that drops to 2,000 fans or 50% capacity, whichever is smaller.
In tier three, fans will continue to be barred from grounds.
In tiers one and two, business events can also resume inside and outside with tight capacity limits and social distancing, as can indoor performances in theatres and concert halls, the government's plan says.
Labour leader Sir Keir Starmer described the government's return to the regional system as ""risky... because the previous three-tier system didn't work"".
He added that decisions on which areas will belong to each tier must be taken without delay - ""I just can't emphasise how important it is that these decisions are taken very quickly and very clearly so everybody can plan.
""That is obviously particularly important for the millions who were in restrictions before the national lockdown, because the message to them today seems to be 'you will almost certainly be back where you were before the national lockdown - probably in even stricter restrictions'.""
Helen Dickinson, of the British Retail Consortium, said shops would be ""relieved"" at the decision to allow them to reopen.
""Sage data has always highlighted that retail is a safe environment, and firms have spent hundreds of millions on safety measures including Perspex screens, additional cleaning, and social distancing and will continue to follow all safety guidance,"" she said.
But the UK hospitality industry warned the new rules ""are killing Christmas and beyond"" and said pubs, restaurants and hotels faced going bust.
Meanwhile, a further 15,450 positive coronavirus cases were recorded across the UK on Monday. There have also been a further 206 deaths within 28 days of a positive test. Figures can be lower on a Monday, due to a lag in reporting.
Earlier, it was announced that daily coronavirus tests will be offered to close contacts of people who have tested positive in England, as a way to reduce the current 14-day quarantine period.
Mr Johnson said people will be offered tests every day for a week - and they will not need to isolate unless they test positive.
He also said rapid tests will allow every care home resident to have up to two visitors tested twice a week."
"
Share this...FacebookTwitterEven with all the global warming we are supposedly having, summers are still too short. So I’m taking a few days off to enjoy some time outside.
Back early next week!
Here’s something to watch in the meantime (h/t Andrew Bolt):

Share this...FacebookTwitter "
nan
"During a period of ill health a few years back, as I struggled with depression and addiction, three of the four elements that helped me to recover were straightforward: psychiatry; psychotherapy; and the support of others. The fourth was more mysterious, and I set out on a journey to try to understand it. In my teenage years, I had developed an aptitude for drinking, as many adolescents do. But it was while working as a music journalist in my 20s that my addictions accelerated. By April 2012, at the age of 27, I was on my knees. I knew that the drink and drugs had to go: the highs were getting harder to achieve; the lows were becoming more dangerous, more self-destructive. I knew my mental and physical health were deteriorating, but I couldn’t seem to stop or cut down. After months of trying to do this alone, I found a rehabilitation programme and set out on a path to sobriety.  I had time on my hands, weekends loomed large and I needed to keep busy. So I started walking, wandering daily on Walthamstow Marshes in north-east London to watch the kestrels, caterpillars and the shaggy old heron. It made me feel safe and secure. Gradually, I realised that my mind needed these walks and I grew to rely on them. The natural world had become a kind of rehab: it soothed my rawness and patched me back together. In those very early sober months, when I was learning to live without the crutches I had used for years, I felt as if I was walking around without skin. The critical inner voices that had contributed to my periods of mental illness had folded me into myself. But on the marshes, I started to look up and outwards. Even though I was usually alone when I walked, I never felt lonely. I was realising that I belonged to a wider family of species, the matrix of life, from the spiders to the lichen and the cormorants to the coots. Nature picked me up by the scruff of my neck, and I rested in her care for a while. With the kind of urgent desire I had once had for mind-altering substances and music, I was now drawn to trees, birds, flowers and plants . I understood that time in nature softened the voices in my head and stabilised my mood, but I didn’t, at the beginning, understand what was happening to my body, brain and mind. I hadn’t realised that the essence of nature – the geometry, the scents, the sounds, the colours, the textures, the chemical makeup – could have such a life-changing power but, quite quickly, this became apparent. I began to plant things to watch them grow. One of the first things I noticed was that after gardening, digging my hands deep into the soil, I felt happy, upbeat, less stressed and generally more positive. Reading up on this, I saw that there may be a biological reason for it. One of the species of bacteria found in soil, M vaccae, has been found to affect the brain and increase stress resilience. In 2004, lung cancer patients at the Royal Marsden hospital in Surrey who were given an immunisation containing the bacteria reported feeling happier. As Dr Christopher Lowry, one of the leading neuroscientists in the field, said at the time: “These studies leave us wondering if we should all spend more time playing in the dirt.” Our microbiota are healthiest when they are diverse – and a diverse microbiota is influenced positively by an environment filled with organisms, which are found more abundantly outside. We are woven into the land, and wider ecosystems, more than we realise. Crucially, these old friends that we evolved with are able to treat or block chronic inflammation, which can also affect the brain and have a direct impact on mood. Studies have shown that spending just two hours in a forest can significantly lower levels of cytokines – an inflammation biomarker – in the blood, which could be caused by exposure to important organisms. It is all very well thinking about how more connection with the natural world would make people happier and healthier. But it is all the more pressing given the situation we find ourselves in. How can forest bathing be prescribed when forests are threatened and diminishing across the world? How can people spend more time in green spaces when many of our parks are in decline?  As I fell in love with the trees and the soil, I began to see how endangered much of nature had become, and how these opportunities to commune with other species were slipping through our fingers. Alongside our disconnection from nature is, of course, the fact that the natural world is rapidly vanishing; our time on this Earth is haunted by habitat destruction, species loss and climate breakdown. While revelling in the glory of the rest of nature, I also fell into a state of ecological grief, the name for the psychological response to the nature and climate emergency; a mourning for the communities, wildlife and landscapes that are disappearing and a fear of what is to come. Of course, people in many countries – including Britain’s flooded areas – are already suffering directly from the impact of the climate emergency. Others are experiencing anxiety, worry and dread about an anticipated future of ecological loss. The more I learned about the benefits a connection to nature can have on our minds, the more it seemed appalling that access to nature is so threatened in some places. The destruction of ancient woodlands across the country; the felling of much-loved street trees in Sheffield and other urban areas; children barely given opportunities to play in woods, fields and parks; the legislation that is failing to protect our rivers, streams and wild places; and the fact that the UK will miss almost all its biodiversity targets that were set a decade ago. Spending time in restorative natural environments is dependent, partly, on weather, which is in flux. A study of antidepressants by the environmental psychologist Terry Hartig has suggested that colder summers may constrain restorative activities that reduce stress and depressive symptoms. Rates of SSRI prescriptions increased for men and women in an unseasonably cooler July in Sweden. At the other extreme, emerging evidence links potentially dangerous high temperatures to increased mental health problems, illness and suicide. For those on the frontline of the climate crisis, the mental health impacts of ecological loss are already severe. In Kulusuk, Greenland, where the ice has melted, rates of depression, suicide and alcoholism have risen. In the Nunatsiavut region of Labrador, Canada, residents report feeling stressed, depressed and anxious because of the melted ice and changing weather patterns. It strikes at the very heart of identity. Farmers in the Australian wheat belt, whose farms have blown away in dust storms, have compared losing their farms to a death. As Glenn Albrecht, the Australian academic who coined the term “solastalgia”, to describe the distress caused by environmental change, points out, what is disordered isn’t ecological grief, eco-anxiety or global dread but “the world that is causing you to feel that way”. It is a natural response to loss – and it is likely to become more common. Our relationship with nature, even if it could be restored, isn’t quite as simple as a soothing, serene ramble in the wild. The wild barely exists. So what is the effect of biodiversity loss on our minds, our inner selves, and the collective psyche? When we walk in the woods, or by a lake, or spend time in a garden or park, evidence suggests that our parasympathetic nervous system is more likely to be activated. This is responsible for the “rest and digest” processes at work inside your body, associated with feelings of contentment, sleep and safety. The sympathetic nervous system’s main function is to stimulate the body’s reaction to stress, and ignore any non-essential business, such as immune function. Ideally, we want a balanced nervous system. After exposure to nature, our stress-recovery response is faster and more complete when compared with exposure to built environments. This has important consequences for our health at a time when stress-related diseases are on the increase. It also suggests that if we, as a society, are allowing trees to be cut down, or natural spaces in urban areas to be paved over, we are acting in a way that is damaging to public health. We need nature in order to recover from the stresses of life. There are many studies that link natural sounds – particularly diversity and richness of bird song – to decreased stress and a quicker recovery of a balanced nervous system. Even people under anaesthetic have been found to produce fewer chemical biomarkers associated with stress – such as amylase in saliva – when played a recording of soft wind or birdsong. A few years into my research, I had a baby and moved to a town in Hampshire. Near my new home, I found a beautiful, wild cemetery containing ancient ruins, where a majestic beech tree shines batter-yellow in the autumn sun. Rabbits flicker in the long grass, occasionally stopping long enough so I can see the black inkwells of their eyes; goblets of ancient epiphytes decorate the brick walls. When I look closer at the yellow blotches of lichen with my pocket microscope, I see tiny cities made of gold, with depth and dimension and cherry-red microscopic bugs invisible to the naked eye. The science of awe was first considered by a psychologist called Dacher Keltner at the University of California at Berkeley in a landmark paper published in 2003. Many experiences of awe in the modern world still come from an encounter with nature, despite our disconnection from it. Keltner found that awe increases happiness and lowers stress, perhaps unsurprisingly, but he also discovered just how powerful an experience of awe can be to the body and mind. The lab found that people were more ethical, kind and generous after feeling awe. Why? Perhaps from simply being in a good mood. But using functional MRI to look at the brains of participants, scientists saw that awe reduced activity in the default mode network, the area of the brain associated with a sense of self. It reminded me of a phenomenon I had heard about from recovering addicts: “addiction.fm” or the “washing-machine head”. In other words, the self-centred, negative ruminations that a substance can hush temporarily but, in the long run, will only feed. I visited the cemetery with renewed vim, searching for moments of awe and finding them everywhere. Around that time, I got postnatal depression and there was a frightening period when nature didn’t touch the sides. I felt nothing in the wild spaces that had previously brought me wonder and succour. But after medical help, I found that taking a walk through the cemetery instead of down a busy, polluted road, made me feel noticeably calmer and lighter. I was drawn to the effulgent green moss, the old, sprawling yews, the buzz of spotting a nuthatch or goldcrest or sparrowhawk. It turns out that these walks may have been affecting my brain in immediate and significant ways. Researchers in Edinburgh asked a group of people to walk from a busy urban space to a public park, or vice versa. Both groups started with a high stress response. What was interesting was how green space seemed to have a buffering effect on the stresses of the urban area. Those who started in green space and walked into a busy built-up space experienced an increase in alpha brainwaves – the electrical activity of the brain associated with relaxation. Nature seemed to undo the stress of the city, in the moment. When I started on this journey, I might have said that a relationship or connection with the rest of nature isn’t for everyone; that some people just don’t like the outdoors. But, in fact, research shows that background nature is essential across the population for good mental health. Without access to natural landscapes, rich in biodiversity, our potential for restoration, peace and psychological nourishment is sorely degraded. If we feel it, we can be galvanised by our ecological grief. We need a new relationship with the Earth, one that positions us not as conquerors, but co-tenants with wildlife and rivers and mountains and trees, respecting and caring for natural spaces because it is the right thing to do – and because we need the rest of nature for our lives and for our sanity. Losing Eden: Why Our Minds Need The Wild by Lucy Jones is published by Allen Lane on 27 February"
"

Bangkok is a crowded city, teeming with vehicles that observe the rules of the road more in the breach than in the practice. Despite this maelstrom, surprisingly, everyone seems to know how to handle the confusion, and even the ubiquitous tuk‐​tuks (three wheeled motorized open‐​air taxis) thrive in this atmosphere. Indeed, trying to impose order on this chaos would only cause more of a mess than it would solve.



Thailand now faces an economic situation similar to the haphazard streets of Bangkok, only the vehicles and all the drivers are making a dash for the road out of town. The epicenter of the Asian crisis of 1997, Thailand is now being confronted with a similar problem, as capital is starting to desert the country in droves and the baht has lost 15 percent of its value since last year.



The mood turned even gloomier last week as Chatu Mongol, the Bank of Thailand’s governor and the man who helped to pull Thailand out of the Asian crisis, was ousted after an argument with the government over interest rate policy. Investors have become unsettled by this change of management, and the question on everyone’s lips is, will Thailand follow Malaysia’s lead in imposing capital controls? And will these restrictions be able to control the flow of capital, or will they merely force a traffic jam and disrupt a functioning system?



The economic arguments against capital controls rest on both efficiency and equity considerations. Capital controls are highly ineffective for myriad reasons, not least of which is that they don’t work. The longer they are in place, the craftier people become in evading them, through methods such as falsification of invoices in trading, leads and lags in paperwork, substitution of exempted flows with restricted flows, and illegal methods (such as bribery and smuggling). Capital controls are the proverbial immovable object that the irresistible force always overwhelms.



More important than the theoretical arguments against capital controls is the instructive experience of Thailand’s neighbor, Malaysia, which imposed them at the end of the Asian crisis in October 1998 after much of the worst damage from the Asian crisis had passed. Ironically enough, Malaysia was well poised to weather the storm, having undergone a major banking crisis in 1986 and having embarked on a fairly successful restructuring immediately afterwards. Moreover, the short‐​term flows that were blamed for the volatility made up a relatively small part of Malaysia’s capital stock–the most portfolio equity ever accounted for was 5.7 percent of gross domestic product in 1993, and this had fallen to 4.38 percent in 1996.



Indeed, Malaysia’s imposition of capital controls seemed to be a nakedly political move by despot Mahathir Mohammed, who sought to consolidate his own grip on power and gave him a pretense to remove (and jail) his main political rival, Finance Minister Anwar Ibrahim. While they may have saved Mahathir’s political position, the arcane array of capital controls didn’t do their highly touted job. The international markets, in particular, were just waiting out the controls: in the first quarter this year, after Malaysia dismantled most of its restrictions, Malaysia’s central bank lost nearly $1 billion a month in foreign reserves, and speculation that the overvalued ringgit may be devalued is prompting more capital flight.



With Mahathir now in charge as the Finance Minister as well as Prime Minister (following the resignation of Daim Zainuddin at the beginning of May), the capital markets are not sanguine about the necessary reforms in Malaysia being pushed through. Thus, the effect of the controls was to poison the investment climate and merely postpone the inevitable. Somehow, this doesn’t appear to be a policy that should be replicated.



But perhaps Thailand has learned some of the lessons of the Asian crisis, as Finance Minister Somkid Jatusriptiak explicitly ruled out controls in an interview with the Financial Times last week. Since the Asian crisis, Thailand has faltered, but has generally remained open to the world and has undergone an extraordinary political shift. Malaysia, on the other hand, remains mired in its authoritarian ways and further estranged from the liberalization that the region needs.



Thailand’s eschewing of capital controls may prove to be the decisive lesson in a region that was once hailed as the new model for economic policy; as Malaysia has demonstrated, any attempts to rollback liberalization can leave a country stranded in an endless roundabout.
"
"Greta’s father, Svante, and I are what is known in Sweden as “cultural workers” – trained in opera, music and theatre with half a career of work in those fields behind us. When I was pregnant with Greta, and working in Germany, Svante was acting at three different theatres in Sweden simultaneously. I had several years of binding contracts ahead of me at various opera houses all over Europe. With 1,000km between us, we talked over the phone about how we could get our new reality to work. “You’re one of the best in the world at what you do,” Svante said. “And as for me, I am more like a bass player in the Swedish theatre and can very easily be replaced. Not to mention you earn so damned much more than I do.” I protested a little half-heartedly but the choice was made. A few weeks later we were at the premiere for Don Giovanni at the Staatsoper in Berlin and Svante explained his current professional status to Daniel Barenboim and Cecilia Bartoli. “So now I’m a housewife.” We carried on like that for 12 years. It was arduous but great fun. We spent two months in each city and then moved on. Berlin, Paris, Vienna, Amsterdam, Barcelona. Round and round. We spent the summers in Glyndebourne, Salzburg or Aix-en-Provence. As you do when you’re good at singing opera and other classical music. I rehearsed 20 to 30 hours a week and the rest of the time we spent together. Beata was born three years after Greta and we bought a Volvo V70 so we’d have room for doll’s houses, teddy bears and tricycles. Those were fantastic years. Our life was marvellous. One evening in the autumn of 2014, Svante and I sat slumped on our bathroom floor in Stockholm. It was late, the children were asleep. Everything was starting to fall apart around us. Greta was 11, had just started fifth grade, and was not doing well. She cried at night when she should be sleeping. She cried on her way to school. She cried in her classes and during her breaks, and the teachers called home almost every day. Svante had to run off and bring her home to Moses, our golden retriever. She sat with him for hours, petting him and stroking his fur. She was slowly disappearing into some kind of darkness and little by little, bit by bit, she seemed to stop functioning. She stopped playing the piano. She stopped laughing. She stopped talking. And she stopped eating. We sat there on the hard mosaic floor, knowing exactly what we would do. We would change everything. We would find the way back to Greta, no matter the cost. The situation called for more than words and feelings. A closing of accounts. A clean break. “How are you feeling?” Svante asked. “Do you want to keep going?” “No.” “OK. Fuck this. No more,” he said. “We’ll cancel everything. Every last contract,” Svante went on. “Madrid, Zurich, Vienna, Brussels. Everything.” One Saturday soon afterwards, we decide we’re going to bake buns, all four of us, the whole family, and we’re determined to make this work. It has to. If we can bake our buns as usual, in peace and quiet, Greta will be able to eat them as usual, and then everything will be resolved, fixed. It’s going to be easy as pie. Baking buns is after all our favourite activity. So we bake, dancing around in the kitchen so as to create the most positive, happiest bun-baking party in human history. But once the buns are out of the oven the party stops in its tracks. Greta picks up a bun and sniffs it. She sits there holding it, tries to open her mouth, but… can’t. We see that this isn’t going to work. “Please eat,” Svante and I say in chorus. Calmly, at first. And then more firmly. Then with every ounce of pent-up frustration and powerlessness. Until finally we scream, letting out all our fear and hopelessness. “Eat! You have to eat, don’t you understand? You have to eat now, otherwise you’ll die!” Then Greta has her first panic attack. She makes a sound we’ve never heard before, ever. She lets out an abysmal howl that lasts for over 40 minutes. We haven’t heard her scream since she was an infant. I cradle her in my arms, and Moses lies alongside her, his moist nose pressed to her head. Greta asks, “Am I going to get well again?” “Of course you are,” I reply. “When am I going to get well?” “I don’t know. Soon.” On a white sheet of paper fixed to the wall we note down everything Greta eats and how long it takes for her to eat it. The amounts are small. And it takes a long time. But the emergency unit at the Stockholm Centre for Eating Disorders says that this method has a good long-term success rate. You write down what you eat meal by meal, then you list everything you can eat, things you wish you could eat and things you want to be able to eat further down the line. It’s a short list. Rice, avocados and gnocchi. School starts in five minutes. But there isn’t going to be any school today. There isn’t going to be any school at all this week. Yesterday Svante and I got another email from the school expressing their “concern” about Greta’s lack of attendance, despite the fact that they were in possession of several letters from both doctors and psychologists explaining her situation. Again, I inform the school office of our situation and they reply with an email saying that they hope Greta will come to school as usual on Monday so “this problem” can be dealt with. But Greta won’t be in school on Monday. Because unless a sudden dramatic change occurs she’s going to be admitted to Sachsska children’s hospital next week. Svante is boiling gnocchi. It is extremely important that the consistency is perfect, otherwise it won’t be eaten. We set a specific number of gnocchi on her plate. It’s a delicate balancing act; if we offer too many our daughter won’t eat anything and if we offer too few she won’t get enough. Whatever she ingests is obviously too little, but every little bite counts and we can’t afford to waste a single one. Then Greta sits there sorting the gnocchi. She turns each one over, presses on them and then does it again. And again. After 20 minutes she starts eating. She licks and sucks and chews: tiny, tiny bites. It takes for ever. “I’m full,” she says suddenly. “I can’t eat any more.” Svante and I avoid looking at each other. We have to hold back our frustration, because we’ve started to realise that this is the only thing that works. We’ve explored all other tactics. Every other conceivable way. We’ve ordered her sternly. We’ve screamed, laughed, threatened, begged, pleaded, cried and offered every imaginable bribe. But this seems to be what works the best. She devotes the whole Christmas break to telling us about unspeakably awful incidents Svante goes up to the sheet of paper on the wall and writes: Lunch: 5 gnocchi. Time: 2 hours and 10 minutes. Not eating can mean many things. The question is what. The question is why. Svante and I look for answers. I spend the evenings reading everything I can find on the internet about anorexia and eating disorders. We’re sure it’s not anorexia. But, we keep hearing that anorexia is a very cunning disorder and will do anything to evade discovery. So we keep that door wide open. I speak endlessly to the children’s psychiatry service (BUP), the healthcare information service, doctors, psychologists and every conceivable acquaintance who may be able to offer the least bit of knowledge or guidance. At Greta’s school there’s a psychologist who is experienced with autism. She talks with both of us on the phone and says that a careful investigation must still be conducted, but in her eyes – and off the record – Greta shows clear signs of being on the autism spectrum. “High-functioning Asperger’s,” she says. Meeting after meeting follows where we repeat our story and explore our options. We talk away while Greta sits silently. She has stopped speaking with anyone except me, Svante and Beata. Everyone really wants to offer all the help they can but it’s as if there’s no help to be had. Not yet, at least. We’re fumbling in the dark. After two months of not eating Greta has lost almost 10kg, which is a lot when you are rather small to begin with. Her body temperature is low and her pulse and blood pressure clearly indicate signs of starvation. She no longer has the energy to take the stairs and her scores on the depression tests she takes are sky high. We explain to our daughter that we have to start preparing ourselves for a stay at the hospital, where it’s possible to get nutrition and food without eating, with tubes and drips. In mid-November there’s a big meeting at BUP. Greta sits silently. As usual. I’m crying. As usual. “If there are no developments after the weekend then we’ll have to admit you to the hospital,” the doctor says. On the stairs down to the lobby Greta turns round. “I want to start eating again.” All three of us burst into tears and we go home and Greta eats a whole green apple. But nothing more will go down. As it turns out, it’s a little harder than you think to just start eating again. We take a few careful, trial steps and it works. We inch forward. She eats tiny amounts of rice, avocado and bananas. We take our time. And we start on sertraline, an antidepressant. Svante and Greta have been at the end-of-term ceremony at school where they tried to make themselves invisible in the corridors and stairwells. When students openly point and laugh at you – even though you’re walking alongside your parent – then things have gone too far. Way too far. At home in the kitchen, Svante explains to me what they’ve just experienced while Greta eats her rice and avocado. I get so angry at what I hear that I could tear down half the street we live on with my bare hands, but our daughter has a different reaction. She’s happy it’s in the open. She devotes the whole Christmas break to telling us about unspeakably awful incidents. It’s like a movie montage featuring every imaginable bullying scenario. Stories about being pushed over in the playground, wrestled to the ground, or lured into strange places, the systematic shunning and the safe space in the girls’ toilets where she sometimes manages to hide and cry before the break monitors force her out into the playground again. For a full year, the stories keep coming. Svante and I inform the school, but the school isn’t sympathetic. Their understanding of the situation is different. It’s Greta’s own fault, the school thinks; several children have said repeatedly that Greta has behaved strangely and spoken too softly and never says hello. The latter they write in an email. They write worse things than that, which is lucky for us, because when we report the school to the Swedish schools inspectorate we’re on a firm footing and there’s no doubt that the inspectorate will rule in our favour. I explain to Greta that she’ll have friends again, later. But her response is always the same. “I don’t want to have a friend. Friends are children and all children are mean.” Greta’s pulse rate gets stronger and finally the weight curve turns upwards strongly enough for a neuropsychiatric investigation to begin. Our daughter has Asperger’s, high-functioning autism and OCD, obsessive-compulsive disorder. “We could formally diagnose her with selective mutism, too, but that often goes away on its own with time,” the doctor tells us. We aren’t surprised. Basically, this was the conclusion we drew several months ago. On the way out, Beata calls to tell us she’s having dinner with a friend, and I feel a sting of guilt. Soon we’ll take care of you too, darling, I promise her in my mind, but first Greta has to get well. Summer is coming, and we walk the whole way home. We almost don’t even need to ration the burning of calories any more. Six months after Greta received her diagnosis, life has levelled out into something that resembles an everyday routine. She has started at a new school. I’ve cleared my calendar and put work on the back burner. But while we’re full up with taking care of Greta, Beata’s having more and more of a tough time. In school everything is ticking along. But at home she falls apart, crashes. She can’t stand being with us at all any more. Everything Svante and I do upsets her and in our company she can lose control. She is clearly is not feeling well. One day near her 11th birthday I find her standing in the living room, hurling DVDs from the bookshelf down the spiral staircase to the kitchen. “You only care about Greta. Never about me. I hate you, Mum. You are the worst bloody mother in the whole world, you bloody fucking bitch,” she screams as Jasper the Penguin hits me on the forehead. It’s autumn 2015 when Beata undergoes an evaluation for various neurodevelopmental disorders. She is diagnosed with ADHD, with elements of Asperger’s, OCD and ODD [oppositional defiant disorder]. Now that she has the diagnosis it feels like a fresh start for her, an explanation, a redress, a remedy. At school she has marvellous teachers who make everything work. She doesn’t have to do homework. We drop all activities. We avoid anything that may be stressful. And it works. Whatever happens we must never meet anger with anger, because that, pretty much always, does more harm than good. We adapt and we plan, with rigorous routines and rituals. Hour by hour. We try to find habits that work. The fact that our children finally got help was due to a great many factors. In part it was about existing care, proven methods, advice and medication. It was also thanks to our own toil, patience, time and luck that Greta and Beata found their way back on their feet. However, what happened to Greta in particular can’t be explained simply by a psychiatric label. In the end, she simply couldn’t reconcile the contradictions of modern life. Things simply didn’t add up. We, who live in an age of historic abundance, who have access to huge shared resources, can’t afford to help vulnerable people in flight from war and terror – people like you and me, but who have lost everything. In school one day, Greta’s class watches a film about how much rubbish there is in the oceans. An island of plastic, larger than Mexico, is floating around in the South Pacific. Greta cries throughout the film. Her classmates are also clearly moved. Before the lesson is over the teacher announces that on Monday there will be a substitute teaching the class, because she’s going to a wedding over the weekend, in Connecticut, right outside of New York. “Wow, lucky you,” the pupils say. Out in the corridor the trash island off the coast of Chile is already forgotten. New iPhones are taken out of fur-trimmed down jackets, and everyone who has been to New York talks about how great it is, with all those shops, and Barcelona has amazing shopping too, and in Thailand everything is so cheap, and someone is going with her mother to Vietnam over the Easter break, and Greta can’t reconcile any of this with any of what she has just seen. She saw what the rest of us did not want to see. It was as if she could see our CO2 emissions with her naked eye. The invisible, colourless, scentless, soundless abyss that our generation has chosen to ignore. She saw all of it – not literally, of course, but nonetheless she saw the greenhouse gases streaming out of our chimneys, wafting upwards with the winds and transforming the atmosphere into a gigantic, invisible garbage dump. She was the child, we were the emperor. And we were all naked. ‘You celebrities are basically to the environment what anti immigrant politicians are to multicultural society,” Greta says at the breakfast table early in 2016. I guess it’s true. Not just of celebrities, but of the vast majority of people. Everyone wants to be successful, and nothing conveys success and prosperity better than luxury, abundance and travel, travel, travel. Greta scrolls through my Instagram feed. She’s angry. “Name a single celebrity who’s standing up for the climate! Name a single celebrity who is prepared to sacrifice the luxury of flying around the world!” I was a part of the problem myself. Only recently I had been posting sun-drenched selfies from Japan. One “Good morning from Tokyo” and tens of thousands of “likes” rolled in to my brand-new iPhone. Something started to ache inside of me. Something I’d previously called travel anxiety or fear of flying but which was now taking on another, clearer form. On 6 March 2016 I flew home from a concert in Vienna, and not long after that I decided to stay on the ground for good. A few months later we walked home from the airport shuttle having met Svante and Beata off a flight from Rome.“You just released 2.7 tonnes of CO2,” Greta says to Svante. “And that corresponds to the annual emissions of five people in Senegal.” “I hear what you’re saying,” Svante says, nodding. “I’ll try to stay on the ground from now on, too.” Greta started planning her school strike over the summer of 2018. Svante has promised to take her to a building supplier’s to buy a scrap piece of wood that she can paint white and make a sign out of. “School Strike for the Climate”, it will say. And although more than anything we want her to drop the whole idea of going on strike from school – we support her. Because we see that she feels good as she draws up her plans – better than she has felt in many years. Better than ever before, in fact. On the morning of 20 August 2018, Greta gets up an hour earlier than on a regular school day. She has her breakfast. Fills a backpack with schoolbooks, a lunchbox, utensils, a water bottle, a cushion and an extra sweater. She has printed out 100 flyers with facts and source references about the climate and sustainability crisis.  She walks her white bicycle out of the garage and rolls off to parliament. Svante cycles a few metres behind her, with her home-made sign under his right arm The weather is rather lovely. The sun is rising behind the old town and there is little chance of rain. The cycle paths and pavements are filled with people on their way to work and school. Outside the prime minister’s office, Greta stops and gets off her bicycle. Svante helps her take a picture before they lock the bicycles. Then she nods an almost invisible goodbye to Dad and, with the sign in her arms, staggers around the corner towards the government block where she stops and leans the sign against the greyish-red granite wall. Sets out her flyers. Settles down. She asks a passerby to take another picture with her phone and posts both pictures on social media. After a few minutes the first sharing on Twitter starts. The political scientist Staffan Lindberg retweets her post. Then come another two retweets. And a few more. The meteorologist Pär Holmgren. The singer-songwriter Stefan Sundström. After that, it accelerates. She has fewer than 20 followers on Instagram and not many more on Twitter. But that’s already changing. Now there is no way back. A documentary film crew shows up. Svante calls and tells her that the newspaper Dagens ETC has been in touch with him and are on their way. Right after that [another daily newspaper] Aftonbladet shows up and Greta is surprised that everything is moving so fast. Happy and surprised. She wasn’t expecting this. Ivan and Fanny from Greenpeace show up and ask Greta if everything is OK. “Can we help with anything?” they ask. “Do you have a police permit?” Ivan asks. She doesn’t. She didn’t think a permit would be needed. But evidently it is. “I can help you,” Ivan says. Greenpeace is far from alone in offering its support. Everyone wants to do their utmost to help out. But Greta doesn’t need any help. She manages all by herself. She is interviewed by one newspaper after the next. The simple fact that she is talking to strangers without feeling unwell is an unexpected joy for us parents. Everything else is a bonus. The first haters start to attack, and Greta is openly mocked on social media. She is mocked by anonymous troll accounts, by rightwing extremists. And she is mocked by members of parliament. But that’s no surprise. Svante stops by to make sure that everything is OK. He does this a couple of times every day. Greta stands by the wall and there are a dozen people around her. She looks stressed. The journalist from [newspaper] Dagens Nyheter asks whether it’s OK if they film an interview, and Svante sees out of the corner of his eye that something is wrong. “Wait, let me check,” he says, and takes Greta behind a pillar under the arch. Her whole body is tense. She is breathing heavily, and Svante says that there’s nothing to worry about. “Let’s go home now,” he says. “OK?” Greta shakes her head. She’s crying. “You don’t need to do any of this. Let’s forget about this and get out of here.” But Greta doesn’t want to go home. She stands perfectly still for a few seconds. Breathes. Then she walks around in a little circle and somehow pushes away all that panic and fear that she has been carrying inside her for as long as she can remember. After that she stops, and stares straight ahead. Her breathing is still agitated and tears are running down her cheeks. “No,” she says. “I’m doing this.” We monitor how Greta is feeling as closely as we can. But we can’t see any signs that she’s feeling anything but good. She sets the alarm clock for 6.15am and she’s happy when she gets out of bed. She’s happy as she cycles off to parliament, and she’s happy when she comes home in the afternoon. During the afternoons she catches up on schoolwork and checks social media. She goes to bed on time, falls asleep right away and sleeps peacefully the whole night long. Eating, on the other hand, is not going well. “There are too many people and I don’t have time. Everyone wants to talk all the time.” “You have to eat,” Svante says. Greta doesn’t say anything. Food is a sensitive topic. The most difficult one. But on the third day something else happens. Ivan from Greenpeace stops by again. He’s holding a white plastic bag. “Are you hungry, Greta? It’s noodles. Thai,” he says. “Vegan. Would you like some?” He holds out the bag and Greta leans forward and reaches for the food container. She opens the lid and smells it a few times. Then she takes a little bite. And another. No one reacts to what’s happening. Why would they? Why would it be remarkable for a child to be sitting with a bunch of people eating vegan pad thai? Greta keeps eating. Not just a few bites but almost the whole serving. Greta’s energy is exploding. There doesn’t seem to be any outer limit, and even if we try to hold her back she just keeps going. By herself.  Beata sits with Greta one day in front of parliament. But this is Greta’s thing. Not hers. The sudden fuss over her big sister is not easy to handle. Beata sees that Greta suddenly has 10,000 followers on Instagram, and we all think that’s crazy. But Beata handles it well. Even when her own feed is filled with comments about Greta, and can you tell her this and that. All everyone suddenly cares about is Greta, Greta and Greta. “It’s nuts,” Beata says one afternoon after school. “It’s exactly like Beyoncé and Jay-Z,” she states, with an acerbic emphasis. “Greta is Beyoncé. And I’m Jay-Z.” We get death threats on social media, excrement through the letter box, and social services report that they have received a great number of complaints against us as Greta’s parents. But at the same time they state in the letter that they “do NOT intend to take any action”. We think of the capital letters as a little love note from an anonymous official. And it warms us. More and more people are keeping Greta company in front of parliament. Children, adults, teachers, retirees. One day an entire class of elementary-school pupils stops and wants to talk, and Greta has to walk away for a bit. Feels mild panic. She steps aside and starts crying. She can’t help it. But after a while she calms herself down and goes back and greets the children. Afterwards she explains that she has a hard time associating with children sometimes because she has had such bad experiences. “I’ve never met a group of children that hasn’t been mean to me. And wherever I’ve been I’ve been bullied because I’m different.” Several times a day people come up and say that they have stopped flying, parked the car or become vegans thanks to her. To be able to influence so many people in such a short time is bewildering in a good way. The phenomenon keeps growing. Faster and faster by the hour. In the run-up to the end of the strike, Greta is being followed by TV crews from the BBC, German ARD and Danish TV2. Altogether 1,000 children and adults sit with Greta on the last day of the school strike. And media from several different countries report live from Mynttorget Square. She has succeeded. Some say that she alone has done more for the climate than politicians and the mass media have in years. But Greta doesn’t agree. “Nothing has changed,” she says. “The emissions continue to increase and there is no change in sight.” At three o’clock Svante comes and picks her up and they walk together over to the bicycles outside Rosenbad. “Are you satisfied?” Svante asks. “No,” she says, with her gaze fixed on the bridge back towards the old town. “I’m going to continue.” The next day is Saturday 8 September. It’s the day before the Swedish parliamentary elections and Greta is going to speak at the People’s Climate March in Stockholm. She has only given one speech before at a small event. Prior to that she’d never spoken in front of more people than fit in a classroom, and on those few occasions she had not exactly seemed at ease. There are a lot of people in the park for the march and the rally. Almost 2,000 have crowded together at the stage and more are on their way. Somehow there’s a different feeling about this protest. It doesn’t feel the same as usual. It feels as if something might happen. Soon. It’s no longer just the familiar faces. The regulars. The activists. The Greenpeace volunteers in polar-bear suits. Here, suddenly, are all conceivable kinds of people and characters. People who might have all sorts of jobs. “This is my first demonstration,” states a well-dressed man in his 40s. “Mine too,” a woman next to him says, with a laugh. The host introduces Greta and she walks slowly but steadily into the middle of the stage. The audience cheers. Svante, on the other hand, is scared out of his wits. What will happen now? Will she start crying? Is she going to run away? He feels like an awful parent for not putting his foot down and saying “No” from the start. All this is starting to feel too big and unreal. But Greta is as calm as can be. She takes the speech out of her pocket and looks out over the sea of people. Then she grasps the microphone and starts speaking. “Hi, my name is Greta,” she says in Swedish. “I am going to speak in English now. And I want you to take out your phones and film what I’m saying. Then you can post it on your social media.” “My name is Greta Thunberg and I am 15 years old. And I have schoolstriked for the climate for the last three weeks. Yesterday was the last day. But…” She pauses. “We will go on with the school strike. Every Friday, as from now, we will sit outside the Swedish parliament until Sweden is in line with the Paris agreement.” The crowd cheers. Greta continues. “I urge all of you to do the same. Sit outside your parliament or local government, wherever you are, until your country is on a safe pathway to a below-two-degree warming target. Time is much shorter than we think. Failure means disaster.” Her voice is steady and there are no signs of nervousness. She appears to be at ease up there. She even smiles sometimes. “The changes required are enormous and we must all contribute in every part of our everyday life. Especially us in the rich countries, where no nation is doing nearly enough.”  The audience stands up. Shouting, applauding. The ovation doesn’t stop. And Greta is smiling the most beautiful smile I have ever seen her smile. I’m watching everything from a live stream on my phone in the hallway outside the dressing rooms at the Oscarsteatern. The tears keep coming. • This is an edited extract from Our House Is on Fire: Scenes of a Family and a Planet in Crisis by Malena and Beata Ernman, Greta and Svante Thunberg, published by Penguin on 5 March (£16.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15"
"There has been a huge increase in attention recently to the problem of wildlife poaching, mostly from the stream of grisly stories from Africa about rhino and elephants illegally killed for their horn and ivory.  At the same time there has been a growing awareness of trophy hunting, with pictures of hunters, sometimes minor celebrities, posing next to their kill prompting furious outcry. But these two forms of wildlife are being wrongly lumped together. Despite both resulting in an animal’s death, they are entirely different. Poaching is the illegal killing of wildlife, undertaken for reasons that may include revenge, meat for food or sale, tradition or money.  Poachers might be poor locals from the area to foreigners capitalising on the lucrative illegal wildlife trade. By contrast, trophy hunting is the entirely legal killing of wildlife, often carried out by rich foreigners for sport and enjoyment. Both result in one animal fewer in the wild, but the similarity ends there. Poaching is uncontrolled and unmonitored. With wildlife seen as a common pool resource by poachers, animals suffer from the tragedy of the (illegal) commons where poachers may feel that if they do not kill (and benefit from) that elephant, someone else will. The result is massive over-exploitation. The basic economic fact of rising prices due to high demand and low supply means that the rarer an animal gets, the higher the price. The incentive drives poachers to kill as many animals as possible. Trophy hunting on the other hand is typically a strictly controlled, monitored and regulated business where safari game parks must apply for government-issued permits for the number of animals they wish to hunt.  If animals are to be taken out of the country, further permits are needed for their transport. So we have a good idea of the number of animals trophy hunting kills worldwide, and because no business wants their income to stop, game parks tend to look after their wildlife very well, which sees population numbers increase over the years. For comparison, take a typical livestock farmer: he wouldn’t take all his cattle to market at once as there would be none left to sell the following trip. It makes sense to only sell a few at a time, and in the meantime grow his herd in order to have more animals to take on the next trip and so more to sell. Safari game parks see their wildlife in a similar fashion and prefer to increase their game animals.  In fact, time and again it has been shown that trophy hunting operations can expand wildlife populations rather than reduce them. As they are able to legally reap the economic rewards from the wildlife they look after, the game parks have an incentive to conserve these species. And some benefits trickle down to the local community – for example creating more jobs – which means locals have an incentive not to poach wildlife illegally because now they can benefit from its value through the legal economy.  This approach has worked wonders in Namibia through the conservancy model, in which local communities living around the wildlife to be protected are put in charge of their wildlife. They are able to offer trophy hunts to tourists, and so reap the benefits.  This has dramatically reduced poaching, and some trophy species populations are booming as a result. So the call to ban trophy hunting or prevent hunters from bringing home their trophies is worrying. For example, the recent ban of ivory imports from Tanzania and Zimbabwe to the US may have a dramatically negative effect on those countries elephant populations. By taking away the opportunity for local communities to benefit from these animals the land is likely to be turned over to agriculture.  In a previous instance, Kenya stopped trophy hunting in the 1970s, and land that had previously been set aside for wildlife was converted to livestock and crop farming. The result was widespread population declines of all game species, as crops and cattle now held more value. Whether this is the result in Botswana and Zambia, where trophy hunting was recently banned, remains to be seen. Not all forms of wildlife killing are the same, and we must be careful to ensure the differences are understood. It’s easy for Westerners to jump to conclusions about wildlife management, to see a picture of a dead lion and suppose any form of killing to be wrong. But if we take a step back and see the big picture about how these communities – isolated, poor – have to live with dangerous animals such as elephants and lions, we must understand that the picture is far more complex.  Indeed, as the Namibian example has shown, carefully managed trophy hunting may be one way to reduce poaching of wildlife."
"Landslides don’t attract the same media attention as more familiar geological hazards such as earthquakes and volcanoes. And yet they can be just as disastrous and, in fact, 2014 has been a particularly bad year.   In Hiroshima, Japan, a series of landslides has left 39 people confirmed dead and a further 52 missing. In March a hillside collapsed in Washington state, US, leaving 43 dead, and in May massive mudslides in Afghanistan caused several thousand deaths. In early August, landslides in Nepal left almost 200 dead or missing. Landslides can vary greatly in speed, water content and size. Different landslides can look very different and can vary greatly in their destructive power – size is not everything. Nevertheless, they pose a significant hazard to human life, buildings and transport routes.  Statistics about deaths from landslide-generated disasters can be a little difficult to come by, since some agencies group “wet” landslides with floods and damage caused by landslides generated during earthquakes are often classed as earthquake damage.  The recent Japanese landslides, for example, are mud flows or debris flows (a watery mix of rock and soil) generated from a slope collapsing further up river. Heavy rainfall had caused large volumes of slope collapse material to be incorporated into the river waters, giving the mud flows and debris flows, which have overwhelmed villages within the mountain valleys. Villages built a little further up from the floor of the valleys will be much less prone to the destructive debris flows, which occur in the base of the mountain valleys. Landslides tend to be most frequent and destructive in steep mountainous areas, as they are an expression of a natural process which reduces steep slopes to less steep slopes. Data collected over many years suggests that landslides are ranked 7th in the natural disaster table well after the major historical killers of droughts, floods and storms, but close on the heels of earthquakes and volcanic eruptions.  Landslides generally require a trigger, most commonly extreme rainfall or large earthquakes. Earthquakes initiate landslides by locally – and very briefly – changing the gravity experienced by a slope, which tips it beyond its stability point. Extreme rainfall temporarily drives the water pressure within a slope to a critical level; the stresses within the slope then exceed their stability point and the land begins to slip down the slope. Landslides are then driven simply by gravity, often assisted by a loss of strength in the region at the base of the slide.  So parts of the world with steep slopes, intense rainfall and large earthquakes tend to be most prone to this kind of geological disaster. But they aren’t always massive, deadly disasters. Smaller, often more benign landslides can be initiated by roadway construction, building works, river or wave/tidal undercutting of slopes. If engineers or builders mess up and make a slope too steep, it may no longer be held up by the intrinsic strength of the rock. In the same way, apparently simple things like leaky water pipes or inadequate drainage on man-made slopes can also start landslides. This is particularly so water is either retained in, or drains particularly slowly from, the rock or soil of the slope. Certain kinds of clays are particularly notable for these features – and this seems to have exacerbated the landslides at Hiroshima.  Such conditions are generally well understood by geo-technical engineers, so can normally be predicted in artificial slopes. Next time you are on a train or car look out for adjacent slopes which have lines of gravel or rock-fill running down them to drain the water away. These have been assessed by engineers as potential unstable slopes, should the area be exposed to heavy rainfall over long periods of time. A lack of trees can also make steep slopes more prone to landslide, since trees naturally intercept and slow heavy rainfall and their roots help bind the soil together. This is another example of natural and man-man factors overlapping – environmentalists blamed deforestation for a recent landslide in India which killed 30 people. These facts gives clues as to how to limit landslides (triggered by heavy rainfall) on natural slopes, by managing water flow across slopes, limiting water ingress into slopes and by planting tree cover to slow water delivery into water courses and so provide natural slope binding. The most destructive landslides of all are those that end up in water (or occur on underwater slopes) and generate tsunamis. For example, this kind of destruction can happen if a volcano flank collapses into water while erupting. Indeed, the largest tsunami ever recorded was set off by a landslide in Lituya Bay, Alaska, in 1958. Waves reached a height of more than 500m, far taller than any skyscraper built at the time, but killed just five people in the sparsely populated fjord. Fortunately such disasters have been very rare in human history – the 1792 Unzen eruption, earthquake, landslide and tsunami, which killed 15,000 people in Japan is a notable instance. However we know from geological evidence on the sea floor, for example around the Canary Islands, that mega-tsunamis must have been generated by similar slides – landslides that would make 2014’s crop seem tiny by comparison."
"
Of course the big news today is the 8.8 earthquake in Chile and the Tsunami warning stemming from it. There’s not much I could add that’s not already being covered, but I thought this image from the American West  Coast and Alaska Tsunami Warning Center was interesting. They posted this map with  estimated arrival times of tsunami waves generated by the 8.8 earthquake  earthquake off the coast of Chile:

Even more interesting is the map they published of the path of energy distribution in the waves. It looks like Hawaii will dodge the worst of it:

The image above depicts wave height in centimeters.
I’m not posting direct links to these images at the center since I don’t want their server to be overwhelmed, so I’ve stored them locally.
It looks like the Aleutian islands may get some significant portion of this as will New Zealand.
The Tsunami Warning Center has a very detailed list of estimated  arrival times for waves generated by Saturday’s 8.8 magnitude  earthquake at many locations along the west coast of the United States. On the US West coast, the first waves to arrive will be in San Diego just after noon PST.
BONUS:
Quite possibly the stupidest science headline ever,  from MSNBC and LiveScience:
Big quake question: Is nature out of control?
and
Chile Earthquake: Is Mother Nature Out of Control?
Newsflash: Nature has never been within our control.
This article at Livescience which MSNBC picked up was written by Jeanna  Bryner, 
who has also written articles on “The Perils of Text Messaging While Walking” and “Wanted:  The Equation of Love”
Her apparent justification for the current headline:
“One scientist, however, says that relative to a time period in the past,  the Earth has been more active over the past 15 years or so.”
Since the introduction of the Internet and proliferation of live global satellite news coverage, also in the past 15-20 years, we certainly do hear more about what goes on around the planet, often within minutes of occurrence. Does that mean the planet is getting more active? Not neccessarily, but you can draw the conclusion that are reporting system has improved dramatically during that period.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8dd9fedf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI get mail, too.
Though not very friendly at times. But here’s one from reader Jimbo who delivers quite a neat collection of contradictory reports. No matter if it’s hot-cold, wet-dry, green-brown, windy-calm, bad-nice – its all due to manmade climate change.
=====================================================
Hi Pierre Gosselin,
The last time you posted the following list on Notrickszone a Warmist [Hunneycut?] attacked it as being from news stories. I have now changed all the links to point to PEER REVIEWED materials and made it even longer.
Partly referenced and inspired by Numberwatch.
What I want to know from ‘Warmists’ is what would falsify the theory of Anthropogenic Global Warming?
Jimbo
LIST
Amazon dry season greener
Amazon dry season browner
Avalanches may increase
Avalanches may decrease – wet snow more though [?]
Bird migrations longer
Bird migrations shorter
Bird migrations out of fashion
Boreal forest fires may increase
Boreal forest fires may continue decreasing
Chinese locusts swarm when warmer
Chinese locusts swarm when cooler
Columbia spotted frogs decline
Columbia spotted frogs thrive in warming world
Coral island atolls to sink [?]
Coral island atolls to rise [? – ?]
Earth’s rotation to slow down
Earth’s rotation to speed up
East Africa to get less rain
East Africa to get more rain – pdf
Great Lakes less snow
Great Lakes more snow
Gulf stream slows down
Gulf stream speeds up a little
Indian monsoons to be drier
Indian monsoons to be wetter
Indian rice yields to decrease – full paper
Indian rice yields to increase
Latin American forests may decline
Latin American forests have thrived in warmer world with more co2!
Leaf area index reduced [1990s]
Leaf area index increased [1981-2006]


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Malaria may increase
Malaria may continue decreasing
Malaria in Burundi to increase
Malaria in Burundi to decrease [?]
North Atlantic cod to decline
North Atlantic cod to thrive
North Atlantic cyclone frequency to increase
North Atlantic cyclone frequency to decrease – full pdf
North Atlantic Ocean less salty
North Atlantic Ocean more salty
Northern Hemisphere ice sheets to decline [? – ? – ?]
Northern Hemisphere ice sheets to grow [?]
Plant methane emissions significant
Plant methane emissions insignificant
Plants move uphill
Plants move downhill [?]
Sahel to get less rain
Sahel to get more rain
Sahel may get more or less rain
San Francisco less foggy
San Francisco more foggy
Sea level rise accelerated
Sea level rise decelerated – full pdf
Soil moisture less
Soil moisture more
Squids get smaller
Squids get larger
Stone age hunters may have triggered past warming [?]
Stone age hunters may have triggered past cooling
Swiss mountain debris flow may increase
Swiss mountain debris flow may decrease
Swiss mountain debris flow may decrease then increase in volume
UK may get more droughts
UK may get more rain
Wind speed to go up [?]
Wind speed slows down [?]
Wind speed to speed up then slow down
Winters maybe warmer [? – ?]
Winters maybe colder ;O)
—END—
=================
Update: Also see Joe d’Aleo’s link here:
http://icecap.us/index.php/go/joes-blog/ten_major_failures_of_so_called_consensus_science/
Share this...FacebookTwitter "
"
Confident predictions of catastrophe are unwarranted.

A commentary by Richard S. Lindzen in the WSJ
Is there a reason to be alarmed by the prospect of global warming? Consider that the measurement used, the globally averaged temperature anomaly (GATA), is always changing. Sometimes it goes up, sometimes down, and occasionally—such as for the last dozen years or so—it does little that can be discerned.
Claims that climate change is accelerating are bizarre. There is general support for the assertion that GATA has increased about 1.5 degrees Fahrenheit since the middle of the 19th century. The quality of the data is poor, though, and because the changes are small, it is easy to nudge such data a few tenths of a degree in any direction. Several of the emails from the University of East Anglia’s Climate Research Unit (CRU) that have caused such a public ruckus dealt with how to do this so as to maximize apparent changes.
The general support for warming is based not so much on the quality of the data, but rather on the fact that there was a little ice age from about the 15th to the 19th century. Thus it is not surprising that temperatures should increase as we emerged from this episode. At the same time that we were emerging from the little ice age, the industrial era began, and this was accompanied by increasing emissions of greenhouse gases such as CO2, methane and nitrous oxide. CO2 is the most prominent of these, and it is again generally accepted that it has increased by about 30%.
The defining characteristic of a greenhouse gas is that it is relatively transparent to visible light from the sun but can absorb portions of thermal radiation. In general, the earth balances the incoming solar radiation by emitting thermal radiation, and the presence of greenhouse substances inhibits cooling by thermal radiation and leads to some warming.
That said, the main greenhouse substances in the earth’s atmosphere are water vapor and high clouds. Let’s refer to these as major greenhouse substances to distinguish them from the anthropogenic minor substances. Even a doubling of CO2 would only upset the original balance between incoming and outgoing radiation by about 2%. This is essentially what is called “climate forcing.”
The full article may be found here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90aa8677',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Britain, like many countries, has already witnessed the establishment of many non-native species of plants and animals, and about 15% have become problematic and so termed “invasive”. Some were deliberately introduced, for example plants brought for their attractive flowers such as giant hogweed (Heracleum mantegazzianum), while others have stowed away on ships or escaped from captivity.  These new arrivals can be considered one of the major threats to native biodiversity. Some prey directly on native species, compete for the same food or habitat, or bring with them diseases to which native species have no defence. Some generate such profound effects on their new environment they are called “ecosystem engineers”. This is a threat that has to be taken seriously and managed.  We brought together a group of experts to examine the changing landscape and predict what species may pose problems in the next 10 years in order to be better prepared.  While our international group of ecologists covered potential threats from land, sea, and freshwater habitats, we looked only at the impact they might have on biodiversity. More often than not however, it’s likely that new invasive alien species threats would have an impact on the economy too, and could have also an impact on human health. Our experts drew up a list of species with the potential to arrive in Great Britain and become problematic within the next ten years. A total of 591 non-native species were considered, of which 93 were considered to constitute at least a medium risk to native biodiversity, and a final ranked list of 30 species was compiled. Seen as the most serious threat, at the top of that list is the quagga mussel (Dreissena rostriformis bugensis). It was unanimously awarded the highest score for the threat it poses based on its arrival, its ability to establish populations, and the impact it has on its new environment. This ecosystem engineer is capable of dramatically altering the aquatic environments in which it thrives.  Quagga mussels are extremely efficient filter-feeders, changing the chemical nature of the water, resulting in water becoming clearer. A seemingly simple change, this can have serious cascading effects that affect the vital phytoplankton and zooplankton upon which the food webs are based. This in turn could stimulate the competitive release of cyanobacteria which would lead to an increase in frequency of toxic algal blooms. Argentine ants (Linepithema humile) are another example of an ecosystem engineer highlighted as an impending threat. They form enormous super-colonies, with one such super-colony recorded as covering 6000km from Spain to Italy.  The Asian shore crab (Hemigrapsus sanguineus) was a species ranked within the top ten, and has subsequently been confirmed as a new arrival to Britain, found on beaches in Wales and Kent.   Other species within the top ten include the raccoon (Procyon lotor), the African scared ibis (Threskiornis aethiopicus) and the Asian hornet (Vespa Vespa velutina).   Racoons are considered a threat due to their highly adaptable and intelligent nature, and the possibility of establishment in Britain. This could come from escape from zoos or private collections.  The African sacred ibis is a wading bird capable of heavily preying on birds, fish, amphibians and invertebrates, and a breeding population is already established in France.  The Asian hornet is also now well-established on the continent and is a voracious predator of pollinating insects, including honeybees and is anticipated to arrive in Great Britain from the continent. The GB Non-native Species Secretariat has been forward-thinking in developing an alert system for the arrival of the Asian hornet. It’s hoped this system will, in time, provide a means to watch for many potential new arrivals to Great Britain.  It’s essential that we are prepared not only to manage problematic invasive species effectively but also to work to prevent new arrivals establishing populations that could cause threats in the first place.  Increasing our understanding of invasion biology is a necessary building block that will underpin action we take in the future. A draft EU regulation on invasive species has been adopted and will influence national policy across Europe in coming years, and a Europe-wide index of invasive species has been compiled.  It’s important to raise awareness among the public of what aliens to expect and where they might see them, as surveillance will be critical. An alert system from the Biological Record Centre designed to help tracking the appearance of invasive alien species provides an excellent starting point for people to record what they’ve seen."
"On Thursday, when climate activists spilled on to the road, singing “We’ll take climate change seriously” to the tune of My Bonnie Lies Over the Ocean, they felt confident of a big victory. Heathrow’s expansion had been declared illegal because it is incompatible with legislation committing the government to zero carbon emissions by 2050. The proposal, which would have brought 260,000 new flights a year to the airport, was dead and campaigners celebrated because similar schemes were now doomed.  But are they? It’s tempting to forecast the future using simple cause and effect, assuming that the court of appeal ruling must now cause Heathrow and similar climate-busting projects to stop, thus accelerating Britain’s transition to a carbon-free economy. But sadly, that’s just one possible scenario. The effect of the ruling could be to expand smaller, regional airports. Most Heathrow flights are for pleasure; determined travellers could pile into Luton, Stansted, Birmingham. More might decide to drive. Even though the government says it won’t appeal, the transport minister, Grant Shapps, has also said, in principle, it does “want to see airport expansion”. Heathrow will appeal against the decision. It could win. Nature could still lose. The future is uncertain because cause and effect are rarely simple. A single cause can have any number of effects, some predictable but many unforeseen. Something can be generally certain – such as the law courts having to take the government’s commitment to the Paris accord seriously – but specifically ambiguous: because no two projects are identical, all future judgments may not favour the environment. Minor differences in projects may result in widely varying outcomes. You can see the same difficulty with epidemics. That they will keep happening is generally certain, but it is risky to extrapolate from one to the other because there is no profile of an epidemic. We don’t know where or when the next epidemics will break out, or what diseases might cause them. They don’t repeat themselves. Expecting to see the future through a simple unfolding of cause and effect is easy but risky. But simple cause-and-effect thinking has plagued forecasting since telling the future became big business at the end of the 19th century. One of the founding fathers of economic forecasting, Roger Babson, was adamant in his Newtonian view of the world. For him, every action had to be associated with an equal and opposite reaction. He not only assembled the largest collection of Newtoniana in North America, but based his entire economic theory on this premise, one of the first (but not the last) to believe he could develop laws of economics that had the same absolute certainty as the laws of physics. His view of cause and effect had a boost in 1929 when, as he had predicted, Wall Street crashed. This, Babson argued, was because periods of wasteful exuberance always produce the need for sensible self-discipline. But he had been forecasting a correction for the previous three years. And when, in May 1931, he announced the market had bottomed out and it was time to get back in, he was wrong: the US economy didn’t recover for a decade. Like most pundits, Babson was poor at reviewing his own forecasts; he continued to believe everything in life could be reduced to simple laws. As a young man, he was told he had contracted TB – at the time, the single greatest cause of death in the western world. However shattering, the diagnosis was nonetheless ambiguous: it could mean a quick death or that the disease would lie latent for decades. Not a man to stand by and wait, Babson devised a health regimen that he was convinced would keep catastrophe at bay. He chose an office surrounded by windows, all of which he kept wide open, even throughout the brutal Massachusetts winters. He and his secretary wore mittens and hooded, floor-length wool cloaks as they worked, the secretary typing with wooden mallets and Babson wearing an electrically heated pad in his back. Was it these eccentricities that allowed Babson to live to 91? It’s impossible to know. He might not even have had TB, since no accurate test was devised until years later. The same problem bedevils studies of leadership. It is fashionable to follow high-profile business leaders and draw from them lessons of success. Steve Jobs and Bill Gates created huge, valuable companies – but how far was their success attributable to them as individuals, and how far the result of other factors: the thousands of people who worked for them, the short-sightedness of their competitors, the wealthy ecosystem they inherited, and a growing educated populace? It’s impossible to do the experiment, Apple without Jobs, or Microsoft without Gates. Seeing individuals as single causes of complex success is an attribution error routinely made when analysing politicians too: did Churchill really win the Second World War, or were Hitler’s unforced errors and the brute force of the Soviet Union more important? Simple causes make memorable stories, but that doesn’t mean they’re true. The sheer complexity of life militates against straightforward narratives and it can feel as though the sides have come off the billiard table, balls dropping off or arriving from anywhere. You don’t know how many variables there are, where they may come from or which, if any, will matter. That’s why, increasingly, efficiency gives way to robustness. When the stakes are high (as in epidemics) a bold response is safer than complacency. Which is why the campaigners celebrating last week’s appeal court judgment will have to return to their fight if they want to increase their chances of environmental success. • Margaret Heffernan is the author of Uncharted: How To Map the Future Together"
"**Northern Ireland businesses have called for urgent financial aid after Stormont ministers agreed a multi-million pound support package to help people hit by Covid-19 lockdown restrictions.**
From Friday, non-essential shops and businesses will close for two weeks, as part of tougher measures across NI.
The executive had pledged to provide additional financial support to businesses forced to close.
B&B owner, Scott Borthwick, said ""there is a real sense of fear"".
The Stormont Executive's immediate package will be worth about Â£338m, while Â£150m is being set aside for longer-term rates relief.
Mr Borthwick, who is sole director of a bed and breakfast in Portrush, County Antrim, said he had yet to receive any financial assistance.
He told BBC NI's Good Morning Ulster programme: ""Where is the money? I have not seen any of the money.
""I know there are hundreds of people who messaged me last night haven't seen the money.
""What are they doing, why has the money not gone out? They have had months to do this.""
Mr Borthwick said he had taken a factory job for 12 weeks during the initial lockdown in order to survive.
""It was a sink or swim situation for myself, we had no money coming in - it couldn't have hit us at a worse time, the hospitality sector, this has come in the middle of March when you are coming out of a winter period,"" he added.
His business was able to reopen for a short time but has now closed again.
""There's no money left, it's got to the stage where there is nothing,"" he said.
Ciaran Smyth, who is the owner of licensed premises in Belfast city centre welcomed Monday's announcement for ""wet bars"", and said he had been ""devastated by four months of closure"".
""We are glad to receive something, the government aid doesn't really cover the expenses it is slightly token,"" he said.
""I maintain they don't have the money to cover us - you have rent, insurance, security, furlough shortfalls.
""They tend to forget the huge loans that we took out which all have to be paid now.""
Mr Smyth said at this stage they had ""no idea"" what they were going to get in terms of a financial package.
""My issue is we can't live just guessing what is going to happen the next day and that is the situation we are put in,"" he added.
""The executive should be considering these issues a long time ahead of what has happened.""
Finance Minister Conor Murphy set out full details of the plan in the assembly on Monday afternoon.
Mr Murphy said uncertainty with the virus and not knowing how much Stormont would receive from the Treasury had made planning difficult.
He said the financial support package he was announcing was as a result of an additional Â£400m provided from Westminster two weeks ago to support the executive's response.
The executive had faced criticism for not having new financial support in place before it announced the lockdown measures last Thursday evening.
But Mr Murphy said his officials were working as quickly as possible to process payments to those who need them."
"The Great British Bake Off has been attracting its highest-ever ratings after a return to our screens. While many different explanations have been offered for the show’s success, the current popularity of baking – along with other domestic crafts such as growing vegetables, sewing, knitting and mending – seems to be one important factor.  It is often said that viewers of Bake Off are simply content to watch cake being baked, rather than doing it themselves, but the show’s reported impact on the sale of baking equipment and ingredients tells another story. The context for this revival of interest in baking and sewing is, of course, austerity. In the wake of the financial crisis, we have tightened our belts, donned our aprons, and taken on board the historical lessons of “austerity Britain” – that period during and after World War II when rationing was imposed and British citizens learned how to “dig for victory”, “make do and mend” and “win the war on the kitchen front”. More than five years after prime minister David Cameron warned that a Conservative government would bring about a new “age of austerity”, popular media culture continues to be saturated with reference to this period. We can’t get enough of the stories of thriftiness, resourcefulness, and resilience. Many commentators have been highly disparaging of this backward-looking popular austerity culture, arguing that it provides ideological backup for the coalition government’s austerity measures. Some have argued that “austerity nostalgia” is nothing but an opportunity to moralise about the virtues of “making do” – a morality that reinforces class distinction and difference, because of its particular appeal to the middle classes.   These critics make an important point. Viewed in the context of the revival of austerity Britain, the popularity of shows like Bake Off do seem to provide evidence of a general acquiescence to the strictures of economic austerity, and of our consent to a scaling-back of our expectations and prospects.  In romanticising wartime resilience in the face of crisis, austerity culture is arguably guilty of exploiting the “very real hardships” currently experienced by many in the UK. Yet this analysis of Bake Off and austerity nostalgia obscures an alternative and highly significant interpretation. Long before the financial crisis, environmental campaigners put a great deal of work into reinventing the concepts of “austerity” and “thrift” for green purposes. And the historical period of austerity Britain became an absolutely critical resource for this campaigning activity.  As far back as 2001, the writer Andrew Simms – then policy director at the think-tank the New Economics Foundation – was arguing that Britain’s experience in wartime is highly relevant to the challenge of climate change. This intervention inspired many others, from the Green Party’s New Home Front project to the turn to “dig for victory” in the allotment movement. Gardening, sewing, knitting and mending are central to this eco-austerity movement because they are seen as part of the “great re-skilling” necessary for transition to a post-growth economy. The revival of popular slogans from that era has given new meaning and value to these activities. You aren’t just baking a cake or knitting a jumper; you’re doing your bit. The resurgence of interest in these practices – evidenced by the widely-reported phenomenon of waiting lists for allotments – is part of a growing attentiveness to material scarcity and environmental responsibility. On Simms’s account, shows such as Bake Off, The Great British Sewing Bee, and The Big Allotment Challenge are indicative of Britain’s appetite for re-learning these skills and for a vision of a more sustainable future. But hang on: how does cake-baking sit within environmental politics? Given that the show appears to boost sales of non-essential commodities such as piping bags and mixing bowls, what kind of green credentials can be claimed for Bake Off and its audience?  Here, critics who argue that austerity culture is no longer about any environmentalism seem to gain ground. Indeed, some have claimed that the nebulous eco-austerity credentials of domestic crafts have actually served to reinforce the morality of “war-style cuts in people’s choices and living standards” and to shame those who aren’t seen to be living adequately “austere” lives. I don’t think we have to choose one or other interpretation of Bake Off and austerity culture. Because so many different actors have made use of the iconography of austerity Britain, it’s very hard to discern the political implications of a particular show.  Bake Off offers both an exemplary lesson in ideological compliance and an endorsement of “re-skilling” for a transition towards a sustainable economy. It’s an extremely contradictory vision of the future. On the one hand, austerity for transition envisions a de-growth or post-growth economy, while on the other, austerity as implemented by the government is notionally aimed at cutting the deficit and boosting growth. And even as the UK does return to economic growth, the echo of eco-austerity should still remind us that the challenges of global warming and climate change have not gone away. Austerity culture may yet provide us with important critical resources to imagine and to develop just and sustainable economic and environmental futures."
"**At Christmas, coronavirus restrictions will be eased to allow people to mix with a slightly wider circle of family and friends.**
Across the UK, people will be able to form ""bubbles"" of three households over a five-day period. Who can be in your bubble will depend on where you are.
Between 23 and 27 December, the three households in a ""Christmas bubble"" can mix indoors and stay overnight.
Northern Ireland has a window of 22 to 28 December, to allow time to travel between the nations.
Bubbles will be allowed to meet each other:
The bubbles will be fixed, so you will not be able to mix with two households on Christmas Day and two different ones on Boxing Day. Households in your Christmas bubble can't bubble with anyone else.
There will be no limit to the number of people who can join a bubble in England, Wales and Northern Ireland, although the English guidance says it should be ""as small as possible"".
But the Scottish government has said Christmas bubbles should contain no more than eight people. Children under 12 will not count in the total.
The rules about what counts as a household also depend on where you are.
In England if you have formed a support bubble with another household, that counts as one household, so you can join with two other households in a Christmas bubble.
The Scottish government guidance says any Christmas bubble should contain no more than one ""extended household"".
People who are self-isolating should not join a Christmas bubble. If someone tests positive, or develops coronavirus symptoms up to 48 hours after the Christmas bubble last met, everyone will have to self-isolate.
You will still be able to see other people who are not in your bubble outside your home, subject to the rules of the tier where you are staying.
Travel restrictions will be lifted to allow people to visit their families anywhere in the UK.
But the government has warned that there will not be extra public transport laid on.
It has urged people to plan their travel in advance.
You will not be allowed to go with your Christmas bubble to hospitality settings, such as pubs and restaurants, or to entertainment venues.
You can meet people outside your Christmas bubble, but only outside the home and in line with the tier rules of the area in which you are staying.
Places you can meet include parks, beaches, open countryside, public gardens, allotments and playgrounds.
Some traditional Christmas activities will also be allowed in England:
Scottish First Minister Nicola Sturgeon said the relaxation of restrictions will not be extended to cover New Year's Eve.
She said: ""I know New Year is special for people, perhaps slightly more so in Scotland than in other parts of the UK, but the virus is still there.""
The planned firework displays in Edinburgh and London have been cancelled.
The virus will not call a truce because it is Christmas, and will be as contagious as at any other time, UK Prime Minister Boris Johnson has said.
Even if it is within the rules, meeting friends and family over Christmas will be a ""personal judgement"", the government says.
People should consider the risks to themselves and others, particularly those who are vulnerable.
Scotland's First Minister Nicola Sturgeon urged people to use the Christmas rules responsibly and ""only if you think it is necessary"".
Children aged under 18 whose parents live apart are allowed to be part of two separate Christmas bubbles. This means they can see both parents without being counted as part of another household.
University students may return to their parents in the early December ""travel window"" and be counted as part of their household straight away.
But if parents have three or more grown-up children who are not at university, then they cannot all form a Christmas bubble with their parents.
Individual households can split for Christmas. So, if three people are sharing a home, they can all go and form separate Christmas bubbles with their families and come back to form a single household again afterwards.
In England, care home residents have been told not to take part in Christmas bubbles, while visits out of the home should only be considered by residents who are of ""working age"". This is because of the increased risk of the resident catching coronavirus, and spreading it to other vulnerable people when they return, the government says.
People are asked to consider whether visiting at the care home would ""provide meaningful contact in a safer way"", while residents who do leave will be tested and asked to isolate when they return."
"
The  GISS Temperature Record Divergence Problem
Guest post by Tilo Reber
In connection with James Hansen’s  explanation of why his GISS temperature record diverges from that of  HadCRUT, I decided to check on the legitimacy of what GISS was doing.  Dr. Hansen’s article is here at RealClimate. This is the specific chart of interest:

(click  on chart to expand)
In this chart, Dr. Hansen explains how the GISS  record is different from HadCRUT.   The essential difference is that  there are areas at the poles where GISS has filled in the values by  extrapolating from the nearest land based records.   Dr. Hansen created a  mask of all the areas that HadCRUT does not cover.  He then applies  this mask to the GISS record and deletes the readings for the areas that  are masked off.  The resulting chart is marked GISS/HadCRUT mask  above.   Dr. Hansen then goes on to provide a graph which shows that the  GISS divergence with HadCRUT no longer exists after the missing HadCRUT  boxes have been removed from the GISS record.
So far so good.  I think that it is safe to say  that the divergence of the GISS record is due to the interpolation and  extrapolations at the poles.  Now we need to ask the next question –  are the GISS interpolations/extrapolations legitimate.  GISS has 2005 as  the hottest year for his surface temperature record.  HadCRUT and the  satellite records (UAH, RSS) have 1998 as the hottest year.  So Dr.  Hansen compares 1998 to 2005 in his chart, allowing the reader to see  why the difference exists.  Of course Dr. Hansen considers that his  method produces the more accurate result.
One of the first things that pops out about the  charts is how different the top row of polar cells is between the two  records.  For example, if one looks at the top row of the HadCRUT 2005  chart, one sees a group of cells directly above Svalbard that are shown  as having a cool anomaly in that record.  Then, when one looks up at the  same cells in the GISS record, one sees that GISS has the same cells  colored to the maximum hot anomaly.  In fact, the cells that HadCRUT has  in the top row (polar area) of that record look very different from the  top row of the GISS record.  The fact that these cells are so different  and that they are accounted for in both records makes me wonder what is  going on.  Looking at the GISS site, they say  this.
“Areas covered occasionally by sea ice are masked  using a time-independent mask.”
So if there is sea ice coverage for any part of the  year, GISS will not use SST values to cover those cells for the  entire year.  Those cells must be covered by extrapolations from land  for that year.  This means that when the area is cover with ice or with  water or with part ice part water, it will have it’s anomaly  extrapolated from land, regardless.  HadCRUT, on the other hand, does  not extrapolate their coverage.  But they will use SST values for a cell  when SST values are available for part of the year.  If the area is  covered with ice for the entire year, HadCRUT will not assign it a  value.  Therefore we get polar areas that are covered by extrapolation  by GISS and not covered at all by HadCRUT.
When we look at the HadSST2  record, we see that the cool cells that show up above Svalbard in  2005 are consistent with the numbers in that record.   And these then go   into creating the sea surface portion of the HadCRUT3 temperature  record.  So, obviously, how cells are filled with data can have a  profound effect on the anomaly value that those cells have.  This leads  one to wonder if extrapolations at the pole are legitimate.  I decided  to look at some of the northern Russian stations, at the GISS site, that  show up as being so hot in the 2005 version of the GISS chart when  compared with the 1998 version of the chart.  I found that those big  changes are in fact represented in the individual records – especially  for the coastal stations.  Here are three of them.
Kanin  Nos:   68.7  N, 43.3 E.
1998 Annual Mean –   -3.39
2005 Annual Mean –   0.60             1998 – 2005 delta    3.99 C
Ostrov Vize:   79.5  N,  77.0 E.
1998 Annual Mean – -14.99
2005 Annual Mean – -10.79            1998 – 2005 delta   4.2 C
Gmo  Im.E.K F:  77.7  N,  104.3 E.
1998 Annual Mean –  -15.96
2005 Annual Mean –  -12.67           1998 – 2005 delta   3.29 C
For comparison, let’s look across the Arctic ocean and see what was  happening in Canada and Alaska at the same time.
Eureka, N.W.T.:   80.0  N, 85.9 W.
1998 Annual Mean –  -17.38
2005 Annual Mean –  -17.34           1998 – 2005 delta   0.04 C
Barrow,  Ak.:     71.3  N, 156.8 W.
1998 Annual Mean –  -8.80
2005 Annual Mean –  -10.44           1998 – 2005 delta   -1.64 C
So it seems that the North American side of the Arctic changed  little, or even got cooler between 98 and 05, the Russian side warmed  considerably.  Why is that?  I think that this ice  cover map gives us the answer.  As is immediately apparent, the  coastal ice cleared out far earlier in 2005 in northern Russia than it  did in 1998.  This is even though the rest of the globe was slightly  warmer in 1998 than in 2005.  When dealing with coastal stations,  removing the ice and exposing the water is like taking the hatch off a  heating source for the coastal thermometers.  For stations that are in  areas where the temperature is well below zero, exposing the immediate  area of that thermometer to a surface that is above zero, changes  everything.  Looking at Ostrov Vize, we see that it is a small island,  and therefore even more subject to changes in coastal sea ice.  And when  we compare 1998 months on this island with 2005 months we can see that  there are differences in some of the monthly means that are larger than  10C.  Even a partial ice cover as opposed to a complete ice cover will  supply the stations with more heat.
So I think that we can safely say that the huge change in  the anomalies of Russian coastal stations is mostly due to coastal sea  ice changes.  In fact, if we look at stations further inland in Russia,  the coastal effect begins to decline.  With this in mind, we need to ask  if the GISS extrapolations of land based stations, particularly coastal  stations,  to the poles is appropriate.
The answer would seem to be that it is not, and the Svalbard case  makes this perfectly clear.  There we had a case where the SST anomaly  was actually cool, and yet the land based extrapolation actually turned  those sea based cells more than 3C hotter.  Reaching across the Arctic  Ocean with temperatures that are the result of a coastal sea ice effect  cannot give valid answers for what the temperature anomalies away from  those coastal stations should be.  In fact, taking the variation that is  represented by those coastal stations and extrapolating into the  interior of Russia is also not appropriate, because the interior areas  did not undergo the magnitude of temperature change of the coastal  stations.
Looking at the SST temperature  anomalies that NOAA uses for 1998 and  2005 it again looks like nothing  exceptional was happening in the Arctic (Note, the chart will not retain the months that I selected;  so use your own sample months and they will plot).  It seems, from  this analysis, that GISS polar extrapolations and interpolations are  likely to simulate large variations away from the Arctic coasts that are  really only present as changes at the Arctic coasts.  And the GISS  divergence from HadCRUT, as well as from UAH and RSS are likely to be  errors instead of enhancements.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f0a7d59',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Last week’s presidential debate revealed to 40 million viewers that global warming is an issue on which the candidates have clear differences, both on policy and in the veracity of their responses. Gov. Bush argued that “some of the scientists…have been changing their opinion a bit on global warming” and that we need a “full accounting” of the issue before creating policy. 



Bush is clearly correct here, and Vice President Gore is not countenancing the whole truth when he cites the supposedly unified opinion of scientists. Many scientists have reappraised global warming, most notably NASA’s James Hansen, who now argues that the rate of warming is much lower than initially forecast because plants are taking up carbon dioxide at an increasing rate. 



In other words, the planet is becoming greener. This is precisely the same position that has been maintained by the coal and oil industries for years. There has been no “full accounting” of global warming because no one has yet been able to devise a system whereby scientists who assess the problem do not also profit from defining it as a problem. 



Gore tried this with his much‐​awaited “National Assessment” of global warming, which is now held up by a lawsuit. The Assessment team, much like Mrs. Clinton’s health care consortium, apparently did an awful lot behind locked doors. 



Gore also intoned that “many people see the strange weather conditions that the old‐​timers say they’ve never seen before in their lifetimes” and that “storms are getting more violent an unpredictable.” 



Those claims are totally false, as anyone who studies weather knows. There are dozens of different weather parameters measured every day: high and low temperatures, rainfall, snowfall, and wind speeds, for example. The chance that an individual will see one of those parameters at an extreme value in their lifetime is exactly 100 percent. Some day must be the hottest day in your life.



Furthermore, the chance that an extreme value will appear in a given year is also high. Let’s work an example with monthly temperature and rainfall. There are 12 months in a year, each of which is ranked according to temperature and precipitation. That’s 48 chances in a year that a given month will be record warm, cold, wet or dry. Most climate information started being recorded in 1948, giving 52 years of data. Rounding the numbers off, if there are the same number of chances to set a record as there are years of observations, the chance that a record will be set this year is 50–50. 



Gore is fond of pointing to increased flood frequency in the United States, based on a study by federal climatologist Tom Karl. But other, equally esteemed climatologists at the U.S. Geological Survey just wrote to Gore’s assessment team admonishing that Karl’s result could be duplicated with random numbers. 



According to the United Nations, hurricane severity is decreasing for the storms that strike the United States. Tornado deaths are also declining. New research shows that, along with global warming, the extreme U.S. coldest temperatures have risen sharply while extreme high temperatures have declined. 



In the policy sphere, Bush’s most intriguing response during the debate was in code. Although the science promoting global warming is shaky at best, Bush is a big supporter of “clean coal technologies” and has proposed spending about $2 billion on their advancement.



This used to be a buzzword for getting pollutants such as sulfur and nitrogen oxides out of the combustion stream. But now it could mean more, such as getting carbon dioxide–the biggest contributor to the greenhouse effect–out, too. That’s a difficult operation, but engineers can do it today. 



How much will this raise the cost of energy? It might not be as expensive as initially suspected. If this technology is imposed on all fossil fuels, coal still comes out as comparatively cheaper, because there are about a jillion tons of it under our feet. Under this scenario, if you believe global warming is serious, locking up federal land and prohibiting mining is about the dumbest thing you can do for the environment, as President Clinton recently did in Utah. It also isn’t very savvy. Coal miners in Democratic West Virginia just announced they support Bush, in large part because of their fear that Gore, in his Jihad against global warming, will dial coal out of the nation’s energy stream.



Who says Dubya is slow? Gore reiterated in the debate that global warming is “the central organizing principal for civilization,” whereas Bush proposed a program that fights climate change and would have the enthusiastic support of both the fossil fuel industry and the United Mine Workers.
"
"

Like many indulgent boomers, I live in a big old house — thousands of square feet of Victorian, to be specific, overlooking the bucolic Shenandoah Valley of Virginia. Years ago, I did as I was told, installing a gas furnace, stove and, recently, a water heater. The very leakiness of houses like this frees them from zillions of trapped allergens and the buildup of dreaded radon, but the price is that heated air escapes, too. So last month I paid $600 to heat air, food and water.



Why? Two related reasons: The price of natural gas is at historic highs, even after allowing for inflation, and the November‐​December average temperature was the lowest ever measured in our 106‐​year temperature record.



This, in turn, has created a short supply, resulting in some compensatory behavior. Because prices are high, I now turn the heat way down at night. And the cats, once thrown out, are now invited in as bed‐​warmers, showing once again why throughout civilization, the number of Homo sapiens and Felis domesticus has been roughly equal.



But there’s another way to manage energy shortages that I discovered by searching the newspapers for what happened in previous cold winters. The last ones that looked like this one occurred consecutively, in 1976–77 and 77–78. Rather than allow prices to rise, President Carter announced we were in an “energy crisis” that was “the moral equivalent of war.” The headline in the Jan. 30, 1977 _Washington Post_ , 10 days into his presidency, screamed “Carter Urges Four‐​Day Work Week.”



He actually proposed legislation outlawing work. The Democratic Governor of New Jersey, Brendan Byrne, went one better. According to page A5 of the same _Post,_ “Byrne told homeowners to lower thermostats to 65 degrees in the day and 60 at night or face stiff fines and even prison sentences.”



So which do you prefer: markets or jail?



It’s also worth noting that similarities abound whenever the weather is extreme. That same 1977 edition of the _Post_ also carried this headline, “Changes in the Earth’s Weather Are Expected to Bring Trouble,” a story that blamed cold temperatures on global warming. Nevermind that this vetoes the First Law of Thermodynamics, which states that heating causes warmth and vice‐​versa. And, just as the story played ad nauseam a quarter‐​century later, climate fluctuation was painted as an abject disaster. The _Post_ cited a then‐​recent (1974) CIA report that said that “climate is now a critical factor. The politics of food will become the _central issue of every government._ ”



Nothing could have been more wrong, but little has changed. Two years ago I attended a hush‐​hush Defense Department briefing where the CIA again stated that climate would be the central issue for global security. The fact that the agency has botched this for 25 years speaks volumes about the continuity of culture in our nation’s capital. In that quarter‐​century world crop production increased at a demonstrably greater rate than food consumption.



What about the future? Dreaded global warming will tend to ameliorate the coldest temperatures of winter more than anything else, “tend” being the key word. In some years, like this one, it won’t be sufficient to relieve our usual misery. In others it will be a downright windfall. Consider 1997–98, when warming, in concert with El Nino, reduced the winter demand for energy by a whopping 15 percent, ultimately sending gasoline prices to their lowest level in decades and helping to kite dozens of stocks, which built hundreds of wealth‐​effect beach houses. Unless my profession is completely wrong (a distinct possibility), winters like that one will become more common.



Nonetheless, sometime in the coming decades this current winter, or something like it, will probably repeat. If we don’t have the energy supply, so will this year’s outrageous costs. While the last quarter‐​century has taught us that markets beat the slammer as rational energy use, it’s also time to acknowledge that abundant energy and economic wealth go hand‐​in‐​hand. Can we drill for energy in the Arctic National Wildlife Refuge and keep the Caribou alive? Of course, as the Trans‐​Alaska Pipeline, built in response to the 1973 Arab Oil Embargo, demonstrates.



But imagine, if you will, the state of the nation this chilly January if the U.S. Senate had ratified the Kyoto Protocol on global warming, which would have required us to reduce energy consumption by around one‐​third. Many calculations show that this would require a doubling in the price of fuel. In that case, only the rich could heat their old houses, cats wouldn’t do enough to ward off the cold and, as with Jimmy Carter, we’d be inaugurating a new president in four years.
"
"

Out of the smoke of November 2002, one issue‐​conspicuous by its absence in the recent campaign‐​is about to emerge. That issue is the global environment. And it may make Al Gore, who’s scheduled to announce his presidential plans soon, the surprise rising star for 2004. For what it’s worth, he’s already been booked for Barbara Walters, Hardball, and Saturday Night Live in the last few weeks.



Sure, other issues are more compelling: Social Security, Iraq, and taxes, for example, but anyone that tried to beat the administration with those issues wound up beating themselves. It’s hard to get passionate about committing political suicide. In every race where Social Security was an issue, candidates who favored private accounts won. Almost everyone‐​win or lose‐​was with the president (at least publicly) on Iraq, and raising taxes just isn’t vote‐​productive. 



What’s left? The Washington political process, in its irrepressible attempt to produce productive controversy (i.e., votes), is behaving like Tom Wolfe’s doomed test pilots in The Right Stuff. “I’ve tried A. I’ve tried B,” and the darned thing won’t stop rolling.



Well, try “E.” About the only fights that may produce political gain are going to be over energy and environment, which means fights over drilling in the Arctic National Wildlife Refuge, fat subsidies tilting at windmills and solar energy, and “directions” to industry to produce tons of power from the same subsidized sources. If you’re wondering who pays for this, look in the mirror.



All of the above are in the current energy bill, a bipartisan compromise between the House and the then‐​Democratic Senate. Along with most of the other unfinished business from the last Congress, there’s no way it’s going to remain intact. How much it will be changed is a matter of conjecture. But one provision, ultimately requiring just about every business in the country to report its annual emissions of carbon dioxide (read: fuel consumption), is surely outta’ there. Everyone, on both sides of the aisle, knows this is the first step towards a cap on energy use, which must be defined before it can be cut any specific amount. 



Carbon dioxide, of course, is the principal cause of dreaded global warming, so any attempt to dilute current legislation will raise the bloody green shirt. And where global warming goes, Gore is soon to follow. 



For nearly a year, Dick Morris has been pleading with the Democrats to pick up this issue. In fact, in his recent book, “Power Plays,” he argues that Gore would have won in 2000 if he had followed his own instincts on fighting climate change, which is the real reason why he wanted to be president. Democrats see this coming, too. Last month, the Democratic Leadership Council issued a broadside entitled “Turn Up the Heat on Climate Change,” spoiling for a fight on global warming.



Don’t expect a clear, reflexive Republican voting pattern. The DLC is touting draft legislation written in part by Lincoln Chaffee (R-RI) who is at odds with the White House on global warming. Expect him to forge an alliance with other down‐​east Republicans, like Maine GOP‐​ers Susan Collins and Olympia Snowe, and John McCain (R‐​Ariz.), another global warming hawk, who hasn’t yet tired in his campaign for more, more, and more face‐​time against the President.



It’s not that Gore is a proven winner. But on the environment he is the proven champion. And if, maybe, he gains traction by railing against the administration along with McCain, et. al., he becomes a serious contender for the nomination. 



On the other hand, Gore may simply be too radioactive, still burning from the loss in 2000 and having actively campaigned in many of the debacles of 2002. In that case, John Kerry (D‐​Mass.) is sure to emerge as the global warming maven. He’s actually more radical (if less versed) on the issue than Gore, and there’s a school of thought in Democratic circles these days that says they lost the Senate because they were too much like Republicans. Kerry is no Republican.



All of that makes climate change look like a big issue in the next election cycle, with the flashpoint being current energy legislation. Will it be big enough to determine who runs against George Bush, and will that person stand much of a chance of success? Right now it doesn’t look good, but “A” and “B” have been tried, and maybe it’s time for “E.”
"
"Christopher de Bellaigue makes quite a jump from the 1947 Agriculture Act and its guaranteed payments system to the EU arable area payments regime (Is this the future of farming?, 25 February). Once again under the “oven ready” new environmental land management proposals, farmers – as they mostly always have done – will be asked to farm under some sort of intrusive government regulation. In return for what? There is no disagreement that farming is a risky enterprise, needing good forward planning and security, and with climate change this will get worse. Having farmed under the transitional arrangements, with arable area and hill subsidies still in place, some sort of transition will still be necessary and a fallback, either through tax regulations or a form of “agrilite” subsidy support mechanism, will help farmers in the transition.  Farming without direct support may produce a generation of growers prepared to farm intensively without subsidy and with their own nods to biodiversity, causing a denudation of hills and marginal land with the subsequent loss of rural communities. All parts of the UK can benefit from more environmentally friendly farming practices, but to succeed the regulations must be less intrusive and not dilute much of the good work, done by the UK and EU in partnership, on reducing the input of agricultural chemicals, and on water and air qualityChris Jones(Nature reserve manager), Biddenham, Bedford • Christopher de Bellaigue covers in depth many aspects of the problems we face in our management of the land. He mentions very briefly “mixed” farming which is using a rotational system whereby fertility removed by arable crops is returned by other crops such as turnips and grass leys used by grazing animals. Thomas Coke invented the Norfolk four-course rotation and put it into practice on his Holkham estate in Norfolk. In 2015, over 140 years later, a similar updated six-course rotation was introduced on the same estate and the results in terms of increased yields etc have been remarkable. As somebody who farmed in the era of chemical farming, I think it fair to say that many farmers, with much encouragement from the fertiliser and chemical giants, found there was no need to rotate and basically forgot how to farm. This has ultimately resulted in deterioration of soil structure, wildlife habitats, and water quality, and obviously cannot continue. A return to a sustainable rotational system is not impossible, and the old farming chestnut “farm the land as though you are going to live forever and run the farm business as if you were going to die tomorrow” should be remembered.Kevin CaveneyWick, Glastonbury, Somerset   • Christopher de Bellaigue accurately sets out the facts which will ensure that British farming is about to enter a process of major change. In doing so he points out that 71% of the land area of Britain is devoted to farming, yet this industry only produces 10% of greenhouse gas emissions. In a previous Guardian article (Lab-grown food will soon destroy farming – and save the planet, George Monbiot, 8 January) it was suggested that agriculture is a major source of GG emission. Would it not be better if we were to concentrate on reducing the 90% of greenhouse gas emissions that emanate from only 30% of the land area?Richard HarveyOakham, Rutland  • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition  "
"During Chinese premier Li Keqiang’s last visit to the UK, China signed a series of deals on energy and low carbon technology, and a declaration of cooperation on climate change. A few weeks later, similar deals were signed with the US. The question is whether this demonstrates a genuine commitment from China to environmental protection, or whether it’s just rhetoric in the run up to important climate change conferences in New York and Paris. There is no doubt that the Chinese leadership has instigated a number of measures to address the problems of environmental pollution at home. The world’s largest polluter has done much more than any other government in the developing world. In recent years China has standardised environmental laws and regulations, encouraged local governments to take on responsibility for environmental protection, and introduced the environmental information disclosure, urging enterprises to meet certain requirements.  Environmental NGOs, the mass media and online communication are also playing an increasing role in China’s environmental policy-making. The country is now the largest investor in renewable technology and ambitious targets have already been set to increase the portion of renewable energy generated in China and reduce its dependence on polluting coal. China sees international cooperation on climate change as an opportunity. Its leadership wishes to be regarded as responsible global stakeholders by collaborating with major powers on big problems, without touching on sensitive issues concerning human rights, cyber-espionage or tensions in the East and South China Seas.  There is also an opportunity for China to open up foreign markets for its low-cost renewable energy products such as solar panels, ensuring itself a slice of the fast-growing renewable energy industry market. Nowhere is this most apparent than in those countries where Chinese state-owned companies and banks already provide infrastructure, investment and finance, in Africa and Latin America. As their economies grow these countries will face similar environmental problems, and so they watch China’s transformation carefully. Not only to see how a leader of the developing world tackles climate change and environmental issues, but also as a potential future supplier of environmentally-friendly technology.  Despite China’s aspirations and the positive ring to the speeches and deals signed with western nations, the key fact remains that China is not willing to sign up to any major international commitments on climate change. This has been demonstrated before: just before the Copenhagan COP15  in 2009, China announced its intention to reduce carbon dioxide emissions per unit of GDP by 40-45% (based on 2005 levels) by 2020. But during the conference it refused to agree to any specific targets.  China not only advocated on behalf of developing countries but also accused major powers of trying to divide the developing world. The Copenhagen summit took place just a few weeks after President Obama’s first visit to China, when the two countries agreed on joint cooperation on climate change and to establish the Sino-American Clean Energy Research Centre. How much China commits itself to cutting emissions will depend on how far  western powers will go in their own national climate policies. With the current divisions among EU member states on directions for energy strategy and a declining priority on the issue following the recession, the Chinese leadership can feel at ease. Subsequent UN climate talks in Bonn in June have also yielded only disappointing results. Other nations’ lack of commitment to emissions reduction gives China more room to manoeuvre. Next year’s COP21 summit in Paris to replace the Kyoto Protocol might be another occasion for Chinese leaders to cut western counterparts down to size.  China’s leaders’ rhetoric is clear: the developed world should take the lead in addressing climate change, while China – seen when it needs to be as a developing country despite its place as the world’s largest economy and second largest polluter – prioritises national economic development.  Seen in this light, cooperation is best understood if placed in the context of China’s interests. Bilateral agreements under the umbrella of renewable energy can help China advance technologically, provide opportunities for state-owned enterprises to access markets abroad, and enhance China’s energy security. These are likely to succeed; any collaboration beyond this scope – agreements for the good of the international community and the world at the expense of Chinese interests – is doomed to failure. "
nan
"The rise of hydraulic fracturing, or fracking, has ushered in an era of intense drilling that has been called the great shale gas rush. Fracking allows oil and natural gas to be extracted from horizontal wells, thousands of metres below the Earth’s surface. We tried to piece together the environmental impact of the great shale gas rush, and quickly discovered how little is actually known about the effects this booming industry is having on plants and wildlife. To help shed light on this area where there is little research, we convened a team of eight scientists with diverse expertise in plants, birds, amphibians, mammals, wildlife disease, hydrology, and public policy. Our study, published in the journal Frontiers in Ecology and the Environment, examined 12 ways in which shale development can harm ecosystems. Using an objective ranking system, we identified the highest-priority threats for future research.  Much of the public debate around shale development has focused on the technique of fracking itself. The above-ground footprint of each well is relatively small at between 1.5-3 hectares of land cleared per well, but many are drilled in close proximity and each is connected by a labyrinthine network of roads and pipelines which must be built and in place before the fracking can start. This can add up to many thousands of hectares of disturbed land, contributing to habitat loss and fragmentation, generating light and noise pollution, and affecting air quality. The huge amounts of water required for fracking, on average 20m litres over the lifespan of a single well, can take a heavy toll on groundwater levels and water flow in streams, adding to siltation and affecting wildlife dependent on those habitats. The whole process of drilling for oil and gas also provides opportunities for unpredictable and potentially disastrous events – chemical spills or pipeline ruptures – which pose high risks to surrounding ecosystems. Understanding these risks and effects is essential because many shale basins occur in regions of exceptional biological diversity. For example, the Devonian and Marcellus Shale in the Appalachian Basin overlap with the highest areas of amphibian and freshwater fish diversity in the United States. Each of the threats we identified were assessed on their extent, duration, the difficulty of mitigating their effects, and the amount of relevant scientific information available. With limited resources, our top research priorities should be threats that are under-studied, widespread and whose effects take considerable time or effort to reverse. We found the highest risk and therefore highest research priorities were related to chemical contamination from spills, equipment failure, the movement of fluid underground, or storage leaks. Although our focus was on plants and wildlife, these problems could also affect human health. More surprising was the lack of accessible and reliable information on the chemicals used in fracking (as part of the fracking fluid blasted underground to fracture the rock), spills, and the disposal of this fluid as wastewater. We reviewed the chemical disclosure statements provided by fracking companies for 150 wells in three of the top gas-producing states and found that, on average, two out of three wells were fracked with at least one undisclosed chemical, while some wells were fracked with 20 or more. It’s impossible to accurately predict the effects of environmental pollution if we do not know the identity and concentration of chemicals being released into soils and waterways. Government and industry have made small steps towards openness, for example by creating the FracFocus registry and by maintaining regional databases. (for example the Pennsylvania Department of Environmental Protection). But these steps are not enough. Only five of the 24 US states involved in fracking – Pennsylvania, Colorado, New Mexico, Wyoming and Texas – maintain any public records of spills, and a lack of complete and consistent reporting is a major obstacle to understanding the environmental impact of shale development. For instance, less than half the reports in the Pennsylvania database included information on where spills occurred and fewer than one in 15 reports noted the date and time when the spill occurred. In fact, in Pennsylvania alone (an area about half the size of the UK), the Department of Environmental Protection cited companies 22 times in 2013 for discharging industrial waste into waterways.  The collective impact of shale development as illustrated above is not easily seen from the ground. Yet each modest-sized drilling operation contributes to at least 12 distinct environmental threats. The combined impact of these threats is itself a major research priority, but is exceptionally difficult to isolate and study.  With shale production projected to increase exponentially during the next 30 years, it’s vital that scientists, the industry and government regulators co-operate to minimise damage to the natural world. Scientists need access to reliable information about spills, violations, fluid disposal and chemicals used in fracking. This information must be seen as essential to prevent the shale industry from posing unacceptable risks to the living world, our families and communities. "
"Having reached the self-evident conclusion that one can’t win government without the support of at least a handful of regional communities, Labor is now coming to terms with its use of the C-word. With their opponents to their left and their right taking up the crude chant of “coal, coal, coal” – as shorthand for economic betrayal on the one side and environmental betrayal on the other – watching Labor field questions on climate is like watching a tightrope walker attempt to cross Niagara Falls. For a party that was designing and implementing detailed economy-wide carbon-reduction policies in government just a decade ago, their journey to the trenches of the coal culture war that passes as climate debate these days is like a descent into hell. In some heartland communities coal has become Labor’s kryptonite: the lack of vocal support for the industry a sure sign of neglect and betrayal, “just transition” code for human redundancy, green jobs a fairytale. When Labor talks of climate action, these communities hear that they are being asked to sacrifice their livelihoods for a greater good that doesn’t include them – a message the Nationals and those further to the right are only too happy to reinforce. But for Labor’s progressive city base, the lack of a vocal rejection of coal is portrayed as a lack of genuine commitment to climate action. For them coal is more like a herpes sore: Labor’s got it, they are not going to get rid of it, they don’t want to talk about it, but they will be duty bound to ’fess up to their condition before they can expect anyone to kiss them. Ergh. In this context, securing an alliance to win government becomes a challenging two-step. First, Labor must convince heartland communities that action on climate change does not render them collateral damage. Then they will need to convince progressives that maintaining the coal industry does not render them incapable of meaningful climate action. Opposition leader Anthony Albanese has begun to confront the issue head on in recent weeks, bringing the party into line with global and state governments with a 2050 target for net zero carbon emissions while asserting an ongoing role for coal exports in the economy. As this week’s Essential report shows, the first part of this formulation is not controversial, with the long-term zero target is accepted among the vast majority of voters. Indeed, Coalition support for net zero has shifted net 24 points in just a month: https://essentialvision.com.au/climate-change-policy-proposals. Despite the broad public sentiment, the Coalition can sniff division, seizing on these targets as a blunt instrument, with the “cost of action” mantra re-emerging amid threats of rural Armageddon. But it’s not just Labor supporting the zero by 2050 mechanism. Independent Zali Steggall has introduced a private members bill to this effect. In the unlikely event that the bill ever sees the light of day, there will be a cohort of Coalition MPs who will be forced to make a call between backing in the global benchmark or staying on the side of denial. Fun times for them, to be avoided at all cost. But given the 2050 target is endorsed by everyone from the NSW premier to Tory PM Boris Johnson, landing on the long-term target may be the easy part of Labor’s coal conundrum. Coming up with a way of keeping faith with progressives, particularly younger people demanding urgent action that includes the rapid decarbonisation of the energy sector, will be the real challenge. New Greens leader Adam Bandt’s response to Albanese’s weekend media where he accepted a long-term future for coal exports is instructive: “Scott Morrison loves coal, but it seems Anthony Albanese does too,” he tweeted. “If Labor thinks we’ll be mining and selling thermal coal in 2050, they’re not serious about climate change or ‘zero emissions’.” But even in this attack, Bandt is using nuance, distinguishing “thermal coal” used to produce electricity with “metallurgical” or “coking” coal, which still is a necessary ingredient for producing steel. Shining a light on this nuance may be the way through. It starts with being open about what decarbonisation really means: a long-term transition in energy that will see coal-fired power replaced by renewable technologies at both a local and global level. Over time it may also see new technology, such as hydrogen, replacing coal in steel production, although that technology is still nascent. For now, the transition means thinking through the next phase of government investment in energy, where the debate over new coal-fired power is critical and the points of difference between the government and opposition most stark. What should give the opposition leader some succour in embracing the C-word is that voters are ready to accept this sort of nuance. When given the choice, nearly half support a staged phase out of coal-fired power plants, rather than a rapid shutdown or taxpayer-funded life support. But the debate on coal can’t stop there. Labor needs to be upfront that coal exports are central to the Australian economy and prosperity, with steel critical to the development of our neighbours. In this context, differentiating types of coal and recognising the importance of this trade, when resources account for 60%  of our export income, is crucial. Finally, Labor can make the case to those who demand an immediate end to coal exports that this is outside the global frameworks that they champion, that the path to global abatement will come from domestic targets, not a unilateral closure of international trade. Playing a meaningful role in the evolution of the global framework, rather than sabotaging the process as the current government does, is the best way to ensure that exports taper off. None of these debates are easy to mount, or popular for those who see the moral imperative to act in black and white terms. But engaging progressives by talking about coal as a resource to be managed and not as poison to be banned seems the only way through the morass. Of course, by taking a middle track, Labor risks being wedged. When the political debate on climate action is reduced to a binary proposition on coal – for or against – the tightrope is impossible for Labor to walk. It’s not only politically fraught but nonsensical in practice. But by confronting the complexity of coal, its importance to the economy, the difference between domestic and export use, Labor can bring the discussion back down to earth, and in doing so, offer its credentials to lead Australia through this complex transition. After all, the 2050 net zero target will require transformation across many industries beyond energy, and what could be worse than a culture war every time? Trucks: for or against? Beef: for or against? And so on. It was a tantalising glimpse of climate wars 2.0 when agriculture minister David Littleproud posited that Labor’s net zero target meant “most of the national herd would likely have to go”. The problem with so much of the climate debate is that it is defined by slogans rather than nuance. “Kill coal”, while an eventual consequence of energy transition, is not a theory of change. Climate change is too big a challenge for slogans on either side of the debate. Finding a pathway to decarbonise the economy starts with changing government, and that will be even harder if Labor can’t use the C-word. Despite the sometimes deafening noise, our numbers suggest there are plenty of Australians open to a discussion about coal that’s not just black and white. • Peter Lewis is an executive director of Essential, a progressive strategic communications and research company"
"
Share this...FacebookTwitterDer Spiegel today has a report here on why the Vikings left Greenland, hat-tip reader DirkH. Also read Brown University press release here in English.
Proxies from all over the world have shown that global climate was as warm or even warmer during the so-called Medieval Warm Period back around a thousand years. It’s not called a “warm period” for nothing!
William D'Andrea, right, and Yonsong Husang, left, with sediment cores. Photo credit: William D'Andrea/Brown University. 
Der Spiegel writes about how scientists from USA and Great Britain examined sediment cores from two lakes near Viking settlements in Greenland and how the Little Ice Age hit and was one of the big factors that drove the Vikings off Greenland beginning in the middle of the 14th century. The scientists were able to produce a temperature reconstruction going back 5600 years. This is now reported in the Proceedings of the National Academy of Science.
Der Spiegel quotes William D’Andrea of Brown University in Providence, Rhode Island.
‘There really was a drop in temperature shortly before the Vikings disappeared.’ As a consequence the times for crop growing was shortened and little feed was provided to cattle.”
Yes, there really was a drop in temperature. How many more proxies is it going to take before the warmists abandon the fantasy that temperatures were more or less stable over the last 1000 years and shot up only when the Industrial Revolution began? D’Andrea adds:
“It is interesting to consider how rapid climate change may have impacted past societies, particularly in light of the rapid changes taking place today.”
Of course it’s plain to see where D’Andrea is headed. But there are big differences today. Firstly, society is much better equipped technically to adapt to climate change, and 2) climate gets nasty when it cools, and not when it warms. History shows that warm has benefited human and natural development, and cold always set it back. Indeed we are the first generation where we have a few whiners who constantly complain about warmer conditions.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There are also some non-differences. For example, the change we’ve seen over the last 100 years is no more remarkable than changes we had in the past. There is no more evidence of a significant human imprint today than there was 1000 or 500 years ago.
As another example of the misery brought on by cold, the Brown University press release writes:
The researchers examined how climate affected the Saqqaq and Dorset peoples. The Saqqaq arrived in Greenland around 2500 B.C. While there were warm and cold swings in temperature for centuries after their arrival, the climate took a turn for the bitter beginning roughly 850 B.C., the scientists found.
Yes – warm swings, cold swings, and turn for ther bitter, were all natural. The press release continues.
The Saqqaq exit coincides with the arrival of the Dorset people, who were more accustomed to hunting from the sea ice that would have accumulated with the colder climate at the time. Yet by around 50 B.C., the Dorset culture was waning in western Greenland, despite its affinity for cold weather.
Just how abrupt was climate change back then? The very beginning of the Brown University press release tells us (emphasis added):
Greenland’s early Viking settlers were subjected to rapidly changing climate. Temperatures plunged several degrees in a span of decades, according to research from Brown University.”
And today’s global temperature rise of 0.7°C over the last 100 years is supposed to be “unprecedented”? Yeah, right. Today’s climate change is no different than what we’ve seen in the past. If anything, some change-episodes in the past were considerably worse.
====================================================
Note: Scanning the German headlines, centrist and conservative publications are reporting this Greenland story while leftist publications are ignoring it.
Share this...FacebookTwitter "
"

The wrangling over the corporate tax bill in Congress continues, but prospects for passage before the election are becoming increasingly dim.



That may not be a bad thing because the bill is the worst show of special interest tax lobbying in years. Hundreds of narrow provisions litter the bill, illustrating congressional sausage‐​making at its most complex. The list of provisions in the Senate version of the bill is 7 times longer that the list of provisions in last year’s income tax cut bill. 



This sausage started out as a lean reform bill, which responded to a World Trade Organization ruling a special tax break for U.S. exporters was illegal. The House and Senate both agreed the break should be repealed to comply with WTO rules. Then each chamber added some meaty reforms to simplify the tax code and help U.S. corporations compete abroad. 



Currently, the United States applies uniquely complex rules to the foreign activities of its corporations. These rules are so complex that, for example, four‐​fifths of Dow Chemical’s 7,800-page federal tax return relate to its foreign investments. 



Proposed reforms to such items as the “interest expense allocation rules” and “foreign tax credit baskets” would help U.S. firms compete with firms based in countries with less burdensome systems. 



But that’s where the lean ends and the pork begins. The House bill includes tax breaks for alcohol fuels, electric vehicles and $10 billion for tobacco farmers. The Senate bill includes dozens of tax incentives for coal, oil and gas, fuel cells, biodiesel and other energy activities. 



While most such special interest provisions distort the economy, some may be justified if they make the tax code simpler and more neutral. For example, the House bill repeals the 10 percent excise tax on fishing tackle boxes. Tackle box manufacturers apparently suffer because fishermen buy similar utility and sewing boxes that don’t face the tax. In this case, repeal makes sense to neutralize an existing distortion. 



However, the biggest item in the House and Senate bills is a tax cut for manufacturers, which would add a large distortion to the tax code. The House bill would reduce the tax rate for manufacturers from 35 percent to 32 percent while the Senate bill would create a special deduction. That would increase tax code complexity and put manufacturers into a separate lobbying camp less interested in overall tax reforms. 



Many other industries add great value to the U.S. economy such as financial services. Shouldn’t tax policy encourage growth in those industries as well? 



There is a better way. If the House and Senate can’t reconcile their different bills, Congress should start fresh with a simple across‐​the‐​board corporate tax rate cut. That would provide a direct competitive response to the recent decline in tax rates around the world. 



The average corporate tax rate for the 30 major industrial countries has fallen from 38 percent in 1996 to just 30 percent today. By contrast, U.S. corporations face a rate of about 40 percent, including the 35 percent federal rate plus an average state rate of 5 percent. 



The downward trend in global corporate tax rates is expected to continue, further increasing competitive pressures on U.S. companies. This will cause investment to gravitate toward countries with more attractive tax climates, reducing U.S. productivity and wages. Another problem with a high tax rate is it creates a big incentive for firms to adopt Enron‐​style tax shelters to move paper profits abroad. 



Most analysts would agree a corporate rate cut makes economic sense, but point to the huge budget deficit as a barrier to reform. The solution is to combine a corporate tax cut with an equal cut of federal spending on corporate subsidies, which total about $90 billion yearly. Such a reform package would reduce special handouts that distort the economy while spurring growth across all industries. 



Because such a clean reform package would have to overcome special interest lobbying, presidential leadership would be needed. The current corporate tax bill is a mess partly because the Bush White House has hinted from the sidelines it would sign any pork barrel bill that passes Congress. White House invisibility on the issue has been irresponsible, but perhaps understandable, given election year politics. 



The good news is Democratic presidential hopeful Sen. John Kerry has introduced his own corporate tax plan, which gives the White House cover to begin engaging the issue. Mr. Kerry’s plan includes wrongheaded rule changes for foreign investment, but he does propose a small corporate tax rate cut. Mr. Kerry also says he favors cutting corporate subsidies. 



President Bush should one‐​up Mr. Kerry and draft a reform bill that cuts the tax rate enough to put U.S. companies on an equal footing with foreign competitors. 



While the public may think both Republicans and Democrats shower corporations with big tax breaks, the truth is corporations have not received a substantial tax cut in decades. The landmark 1986 tax act cut the corporate tax rate, but raised corporate taxes overall. The depreciation tax cut enacted in 2002 expires at the end of this year. Thus there is a pent‐​up demand for corporate tax reforms, especially since other countries have cut tax rates so much. 



That pent‐​up demand is why the corporate tax bill has generated such a lobbying frenzy this year. The Bush administration can play a constructive role by channeling that frenzy into tax changes that are good for the whole economy, not just businesses on the inside track in the halls of Congress.
"
"

 ** _Congress should_**



• recognize that the U.S. economy has experienced a downward shift in the long‐​term rate of growth, with the pace of growth one‐​half to two‐​thirds that of the 20th century;  
  
• understand the huge impact of the growth slowdown: if current growth rates persist, a person born in 2000 will spend her retirement years in an economy half as big as it would have been if 20th‐​century growth rates had been maintained;  
  
• recognize that reversing the growth slowdown should be a top priority across the political spectrum, as disagreements about how to divide the economic pie are subordinate to the common interest in a larger pie;  
  
• understand that policy changes can have a significant positive effect on the long‐​term rate of growth; and  
  
• seek out pro‐​growth reforms that attract support across the political spectrum, so that the common interest in higher growth is not a casualty of partisan polarization.



The 21st century has witnessed a major downward shift in the trajectory of U.S. economic growth. From 1900 to 2000, real (i.e., inflation adjusted) gross domestic product (GDP) per capita rose at an average annual rate of 2.1 percent. The long-term growth path remained remarkably steady even in the face of massive macroeconomic fluctuations. For example, over the 20 years from 1929 to 1949 — a period that encompassed the twin convulsions of the Great Depression and World War II — the average growth rate clocked in at 2 percent per year, right on trend.



In contrast, from 2000 to 2015, annual growth in real GDP has averaged only 1 percent — half the rate of 20th-century growth. At the center of this story is the Great Recession of 2007–2009 and its aftermath. Between the fourth quarter of 2007 and the second quarter of 2009, U.S. GDP shrank by 4.2 percent — the sharpest decline since the Great Depression. Normally in U.S. economic history, severe recessions are followed by vigorous recoveries, but not this time. Instead, the economy experienced its weakest expansion since World War II. As of October 2013, 70 months after the recession began, real GDP had grown only 5.3 percent. In contrast, after the prior six recessions, real GDP growth averaged a robust 20 percent over the same period of time.



For most Americans, the economy has been performing even worse than these aggregate numbers suggest. With the rise in income inequality, the benefits of growth in terms of rising incomes are now skewed toward the upper reaches of the socioeconomic scale. As a result, most Americans have been experiencing not just a slowing rate of improvement, but stagnation and even decline. Real median household income (family income for Americans in the exact center of the income distribution) was 7 percent lower in 2014 than it was in 2000. Using different adjustments for inflation, it is possible to massage those figures to make them look slightly less bleak. It is impossible to massage them into looking good.



Unfortunately, the economy's sluggishness is likely to persist. At a Cato Institute conference in December 2014, two of the nation's leading experts on productivity growth presented long-term growth projections for the U.S. economy. Dale Jorgenson of Harvard University projected annual growth in aggregate real GDP of 1.75 percent, while John Fernald of the Federal Reserve Bank of San Francisco projected a slightly faster growth rate of 2.1 percent. After taking account of likely population growth, Jorgenson's projection puts annual growth in real GDP per capita at below 1 percent, while Fernald's comes in under 1.4 percent. In other words, in Fernald's more optimistic scenario, growth in the years to come will be more than a third off its 20th-century pace, while in Jorgenson's scenario, the long-term growth rate has been cut in half.



The reasons for Jorgenson and Fernald's pessimism have nothing to do with any recent events — neither lingering effects from the severe recession nor problems with policies enacted in its wake. Rather, both recognize the impact of deep-seated factors that once propelled growth but that now have shifted in an unfavorable direction.



To understand what's going on, let's break down measured economic growth into the constituent elements tracked by conventional growth accounting: (1) growth in labor participation, or annual hours worked per capita; (2) growth in labor quality, or the skill level of the workforce; (3) growth in capital deepening, or the amount of physical capital invested per worker; and (4) growth in so-called total factor productivity, or output per unit of quality-adjusted labor and capital.



Over the course of the 20th century, these various components fluctuated in their contributions to overall growth. The fluctuations, however, tended to offset each other, so that the long-term trend line of growth overall remained stable. In the 21st century, however, this pattern of offsetting fluctuations has come to a halt as all growth components have fallen off simultaneously.



One way to get faster growth is for more people to work or for people to work longer hours. Between the mid-1960s and 2000, average annual hours worked per capita surged from under 800 to above 950, powered by rising labor force participation among women and the influx of baby boomers into the work force. Since 2000, however, the labor force participation rate for both men and women has been in steep decline: from its overall peak of 67.3 percent in early 2000, it has dropped all the way to 62.7 percent as of June 2016, the lowest rate since 1978 (Figure 6.1). With labor hours shrinking, output per worker hour (otherwise known as labor productivity) has to rise just to keep the economy from shrinking. Accordingly, hours worked per capita has gone from providing a strong tailwind for growth to now resisting growth with a stiff headwind.





SOURCE: Data from 1900–2005 come from Valerie Ramey, http://www.econ.ucsd.edu/~vramey/research.html. Data from 2006–2010 are a recreation of Ramey's methods using Current Population Survey civilian hours plus Ramey's military figures divided by the U.S. Census's annual population figures.



While the quantity of labor supplied is an important factor in determining output, so is the quality of labor. Since workers with higher skills and more experience produce more output in a given hour than do their less skilled, more junior colleagues, boosting the skill level of the workforce is an important way to create economic growth. Over the course of the 20th century, huge investments in mass schooling — first at the secondary level, then at the postsecondary level — led to a much more highly skilled workforce. Harvard economists Claudia Goldin and Lawrence Katz estimate that rising educational attainment accounted for about 15 percent of total growth over the period 1915–2005. But the rate of increase in years of schooling per worker has slowed dramatically in recent decades: the pace of improvement between 1980 and 2005 was less than half that during the period between 1960 and 1980 (Figure 6.2). And looking ahead, the educational level of the workforce is expected to plateau. Another important source of growth has therefore petered out.





SOURCE: 1900–1939: ""120 Years of American Education: A Statistical Portrait"" found at http://0-nces.ed.gov.opac.acc.msmc.edu/pubs93/93442.pdf. 1940–1990: Current Population Survey of the BLS Table A-1. 2000–2010: UNDP, http://hdrstats.undp.org/en/indicators/103006.html.



An additional source of growth is investment: workers with more and better tools are able to produce more. Unfortunately, net national investment (investment net of depreciation charges) as a percentage of net national product has been falling for decades, dragged down by the more widely reported drop in the national savings rate (Figure 6.3). There is therefore no current basis for expecting a surge in investment to counteract the unfavorable trends regarding hours worked and educational attainment.





SOURCE: Jagadeesh Gokhale, Cato Institute.



In the case of labor hours, worker skills, and investment, growth is created by adding more inputs. If you increase inputs with more hours worked, more training, and more equipment, then you will produce more output. The final source of growth, innovation, involves figuring out how to get more output from a given set of inputs — either through inventing new products or by developing more efficient production processes. Economists' best measure of innovation is known as total factor productivity (TFP) growth: the increase in output from a given unit of labor and capital. From 1996 to 2004, TFP growth surged after a long slump that began in the 1970s; since 2004, however, TFP growth has returned to the low rates of decades past (Figure 6.4). Admittedly, shifts in the rate of TFP growth are unpredictable, so it is possible that another round of rapid growth is just around the corner. But at present, no signs of such a turnaround are visible.





SOURCE: 1871–1950: Robert Gordon's 1999 ""U. S. Economic Growth Since 1870: One Big Wave."" 1950–2011: The Bureau of Labor Statistics, ""MFP Tables: Historical multifactor productivity measures (SIC 1948–87 linked to NAICS 1987–2011),"" Nonfarm Tables, June 2013, http://www.bls.gov/mfp/.



In the 21st century, the U.S. economy has thus experienced simultaneous weakening in all four components of economic growth. This does not mean that slow growth is inevitable from here on out: the current trends are not set in stone. Nevertheless, it is difficult to avoid the conclusion that the conditions for growth are less favorable than they used to be.



The long-term implications of a 50 percent drop in the growth rate are huge. With real GDP per capita rising 2 percent a year, output per capita doubles in around 35 years. With the growth rate cut to 1 percent, doubling takes 70 years. Accordingly, if 21st-century growth rates persist, a person born in 2000 will spend her old age in an economy only half as rich as it would have been if 20th-century growth rates could have been extended.



Conservatives and libertarians should require little convincing that more economic output is generally something to be desired, and that therefore the prospect of a prolonged growth slump is a matter of serious concern. Progressives may be more skeptical. In particular, they might object that, because of income inequality, all the extra output created by higher growth wouldn't translate into commensurate income gains for most Americans.



That's an understandable concern, but disappointment with the pace of median income growth will hardly be assuaged by a slowdown in the economy's overall expansion. In the era of rising inequality and falling median wage growth that began in the 1970s, there has been only one period of strong growth in real earnings: the late 1990s, when GDP and productivity growth were surging as well. Thus, even in an age of income inequality, faster growth redounds to the benefit of average workers. Indeed, in the current circumstances, it appears that only strong growth can stave off stagnation or disappointingly sluggish growth in workers' pay.



Many progressives might argue that, even with faster growth, the gains from economic growth are no longer shared widely enough. More redistribution, they contend, is needed to correct the imbalance. Libertarians and conservatives can be counted on to dispute the point, but that is an argument for another day. For present purposes, it suffices to point out that slow growth will make it all but impossible to fund any more generous provision for the less well-off.



Because of the aging U.S. population and rising health care spending, entitlement spending on the elderly figures to put the squeeze on everything else the federal government does. According to the Congressional Budget Office, spending on Social Security and the major federal health care programs is projected to balloon to 14 percent of GDP by 2039, double the 7 percent average over the past 40 years. Meanwhile, spending on everything besides interest payments would fall to 7 percent of GDP, well below the 11 percent 40-year average — indeed, a smaller share of GDP than at any time since the late 1930s. And even with this hit to the relative size of nonentitlement spending, by 2039, federal debt as a share of GDP is projected to hit the all-time historical peak of 106 percent (set back in 1946 at the end of World War II). It will continue upward from there. To put it mildly, this is not a fiscal environment that augurs well for big new federal social programs.



Alas, we cannot simply grow our way out of this predicament. Yes, higher growth directly inflates the denominator of the debt-to-GDP ratio; but it also leads to increased spending under entitlement programs and thus works indirectly to boost the numerator as well. Nevertheless, faster growth does mean a considerably larger economy over the longer term and, consequently, more resources available for funding income transfers than would otherwise be the case.



Consequently, progressives and libertarians should be united on the desirability of higher growth. They may have different ideas about what to do with the extra money, but both sides have a stake in getting the chance to fight it out.



In addition to economic reasons for favoring higher growth regardless of ideology, there are also important political reasons. Indeed, in our present situation, the political stakes have assumed an urgency that swamps any calculations of mere dollars and cents. The health of American democracy and the basic character of American society are now on the line. As Harvard economist Benjamin Friedman documented in _The Moral Consequences of Economic Growth_ , a prosperous, growing economy promotes the democratic virtues of tolerance and openness. When incomes and living standards are rising generally, the welfare of other groups is less likely to be perceived as a threat to one's own. But when the economy stagnates, gains for some necessarily mean losses for others. In this zero-sum environment, the ugly, defensive reactions of bigotry, xenophobia, and belligerent nationalism gain traction.



With the recent rise of authoritarian populism here in the United States, as well as in countries across Europe, the broader, political implications of the growth rate are no longer a matter of merely theoretical concern. Fundamental American ideals are under active assault, and all who still hold to those ideals, regardless of their position on the political spectrum, need to recognize the role that the growth slowdown is playing in strengthening the other side. Reviving economic growth is perhaps the most potent means at our disposal to counter and defeat this illiberal challenge to the country's founding principles.



Can anything be done to stir the economy out of its current doldrums? The heartening answer is yes, absolutely. Current trends in labor participation, labor quality, investment, and innovation point to a permanent reduction in the U.S. economy's long-term growth path, but those trends do not exist in a vacuum. They are situated in the larger context of the nation's laws and economic policies, which combine to shape the incentives of individuals and firms along countless different margins. If you change those laws and policies, then you can change those incentives; change those incentives, and you can change the economic trends.



The fact is — and it's hard to imagine who would disagree — that American public policy is far from optimal when it comes to facilitating economic growth. Look at the factors that shape each component of growth and you will find laws and policies that push in the wrong direction: laws and policies that discourage participation in the labor force, frustrate accumulation of human capital, deter productive investment, and inhibit innovation or block its diffusion throughout the economy. In the circumstances, this is good news: it means that there is wide room for improving public policy, and consequently wide room for improving economic performance.



To explore the wide variety of possible pro-growth reforms, the Cato Institute hosted a special online forum during late 2014, in which 51 of the nation's top economists and policy experts were asked to identify one or two policy changes that could trigger faster economic growth, whether temporarily through a one-time change in the level of output or indefinitely through accelerating the growth rate. The proposed reforms covered a long list of policy domains: tax policy; budget policy; education and training policy; health care financing policy; financial regulation; monetary policy; health, safety, and environmental regulation; regulations on starting a business; trade policy; immigration policy; intellectual property law; land use regulation; and even foreign policy. In addition, some of the contributors have advocated what might be called ""meta policy"" changes — that is, reforms to the policymaking process rather than specific substantive changes to rules or programs.



While everyone might agree on the need to change public policy, finding agreement on what particular changes to make is considerably trickier. After all, American politics today is characterized by deep ideological divisions and intense partisan polarization over government's proper role in the economy. There are many fronts in the political conflicts of recent years: the trajectory and composition of federal spending, the level and structure of taxation, health care policy, regulation of the financial sector, immigration, climate change, and environmental regulation more generally. All involve policy issues with important implications for the level of output or the permanent rate of growth. On these questions, and many others besides, the respective sides are miles apart when it comes to the proper direction of policy change. Yes, there is a shared interest in continued healthy economic growth that transcends the left-right divide. But agreement on ends need not translate into agreement on means, and in the present case, there is disagreement aplenty.



Under current political conditions, the most promising path forward is to identify policy ideas that are not already the subject of high-profile, politically polarized debate. America's growth slowdown is a new problem, and policy responses that address that problem are more likely to gain traction if they are not recycled ideas originally put forward to address other problems. And if a policy idea is already clearly associated with either the left or the right, in today's highly contentious environment, it is all but guaranteed that the other side will fight tooth and nail against it. That makes progress of any kind difficult in the absence of large congressional majorities and unified partisan control of the White House and Congress.



The good news is that, notwithstanding the extent of polarization, it is still possible to construct an ambitious and highly promising agenda of pro-growth reforms that steers largely clear of the red-versus-blue divide. Chapter 7 explains how.



Friedman, Benjamin. _The Moral Consequences of Economic Growth_. New York: Vintage Books, 2006.



Lindsey, Brink. ""Why Growth Is Getting Harder."" Cato Institute Policy Analysis no. 737, October 18, 2013.



\---. ""Low-Hanging Fruit Guarded by Dragons: Reforming Regressive Regulation to Boost U.S. Economic Growth."" Cato Institute White Paper, June 22, 2015.



Lindsey, Brink, ed. _Reviving Economic Growth: Policy Proposals from 51 Leading Experts_. Washington: Cato Institute, 2015.



\---. _Understanding the Growth Slowdown_. Washington: Cato Institute, 2015.
"
"There are an awful lot of insects. It’s hard to say exactly how many because 80% haven’t yet been described by taxonomists, but there are probably about 5.5m species. Put that number together with other kinds of animals with exoskeletons and jointed legs, known collectively as arthropods – this includes mites, spiders and woodlice – and there are probably about 7m species in all. Despite their ubiquity in the animal kingdom, a recent report warned of a “bugpocalypse”, as surveys indicated that insects everywhere are declining at an alarming rate. This could mean the extinction of 40% of the world’s insect species over the next few decades.  What is particularly worrying is that we don’t know exactly why populations are declining. Agricultural intensification and pesticides are likely a big part of the problem, but it’s certainly more complicated than that, and habitat loss and climate change could also play a part. Although some newspaper reports have suggested that insects could “vanish within a century” total loss is unlikely – it’s probable that if some species die out, others will move in and take their place. Nevertheless, this loss of diversity could have catastrophic consequences of its own. Insects are ecologically important and if they were to disappear, the consequences for agriculture and wildlife would be dire.  It’s difficult to overstate how many species there are. Indeed, the 7m estimate above is likely a major underestimate. Lots of insects that look alike – so-called “cryptic species” – are distinguishable only by their DNA. There are an average of six cryptic species for every easily recognisable kind, so if we apply this to the original figure, the potential total number of arthropods balloons to 41m. Even then, each species has multiple kinds of parasites which are mostly specific to just one host species. Many of these parasites are mites which are themselves arthropods. Conservatively allowing just one kind of parasitic mite per host species brings us to a potential total of 82m arthropods. Compared with only around 600,000 vertebrates – animals with backbones – that’s 137 species of arthropod for every vertebrate species. Astronomical numbers like these caused the physicist-turned-biologist Sir Robert May to observe that “To a good approximation, all [animal] species are insects.” May was good at guessing big numbers – he became the UK Government’s chief scientist – and his quip in 1986 now seems pretty close to the mark. 


      Read more:
      Insect 'Armageddon': five crucial questions answered


 That’s just diversity though. How many individual insects would be lost in a mass extinction? And how much might they weigh? Their ecological importance will likely depend on both measures. It turns out that insects are so numerous that even though they are small, collectively their weight far outstrips that of the vertebrates. Perhaps the most celebrated ecologist of his generation, the Harvard ant enthusiast E.O. Wilson estimated that each hectare (2.5 acres) of Amazonian rainforest is inhabited by only a few dozen birds and mammals but well over one billion invertebrates, almost all of which are arthropods. That hectare would contain about 200kg dry weight of animal tissue, 93% of which would be made up of invertebrate bodies, and a third of that being just ants and termites. This is uncomfortable news for our vertebrate-centric view of the natural world. The role allotted to all these tiny creatures in the grand scheme of nature is to eat and be eaten. Insects are the key components of essentially every terrestrial food web. Herbivorous insects, which make up the majority, eat plants, using the chemical energy plants derive from sunlight to synthesise animal tissues and organs. The job is a big one, and is split into many different callings.  Caterpillars and grasshoppers chew plant leaves, aphids and plant hoppers suck their juices, bees steal their pollen and drink their nectar, while beetles and flies eat their fruits and devastate their roots. Even the wood of huge trees is eaten by wood-boring insect larvae.  In turn, these plant-eating insects are themselves eaten, being captured, killed or parasitised by yet more insects. All of these are, in their turn, consumed by still larger creatures. Even when plants die and are turned to mush by fungi and bacteria, there are insects that specialise in eating them. Going up the food chain, each animal is less and less fussy about what kind of food it will eat. While a typical herbivorous insect might consume only one species of plant, insectivorous animals (mostly arthropods, but also many birds and mammals) don’t much care about what kind of insect they catch. This is why there are so many more kinds of insect than birds or mammals. Because only a small fraction of the material of one kind of organism is transformed into that of its predators, each successive stage in the food chain contains less and less living matter. Even though efficiency in this process is known to be greater higher up the food chain, the animals “at the top” represent only a few percent of the total biomass. This is why big, fierce animals are rare. And so it’s obvious that when insect numbers decrease everything higher up in the food web will suffer. This is already happening – falling insect abundance in Central American tropical forest has been accompanied by parallel declines in the numbers of insect-eating frogs, lizards and birds. We humans ought to be more careful about our relationship with the little creatures that run the world. As Wilson commented: The truth is that we need invertebrates, but they don’t need us. Knowing about insects and their ways is not a luxury. Wilson’s friend and sometime colleague Thomas Eisner said: Bugs are not going to inherit the earth. They own it now. If we dispossess them, can we manage the planet without them?"
"The tropical cyclone rampaging south-eastern Africa has been described as one of the worst disasters ever to strike the southern hemisphere, with up to 2.6m people potentially affected in Mozambique, Malawi and Zimbabwe. The death toll may not be known for months, but it is already likely to have run to hundreds and possibly thousands of people. The brunt of the disaster has been borne by the coastal city of Beira in central Mozambique, 90% of which has been reportedly destroyed.  It is inevitable that people will connect Idai and climate change. It is always tricky to establish a direct causal link, but thanks to the evidence provided by a number of reports from the Intergovernmental Panel on Climate Change (IPCC), including this most recent one from October 2018, we know that climate change is bound to increase the intensity and frequency of storms like Idai. At the very least, this crisis is a harbinger of what is coming. Knowing this is a luxury that we must not squander. The IPCC estimates we have 12 years to prevent the Earth’s climate from crossing the 1.5℃ warming threshold, beyond which the effects are likely to become significantly worse. We should be spending this time both trying to minimise the increase in global temperatures and making people more prepared for similar events in future. The West has a duty to shoulder most of the burden here, for reasons we’ll explain in a moment. To do so, it needs to rethink its entire approach to international development.  Over the next few weeks, we may hear that Beira’s geography makes it particularly prone to natural disasters. We may hear that the region lacks an efficient early-warning system to alert its population. We may even hear some victim-blaming rhetoric that local people refused to leave despite being warned. Arguments like these all obscure a much more important explanation for what has happened.  It is not only the intensity of environmental disasters that makes them devastating – poverty also has a huge bearing on how things play out. Houses in poorer areas will often be less stable, storm barriers may be weaker, sanitation is often a problem, emergency services will be poorly resourced – and preventing disease outbreaks may be hindered by the poor state of public health services. The list of disadvantages goes on and on. Good example are hurricanes Katrina and Sandy in the US. Katrina struck New Orleans and the surrounding region in 2005 while Sandy hit New York and New Jersey in 2012. Sandy hit a far more densely populated area, but the death toll was at least five times lower than Katrina and it only caused about half the damage.  While Sandy was a category three storm to Katrina’s five, this was certainly not the only reason for the disparity. New Orleans, one of the poorest cities in the US, had poorly constructed levees which were easily overcome by the flood. Many people did not have cars, so couldn’t evacuate easily when the authorities told them to.  The earthquakes that struck Haiti and Japan in 2010 and 2016 respectively are another example. Both were of similar magnitude, but between 100,000 and 316,000 died in Haiti while in Japan it was just 42. One reason the Haitian disaster was so much worse  was the many thousands of unstable shanty houses in Port-au-Prince.  Inequalities within countries matter as well. The most vulnerable people are usually women, children, the poor, the elderly, ethnic minorities or indigenous people. Hurricane Katrina hit the elderly poor of New Orleans disproportionately hard, for instance, since they found it hardest to escape.  In all this inequality, the world’s wealthiest countries are heavily culpable. It stems from a complex economic system that disadvantages the Global South – not to mention the centuries-long experience of colonialism, the effects of which have hampered human development until this day.  In a world where 26 billionaires own as much wealth as the bottom half of humanity, the prospect of more frequent and intense climate disasters is only bound to exacerbate those inequalities. At the same time, Mozambique, Malawi and Zimbabwe contribute only a small fraction of the emissions that are causing such disasters. The West’s responsibility – along with other 
big emitters such as China – is therefore also a matter of climate justice. Part of that responsibility lies in changing the current approach to disaster aid. In major donor countries such as the US and UK, the guiding modus operandi of disaster relief has been reactive as opposed to proactive measures. The UK spent £1.2 billion in 2018/19 on emergency responses such as humanitarian interventions, while disaster prevention and preparedness has received a mere £76m. In the case of Cyclone Idai, the Department for International Development has now earmarked £18m to assist humanitarian relief efforts in Mozambique and Malawi – tripling the original pledge from a couple of days earlier.  To be clear, humanitarian responses are absolutely key, but insufficient on their own. They bandage wounds rather than fix what caused them. Instead, donor countries need to prioritise identifying the most vulnerable people both before and after a disaster, and ensure they receive the required support and are granted the agency to be actively involved in the process.   Besides the high-profile attempts to reduce global emissions, countries such as the UK should be offering support to poorer countries with everything from building flood defences to supporting social services to transferring technology. They should be forgiving national debt, redistributing wealth or at least giving them preferential trade deals to help them adapt to climate change themselves. This requires a rethinking not just of humanitarian aid but of development assistance in general.  Fortunately this is not just blue skies thinking on our parts. The House of Commons International Development Committee is currently reviewing the aid budget and considering an approach built around climate justice. This emerging discipline is gaining traction and credibility around the world and will be the subject of a World Forum taking place in Glasgow in June. Ahead of that, Tahseen Jafry – one of the co-authors of this article – will be making a keynote presentation in New York in April.  In short, it feels like momentum is steadily building. The UK caused a disproportionately large part of climate change. Now it ought to show leadership by pioneering a new approach to development that has inequality at the top of the agenda.  Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

Despite the overwhelming evidence that markets perform best when left alone by the government, it is still virtually taken for granted that one consumer product should be completely controlled by every government in the world. One product, so ubiquitous, that it’s used by almost everyone in the world on a daily basis: money.



Money is vitally important; the lifeblood of our financial system, but it is a product nonetheless. Consumers use this product not just as a medium of exchange but also as a liquid store of value and as a basis for accounting. Money producers, i.e., central banks, profit through seignorage, the ability to earn interest on their assets while issuing notes, e.g., dollar bills that pay no interest. The Fed creates money from thin air when it buys an interest bearing Treasury security and credits the seller with dollars. These dollars are not backed by anything in the sense that the Fed is not obligated to buy or convert dollars into anything.



Now there would be nothing wrong with this if the Fed were just some private institution trying to earn a living in an unregulated market. But of course, the Fed is a government‐​protected monopoly. Even prior to the Fed’s creation in 1913 there was always some level of government regulation of money. Prior to the Civil War, private banks used to issue dollar notes that were convertible into gold. Scholars debate whether the banking crises and panics during this period were a product of government regulation of these currencies and banks. What is clear is that there was never a completely free market in currency issuance. Well, the time has come.



To believe that the Fed is necessary is to believe that money is such a special product that it is optimal to give power to a group of expert economists either to use their best collective judgment in setting policy or to remove their discretion and create certain rules for them to administer. The only alternative is to eliminate government control of money.



Let’s examine these beliefs. Currently, Fed policy is essentially targeting low inflation. There are discussions about whether there is sufficient productivity growth to allow a high rate of GDP growth but the bottom line is protecting against an increase in inflation. While the target is not explicit, I would argue that markets are free enough and evolved enough to force the Fed towards this implicit target. 



The Fed controls the spot interest rate market (Fed funds) by creating or destroying spot dollars. All the other points along the money market yield curve are more or less determined without government interference. If the market feels the Fed is being lax on inflation, all the non‐​spot money market rates will rise and spot dollars will be sold for inflation hedges like commodities and foreign currencies. These trends will continue until the market catches the Fed’s attention and the Fed funds rate target is increased.



A more efficient policy would be to announce an explicit inflation target. With the continued development of the inflation protected bond market, which trades on real rates, this policy could be accomplished by explicitly targeting the market’s expected inflation rate (a first order approximation is calculated by taking the geometric difference of real rates from the nominal rates of comparable bonds). The Fed could choose the means by which to affect the markets. But this still leaves the problem of what inflation rate to target and how to define an inflation index.



It is arguable whether stock and real estate prices should be included in an inflation index because price bubbles are destabilizing and an indication that monetary policy is too loose. There certainly are non‐​linear effects in markets that can cause self‐​reinforcing trends that push markets away from their fundamentals. But bubbles, at best, can only be defined after the fact. Like an Oliver Stone movie where lack of evidence is an indication of the strength of the conspiracy, prices continuing to go up are evidence of the increasing size of the bubble! The term is meaningless to describe current conditions because it can’t be disproved; prices go up because there is a bubble; prices go down because there was a bubble, either outcome is proof of the existence of a bubble. So let’s stick with CPI.



Changes in the CPI can occur for non‐​monetary reasons (i.e., events independent of too much or little money creation). If the Fed is going to target CPI and minimize inflation due to monetary policy it has to be able to adjust for these non‐​monetary events. Central bankers are happy (and correct) to overlook price rises from supply shocks, such as droughts. The flipside is that price drops due to supply shocks coming from productivity growth also need to be adjusted for. If some innovation allows the cost of producing widgets (of equal quality) to drop the credit should not go to monetary policy. 



So the ability to measure productivity is critical regardless of whether an inflation target is explicit or implicit. Even with the way current Fed policy is formulated, productivity is at the core of the debate over the appropriateness of policy. Higher productivity growth means the economy is running more efficiently and can therefore run faster without increased inflation. But the technological and financial advances that we have seen and will continue to see (if the government can stay somewhat out of the way) are quickly transforming our ability to define let alone measure productivity. 



In the economy of the 1950s productivity was easy to measure as it was largely a function of widgets produced per man‐​hour. Today more and more firms are valued not for how many widgets they produce but for the quality of the ideas they produce — their intellectual property. These ideas may not generate any revenue today but in the future, and can therefore only be calculated today by looking at a firm’s market capitalization, a highly volatile measure. (This is not to argue that a firm’s market value is a perfect measure but that a priori it is the best unbiased measure available.)



Consider a firm whose sole function is to develop patents for licensing. The only quantitative measure of their output will be the number of patent applications generated per man‐​hour. However, what is important is the economic value of these patents, which may not correlate with the number of patents generated. This economic value can only be ascertained by seeing what value the market gives to the company.



While there may not be a lot of pure research companies like this, these types of companies effectively reside within virtually every tech firm. The research aspect of a tech firm is perhaps its most vital part because the trade secrets and patents produced are the engine of its future growth.



Many would be happy to sweep these issues under the table because, in their view, the economy is doing great and the Fed deserves much of the credit. In other words, if it ain’t broke don’t fix it. After all, the stock market has had an incredible run (despite being down some this year), the dollar is strong, inflation is seen to be under control, and the last time unemployment was this low Bill Clinton was busy not inhaling. Going back to the start of 1995, the S&P 500 has more than tripled and GDP growth has been 4.1 percent (go back further and this rate drops).



But as good as things have been, they fall short in comparison with the average GDP growth of over 5 percent in the hundred years prior to 1971, when Nixon closed the gold window and floated the dollar. This may not sound like much of a difference but it means that national income would have been more than 25 percent higher during these past five years had the economy been growing at a historically average rate.



Additionally, while inflation is low, it isn’t zero (even considering any and all measurement biases). So our money is still being debased, albeit at a slow pace. Given the tremendous technological advances and the subsequent increases in productivity, we should really see falling prices if the dollar were truly maintaining its usefulness as a store of value. Not a 1930s‐​style severe deflation that was caused by a variety of bone‐​headed fiscal and monetary policies that squashed both demand and supply. Just a steady, modest price decline reflecting productivity growth adjusted for quality improvement in products and services, where even products whose quality stays the same become cheaper to produce, raising the standard of living for all.



This recent period has also had many advantages over the prior period. With the Cold War over and defense spending being cut, valuable capital and human resources have been deployed in far more productive activities. Financial markets have evolved to be far more efficient, sophisticated, and global than a generation ago, allowing capital to be allocated more efficiently.



U.S. corporate performance has reaped the benefits of the restructurings during the 1980s. Granting employee options is now prevalent. This gives both better incentives to workers and provides a tax‐​advantaged way of compensation resulting in a far more efficient way to pay staff.



Yet despite these advantages we can’t seem to match the growth rates of the past. Of course this foregone income cannot all be chalked up to the failings of monetary policy. Certainly the enormous increases in taxes, regulation and government spending bear much of the blame.



But a truly market‐​based monetary policy might not have been such a willing accomplice to the increasing encroachment of government into the private sector. Without the implicit inflation tax and monopoly profits from the Fed, government would be forced explicitly to raises taxes (politically difficult) or to lower spending and deregulate. If the latter path is accompanied by tax cuts, experience shows that the economy benefits. In other words, the dynamic of tighter monetary policy (to eliminate current inflation) hurting short‐​term growth can be completely offset by fiscal policy. (Shockingly, many still hold to the Keynesian notion that increasing spending is the fiscal antidote to slowing growth when, in fact, it will exacerbate poor economic conditions by displacing the market’s more efficient allocation of resources. Japan in the 1990s is the latest example of this although they suffer under many other bad policies too.)



There’s certainly reason to have confidence that competent Fed officials will be able to avoid a sustained resurgence of inflation. It is also clear, as shown above, that there are many reasons to believe that we should be doing better and that therefore monetary policy is not optimal. The simple fact is that it is not just hard but impossible to tell what exactly the right policy should be at any given point in time. Economies are just too dynamic and are composed of too many players. No matter how smart members of the Fed may be, it makes no more sense to have them set monetary policy that it did for elite economists in centrally planned economies to set the price of food, cars or any other product.



Even if it were possible for Fed members to have completely mastered the art of central banking, is it healthy to vest so much power in one person or group of people? Imagine how the markets would react if Greenspan should have a very sudden demise.



So, how do we solve this problem of imperfect people using imperfect data creating a one size fits all policy? The first step is to remove discretion over policy from the government. The explicit targeting of inflation, previously mentioned, accomplishes this except that setting the goal would still be discretionary and likely to be sub‐​optimal given productivity measurement problems. An alternative is a return to a gold standard as way to effectively automate monetary policy. Despite its popularity in some quarters there are serious problems with any type of gold standard.



Go ahead and ignore the fact that no country has ever been able to maintain a gold standard (perhaps with Hong Kong and Argentina both having kept their currencies fixed to the dollar even during periods of severe economic distress, it is an indication that times have changed). The main flaw in fixing the dollar price of gold is that it assumes that the value of gold doesn’t change, like some physical constant, as immutable as the maximum speed of light. Gold’s value comes in two parts. First as a commercial product with limited use as an industrial metal and as jewelry or ornamentation. Clearly, the price of any commercial product will vary for a variety of reasons having nothing to do with an economy’s general price level.



The other component of gold’s value derives from its long history as a store of value and is wrongly assumed to be intrinsic. The supply of gold varies as new mines are found and mining techniques improved. Demand for gold as a store of value is based on the ability of gold to compete against other liquid stores of value. So, the only advantage of gold over a fiat currency is that the production of gold is not controlled by the government (ignoring taxes and regulation).



But if we fix the dollar to something that floats are we accomplishing anything? If the supply of gold increases due to a new way to mine, the dollar price of gold needs to fall. (As an example if the supply of gold doubled as a result of productivity boosts and there was no change in the productivity of producing other goods, the dollar price of gold should be halved as to protect the purchasing power of the dollar.) If the dollar price of gold is held constant, as in the gold standard, inflation will ensue (which is historically what happened under these conditions). 



Irving Fisher attempted to solve this problem with his compensated dollar plan. The plan allows for the dollar price of gold to be adjusted by a CPI priced in gold. The problem is that this plan only works if productivity in the mining sector is the same as in all other sectors of the economy. Not a bad assumption, perhaps, in Fisher’s day but unlikely to be true now and even less likely in the future. Two other possible solutions are to let the price of gold vary based on the relative productivity of gold production to productivity in the rest of the economy or instead of fixing the price of gold, fix the dollar price of gold plus other commodities (i.e., define the dollar as some fixed basket of commodities).



Well, the first solution is pretty messy, as it would require the government to calculate productivity rates that as argued above are inherently impossible to measure precisely. Furthermore, even if productivity could be measured precisely, it could in practice only be measured with a time lag. Not very helpful for knowing today’s price of something.



The second solution sounds more workable but determining the composition of the basket that would define the dollar is tricky. Leave something out of the basket and you have the same problem of the relative production productivities of what’s in and out of the basket. So just about everything the economy produces must go in the basket (obviously there are diminishing returns to accuracy for each additional product added, so you start with the most important products and work towards things that are used less). Now with many products in your basket you need to determine relative weights to reflect relative usage and importance of these products. But the resultant basket is just the same as that used to calculate inflation (with the identical problems as mentioned earlier)! Linking the dollar to this basket is exactly the same as having the Fed explicitly target zero inflation. 



The key to getting better monetary policy is not to merely limit the discretion of the government but to get the government out of the money business altogether by privatizing the Fed. Sell the whole thing to the highest bidder. The government would also have to deregulate enough to allow competitors to arise in the currency issuance game. 



By eliminating this government protected monopoly, more of the informational value of consumers ever changing preferences and behaviors could be used by producers to create a product that can best balance everyone’s needs. The free market can be looked at as a computer, calculating within all the constraints and using the near infinite amount of interrelationships and feedback between economic actors, on a global scale, to solve the problem of what are the best forms of money. This is an impossible calculation to do any other way.



Consumers would choose between U.S. dollars, euros, Citi dollars, GE dollars, etc. This choice would be based on confidence in the issuer and how well the product serves the consumer’s needs. Companies would issue money solely as a means to profit. Produce too much money and it becomes worthless, too little and not enough people will be able to use your money for you to profit. The notion of a government having a monetary policy goes the way of governmental industrial policy (or choosing which firms receive government help — see Asia). 



If private firms were allowed to compete on equal legal footing as a private Fed, currency competition would lead to better money just as market forces improve the quality of any product. In an unfettered environment, using precious metals as a backing for a currency is just a starting point. Nobody can predict the improvements and ingenious ideas that would emerge. Already consumers have reaped benefits from quasi‐​currencies like airline miles and credit card rebates in the form of products (clearly, these things don’t currently have the liquidity to make for a good form of money). 



There are usually several objections to private money. It is argued that it is necessary to have a lender of last resort in times of crisis and that only the Fed can fill this role. Of course, there is no reason why a private central bank couldn’t provide sufficient liquidity during a crisis and no reason why private regulators couldn’t get big financial institutions together to provide liquidity (J.P. Morgan did this in the panic of 1907, prior to the Fed’s creation). Also prior to the Fed’s creation, private clearinghouses would lend to members who were solvent but needed liquidity. Furthermore, using the Fed as a lender of last resort creates moral hazard. If the perception is that the Fed will be compelled to act in a crisis, the effect will be to allow participants to take more risk, as they believe they are receiving some downside protection from the Fed.



In 1998, a financial crisis that started in Asia threatened world markets. “Contagion” was used to describe the effect of countries with healthy economies seeing their markets roiled. The contagion spread through trade and capital flows between countries and from the market realizing countries with a certain economic profile (high current account deficits, low foreign exchange reserves, and a pegged currency) were vulnerable. Markets in developed countries became infected as credit spreads widened and volatility increased (a mathematical implication being that correlations between markets increase). To extend the biology analogy, the best defense for a population against mutant viruses is genetic diversity. The economic equivalent is the diversity of products and regulation that spring forth from a free market. This diversity would reduce the frequency of crises that might need a lender of last resort.



Another concern about a world with only private money is that things would be too complicated with all the exchange rates that would exist for these new currencies. Technology can easily solve this problem. Only want to see prices in one particular currency? Your handheld device or browser could automatically convert all prices and even facilitate conversion of your currency to one that would be acceptable to a merchant. Currencies that were too volatile would quickly fall into disuse, as part of their utility would always derive from them being a stable store of value.



As with any free market reform, there is no expectation that private money would lead to a perfect world where there are no crises or problems, just a better world. Better not just in a strictly utilitarian sense but also in a moral sense as people could store the fruits of their labor however they see fit and not be forced to submit to a tax (inflation) that is not explicitly levied and voted on.



Many involved in the information revolution are confident that with the help of new technologies, markets will continue to evolve and reach a point that government’s ability to tax and regulate will be stifled. What these optimists miss is that government too has the ability to evolve and counter these trends. Government’s monopoly on the use of force can trump any market innovation. Ultimately, it is necessary for the political climate to change before government will acquiesce and not try to fight the liberalizing effects of technology.



These changes are certainly not around the corner, but they should be our goals. 



Deregulate. Privatize. To improve our standard of living, to increase the level of freedom, and to set a powerful example for the world to follow. 
"
"
Foreword: I give thanks to Steve McIntyre for this analysis. Steve came to a conclusion similar to what I alluded to in my initial rebuttal where I said:
“For all I know, they could be comparing homogenized data from CRN1 and 2 (best stations) to homogenized data from CRN 345 (the worst stations), which of course would show nearly no difference.“
Steve does a superb job of deconstructing the memo’s undocumented results. Perhaps someday Dr. Thomas Peterson of NCDC will tell us how he did his analysis and show supporting data and methods. – Anthony
The Talking Points Memo
by Steve McIntyre reposted from Climate Audit
The NOAA Talking Points memo falls well short of a “full, true and plain disclosure” standard – aside from the failure to appropriately credit Watts (2009).
They presented the following graphic that purported to show that NOAA’s negligent administration of the USHCN station network did not “matter”, describing the stations as follows:
Two national time series were made using the same gridding and area averaging technique. One analysis was for the full data set. The other used only the 70 stations that surfacestations.org classified as good or best… the two time series, shown below as both annual data and smooth data, are remarkably similar. Clearly there is no indication for this analysis that poor current siting is imparting a bias in the U.S. temperature trends.

Figure 1. From Talking Points Memo.
Beyond the above sentence, there was no further information on the provenance of the two data sets. NOAA did not archive either data set nor provide source code for reconciliation.
The red graphic for the “full data set” had, using the preferred terminology of climate science, a “remarkable similarity” to the NOAA 48 data set that I’d previously compared to the corresponding GISS data set here  (which showed a strong trend of NOAA relative to GISS). Here’s a replot of that data – there are some key telltales evidencing that this has a common provenance to the red series in the Talking Points graphic.

Figure 2.  Plot of US data from www1.ncdc.noaa.gov/pub/data/cirs/drd964x.tmpst.txt
An obvious question is whether the Talking Points starting point of 1950 is relevant. Here’s the corresponding graphic with the 1895 starting point used in USHCN v2. Has the truncation of the graphic start at 1950 “enhanced” the visual impression of an increasing trend? I think so.

Figure 3. As Figure 2, but to USHCN v2 start
The Talking Points’ main point is its purported demonstration that UHI-type impacts don’t “matter”. To show one flaw in their arm-waving, here is a comparison of the NOAA U.S. temperature data set and the NASA GISS US temperature data set over the same period – a comparison that I’ve made on several occasions, including most recently here. NASA GISS adjusts US temperatures for UHI using nightlights information, coercing the low-frequency data to the higher-quality stations. The trend difference between NOAA and NASA GISS is approximately 0.7 deg F/century in the 1950-2008 period in question: obviously not a small proportion of the total reported increase.

Figure 4. Difference between NOAA and NASA in the 1950-2008 period.  In def F following NOAA (rather than deg C)
As has been discussed at considerable length, the NASA GISS adjusted version runs fairly close to “good” CRN1-2 stations – a point which Team superfans have used in a bait-and-switch to supposedly vindicate entirely different NASA GISS adjustments in the ROW, (adjustments which appear to me to be no more than random permutations of the data, a point discussed at considerable length on other occasions.)
For present purposes, we need only focus on the observation that there is a substantial trend difference between NOAA and GISS trends.
Given that, when NOAA’s Talking Points claim that there is a supposedly negligible difference between the average of their “good” stations and the NOAA average (which we know to run hot relative to GISS), then arguably this raises issues about the new USHCN procedures.
Y’see, while NOAA doesn’t actually bother saying how it did the calculations, here’s my guess as to what they did. The new USHCN data sets (as I’ll discuss in a future post) ONLY show adjusted data.  No more inconvenient data trails with unadjusted and TOBS versions.
When I looked at SHAP and FILNET adjustments a couple of years ago, one of my principal objections to these methods was that they adjusted “good” stations. After FILNET adjustment, stations looked a lot more similar than they did before. I’ll bet that the new USHCN adjustments have a similar effect and that the Talking Points memo compares adjusted versions of “good” stations to the overall average.
So what they are probably saying is this: after the new USHCN “adjustments” (about which little is known as the ink is barely dry on the journal article describing the new method and code for which is unavailable), there isn’t much difference between the average of good stations and the average of all stations.
If the NASA GISS adjustment procedure in the US is justified (and most Team advocates have supported the NASA GISS adjustment in the US), then the Talking Points memo merely demonstrates that there is something wrong with the new USHCN adjustments.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95059674',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Mayon – Shades of Pinatubo
2001 Image from NASA via the Space Shuttle: click for very hi res image
Here’s a recent AP report and bulletin from local authorities. Meanwhile, fools rush in as 2400 tourists a day flock to the area.

From the Philippine institute of Volcanologyand Seismology
30 December 2009 7:00 AM
For the past 24 hours, one ash explosion occurred at Mayon Volcano (13.2576 N, 123.6856 E). The explosion produced a dirty white ash column that rose to about 100 meters above the summit and drifted to the northwest. Lava continued to flow down along the Bonga-Buyuan, Miisi and Lidong gullies. The lava front has now reached about 5.9 kilometers from the summit along the Bonga-Buyuan gully.
Mayon Volcano’s seismic network recorded 16 volcanic earthquakes. A total of 150 rock fall events related to the detachment of lava fragments at the volcano’s upper slopes was also detected by the seismic network. Yesterday’s measurement of Sulfur Dioxide (SO2) emission rate yielded an average value of 4,397 tonnes per day (t/d). The volcano edifice remains inflated as indicated by the electronic tilt meter installed at the northeast sector of the volcano.
The status of Mayon Volcano is maintained at Alert Level 4. PHIVOLCS-DOST reiterates that the Extended Danger Zone (EDZ) from the summit of 8-km on the southern sector of the volcano and 7-km on the northern sector should be free from human activity.  Areas just outside of this EDZ should prepare for evacuation in the event hazardous eruptions intensify.  Active river channels and those perennially identified as lahar prone in the southern sector should also be avoided especially during bad weather conditions or when there is heavy and prolonged rainfall. In addition, Civil Aviation Authorities must advise pilots to avoid flying close to the volcano’s summit as ejected ash and volcanic fragments from sudden explosions may pose hazards to aircrafts. PHIVOLCS–DOST is closely monitoring Mayon Volcano’s activity and any new significant development will be immediately posted to all concerned.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e902da7c3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Like something out of a zombie movie, species that were once thought extinct seem to be rising from the dead. Between February 21 and March 4 2019, three notable rediscoveries were announced – the Fernandina Island Galápagos tortoise (Chelonoidis phantasticus), which was last seen in 1906; Wallace’s giant bee (Megachile pluto), which had supposedly disappeared in 1980; and the Formosan clouded leopard (Neofelis nebulosa brachyura), which disappeared after the last sighting in 1983 and was officially declared extinct in 2013.  These rediscoveries suggest we may know very little about some of the world’s rarest species, but they also raise the question of how species are declared extinct in the first place. The IUCN Red List collates a global register of threatened species and measures their relative risks of extinction. The Red List has a set of criteria to determine the threat status of a species, which are only listed as “Extinct” when… … there is no reasonable doubt that the last individual has died. According to the Red List, this requires… … exhaustive surveys in known and/or expected habitat, at appropriate times… throughout its historic range [which] have failed to record an individual. Surveys should be over a time frame appropriate to the taxon’s life cycle and life form. Given all the evidence – or rather, lack of evidence – that’s needed, it’s surprising that any species is ever declared extinct. The criteria show that to understand whether a species is extinct, we need to know what it was doing in the past.  Sightings at a certain time and in a certain place make up our knowledge of a species’ survival, but when a species becomes rare, sightings are increasingly infrequent so that people start to wonder whether the species still exists. People often use the time since the last sighting as a measure of likelihood when deciding if a species has died out, but the last sighting is rarely the last individual of the species or the actual date of extinction. Instead, the species may persist for years without being seen, but the length of time since the last sighting strongly influences assumptions as to whether a species has gone extinct or not. But what is a sighting? It can come in a variety of forms, from direct observation of a live individual in the flesh or in photographs, indirect evidence such as foot prints, scratches and faeces, and oral accounts from interviews with eyewitnesses.  But these different lines of evidence aren’t all worth the same – a bird in the hand is worth more than a roomful of recollections from people who saw it in the past. Trying to determine what are true sightings and what are false complicates the declaration of extinction. The idea of a species being “rediscovered” can confuse things further. Rediscovery implies that something was lost or forgotten but the term often gives the impression that a species has returned from the dead – hence the term “lazarus species”. This misinterpretation of lost or forgotten species means the default assumption is extinction for any species that hasn’t been seen for a number of years. So, what does this mean for the three recently “rediscovered” species? While a living specimen of the Fernandina Island Galápagos tortoise had not been seen since 1906, indirect observations of tortoise faeces, footprints and tortoise-like bite marks out of prickly pear cacti had been made as recently as 2013. The uncertainty around the quality of these later observations and the long time since the last living sighting probably contributed to it being declared “Critically Endangered (Possibly Extinct)” in 2015. In the natural world, a species is presumed extinct until proven living. Wallace’s giant bee may not have been recorded in the last 38 years but it was never actually declared extinct according to the IUCN Red List. In fact, for many years it languished under the criteria of Data Deficient and was only recently assessed as Vulnerable. So, while this is an exciting find for something that hadn’t been seen for so long, its rediscovery shows how little is known about many rare species in the wild, rather than how scarce they are. The Formosan clouded leopard, meanwhile, was actually listed as Extinct. The last sighting of the species was in 1983, based on interviews with 70 hunters, and extensive camera trapping during the 2000s failed to detect its presence. It was officially declared extinct in 2013.  While the giant tortoise and bee were proclaimed alive after living specimens were found, the clouded leopard’s rediscovery is more uncertain. Based on sightings on two separate occasions by two sets of wildlife rangers, the evidence is compelling. But whether the Formosan Clouded Leopard has really risen from the dead will require considerably more effort to prove."
"
I was working on another project related to the CRU emails and came across this email from Dr.Phil Jones. I was stunned, not only because he was dissing another dataset, but mostly because that dissing hit many of the points about problems with the NASA GISS products we’ve covered here on WUWT and at Climate Audit.
Here’s the email with my highlights added. Email addresses have been partially redacted.
click for larger image
The original email can be seen at this link:
http://www.eastangliaemails.com/emails.php?eid=1042&filename=1254850534.txt
Here’s the thing, we’ve seen the problems with CRU’s temperature series in the code already. If Dr. Jones is aware of those problems, and he thinks GISS is inferior, well then, wow, just how bad is GISS?
I thought this statement was quite telling:
Their non-use of a base period (GISS using something very odd and NCDC first differences) means they can use
very short series that we can’t (as they don’t have base periods) but with short series it is impossible to assess for homogeneity.
One thing about GISS that has bothered a lot of people – the base period they use for calculating temperature anomaly is for 1951-1980. See it listed here on the GISTEMP page. No other data sets use that period. Critics (including myself) have said that by using that period, it makes this graph’s trend look steeper than it would if the current 30 year period was used.
click for larger image
In the past couple of years we’ve seen two significant errors with NASA GISS that had to be corrected after they were discovered through the work done here at at WUWT and Climate audit. Public errors have not been found in CRU products during that time, because the data an code have been withheld.
To the credit of NASA GISS, they have been more transparent than CRU on data, stations used, and code.
Here are some of the relevant posts on WUWT where we address issues found with the NASA GISS temperature products:
How bad is the global temperature data?
And now, the most influential station in the GISS record is …
GISS for June – way out there
NASA GISS: adjustments galore, rewriting U.S. climate history
Absence makes the chart grow fonder
A comphrehensive comparison of GISS and UAH global Temperature data
Getting crabby – another missing NASA GISS station found, thanks to a TV show
More on NOAA’s FUBAR Honolulu “record highs” ASOS debacle, PLUS finding a long lost GISS station
Revisiting Detroit Lakes
Weather Station Data: raw or adjusted?
GISS Divergence with satellite temperatures since the start of 2003
Divergence Between GISS and UAH since 1980
GISS’s Gavin Schmidt credits WUWT community with spotting the error
GISS, NOAA, GHCN and the odd Russian temperature anomaly – “It’s all pipes!”
Corrected NASA GISTEMP data has been posted
Adjusting Pristine Data
A new view on GISS data, per Lucia
The Accidental Tourist (aka The GISS World Tour)
Rewriting History, Time and Time Again
Why Does NASA GISS Oppose Satellites?
Cedarville Sausage
How not to measure temperature, part 52: Another UFA sighted in Arizona
How not to measure temperature, part 51.
NASA’s Hansen Frees the Code !
Does Hansen’s Error “Matter”? – guest post by Steve McIntyre
1998 no longer the hottest year on record in USA


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91068242',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**On Wednesday, Chancellor Rishi Sunak begins setting out plans for what he hopes will be an economy beyond Covid-19.**
This Spending Review - detailing the money government departments will get for things like the NHS, education, roads, and police - only covers the financial year 2021-22. It will also set out money for the devolved administrations Scotland, Wales and Northern Ireland.
In normal times, reviews cover three or four years. But such is the economic uncertainty that this look-ahead has been limited to the next 12 months.
Even so, Mr Sunak will point to the direction of travel for spending (and possibly tax rises) for future years. Few reviews can have been so anticipated. Here's what to watch out for.
The economic shock has left the UK poorer. By the end of this year the economy is expected to be at least 10% smaller than pre-pandemic.
Alongside the Spending Review, Mr Sunak will disclose latest forecasts for the economy and public finances from the independent Office for Budget Responsibility (OBR).
Earlier this year, the OBR forecast a 13% contraction. While it is not expected to be that bad, the shrinkage will still likely be in the double-digits, and with public borrowing topping Â£350bn - something not seen in peacetime.
A difficulty for the chancellor is that big tranches of public service spending have already been made. Despite that, some areas will reportedly get more: NHS England, schools and defence. According to the Institute for Fiscal Studies (IFS), some two-thirds of public service spending has been pre-determined.
The key question is whether the remaining third is enough to go round. The answer is almost certainly not. The IFS thinks ""unprotected"" services such as the courts, prisons or local government are vulnerable to cuts. The overseas aid budget is also in the line of fire.
Saving, not spending, will dominate Wednesday's agenda. And one of the biggest savings could be a public sector pay freeze. It would be hugely controversial. Media leaks last week claimed Mr Sunak wants a freeze for everyone except frontline NHS staff.
That won't go down well with the police, teachers, civil servants or anyone who thinks they've done their bit to ensure the public sector keeps going in tough times. Even a return to a 1% cap is likely to be fiercely resisted.
Some commentators think the media reports were Treasury kite-flying. Even so, in the summer, Mr Sunak suggested that as private sector pay had taken a huge hit, in the ""interest of fairness"" the public sector's 5.4 million workers should share some pain.
Trouble is, relative to pay in the private sector, public sector pay has fallen to its lowest level in decades, according to the IFS.
Only during the pandemic has public sector pay performed more strongly than in the private sector. Union leaders have already warned of industrial action to ensure members' pay does not fall further behind.
Many promises have been thrown off-course because of the pandemic, and the government will be keen to get its north-south levelling up agenda back on track as soon as possible. Infrastructure spending is key to this.
The north has long complained that the Treasury methodology used to calculate the cost-benefit of spending money on big projects is inherently biased towards London and the rest of the south east. So, expect some changes to these calculations. And watch out for whether any spending promises are new money, or simply projects brought forward.
To underline his commitment to spend on big long-term projects, there is talk that Mr Sunak could publish details of a National Infrastructure Strategy and a Research and Development Strategy.
And in a symbolic move that levelling up is more than a question of infrastructure, the Financial Times has reported that the chancellor could also announce that parts of government could relocate from the capital - with the Treasury leading the way.
While Wednesday will be about spending and borrowing, at some point the chancellor will have to decide how it will be paid for. He will start to address this in next March's Budget, although most economic commentators feel the economy will still be too fragile for major tax rises.
It is possible that, with the success of a Covid vaccine, the economy could bounce back, limiting the need for big rises. However, Paul Johnson, director of the IFS, told the BBC that four or five years down the road he still expects the economy to be about 4%-5% smaller than before the pandemic.
Rein in spending and raise taxes too early, and recovery will be choked off. Leave it too late, and the public finances will spin out of control.
""It's a fine judgement,"" said Mr Johnson. Both the chancellor and Prime Minister Boris Johnson have, however, said they don't want a return to austerity.
There have been reports the Treasury could raise money from changes to Capital Gains Tax, pensions relief or self-employment taxes. But this is tinkering.
Mr Johnson believes Â£40bn of tax rises are necessary over the short-term, and that sort of money cannot be raised without touching the Big Three: income tax, VAT or national insurance. These bring in almost two-thirds of government revenue."
"

Last November, my book on the distortion of the science of global warming hit the streets. If I had not finished it by then, I would have devoted a chapter to CNN’s handling of the subject in its global warming documentary that aired on March 27 and is set to re‐​air three times on April 2.



The thesis of my book is simple: All scientific issues, including global warming, compete with each other for a finite amount of taxpayer largesse. So, logically, in order to gain advantage in that competition, scientists tend to pitch dire and drastic scenarios whenever they can.



The projections of gloom and doom by eminent scholars merit news coverage. The politicians respond to the incessant drumbeat by holding hearings and writing legislation for funding or regulation. What sane scientist would testify that global warming may be no big deal? After all, it’s currently so big a deal that the new budget proposes spending $4 billion researching it.



In its documentary, CNN had an opportunity to present a dramatically different view of global warming. They asked me to talk about global warming science. Here is what they were told.



To study climate change, we really have only two tools at our disposal: the historical record and computer models for the future. Neither is very satisfying when applied singly. But together they provide a very clear picture.



Climate models are simply strings of computer code that attempt to simulate the earth’s varied weather patterns and then estimate how they change if we slightly alter the planet’s natural greenhouse effect. That’s an atmospheric property, contributed largely by water vapor and secondarily by carbon dioxide, that makes the surface and the lower atmosphere warmer than it would be in their absence. Add more of either and the temperature should rise a bit.



Although there are dozens of these models, a comparison of them yields this average behavior: Once warming is established from human addition of carbon dioxide to the atmosphere, it tends to place at a constant rate. The different models largely just produce different rates.



So, to determine what the future holds, all we have to establish is 1) that warming in recent decades is largely from human addition of carbon dioxide, and 2) that it is truly a constant rate. If those two are satisfied, then we know the rate of future warming.



It’s easy to establish that the warming that began around 1970 is indeed largely from human influence. Greenhouse‐​effect science predicts that cold, dry air should warm preferentially to warm, moist air. This has been the case, with the greatest nonpolar warming observed over cold, dry Siberia in the winter.



All that’s left then is to demonstrate that the global temperature change is indeed constant. In fact, the warming trend has been so steady that there has been virtually no departure from a straight‐​line trend.



So nature has now discriminated between all those models, and the warming trend works out to a mere 1.2°F for the next half‐​century. This is right at the low end of the range of projections made by the United Nations in 2001. But, unless all the money we threw into climate models has been wasted to the point that we can’t even tell whether it will be a constant or an increasing rate, we now know the answer to a very small range of error.



How small is that range? The variation in the constant warming trend has been so tiny over the last 35 years that one can say with confidence that the range should be between 1.0 and 1.4°F from 2001 to 2050. 



CNN had the chance to prove my book wrong and to show something new and different at the same time. But the cable network failed to do so. The program transcript is 6,497 words long. Two scientists, myself and MIT’s Richard Lindzen, different than the 19‐​odd other people promoting gloom and doom, were awarded 98 and 68 words, respectively. My segment was followed by the host, Miles O’Brien, saying, “Michaels’ position is in the minority.”



Hardly. It is the consensus of dozens of climate models, produced by an army of researchers, simply adjusted to the very constant rate of warming that has been observed.



Incidentally, the complete title of the book is _Meltdown: The Predictable Distortion of Global Warming by Scientists, Politicians, and the Media_.
"
"**Taxi drivers say they are struggling to pay bills and put food on the table because of fewer passengers during the coronavirus pandemic.**
Unite Wales, which represents cabbies, claims ministers have ""forgotten"" the industry, while concentrating support on buses and trains.
One private hire driver in Cardiff said he had been lucky to earn Â£25 a day during the restrictions.
The Welsh Government said it was continuously reviewing support.
Taxi drivers are preparing to hold a demonstration outside Cardiff's City Hall on Tuesday.
During lockdown, pubs, bars and restaurants were closed and events were cancelled, and while many venues have now reopened, early closing times and social distancing measures are in place with people being urged to work from home.
Unite branch secretary Yusef Jama said the restrictions meant many cabbies were spending hours waiting for a fare and many were earning less than Â£25 a day.
Mr Jama said anxiety and depression among drivers was now at a crisis point, and many were struggling financially and worried for their families.
""I've had really, really concerning conversations with drivers, where they feel left on their own, nobody to turn to and to speak about the problems they go through,"" he said.
""Some of these conversations, it's got to the point where I don't know if these drivers are going to be alive tomorrow.
""They feel like they're going to lose their house, they're having problems with their family because they're not providing for their families.""
At the rank at Cardiff Central Station, Abdel Kadir, a driver in the city for 18 years, said he had waited three hours before his first fare.
Before the pandemic, he said, the rank would be busy taking people from the station, but now there are fewer passengers to transport across the city.
""All the taxis here depend on the London train. Before there would be 10 to 15 cars moving, now maybe one or two or three if you are lucky because the trains come empty,"" he said.
""With this pandemic people aren't moving and all the staff are working from home or by video.""
Father-of-three Rofikul Islam said his family was terrified of him going to work and, potentially, being exposed to the virus, but he worried about falling into debt.
""It's very bad at the moment,"" he said. ""We have to wait three to four hours for a fare and after the three hours, four hours, we get a Â£6 fare. All day we work for Â£25, Â£27.""
He added: ""Nobody's helping us, so we have to try ourselves.
""We're taking a risk, we can't do nothing, we have to come out to do our best.""
The majority of taxi drivers are self-employed and are entitled to the UK government's Self Employed Income Support Scheme grant extension.
The grant, which is taxable, covers 80% of profits for November, December and January, up to a limit of Â£7,500.
But drivers claim that the industry was struggling before the pandemic, with many making little profit once vehicle costs and licences were taken from the fares.
Eva Dukes' income disappeared overnight when schools and offices closed in March.
The private hire driver, who lives in Cardiff with her three children, said her contracts stopped and, with her husband also being a taxi driver, the family were struggling to make ends meet.
While Eva and husband Philip received grants under the scheme, it worked out at about Â£1,000 a month, and she said they had gone through all their savings.
""It's dire to say the least. Just putting food on the table is a weekly struggle,"" she said.
""We need help now. They've given money to the buses and the trains and all the other transport, and now we need help for taxi transport, private hires, contractors, they need help.""
In Scotland, the government announced a Â£30m means-tested fund for drivers, and in Northern Ireland a Â£14m scheme has been set up.
Union representatives say they want the Welsh Government to provide the same kind of funding.
Alan McCarthy, from the Unite union, said with people not going out and being told to stay home, many drivers had been back at work but unable to make a living for months.
The BBC requested an interview with Transport Minister Ken Skates but it was declined.
In a statement the Welsh Government said support was available for drivers, but it recognised the extent of the challenge coronavirus had caused.
""We are in regular discussion with the sector and will continue to review what support can be made available,"" said a spokesman."
"**The process of applying for a Welsh Government coronavirus grant has been described as ""unfair"" and ""shambolic"" by businesses who tried to access it.**
The Â£100m available from the third phase of the Economic Resilience Fund (ERF) opened on 28 October but closed the next day.
While many firms were still chasing quotes and bank statements at the time, others could apply without them.
The Welsh Government said it had worked hard to support businesses.
But the Welsh Conservative economy spokesman, Russell George, said businesses had been ""let down by lack of communication; let down by a lack of clarity; let down by the Welsh Labour Government"".
While Plaid Cymru's spokeswoman Helen Mary Jones said targeted support was needed for businesses unable to resume trading until the coronavirus pandemic was over.
Applicants were allowed 20 minutes to complete the questions on each page and asked to provide supporting documents for their application, including three months of bank statements, proof that they could match-fund 10% of the grant and quotes for work to be carried out.
The guidance notes for the application stated the documents were ""required as part of the application process"" and that ""if application forms are sent without the required additional documents, your application will be declined"".
Despite this, some businesses were allowed to provide these documents after submitting their application form with an extended cut-off date of 2 November, according to evidence seen by BBC Wales.
On 30 October, less than 48 hours after the fund opened, the Welsh Government stated that ""more than 5,500 businesses"" had applied for the grant and it was now ""fully subscribed"".
Jamie Williams, who runs North Wales Active in Betws y Coed, missed the grant ""by a few hours"" and said the process was ""shambolic"".
After taking a couple of days to gather supporting evidence, Mr Williams thought they would ""fly through"" the process.
""But as we tried to apply for it the next day, it had obviously gone, which is just ridiculous,"" he said.
The Â£10,000 grant was ""hugely important"", Mr William added. ""Nobody's booking our activities because either they are in lockdown or we are in lockdown.""
Natalie Isaac and her family run five restaurants including two in Cardiff - Assador 44 and Bar 44.
Applications were made for both. Bar 44 was turned down for not attaching bank statements, which Ms Issac said they did.
But for the other, the business was given an opportunity to submit information to a case officer.
""So that's two establishments with two completely different approaches,"" said Ms Issac, who is contesting the claim regarding Bar 44.
""It should be a fair and reasonable process. We don't believe that everyone's being treated equally.""
Ms Isaac said their business was currently ""Â£3.7m down on sales"", with staff numbers across their five restaurants down from 120 to 52 since March.
She said some of the staff that were let go could have been brought back had she got the ERF.
Kerry DeCaux, who runs Rolfe's florist in Cardiff, was also rejected for not uploading enough supporting documents.
""The application was timed, I wasn't expecting that, I read the guide notes as best as I could as I completed each step, then sent it.""
She said she felt ""quite pressurised"", adding: ""I did it the best I could.""
Ms DeCaux said when she received the rejection email she asked for ""an opportunity to fix the elements I failed on, but was told there was no appeal process"".
""I honestly felt broken. I sat on the floor of my shop and sobbed my heart out,"" she added.
North Wales Tourism surveyed 111 members on their experiences of applying for the Â£100m fund - 44 had managed to make an application and were awaiting an outcome.
Of the 67 who had not made an application, 29 referred to a ""very complex process"" and ""time constraints"".
The Chief Executive of North Wales Tourism, Jim Jones, said the grant ""did not provide the help that is so desperately needed.""
He added firms that did not get the support ""can't just be left in complete limbo"" without help into 2021.
A Welsh Government spokesman said: ""We've worked hard to support businesses through this incredibly difficult year and we've already provided businesses with Â£100m from the third phase of the Economic Resilience Fund since it was launched last month.
""We also ring-fenced funding in third phase for tourism and hospitality businesses.
""This is in addition to the hundreds of millions of pounds provided to firms through the first two phases of the ERF and our Covid-19 business rates based grants.""
He said to date, 80% of recipient firms of the ERF, including phases one and two, had been micro businesses accessing support through the digital application process.
The Welsh Government said it had launched its eligibility checker three weeks before ERF3 ""to give businesses plenty of time to prepare their applications"" and asked them to provide necessary supporting documentation ""to ensure necessary due diligence"".
""We recognise that some businesses missed out on this round of funding and we are exploring options for how we might allocate any future support,"" the spokesman added.
He said the finance minister had set aside funding for a fourth phase of the ERF and was currently developing proposals."
"
Not only does the Met Office/Hadley Climate Center have trouble with pesky “moles” this week, they are now finding a staunch ally, the BBC, is questioning their forecasting ability. One wonders if they will improve using “deep black”, the 1.2 megawatt supercomputer they just purchased.

Met Office cools summer forecast
 
 By Roger Harrabin 
 BBC environment analyst 
excerpts:
You will need a brolly on holiday in the UK in August – the Met Office is issuing a revised forecast for more unsettled weather well into the month.
It is a far cry from the “barbecue summer” it predicted back in April.
The news will raise questions about the Met Office’s ability to make reliable seasonal forecasts.
…
It did indeed stress at the time of the summer forecast in April that the odds of a scorching summer were 65%. It explains that it coined the phrase “barbecue summer” to help journalists’ headlines.
But this has come back to bite the organisation because many people do not feel like they have been enjoying a “good” summer, especially compared with previous searing years.
Jet stream
Some now ask if the Met Office risks its reputation by attempting to popularise its work this way.
…
The real problem for the Met Office is that this is the third summer in a row where its forecast has failed. In 2007, the Met Office chirped: “The summer is yet again likely to be warmer than normal. There are no indications of a particularly wet summer.”
We got downpours and floods in the wettest summer for England and Wales since 1912. Temperatures were below average.
In April 2008, the Met Office forecast: “Summer temperatures are likely to be warmer than average and rainfall near or above average.”
That did not prepare people for one of the wettest summers on record with high winds and low sunshine.
In both instances, the Met Office failed to predict the movements of the jet stream – the high-level wind that races round the world 10km above the surface.
…
read the entire article at the BBC here
h/t to WUWT reader Kristinn


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e949b1838',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

he emergence of John Kerry as frontrunner for the Democratic nomination suggests that free trade might be off the table in 2004, at least as a national issue. It’s certain to come up, however, in a number of congressional, senatorial, and gubernatorial campaigns. And, of course, as long as Lou Dobbs is still kicking at CNN, we’ll continue to hear nightly nativist tirades against the loss of manufacturing jobs, the off‐​shoring of tech jobs, immigration, and general alarmism about the “outsourcing of America.”



The truth, of course, is a bit more complicated than the simplistic picture painted by protectionists. The United States is still far and away the world’s leading exporter of services. Direct corporate investment in India — generally the target of protectionist rants on tech jobs — actually declined from 2001 to 2003. As for manufacturing jobs, sure, it’s likely that free trade agreements played a part in the loss of jobs in the last five years, but so too did a host of other factors, including exchange rates, changing consumer preferences, upgrades in technology and equipment, the recession, and new federal regulations. Michigan’s Mackinac Center for Public Policy, to cite just one example, estimated in 2002 that a federal appeals court ruling favoring procedural matters over hard science in federal environmental regulatory policy could cost the state as much as $2.6 billion, or about 10,000 jobs.



Which brings us to state policy. Time and again, when we look at the states attracting and retaining jobs, and we compare them to the states losing jobs, we find that the states doing well are those with tax and regulatory schemes most friendly to business. It’s only when the cost of staying local becomes too burdensome that companies pick up and relocate elsewhere. Perhaps that’s not surprising. But just how strongly the data shakes out might be.



For example, according to the Economic Policy Institute, the five states losing the most jobs between 1993 and 2000 were, in order, California, New York, Michigan, Texas and Ohio. According to figures from the Bureau of Labor Statistics, New Jersey, Pennsylvania, Illinois and Massachusetts also rank near the bottom, particularly when you take jobs as a percentage of population. The left‐​leaning EPI blames these losses chiefly on NAFTA, and perhaps that’s partially the case. But aggressive tax and regulatory climates play a pretty big role, too.



Each year, _CFO_ magazine asks financial executives to assess the business‐​friendliness of tax policy in their respective states, which the magazine then compiles and ranks. Ranking in the bottom 10? California, New York, Michigan, Texas, Ohio, New Jersey, Pennsylvania, Illinois and Massachusetts — the very states that seem to be bleeding jobs. The most recent unemployment figures from the Labor Department put California, Texas, Ohio, Illinois, and Michigan all in the bottom 10 there, too, all with unemployment rates at 7.0 percent or higher.



The Small Business Survival Committee also puts out a report ranking the states on business‐​friendly public policy. In the SBSC report, Ohio ranks 39th, New York 45th and California 46th. Oregon, also with one of the country’s highest unemployment rates, ranks 41st.



A 2003 ranking by the Tax Foundation focusing mainly on tax policy and business tells the same story. It puts California 49th, Ohio 47th, and New York 44th.



Only Texas and Michigan score relatively well on the Tax Foundation and SBSC reports, suggesting that at least in these two states, free trade may have played a more significant role in job loss than poor public policy (and when you think about what Michigan manufactures, and where Texas is located, that makes some sense).



The Cato Institute’s Alan Reynolds wrote recently about San Jose, California, a city that lost about 120,000 jobs over two years. Reynolds points out that despite the debacle in San Jose, the communities of San Diego, Riverside, and Orange County actually added almost as many jobs over the same span of time.



San Jose was one of the first jurisdictions in the area to implement a so‐​called “living wage” ordinance, mandating that businesses contracting with the city pay their lowest‐​paid workers around $11 per hour, more than double the federal minimum wage. Of course, a living wage law in and of itself won’t wipe out 120,000 tech jobs, but it’s certainly indicative of the sort of “progressive” anti‐​corporate sentiment that might cause local businesses to pick up and spill out into friendlier communities.



Protectionists often bring up Ohio as the prototype of a hard‐​working, breadbasket state whose manufacturing sector has fallen victim to free trade. But Ohio is also a case study in how a state government hostile to business pushes jobs to more hospitable locales. You’ve read the numbers above. But additionally, in the last few years, Ohio legislators have begun to feel the hangover caused by big spending habits fomented back in the freewheeling 1990s. As of 2003, the state faced a $720 million deficit. Ohio governor Bob Taft has promised to shrink the deficit not with cuts in state spending, but with new taxes, tax hikes, and new fees, as well as rollbacks of promised tax breaks. Taft’s tax‐​happy policy earned the Republican condemnation from the Club for Growth’s Steve Moore, who called Taft one of the “worst governors in America.”



The Buckeye Institute, an Ohio free market think tank, reports that Ohio’s aggressive pro‐​labor policies cost the state jobs even during the relatively strong economic period of 1982–1998. Zeroing in on the effect of mandatory union memberships on state economies, the Institute emphasizes that during that 16‐​year period, states that mandated union membership in the manufacturing sector lost a net 996,000 jobs, while “right to work states” gained 493,000.



Let’s look at the flip side. How well are states with business‐​friendly public policy doing at attracting and retaining jobs? The anecdotal evidence suggests they’re doing pretty well.



According to the Bureau of Labor statistics, the only state that actually gained net manufacturing jobs from 2000 to 2003 was Nevada. It ranks 2nd on the SBSC’s business‐​friendly list. It ranks 3rd on the Tax Foundation list. It ranks in the top four of _CFO’_ s list. Alaska lost only 900 manufacturing jobs over those same four years, which is likely due to its population. Still, Alaska too ranked in the top four on the _CFO_ list. Virginia made a big push in the late 1990s to attract tech firms to its D.C. suburbs and the Dulles corridor. Despite the tech bust, Virginia still has one of the lowest state unemployment rates in the country and, perhaps not coincidentally, ranks 14th on the SBSC list (and would likely rank higher were it not for Gov. Mark Warner’s recent promise to raise taxes). South Dakota, which ranks number one on the SBSC list, also has one of the four lowest unemployment rates in the country (as of December 2003).



On its face, this cursory look at the data makes a lot of sense. For all the talk of off‐​shoring, the cost of packing up a domestic plant and moving it overseas is pretty significant. Even outsourcing tech support and programming doesn’t always make economic sense. American workers are still far more productive than, for example, Indian workers, even when you factor in the lower wages. It’s only when the onus of complying with federal, state, and local tax laws and regulations becomes overly burdensome that it makes economic sense for a corporation to shop jurisdictions for a better deal.



So the next time a local politician (or news anchor) blasts NAFTA or greedy corporatism for the loss of local jobs, it might not hurt to take a look at just how friendly that politician’s state or city taxes, regulatory and labor policies are toward business. Check where his state ranks on the Tax Foundation, SBSC or _CFO_ lists. If he’s a governor, see how he did on the Cato Institute’s Governor’s Report Card. If relocation really is the cause of the job hemorrhage he’s complaining about (and often it isn’t), it’s likely that same politician’s policies are a big reason those jobs left. 
"
"

Real Climate’s Misinformation
From Climate Science — Roger Pielke Sr. @ 7:00 am
Real Climate posted a weblog on June 21 2009 titled “A warning from Copenhagen”.  They report on a Synthesis Report of the Copenhagen Congress which was handed over to the Danish Prime Minister Rasmussen in Brussels the previous week.
Real Climate writes
“So what does it say? Our regular readers will hardly be surprised by the key findings from physical climate science, most of which we have already discussed here. Some aspects of climate change are progressing faster than was expected a few years ago – such as rising sea levels, the increase of heat stored in the ocean and the shrinking Arctic sea ice. “The updated estimates of the future global mean sea level rise are about double the IPCC projections from 2007″, says the new report. And it points out that any warming caused will be virtually irreversible for at least a thousand years – because of the long residence time of CO2 in the atmosphere.”
First, what is “physical climate science”? How is this different from “climate science”. In the past, this terminology has been used when authors ignore the biological components of the climate system.
More importantly, however, the author of the weblog makes the  statement that the following climate metrics “are progressing faster than was expected a few years ago” ;
1. “rising sea levels”
NOT TRUE;  e.g. see the University of Colorado at Boulder Sea Level Change analysis.
Sea level has actually flattened since 2006.
2.  “the increase of heat stored in the ocean”
NOT TRUE; see
Update On A Comparison Of Upper Ocean Heat Content Changes With The GISS Model Predictions.
Their has been no statistically significant warming of the upper ocean since 2003.
3. “shrinking Arctic sea ice”
NOT TRUE; see the Northern Hemisphere Sea Ice Anomaly from the University of Illinois Cyrosphere Today website. Since 2008, the anomalies have actually decreased.
These climate metrics might again start following the predictions of the models. However, until and unless they do, the authors of the Copenhagen Congress Synthesis Report and the author of the Real Climate weblog are erroneously communicating the reality of the how the climate system is actually behaving. 
Media and policymakers who blindly accept these claims are either naive or are deliberately slanting the science to promote their particular advocacy position. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94bd62bf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Gobi Desert in East Asia conjures images of a remote landscape, with nomads riding across the steppe. In fact, today it is home to herders and farmers, the world’s fastest-growing economy, vast copper and gold mines and is China’s main domestic energy source. The imagined expanses and agro-pastoral livelihoods exist alongside mountains of coal, modern cities, desert agriculture and environmental challenges to its viability and future well-being. As Chinese president Xi Jingping arrives in Mongolia to discuss a series of trade and energy deals that would give Mongolia better access to global markets, it is worth looking at the shared desert that lies between Beijing and Ulaan Baatar. As the two nations work together, reconciling differences in the Gobi will be a major challenge. At 2.3m km2 the Gobi is the world’s third largest desert, covering most of Mongolia and much of northern China. Yet 25m people live in an area that stretches from the edge of Beijing to the Kazakh and Russian borders in the west and north. Across the Gobi, conservation reflects a shifting balance between human development and natural fragility. Home to the world’s highest sand dunes (more than 300 metres), most of the Gobi is a dry gravel plain and sparse rangeland. The landscape challenges residents with extreme cold (to -40C), hot summers (to 40C), periodic droughts and minimal surface water. Humans drive the need for extensive groundwater use, particularly in China, where the government encourages farming even though annual precipitation is often less than 200mm. This leads to competition for limited environmental resources between agriculture, cities such as Hohhot, Baotou and Urumqi (each more than 2m people), mining and traditional herding (Mongolia) and settled livestock-raising (China). Today the vast majority of water in Inner Mongolia – the autonomous Chinese province that borders Mongolia itself – goes for coal extraction and processing to meet China’s energy demands. Conservation of the tenuous desert environment and rural livelihoods encounter several socio-economic forces and physical challenges. Though a shared landscape, the issues differ greatly between China and Mongolia as policy, culture, history and democracy/autocracy separate the neighbours. In Mongolia water is essential for animals and household needs yet supply is obscured in shallow and deep aquifers that are difficult for locals to tap. Groundwater, essential for the desert’s new copper, gold and coal mining, requires money and technology to exploit and thus is pursued by regional and international mining companies. This results in conflict between local residents and businesses for limited water and raises issues of land use and livelihood viability among mobile herders, still the dominant lifestyle in rural Mongolia. In China strong state control and intervention has resulted in a manipulated water system where farmers need swipe-cards to get allocated water, use of natural pastures for animals is restricted and ecological resettlement sees once-mobile herders settled in villages by government decree. Removal of livestock opens land for farming and most importantly, for profitable mining that often is owned, or directly benefits, local governments. Mining in the region has led to economic growth, jobs, pollution, land degradation, dust generation and settlements that lack basic infrastructure. The notion of conservation and the role of nature in everyday life is integral to the Mongolian conception of the world whereas the Chinese model is focused on economic and infrastructure development irrespective of environmental impact. This splits the Gobi at the border; on one side roads, fencing, settlement, degradation and policy has ended free movement in China and sees the environment as something to be managed and exploited to ultimately benefit the several layers of government.  This leaves one to ask “what conservation” as water, land and vegetation are used for financial benefit, not as an inherent social good to protect. In Mongolia national parks comprise 13% of the country and species such as the Gobi bear, gazelle, marmot and Saker Falcon benefit from social conceptions of nature’s importance and varying degrees of protection. Though a vast area, the Gobi’s harsh environment and intricate ecosystem make wide swathes of open land and limited human use of nature key to conserving flora and fauna. This means creating non-financial value for wild steppe and desert regions. Without care the environment can become less productive and potentially experience desertification. Preserving nature takes insightful policy, sustainable land use, recognition of environmental benefits and the support of rural and mining communities.  In the Gobi this takes place against Mongolia’s weak institutional framework and China’s all-powerful bureaucracy. While the Communist Party remains in power conservation will be sacrificed for perceptions of growth and social stability. The picture in Mongolia is more optimistic as history and cultural preferences favour a strong role for nature in Mongolia’s conception of the world."
"**YouTube has suspended the One America News Network (OANN) for sharing misinformation about a Covid-19 ""cure"".**
The channel, which is a favourite of President Trump, is also suspended from making any money on YouTube.
The suspension will last a week, during which time no new videos can be put up. To make money in future, the channel must rectify the issues.
YouTube is attempting to clean up its platform and has also removed QAnon and pizzagate-affiliated accounts.
Pizzagate is a conspiracy theory about a paedophilia ring involving members of the US Democratic Party operating out of a Washington pizza restaurant, while QAnon believers think President Donald Trump is waging a secret war against elite Satan-worshipping paedophiles in government, business and the media.
In a statement about OANN, YouTube said. ""Since early in this pandemic, we've worked to prevent the spread of harmful misinformation associated with Covid-19.
""After careful review, we removed a video from OANN and issued a strike on the channel for violating our Covid-19 misinformation policy, which prohibits content that claims there's a guaranteed cure.""
YouTube operates a three-strikes policy - any second violation in a 90-day period would result in a two-week suspension. A third strike would lead to a permanent ban.
OANN must also reapply to the YouTube Partner Program if it wants to make money from its videos.
YouTube, along with rival social media firms Facebook and Twitter, is under intense scrutiny for the amount of misinformation and hate speech that finds its way onto its platform.
It said that it had removed 200,000 videos related to dangerous or misleading Covid-19 information since February.
This week, a group of Democratic senators wrote to YouTube chief executive Susan Wojcicki, urging it to remove all election outcome misinformation.
YouTube has removed adverts and added a warning label to a video falsely declaring that President Trump had won the election.
President Trump has tweeted that OANN and other news sources which question Joe Biden's election victory are a ""great alternative"" to Fox News."
"

For over a decade, many industry groups have maintained that putting carbon dioxide in the air would produce a general “greening” of the planet. In fact, that’s the thesis of a famous 1992 video, “The Greening of Planet Earth,” which riled the environmental community more than just about anything else that business has done in its own defense on this issue.



“Greening” was put out by energy‐​industry activists (you can get your own copy by contacting http://​www​.greeningearth​so​ci​ety​.org), who discovered that several big‐​name scientists were willing to appear and argue that carbon dioxide will enhance global plant growth by directly stimulating plants and by warming the coldest air of winter. These scientists were confident because the growth stimulation had been observed in literally thousands of controlled lab experiments and reported in the scientific literature. At the same time, the climate data were pouring in showing that “global” warming was much more “winter” warming, and in the coldest air, which was sure to lengthen the growing season for plants.



On September 16, industry will be vindicated with a vengeance in the Journal of Geophysical Research, when Liming Zhou and five co‐​authors publish their paper demonstrating a profound greening of the planet poleward of latitude 40º, or north of a circle passing through New York City.



Using a satellite designed to measure changes in vegetation, they found that the time of active growth has advanced as much as 18 days per year in Eurasia. (The freeze‐​free period averages around 170 days at latitude 40º.) In North America the results are more spotty, with a few areas increasing by up to 12 days. On our continent, the results are confounded by the well‐​known (to scientists, not to newspaper‐​readers) cooling trend in northeastern North America that has been going on for about 70 years.



Zhou et al. attribute this “greening” (their word) to “global warming,” because it matches areas that show maximum warming since 1980 in land‐​based climate records. In fact, the winter warming in the dead of Siberia has been quite striking for decades, and, everything else being equal, this will lengthen the growing season. (Last year’s record cold merely proves that climate is a very variable thing.) Summer warming in Siberia has been less profound, a fact not generally disseminated because it doesn’t support the popular global‐​warming‐​as‐​disaster paradigm.



What isn’t noted in the paper or the brief flurry of news reports — there would probably be a bit more coverage if Zhou had written “global warming is killing the North Woods” — is that the beginning of their satellite record, in 1981, corresponds in the Northern Hemisphere to the end of the coolest era of the last 70 years. The fact is that all analyses show a cooling of our hemisphere from roughly the mid‐​1940s to the late 1970s. During this climate spasm “global cooling” became popular. So this paper starts at an unusual point, but, unfortunately, that is when the satellite went up. If the satellite went up in, say, 1950, the changes it would have found in the growing season would have been much smaller.



The contra is also true: According to Zhou’s findings, if we had continued down that cooling spiral, the world north of 40º would now be much more barren than it was 20 years ago. It’s quite reasonable to ask if human‐​induced global warming has saved the world from a food crisis.



What really ticked off the greens about “The Greening of Planet Earth” were the many sound bites from prominent agricultural scientists about how the future atmosphere would be much more conducive for food production. But look at what NASA, which funded this study, now says about Zhou’s work: “The pattern of high growth is especially noteworthy in boreal [northern] Eurasia…This includes the grasslands and croplands of south central Russia.…[emphases added].”



In other words, dreaded global warming will produce more food for Russia.



Russians rightfully fear the cold. In 1972, near the bottom of the mid‐​century cooling (and around the height of global cooling fear) they were so short of food that they purchased just about every kernel of American grain. This sent grocery store prices here to alarming levels. By the end of the crop year 1972, world grain reserves stood at a stunningly low 19 days. Since we warmed up, those fears have become a thing of the past. Now food shortages are largely local and political, and commodity prices have been in the tank for years, reflecting vast supply compared to demand.



So is this what global warming has wrought? It appears to have created a more comfortable planet with more food. The video was right. The greens were wrong. The world is greener.
"
"The world’s accountants must put the climate crisis at the forefront of their work to spur global companies to adopt green policies and help prepare them for the risks, according to the Institute of Chartered Accountants in England and Wales (ICAEW) and other industry groups. They have called on a global alliance of accountants, representing more than 2.5 million professionals worldwide, to put their skills to use by helping companies prepare for a climate emergency. The groups want all members of the Accounting for Sustainability Project (A4S) network to integrate climate risk into their company audits to drive companies to set more sustainable business strategies. Michael Izza, the chief executive of the ICAEW, said it is crucial that chartered accountants use their “unique position” at the core of almost every business, government and non-governmental organisation to make the case for sustainability. “Chartered accountants bring practical skills like measurement and management to the table, and can work with business to build green policies into their working practices. We need to make this a decade of transition for business; failure to make this move will make the inevitable adjustments required much more difficult,” he said. Accountants should demonstrate the risks to business posed by the climate crisis, such as the impact of flooding or the effect of drought on the price of crops needed in the supply chain, the industry groups said. The call for climate action comes after the accounting watchdog, the Financial Reporting Council, launched a review into whether companies and their auditors are adequately reflecting the financial risks of the climate crisis in their annual financial accounts. Sir Jon Thompson, the FRC’s chief executive, said auditors have a responsibility to challenge management to face up to the financial risks of the climate crisis and provide clear information to investors. Bruce Cartwright, the chief executive of Icas, the global professional body, said there are “dramatic implications for failing to tackle climate change – environmental, social and economic”. He said: “It is our future that is at risk and the urgency at which we are required to act must remain front of mind. Icas is working with fellow accountancy bodies to act now to limit the negative effects of climate change. Our individual actions, collectively, have the ability to make a difference.” "
"

Reposted from Climate Audit:
A savage article in the Times today by Ben Webster about the UEA submission to the  UK Parliamentary Inquiry – the letter in which they tried to “trick” the  Committee about the contents of the letter from the Information  Commissioner. (A “trick”, according to Gavin Schmidt and the Penn State  Inquiry, is a “good way” to solve a problem.)
The article – worth reading in full – re-caps correspondence  discussed in yesterday’s  post on the topic.
The UEA has now posted  up all its correspondence.
Webster provides an interesting new statement from Dr Evan Harris,  Liberal Democrat member of the Science and Technology Committee:
“It seems unwise, at best, for the University of East  Anglia to attempt to portray a letter from the Information  Commissioner’s Office in a good light, in evidence to the select  committee, because it is inevitable that the Committee will find that  letter, and notice any discrepancy.
“It would be a wiser course for the university not to provide any  suspicion that they might be seeking to enable the wrong impression to  be gained.”
Yup.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e000ae2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Here’s a disaster script that might interest the burgeoning Virginia film industry: Three hundred thousand acres, including much of Shenandoah National Park, go up in smoke; downtown D.C. hits 110 degrees day after day; mega‐​drought costs Virginia $1 billion, sending state finances into a death spiral; the president declares a national emergency; and hordes of thirst‐​crazed snakes attack a turkey farm.



This isn’t the storyboard for the next global warming flick. It’s a scene from a much more fearful genre: reality. Welcome to the summer of 1930.



Judging droughts is a little like judging an ugly contest, a somewhat subjective affair. With that caveat, I’ll opine that the current one is very reminiscent of the prolonged moisture shortage that plagued our region and much of the Northeast from 1964 to 1967. And even though Virginia Gov. Mark R. Warner declared a state of emergency on Friday, this drought is nothing compared with what happened in the summer of 1930. That was the peak of a two‐​year drought that remains the benchmark for this area in the era of modern rainfall measurement, which began in 1895. On a graph charting the Palmer Drought Severity Index, a measure of drought for central Maryland, the 1930 outlier is obvious.



Our current misery is one of the low points in a prolonged rainfall deficit that began during the spring of 1998, right after the end of that generator of misplaced fears, El Niño. Within a year, disaster struck Virginia’s Shenandoah Valley, as crop yields took their biggest hit in postwar history. Ironically in view of all this suffering, Maryland got into a misplaced water war with Virginia over the Potomac (the river’s supply was and will remain fine, because there’s been adequate flow into the huge upstream reservoirs that water Washington and its vicinity). Since 1999, things have broiled right along in fits and starts, with the latest dry spell giving everyone a lot more of the former than the latter. The situation is particularly terrible in central Maryland, the Virginia Piedmont (with the exception of Northern Virginia, which has been lucky with thunderstorms compared with its surroundings), and the central Shenandoah Valley, where an unknown number of small farmers still struggling in the wake of the 1999 disaster now face bankruptcy.



Most of the time, one person’s drought can be a mere inconvenience for someone else. A short‐​term deficit — merely six summer weeks with little rain — can devastate crops. Droughts lasting a year, which occur in the Mid‐​Atlantic when we get about 60 percent of our normal 40 inches of rain, begin to draw down shallow‐​water wells and dry up livestock ponds. But the most severe droughts, like 1930’s, touch everyone in ways that are not immediately obvious.



For example, dry ground produces spectacular heat. Normally, our high temperatures (away from the urban cores) cap at around 95 to 98 degrees on a sunny day. That’s because it’s usually wet around here (the official climate designation for the Mid‐​Atlantic is “humid subtropical”), and a lot of the sun’s energy goes toward the evaporation of water, contributed by the jungle of vegetation that covers our region.



In really dry years, like 2002, 1999 or 1930, the soil becomes so arid that trees stop transpiring and the sun finds no water to evaporate. This began in earnest in early August this year, as trees started to shed leaves normally retained until October. When things get this dry, all the sun’s energy goes toward heating the surface and rural temperatures bump from the high 90s into the low 100s, usually hot enough to beat daily records lasting a century or so.



In fact, most Mid‐​Atlantic weather stations average only about one 100‐​degree day per year. Not in 1930. Rural Woodstock, hard in the middle of the Shenandoah Valley, had 21 of them. The high on July 20 was 109, which remains Virginia’s official record. Rainfall was virtually nonexistent that summer. Except for a few places that received a lucky thunderstorm, much of the Mid‐​Atlantic saw less than an inch between June 20 and the end of August, or about 10 percent of normal rainfall. For comparative purposes, the driest regions currently are running around 40 percent of normal for the past three months (not counting last week’s beneficial rain).



The 1930 horror show reached its peak between July 19 and Aug. 10. Here’s the run of that summer’s high temperatures recorded at Charlottesville’s Leander McCormick Observatory, a rural station on a hilltop covered with thick vegetation. If anything, these figures should have been lower than those in the surrounding areas. Starting on July 19, they were: 103, 107, 106, 105, 97, 92, 102, 104, 98, 103, 106, 90, 95, 91, 97, 101, 106, 102, 90, 100, 99, 105 and 98.



 _The Washington Post_ maintained an accurate thermometer at a downtown newspaper kiosk that didn’t have the luxury of being a well‐​vented “official” site. Shade temperatures reached 107 on July 19, 1930, 109 on July 27 and Aug. 3, and 110 on July 20 and Aug. 9. Thousands of Washingtonians slept in the District’s parks. Former Minnesota congressman Charles Davis died of heatstroke. On Aug. 7, The Post headlined a story, “30 Thirsty Snakes Die for Attacking Turkeys.” (Farmers reported that the reptiles were after blood.)



What would happen if 1930 repeated itself in 2003? Given a drought of similar magnitude and geographical distribution, D.C. would be much hotter. The city is much larger than it was, and airflow is even more impeded. Conservatively estimating, urban core temperatures would probably reach around 113.



It is impossible to predict exactly how many deaths would result. If the power stayed on, research by my University of Virginia colleague Robert Davis and myself suggests maybe 50 to 100. But it’s not at all clear that the air conditioners would continue to run. A similar, but short‐​lived, hot blast occurred on July 4 and 5, 1999, again enhanced by very dry conditions. Rural temperatures peaked at 105 degrees. Regional power people have told me they fear that only the Fourth of July holiday prevented a major electrical failure. If it had been a workday, the system would likely have exceeded its previous peak, the frigid morning of Jan. 11, 1994, which was marked by rolling brownouts as people turned up their heat.



Soon after the power goes out, deaths begin. We know this from the Chicago heat wave of July 1995, when, a thundershower caused a major outage. About 500 died.



Air quality? Some 300,000 acres of Virginia forest firecrackered skyward in 1930, about 30 times the current annual average. Atmospheric chemistry doesn’t care whether polluting organic compounds or obnoxious oxides are produced by a burning tree or an aging Belchfire 8. The result is the same: unhealthy air.



Low‐​level ozone, a pulmonary irritant, increases exponentially with temperature. There have been 10 Code Red (unhealthy) and two Code Purple (very unhealthy) air quality days in Virginia in 2002. Prior to this year, we’d had only one Code Purple, in 1999. Mix in 1930’s temperatures, add an increasing amount of urban warming and a bit more forest‐​fire smoke, plus the shimmering conga lines of SUVs panting on Interstates 66 and 395, and we would probably have to invent a new category: Code Black (deadly).



Droughts are expensive. After adjusting for inflation, the 1930 drought remains the most costly natural disaster in the modern Virginia record, totaling close to a cool $1 billion. It is noteworthy that the governor has called attention to the current drought as an additional load upon already strapped Virginia finances. The cost of 1930 puts some legendary local events to shame, namely Hurricanes Camille (1969), Agnes (1972) and Floyd (1999). Given our infrastructural changes, including dependence on automobile commuting, which would likely be severely curtailed because of the exponential increase in pollution caused by extreme heat, a repeat of 1930 could easily hit the multibillion‐​dollar threshold. Which costs more, closing National Airport because of terrorists or closing I-395 because of ozone?



In 1930, the James River was a sea of rocks. Water was so hard to come by that thirsty state highway road crews were refused drinking water by nearby residents. I’d like to say that’s as bad as the disaster documentary can get here — fires, incivility, deaths, thirsty snakes and agricultural bankruptcy — but then there’s the Fortress Monroe, Va., precipitation record. Virtually continuous back to 1837 when combined with figures from nearby Norfolk, it is one of only a handful of precipitation records of such quality and length over all of North America. And it provides a perspective that puts even 1930 in the shade.



Droughts are defined by a combination of intensity and duration. On the one‐​year time frame, droughts are considered large when the 12‐​month rainfall averages about 60 percent of normal. That’s been true of the driest periods in our current long‐​term drought. The driest 12‐​month periods in the drought that peaked in the summer of 1930 ran approximately 50 percent of normal. That additional 10 percent deficit accrued mainly in the summer and is the difference between this year’s misery and 1930’s tragedy.



On the multi‐​year time scale, 75 percent of normal rainfall indicates a serious problem, which is what makes the Fortress Monroe record so ominous. The entire period from 1851 to 1855 — half a decade — shows the area receiving only 60 percent of its average rainfall of 44 inches per year. For comparative purposes, over the past three drought‐​prone years, Maryland and Virginia have averaged between 80 percent and 90 percent of normal rainfall.



I first stumbled upon the Fortress Monroe record, as well as the better‐​known 1930 history, soon after I became state climatologist, more than 22 years ago. Since then, through several small‐​fry droughts and a couple of whoppers, I have made every effort to draw the remarkable Fortress Monroe anomaly to public attention. It is a worst case with unimaginable consequences, a movie I do not want to see, and a contingency for which there has been no preparation.



All we can do in the current situation is conserve water where it is needed. Rather than invoke statewide measures, a clear political mistake made in Maryland in 1999 (the drought was not statewide), we now rely more on local authorities and rational geography. On Friday, Warner ordered conservation methods for specific river basins rather than artificially derived political boundaries. These actions are in concert with local restrictions, which tend to be applied conservatively. Local authorities respond to local politics and they surely are loath to impose discomfort when it is not necessary. Whether conservation is mandated or set by market pricing (an increasingly attractive option in many local communities), people soon realize that the SUV with the “Save the Earth” bumper sticker — that rolling non sequitur common at regional beaches — doesn’t have to be washed every week. Nor do our children require half‐​hour showers.



These measures will help. But I find little comfort in history. If people think conditions are bad now, they need to look first backward, to 1930 and the mid‐​19th century, and then forward. How will we adapt to the inevitable repetition?
"
nan
"**The government has said there were no delays in delivering rapid coronavirus tests kits to Hull.**
Earlier, the prime minister pledged to ""immediately"" investigate why the kits had not yet arrived.
Hull was due to receive 10,000 ""lateral flow"" kits as part of a national rollout announced on 10 November.
The Department of Health and Social Care (DHSC) said it would send the tests ""once delivery information"" was received from the local authority.
Hull City Council has been approached for comment.
The city continues to have the worst infection rates in England, with 568.6 cases per 100,000 people in the week to 19 November.
Hull North Labour MP Dame Diana Johnson raised the issue on Monday in the House of Commons.
She said: ""Two weeks ago Hull was promised 10,000 lateral flow tests, but they still have not arrived.
""Shouldn't the prime minister focus on delivering on the ground what has already been announced, rather than grand new promises?""
Mr Johnson replied: ""I will take up immediately the point she makes about Hull and try to understand why they haven't got the lateral flow tests that she rightly wants to see.""
However, the Department of Health and Social Care later said there had been ""no delays in delivering tests to Hull"".
""We have requested the delivery information from the Local Authority and once this is received the tests will be sent,"" a spokesman added.
Last week, Hull City Council leader Stephen Brady wrote a letter to residents urging them to strictly follow lockdown measures imposed by the government on 31 October.
In the letter, he said: ""This really is our final chance to stop the spread by staying apart and thinking of others.""
He warned restrictions for the city were ""expected to remain severe"" after the current national lockdown ends on 2 December.
According to figures from Public Health England, Hull's infection rate has been slowing down but is still the highest in the country.
The city's rate has fallen from 779.9 per 100,000 people in the week to 12 November.
_Follow BBC East Yorkshire and Lincolnshire on_Facebook _,_Twitter _, and_Instagram _. Send your story ideas to_yorkslincs.news@bbc.co.uk _._"
"**Plans to raise tolls on the Tamar Bridge and Torpoint Ferry have been halted after a Â£1.6m government grant, managers have said.**
Crossings owners Cornwall Council and Plymouth City Council were considering increases to cover financial shortfalls caused by Covid-19 reducing traffic.
The cash meant plans to increase tolls at the start of 2021 ""will not need to go ahead at this stage"", managers said.
A charge for a car would stay at Â£2 or Â£1 for pre-paid journeys, they added.
The routes are free to cross from Plymouth, but tolls apply when coming from Cornwall.
The Tamar Bridge and Torpoint Ferry JointÂ Committee said each council was receiving Â£821,553 from a compensation scheme set up by the government to help local authorities deal with the impact of the coronavirus and it would cover the period up to July 2020.
The money meant ""proposals to increase toll prices at the beginning of next year will not need to go ahead at this stage"", it said.
Bosses were planning to apply for more money for the period after July, it added.Â
The bridge and ferry carry about 18 million vehicles a year.
They receive no government subsides and had an operational budget of about Â£10m in the 2018/19 financial year."
"Greta Thunberg will visit the UK next week to take part in a youth protest in Bristol. The 17-year-old climate activist, who launched a global youth-based movement when she began a “climate strike” outside Sweden’s parliament in 2018, plans to join protesters on College Green on Friday.  It will be second time Thunberg has joined protesters in the UK in the past year. Last April, she addressed Extinction Rebellion activists in central London. She also made a speech in parliament, telling MPs the UK’s support for fossil fuels and airport expansion was “beyond absurd”. One of the organisers of the Bristol Youth Strike 4 Climate, seventeen-year-old Milly Sibson, told PA Media: “We are all just so excited – everyone is so excited about the thought of hearing her talk. “I would love the chance to meet her because she is the founder of this movement and she is so important to it – she is an idol even though she is younger than me. We really hope loads of people join us to welcome her to Bristol.” Milly said Greta had originally planned to visit London, but as the area planned for the protest in the capital was too small the organisers had recommended Bristol instead. Heading for the UK! This Friday, the 28th, I’m looking forward to joining the school strike in Bristol! We meet up at College Green 11am! See you there! @bristolYS4C pic.twitter.com/n1GOJqMUVQ Bristol was awarded the title of European Green Capital in 2015. In 2018, the city became the first UK authority to declare a “climate emergency”, with the council unanimously backing a commitment to be carbon neutral by 2030. By contrast, the government has committed to net zero carbon emissions in the UK by 2050. Last month, Bristol declared an ecological emergency because of the local decline of many birds, insects and some mammals."
"**After being labelled the ""patient zero"" of an outbreak of Covid, a Congolese-Canadian physician says he became a target for racist threats, a pariah in his community, and a ""scapegoat"" for local officials.**
When Dr Jean-Robert Ngola heard that he had to pick up his daughter last May, he quickly did the maths.
His daughter lived in Montreal with her mother, about seven hours away from his home in Campbellton, New Brunswick.
As a family physician, he knew that he needed to be as safe as possible to limit the spread of Covid to others. But as a parent, he had to come get his child so that her mother could attend a family funeral in Africa.
In order to get her and have contact with as few people as possible, he hopped in his car and drove all day, spending the night at his brother's before driving her back.
Before leaving, Dr Ngola called the local police, asking for clarity around the laws about self-isolation.
New Brunswick has one of the strictest quarantine policies in Canada. Along with several other eastern provinces, it has formed an ""Atlantic bubble"" - in the early months of the pandemic, most forms of travel into the bubble were restricted, and anyone entering had to quarantine for 14 days.
But as a frontline worker, he says police told him he was exempt.
Not wanting to leave his patients without a doctor, he decided to go back to work.
On 25 May, Dr Ngola heard that one of his patients had been diagnosed with the virus. He got tested, and began to self-isolate with his four-year-old daughter.
At 11am on 27 May, he learned he, too, had the virus, although he had no symptoms.
Then, his life began to fall apart. Within the hour, his identity began to spread online. Later in the afternoon, Premier Blaine Higgs, who leads the provincial government, was chastising him on live television.
At least two other people had contracted coronavirus ""due to the actions of one irresponsible individual,"" Mr Higgs said, after nearly two weeks without a single case.
Although the premier did not name Dr Ngola directly, by that time, people had connected the dots, and photographs of his office were circulating online.
Provincial health officials told the media Dr Ngola had contracted the disease in the neighbouring province of Quebec, and spread it to others because he did not follow the 14-day mandatory quarantine for people who had been out of New Brunswick.
But Dr Ngola, and his lawyer Joel Etienne, say the rules were not clear, and Dr Ngola was following the same practices as people around him.
They also dispute the province's claim that he was ""patient zero"".
Although no criminal charges were filed, Dr Ngola faces a civil charge for violating the Emergency Measures Act and could face a fine up to C$10,200 ($7,600, Â£6,000). The case is currently making its way before the courts.
His employer, VitalitÃ© Health Network, immediately suspended him without pay for breaking protocols.
""I was the scapegoat. As soon as my diagnosis is madeâ¦ one hour later, my life changed,"" he said.
In a statement to the BBC, a spokesperson for Vitalite confirmed that Dr Ngola's suspension continues, and declined to comment further.
The premier's office did not respond to the BBC's request for comment.
The reaction from the community was swift and brutal. Dr Ngola, who is originally from the Democratic Republic of Congo, said people were telling him to ""go back to Africa"" and other forms of racist abuse.
Quarantining in his home with his small daughter, he feared for his safety when his address was leaked online and had to go under police protection. But he was also constantly under police scrutiny, he says, because people kept phoning in with bogus ""tips"" claiming to have seen him break quarantine.
Dr Ngola says the harassment was so bad, he had to leave the province. He was offered a job in a small community in Quebec, where he has been living for the past several months. He says he feels welcome there, but his experience in Campbellton has left psychological scars.
""I cannot have the same life because now I'm public,"" he says.
This is not the first time he's had to start over. Born in the Congo, Dr Ngola had wanted to be a doctor since he was a small child, after coming down with polio and being unable to get the appropriate medical treatment because it was too costly.
""My childhood ambition was to become a doctor in order that children like myself would be spared,"" he says.
He paid his way through school by tutoring other students, and practised medicine during his country's brutal civil war.
In 2000, he immigrated to Belgium, where he had to retrain in order to continue to be a doctor. Then five years later, he relocated to Canada, because he felt he would face less prejudice as an African immigrant. Once again, he went back to school in order to practise medicine.
Now, he feels disillusioned. Dr Ngola says he believes his race and immigration status played a factor in how he was treated not only by the public at large, but by the province's top officials.
""What is the difference with me? The difference is I'm black and I'm a foreigner,"" Dr Ngola said.
Although he felt like a pariah at home, around the country his fellow physicians were rallying to his defence. In September, 1,500 physicians signed a letter condemning his treatment, and demanding an investigation into how his name was leaked to the press.
""All of us signed below have felt tremendous anger, discomfort and frustration with the backlash that followed once you were publicly identified. What unravelled thereafter was unjust, unkind and dehumanizing,"" the letter, which was spearheaded by Dr Danusha Foster, read.
""We strongly believe that systemic racism coupled with the stigma surrounding individuals infected with the Covid-19 virus have significantly contributed to the crucifixion of your character within the public eye.""
Dr Foster, a family physician who lives in Ontario, says that when she first heard about Dr Ngola, she, like many others, judged him.
""We're in a deadly pandemic, and health professionals should be held to a higher standard at this time, because we're supposed to be modelling for the general population what we should be doing,"" she told the BBC.
But after hearing about the abuse he was suffering, and reading media articles critical of the province's investigation, she began to feel sympathy.
""He was being judged in the public eye before the facts were known,"" she says.
She says patient confidentiality is the ""core"" of the doctor-patient relationship, and whoever leaked his name should be held to account.
After talking about his case on online physicians groups, she decided to organise the letter of support, to show Dr Ngola that he wasn't alone.
""I hadn't even written my letter, and I already had 800 names that wanted to sign,"" she says. ""We realised as we watched this case that what happened to Dr Ngola, could have happened to anyone of us... if we made one little mistake.""
Over 40 cases and two deaths have been connected to the outbreak in the Campbellton region since 27 May. But it remains unclear if Mr Ngola was the source.
Anyone entering New Brunswick from another province outside the Atlantic Bubble is supposed to quarantine for 14 days.
But residents of Campbellton, which is on the border of Quebec, were allowed to cross the border without self-isolating for certain reasons, such as if they worked in Quebec, had to attend a medical appointment in Quebec or if they shared custody of a child in Quebec.
According to the provincial guidelines, ""all such workers and individuals who are exempt from self-isolation must travel directly to and from work and/or their accommodations, self-monitor and avoid contact with vulnerable individuals""
Dr Ngola spent about 30 hours in Quebec, including an overnight stay with his brother to rest up after the seven-hour drive. He also saw two colleagues in Trois-Rivieres, although they were masked and socially distanced. He also had contact with a petrol-station employee.
Dr Ngola said many of his co-workers went back and forth to Quebec and did not fully self-isolate, so he did not think he was breaking the rules.
His employer, Vitalite Health, told the CBC's Fifth Estate that all workers were ordered to self-isolate after returning to the province unless they lived in Quebec.
Mr Etienne says regardless of whether his client broke the rules, the province failed to do its due diligence before blaming him for the outbreak.
He says they had not finished contact-tracing the four individuals in Quebec whom Dr Ngola had contact with, before claiming the doctor was the source of the Covid cluster in New Brunswick. He says their own private investigator found that neither his brother, the two colleagues, nor the gas station employee tested positive for Covid.
His daughter, however, did. Both she and her father have made a full recovery.
There had been at least one confirmed case of Covid in Campbellton in the days prior to Dr Ngola's diagnosis, and his lawyer says he could have got it from community spread."
"**Scotland's largest teaching union has said there is ""clear support"" among its members for industrial action over Covid safety concerns in some schools.**
The EIS has been calling for schools in level four areas to either close or move to a blended learning model.
But the Scottish government insists that the evidence clearly supports schools remaining open.
The union has been asking members whether they would potentially take industrial action over the issues.
It has now published the results of the online survey, with half of the 18,733 respondents backing schools in level four areas closing to pupils, while about a third supported a move to a part-time blended model.
About two thirds (65.7%) of the respondents said they would be willing to take industrial action, including potentially strike action, if those options were rejected by councils or Public Health Scotland.
The EIS said many teachers who responded to the survey had indicated they do not currently feel safe at work despite the social distancing and additional hygiene measures that have been put in place.
And it argued that keeping schools fully open ""cannot come at the expense of teacher and pupil wellbeing"".
The union's general secretary, Larry Flanagan, said the survey showed that teachers held a range of opinions on the best means of keeping people safe in schools.
But he said there was ""clear support for moving to industrial action in higher risk areas to protest where teachers feel that the measures required to keep schools safe have not been delivered.""
Mr Flanagan added: ""For level four restrictions to be as effective as we would wish them to be, short term closure or part closure of schools need to be considered.""
The survey results showed that only 4.6% of respondents felt very safe in schools, while 25.9% felt safe.
A further 26.3% said they felt neither safe or unsafe, with 32.5% saying they felt unsafe and 9.5% very unsafe.
Mr Flanagan said the feeling of being at risk was heightened for teachers in secondary schools, for teachers in higher risk areas under level three or level four restrictions, and for teachers in vulnerable groups or who live with or provide care for vulnerable family members.
The Scottish government says the evidence shows that schools are ""not a significant area of transmission"".
But Education Secretary John Swinney said he was ""concerned"" that some teachers who responded to the survey had said they do not feel safe.
He added that extensive guidance was in place to reduce the risk of Covid transmission in schools, and that enhanced risk mitigations were in place in level three and level four areas to protect clinically vulnerable staff and pupils.
Mr Swinney said school staff could already to get a coronavirus test even if they do not have symptoms, with plans being made to potentially pilot and roll out rapid testing in schools.
But Mr Swinney acknowledged: ""We need to do more to ensure everybody feels safe"".
The education secretary told BBC Scotland last week that the number of positive cases among pupils represented only 0.1% of all pupils.
He said the level for teachers varied between about 4% and 7%, which he said was ""no different to any other workforce in that category"".
Parents group Us For Them Scotland has called for the government to keep schools open - even in level four areas.
""We know there are influential groups who've wanted schools shut right from the start, and now strike action is being used as another tactic to force this through,"" organiser Jo Bisset said last week.
""All of this serves to damage the wellbeing and prospects of children.""
""I really don't know many teachers who want to close schools, but increasingly we're asking each other: 'Do you think it's safe to be here?',"" one teacher - who asked to remain anonymous - told BBC Scotland.
He said there had been ""a lot of tears"" from colleagues who were anxious to be at work.
In his school, which is in a level four area, several teachers have lost loved ones to Covid.
The teacher said now was the time to move to blended learning - ""we worked so hard planning for it, now is the opportunity. Teachers are still in every day, it's not that we want to close the schools. We just want to be safe,"" he said.
He added that some children were now into their third period of isolating since the summer, yet teachers were hardly being told to isolate at all.
He said teachers were angered by claims that transmission rates were not increased in schools.
These claims were ""patronising"" given how ""blatant"" it was that transmission was occurring, he added."
"**Chancellor Rishi Sunak is promising a Â£4.3bn package to help hundreds of thousands of jobless back to work as he prepares to unveil his Spending Review.**
He said in an announcement ahead of Wednesday's review that it would include Â£2.9bn for a new Restart jobs scheme and Â£1.4bn to expand the Jobcentre Plus agency.
Mr Sunak said his ""number one priority is to protect jobs and livelihoods"".
The review will outline spending for such things as roads, police and NHS.
But it comes against a backdrop of an economy hit by the coronavirus pandemic and huge job losses.
Earlier this month, official figures showed the UK's unemployment rate rose to 4.8% in the three months to September, up from 4.5%. There was a big rise in the number of 16 to 24-year-olds out of work.
Earlier this month, the Bank of England forecast that the jobless rate could rise to nearly 8% by the middle of next year
Under the Restart scheme, people who have been out of work for more than 12 months will be provided with regular intensive support tailored to their circumstances.
Mr Sunak will also confirm in his Commons statement on Wednesday more funding for the next stage of his Plan for Jobs - including Â£1.6bn for the Kickstart work placement programme, which the Treasury says will create up to 250,000 state-subsidised jobs for young people.
The scheme, first launched in August, offering employers Â£2,000 for every new worker they take on, is to be extended to the end of March.
There will also be a Â£375m skills package, including Â£138m of new funding to deliver Prime Minister Boris Johnson's Lifetime Skills Guarantee.
Mr Sunak said on Tuesday: ""This Spending Review will ensure hundreds of thousands of jobs are supported and protected in the acute phase of this crisis and beyond with a multibillion package of investment to ensure that no-one is left without hope or opportunity.""
The package has won the support of business and industry. Matthew Fell, policy director at the CBI employers' group, said the chancellor was right to focus on job creation.
""Covid-19 has swept away many job opportunities, for young people in particular,"" he said. ""The scarring effects of long-term unemployment are all too real, so the sooner more people can get back into work the better.""
Claire Walker, co-executive director, at the British Chambers of Commerce, said retraining and reskilling was vital to getting people back to work.
""Investment in Kickstart, in which Chambers are playing a leading role, and the launch of the Restart scheme, will be critical in helping support the recovery,"" she said.
However, economist Nye Cominetti, from the Resolution Foundation, said the government must learn lessons from previous schemes which failed to meet expectations.
""The chancellor is right to put in place help for those out of work for long periods as they often struggle most in periods of high unemployment.
""The Restart Scheme is a big step up in terms of job support. The Â£2.9bn allocated for the coming three years exceeds that spent on the Work Programme over five years after the financial crisis.
""But for the new approach to be effective, ministers must learn lessons from the patchy record of that scheme, particularly the need for more intensive support for harder-to-help groups, who were too often side-lined."""
"
Share this...FacebookTwitterThe German online CO2 Handel here writes that things are looking bleaker than ever for a Kyoto successor climate treaty.
Once again German Environment Minister Norbert Röttgen has dumped more cold water on expectations for a climate treaty in Durban later this year. CO2 Handel writes:
‘A breakthrough for a global climate protection treaty as a successor to the Kyoto Protocol which expires in 2012 is unrealistic,’ said Röttgen on Friday in Capetown.”
Even worse for the junk-science-based climate rescue movement is that their numbers are shrinking rapidly and approaching fringe status. As China and USA refuse to sign a treaty, CO2 Handel writes:
After the announced departures of Japan and Canada from the Kyoto process, only the EU as well as Norway and a few other countries see themselves contractually obligated to systematically reduce CO2 emissions. Rising CO2 emissions of countries, among them the USA and China, as well as the shrinking number of countries in the Kyoto process could result in Kyoto countries being responsible for only 16% of the global CO2 emissions.”
Share this...FacebookTwitter "
"

One reaction to President Bush’s plan for a permanent moon base and a trip to Mars is, “Great! It’s about time NASA stopped going around in circles in low Earth orbit and returns to real science and exploration.” Unfortunately, there’s not a snowball’s chance in the sun that the same agency that currently is constructing a downsized version of its originally planned space station, decades behind schedule, at 10 times its original budget, a few hundred miles up in orbit, will be able to build a station several hundred thousand miles away on the moon.



If Americans are again to walk on the moon and make their way to Mars, NASA will actually need to be downsized and the private sector allowed to lead the way to the next frontier.



The lunar landings of over three decades ago were among the greatest human achievements. Ayn Rand wrote that Apollo 11 “was like a dramatist’s emphasis on the dimension of reason’s power.” We were inspired at the sight of humans at our best, traveling to another world. In announcing NASA’s new mission, President Bush echoed such sentiments, speaking of the American values of “daring, discipline, ingenuity,” and “the spirit of discovery.”



But after the triumphs of Apollo, NASA failed to make space more accessible to mankind. There were supposed to be shuttle flights every week; instead, there have been about four per year. The space station was projected to cost $8 billion, house a crew of 12 and be in orbit by the mid‐​1990s. Instead, its price tag will be $100 billion and it will have only a crew of three. Worse, neither the station nor the shuttle does much important science.



Governments simply cannot provide commercial goods and services. Only private entrepreneurs can improve quality, bring down the prices, and make accessible to all individuals cars, airline trips, computers, the Internet, you name it. Thus, to avoid the errors of the shuttle and space station, NASA’s mission must be very narrowly focused on exploring the moon and planets, and perhaps conducting some basic research, which also might serve a defense function. This will mean leaving low Earth orbit to the private sector.



Thus, the shuttle should be given away to private owners. The United Space Alliance, the joint venture between Boeing and Lockheed‐​Martin that refurbishes the shuttle between flights, would be an obvious candidate. Let a private owner fly it for paying customers–including NASA, if necessary — if it is still worth flying.



NASA also should give up the money‐​draining space station, and sooner rather than later. The station might be turned over to international partners or, better still, to the mostly private Russian rocket company, Energia — and the Western investors who were in the process of commercializing and privatizing the Mir space station before the Russian government brought it down for political reasons. If need be, NASA can be a rent‐​paying station tenant.



NASA centers that drive up its overall budget but do not directly contribute to its mission should be shut down. If the government wants to continue satellite studies of the climate and resources or other such functions, they could be turned over to other agencies, such as EPA and Interior Department. 



NASA and the rest of the government should contract for launch services with private companies, which would handle transportation to and from low Earth orbit. Contracting with private pilots with private planes is what the Post Office did in the 1920s and 1930s, which helped the emerging civil aviation sector. Further, to facilitate a strong private space sector, the government needs to further deregulate launches, export licensing and remove other barriers to entrepreneurs.



Creating enterprise zones in orbit would help make up for government errors of the past. Rep. Dana Rohrabacher proposes a “Zero Gravity, Zero Tax” plan that would remove an unnecessary burden from “out‐​of‐​this‐​world risk‐​takers.”



NASA will also need to do business in new, innovative ways. For example, if a certain technology is needed for a moon mission, NASA could offer a cash prize for any party that can deliver it. The federal government used such an approach for aircraft before World War II, modeled after private prizes that helped promote civil aviation.



Even if the federal government foots the bill for a moon base, it should not own it. Rather, NASA should partner with consortia of universities, private foundations and even businesses that are interested in advancing human knowledge and commercial activities. NASA could simply be a tenant on the base.



Or consider a radical approach proposed by former Rep. Bob Walker. The federal government wouldn’t need to spend any taxpayer dollars if it gave the first business to construct a permanent lunar base with its own money a 25‐​year exemption from all federal taxes on all of its operations, not just those on the Moon. Think of all the economic activity that would be generated if a Microsoft or General Electric decided to build a base! And the tax revenue from that activity probably would offset the government’s revenue losses from such an exemption.



If we’re true to our nature, we will explore and settle planets. But only individuals with vision, acting in a free market, will make us a truly space‐​faring civilization. 
"
"**A woman who took part in a Covid-19 vaccine trial has said she volunteered due to the ""hope"" it offered.**
Faye Wilson was part of the programme led by Newcastle Hospitals, which has helped the Oxford coronavirus study.
The vaccine has been shown to be highly effective at stopping people developing symptoms, according to a large trial.
Ms Wilson, 72, and from Morpeth in Northumberland, said it was an ""incredible privilege"" to have played a ""tiny part"" in delivering it.
In total the North East recruited the third-highest number of participants in the UK for the study.
Ms Wilson said she was ""ecstatic"" about being part of it.
""I think through all of this, what people have been able to hang on to is some hope and some light at the end of the tunnel,"" she said.
""For me, it was around hope and around knowing there was a way out, and to be part of contributing to that hope and be part, a tiny part of delivering it, has been an incredible privilege.""
Ms Wilson, who has lost friends to coronavirus, encouraged people to think to the future but remember the present threat of the pandemic.
""At the moment it's a really difficult time because it's not there yet and people are frightened and tired and want to change things,"" she added.
""If we don't hold on and hold on to what we are doing, every death will be wasted because we can get there and it will make a difference, so these next few months are really, really challenging times for us all.
""But we have got hope and it's there and we need to hang on.""
Dr Christopher Duncan, honorary consultant in infectious diseases at Newcastle Hospitals, based at the RVI, said it was great that the region played ""such a leading role"" in delivering the vaccine.
He added: ""It's just fantastic news for all of the team and particularly for everyone that has been working so hard to deliver this trial, and every single day at the moment in the hospital there are lots and lots of patients seriously ill with this devastating disease, and it comes at a time where we really, really need it.""
_Follow BBC North East & Cumbria on _Twitter _,_Facebook _and_Instagram _. Send your story ideas to_northeastandcumbria@bbc.co.uk _._"
"
Jeff Id at the Air Vent has been doing some interesting work lately. Before the NSIDC Arctic Sea Ice anomaly plot went kaput due to failure of the satellite sensor channel they have been using, they had created a vast archive of single day gridded data packages for Arctic sea ice extent. Jeff plotted images from the data as viewed from directly over the North Pole. It took him over 15 hours of computational time. An example image is below.

Jeff gathered up all the resultant plotted images and turned them into a movie, but placed them on the website “tinypic” where the movie won’t get much airplay.
I offered Jeff the opportunity to have it hosted on YouTube and posted here, where it would get far greater exposure and I completed the conversion this afternoon.
What I find most interesting is watch the “respiration” of Arctic Sea Ice, plus the buffeting of the sea ice escaping the Arctic and heading down the east coast of Greenland where it melts in warmer waters.
Jeff writes:
I find the Arctic sea ice to be amazingly dynamic. Honestly, I used to think of it as something static and stationary, the same region meltinig and re-freezing for dozens or even hundreds of years – not that I put much thought into it either way. Shows you what I know.
This post is another set of Arctic ice plots and an amazing high speed video. The NSIDC NasaTeam data is presented in gridded binary matrices in downloadable form HERE. 
The data is about 1.3Gb in size so it takes hours to download, I put it directly on my harddrive and worked from there. The code for extraction took a while to work out but was pretty simple in the end. This code ignores leap years. Formatting removed courtesy of WordPress.
filenames=list.files(path=”C:/agw/sea ice/north sea ice/nasateam daily/”, pattern = NULL, all.files = TRUE, full.names = FALSE, recursive = TRUE)
trend=array(0,dim=length(filenames)-1)
date=array(0,dim=length(filenames)-1)
masktrend=array(0,dim=length(filenames)-1)
for(i in 1:(length(filenames)-1))
{
fn=paste(”C:/agw/sea ice/north sea ice/nasateam daily/”,filenames[i],sep=””) #folder containing sea ice files
a=file(fn,”rb”)
header= readBin(a,n=102,what=integer(),size=1,endian=”little”,signed=FALSE)
year=readChar(a,n=6)
print(year)
day=readChar(a,n=6)
print(day)
header=readChar(a,n=300-114)
data=readBin(a,n=304*448,what=integer(),size=1,endian=”little”,signed=FALSE)
close(a)
if(as.integer(year)+1900<=2500)
{
date[i]=1900+as.integer(year)+as.integer(day)/365
}else
{
date[i]=as.integer(year)+as.integer(day)/365
}
if(i==1)
{
holemask= !(data==251)
}
datamask=data<251 & data>37 ## 15% of lower values masked out to match NSIDC
trend[i]=sum(data[(datamask*holemask)==1])/250*625
}
###mask out satellite F15
satname=substring(filenames,18,20)
satmask= satname==”f15″
newtrend=trend[!satmask]
newdate=date[!satmask]
After that there is some minor filtering done on 7 day windows to dampen some of the noise in the near real time data.
filtrend=array(0,dim=length(newtrend))
for(i in 1:(length(newtrend)))
{
sumdat=0
for(j in -3:3)
{
k=i+j
if(k<1)k=1
if(k>length(newtrend)-1)k=length(newtrend)-1
sumdat=sumdat+newtrend[k]
}
filtrend[i]=sumdat/7
}
So here is a plot of the filtered data:

Here is the current anomaly.

This compares well with the NSIDC and cryosphere plots. This anomaly is slightly different from some of my previous plots because it rejects data less than 15% sea-ice concentration. Cryosphere rejects data less than 10%. In either case the difference is very slight but since we’ve just learned that the satellites have died and are about 500,000km too low, my previous graph may be more correct. I hope the NSIDC get’s something working soon.
All of that is pretty exciting but the reason for this post is to show the COMPLETE history of the NSIDC arctic sea ice in a video. I used tinypic as a service for this 27mb file so don’t worry, you should be able to see it quite well on a high speed connection. It took my dual processor laptop computer more than 15 hours to calculate this movie, I hope it’s worth it. Brown is land, black is shoreline, blue is water except for the large blue dot in the center of the plot. The movie plays double speed at the beginning because the early satellite collected data every other day. You’ll see the large blue circle change in size flashing back and forth between the older and newer sat data just as the video slows down.
After staring at the graphs above you think you understand what is happening as ice gradually shrinks away. Well the high speed video shows a much more turbulent world with changing weather patterns in 2007 and 2008 summer blasting away at the south west corner of the ice. I’ve watched it 20 times at least, noticing cloud patterns (causing lower ice levels), winds, water currents and all kinds of different things. I’m not so sure anymore that we’re seeing a consistent decline to polarbear doom, with this kind of variance it might just be everyday noise.
Maybe I’m nuts, let me know what you see. 
No Jeff you aren’t nuts. Here is the YouTube Video, suitable for sharing:

Here is another video I posted on You Tube last month which shows the flow of sea ice down the east coast of Greenland. Clearly there is more at work here than simple melting, there is a whole flow dynamic going on.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e956e93f9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Conservationists have recently become very excited about financial incentives. The idea is to pay people to do things that will help biodiversity, for example, where farmers are paid not to till crops that reduce soil erosion or where landowners are given money to plant trees to capture carbon in the atmosphere.  This technique, considered by some as bribery to do what you want, has actually changed environmental behaviour for the better in some significant instances. “Great”, I hear conservationists say, “let’s pay everyone to do exactly what we want!”  The possibilities are endless: we could give farmers money to set aside land for nature, we could make fishing more sustainable, or we could reward people to not kill threatened predators.  There are, however, problems with the system behind paying people to do things that they wouldn’t normally do, not least because sometimes money cannot solve everything. Take carnivores, for instance: menacing, bloodthirsty killers of infamous myth and legend that have plagued farmers’ nightmares for millennia.  These species provoke such raw emotions in some people that intolerance has become a cultural identity, learned through generations of ingrained hatred.   So when conservationists came up with the idea to pay reindeer herders to tolerate wolves in Sweden, it is no surprise that the herders declined the payment in favour of managing wolves themselves (that is: killing them). Such a long-standing conflict between livestock farmers and carnivores is too embedded in the psyche to overcome purely by throwing money at people.  If anything, it could be seen as an insult to their integrity, just as financial incentives paid to families of organ donors have been criticised for turning human life into a commodity.  To many farmers, a cow is worth more than its market value, just like their farm is worth more to them predator-free (at the expense of compensation) than with predators (and a financial reward).  This may be one of the reasons why schemes have often failed to increase tolerance even when reimbursements have been paid for livestock killed by predators. To pay a farmer to accept a predator onto a farm is almost as absurd as paying Israelis to tolerate Palestinians: both are long-standing, deep-rooted conflicts that cannot be bought out unless underlying issues are resolved.  Both situations can therefore be regarded as “wicked problems” with no clear way.  As such, is it even useful to use financial incentives for carnivore conservation? As with other challenges in wildlife management, the answer is not clear cut.  Yes, sometimes it does appear to work with species that may not have caused much controversy in the past, such as the aforementioned Swedish scheme which increased tolerance towards wolverine and lynx.  But for highly emotive species such as wolves or lions, bribery appears impotent.  Consequently, we must try other tactics to improve opinions towards the predator species that produce these immensely charged responses.   Educating children on how important predators are to the ecosystem may improve attitudes before they have been shaped by their parents’ perceptions, but this tactic takes time and effort (and may tread on dodgy ethical ground if it is seen to be brainwashing children ). Or it may be that farmers find their own way to benefit from carnivore presence on their land, such as by offering tourists or trophy hunters the chance to “shoot” wildlife (with a camera or a gun).   But this will not sway the extremists who cannot fathom accepting such a beast onto their land.  For these old-school farmers, the only way they can envisage a predator is hanging as a skin on their wall or in the security of a fenced protected area far from their land."
"When the cross-party Treasury committee is officially appointed on Monday, one of its first tasks will be to question Andrew Bailey over his role as the next governor of the Bank of England, which he is due to start on 16 March. This is standard procedure for the influential group of MPs, but comes at a time of great challenge for an incoming governor, not least amid the unfolding economic collapse caused by the coronavirus outbreak. Bailey, who replaces Mark Carney and was chosen as his successor in December by the former chancellor Sajid Javid, will face several urgent tasks in his opening weeks and months as governor to address – as well as some potentially awkward questions. Here are some of the key subjects the committee is likely to quiz him on. His recordWhen Bailey was appointed he was widely hailed as a “safe pair of hands”. However, critics of his time at the Financial Conduct Authority beg to differ. Gina Miller, the businesswoman best known for her legal challenge to the government’s attempt to push through Brexit without parliamentary approval, is leading a group calling for a review of Bailey’s appointment and has welcomed the Treasury committee’s opportunity to examine his record. Miller has highlighted his reaction to the collapse of investments run by London Capital & Finance, and the liquidation of Neil Woodford’s once-popular flagship investment fund, among other things. “The hundreds of thousands of Britain’s depositors, savers, investors and pensioners whose lives have been ruined and savings squandered, on Mr Bailey’s watch as CEO of the Financial Conduct Authority, deserve no less,” Miller said. Bailey has argued on many of the issues that the FCA’s statutory powers were limited. However, the scandals have overshadowed his time at the regulator. CoronavirusThere are questions about how much the Bank can do to support the economy given the global nature of the coronavirus outbreak and the limited firepower central banks have at their disposal to combat an economic downturn. Bailey arrives with interest rates still close to the lowest levels in the Bank’s 325-year history, at 0.75%, only 0.25 percentage points higher than when Carney started at the Bank almost seven years ago and limiting the scope for further cuts if deemed necessary. Whether Bailey will be able to make much of an inroad into returning rates to more normal levels will be closely watched. BrexitThe British economy could be put under renewed strain later this year as the Brexit negotiations gather pace ahead of the deadline for the UK’s exit from the transition period at the end of December, which Boris Johnson has warned he will not extend. Once the dust from the coronavirus outbreak has settled, the path for interest rates and the continued stability of the financial system will partly depend on the nation’s new trading relationship with the EU, and the extent to which there are tougher barriers, friction at Britain’s borders and whether there is a smooth or abrupt transition. Carney had been accused of bringing the central bank’s independence into question, given his repeated warnings about the threat posed by Brexit to the UK economy and banking system. Serving under the government of Boris Johnson, the new governor could face more questions about Britain’s future relationship with the EU. ClimateAs governor, Mark Carney brought to the top of the agenda the importance of global heating as a problem for central bankers to address, warning about the existential crisis facing the world financial system. Threadneedle Street is preparing to run a “catastrophic” climate-related stress test next year, a landmark moment. Bailey will need to show early on that he will take the issue as seriously as his predecessor. Hedge fund scandalBailey is likely to face questions about the embarrassing revelation  about a backup audio feed that without the Bank’s knowledge potentially allowed traders early access to an audio feed of market-moving Bank press conferences. The answers to those questions could have broader implications for how the Bank protects key national infrastructure, amid criticisms of where accountability lies for failures at the institution. DiversityBailey is set to become the 121st governor – and the 121st man to take the role. That imbalance is mirrored throughout the top levels of the Bank, with a lack of gender or ethnic diversity in top jobs. There is only one woman, Silvana Tenreyro, on the Bank’s nine-member rate-setting monetary policy committee. Across the Bank Bailey will be pressured to close ethnicity and gender pay gaps, as well as increase representation, which sometimes went backwards during Carney’s tenure. IndependenceThe Bank of England is run at arm’s length from the government to preserve its independence at setting interest rates and producing economic forecasts. As the government expands its spending – boosting the economy – the Bank of England may feel the need to raise interest rates to stop inflationary pressures building. That could bring the Bank into conflict with the government and Johnson’s choice as chancellor, Rishi Sunak. How Threadneedle Street and Downing Street work together will be a key question of Bailey’s regime – and potentially his legacy as governor."
"I’m reading one of a small forest’s-worth of beautiful new picture books about the environment with my eight-year-old twins. The Sea, by Miranda Krestovnikoff and Jill Calder, takes us into mangrove swamps and kelp forests and coral reefs. We learn about goblin sharks and vampire squids and a poisonous creature called a nudibranch. Then we reach the final chapter on ocean plastics. When we learn that by 2050 there could be more plastic in the ocean than fish, Esme bursts into inconsolable tears. Since the deserved success of The Lost Words, a large book filled with beautiful paintings of everyday wildlife by Jackie Morris and “spells” by Robert Macfarlane, which my children loved chanting aloud, bookshops have been filled with other gorgeously illustrated oversized tomes about nature and the environment. There’s Ben Hoare’s gilt-edged Anthology of Intriguing Animals, A Wild Child’s Guide to Endangered Animals by Millie Marotta, and other recent classics such as the lovely How Does My Garden Grow by Dutch author Gerda Muller and A First Book of Nature by Nicola Davies. Consumers are spending more money than ever on books for children, and the number of new children’s books about the climate crisis and wildlife has more than doubled over the past year, with sales also doubling, according to data from Nielsen Book Research. The Greta Thunberg effect is creating a new library of children’s books about eco-warriors. Nosy Crow rushed out Earth Heroes four months after signing up author Lily Dyu, selling 9,000 copies in the UK since publication in October. Macfarlane and Morris have just announced a Lost Words sequel, which will be published this autumn. Bloomsbury is publishing what appears to be the apotheosis of right-on environmentalism for kids in February, Fantastically Great Women Who Saved The Planet, by Kate Pankhurst. I love reading with my children and I’m passionate about the environment but my inner sceptic wonders if this worthiness explosion is chasing a phantom market. Who wants heavy-handed moral tales for bedtime? What if eco-doom just generates despair? And how many big picture books are dull-but-worthy Christmas presents that lie unread by real children? Jill Coleman, the director of children’s books at BookTrust, Britain’s largest reading charity, insists the environmental books boom is driven by “a genuine interest and passion from children”, with publishers following their lead. She believes there are many different books that inspire action, rather than despair, and BookTrust offers recommended reading lists of environmental stories for children of all ages. “There have always been lots of nature books for kids – wonderful authors like Nicola Davies help children to connect with and understand the natural world – but the recent trend is books which help children think about what they can do,” she says. “Reading is important because it gives children power and agency in all sorts of ways. We need them to feel they can make a difference. I know there are much bigger issues than not buying a plastic bag in a supermarket but when you are nine you can only do what you can do, and you need to feel that you can do something.” As well as Earth Heroes, Nosy Crow has sold 12,000 UK copies of How To Help A Hedgehog And Protect A Polar Bear, by Jess French and Angela Keoghan, and later this year will publish YouthQuake by Tom Adams and Sarah Walsh, a book about 50 young people who “shook the world”, invariably featuring Thunberg. “It isn’t just a move by publishers, there is a real hunger for it at the moment,” says Kate Wilson, the managing director and co-founder of Nosy Crow. She believes environmentalism is “becoming less siloed” within publishing as environmentally themed books infiltrate many genres, even the cutesy end of fiction for eight-year-olds with Nosy Crow’s reissue of a Holly Webb eco-series (Maya’s Secret, Izzy’s River, Emily’s Dream, Poppy’s Garden). It was originally published in 2012, when “the world was not ready,” as Wilson puts it. Wilson admits there is a risk that children’s publishing becomes overly didactic, pushing a vision of how it wants the world to be “because we have a real commitment to the messages that children are given”. Then again, why wouldn’t we want to encourage care for the planet, Wilson argues: children’s publishers perpetually take moral positions, and “never publish books where bullies win, for instance”. For wildlife journalist and author Ben Hoare, the dilemma is the same as that repeatedly expressed by David Attenborough with his epic natural history TV series. Do writers focus on the wonders of nature and thereby inspire young readers or are they duty-bound to cover the planetary extinction crisis and the climate emergency, even when it risks fuelling climate-anxiety at a young age? Hoare’s lavish Anthology of Intriguing Animals, which has sold in numbers that authors of adult nature writing can only dream of, goes for inspiration with an awareness of global crises. Like many children’s authors, Hoare tested his writing on his own children. “My two girls are adopted and they grew up in totally bookless households for five years. I’ve watched them discover books and reading fairly late compared to their peers. They were reluctant readers so they would’ve found a traditional encyclopedia a turn-off.” Hoare says he tries to add “whimsy, rhyme, alliteration, quirkiness, things that make you smile” to a conventional encyclopedia-type format. “I’ve found that’s what my children really relate to.” There is one type of environmental non-fiction Hoare is not a fan of: “Nature activity books that explain how to roll down a hill, look at a sunset or make things out of sticks. Grandparents and parents buy these books because they want to tackle the dreaded screens but I doubt these books get opened. I’m not sure children need to read about these things – just let them go outdoors and discover nature.” He wonders whether fiction is a more powerful vehicle for raising environmental awareness. His own formative childhood reading included Watership Down, Animals of Farthing Wood and Beatrix Potter, who was never “cutesy”. His daughters have been inspired by Enid Blyton’s The Faraway Tree. “Blyton is laughably dated but this idea of a magical tree really had an impact on my girls. It transformed our walks in the woods. We passed a gnarly oak tree and my daughter said, ‘Look, Dad, there’s the faraway tree’. Fiction is quite a stealthy way of talking to children about big themes.” Lucy Mangan, the author of Bookworm: A Memoir of Childhood Reading, agrees. “Very few children up to the age of 11 can or will read straight non-fiction,” she says. “For the young brain, stories are the way to attract attention and get any message across.” Mangan fears that the moral instruction in environmental non-fiction is too obvious, not least because “there is only one stance that 99% of scientists and writers would want to take so there’s not much room for doing anything with it”. She says: “Children’s literature began with a desire to instruct but if it had stuck to its guns we wouldn’t still be reading it.” The “golden age” of children’s literature only arrived when authors such as J M Barrie to Edith Nesbit cast off Victorian moralising and wrote from a children’s point of view. “Moralising doesn’t get you very far,” argues Mangan. Environmental fiction for children aged 10 and above and dystopian novels for young adults are also proliferating in this age of anxiety. At BookTrust, Coleman has particularly enjoyed The Dog Runner by Australian author Bren MacDibble (who lost her home in a wildfire) and Run Wild by Gill Lewis, a vet who tackles complex wildlife themes in her books, from the persecution of birds of prey as well as bear-bile farming and endangered gorillas. Can this outpouring of environmentalism capture children’s imagination without terrifying them? I test some books on my eight-year-olds. We read Pankhurst’s worthy-sounding Fantastically Great Women Who Saved The Planet. To my surprise, Milly and Esme love it, and I do too: it is funny and interesting, telling stories not just of westerners such as Jane Goodall but also Wangari Maathai, from Kenya, and Isatou Ceesay, from the Gambia. A few days later, Milly tells me they’ve discussed Pankhurst’s story of the Chipko movement in the Indian Himalayas with their teacher. “In the Chipko movement it starts off with one person and then they all protect a whole range of trees from being cut down,” says Milly. “Even a tiny difference makes something big.” A few nights later, Milly announces she has read Hoare’s hefty Anthology of Intriguing Animals in one evening. She adores the pages edged in gold. “You’ll learn hundreds of lovely new things from that book,” she says. Hoare aside, I’m still not convinced by the clunky writing in some of the new eco non-fiction. It seems unfair to pick on one but in Miranda Krestovnikoff’s The Sea, for instance, we learn that kelp, for instance, can grow 30 metres tall. How high is that in real life? How do children relate to that? But my views don’t matter. What do the children think of The Sea? After Esme’s tears over ocean plastics, she consoles herself with its stories of otters. “Everyone should try this book,” she says. “If you read the start you think, ‘meh’ but you’ll learn a lot. It’s really good.”"
"

It has been four years since President Bush declared the Kyoto Protocol a dead letter, but the campaign to impose industrial greenhouse‐​gas emission controls on the American economy shows no signs of letting up. Although Sen. John McCain’s bid last month to include concrete emission controls in the pending energy bill attracted only 38 votes, the Senate subsequently passed a resolution calling on Congress to “enact a comprehensive and effective national program of mandatory, market‐​based limits on emissions of greenhouse gases that slow, stop and reverse the growth of such emissions.” An attempt to table the resolution was opposed by Republican senators Lamar Alexander, Lincoln Chafee, Susan Collins, Mike DeWine, Pete Domenici, Lindsey Graham, Judd Gregg, Richard Lugar, John McCain, Olympia Snowe, Arlen Specter, and John Warner. Apparently, even red‐​state Republicans are having doubts about Bush’s position on climate change.



One might think that the increased political buzz around global warming is driven by science. One would be wrong. The scientific case for alarm is no more compelling today than it was yesterday. 



The first (and sometimes last) stop in the global‐​warming debate is the question, Is it real? The answer seems to be yes. Ground‐​based and oceanic temperature records show warming of about three‐​quarters of a degree Celsius in the last century. About half of that warming, however, occurred before World War II and is widely thought to be related to solar activity. Satellite and weather‐​balloon records, which do not go back as far, show less warming in the late 20th century than the land‐​based stations.



What’s causing this warming? We don’t know. As the vice president of the U.N.‘s Intergovernmental Panel on Climate Change (IPCC), Yury Izrael, wrote bluntly last month, “There is no proven link between human activity and global warming.” Given the extreme variability of global temperature, warming might simply be statistical noise. It might result from solar and/​or volcanic activity. It might be caused by industrial emissions. And it might come from some combination of the three.



What do most scientists suspect is going on? The best way to ascertain the “scientific consensus” is to look at the latest report of the IPCC (released in 2001), which purports to summarize the state of scientific knowledge on global warming. Here’s what it says: “Most of the observed warming over the last 50 years is likely to have been due to the increase in greenhouse gas concentrations.” The report finds that it is “unlikely (bordering on very unlikely) to be entirely the result of internal variability,” and that “natural forcing alone [i.e., solar and/​or volcanic activity] is unlikely to explain the increased rate of global warming since the middle of the 20th century.”



The promiscuous use of such vague terms as “likely” and “unlikely” by scientists who are trained in precision speaks volumes about how much is unknown. At the very least, such language makes it impossible to accept the Greens’ claim that “the debate is over,” particularly given all the uncertainty — fully discussed in the IPCC report — regarding long‐​term climate records and important data on atmospheric feedbacks. In fact, uncertainty about future climate conditions is greater in the 2001 IPCC report than it was in the 1995 IPCC report.



Do other reviews of the scientific literature tell a different story? It depends on whom you ask. An article by Naomi Oreskes in Science last December examined 1,000 scientific papers published since the early 1990s. Oreskes concluded that 75 percent of those papers either directly or implicitly supported the argument that industrial emissions are driving global warming, and none directly argued to the contrary. A subsequent review of the same articles by Benny Peiser, a senior lecturer on the science faculty at Liverpool John Moores University, found nothing of the kind. Peiser concluded that only one‐​third of the papers reviewed by Oreskes actually supported the “consensus view,” and only 1 percent did so explicitly.



In any case, debating what constitutes the mainstream thinking on climate change is not particularly enlightening. Regardless of how one defines “the consensus,” scientific truth is not revealed by a show of hands. As Thomas Kuhn demonstrated in The Structure of Scientific Revolutions, the history of scientific progress is a history of once‐​solid consensuses being overthrown by minority skeptics. In short, today’s consensus proves nothing.



Even more heated than the debate about the cause of climate change is the debate about its likely effects. In fact, most of the so‐​called skeptics who publish in peer‐​reviewed literature accept the contention that mankind is probably responsible for most present‐​day warming. They argue, however, that the warming has been and will continue to be quite modest, and that the pattern of warming we’re seeing does not suggest that a parade of horribles awaits us.



The skeptics are on solid ground here because the atmosphere simply has not proven to be as sensitive to industrial greenhouse‐​gas emissions as some theorists have feared. Unless some temporary phenomenon is masking the effect of such emissions, atmospheric physics suggests that warming will occur at a linear rate — a conclusion affirmed by almost all the computer climate models in existence. This insight suggests a simple exercise: Plot temperature data over the last 50 years and draw a trend line to see what the future has in store. Doing so suggests that warming will likely be at the low end of the IPCC’s projections — about 1.5 degrees Celsius by the year 2100.



Should we worry about such modest warming? From an ecological perspective, probably not. Because water vapor is responsible for 94 percent of the natural greenhouse effect, industrial greenhouse gases have a greater impact in dry air masses than in wet ones. Fully 78 percent of the warming has been concentrated in the driest air masses, which are primarily found during the winter (when 69 percent of the warming has occurred), at night, and in the northern latitudes.



The fact that winter nighttime lows in the Northern Hemisphere aren’t quite as cold as they used to be need not cause anyone to panic — and there seems not to be an increased incidence of the destructive weather events that would follow from warming in wet air masses. According to the IPCC, “[T]here is little sign of long‐​term changes in tropical storm intensity and frequency,” and “no compelling evidence” that local severe‐​weather events are on the rise. Most important, “no significant acceleration in the rate of sea level rise during the 20th century has been detected.” Precipitation in the northern hemisphere has likely increased by a meager 0.5–1 percent a decade, but “no comparable systematic changes in precipitation have been detected in broad latitudinal averages over the Southern Hemisphere.”



There are good reasons to think that a warmer world might be a better world. Agronomists, for instance, are fairly convinced that heavier concentrations of carbon dioxide in the atmosphere, as well as the longer and somewhat wetter growing seasons that follow from the greenhouse warming pattern, have already increased crop yields and will continue to do so. Warmer weather also leads to declines in energy use, and probably fewer weather‐​related deaths.



Not surprisingly, economists who have examined the implications of the warming projections offered by the IPCC have had a hard time proving the existence of net negative effects. In fact, Yale forestry economist Robert Mendelsohn has demonstrated that nations north of the equator will probably benefit from global warming, and that warming will likely prove an economic wash for the world as a whole.



Both sides in the global‐​warming debate contend that “sound science” should dictate public policy. For the foreseeable future, though, it’s unlikely that scientists will be able absolutely to prove or disprove the proposition that industrial greenhouse‐​gas emissions are ushering in a dangerous warming trend. Even if scientists could prove this, they have no particular expertise at choosing among competing policy responses. Nor are scientists’ levels of risk aversion, or their choices about how to hedge against risk, necessarily superior to those of anyone else.



Scientists cannot tell us how best to handle the threat posed by global warming, no matter how much we, or they, wish otherwise. They can help inform the debate — but they have less to contribute than most people think.
"
"
Share this...FacebookTwitterSkepticism of junk climate science has taken root even in Germany. Already the 4th International International Climate And Energy Conference is taking place in Munich on November 25-26, 2011.
Leading scientists, experts and critics of the AGW science and man-made climate change are gathering to present the newest results. Mark it down on your calendar!
New speakers this year will include:
– Andrew Montford
– Werner Kirstein
– Henrik Svensmark
– Chris Horner
– Piers Corbyn
They’ll be joining an impressive line-up of speakers like Nir Shaviv and Jan Veizer. Last year I attended the conference in Berlin and I hope to attend this year too.
This year’s line up of speakers looks even better, and the program (still preliminary and likely to change some) has been expanded to 2 full days. Read more at EIKE. Here’s how the program looks right now:
Friday – November 25
8:00 a.m.
Registration
9:00 a.m.
* Welcome – Why do we still deal with climate change?
Dr. Holger Thuss, European Institute for Climate and Energy (EIKE)
Wolfgang Müller, Berlin Manhattan Institute (BMI), European Institute for Climate and Energy (EIKE)
9:30 a.m.- 12:30 p.m.
Panel: Measuring vs. modelling
* Real temperature measurements vs. climate alarmism
Prof. em. Dr. Horst-Joachim Lüdecke Hochschule für Technik und Wirtschaft des Saarlandes
* Glaciers as climate witnesses
Prof. em. Dr. Gernot Patzelt, University of Insbruck
* Anthropogenic sea level rise: from scenario to panic
Dipl. Meteorologe Klaus-Eckart Puls, Press spokesperson, EIKE
11:30 a.m. – 12.00 a.m. 
Break
– Mission impossible – geological facts of carbon capture and storage in Germany
Prof. em. Dr. Friedrich-Karl Ewert, University of Paderborn
12:30 p.m. – 2:00 p.m. 
Lunch – conference venue
2:00 p.m. – 4:00 p.m.
Panel: Cosmic rays, CO2 and climate
* Climate, water, CO2 and the sun
Prof. Dr. Jan Veizer, Department of Earth Sciences, University of Ottawa
* The cosmic ray climate link – evidence and implications to the understanding of
climate change
Prof. Dr. Nir Shaviv, Racah Institute of Physics – The Hebrew University of Jerusalem
* The impact of solar activities and cosmic rays on the world climate
Prof. Dr. Henrik Svensmark, Centre for Sun-Climate Research of the Danish National Space Centre
4:00 p.m. – 4:30 p.m.
Break
4:30 p.m. – 5:30 p.m.
Panel: Update on the CERN study cosmic rays and climate change
Prof. Dr. Jan Veizer, Prof. Dr. Nir Shaviv, Prof. Dr. Henrik Svensmark
5:30 p.m. – 6:30 p.m.
Climategate – The story of a cover-up
Andrew Montford, Bishop Hill Blog
Followed by dinner – buffet


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Saturday – November 26
9:00 a.m. – 11:00 a.m.
Panel: Forecasts vs. scenarios
* Climate change between statistics, models and substitute religion
Prof. Dr. Werner Kirstein, Institute for Geography, University of Leipzig
* Accurate long term weather forecasts are possible
Piers Corbyn, Weather Action, London
11:00 a.m. – 11:30 a.m.
Break
11:30 a.m. – 12:15 p.m.
* The urban legend of the Hockey Stick
Andrew Montford, Bishop Hill Blog
12:15 p.m. – 13:00 p.m.
* Investing wisely– opportunities and dangers in alternative energy
(tbc)
13:00 p.m. – 2:30 p.m.
Lunch – at conference venue
2:30 p.m. – 4:00 p.m.
Panel: Climate- and Energy Policy – Wish and Reality
* The green economy: Crony capitalism’s newest big idea
Dr. Christopher C. Horner
Center for Energy and Environment – Competitive Enterprise Institute, Washington, DC
* The costs of Germany’s green energy agenda – plan vs. reality
Prof. Dr. Gerd Ganteför, University of Konstanz
4:00 p.m. – 4:30 p.m.
Break
4:30 p.m. – 5:30 p.m.
* Covering their tracks: the IPCC and transparency
Dr. Christopher C. Horner
* Center for Energy and Environment – Competitive Enterprise Institute, Washington, DC
5:30 p.m. – 6:30 p.m.
* Climate policies – a threat to liberty
Prof. Dr. Gerd Habermann; University of Potsdam, Hayek Society
6:30 p.m. – 7:00 p.m.
* Closing remarks
Dr. Holger Thuss, President Of The European Institute for Climate and Energy (EIKE)
7:00 p.m.
Reception / end of conference
Prices for admission
€80.00 for 1 day, private individuals
€140.00 private individuals for 2 days
€290.00 person for companies
Prices already include VAT.
Also included are the documentation, 3 meals and 4 coffee breaks/day. Note this is only the preliminary plan and is subject to change!
Registration form will be made available soon.
More info here at EIKE.
Share this...FacebookTwitter "
nan
"
By Bill Steigerwald

“Abraham, Martin & Grandpa”
WASHINGTON, D.C.
Grandpa was afraid their special glasses weren’t working properly and the police officers could tell they were polar bears, so he yelled from the sidewalk.
“Hello, officer. We’re visiting from out of town. Do you know if there’s a Wal-Mart nearby that’s open all night?’
“What do you want with a Wal-Mart?” the suspicious policeman responded, moving his hand to the handle of his service pistol. “They’re illegal in this city.”
“It’s his favorite store,” Junior piped up. “He doesn’t even care if the toys are made in China.”
“We’re just looking to ‘save money and live better,’” Mother said, smiling as innocently as she could. Her big white teeth and eyeglasses reflected the shine of the policeman’s light.
The policeman stared at the bears for what seemed like forever. “Move along,” he finally said as he switched off the spotlight and slowly drove away.
“Man,” the policeman said to his sleepy partner in the front seat, “Didn’t she remind you of that Sarah Palin woman?”
“Nah,” his partner said. “Too big and hairy. They must be from Russia or something.”
Free from the watchful eyes of the DC police, the bears continued to their next stop, the venerable Lincoln Memorial. Through the locked entrance doors the bears could see the statue of Abraham Lincoln sitting in his gigantic stone chair.
Standing on the memorial’s marble steps in the dawn’s early light, the three bears admired the Washington Monument and its reflecting pool.
“These steps are where Martin Luther King gave his ‘I Have a Dream’ speech in 1963,” Grandpa said to Junior. “See that tall white dome with the point on it?” he said, pointing to a building behind the Washington Monument.
“That’s the U.S. Capitol Building. It’s where the country’s laws are made, for better or for worse. I’ll be there tomorrow fighting for the freedom of all the world’s polar bears.”
“Aren’t you scared, Grandpa?” Junior asked.
“Not any more,” Grandpa said. “I know I’m fighting for what’s right. Plus, I have Mr. Jefferson, Mr. Lincoln and the Rev. King on my side.”
When the three bears arrived at the U.S. Capitol Building at 9 a.m., a wild scene awaited them on the West Lawn.
Thousands of people had come to demonstrate their love for polar bears and their support for a new law to place them on the Endangered Species list. Everywhere the three real bears looked they saw cute and cuddly make-believe polar bears.
High above them were two polar bear hot-air balloons and blowing along the snowy ground were a dozen inflatable bears eight feet tall and topped with Santa Claus hats. Scores of humans danced in cheap polar bear costumes and wore “Stop Global Warming” T-shirts or waved hand-made signs that read “Polar Bear SOS!” or “Save Me.”
Hundreds of public school children brought in by bus were chanting “Save our polar bears” and selling plastic polar bear figurines to raise money to fight against drilling for oil in the Arctic.
In a row of carnival booths surrounding the Capitol Building Christmas tree, a lot of money was being made by the many environmentalist groups. The Natural Resource Defense Council was seeking donations for its “Polar Bear S.O.S.” campaign. The World Wildlife Foundation offered symbolic polar bear adoption kits for $250.
Grandpa, Mother and Junior made their way through the crowd in their human costumes. Thanks to their special eyeglasses, no one realized they were polar bears. When they reached the bottom of the Capitol Building’s steps, Grandpa turned and surveyed the crazy scene.
“Saving polar bears is big business. Too bad we don’t get any royalties,” he said with a laugh. Then Grandpa pushed his eyeglasses to the back of his nose, unbuttoned his suit coat and started up the steep steps of the Capitol Building. “Come on, kids. Let’s roll.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f3269cb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The number of daily deaths from Covid-19 in Scotland's second wave has begun to go down. Hospital admissions are also declining, as are infection rates in the worst affected areas. Has Scotland's second wave peaked, or is there still danger ahead? Here are five numbers to watch.**
Across the whole of Scotland, the average number of new cases every day is no longer rising.
The figure appears to have peaked towards the end of October and has mainly been going down since then.
However, there are still significant areas of Scotland where cases are on the rise and this remains a concern for the Scottish government as it attempts to slow the spread of the virus.
On 20 November, the 11 local authorities with the highest infection rates were moved to level four, which has the toughest set of restrictions under Scotland's five-tier system.
The three worst-affected local authorities in level four at the end of last week were Glasgow City, North Lanarkshire, and Renfrewshire.
The infection rate in North Lanarkshire has mainly been declining since mid-October. The rate also now appears to be going down in Glasgow after several weeks at ""stubbornly high"" levels - as ministers have repeatedly noted.
However, there's no such decline evident in Renfrewshire, which has seen fluctuating rates between 200 and 300 cases per 100,000 since early October.
It's worth nothing that South Lanarkshire, which reached almost 400 cases per 100,000 people in October, has shown a sustained decline in infections since then.
But it's not just the local authorities with the highest rates that cause concern.
Anywhere which shows a big increase in cases over a short period will catch the attention of public health officials, who want to stop the virus becoming seated in a new community.
Two weeks ago the focus was on Angus, Fife and Perth and Kinross, which all moved from level two to level three restrictions after a sharp rise in cases.
Despite a dip in the number of cases in Fife, it's too soon to tell if the rate will show a sustained decline here - and in Perth and Kinross the rate is still very much on the rise.
First Minister Nicola Sturgeon also highlighted significant rises in Stirling and Inverclyde.
The rate in Inverclyde is now declining, but it's still going up in Stirling - as it is in neighbouring Clackmannanshire, which up until October had seen some of the lowest infection rates across the central belt.
Infection rates are not the only indicator the Scottish government uses to review levels, but they are a key measure to watch.
The daily number of new confirmed cases of Covid-19 is an important statistic - but it is difficult to make a comparison between now and March using this figure as the number of people being tested has risen so significantly.
One number that is arguably a better gauge of where Scotland is in its second wave is hospital admissions.
The daily admissions for Covid-19 in Scotland hit a low in mid-July.
They then started a slow rise which gathered pace into the autumn, but for the last two weeks the average number of admissions has mainly been going down.
If this decline is sustained, the peak will have been significantly lower than in the spring.
The sickest Covid-19 patients are likely to end up in intensive care, where the mortality rate is high.
A report by Public Health Scotland published in July found that almost 40% of coronavirus patients died within 30 days of being admitted to intensive care.
The number of daily ICU admissions began to rise at the end of the summer - there were six Covid-19 patients in intensive care on 11 September and there were 111 on 8 November.
Numbers have been fluctuating in the last few weeks, but there doesn't yet appear to be the same decline in ICU admissions seen in hospital admissions.
The Scottish government announces daily figures of deaths within 28 days of a positive test for Covid-19.
Scotland went 40 days over the summer with no deaths recorded at all using this measure.
The average number of new Covid-19 deaths being registered each day rose steadily from mid-September, but has now begun to decline.
The National Records of Scotland counts all death certificates that mention Covid-19, even if the person has not been tested for the virus.
A similar pattern is evident with this measure, with the first decline in weekly deaths since the beginning of October recorded last week.
It appears that daily hospital admissions have peaked at a much lower rate than they did in the spring, so why is that when the number of new cases has undoubtedly surged in the past two months?
One reason could be that fewer over-65s are being infected now than they were in the early stages of Scotland's pandemic in the spring.
Younger age groups are much less likely to end up in hospital, or die, from Covid-19.
This next chart shows that there were more infections among younger age groups at the start of Scotland's second wave, with that steep rise in mid-September driven by outbreaks in student accommodation.
Infection rates among older age groups began to rise in September and October, but now appear to have stabilised.
So has Scotland's second wave peaked?
The decline in both death rates and hospital admissions is encouraging, but there are significant risks ahead.
Local outbreaks are still occurring and health officials will be very aware that the relaxation of restrictions over Christmas will likely cause an increase in infections.
The Scottish government and NHS Scotland will be working to drive cases as low as possible before Christmas to avoid that increase turning into a new surge."
"No longer should our survival be an afterthought. If we are to withstand the climate crisis, every decision should begin with the question of what the planet can endure. This means that any discussion about new infrastructure should begin with ecological constraints. The figures are stark. A paper published in Nature last year showed that existing energy infrastructure, if it is allowed to run to the end of its natural life, will produce around 660 gigatonnes of CO2. Yet, to stand a reasonable chance of preventing more than 1.5°C of global heating, we can afford to release, in total, no more than 580 gigatonnes. In other words, far from building new fossil power plants, the survival of a habitable planet means retiring the damaging projects that have already been built. Electricity plants burning coal and gas and oil will not secure our prosperity. They will destroy it. But everywhere special interests dominate. Construction projects are driven, above all, by the lobbying of the construction industry, consultancies and financiers. Gigantic and destructive schemes, such as the Oxford-Cambridge Expressway, are invented by lobbyists for the purpose of generating contracts. Political support is drummed up, and the project achieves its own momentum; then, belatedly, a feeble attempt is made to demonstrate that it can somehow become compatible with environmental promises. This is what destroys civilisations: a mismatch between the greed of economic elites and the needs of society. But last week something momentous happened. The decision to build a scheme with vast financial backing and terrible environmental impacts was struck down by the court of appeal. The judges decided that government policy, on which planning permission for a third runway at Heathrow was based, had failed to take account of the UK’s climate commitments, and was therefore unlawful. This is – or should be – the end of business as usual. The Heathrow decision stands as a massive and crucial precedent. Now we must use it to insist that governments everywhere put our survival first, and the demands of corporate lobbyists last. To this end, with the Good Law Project and Dale Vince, the founder of Ecotricity, I’m pursuing a similar claim. In this case, we are challenging the UK government’s policy for approving new energy projects. On Tuesday, we delivered a “letter before action” to the Treasury solicitor. We’ve given the government 21 days to accept our case and change its policy to reflect the climate commitments agreed by parliament. If it fails to do so, we shall issue proceedings in the high court to have the policy declared unlawful. We’ll need money, so we’ve launched a crowdfunding appeal to finance the action. It’s hard to see how the government could resist our case. The Heathrow judgment hung on the government’s national policy statement on airports. This, the judges found, had not been updated to take account of the Paris climate agreement. New fossil fuel plants, such as the gas burners at Drax in Yorkshire the government approved last October, are enabled by something very similar: the national policy statements on energy infrastructure. These have not been updated since they were published in 2011. As a result, they take no account of the Paris agreement, of the government’s new climate target (net zero by 2050, as opposed to an 80% cut) or of parliament’s declaration of a climate emergency. The main policy statement says that the EU Emissions Trading System “forms the cornerstone of UK action to reduce greenhouse gas emissions from the power sector”. As we have left the EU, this obviously no longer holds. The planning act obliges the government to review its national policy statements when circumstances change. It has failed to do so. It is disregarding its own laws. Once a national policy statement has been published, there is little objectors can do to prevent damaging projects from going ahead, as the statements create a presumption in favour of new fossil fuel plants. In approving the Drax plant, Andrea Leadsom, the secretary of state for business and energy at the time, insisted that the policy statement came first, regardless of the climate impacts. Catastrophic decisions like this will continue to be made until the statements change. They are incompatible with either the government’s new climate commitments or a habitable planet. While we are challenging the government’s energy policies, another group – the Transport Action Network – is about to challenge its road-building schemes on the same basis. It points out that the national policy statement on road networks is also outdated and incompatible with the UK’s climate commitments. The policy statement, astonishingly, insists that “any increase in carbon emissions is not a reason to refuse development consent”, unless the increase is so great that the road would prevent the government from meeting its national targets. No single road project can be disqualified on these grounds. But the cumulative effect of new road-building ensures that the UK will inevitably bust its carbon targets. While carbon emissions are officially disregarded, minuscule time savings on travel are used to justify massive and damaging projects. Transport emissions have been rising for the past five years, partly because of road-building. The government tries to justify its schemes by claiming that cars will use less fossil fuel. But because they are becoming bigger and heavier, new cars sold in the UK now produce more carbon dioxide per kilometre than older models. The perverse and outdated national policy statement locks into place such damaging projects as the A303 works around Stonehenge, the A27 Arundel scheme, the Lower Thames crossing, the Port of Liverpool access road, the Silvertown tunnel in London and the Wensum link road in Norfolk. A government seeking to protect the lives of current and future generations would immediately strike down the policy that supports these projects, and replace it with one that emphasised walking, cycling and public transport. A third action has been launched by Chris Packham and the law firm Leigh Day, challenging HS2 on similar grounds. Its carbon emissions were not properly taken into account, and its environmental impacts were assessed before the government signed the Paris agreement. Already, the Heathrow decision is resonating around the world. Now we need to drive its implications home, by suing for survival. If we can oblige governments to resist the demands of corporate lobbyists and put life before profit, humanity might just stand a chance. • George Monbiot is a Guardian columnist"
"
For those that don’t read a lot of the WUWT comments closely, there has been a scholarly argument going on between  Tom P of the UK and several WUWT commentators over the methodology Steve McIntyre used to illustrate the “breathtaking difference” between the plot of  the hand picked set of 12 Yamal trees and the larger Schweingruber tree ring data set also from Yamal. Tom P. reworked Steve’s R-code script (which he posted on WUWT) to include both the 12 excluded and the Schweingruber and  thought he found “insensitivity to additional data”, saying “There is no broken hockeystick”.
Jeff Id audited the auditor of an auditor and found that Steve’s work still holds up “robustly”. – Anthony

Jeff Id writes on The Air Vent
Just a short post tonight I hope. Tom P, an apparent believer in the hockey stick methods posted an entertaining reply to Steve McIntyre’s recent discoveries on Yamal. He used R code to demonstrate a flaw in SteveM’s method. His post was on WUWT, brought to my attention by Charles the moderator and is copied here where he declares victory over Steve.

Tom P writes on WUWT:
===========
Steve McIntryre’s [sic] reconstructions above are based on adding an established dataset, the Schweingruber Yamal sample instead of the “12 trees used in the CRU archive”. Steve has given no justification for removing these 12 trees. In fact they probably predate Briffa’s CRU analysis, being in the original Russian dataset established by Hantemirov and Shiyatov in 2002.
One of Steve’s major complaint about the CRU dataset was that it used few recent trees, hence the need to add the Schweingruber series. It was therefore rather strange that towards the end of the reconstruction the 12 living trees were excluded only to be replaced by 9 trees with earlier end dates.
I asked Steve what the chronology would look like if these twelve trees were merged back in, but no plot was forthcoming. So I downloaded R, his favoured statistical package, and tweaked Steve’s published code to include the twelve trees back in myself. Below is the chronology I posted on ClimateAudit a few hours ago.
TomP' s plot. Click to enlarge Source: http://img80.yfrog.com/img80/1808/schweingruberandcrud.png
The red line is the RCS chronology calculated from the CRU archive; black is the chronology calculated using the Schweingruber Yamal sample and the complete CRU archive. Both plots are smoothed with 21-year gaussian, as before. The y-axis is in dimensionless chronology units centered on 1.
It looks like the Yamal reconstruction published by Briffa is rather insensitive to the inclusion of the additional data. There is no broken hockeystick.

=============
Jeff Id writes:
He did a fantastic job in reworking R code to create an improved hockey stick graph.  To see his code the link is here.
.




Jeff Id’s version of TomP’s graph – Click to expand



I spent some time tonight looking at his results. Time planned for analyzing Antarctic sea ice. I found that essentially the only difference in the operating functions of the code is the following line.
.
Steve M  —- tree=rbind(yamal[!temp,],russ035)
Tom P —– tree=rbind(yamal,russ035)
.
 
The !temp in Steve’s line removes 12 series of Yamal for the average while Tom’s version includes it. I’m all for inclusion of all data, but I am a firm believer that Briffa’s data is probably a cherry picked set of trees to match temp or something. Therefore by inclusion of the sorted Briffa Yamal version, we have an automatic exclusion of data which would otherwise balance the huge trend. However, this is not the problem with Tom’s result. The problem lies in this plot, also created by Tom P’s code.
Tom P’s Yamal Reconstruction – Count per Year.  Click to Expand

Here is the zoomed in version:

Above we can see that everything in TomP’s curve after 1990 is actually 100%  Briffa Yamal data.
So the question becomes – What does the series look like if the Yamal data doesn’t create the ridiculous spike at the end the curve?
I truncated the black line at 1990 below.


The black line is truncated at the end of the Schweingruber data and it looks pretty similar to the graph presented in the green line by Steve McIntyre again below.

Don’t be too hard on Tom P, he honestly did a great job and took the time to work with the R script which is more than most are willing to. Steve is a very careful worker though and it’s damn near impossible to catch him making mistakes. Trust a serious skeptic, it’s not easy to find mistakes in his work and some of us check him just as I spent over an hour checking Tom’s work. In my opinion Tom deserves congratulations for his efforts and checking, this way we all learn.
I’ve now been all the way through SteveM’s scripts from beginning to end and can’t find any problems with the script, maybe others can!

Steve McIntyre adds in WUWT comments :
 Steve McIntyre (21:35:13)
Here is some conclusive evidence in respect to the following misrepresentation by Tom:
Steve McIntyre said they may well have been just the most recent part of Hantemirov and Shiyatov’s dataset and no selection would have been made.
In my first post in this sequence http://www.climateaudit.org/?p=7142, I identified a common pattern to the IDs for cores and observed:
There are 252 distinct series in the CRU archive. There are 12 IDs consisting of a 3-letter prefix, a 2-digit tree # and 1-digit core#. All 12 end in 1988 or later and presumably come from the living tree samples. The nomenclature of these core IDs url (POR01…POR11; YAD04…YAD12; JAH14…JAH16 – excluding the last digit of the ID here as it is a core #) suggests to me that there were at least 11 POR cores, 12 YAD cores and 16 JAH cores.
It is “possible” that they skipped ID numbers, but this is a farfetched theory even for Tom. As surmised here, the missing ID numbers are “evidence” of at least 39 cores and that the present archive is not only too small, but incomplete.
=========
and also this comment:
 Steve McIntyre (20:13:22)
I am online too much, but I am not online 24/7. I’ve been out playing squash. Surely I’m allowed to be offline occasionally without a poster commenting adversely on this.
While I was out, CA crashed as well.  Thus, it was “quiet.
Contrary to Tom’s speculations and misrepresentation of my statements, it is my opinion that there is considerable evidence that the 12 cores are not a complete population i.e. that they have been picked form a larger population. Rather than quote form actual text, Tom puts the following words in my mouth that I did not say:
Steve McIntyre said they may well have been just the most recent part of Hantemirov and Shiyatov’s dataset and no selection would have been made.
This is not my view.
The balance of Tom’s argument is:
No, they are the twelve most recent cores. There’s been no evidence provided to suggest they are in any way suspect. ..There is no obvious reason to exclude them.
I disagree. I do not believe that they constitute a complete population of recent cores. As a result, I believe that the archive is suspect. There is every reason to exclude them in order to carry out a sensitivity as I did. The sensitivity study showed very different results. I do not suggest that the sensitivity run be used as an alternative temperature history. Right now, there are far too many questions attached to this data set to propose any solution to the sampling conundrum. It’s only been a couple of days since the lamentable size of the CRU sample became known and it will take a little more time yet to assess things.
Reasons why I “suspect” that a selection was made from a larger population include the following. A field dendro could take 12 cores in an hour. We took a lot more than that at Mt Allegre and a field dendro could be far more efficient. Thus, it seems very unlikely that the entire population of cores from the Yamal program is only 12 cores and on this basis, it is my surmise that a selection was taken from the cores. Standard dendro procedures use all crossdated cores and definitely use more than 10 cores if they are available.
This doesn’t “prove” that a selection was made, but it is reasonable to “suspect” that a selection was made and to ask CRU and their Russian associates to provide a clear statement of their protocols. There’s no urgency to do anything prior to receiving a statement of their sampling protocols. For this purpose, it doesn’t matter a whit whether the selection was made by the Russians or at CRU or a combination. In my first post on this matter – which Tom appears not to have read, I canvass the limited evidence for and against. There is certainly evidence supporting the idea that the 12 cores were among 17 selected by the Russians, but in other parts of the data set, the CRU population is larger than that used in the Hantemirov and Shiyatov chronology. The construction of the CRU data set is not described in any literature; the description in Hantemirov and Shiyatov has something to do with it, but doesn’t yield the CRU data data set. Some sort of reconciliation is required.
In addition, the age distribution of the CRU 12 is very different than the age distribution from the nearby Schweingruber population. In my opinion, the uniformly high age of the CRU12 relative to the Schweingruber population is suggestive of selection – in this respect, perhaps and even probably by the Russians. Again this isnt proof. Maybe they were just lucky 12 straight times and, unlike Schweingruber, they got very long-lived trees with every core. Without documentaiton, no one knows. In any event, this doesn’t help the Briffa situation. If these things are temperature proxies, the results from two different nearby populations should not be so different and protocols need to be established for ensuring that the age distribution of the modern sample is relatively homogeneous with the subfossil samples (and they aren’t.)
The prevailing dendro view is that an RCS chronology requires a much larger population than a “conventional” standardization. Thus, even if the data set had been winnowed down to 10 cores in 1990 and 5 cores at the end, this is an absurdly low population for modern cores, which are relatively easily obtained. Use of such small replication is inconsistent with Briffa’s own methodological statements.
Tom also misses a hugely important context. There is a nearby site (Polar Urals) with an ample supply of modern core. Indeed, at one time, Briffa used Polar Urals to represent this region. My original question was whether there was a valid reason for substituting Yamal for Polar Urals. The microscopic size of the modern record suggests that there was not a valid reason. However, this tiny sample size was not known to third parties until recently due to Briffa’s withholding of data, not just from me, but also to D’Arrigo, Wilson et al.
Until details of the Yamal selection process are known, my sense right now is that one cannot blindly assume – as Tom does – that what we see is a population. Maybe this will prove to be the case, but personally I rather doubt it. A better approach is to use the Polar Urals data set as a building block.
As to Tom’s argument that none of this “matters”, the Yamal data set has a bristlecone-like function in a number of reconstructions. While the differences between the versions may not seem like a lot to Tom, as someone with considerable experience with this data, it is my opinion that the revisions will have a material impact on the medieval-modern difference in the multiproxy studies that do not depend on strip bark bristlecones.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92c33482',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Within ten years, the US will phase out fossil fuels and source its energy from 100% renewable sources. That’s what a letter, signed by over 600 people and sent to Congress on January 10 2019, proposes and demands.  The signatories also call for making every building in the US energy efficient and eliminating greenhouse gas emissions from manufacturing, agriculture and transport. Achieving all these targets within the next 25 years would be very ambitious. Within the next ten years requires not just a green transition but a green revolution. The signatories know that, which is why they propose that the federal state should lead this revolution and become the primary funder of this Green New Deal. But how exactly would that work and what would it mean for the economy? The Green New Deal demands the polar opposite of austerity from government. Instead of fiscal frugality, the state would have to pump large sums of money into society and the economy in an effort to overhaul everything. This follows the ideas of Modern Monetary Theory, which sees government debt as normal and possibly even desirable as long as it helps foster an economy that benefits the common good and keeps people economically active.  


      Read more:
      Explainer: what is modern monetary theory?


 Governments and central banks would use state expenditure to keep unemployment low and to subsidise key decarbonisation activities, such as creating  electrified mass transport and making buildings energy efficient, even though this would lead to a noticeable increase in national debt. The government would also have to increase taxes, especially on wealth, capital gains and corporations. In addition to being the main funder of the transition, the state would also act as an employer of last resort, since the Green New Deal promises employment for everybody. This is a huge commitment, considering that there would be significant job losses in fossil fuel dependent industries. It won’t just be coal workers being made redundant – many workers in the car industry, steel manufacturing, large-scale agriculture and food processing would lose their jobs, too. While it’s true that many new jobs will be created as part of the green transition, two issues remain. Can workers be retrained quickly enough to take on these new jobs or are the skills required simply too different? Will new green jobs be sustainable, or will there just be a green boom during the transition followed by a harsh awakening and growing unemployment? In the short and medium term, full employment seems actually quite unrealistic, unless the state forces people to work in jobs they don’t want. Instead, a Green New Deal should prioritise the introduction of a universal basic income to give people the freedom to refuse poorly paid work with bad working conditions. The idea of a universal basic income is simple. Instead of means-tested social welfare payments for people who are outside of paid employment, the state would pay every member of society a monthly basic income allowance. This money would come from absorbing some existing means-tested welfare streams and taxing the new hubs of economic activity, such as green technology manufacturing. In an economy where not everybody might be able to get suitable employment, this basic income allowance would cover essential living costs and allow people to pursue meaningful activities outside of work.  Introducing a universal basic income would have advantages for delivering a Green New Deal. If the radical changes of the Green New Deal aren’t supposed to punish workers in the current fossil-fuel dependent economy, giving these people, who most likely will lose their jobs, a guaranteed alternative would create support for the transition and make sure that those most vulnerable to the proposed changes don’t get left behind. Instead of creating state-funded jobs just for the sake of employing people, the state could empower many of its citizens to lead more sustainable lives as part of the green transition. A universal basic income might offer citizens time to engage in fulfilling community-based work that doesn’t generate profit but which has social value. Taking them out of their cars in long lines of commuter traffic and putting them in allotments growing food or in parks enjoying nature could help usher a whole new way of life.  


      Read more:
      How a basic income could help build community in an age of individualism


 A generous universal basic income would destigmatise work outside of paid employment, such as domestic care work and volunteering in the arts and community sectors and it would allow people to refuse environmentally harmful and badly paid jobs. New income detached from wage labour would mean new flows of money in the economy, breaking the cycle of energy-intensive production and consumption which drives much of emissions.  To win popular support for the Green New Deal its benefits must be truly universal. What better way to guarantee a just, green transition and ensure no one is left behind than universal basic income? Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"Ever had one too many at the zoo and thrown your beer at a tiger, or stripped off and attempted to jump into the penguin pool? I’d hope not, but these are just two examples of inappropriate behaviour by visitors at London Zoo’s controversial party nights.  We all know how alcohol causes people do silly things and temporarily lose control. In a zoo we are there to observe, learn and enjoy – not to interact with the animals and certainly not to bother them. At their best, zoos are a wonderful form of theatre; at their worst, a grotesque pantomime, featuring unwilling animal actors. Theatres and zoos survive in the days of cinema and wildlife documentaries because they provide a more personal experience.  In a world where people share their life through social media a visit to the zoo provides them with something that their friends can experience, without being the cloned experience of watching a wildlife documentary. Unfortunately, we have all heard a cell phone ringing in a theatre, thereby breaking the suspension of reality.  But should zoo visitors be passive observers?  If you follow the school of immersive zoo design, where the visitor is transported through enclosure design to the tropical rainforest of Africa, then a cell phone going off is going to ruin that experience. In my experience there is never a signal in such remote places. Talking during a performance has always been a big no-no in theatres. But in zoos, talking at normal volume is not a problem; however, I personally find it extremely uncomfortable when people start to shout at zoo enclosures.  And the animals, how do they feel?  Before answering this question we should look at why people shout at animals in zoos.   Many zoo animals are nocturnal and so we are visiting them at their least active time of day. And a number of popular species such as lions are naturally very inactive. For every gazelle hunt worthy of a BBC Nature special there are many, many hours of lazing around. Of course this isn’t what zoo visitors have paid their money for – they want to see active animals. In some of my recent research we showed that a number of zoo species become more active when the public is noisy. Chimpanzees start to roam around their enclosure, for example. And there may be positive feedback at work here; that is, loud visitors provoke higher levels of animal activity, which in turn causes the humans to shout or flash their cameras, which disturbs the animals further, and so on. Undoubtedly zoo visitors affect animal behaviour and well-being.  Studies show, in general, that non-agitated and quiet groups of visitors do not appear to stress animals, whereas large, agitated crowds cause all kinds of unwanted changes. Animals can become more aggressive, less sociable or more vigilant (an indication they feel less secure). Our research has shown that zoo visitors increase noise levels on average by more than 10 decibels; that is, they double sound noise levels (decibels are measured on a logarithmic scale).  If you have even been in a noisy bar you know one consequence of people talking loudly is you need to talk louder yourself to be heard (what is known as the Lombard Effect).   Many species naturally depend on their hearing to know when to feed their young or when a predator is nearby. Thus, being deprived of this sense is stressful. So what can be done about loud zoo visitors? Simply asking people with signs to be quiet is not usually effective. When I worked at Edinburgh Zoo in the 1990s, we deliberately made the visitor areas dark.  People in spaces with reduced lighting generally talk more quietly and are less agitated – think of libraries.  But zoos should not be libraries – but they should be places of learning.  I think a good zoo should encourage people to discuss what they are seeing. Some zoos have human attendants at the enclosures to control public behaviour but this seems rather draconian.  There must be better solution and of course environmental education can help to a certain extent. But the education message can go too far: many zoo visitors now feel that must say they visit a zoo for conservation purposes. Ironically, the elephant in the room at the zoo is to say you are there for a good day out.  Despite the fact that people enjoying themselves will learn more about conservation. We can of course provide zoo animals with enclosures that are acoustically isolated from the public.  But I feel this would deprive the public of the great sensory experiences of zoos: their sounds and smells.  The deafening duets of gibbons and the foul smell of maned wolf faeces have stayed in my memory from my first zoo visits more than 40 years ago. Good zoos take the visitor experience very seriously and the welfare of their animals even more so. The challenge is to make zoos a pleasant experience for both animals and people – a happy medium that lets us enjoy and appreciate a zoo’s inhabitants, without causing adverse stress levels."
"Republican lawmakers under pressure to address the climate crisis are trying to move beyond denying the problem and start proposing solutions. But they still refuse to commit to what scientists say is necessary if the US is to rapidly cut back on burning fossil fuels.  A recent package of legislation proposed by House Republican leader Kevin McCarthy would encourage capturing climate pollutants from power plants but use them to drill for oil. It would also lead to the planting of a trillion trees to absorb carbon emissions but also ramp up logging, an idea Donald Trump has endorsed. “It’s greenwashing,” said Randi Spivak, public lands director at the Center for Biological Diversity, which organized a letter of opposition on the tree bill from dozens of environment groups to the House natural resources committee. “The science is very clear,” Spivak said. “We need to slash our carbon pollution by 50% over the next 10 years if we want to avert the worst impacts of global warming and keep global warming to a 1.5C increase.” The Republican party has seen a pendulum swing. In 2008, its election platform emphasized the importance of cutting emissions. By 2016, Donald Trump ran on a campaign of climate denial. Now at least some prominent Republicans are breaking from the president and veering away from questioning the science and toward efforts that do not directly attack the coal, oil and gas industries. Facing pushback from the far right, on the other side of the aisle they are criticized by some Democrats who believe they are not proposing legislation in good faith. Alex Flint, executive director of the Alliance for Market Solutions – a right-of-center organization that advocates reducing carbon pollution while growing the US economy – said the House bills were “directionally correct” but “need to grow to address the scale of our climate problem”. Flint’s group backs revenue-neutral carbon tax and deregulation. “I give [House Republicans] a great deal of credit for acknowledging the problem and stepping forward with proposals and recognizing that the politics of this has changed,” he said. “But also acknowledging that they are in the early stages of really substantive climate proposals.” Bruce Westerman, an Arkansas Republican who introduced the trees bill, insisted his proposal would curb emissions even as it promotes logging. When trees are cut down, they stop absorbing carbon. Westerman, a forester who worked for an engineering consulting firm in the timber, pulp and paper business, said forest managers would just plant them again. Products made from the trees would pay for continual planting, he said, and the wood would be used for sustainable buildings with lower emissions footprints. A scientist called to testify by Democrats, Yale ecology professor Carla Staver, strongly disagreed with Westerman’s proposals. Forest management is an important way to fight climate change but it is not enough and it must be done properly, experts agree. A bill from Democrats would aim to conserve forests without increasing logging and by banning oil and gas drilling on public lands if climate emissions exceed targets. Westerman told the Guardian Democrats should be working with Republicans who want to address climate change. “Even someone who’s not a forester should be able to recognize an olive branch when they see one,” he said. Even as Westerman defended his bill, the top Republican on his committee seemed to dismiss the overall effort. At the beginning of the meeting, to which he was late, Rob Bishop of Utah suggested the elevators in House office buildings would be more timely under the rule of Benito Mussolini than they are with Democrats in charge. Democratic climate proposals, he said, offered not a “silver bullet” to fix the problem but “another bullet that is going to be used to shoot ourselves in the foot”. He then showed a graph demonstrating how US heat-trapping emissions have declined over time. While the Republican bills are far from aggressive, the right of the party is pushing back. The conservative Club for Growth Pac painted the package as “stifling liberal environmental taxes, regulations, and subsidies” and vowed to withhold support from any backers. At the annual CPAC gathering near Washington, the climate change denial group the Heartland Institute presented a German teen activist who calls herself a “climate sceptic” as a foil to Greta Thunberg. In Oregon, Republican state senators fled the state capitol in order to derail a climate change bill. Despite that backdrop, the top Democrat on the House natural resources committee welcomed Westerman’s proposal. Raúl Grijalva said he hoped for a “new chapter”, focusing on solutions not denial. He said: “For too long my friends on the other side of aisle denied that this was even a real issue. They would reject or even mock the overwhelming scientific consensus that climate is warming humans are responsible and urgent action needs to be taken.”"
"
In an announcement sure to cause controversy over Svensmark’s theory of cosmic ray to cloud modulation, which is said to be affecting earth’s climate. Svensmark says this is now leading to a global cooling phase. Just a couple of weeks after Svensmark’s bold announcement, NASA has announced that we have hit a new record high in Galactic Cosmic Rays, GCR’s. Apparently, Nature is conducting a grand experiment. – Anthony
Click for larger image - Source: NASA (ACE) spacecraft 
From NASA News: Cosmic Rays Hit Space Age High
Planning a trip to Mars? Take plenty of shielding. According to sensors on NASA’s ACE (Advanced Composition Explorer) spacecraft, galactic cosmic rays have just hit a Space Age high.
“In 2009, cosmic ray intensities have increased 19% beyond anything we’ve seen in the past 50 years,” says Richard Mewaldt of Caltech. “The increase is significant, and it could mean we need to re-think how much radiation shielding astronauts take with them on deep-space missions.”
The cause of the surge is solar minimum, a deep lull in solar activity that began around 2007 and continues today. Researchers have long known that cosmic rays go up when solar activity goes down. Right now solar activity is as weak as it has been in modern times, setting the stage for what Mewaldt calls “a perfect storm of cosmic rays.”
“We’re experiencing the deepest solar minimum in nearly a century,” says Dean Pesnell of the Goddard Space Flight Center, “so it is no surprise that cosmic rays are at record levels for the Space Age.”
An artist's concept of the heliosphere, a magnetic bubble that partially protects the solar system from cosmic rays. Credit: Richard Mewaldt/Caltech
Galactic cosmic rays come from outside the solar system. They are subatomic particles–mainly protons but also some heavy nuclei–accelerated to almost light speed by distant supernova explosions. Cosmic rays cause “air showers” of secondary particles when they hit Earth’s atmosphere; they pose a health hazard to astronauts; and a single cosmic ray can disable a satellite if it hits an unlucky integrated circuit.
The sun’s magnetic field is our first line of defense against these highly-charged, energetic particles. The entire solar system from Mercury to Pluto and beyond is surrounded by a bubble of solar magnetism called “the heliosphere.” It springs from the sun’s inner magnetic dynamo and is inflated to gargantuan proportions by the solar wind. When a cosmic ray tries to enter the solar system, it must fight through the heliosphere’s outer layers; and if it makes it inside, there is a thicket of magnetic fields waiting to scatter and deflect the intruder.
“At times of low solar activity, this natural shielding is weakened, and more cosmic rays are able to reach the inner solar system,” explains Pesnell.
Mewaldt lists three aspects of the current solar minimum that are combining to create the perfect storm:

 The sun’s magnetic field is weak. “There has been a sharp decline in the sun’s interplanetary magnetic field (IMF) down to only 4 nanoTesla (nT) from typical values of 6 to 8 nT,” he says. “This record-low IMF undoubtedly contributes to the record-high cosmic ray fluxes.”
 The heliospheric current sheet is shaped like a ballerina’s skirt.  Credit: J. R. Jokipii, University of Arizona
› Larger image
The solar wind is flagging. “Measurements by the Ulysses spacecraft show that solar wind pressure is at a 50-year low,” he continues, “so the magnetic bubble that protects the solar system is not being inflated as much as usual.” A smaller bubble gives cosmic rays a shorter-shot into the solar system. Once a cosmic ray enters the solar system, it must “swim upstream” against the solar wind. Solar wind speeds have dropped to very low levels in 2008 and 2009, making it easier than usual for a cosmic ray to proceed.
 The current sheet is flattening. Imagine the sun wearing a ballerina’s skirt as wide as the entire solar system with an electrical current flowing along the wavy folds. That is the “heliospheric current sheet,” a vast transition zone where the polarity of the sun’s magnetic field changes from plus (north) to minus (south). The current sheet is important because cosmic rays tend to be guided by its folds. Lately, the current sheet has been flattening itself out, allowing cosmic rays more direct access to the inner solar system.

“If the flattening continues as it has in previous solar minima, we could see cosmic ray fluxes jump all the way to 30% above previous Space Age highs,” predicts Mewaldt.
Earth is in no great peril from the extra cosmic rays. The planet’s atmosphere and magnetic field combine to form a formidable shield against space radiation, protecting humans on the surface. Indeed, we’ve weathered storms much worse than this. Hundreds of years ago, cosmic ray fluxes were at least 200% higher than they are now. Researchers know this because when cosmic rays hit the atmosphere, they produce an isotope of beryllium, 10Be, which is preserved in polar ice. By examining ice cores, it is possible to estimate cosmic ray fluxes more than a thousand years into the past. Even with the recent surge, cosmic rays today are much weaker than they have been at times in the past millennium.
“The space era has so far experienced a time of relatively low cosmic ray activity,” says Mewaldt. “We may now be returning to levels typical of past centuries.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9301c8ce',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

After years of promoting the Kyoto Protocol on global warming, British Prime Minister Tony Blair his finally seen the light: the Protocol is simply not the way to deal with planetary warming, and it will take away capital that would more wisely be invested in technological development. 



Blair’s venue was as important as his words. He killed Kyoto in front of former president Bill Clinton, whose vice president, Al Gore, made Kyoto possible. Speaking at the former president’s “Clinton Global Initiative” conference in New York last month, Blair declared, “I would say probably I’m changing my thinking about [global warming] in the past two or three years…We have got to start from the brutal honesty about the politics of how we deal with it. The truth is no country is going to cut its growth or consumption substantially in light of a long‐​term environmental problem.” 



Blair recognized Kyoto’s fatal flaw: The U.S. is simply not going to implement it. “Some people have [ratified] Kyoto, some people haven’t signed Kyoto, right. That’s a disagreement. It’s there. It’s not going to be resolved.” 



Further, Blair eschewed another Kyoto‐​style treaty: “I don’t think people are going, at least in the short term, to start negotiating another major treaty like Kyoto,” he said. 



What’s remarkable is how close Blair’s position on climate change has become to that of the Bush administration: Kyoto won’t do anything about global warming. Instead, a rational approach to climate change — should policy changes really be necessary — requires major technological development and investment. 



It’s well known in scientific circles that Kyoto would only change global temperature by seven‐​hundredths of a degree Celsius in fifty years. But it would require reductions in carbon dioxide emissions to 7 percent below 1990 levels by the period 2008–2012. The only way this is possible is with a massive series of highly regressive energy taxes. 



How high must they be? No one knows, but a mini‐​experiment is currently running in the United States, thanks to the reverberations of Hurricane Katrina on limited gasoline and oil supplies. If prices at or near current levels continue for an extended period of time (and some economists think that may be the case), what’s going to be the impact on consumption? Whatever number, it’s not going to be the 25 percent drop required for the U.S. to meet Kyoto. The average annual growth rate in our emissions since 1990 has been 1.3 percent. 



Three dollars a gallon for gasoline translates to proportional increases across all other fossil fuels, all of which directly impact consumers’ spending on energy and transportation. It also indirectly raises the price of everything that requires fossil energy for manufacturing. People wind up paying more, which leaves them less to invest. 



On the subject of investment in efficient technologies, Blair asked Clinton’s forum, “How do we put these incentives in the system so that the private sector, as well as the public says, this is the direction policy is going to go?” 



The answer is obvious: If we want people to invest in the technologies of the future, they must have funds for investment. Taking money away in a futile attempt to affect the climate, like Kyoto, destroys that capital, putting it in the hands of governments that must of necessity bias their investments in ineffective, politically correct technologies like solar energy and windmills. 



Tony Blair has seen the light. With regard to planetary temperature, the future is not today’s Kyoto Protocol or any future sibling. Instead it lies in investment. For some reason, though, he has not yet learned that this is much better done by individuals than by governments, upon whom lobbyists force inefficient choices. 



Blair has also seen the light about the futility of the Kyoto Protocol and the importance of private investment. Now, if someone could only illuminate him with the notion that politics makes government a bad investor.
"
"

Africa defies conventional logic: grinding poverty amidst immense mineral riches. Africa’s economic growth of 5 percent in 2004, though more respectable than in previous years, was less than the 7 percent needed to achieve the United Nations Millennium Development Goals of reducing poverty and child mortality and improving education. At that rate, the United Nations Development Program has warned that the achievement of the millennium development goals may take 150 years.



The Commission on Africa, which was established by British prime minister Tony Blair, seeks to raise $50 billion a year on the international capital markets and use it to reverse Africa’s economic atrophy. Blair made aid to Africa the centerpiece of the British presidency of the G-8 meeting in Gleneagles, Scotland, in July 2005. President George Bush has tripled aid to Africa to $4.3 billion since he took office in 2001. In addition, the Bush administration’s Millennium Challenge Account (MCA) seeks to boost grants to poor African countries. France proposes an international tax on financial transactions or items such as plane tickets. Japan favors a $200 million fund to nurture private‐​sector companies in Africa to improve the continent’s investment climate and credit rating. The UN is calling on rich countries to increase their foreign aid to 0.7 percent of GDP by 2015. The UN argues that lack of resources is a major impediment to economic growth and that additional funds will be well spent. But will any of those plans help Africa?



Most Africans are skeptical. They have heard those righteous calls before. Every decade or so, a throng of Western donors, African governments, and international organizations gathers to announce grand initiatives to pull the world’s poorest continent out of its economic miasma. Congratulatory pats on the back are exchanged. Delegates return home and then nothing much is heard after that. Back in 1985, the United Nations held a Special Session on Africa to boost aid to Africa. In March 1996, the United Nations launched a $25 billion Special Initiative for Africa. They all fizzled. Why should Africans place any faith in the current initiatives to reverse Africa’s economic atrophy? 



**The Failure of Aid**



Helping Africa is a noble cause, but the campaign has become a theater of the absurd – the blind leading the clueless. The record of Western aid to Africa is one of abysmal failure. More than $500 billion in foreign aid – the equivalent of four Marshall Aid Plans – was pumped into Africa between 1960 and 1997. Instead of increasing development, aid has created dependence. The budgets of Ghana and Uganda, for example, are more than 50 percent aid dependent. Said President Aboulaye Wade of Senegal: “I’ve never seen a country develop itself through aid or credit. Countries that have developed — in Europe, America, Japan, Asian countries like Taiwan, Korea and Singapore — have all believed in free markets. There is no mystery there. Africa took the wrong road after independence. 1



The more aid poured into Africa, the lower its standard of living. Per capita GDP of Africans living south of the Sahara declined at an average annual rate of 0.59 percent between 1975 and 2000. Over that period, per capita GDP adjusted for purchasing power parity declined from $1,770 in constant 1995 international dollars to $1,479. The evidence that foreign aid underwrites misguided policies and feeds corrupt and bloated state bureaucracies is overwhelming. 



Tanzania’s ill‐​conceived socialist experiment, Ujaama, for example, received much Western support. Western aid donors, particularly in Scandinavia, gave their enthusiastic backing to Ujaama, pouring an estimated $10 billion into Tanzania over a period of 20 years. Yet, between 1973 and 1988, Tanzania’s economy contracted at an average rate of 0.5 percent a year, and average personal consumption declined by 43 percent. Today, Tanzania’s largely agricultural economy remains devastated. Some 36 million Tanzanians are attempting to live on an average annual per capita income of $290—among the lowest in the world. Other African countries that received much aid between 1960 and 1995 – Somalia, Liberia, and Zaire – slid into virtual anarchy. 



Much of the aid received was simply looted. Speaking at the New Partnership for African Development (NEPAD) meeting in Abuja, Nigeria, in December 2003, the former British secretary of state for international development, Lynda Chalker, noted that 40 percent of the wealth created in Africa is invested outside the continent. “If you can get your kith and kin to bring the funds back and have it invested in infrastructure, the economies of African countries would be much better than what they are today,” she said. 2 The chairman of the session and president of the African Business Round Table, Alhaji Bamanga Tukur, agreed: “It is really difficult to ask foreign investors to come and invest on our continent when our own people are not investing here. There is no better factor to convince foreign investors than for them to see that our own people, both those based at home and those in the Diaspora, invest in Africa. 3



Indeed, the amount of capital leaving Africa is staggering. Percy Mistry of the Oxford International Group pointed out that the external stock of capital held by Africans overseas could be as much as $700 billion to $800 billion. 4 The World Bank estimated that “nearly 40 percent of Africa’s aggregate wealth has fled to foreign bank accounts. 5 Considering the missing billions in export earnings from oil, gas, diamonds, and other minerals that are not openly accounted for, it is dubious that Africa suffers from a poverty trap, as Jeffrey Sachs of Columbia University argues. 



Africa’s case for more aid and debt relief has not been helped by President Olusegun Obasanjo of Nigeria, which is arguably the most mismanaged economy in Africa. As he was pleading for more aid at the World Economic Forum in Davos, Switzerland, in February 2005, four of Obasanjo’s state governors were being probed by London police for money laundering. The most galling was the case of Plateau State Governor, Chief Joshua Dariye, who was accused of diverting some $90 million into his private bank accounts. Dariye was dragged before the Federal High Court in Kaduna by Nigeria’s Economic and Financial Crimes Commission. Incredibly, Justice Abdullahi Liman ruled that although Dariye was a principal suspect in the case, Section 308 of the Nigerian Constitution protected sitting governors from criminal prosecution.



In February 2005, Nigeria’s police chief, Inspector General Tafa Balogun, was forced into early retirement after investigators probing money‐​laundering allegations found $52 million hidden in Balogun’s network of 15 bank accounts. At the time of his early retirement, Balogun had been on the job for only two years. Even Nigeria’s senate is riddled with scams and inflated contracts, with proceeds pocketed by sitting senators. According to the president of the Institute of Chartered Accountants of Nigeria, Chief Jaiye K. Randle, individual Nigerians are currently lodging $170 billion in foreign banks – far more than Nigeria’s foreign debt of $35 billion.



In July 2005, Nigeria’s Economic and Financial Crimes Commission revealed that a succession of military dictators stole or squandered $500 billion – equivalent to all Western aid to Africa over the past four decades. 6 Even when the loot is recovered, it is quickly re‐​looted. The Nigerian state has recovered $983 million of the loot of the former president, General Sani Abacha, and his henchmen. But the Senate Public Accounts Committee found only $12 million of the recovered loot in the Central Bank of Nigeria. 7



 **Aid for Reform**



Foreign aid given to support reform in Africa has not been successful either. According to the United Nations Conference on Trade and Development: “Despite many years of policy reform, barely any country in the region has successfully completed its adjustment program with a return to sustained growth. Indeed, the path from adjustment to improved performance is, at best, a rough one and, at worst, disappointing dead‐​end. Of the 15 countries identified as ‘core adjusters’ by the World Bank in 1993, only three ( Lesotho, Nigeria and Uganda) are now classified by the IMF as ‘strong performers.’ 8



The World Bank evaluated the performance of 29 African countries to which it had provided more than $20 billion in “structural adjustment” loans between 1981 and 1991. The bank’s report, _Adjustment Lending in Africa_ , concluded that only six African countries had performed well: The Gambia, Burkina Faso, Ghana, Nigeria, Tanzania, and Zimbabwe. That gives a failure rate in excess of 80 percent. More distressing, the World Bank concluded that “no African country has achieved a sound macro‐​economic policy stance.” Since then, the World Bank’s list of “success stories” has shrunk. The Gambia, Nigeria, and Zimbabwe are no longer on that list.



In 1998, four new “success stories” were added ( Guinea, Lesotho, Eritrea, and Uganda). However, the senseless Ethiopian‐​Eritrean war and the eruption of civil wars in western and northern Uganda have knocked those two countries off the new “success stories” list. Uganda depends on foreign aid for 58 percent of its budget. There are growing concerns about its democracy, defense spending, and rampant corruption. Yet, in December 1999, Uganda’s aid donors announced the country’s biggest‐​ever loan of $2.2 billion – with no visible strings attached. As _The Economist_ pointed out, “Cynics might say that Uganda can hold the world to ransom because the World Bank, the IMF and the other foreign donors cannot afford to let their star pupil go under. 9



 **The Western Aid Lobby Is Partly to Blame**



Africans themselves have realized that Western aid has not been effective. David Karanja, a former Kenyan member of parliament, for example, said: “Foreign aid has done more harm to Africa than we care to admit. It has led to a situation where Africa has failed to set its own pace and direction of development free of external interference. Today, Africa’s development plans are drawn thousands of miles away in the corridors of the IMF and World Bank. What is sad is that the IMF and World Bank experts who draw these development plans are people completely out of touch with the local African reality. 10



The donors themselves contributed much to the failure of Western aid to Africa. Foreign loans and aid programs in Africa were badly monitored and often stolen by corrupt bureaucrats. “We failed to keep a real hands‐​on posture with aid,” said Edward P. Brynn, former U.S. ambassador to Ghana. “We allowed a small, clever class that inherited power from the colonial masters to take us to the cleaners. It will take a whole lot of time and money to turn Africa around. 11



More maddening, the donor agencies knew or should have known all along about the motivations and activities of corrupt African leaders. They knew or should have known that billions of aid dollars were being spirited into Swiss banks by greedy African kleptocrats. “Every franc we give impoverished Africa comes back to France or is smuggled into Switzerland and even Japan” wrote the Paris daily, _Le Monde_ in March 1990. 12 Even famine relief assistance to Africa was not spared. Dr. Rony Brauman, head of Médecins sans Frontières, lamented in the 1980s: “We have been duped.… Western governments and humanitarian groups unwittingly fuelled – and are continuing to fuel – an operation that will be described in hindsight in a few years’ time as one of the greatest slaughters of our time. 13



Patricia Adams of Probe International, a Toronto‐​based environmental group, charged that, “in most cases, Western governments knew that substantial portions of their loans – up to 30 percent, says the World Bank – went directly into the pockets of corrupt officials, for their personal use. 14



Yet, the World Bank considered those same African governments “partners in development.” When the United Nations launched a $25 billion Special Initiative for Africa in March 1996, the bank’s president James Wolfensohn said that he was “pleased that the Special Initiative is designed to be supportive of and a ‘true partnership’ with African leadership.’ 15 In fact, World Bank loans have often bailed out tyrannical regimes in the past. After shattering Ghana’s economy, the Marxist government of Jerry Rawlings found that the Soviet and Cuban governments could no longer provide it with assistance. Rawlings made overtures to the West, which responded with alacrity, eager to win one more “convert.” The regime signed a “structural adjustment” agreement with the World Bank in 1983. Slight improvements in the economy were hailed, and Ghana was declared a “success story” and a “role model for Africa.” Twelve years later and after the infusion of more than $4 billion in loans, the World Bank admitted that declaring Ghana a “success story” was a mistake and not in the country’s own best interest. 16



In recent years, loans provided by the World Bank for various poverty‐​reduction programs in Ghana have continued to be embezzled by the political elites. According to Goosie Tanoh, leader of the newly formed National Reform Party, “It is an open secret that so many grants from Japan, Canada, USA and Britain had been given to party functionaries who have misapplied it. 17



In Kenya, Nairobi’s deputy mayor, Abdi Ogle, demanded the resignation of the World Bank’s country director for Kenya, Harold Wackman, a Canadian, accusing him of turning a blind eye to embezzlement of an emergency loan of $77.5 million in July 1998 to repair infrastructure damaged by heavy rains. “Not a cent of this money has come to the City Council because it has disappeared into private pockets within the Ministry of Local Government,” fumed Ogle. 18



Kenyan constitutional reform, which was supposed to have addressed the problem of pervasive corruption in that country, has stalled under the watchful eyes of the ruling elite. Widespread government corruption has caused international donors to withhold money allocated to fight AIDS. The disease has killed about 1.5 million in Kenya since 1984. The government estimates that about 1.4 million Kenyans are infected with HIV/AIDS. Yet Kenya’s health ministry is riddled with graft. A recent audit revealed the existence of “ghost workers” whose salaries worth $6.5 million per year are collected by living workers. In June 2004, the same health ministry paid $1.8 million for a radiography machine for the Kenyatta National Hospital that was never delivered. Over the course of the past 12 months, Kenya has been rocked by corruption scandals in various ministries, but little action has been taken. The ministers who were involved were sacked, but not prosecuted to recover the loot.



During his 24 years in power, Daniel Arap Moi’s government embezzled and stole an estimated $3 billion to $4 billion. The country’s central bank was looted. The money was stolen by making fictitious payments on foreign debt; kickbacks were collected on all public contracts, and when that didn’t supply enough cash, politicians awarded themselves fake contracts. A report by Kenya’s recently created Anti‐​Corruption Commission estimates that up to $3 billion of the missing money is still stashed overseas. After he left office, Moi and his family were among the wealthiest people in Kenya, with seven big homes and connections to at least 30 major business firms. But he also left behind an economy crippled with foreign debt, collapsed infrastructure, unemployment hovering at 70 percent, and nearly two‐​thirds of the population living under the poverty line. 



**The Need for Domestic Reform**



Foreign funds can help only those African countries that undertake political, economic, and institutional reform, but the commitment to reform has been woefully lacking. The democratization process in Africa has stalled through political chicanery and strong‐​arm tactics. Only 16 of the 54 African countries are democratic, and political tyranny remains the order of the day. Often, those countries that are democratic remain deeply corrupt. Intellectual freedom is stuck in the Stalinist era: only eight African countries have free and independent media. The record on economic reform is abysmal. Only Botswana, Mauritius, Namibia, and South Africa can be described as “success stories.” 



At the July 2004 African Union Summit in Abuja, Nigeria, frustrated UN Secretary‐​General Kofi Annan told African leaders of their lack of progress on meeting the UN’s Millennium Development Goals that they agreed to in 2000. Four years earlier, he was less restrained, lashing out at African leaders and blaming them for most of the continent’s problems. 19



African children echo the same sentiments. At the United Nations Children’s Summit held in May 2002 in New York, youngsters from Africa ripped into their leaders for failing to improve their education and health. “You get loans that will be paid in 20 to 30 years and we have nothing to pay them with, because when you get the money, you embezzle it, you eat it,” said 12‐​year‐​old Joseph Tamale from Uganda. 20



Tony Blair and Jeffrey Sachs should listen to the voices of average Africans, who have not benefited from aid in the past and are unlikely to benefit in the future. Without domestic reforms, African politicians will line their pockets, but Africa will remain desperately poor.



 **Notes:**



1 Norimitsu Onishi, “Senegalese Loner Works to Build Africa, His Way,” _New York Times_ , April 10, 2002, p. A3.



2 Kunle Aderinokun, “Africa at Large: 40% of Continent’s Wealth Invested Outside,” _This Day_ , Nigeria, December 4, 2003, cited in George Ayittey, _Africa Unchained_ ( New York: Palgrave Macmillan, 2005), p. 324. We are indebted to George Ayittey, from whose research and writing we have borrowed in preparing parts of this text.



3 Aderinokun.



4 Percy Mistry, “Aiding Africa,” letter to _The Economist_ , July 14, 2005.



5 Karen DeYoung, “Giving Less: The Decline in Foreign Aid,” _Washington Post_ , November 25, 1999, p. A1.



6 Peter Goodspeed, “Corruption’s Take: $148B,” _National Post_ ( Canada), July 4, 2005, p. A1.



7 Ayittey, _Africa Unchained_ , p. 439.



8 George Ayittey, “Corruption, the African Development Bank and Africa’s Development,” Testimony before the Senate Foreign Relations Committee, September 28, 2004, p. 7.



9 Cited in Ayittey, _Africa Unchained_ , p. 161.



10 Cited in George Ayittey, _Africa in Chaos_ (New York: St. Martin’s, 1998), p. 275.



11 Blaine Harden, “The US Keeps Looking for a Few Good Men in Africa,” _New York Times_ , August 27, 2000, p. 1.



12 Cited in Jonathan C. Randal, “French‐​Speaking Africa Hit by Popular Discontent,” _Washington Post_ , March 26, 1990, p. A17.



13 Rony Brauman, “Famine Aid: Were We Duped?” _Reader’s Digest,_ October 1986. 



14 Patricia Adams, “The Debts of Corruption,” _Financial Post_ ( Canada) May 10, 1999.



15 _African Recovery_ , May 1996, p. 13.



16 World Bank, _Ghana Country Assistance Review: A Study in Development Effectiveness_ (Washington: World Bank, January 1996). 



17 _Ghanaian Chronicle_ , August 14, 2000.



18 _Daily Graphic_ , January 9, 1999, p. 5.



19 _Daily Graphic_ , July 12, 2000; p. 1.



20 “African Children Accuse Leaders,” _BBC News_ , May 10, 2002. 
"
"**The word ""quarantine"" has taken on an extra meaning and been named ""word of the year"" by the Cambridge Dictionary.**
Editors said the word was now ""synonymous with lockdown"" and relates to staying at home to avoid catching the disease.
Previously it was only defined in relation to a person or animal ""suspected of being contagious"".
Other words on the 2020 shortlist included ""lockdown"" and ""pandemic"", it added.
On Tuesday, Oxford Dictionaries said it had expanded its word of the year to encompass several ""Words of an Unprecedented Year"".
Cambridge Dictionary said quarantine was the third most looked-up word overall this year, after ""hello"" and ""dictionary"".
It recorded a surge of searches for ""quarantine"" in March, when restrictions were imposed.
Wendalyn Nichols, its publishing manager, said: ""The words that people search for reveal not just what is happening in the world, but what matters most to them in relation to those events.
""Neither coronavirus nor Covid-19 appeared among the words that Cambridge Dictionary users searched for most this year.
""We believe this indicates people have been fairly confident about what the virus is.""
The new sense of the word, defining it as ""a general period of time in which people are not allowed to leave their homes or travel freely, so that they do not catch or spread a disease"", has been added to the dictionary.
Other news words included ""HyFlex"", which is short for hybrid flexible and denotes a type of teaching where some students are physically present in class and others join from a distance online.
As a result of people stopping shaking hands, kissing or hugging since the outbreak of Covid-19, the phrase ""elbow bump"" has also been added.
It is defined in the dictionary as ""a friendly greeting in which you touch someone's elbow with your elbow"".
_Find BBC News: East of England on_Facebook _,_Instagram _and_Twitter _. If you have a story suggestion email_eastofenglandnews@bbc.co.uk"
"**Counter-terrorism police say Covid-19 could be behind a fall in referrals from people worried about friends or family members becoming radicalised.**
They are concerned that people vulnerable to radicalisation are spending more time online at home.
But due to people not mixing with friends, at school or in workplaces as regularly, reports are not being made.
Det Ch Insp Alistair Stenner said the virus was behind a 64% drop in referrals in the South West of England.
""We're really worried about the impact of Covid and young and vulnerable people spending more time online,"" said Det Ch Insp Stenner, of Counter Terrorism Policing South West.
""It's difficult to know who they are talking to, what they are looking at and what impact this is having on them and their outlook on the world.
""From 1 January to the middle of November there was a 64% reduction in referrals in comparison to the same period last year and whilst there may be a number of reasons for that, I actually believe the main one is Covid,"" he said.
He said that social distancing measures meant people were not mixing in society the way they would before the pandemic, making changes in behaviour or extremist views harder to spot.
""People aren't picking up the phone and reporting things to us. We think that's because there is less mixing between people, individuals are spending less time with friends, at school and in the workplace, so they become more insular.""
He cited the case of Bristol man Andrew Ibrahim, who was jailed for plotting to blow up a city shopping centre, as an example of how important it was to report concerns early.
Brad Evans, professor of political violence and aesthetics at the University of Bath, said radicalisation was a ""very complex"" problem but poverty and education played large roles.
""We need to ask why these children with anger inside them feel they an only direct that anger through violence. There are also wider issues, particularly the use of technology in society and how they engage with technology in a world that looks increasingly divisive.
""We could have a generation of young children growing up who are being radicalised and believe that violence is the way to solve problems and that's deeply troubling.""
A new website and advice line, ACT Early has been launched to encourage people to report concerns."
"**The UK oil and gas sector is in ""economic turmoil"" amid the coronavirus pandemic with about a fifth of firms expecting more redundancies in 2021, according to a new report.**
Aberdeen and Grampian Chamber of Commerce (AGCC) said reduced activity levels and project cancellations had seen business optimism ""slashed"".
Confidence is now said to be as low as during the industry downturn in 2015.
The findings came in the 32nd AGCC Oil and Gas Survey.
It covers the six months to October.
The survey was carried out in partnership with the Fraser of Allander Institute and KPMG UK.
It asked firms about the initial Covid impact, how they expected activity to recover, and further issues such as energy transition and Brexit.
The survey found that only 13% of contractors were working at, or above, optimum levels in the UK Continental Shelf (UKCS) compared with 47% a year ago - with 82% predicting a decrease in revenue in 2020.
A total of 23% of contractors reported cancelling projects as a result of the coronavirus outbreak, with a further 34% putting activities on hold.
More than three quarters of businesses - 78% - were less confident about activities going forward, while only 1% were more confident.
AGCC said that while businesses typically reported higher levels of optimism about their international activities, the latest results marked the lowest recorded levels of confidence in global markets in the history of the survey.
About half of contractors surveyed reported a decline in their workforce - 22% of which said reductions equated to more than 10% of their workforce - and about a fifth of surveyed firms said they expected to make further reductions in 2021.
A total of 83% of contractors furloughed employees.
AGCC research and policy manager Shane Taylor said: ""Over the course of this year we have seen drastic and unpredictable disruption to business globally due to Covid-19, combined with the collapse in oil and gas prices.
""Although government support has had clear value in supporting firms and jobs through this challenging period of suppressed demand, the only sustainable way to give businesses and workers clarity is a clear route to heightened levels of activity in the future.""
Martin Findlay, senior partner at KPMG in Aberdeen, added: ""From the significant oil price decline, which started earlier in the year, to a global pandemic, and localised lockdown in Aberdeen, the oil and gas industry has, once again, endured profound challenge and uncertainty.
""However, there is room for some optimism. The industry, unlike so many others, is incredibly resilient and frequently deals with instability and challenge.""
Industry body Oil and Gas UK (OGUK) said the finding were further confirmation of the ""stark conditions"" faced bythe sector.
Chief executive Deirdre Michie said: ""We remain particularly concerned about the health of our world-class supply chain.
""OGUK continues to work with industry to see what we can to together to safely increase activity and protect jobs.""
The survey involved 100 firms employing more than 22,000 people across the UK and 400,000 globally."
"**Personal protective equipment (PPE) stockpiles in England were inadequate for the Covid pandemic and price rises earlier this year cost taxpayers about Â£10bn, the spending watchdog has said.**
The National Audit Office said there had been a particular shortage of gloves and aprons.
The government said the NAO's report recognised that NHS providers had been able to get what they needed in time.
Almost Â£12.5bn was spent on 32bn items of PPE between February and July 2020.
During the same period in 2019, 1.3bn items were bought at a cost of Â£28.9m.
Each item had been ""substantially"" more expensive in 2020, because of very high global demand, the NAO said, from almost triple the cost for respirator masks to more than 14 times as much for body bags.
Had the government been able to pay 2019 prices, it would have spent Â£2.5bn on PPE in 2020.
In reality, it had spent Â£12.5bn, including hundreds of millions on ""unsuitable"" items that could not be used.
Some had ""passed its expiry date or did not meet current safety standards"", the watchdog said, with ""insufficient checks"" meaning Public Health England had had to recall eye protectors that did not meet standards.
In Parliament, on Wednesday, Labour leader Sir Keir Starmer accused Prime Minister Boris Johnson of ""wasting"" taxpayer's money on equipment that ""can't be used"".
But Mr Johnson replied ""99.5%"" of the 32 billion items of PPE bought between February and July 2020 ""conform entirely to our clinical needs"".
Earlier, the Department of Health and Social Care (DHSC) said ""only 0.49% of all the purchased PPE tested to date"" had not been fit for purpose.
NAO head Gareth Davies said: ""As PPE stockpiles were inadequate for the pandemic, government needed to take urgent action to boost supplies.
""Once it recognised the gravity of the situation... the price of PPE increased dramatically, and that alone has cost the taxpayer around Â£10bn.""
Before the Covid-19 pandemic, there were two emergency stockpiles of PPE:
But the NAO said: ""The EU exit stockpile held few items of PPE other than a large number of gloves.""
Meanwhile, the flu stockpile, as well as having shortages of some key items, did not include any gowns or visors despite the fact they had been ""recommended for inclusion in June 2019 by the New and Emerging Respiratory Virus Threats Advisory Group (Nervtag)"".
Public Health England told the NAO it had been analysing the market to work out which gowns to buy, when the pandemic had begun, which it said was the ""normal approach"" to find a lower price.
In mid-March, the government had still believed its two stockpiles would provide ""most of the PPE needed to manage a Covid-19 pandemic"" and so focused on distributing this PPE rather than buying more, the NAO reported.
The situation had become ""precarious"" in April and May, with stocks threatening to run out.
At one point, only 3% of the required number of gowns had been available.
But the nation did not at any point run out of any type of PPE.
The scramble for PPE in the early stages of the pandemic was not confined to the UK. Every healthcare system was desperate to secure protective equipment and prices soared. But the National Audit Office lays bare how the UK was at the back of the queue, having failed to spot the warning signs and how woefully inadequate the stockpiles were.
A failure to anticipate what might be needed for anything other than a flu pandemic in essence cost the taxpayer Â£10bn - the extra money needed to secure supplies such as gowns and visors during the Covid crisis.
The report highlights poor distribution of PPE with many staff saying they did not have the right equipment. The NAO notes starkly that health and care employers have reported more than 100 deaths among staff because of exposure to coronavirus.
An official inquiry, when it happens, will look hard at many aspects of the UK's preparedness and handling of the crisis and the PPE issue will be central. With a series of reports, the NAO has now done important groundwork but there is much still to find out.
A DHSC official said: ""As the NAO report recognises, during this unprecedented pandemic all the NHS providers audited 'were always able to get what they needed in time' thanks to the Herculean effort of government, NHS, armed forces, civil servants and industry"".
But the NAO heard feedback from care workers, doctors and nurses that showed ""significant numbers of them considered that they were not adequately protected during the height of the first wave of the pandemic"".
Employers have reported 126 deaths among health and care workers linked to exposure at work.
And there were concerns about training and whether the equipment was appropriately fitted, particularly from women and people belonging to ethnic minorities.
In a Royal College of Nursing survey of 5,000 NHS staff, 49% of respondents belonging to ethnic minorities said they had been adequately ""fit tested"" for a respirator, compared with 74% of white nurses.
The DHSC said it was ""listening to the reported practical difficulties with the use of some PPE experienced by women and black, Asian and minority ethnic (BAME) individuals, among others, and... taking action to make sure user needs are adequately addressed in future provisions"".
In a separate report, the Public Accounts Committee, a parliamentary body which works closely with the NAO, said it was ""concerned that the department had no plan before the pandemic for how it might increase critical care equipment in the event of an emergency"".
""This lack of preparedness was exacerbated by the fact that it did not know how many ventilators were available to the NHS to begin with,"" the PAC said.
But it added the government had managed to buy an additional 26,000 ventilators for use in the NHS, a ""significant achievement""."
"**A secondary school in Powys has been closed for two weeks after a spike in coronavirus cases.**
Welshpool High School will be shut from Tuesday until 7 December, following outbreaks among at least three different year groups.
Head teacher Jim Toal said: ""It saddens me to have to resort to this measure but there really isn't an alternative at this stage.""
The school's 1,000 students have been told lessons will continue online.
Year 7 pupils at the school were told to self-isolate on 15 November, while Year 10 and 11 were told they would also need to stay home on Sunday.
However, a decision was taken on Monday to close the school entirely, following a review by staff and council officials.
""Despite the school's best efforts to mitigate the spread of infection a rapid growth of positive cases in the Welshpool area has led to a tipping point over the most recent weekend, and into this morning making further intervention essential,"" Mr Toal added.
""We continue to work with Environmental Health and Public Health Wales to identify those pupils who may be contactsÂ of a positive case.""
Infection rates in the town now stand at 133.8 per 100,000 of the population over the last seven days.
The move follows decisions to close 13 schools on Monday across Ceredigion and Pembrokeshire.
Phyl Davies, who is responsible for education on Powys council, told the Local Democracy Reporting Service: ""The rising cases at the school and in the community is now causing significant concerns for both the school's senior leaders and the council.
""Due to these concerns, it has been decided to keep the high school closed for two weeks to keep pupils and school staff safe."""
"**A main road through Cardiff city centre which has been used as a dining area since July is to reopen on Sunday.**
Castle Street will reopen to buses, taxis and emergency vehicles, although a pop-up cycleway which has been in place during the pandemic will remain.
The pavement opposite the castle will be widened to allow extra outdoor seating for cafes and more room for pedestrians to socially distance.
The council said there had been strong arguments for and against the closure.
Private cars will still have to use alternative routes.
A public consultation is being held on the street's future status.
The council said the temporary partial reopening would help buses and taxis cross from east to west during ongoing road works in the city centre.
Under the new arrangement, Castle Street will have two lanes for traffic, one in each direction, and the cycle lane next to the castle.
The pavement on the adjoining Westgate Street has also been widened to give extra outdoor space for bars and restaurants.
Caro Wild, cabinet member for strategic planning and transport, said: ""The council recognises that the closure of Castle Street has divided opinions, with strong arguments being made in favour and against the changes that have been implemented in recent months.
""Alongside a detailed modelling exercise on future traffic flows, we will undertake a comprehensive consultation exercise, involving city centre businesses, local residents, and citizens across Cardiff, to help determine the final plan for the street.""
The council is also monitoring congestion and air quality across the city centre and will undertake detailed modelling on long-term plans to help decide if extra mitigation measures are needed in neighbouring areas of the city."
"The former weather presenter Francis Wilson has said it is now more important than ever for TV forecasters to be serious experts “in a time of floods and fires” caused by global heating. Wilson, who presented forecasts on BBC Breakfast between 1981 until 1992 and at Sky News from 1993 until 2010, stressed the need to keep audiences engaged and to report accurately as extreme weather becomes more common. “A report has to be engaging,” he told Radio Times. “A stuffy, pompous forecast, laden with jargon, is a switch-off and that’s self defeating. “We want people to be engaged with the weather … every weather presenter has an even greater obligation to get the tone right.” He warned that because of the climate crisis, the UK would now experience more severe weather events such as the floods that have caused catastrophic disruption in the north of England and Wales. In recent weeks, the UK has been hit by Storms Ciara, Dennis and Jorge, which have inflicted destruction on homes, businesses and roads. Councils are facing large bills to repair the damage and on Monday it was reported that the UK had experienced its wettest February on record. Wilson referred to the infamous BBC lunchtime report on 15 October 1987 where weatherman Michael Fish “cheerfully dismissed” a viewer’s concerns that “a hurricane was on the way”. Later that evening, what is now known as the Great Storm of 1987 happened. Eighteen people were killed and around 15m trees were blown over. “That [Great Storm of 1987] was a once-in-200-years storm but now, thanks to global warming, we have a more energetic atmosphere,” Wilson said. “We are already seeing this new reality, here in the UK. It’s common sense that lightly dismissing a viewer’s concerns is wildly inappropriate when large areas of Yorkshire, the Midlands and Wales are under water, but we also need to be confident that the presenter knows what they are talking about. It’s a tricky balance.” Wilson also said there was a “moral obligation” on TV forecasters to tell viewers that the more extreme climate events they are reporting on are caused by global heating. “We need to tell people to stop warming the atmosphere, to stop adding carbon dioxide to the atmosphere,” he said. “That way, viewers won’t lose sight of the fact that they can actually do something about it. The BBC used the Met Office as its main source for forecasts for 94 years until 2017 when it switched to MeteoGroup. Marco Petagna, a senior operational meteorologist at the Met Office, said it continued to provide “high resolution data and weather warnings” for the BBC. Petagna added that his organisation currently provided data and presenter briefings for Sky and ITV. The weather bulletins for Channel 5 are presented by Met Office presenters during the week and they provide voiceover scripts and graphics at the weekend. MeteoGroup also provides weather data for Channel 4. Wilson also referenced the changing intensity of weather events elsewhere around the world. “Across the world, storms will be fiercer, floods will be deeper, droughts will be longer, deserts will be drier and wild fires will be wilder.”"
"
Share this...FacebookTwitterSteffen Hentrich here of the Liberal Institute, a think tank of the Friedrich-Naumann-Foundation for Freedom brings our attention here to a PNAS paper written by Glen P. Peters, Jan C. Minx, Christopher L. Weber and Ottmar Edenhofer titled: Growth in emission transfers via international trade from 1990 to 2008.
Transfering efficient manufacturing to developing countries leads to more CO2 emissions, and not less. Photo source: Library of Congress CALL NUMBER LC-USW36-376
Though not saying it directly, the paper calls current climate policy a failure. It’s right there in the very first sentence (emphasis added):
Despite the emergence of regional climate policies, growth in global CO2 emissions has remained strong. From 1990 to 2008 CO2 emissions in developed countries (defined as countries with emission reduction commitments in the Kyoto Protocol, Annex B) have stabilized, but emissions in developing countries (non-Annex B) have doubled.”
That folks, is what we call POLICY FAILURE – period. In fact climate policy has likely produced just the opposite of what was intended, meaning more CO2 and not less.
The authors quantified the growth in emission transfers via international trade. To do this they developed a trade-linked global database for CO2 emissions covering 113 countries and 57 economic sectors from 1990 to 2008. Here’s what they found, taken from the front page:
…emissions from the production of traded goods and services have increased from 4.3 Gt CO2 in 1990 (20% of global emissions) to 7.8 Gt CO2 in 2008 (26%). Most developed countries have increased their consumption-based emissions faster than their territorial emissions, and non–energy-intensive manufacturing had a key role in the emission transfers. The net emission transfers via international trade from developing to developed countries increased from 0.4 Gt CO2 in 1990 to 1.6 Gt CO2 in 2008, which exceeds the Kyoto Protocol emission reductions. Our results indicate that international trade is a significant factor in explaining the change in emissions in many countries, from both a production and consumption perspective.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




What does all that mean? It means that the developed countries are simply moving their CO2-intensive industry off their territory, and placing it on territories that are exempt from mandatory reductions, i.e developing countries. This what they call emissions transfer. The result: less CO2 emissions at home, but huge, greater increases in the less efficient country that took over the industry. This is what the study has confirmed.
Indeed the whole scheme backfires because undeveloped countries often have lower environmental and technical standards, and so produce the goods with considerably higher emissions and real pollution. Then add the transport of these goods from these developing countries back to Europe or North America, which adds even more CO2. And let’s not even look at the biofuels debacle the fossil fuel hysteria led to.
The PNAS paper writes in the discussion part (emphasis added:
Under the IPCC accounting rules of only reporting territorial emissions, many developed countries have reported stabilized emissions. However, our results show that the global emissions associated with consumption in many developed countries have increased with a large share of the emissions originating in developing countries. This finding may benefit economic growth in developing countries, but the increased emissions could also make future mitigation more costly in the developing countries. In addition, we find that the emission transfers via international trade often exceed the emission reductions in the developed countries.”
So what do the environmental and economic masterminds intend to do about it? Not much for now. Solving the emissions transfer problem of course would mean massive interference in global markets and end up punishing developing countries. The authors recommend:
We suggest that countries monitor emission transfers via international trade, in addition to territorial emissions, to ensure progress toward stabilization of global greenhouse gas emissions.”
Monitor? Now they don’t mean that countries start thinking of ways to restrict trade, now do they? The more they meddle with the economy and trade, the more they are going to mess everything up.
Share this...FacebookTwitter "
"

The U.S. Congress finally passed a stimulus package after making sure to strip out elements that would benefit the economy over the long run. Nonetheless, the plan includes an accelerated depreciation provision that will be beneficial if it is later made permanent. But a new corporate tax survey by KPMG makes clear that this is only the first of many needed business tax reforms in the United States.



KPMG found that the U.S. has the fourth highest corporate‐​income‐​tax rate in the 30‐​nation Organisation for Economic Co‐​operation and Development (OECD). The combined U.S. federal and average state rate of 40% is almost 9 percentage points higher than the average OECD top corporate rate of 31.4%. 



This is a dramatic reversal of the U.S. tax situation. After cutting the federal corporate rate from 46% to 34% in 1986, policymakers fell asleep at the switch, perhaps assuming that the U.S. had claimed a low‐​tax advantage permanently. But most industrial countries followed the U.S. lead and cut tax rates in the late 1980s. Then another round of tax‐​rate cuts began in the late 1990s, with the result that the average OECD corporate rate fell from 37.6% in 1996 to just 31.4% by January 2002. The average corporate rate in the European Union is now 32.5%, down from 38.2% in 1996. 



Americans sometimes write‐​off European countries as uncompetitive welfare states, and ignore that many have improved their business climates. But a recent study by the Economist Intelligence Unit placed the U.S. second, behind the Netherlands, for the “best place in the world to conduct business.” And a study by GrowthPlus, a European think tank, compared 10 major countries to determine which had the best environment for entrepreneurial growth companies. Again, the U.S. finished second, this time behind Britain. 



In the last few years, the corporate tax rate was cut in Denmark, France, Ireland, Germany, Poland, and Portugal. Even socialist Sweden has a top corporate tax rate of just 28%. It is certainly true that overall European taxes, as a share of gross domestic product, are much higher than in the United States. But Europe has shifted about one‐​third of its overall tax burden to less distortionary consumption taxes. 



What the Europeans and others are realizing is that countries shoot themselves in the foot by imposing high tax rates on mobile capital. IMF data show that annual global portfolio‐​capital flows rose six‐​fold during the past decade. United Nations data show that direct investment also rose six‐​fold during this period. The U.S. attracts a big share of these flows because of its large economy and stable currency. But investment flows are increasingly sensitive to taxes, so it makes less and less sense to have a high corporate rate. After all, last year’s recession, the Enron collapse, and the high‐​tech bust all show that the U.S. business sector is not as invincible as it seemed in the late 1990s. 



A high statutory rate isn’t the only aspect of U.S. business taxation that needs reform. The new depreciation rules should be made permanent, and the current global reach of the corporate income tax should be replaced with a “territorial” tax. Glenn Hubbard, chairman of the Council of Economic Advisers, has noted that “from an income tax perspective, the U.S. has become one of the least attractive industrial countries in which to locate the headquarters of a multinational corporation.” As a consequence, there has been a “marked increase” in the number of U.S. firms reincorporating abroad, according to a new U.S. Treasury analysis. 



The critics of course will say that big corporations and their shareholders should pay their “fair share” of taxes, and that the government needs to crack down on tax‐​avoiders like Enron. Such views ignore big‐​picture realities. First, the huge rise in global capital flows means that the corporate tax burden falls more on immobile workers, and less on the mobile capital income it is ostensibly placed on. Second, the high U.S. corporate tax rate is the reason why Enron and other firms go to such wasteful lengths to avoid and evade taxes. 



As the world economy changes, so must U.S. tax policy. Pressures to attract mobile capital through international “tax competition” will continue to increase. These trends dictate that the U.S. reform its tax system by moving away from a high‐​rate income tax system to a low‐​rate consumption‐​based tax system.
"
"Just over ten years ago the Nobel Prize-winning atmospheric scientist Paul Crutzen coined the term “Anthropocene” for a globe totally transformed and dominated by humans, a state he suggested we were in now. Although this idea was not totally new, the term Anthropocene caught on and is now regularly used outside the environmental and earth sciences.  Perhaps the appeal of the term is partly because it adheres to the traditional suffix of “-cene” which in geology indicates a geological epoch, such as the two most recent epochs, the Pleistocene (from 2.5m years ago) followed by the Holocene, technically the current geological period from the beginnings of agriculture around 12,000 years ago to the present day.  In 2009 it was suggested to the world’s authoritative geological bodies that the term should be formalised – something much easier said than done. Defining geological boundaries has never been easy and several important boundaries have only been formally defined in the past 20 years. Geological boundaries are, to a large extent, arbitrary – even political – but they are essential. As the bedrock of geology they are the only way of correlating rock sequences and usefully discussing Earth’s 4.5 billion year history. Several geological committees in the UK and other countries are looking at whether the Anthropocene should be defined as a new geological period and, if so, what type it should be and when it should start. The idea is that the Anthropocene period relates to, and takes its name from the time in which humans have come to completely dominate the planet. When that period began is moot, with some believing it must be a gradual process, affecting some parts more than others over many millennia.  Others favour the idea of using the spike in radionuclides associated with the nuclear age as a marker, while others still believe such dominance is yet to happen. Myself, and others, would argue that it could be dated back to the adoption of agriculture, over the past 6,000 years. Why is this of importance to anyone, even other academics, outside the slightly dusty realm of the earth sciences? There are several reasons, some pragmatic, others more theoretical. Geological classification is used by other disciplines, most notably archaeology and engineering.  A formalised Anthropocene would have little effect on engineering, but it would put archaeology - the study of human activity in the past - in a curious position, namely that of having the vast majority of its subject matter (with a few exceptions such as Cold War archaeology) placed into a past, and by implication pre-human geological period, the Holocene. The disparity would only enlarge the gulf between geological and archaeological ways of looking at the past. It is also difficult to see how this would work in relation to geological maps, as the period assigned to a rock deposit is based on its known age. Even sediments of human origin might or might not be of Anthropocene age, depending upon where the boundary was set.  And what of the existing Holocene epoch, which would end at the beginning of the Anthropocene? It could be unfeasibly short, just a few thousand years, raising the question of whether it was sensible to have carved the Holocene out of geological time in the first place. The principal reason it was separated from the Pleistocene, which itself contains more than 50 similar globally warm periods between ice ages, was that it was the geological period during which human civilisation had evolved.  More theoretically but equally important is that the rules of geological taxonomy were never designed to identify a boundary falling within recorded history and which is not based on natural geological changes. The whole question is reminiscent of the famous statement by the historian Francis Fukuyama who in response to the collapse of socialism in 1989 declared that history was dead. It clearly wasn’t but the world had changed fundamentally, and likewise the Anthropocene would be a geological period like no other. Given these difficulties one is forced to wonder why bother? Its advocates argue that recognising the Anthropocene might move society towards a more sustainable path. This is not the purpose of the geological column, and raises the concern that this is about funding priorities: in a world where research funding is increasingly driven by political agendas and relevance, it’s clear that the Anthropocene could quite wrongly be seen as somehow more relevant and worthy of funds than other areas. So does this academic debate serve any positive purpose? Yes, in that it has forced earth scientists to look deeper at how we recognise, define and demarcate the human component in all parts of the earth system and not just climate. Humans clearly do not act in isolation, and complex fluctuations exist in the atmosphere and oceans that are driven by factors outside of human control. Whether or not the Anthropocene is formalised there is little doubt that we are living at a critical period in Earth’s history, as the world changes around us."
"

Cueball: Catania photosphere image August 31st, 2009 - click for larger image
It has been a strange day. Fires have evacuated the Mt. Wilson Observatory in California, and SOHO images have not been updating all day. Power is down at the mountain and the webcam has gone offline. See status here. Mt. Wilson Observatory is now in the hands of nature and CDF. Let’s hope CDF wins.
The only ""observer"" left at Mount Wilson on Monday afternoon was the automated webcam atop the solar tower. This was its smoky westward view at 6:54 p.m. Pacific time. Still no flames coming over the crests. UCLA Dept. of Physics and Astronomy
It  is about 4 hours now past ooGMT Sept1, 2009 I’ve checked all my sources. Besides the fate of Mt. Wilson, we’ve all been waiting to find out two things:
1- Will we have a spotless calendar month for the sun in August 2009?
2- Do I still have my solar mojo?
The Catania sunspot drawing shows nothing for the 31st.
Catainia Observatory Solar Sketch - click for larger image
Other solar observatories, Uccle in Beligium, Locarno in Germany, both show nothing on August 31st sketches.

This animation from SIDC of the past 30+ days shows nothing for August but DOES show group 1025 popping up on 9/1/2009
http://sidc.oma.be/html/cmap_animator.html
I also checked SIDC’s sunspot report data for August, nothing.
It looks like the spot today, group 1025, squeaked by and was not observed until after August 31st game clock ran out at 00 GMT 91/2009
Then I checked NOAA SWPC….
http://www.swpc.noaa.gov/ftpdir/latest/DSD.txt
Message to NOAA Space Weather: Out damned spot!
And wouldn’t you know it, they have something whereas last year it was the other way around…NOAA had nothing, SIDC (via Catania) did…so where does that leave us?
Leif said last year that SIDC had the last word…so unless they change their report, we may indeed have a spotless calendar month.
We’ll have to see what happens when their report comes out tomorrow. They issue a new report on the first of each month.
http://sidc.oma.be/products/ri_hemispheric/
Watch that space.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93692375',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
By Bill Steigerwald

“Grandpa does Washington”
JEFFERSON MEMORIAL, WASHINGTON, D.C.
It would have been an odd sight if any humans had been around to see three polar bears walking across the Tidal Basin and climbing the steps of the Jefferson Memorial. Open to the cold air and blowing snow, its cavernous icy marble interior was empty except for a 19-foot bronze statue of Thomas Jefferson.
“He’s as big as you are,” Junior said to Grandpa.
“He’s much bigger than I am,” Grandpa whispered as if he were in a church. “See those words engraved there on the wall. They’re from the Declaration of Independence. Thomas Jefferson is the human who wrote them.”
“What do they say?” Junior asked.
Grandpa smiled and winked at Mother. “They say, ‘We hold these truths to be self-evident, that all polar bears are created equal, that they are endowed by their Creator with certain inalienable rights, among these are life, liberty, and the pursuit of happiness.’”
“Those words are some of the greatest ever written about freedom,” Mother said. “Too bad so many humans no longer believe in them,” she added, opening her brown suitcase and taking out a neatly folded stack of human clothes.
“These should fit,” Mother said, handing Grandpa a dark herringbone three-button wool suit, matching vest and wide-striped tie like the one she had seen Jimmy Stewart wearing in the movie “Mr. Smith Goes to Washington.” “Your eyeglasses are in the breast pocket.”
“And here’s your costume, Junior,” Mother said, giving him a pair of home-made blue jeans and a Chicago Cubs T-shirt to go with his backpack and Cubs baseball cap. “And your glasses. Don’t ever take them off when we’re in the presence of humans.”
After Mother put on her black skirt, blouse and seashell pink blazer, she pulled out her pair of gray Kawasaki 704 eyeglasses and put them on. Except for her black nose, she looked eerily like Sarah Palin.
“What do you think, Dad?” Mother asked Grandpa. “They were a little pricy, even on the Internet. But I think they work.”
The three bears looked at each other’s outfits admiringly. They weren’t the latest fashions, but as far as any humans who looked at them could tell the trio looked like an ordinary – if large – family of humans who’d come to Washington to see the sights.
For several hours the three bears explored the snowy, deserted streets of downtown Washington. Grandpa had a long list of places he had always dreamed of visiting and they were all carefully plotted on the old map he carried.
They walked across the frozen Tidal Basin to the Washington Monument, where Grandpa hoped to take an elevator ride to the top. But it was closed because of the horrible weather, so instead they visited the National World War II Memorial and the Vietnam Veterans Memorial.
As they strolled past the brightly lit White House, two wary policemen in a patrol car slowed down to look them over.
“Wave, Mother,” Grandpa said under his breath as the policeman driving the car shined a spotlight on them. “Wave, Junior.”
The policeman hesitated. He squinted his eyes. Something seemed very, very fishy. He unlocked the shotgun attached to the dashboard of his patrol car.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e902222c0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The word juggernaut derives its meaning from the ancient Hindu god Jagan‐​n, or lord of the world, whose powers crushed all those who stood in his way. Formerly, fanatics sometimes threw themselves under the wheels of carts bearing his image to be crushed as a demonstration of the god’s power. Today, the word juggernaut connotes an overwhelming force that crushes or seems to crush everything in its path.



The word accurately describes the powerful position held by the National Education Association and the American Federation of Teachers. These two unions wield massive power over public school teachers and other school employees. They maintain this power through the inertia of a well entrenched monopoly in representing teachers.



Since the 1960s, the NEA and AFT have dominated the market for teacher representation services. In the 34 states that require school boards to bargain collectively with teachers, the NEA and AFT currently share almost 100 percent of the market for teacher representation services.



Although the AFT and NEA assert that their tenure as exclusive representative demonstrates teacher satisfaction with their services, in reality, teachers desiring a different representative, or none at all, face enormous legal and practical obstacles to achieve any such change. For example, the NEA and AFT have a virtually limitless supply of funds and personnel to thwart any effort to achieve representation by a different entity. But the teachers seeking this objective must finance the effort from their personal funds. Litigation, virtually certain to materialize, can bankrupt the dissidents, whereas knowledgeable staff lawyers are available on call to the unions.



The NEA/AFT monopoly in representing teachers at the local level is a grave disservice to teachers who are forced to pay higher than necessary union dues or who don’t like the policies of the NEA and AFT. But the enormous power and privilege of the incumbent union makes it almost impossible for teachers to change to another union or no union at all. The NEA/AFT juggernaut easily crushes any opposition that threatens their monopoly over teacher representation.



To alleviate this problem, competition is required in the teacher representation market. One way to provide that would be to allow for‐​profit and nonprofit entities as well as solo entrepreneurs, labor lawyers, and collective bargaining companies to represent teachers in collective bargaining. Teachers would retain the right to go without an exclusive representative and each of the representative options would compete against all the others. Teacher representation would not be limited to membership organizations as it is now, and teachers could change their choice of representative periodically, perhaps every three years or at the expiration of a collective agreement covering teachers.



Providing increased competition to the NEA and AFT would require some changes in current labor policy in most states. For example, states legislatures would have to do three things: 1) reduce the number of votes required to trigger an election to select a new teacher representative, 2) explicitly allow individuals, nonprofit and for‐​profit organizations to compete for the right to represent teachers, and 3) enable all members in the bargaining unit to vote on the key decisions affecting their terms and conditions of employment. 



Teachers would experience a number of significant benefits from competition to represent them in collective bargaining. These benefits include lower dues, better service, increased choice, and more input in the key decisions affecting their employment. Allowing solo entrepreneurs, professional negotiators, lawyers, and collective bargaining companies to compete with the NEA/AFT for teacher representation would create a more powerful consumer role for teachers who wish to purchase these services. Introducing competition into teacher representation is the best way to insure that unions work for the benefit of teachers.



Also, considering that the two largest teacher unions are the major opponents of school choice, diminishing the role and influence of the NEA/AFT would contribute to a more positive climate for the school choice movement. Without their opposition to school choice, many more options would be available to parents of K-12 children. Parents and their children, therefore, become the secondary beneficiaries of increased competition for teacher representation.



Undoubtedly, enactment of the proposal in one state would publicize the concept nationally to rank‐​and‐​file teachers who have every reason to support legislation that would expand their choice of exclusive representative. It is difficult to see how the NEA/AFT could successfully persuade most teachers that legislation that expands their choice of exclusive representative is harmful to teachers.



In Economics 101, we are told that ease of entry is the most important requirement for a competitive market to emerge. Perhaps if teachers experience the benefits of competition as consumers, they will recognize its value for parents and students. Rolling back the teacher union juggernaut may be the next critical step in improving schools and creating meaningful educational choice for parents and students.
"
"
Scientists seek to remove climate fear promoting editor and ‘trade him to New York Times or Washington Post’

An outpouring of skeptical scientists who are members of the American Chemical Society (ACS) are revolting against the group’s editor-in-chief — with some demanding he be removed — after an editorial appeared claiming “the science of anthropogenic climate change is becoming increasingly well established.”
The editorial claimed the “consensus” view was growing “increasingly difficult to challenge, despite the efforts of diehard climate-change deniers.” The editor now admits he is “startled” by the negative reaction from the group’s scientific members. The American Chemical Society bills itself as the “world’s largest scientific society.”
The June 22, 2009 editorial in Chemical and Engineering News by editor in chief Rudy Baum, is facing widespread blowback and condemnation from American Chemical Society member scientists. Baum concluded his editorial by stating that “deniers” are attempting to “derail meaningful efforts to respond to global climate change.”
Dozens of letters from ACS members were published on July 27, 2009 castigating Baum, with some scientists calling for his replacement as editor-in-chief.
The editorial was met with a swift, passionate and scientific rebuke from Baum’s colleagues. Virtually all of the letters published on July 27 in castigated Baum’s climate science views. Scientists rebuked Baum’s use of the word “deniers” because of the terms “association with Holocaust deniers.” In addition, the scientists called Baum’s editorial: “disgusting”; “a disgrace”; “filled with misinformation”; “unworthy of a scientific periodical” and “pap.”
One outraged ACS member wrote to Baum: “When all is said and done, and you and your kind are proven wrong (again), you will have moved on to be an unthinking urn for another rat pleading catastrophe. You will be removed. I promise.”
Baum ‘startled’ by scientists reaction.
Baum wrote on July 27, that he was “startled” and “surprised” by the “contempt” and “vehemence” of the ACS scientists to his view of the global warming “consensus.”
“Some of the letters I received are not fit to print. Many of the letters we have printed are, I think it is fair to say, outraged by my position on global warming,” Baum wrote.
Selected Excerpts of Skeptical Scientists: 
“I think it’s time to find a new editor,” ACS member Thomas E. D’Ambra wrote.
Geochemist R. Everett Langford wrote: “I am appalled at the condescending attitude of Rudy Baum, Al Gore, President Barack Obama, et al., who essentially tell us that there is no need for further research—that the matter is solved.”
ACS scientist Dennis Malpass wrote: “Your editorial was a disgrace. It was filled with misinformation, half-truths, and ad hominem attacks on those who dare disagree with you. Shameful!”
ACS member scientist Dr. Howard Hayden, a Physics Professor Emeritus from the University of Connecticut: “Baum’s remarks are particularly disquieting because of his hostility toward skepticism, which is part of every scientist’s soul. Let’s cut to the chase with some questions for Baum: Which of the 20-odd major climate models has settled the science, such that all of the rest are now discarded? […] Do you refer to ‘climate change’ instead of ‘global warming’ because the claim of anthropogenic global warming has become increasingly contrary to fact?”
Edward H. Gleason wrote: “Baum’s attempt to close out debate goes against all my scientific training, and to hear this from my ACS is certainly alarming to me…his use of ‘climate-change deniers’ to pillory scientists who do not believe climate change is a crisis is disingenuous and unscientific.”
Atmospheric Chemist Roger L. Tanner: “I have very little in common with the philosophy of the Heartland Institute and other ‘free-market fanatics,’ and I consider myself a progressive Democrat. Nevertheless, we scientists should know better than to propound scientific truth by consensus and to excoriate skeptics with purple prose.”
William Tolley: “I take great offense that Baum would use Chemical and Engineering News, for which I pay dearly each year in membership dues, to purvey his personal views and so glibly ignore contrary information and scold those of us who honestly find these views to be a hoax.”
William E. Keller wrote: “However bitter you (Baum) personally may feel about CCDs (climate change deniers), it is not your place as editor to accuse them—falsely—of nonscientific behavior by using insultingly inappropriate language. […] The growing body of scientists, whom you abuse as sowing doubt, making up statistics, and claiming to be ignored by the media, are, in the main, highly competent professionals, experts in their fields, completely honorable, and highly versed in the scientific method—characteristics that apparently do not apply to you.”
ACS member Wallace Embry: “I would like to see the American Chemical Society Board ‘cap’ Baum’s political pen and ‘trade’ him to either the New York Times or Washington Post.” [To read the more reactions from scientists to Baum’s editorial go here and see below.]
Physicist Dr. Lubos Motl, who publishes the Reference Frame website, weighed in on the controversy as well, calling Baum’s editorial an “alarmist screed.”
“Now, the chemists are thinking about replacing this editor who has hijacked the ACS bulletin to promote his idiosyncratic political views,” Motl wrote on July 27, 2009.
Baum cites discredited Obama Administration Climate Report
To “prove” his assertion that the science was “becoming increasingly well established,” Baum cited the Obama Administration’s U.S. Global Change Research Program (USGCRP) study as evidence that the science was settled. [Climate Depot Editor’s Note: Baum’s grasp of the latest “science” is embarrassing. For Baum to cite the June 2009 Obama Administration report as “evidence” that science is growing stronger exposes him as having very poor research skills. See this comprehensive report on scientists rebuking that report. See: ‘Scaremongering’: Scientists Pan Obama Climate Report: ‘This is not a work of science but an embarrassing episode for the authors and NOAA’…’Misrepresents the science’ – July 8, 2009 )
Baum also touted the Congressional climate bill as “legislation with real teeth to control the emission of greenhouse gases.” [Climate Depot Editor’s Note: This is truly laughable that an editor-in-chief at the American Chemical Society could say the climate bill has “real teeth.” This statement should be retracted in full for lack of evidence. The Congressional climate bill has outraged environmental groups for failing to impact global temperatures and failing to even reduce emissions! See: Climate Depot Editorial: Climate bill offers (costly) non-solutions to problems that don’t even exist – No detectable climate impact: ‘If we actually faced a man-made ‘climate crisis’, we would all be doomed’ June 20, 2009 ]
The American Chemical Society’s scientific revolt is the latest in a series of recent eruptions against the so-called “consensus” on man-made global warming.
On May 1 2009, the American Physical Society (APS) Council decided to review its current climate statement via a high-level subcommittee of respected senior scientists. The decision was prompted after a group of 54 prominent physicists petitioned the APS revise its global warming position. The 54 physicists wrote to APS governing board: “Measured or reconstructed temperature records indicate that 20th – 21st century changes are neither exceptional nor persistent, and the historical and geological records show many periods warmer than today.”
The petition signed by the prominent physicists, led by Princeton University’s Dr. Will Happer, who has conducted 200 peer-reviewed scientific studies. The peer-reviewed journal Nature published a July 22, 2009 letter by the physicists persuading the APS to review its statement. In 2008, an American Physical Society editor conceded that a “considerable presence” of scientific skeptics exists.
In addition, in April 2009, the Polish National Academy of Science reportedly “published a document that expresses skepticism over the concept of man-made global warming.” An abundance of new peer-reviewed scientific studies continue to be published challenging the UN IPCC climate views. (See: Climate Fears RIP…for 30 years!? – Global Warming could stop ‘for up to 30 years! Warming ‘On Hold?…’Could go into hiding for decades,’ peer-reviewed study finds – Discovery.com – March 2, 2009 & Peer-Reviewed Study Rocks Climate Debate! ‘Nature not man responsible for recent global warming…little or none of late 20th century warming and cooling can be attributed to humans’ – July 23, 2009 )
A March 2009 a 255-page U. S. Senate Report detailed “More Than 700 International Scientists Dissenting Over Man-Made Global Warming Claims.” 2009’s continued lack of warming, further frustrated the promoters of man-made climate fears. See: Earth’s ‘Fever’ Breaks! Global temperatures ‘have plunged .74°F since Gore released An Inconvenient Truth’ – July 5, 2009
In addition, the following developments further in 2008 challenged the “consensus” of global warming. India Issued a report challenging global warming fears; a canvass of more than 51,000 Canadian scientists revealed 68% disagree that global warming science is “settled”; A Japan Geoscience Union symposium survey in 2008 reportedly “showed 90 per cent of the participants do not believe the IPCC report.” Scientific meetings are now being dominated by a growing number of skeptical scientists. The prestigious International Geological Congress, dubbed the geologists’ equivalent of the Olympic Games, was held in Norway in August 2008 and prominently featured the voices of scientists skeptical of man-made global warming fears. [See: Skeptical scientists overwhelm conference: ‘2/3 of presenters and question-askers were hostile to, even dismissive of, the UN IPCC’ & see full reports here & here – Also see: UN IPCC’s William Schlesinger admits in 2009 that only 20% of IPCC scientists deal with climate ]
h/t to ClimateDepot.com go there for links to the above referenced stories.
The ACS letters to the editor are here:  http://pubs.acs.org/cen/letters/87/8730letters.html


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e943625d8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"England’s green belts have had, and continue to have, a major impact on town planning. The idea of a ring of countryside surrounding an urban area to prevent sprawl originated in the 1930s and spread to post-war London and was adopted nationally in 1955. Today, about 13% of England is green belt land. But what made sense in the 1950s seems outdated and rather stale now. A one-size-fits-all approach to tackling complex planning issues tends to create more problems than it solves. You don’t need a belt-shaped area of land to check urban sprawl; you don’t need to block all development to promote greener outcomes. Perhaps in the 21st century it is time to admit that green belts are no longer fit for purpose. In theory, the idea of green belts still has strong protection within the government’s planning framework, within five strategic purposes under local authority control: But green belts have been attacked by a range of vested interests for failing to meet these purposes, with different ideas proposed in response. The head of Persimmon housebuilders, for instance, has called for a relaxation of green belt controls to ease the housing crisis. The chancellor wanted more imagination from local planning authorities in where houses are built – including possible incursions into the green belt.  Natural England, the government body responsible for safeguarding England’s natural environment, has previously called for a major policy rethink. In any case green belt protection is potentially illusory.  Greenfield sites (including green belt) are increasingly favoured by developers as they are cheaper to exploit than brownfield sites which have much higher transaction costs. Here economic growth priorities and national planning policy tends to push development pressures onto the urban fringe areas rather than more costly brownfield land.  There is clear evidence that while green belts have stopped urban expansion (for some cities), they have resulted in unintended consequences such as higher-density development at the urban fringe, including disconnected “edge cities”, and “leapfrogging” development over the green belt to undermine other areas of countryside. Green belts have a presumption against development and thus little incentive to be positively managed for environmental, community or economic purposes. This leads to degraded landscapes that, while having a valid planning function, produce limited benefit to communities and the environment – unless of course you are lucky enough to live in or next to one. As with natural assets more generally, the lack of incentive for active management is the greatest cause for concern.  It’s time for a fundamental rethink of the green belt. The “belt” metaphor has had its day. We should define bespoke areas that are functional to local geography and the needs of the cities and towns concerned; so wedges, fingers, belts, bananas or whatever shapes may equally apply. Rather than have green belts used for just major cities we should have a more inclusive, ubiquitous and positive zoning that applies to large towns and major settlements. Rather than a impose a rigid presumption against development we should aim for zones that encourage innovative uses that generate investment in environmental and community benefits in keeping with the principles of sustainable development. Finally, rather than enabling politically convenient incursions into the green belt under the guise of sustainable urban extensions, local planning authorities should define these zones set against the long-term development needs of the area looking 50 years into the future rather than the present 25 years. These principles lead me to propose the idea of “green investment zones”; new positive spaces to invest in. Thus the urban fringe can be rejuvenated by, for example, community food-growing initiatives for health and recreation or wetland creation for flood protection and biodiversity. A green investment zone would be flexible enough to incorporate whatever new initiative an entrepreneur might propose.  Local planning authorities will need to think strategically set within bolder and long term visions about the kind of town or city they want to create. The current 25-year planning lifecycle is not long enough.  Developers shouldn’t see these zones as automatic no-go areas. While housing should not be normally be allowed in them, they act as valuable green spaces that can help to protect new and existing housing development from floods and drought;  they can provide local food growing areas and spaces for play and recreation. They also can be used to protect our agriculture and perhaps more controversially for energy production (solar, anaerobic digestion or biomass) which are neglected planning factors.   In this green belt debate we need to move out of the silo thinking that separates housing, industry, transport, community, landscape and environment needs leading to disintegrated development. The green belt may no longer be fit for purpose but it must not be allowed to become a developers’ charter for just the short-term pursuit of economic growth. We need to create a more equitable and environmentally and socially responsible zoning tool that addresses current planning shortfalls and promotes a more positive image for planning."
"
Just when you think things can’t get any more bizarre with the IPCC, having just learned that the IPPC 2007 report used magazine articles for references, head of the IPCC, Dr. Rajenda Pachauri, provides comedy gold. According to the UK Telegraph, he’s just released what they describe as a “smutty” romance novel, Return to Almora laced with steamy sex, lots of sex. Oh, and Shirley MacLaine.
Here’s the good doctor, grinning like a Cheshire cat at his book launch in India on January 10th.
Click for more photos from his book release
The Telegraph’s Robert Mendick and Amrit Dhillon in Delhi write:
As the UN’s climate change chief, Dr Rajendra Pachauri has spent his career    writing only the driest of academic articles. But the latest offering from    the chairman of the UN’s climate change panel is an altogether racier tome.
Some might even suggest Dr Pachauri’s first novel is frankly smutty.
WARNING ADULT CONTENT FOLLOWS:
(First time I’ve had to do that on WUWT)

Return to Almora, published in Dr Pachauri’s native India earlier this    month, tells the story of Sanjay Nath, an academic in his 60s reminiscing on    his “spiritual journey” through India, Peru and the US.
click for bookseller
On the way he encounters, among others, Shirley MacLaine, the actress, who    appears as a character in the book. While relations between Sanjay and    MacLaine remain platonic, he enjoys sex – a lot of sex – with a lot of    women.
In breathless prose that risks making Dr Pachauri, who will be 70 this year, a    laughing stock among the serious, high-minded scientists and world leaders    with whom he mixes, he details sexual encounter after sexual encounter.
The book, which makes reference to the Kama Sutra, starts promisingly enough    as it tells the story of a climate expert with a lament for the denuded    mountain slopes of Nainital, in northern India, where deforestation by the    timber mafia and politicians has “endangered the fragile ecosystem”.
But talk of “denuding” is a clue of what is to come.
By page 16, Sanjay is ready for his first liaison with May in a hotel room in    Nainital. “She then led him into the bedroom,” writes Dr Pachauri.
“She removed her gown, slipped off her nightie and slid under the quilt    on his bed… Sanjay put his arms around her and kissed her, first with    quick caresses and then the kisses becoming longer and more passionate.
“May slipped his clothes off one by one, removing her lips from his for    no more than a second or two.
“Afterwards she held him close. ‘Sandy, I’ve learned something for the    first time today. You are absolutely superb after meditation. Why don’t we    make love every time immediately after you have meditated?’.”
More follows, including Sanjay and friends queuing to have sexual encounters    with Sajni, an impoverished but willing local: “Sanjay saw a shapely    dark-skinned girl lying on Vinay’s bed. He was overcome by a lust that he    had never known before … He removed his clothes and began to feel Sajni’s    body, caressing her voluptuous breasts.”
Take a cold shower, and read the rest of the steamy  (possibly a water vapor feedback loop) novel at the Telegraph here
Note to the U.N. – Time to kick Pachy to the curb, he’s not just toast now, he’s carbonized.
In other news, The Love Guru has this relevant quote from a hockey team member: “there’s no connection between hockey and my love life”

UPDATE: Steve McIntyre quips:
In breaking news, Vivid Entertainment has bought the film rights to the IPCC Fourth Assessment Report. They plan to give new meaning to the terms Working Group 1, Working Group 2 and Working Group 3. They promise to give “peer review” an entirely new interpretation.

Sponsored IT training links:
The credible HP0-S27 training really helps you pass CISM certification. Get the 642-982 latest dumps to fasten your success in first try.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e93200b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSolar panels, electric cars, windmills, biofuels – It’s all been ballyhooed as the next socio-technological revolution. One that would transform our energy supply and ensure “sustainability”, thus saving the earth from climate doom. “Go green” was the motto.All that was needed was a little help from the state. Now it looks as if even a Soviet style intervention is not going to save the green movement. Rather, it looks as if state intervention has doomed it. Everywhere the green economy is in tatters.
It seems everything that the government touches nowadays ends up turning into a folly. We are seeing it with solar energy, see here, here, here and here. The same goes with wind, biofuels and deforestation, and even the toxic mercury-laden light bulbs that are poisoning the land. How much longer before it all goes bankrupt?
Just recently Marc Morano unplugged the Chevy Volt, revealing the folly behind government supported electric cars, see here. No one wants them – even with the massive subsidies. And not even in Green Germany.
Germans opting more for gasoline and diesel engine cars 
Here a recent study released by oil company Aral shows that Germans are once again interested in acquiring a new car, read here in German. That’s good news for the economy. The problem is that fewer people are expressing interest in buying electric cars and hybrids. The Aral press release writes (emphasis added):
The preferred drive system remains the Otto engine by a clear margin. After 2009 when only 51% were interested in buying this conventional type of drive system, the number has since risen 10 percent to 61%. Also diesel engines can be happy with a share of 28% (+2%). A significant decrease was posted by cars driven by natural gas. Here the number of potential buyers has dropped by 50% over the last 2 years, going from 10% to 5%. Also electric cars have suffered a setback: Only 28% of those surveyed said they could imagine buying an electric car. Two years ago the figure was 36%.
Share this...FacebookTwitter "
"

It’s nice to know, even in these troubled times, that eternal verities remain. One is outright lying about environmental issues in order to stampede world leaders who currently have bigger fish to fry.



The latest example of this agitprop was generated in the Oct. 29 issue of The Guardian, the paper of the loony London left. It was timed to coincide with yet another U.N. meeting over the Kyoto Protocol, in, of all places, Marrakech, which began on the same day.



The Guardian describes the plight of all 10,991 poor inhabitants of Tuvalu, an island in the middle of the Pacific Ocean. They have pestered the New Zealand government into accepting each and every one of them as environmental refugees, cast adrift by sea level rise from dreaded global warming. The New Zealand government took the bait. The first evacuees are scheduled to arrive next year.



However, sea level around Tuvalu has been falling precipitously for the last half‐​century. You could look it up in the Oct. 27 issue of Science, which was available for days before The Guardian went to press.



French scientists, led by Cecile Cabanes, used data collected by altimeters aboard the TOPEX/​Poseidon satellite, and then compared them to a longer record of deep ocean temperatures that extends back to 1955. Sure enough, where the data overlapped (the satellite went up in 1993), there was very good agreement. The warmer (or colder) the ocean became, the more sea level rose (or fell).



There’s a beautiful color map of sea level changes in Cabanas’s Science article. It shows that Tuvalu is near the epicenter of a region where the sea level has been declining for nearly 50 years. In fact, the decline is so steep that even using the U.N.‘s lurid (and wrong) median estimates of global warming for the next century will not get the Tuvalus back to their 1950 sea level until 2050.



Unfortunately, The Guardian isn’t the only left‐​wing paper that didn’t do its homework about sea levels around Tuvalu. On Sept. 9, the Travel section of the Washington Post carried a huge article headlined “The End is Near,” complete with that umptysquat‐​point headline drowning in the ocean.



Writer Mike Tidwell includes the following tidbit: “Perhaps the most surreal indication of what might be in store comes from the idyllic, tourist‐​friendly nations of Tuvalu and Kiribati, in the South Pacific. Tuvalu is developing concrete emigration plans to evacuate its islands — perhaps entirely — in this century, migrating en masse to ‘host countries’ like New Zealand. This is because scientists say sea level rise could inundate Tuvalu and other low‐​lying countries almost entirely as polar ice melts and ocean water expands.”



How difficult would it have been to check the facts first? The same “scientists” have published graphs in widely available U.N. reports showing sea level fall in the Pacific and Indian oceans.



What’s the real reason for the Tuvalu exodus? I must be careful here, as my editors caution that a Cato scholar must use precise and non‐​inflammatory language: Tuvalu sucks.



There are no rivers or sources of potable water. Beachheads are eroding because the sand has been removed for building material. Most of the vegetation has been burned for fuel by the environmentally sensitive natives. The soil is poor. There are no mineral deposits and few exports. A large percentage of the GDP comes from licensing its area code for “900” lines and revenue from the sale of its “.tv” Internet domain.



In short, Tuvalu is a Tuvalu‐​made ecological disaster that is now an economic disaster. The natives want out because they wrecked the place. And why does New Zealand want them in? Perhaps because the socialist‐​green coalition government of Prime Minister Helen Clark sees 10,991 votes, largely without skills or jobs, bought and paid for with plane tickets and nurtured with welfare.



The Tuvalu story is an icon of environmental and political deceit, generated by a compliant media that have no regard for inconvenient facts. There are over 2, 000 members of the Society of Environmental Journalists–all attuned to the goings on in Marrakech–and not one of them has the gumption to see if, in fact, Tuvalu is drowning. Instead, they sit quietly, supporting yet another tired attempt to stampede world leaders into an environmental treaty that all competent scientists know has no detectable effect on world climate, even as it costs a fortune to a world fighting for its very civilization.
"
"

 **Introduction**



Although the stew that is the U.S.-China trade relationship has the potential to reach a full boil, it has been on a low simmer since before the start of the financial crisis and subsequent global economic slowdown. Despite pork bans, poultry bans, a steady stream of antidumping and countervailing duty investigations, dispute settlement judgments from the World Trade Organization, accusations of currency manipulation, admonitions regarding China’s dependence on export‐​led growth, and China’s concerns about the impact of profligate U.S. government spending on its U.S. debt holdings, the relationship has held up fairly well.



But that could all change quickly. By September 17 President Obama is required to render a decision in a potentially combustible case concerning automobile tire imports from China. Pursuant to a petition filed by the United Steelworkers of America under Section 421 of the Trade Act of 1974‐​known colloquially as the “China‐​Specific Safeguard”-the U.S. International Trade Commission has already recommended that Obama impose duties of 55 percent on imports of consumer tires from China. Under the law, the president can adopt, modify, or reject that recommendation.



Although this may sound like just another day in Washington, Obama’s decision will be consequential. It will help clarify his administration’s heretofore opaque tradepolicy objectives. It will set the tone for U.S.-China trade relations for the foreseeable future. And it will affect broader international trade relations, for better or for worse, as America honors or disavows its pledge to the Group of 20 nations to avoid new protectionist measures.



Under the statute, the president has discretion to deny import “relief” if he determines that such restrictions would have an adverse impact on the U.S. economy that is clearly greater than its benefits, or if he determines that such relief would cause serious harm to the national security of the United States. The first condition is met overwhelmingly. And, for good measure, there is a very strong argument that the second is met, too.



But at the end of the day the president is a politician, who is presumed to owe Big Labor for his election last November. Will the president do what is overwhelmingly in the best interest of the country? Or will he do what he thinks is best for himself politically? This paper provides some law and case background and then summarizes why the president should reject the recommendations of the USITC and deny import restrictions altogether.



 **The Section 421 Statute and a Brief History**



Section 421 of the Trade Act of 1974, as amended, is a special statute that applies only to imports from China. It became U.S. law as a condition of China’s accession to the World Trade Organization in 2001. The provision aimed to assuage fears about Chinese competition by establishing a special “safeguard” to deal with increased imports from China for the first 12 years after China’s entry into the WTO.1 The law will expire at the end of 2013.



The broader U.S. “safeguard” law, Section 201 of the Trade Act of 1974, authorizes the imposition of temporary trade barriers against increased imports that are a “substantial cause” of “serious injury” to American producers. The China‐​specific safeguard of Section 421, by contrast, sets a lower threshold for imposing trade restrictions. Specifically, the statute provides:



If a product of the People’s Republic of China is being imported into the United States in such increased quantities or under such conditions as to cause or threaten to cause market disruption to the domestic producers of a like or directly competitive product, the President shall, in accordance with the provisions of this section, proclaim increased duties or other import restrictions with respect to such product, to the extent and for such period as the President considers necessary to prevent or remedy the market disruption.2



Under the statute, “market disruption” exists “whenever imports of an article like or directly competitive with an article produced by a domestic industry are increasing rapidly, either absolutely or relatively, so as to be a significant cause of material injury, or threat of material injury, to the domestic industry.“3 And the term “significant cause” refers to “a cause which contributes significantly to the material injury of the domestic industry, but need not be equal to or greater than any other cause.“4



If the ITC renders an affirmative finding (which is decided by majority vote) or if there is an even split among commissioners, the affirming commissioners must submit recommendations for relief to the president and the U.S. Trade Representative within 20 days of the determination. The USTR then has 55 days to advise the president about the ITC’s findings‐​a period during which it must hold hearings on the matter and solicit views from importers, exporters, and other interested parties. It is also authorized to pursue negotiations to address the underlying market disruption with the Chinese government during this period.



Unless an agreeable settlement is reached, the president must announce import relief by the 150th day after the petition’s filing unless he determines that “provision of such relief is not in the national economic interest of the United States or, in extraordinary cases, that the taking of action … would cause serious harm to the national security of the United States.“5 If the president chooses to grant import relief, it must become effective within 15 days of his decision.



It is also important to appreciate what Section 421 is not. It is not an “unfair trade” statute. Unlike the antidumping and countervailing duty laws, a Section 421 case does not include allegations of prices at less than fair value or prices that benefit from countervailable government subsidies. The evidentiary threshold is much lower. All that is alleged‐​and all that has to be established‐​in a 421 petition is that imports from China are increasing in such a manner as to be a cause of market disruption (or threat thereof) to the domestic industry.



Section 421 is not intended to remedy any wrongdoing on the part of Chinese exporters, but is intended rather to give U.S. producers the opportunity to holler “time out!” as they catch their breath, assess prospects, and attempt to adjust to a new level of competition. Of course there are huge costs to this kind of intervention in the marketplace, thus the president is granted discretion, under the law, to deny relief if he determines that the costs to the broader economy clearly exceed any benefits to the petitioning industry. While such discretion provides some comfort that the law’s relaxed evidentiary standards won’t be routinely abused by domestic interests seeking to stifle competition, there are no guarantees that the president’s discretion will be based exclusively on considerations of the national economic interest. If there were, it would be nearly impossible to conjure a scenario in which the concentrated, temporary benefits to a specific industry receiving protection were not overwhelmed by the costs of that protection on the broader economy. Political considerations always influence decisions that lead to protection.



During the Bush administration (the first administration under which the law was in effect), there were six Section 421 cases filed by domestic parties, and in four of those the ITC found market disruption and recommended import restrictions. In each of those four cases, President Bush exercised his discretion to deny relief. Thus, trade restrictions have never been imposed under this statute.



 **Some Specifics of the Tires Case**



The tires case is noteworthy in several respects, starting with the fact that it is the first Section 421 case initiated during the Obama administration. Petitioners came to regard the law as a dead letter under President Bush, but have been anxious to test its viability under a new president, who promised last year to decide Section 421 cases “on their merits, not on the basis of an ideological rejection of import relief like that of the current administration.“17



The petition in the tires case was filed by the United Steel, Paper and Forestry, Rubber, Manufacturing, Energy, Allied Industrial and Service Workers International Union‐​the United Steel Workers, for short‐​on behalf of workers in the U.S. tire industry. However, the USW represents workers accounting for only 47 percent of U.S. tire production capacity, so most workers in the industry are not officially supporting the petition.7 Furthermore, this is the first 421 case that does not have a domestic producer as the petitioner. Out of the 10 firms determined to comprise the entire domestic tire industry, none supports the union’s petition for import restraints. Thus, this case, initiated on behalf of no producers and less than half of the industry’s workers, and given the acrimony it has engendered within the U.S. tire‐​supply chain, is probably not the kind of case that President Obama idealized when he promised to decide these issues “on their merits.”



In reaching its affirmative conclusion of market disruption in June 2009, the USITC cited the 215‐​percent increase in the volume of tire imports from China between 2004 and 2008 as a cause of material injury to the domestic industry. The conclusion of material injury was based on evidence of declining industry capacity, production, shipments, employment, wages, and financial results.8



The argument put forward by respondents in the case (i.e., various producers, exporters, and importers) during the ITC proceeding, which was ultimately rejected in the majority’s determination, is that tire production is stratified among three quality “tiers,” and that competition across tiers is mitigated. Most of the increase in imports from China was of the lowest tier, while most of the tires produced in the United States are of the top two tiers.



Furthermore, 7 of the 10 U.S. tire producers also manufacture tires in China, as well as in other countries.9 And of those 7 firms, 4‐​Goodyear, Michelin, Cooper, and Bridgestone‐​account for almost 90 percent of U.S. production. Thus, the change in composition of domestic and imported tires in the U.S. market is a function of the decisions of these U.S. producers. And it was a deliberate decision of U.S. producers to reduce production of Tier‐​3 tires‐​the lowest‐​end, lowest‐​profit‐​margin tires‐​at their U.S. plants, and increase sourcing of that tier in China and elsewhere, where lower production costs enable the realization of some profit, which in turn helps support continued production of Tier‐​1 and Tier‐​2 tires in the United States. Thus, the declining employment, production, capacity, and shipments are all attributable to intentional, conscious planning on the part of profit‐​maximizing firms.



For a law that is characterized by its champions as a tool to support our producers vis‐​à‐​vis Chinese producers and to ensure a level playing field, this test case for Obama pits American workers against American producers, and American workers against American workers. By going after Chinese producers, petitioners ensnare their own employers, as well as fellow American workers, organized or otherwise. Although the lightning rod is China (with all of the negative perceptions that have been cultivated about its trade practices), this case has little to do with China per se, and everything to do with organized labor begrudging U.S. producers for pursuing profit‐ maximizing strategies in a globalized world. In seeking sanctions, petitioners are asking Obama to indict globalization.



 **Whom Will Protectionism Help, and How**



Duties on imports of tires from China are more likely to lead to greater production in other developing countries than to greater production in the United States. U.S. producers have chosen to outsource production of their lower‐​tier tires to China because producing those tires in that location makes the most sense economically. But raising the costs of producing in China by imposing trade restrictions would not make U.S. production more attractive. It would not bring back U.S. jobs. It would make Indonesian or Mexican or Brazilian production more attractive, and would likely divert jobs from China to those countries.



According to data compiled by the ITC staff, the average unit price (based on the customs value) of a tire imported from China in 2008 was $38.98.10 A 55‐​percent tariff would drive up the unit value to $60.42. But, in 2008, U.S. producers sold 159 million tires, valued at $11 billion, for an average price of $68.60.11 Factoring in mark‐​up of the Chinese price, it is reasonable to conclude that prices of American‐ and Chinese‐​produced tires might retail for about the same price. But that outcome is highly unlikely to be incentive enough for globalized tire producers to divert production from China to the United States. Instead, producers are much more likely to shift production to Mexico, Brazil, or Indonesia, where the unit prices in 2008 (based on customs value) were $56.26, $48.93, and $32.10, respectively.12



Furthermore, the ITC’s recommended remedy would be in place for three years. The statute expires in four years. What kinds of changes should be expected during the interim that would make the United States a more cost‐​effective place to produce Tier‐​3 tires, or any tires for that matter? There are no changes‐​short of technological advances that raise productivity and reduce the demand for labor‐​that could make the United States a better place to produce tires. But this case is about jobs and nothing else, so even that outcome wouldn’t satisfy petitioners. Three years of “relief” will do nothing but perhaps defer the day of reckoning, while imposing heavy costs on the rest of the economy, taxing our relationship with China, and further sullying America’s international standing.



 **Adverse Economic Impact Is Clearly Greater Than any Benefits**



Formal economic models, testimony, anecdotes from representatives of industries in the tire‐​supply chain, and common sense analysis all reveal an excessive cost burden on the economy from the proposed remedy of 55‐​percent duties in year one; 45‐​percent duties in year two, and; 35- percent duties in year three.



In their dissenting opinion, ITC Vice Chairman Daniel R. Pearson and Commissioner Deanna Tanner Okun concluded:



[W]e find that imposing a trade‐​restrictive quota or tariff on the subject imports will be far more likely to cause market disruption than to alleviate it for domestic producers who have already undertaken significant strategic adjustments to adapt to a changing global market.13



Indeed, economist Thomas Prusa estimates that “the tire manufacturing industry will experience little to no job creation as a result of the tariff. Under the best‐​case scenario more than a dozen jobs will be lost for every job protected.” Prusa estimates a net loss of at least 25,000 U.S. jobs if the recommended tariff is imposed. Under the best case, Prusa finds that higher prices and other inefficiencies stemming from the proposed remedies would sap U.S. consumers of $600 to $700 million per year, translating to an annual cost of $300,000 for every job “protected” in the tire industry.14



According to a statement of the U.S. Tire Industry Association, which represents all segments of the tire industry, including those that manufacture, repair, recycle, sell, service, or use new or retreaded tires, and also those suppliers or individuals who furnish equipment, material, or services to the industry:



Our members, by directly importing or contracting with suppliers, are meeting the demands of a segment of the tire consumer market for lower‐​cost tires. No manufacturing uptick would satisfy this product segment, but instead could create a need for product allocation, resulting in shortages and outages. In the best of times, such occurrences are troubling, but in today’s climate, this could inflict severe financial harm on many retailers and on the motoring public.15



Consumer groups and other organizations have also expressed safety concerns about the impact of higher‐​priced tires on increasingly‐​pinched consumers. The likelihood that an increasing number of consumers will forego the replacement of old and worn‐​out tires presents a whole new category of risk and costs that are difficult to quantify economically.



If President Obama imposes trade restrictions in this case‐​regardless of whether those restrictions are as severe as the ITC’s recommendation or if they are milder‐​the United States will have blatantly violated its commitment to the G-20 earlier this year to avoid new invocations of protectionism. That would be a colossal mistake that simultaneously undermines U.S. credibility on trade and invites other governments to indulge their own protectionist lobbies. The consequences for world trade could be severe. There should be no doubt that the demonstration effect would not only influence other governments toward intervention, but would also encourage other U.S. interests to pursue their own protection under Section 421.



The fact that President Bush rejected the ITC’s recommendations to impose restrictions four times reinforces the perspective held by the Chinese government that the imposition of trade restrictions under Section 421 is firmly a matter of presidential discretion. Unlike antidumping or countervailing duties, which run on statutory autopilot without requiring the president’s attention or consent, Section 421 explicitly requires the attention and participation of the U.S. president. In other words, although there are over 90 outstanding U.S. antidumping and countervailing duty orders against various Chinese products, none of them is considered to reflect the direct wishes of the U.S. president, and thus none of them rise to the level of a potentially explosive trade dispute.



Technically, if the United States imposes restrictions under Section 421, the Chinese are not entitled to formally retaliate. But that’s cold comfort when it’s quite obvious that trade restraints under Section 421 will no doubt be considered by the Chinese to be a directive of the U.S. president, and thus the offense taken and the consequences wrought could be profound. U.S. industries across the manufacturing spectrum have written to President Obama, urging him not to impose restraints in this case for fear that they will bear the brunt of the costs through lost export sales. This is a real possibility.



As to the question of national security, the prospect of a spiraling trade war with China, if duties are levied and retaliation ensues, will strain the commercial ties that have been successfully cultivated over the past few decades and will increase the risk that China becomes less helpful on crucial matters of U.S. foreign and security policy.



 **Conclusion**



Through the first eight months of his tenure, the president has avoided making any decisions of consequence on matters of trade policy. While his actions have not been conclusively protectionist, his tepid rhetorical endorsements of trade and his conditional repudiations of protectionism have sown doubts at home and abroad about the direction of U.S. trade policy. A decision to reject trade restraints in the tires case would be reassuring to a world that is struggling to grow out of recession.



The stakes appear to be much higher for Obama than they were for Bush. The unions feel that they have earned the president’s support and are more emboldened in their position now than in the past. Bush didn’t win the near‐​unanimous support of organized labor leaders in his elections, as Obama did. Nor did Bush promise to get tough on Chinese trade practices, as Obama did. Instead, Bush set the precedent of denying relief‐​and he did it four times.



The USITC’s recommendation of a 55‐​percent tariff is a remedy far more restrictive than the quota sought by the USW. The president, then, may be tempted to offer what he might think is a compromise solution, of lower duties or a tariff‐​rate quota. But the costs of any protectionism at this time and under these circumstances could unleash a protectionist backlash in the United States and around the world. It would be far less costly for the president to reject trade restraints altogether and to capitalize on his earned credibility by moving the trade agenda forward.



 **Notes**



1 Daniel Ikenson, “Bull in a China Shop: Assessing the First Section 421 Trade Case,” Cato Free Trade Bulletin no. 2, January 1, 2003, p.1.



2 19 U.S.C. § 2451(a).



3 19 U.S.C. § 2451(c)(1).



4 19 U.S.C. § 2451(c)(2).



5 19 U.S.C. § 2451(k)(1). Note that in cases in which the ITC is evenly split, the president has complete discretion about whether or not to accept the affirming commissioners’ recommendations for relief.



6 Barack Obama (letter to Cass Johnson, President of the National Council of Textile Organizations, October 24, 2008), http://​www​.ncto​.org/​n​e​w​s​r​o​o​m​/​p​r​2​0​0​8​1​0​2​9.pdf.



7 U.S. International Trade Commission, _Certain Passenger Vehicle and Light Truck Tires from China, Investigations No. TA- 421–7_ , Publication 4085, July 2009, p. I-14.



8 For a full discussion of the statutory questions of increasing imports, material injury, and causation, see USITC, _Certain Passenger Vehicle and Light Truck Tires from China_ , pp. 11–29.



9 Those companies are Toyo, Yokohama, Pirelli, Michelin, Goodyear, Cooper, and Bridgestone. USITC, p. IV-3.



10 USITC, Table IV-4, p. IV-10.



11 Ibid., Table III-5, p. III-13.



12 Ibid., Table II-1, p. II-4.



13 Ibid., p.45.



14 Thomas J. Prusa, “Estimated Economic Effects of the Proposed Import Tariff on Passenger Vehicle and Light Truck Tires from China,” American Coalition for Free Trade in Tires (submission to the International Trade Commission, July 26, 2009).



15 Tire Industry Association (position statement, June 17, 2009), http://​www​.tirein​dus​try​.org/​p​d​f​/​n​e​w​s​_​a​r​c​h​i​v​e​s​/​press release061709.pdf.
"
"The British boats were outnumbered by about eight to one by the French. Before long there were collisions and projectiles were thrown. The British were forced to retreat, returning to port with broken windows but luckily no injuries. The conflict behind this skirmish between British and French fishers in the Bay de Seine at the end of August 2018 was quickly dubbed the “scallop war” in the press. The French had been trying to prevent the British scallop dredgers from legally fishing the beds in French national waters. But the incident exposed tensions that have been simmering for many years. Under the European Union’s Common Fisheries Policy (CFP), the British fishers had the legal right to fish in these waters, as did any boats from an EU member state. The complication came from a French regulation that prevented local boats from fishing these waters between May 16 and September 30 each year, in order to allow stocks to recover from the annual harvest. But under the CFP, an EU country has no authority to prevent another member state’s fleet from fishing its waters. This quirk of the CFP left the French fishers unable to dredge for scallops until October 1, and forced to stand by and watch while fleets from other countries harvested what they saw as a French resource from French waters. When the British boats arrived, the French fishers took on the role of guardians of their resource, actions they believed were justified but viewed by the British fishing industry as illegal. This incident in a small corner of a shared EU sea was settled within a few weeks thanks to a new agreement on how the two countries would share the scallop harvest. But the underlying tensions that the CFP has created over the sharing of national resources go much deeper, with a sense that the rules don’t allow for a fair use of the seas. This sense of unfairness was obviously expressed in the role that fishing played in Britain’s decision to leave the EU. Campaigners promised that “taking back control” of British waters would enable the country to revive its long-declining fishing industry and the communities that rely on it. Yet regardless of the impact the CFP has had on Britain’s fishers, their future after Brexit depends very much on any future trade deal that the government negotiates with the EU. And the history of how Britain has responded to conflicts over fishing rights far bigger than the scallop war doesn’t bode well for the industry.  Research into fishing activity shows that the decline of the British fishing industry began well before the creation of any European fishing policy. In fact, its ultimate origins can be traced to a surprising source: the expansion of the railways in the late 19th century.  Trawling, under the power of sail, had existed for more than 500 years. But without refrigeration, fish could only be delivered for sale to areas close to the ports. The coming of the rail network meant that fish could be transported inland to big towns and cities. To further meet this growing demand, steam trawlers started to replace sail trawlers from the 1880s onwards. The power of these steam vessels greatly increased the scope of trawling and allowed them to trawl for longer and further away from port while towing larger nets. British steam trawlers ventured further away from Britain in search of fish, with the fishing grounds expanding to areas as far as Greenland, north Norway and the Barent’s Sea, Iceland and the Faroe Islands. But as early as 1885, concerns were raised that this advance in technology was having a negative impact on both fishing stocks and their habitat. Evidence from records of fishing activity show that this improvement in technology and the increased size of the fishing fleet was squarely behind the increase in landings. The fishing boom that the railways had unleashed proved unsustainable, and the resulting overfishing would ultimately send the industry into a long-term downturn. After decades of more and more fishing, landings eventually started to decline after World War II, a trend that continued through the second half of the 20th century and into the new millennium. To compensate, the size and power of the fleet continued to increase as more effort was required to catch ever-scarcer fish. From the late 1950s, the amount of fish landed per unit of power declined at a faster rate than fish landings, as the fleet continued to expend more and more effort to maintain the size of catches. However, this effort was all in vain and by 1980 catches had declined to their lowest point in a century. Overfishing wasn’t the only reason for decline, however. The falling fish stocks combined with the improvements in the range and power of the fleet in the post-war years led Britain’s fishers to seek new waters, with more boats moving further away from the UK to catch enough fish to meet domestic demand. And this long range trawling brought the UK fleet into conflict with Iceland.  British fishers had fished these waters from the 15th century. However, Iceland’s fishing industry began to resent this as the steam trawlers began fishing off Iceland in the late 19th century. It led to accusations that British trawlers were damaging the fishing grounds and depleting stocks. In 1952, Iceland declared a four-mile zone around their country to stop excessive foreign fishing, although fish don’t stick to human-made boundaries and stocks could still be depleted outside of this zone. Iceland’s decision drew a response from the UK, which banned the import of Icelandic fish. As a major export market for Iceland’s most important industry they hoped that this would bring them to the negotiating table. In 1958, against a background of failed diplomacy, Iceland expanded this zone to 12 miles and banned foreign fleets from fishing in these waters, in defiance of international law. It led to the first of what became known as the Cod Wars – an act in three stages that lasted nearly 20 years.  During the first Cod War, Royal Navy frigates accompanied the UK fleet into Iceland’s exclusion zone to continue their fishing. A game of cat-and-mouse ensued between the Icelandic Coastguard vessels and the British trawlers. In response to attempts to seize them, the trawlers rammed the coastguard vessels and the coastguard threatened to open fire, although major incidents were avoided. In 1961, the two countries eventually came to an agreement that allowed Iceland to keep its 12-mile zone. In return, the UK was granted conditional access to these waters.  By 1972, however, overfishing outside of this limit had worsened and Iceland extended its exclusive zone to 50 miles and then three years later, to 200 miles. Both these moves led to more clashes between Icelandic trawlers and the Royal Navy escort ships, respectively dubbed the second and third Cod Wars.  Icelandic Coastguard vessels towed devices designed to cut the steel trawl wires (hawsers) of the British trawlers – and vessels from all sides were involved in deliberate collisions. Although these clashes were mainly bloodless, a British fisher was seriously injured when he was hit by a severed hawser and an Icelandic engineer died while repairing damage to a trawler that had clashed with a Royal Navy frigate.  In January 1976, British naval frigate HMS Andromeda collided with Thor, an Icelandic gunboat, which also sustained a hole in its hull. While British officials called the collision a “deliberate attack”, the Icelandic Coastguard accused the Andomeda of ramming Thor by overtaking and then changing course. Eventually NATO intervened and another agreement was reached in May 1976 over UK access and catch limits. This agreement gave 30 vessels access to Iceland’s waters for six months.  NATO’s involvement in the dispute had little to do with fisheries and a large amount to do with the Cold War. Iceland was a member of NATO, and therefore aligned to the US, with a substantial US military presence in Iceland at the time. Iceland believed that NATO should intervene in the dispute but it had up until that point resisted. Popular feeling against NATO grew in Iceland and the US became concerned that this strategically important island nation – which allowed control of the Greenland Iceland UK (GIUK) gap, an anti-submarine choke point – could leave NATO and worse, align itself with the Soviets.  Amid protests at the US military base in Iceland demanding the expulsion of the Americans, and growing calls from Icelandic politicians that they should leave NATO, the US put pressure on the British to concede in order to protect the NATO alliance. The agreement brought to an end more than 500 years of unrestricted British fishing in these waters.  The loss of these Atlantic fishing grounds cost 1,500 jobs in the home ports of the UK’s distant water fleet, concentrated around Scotland and the north-east of England, with many more jobs lost in shore-based support industries. This had a significant negative impact on the fishing communities in these areas.   The UK also established its own 200-mile limit in response to Iceland’s exclusion zone. These limits were eventually incorporated in the 1982 United Nations Convention on the Law of the Sea, giving similar rights to every sovereign nation. The creation of these “exclusive economic zones” (EEZ) was the first time that the international community had recognised that nations could own all of the resources that existed within the seas that surrounded them and exclude other nations from exploiting these resources.  The UK now owned the rights to the 200-mile zone around its islands, which contained some of the richest fishing grounds in Europe but up until this point the principle of “open seas” had existed, with Britain its most vocal champion. Fishing nations, had fished the high seas within 200 miles of their own and others coasts for centuries and now were restricted to their own. Britain’s Exclusive Economic Zone (EEZ), however, wasn’t that exclusive.  On joining the European Economic Community (the forerunner to the EU) in 1972, the UK had agreed to a policy of sharing access to its waters with all member states, and gaining access to the waters of other countries in return. The UN convention effectively gave the EEC one giant EEZ.  The UK government was willing to enter into the agreement as fisheries were one part of overall negotiations that would allow the UK to export goods and services to the European continent with significantly reduced trade barriers.  Although the fishing industry is of high local importance to fishing communities, it is relatively unimportant to the UK economy as a whole. In 2016, the UK fishing industry (which includes the catching sector and all associated industries) was valued at £1.6 billion, against £1.76 trillion for the UK economy as a whole – or just under 0.1%. The UK’s trade with the EU, both import and export, stands at £615 billion a year in comparison. In 1983, the Common Fisheries Policy was adopted, introducing management of European waters by giving each state a quota for what it could catch, based on a pre-determined percentage of total fishing opportunities. This was known as “relative stability” and was based on each country’s historic fishing activity before 1983, which still determines how quotas are allocated today.  The formula that the EEC adopted, based on historic catches, is one of most contentious parts of the CFP for the UK. Many fishers have stories of the years running up to 1983, where foreign vessels increased their fishing activity in UK waters in order to secure a larger share of these fish in perpetuity. Although there is little evidence to support these views, it demonstrates the level of distrust in both the system, and foreign fishers, from the outset. As a result, only 32% of fish caught in the UK EEZ today is caught by UK boats, with most of the remainder taken by vessels from other EU states, Norway and the Faroe Islands (who have also joined the CFP). Therefore, non-UK vessels catch the remaining 68%, about 700,000 tonnes, of fish a year in the UK EEZ.
In return, the UK fleet lands about 92,000 tonnes a year from other EU countries’ waters.  Joining the CFP did not cause a decline in UK fish landings. However, in its early days, it did nothing to stop it. Fish landings continued to decline – and along with this, the industry itself contracted, using improved technology to offset the decline in stocks. Through the 1980s and into the early part of this century the imbalance – enshrined in the relative stability measure of the CFP – has led to the view that the CFP doesn’t work in the UK’s interests. Rather it allows the rest of the EU to take advantage of the country’s fish stocks.  The CFP’s quota system, while credited for helping the industry survive (and even reverse the collapse in fish stocks), is now seen as burdensome and preventing further growth.  


      Read more:
      Brexit: what the UK fishing industry wants


 A recent academic analysis of the current performance of the CFP showed it was not improving the management of the fish stock resources in any of its 17 criteria and was actually making things worse in seven areas. For example, a 2013 reform of the CFP introduced the landing obligation, the so-called “discard ban”, that was designed to stop vessels discarding fish (bycatch) caught alongside the species they were targeting. Environmentalists, and campaigns backed by celebrities such as Hugh Fearnley-Whittingstall, have long voiced concerns over incidents of bycatch being dumped by fishers operating under the quota system. This policy is now seen as potentially disastrous by some representatives of Britain’s North Sea fishing fleet, as so many different types of fish live in the waters and bycatch is common and often unavoidable. They are concerned that boats would be forced to fill their holds with commercially worthless fish and return to port early. Or by exhausting their quota for some species early in the season, they would be forced to stay in port for the rest of the year, despite having quotas available for other species. Evidence given to the House of Lords suggests that this situation has not arisen as non-compliance and a lack of enforcement has undermined the discard ban.  When we interviewed fishers in north-east Scotland in 2018, we found many feared such blanket management across the entire EU would continue to damage their industry because it simply does not take into account the local environment that they work in. The depth of feeling among the UK fishing community was illustrated by the voting figures for the EU referendum in June 2016.  In Banff and Buchan, the constituency in Scotland containing Peterhead and Fraserburgh – the largest and third largest fishing ports in the UK respectively – 54% of people voted to leave the EU, with the size of the fishing industry given as the reason for this result. The result compared to 52% for the whole of the UK and just 38% for Scotland. A survey of members of the UK fishing industry before the vote indicated that 92.8% of correspondents believed that doing so would improve the UK fishing industry by some measure.  


      Read more:
      British fishermen want out of the EU – here's why


 But will Brexit really bring the fishing revival so many have promised and hoped for? British politicians have promised a renaissance in UK fishing after leaving the EU. A Fisheries Bill was launched by the environment secretary, Michael Gove, with an aim to “take back control of UK waters”. However, no definitive plan to remove the UK from the CFP in a transition deal has been made, nor has the industry been given any answers on future access for EU vessels, the apportionment of any new quota – if indeed the quota system remains as it is – the rules that they will be operating under, or even a date on which this will come into effect.  The UK government is seen by many in the fishing industry to be acting against their interests in pursuit of wider goals, for example by using the industry as a bargaining chip in wider UK trade negotiations with the EU. The fishing industry’s distrust of the government has a long tail: many believe they were sacrificed in 1973 by the then prime minister, Edward Heath, in order to secure access to the single market.  Ironically, despite the fishing industry’s support for Brexit and the popular campaign promises, our research suggests fishers don’t simply want to close British waters to European fleets. We interviewed people who were sympathetic to their fellow fishers from abroad and did not wish to see businesses and livelihoods lost. They favour a re-balancing of quotas over time to allow EU vessels to adapt to the change, with all vessels having to adhere to UK rules. This would avoid any situations similar to the Scallop War by ensuring that all vessels with a quota have to abide by local restrictions.  The EU is the main export market for UK fish and fisheries products accounting for 70% of UK fisheries exports by value. Valued at £1.3 billion, this trade far exceeds the £980m value of fish landed in the UK, due to the added value from the processing sector. Some of the remaining 30% of exports that go to countries outside of the EU are governed by trade agreements negotiated by the EU that reduce trade barriers. So the single market, and additional trade agreements, are crucial to the success of the UK fishing industry. This reliance on trade into the EU puts the industry in a position where unilaterally preventing access to UK waters would likely be met by reciprocal trade barriers and tariffs. This would increase the cost of their product, while reducing access to their biggest market. The question for the government, then, is how to balance a political issue against an economic one? The issue centres on the word “control”. If the UK has control of its waters that would simply mean that its government has the power to decide on anything from keeping fishing within UK waters purely for UK vessels, to remaining in or re-entering the CFP, or all points in between. Until the deals are negotiated and signed, the industry will remain in a limbo that has reopened old wounds and reignited distrust in the UK government."
"The UK’s efforts to develop facilities to remove carbon emissions from power stations took a step forward with news of a demonstrator project getting underway at the Drax plant in north Yorkshire. Where most electricity carbon capture projects have focused on coal-fired power, the Drax project is the first to capture carbon dioxide (CO₂) from a plant purely burning wood chips – or biomass, to use the industry jargon.  This so-called Bio Energy Carbon Capture and Storage (BECCS) demonstrator is only a pilot project; it covers just a tiny proportion of emissions from the 4GW plant and Drax has no plan yet for storing the captured gas. But coming after a decade in which various other UK carbon capture initiatives and government competitions have ended up scrapped, it is certainly progress.  Some specialists believe this technology has a bright future in the UK, envisaging big wood-fired power plants whose carbon emissions are prevented from returning to the atmosphere. Other countries are looking at it seriously, too, and Drax and its partners have been talking up the prospect of eventually achieving “negative emissions” at the plant in Yorkshire. But this is fundamentally misleading. Without wanting to reject carbon capture out of hand, it is time to get realistic about what can be achieved with this technology.   The logic of the negative emissions argument is that burning wood is “carbon neutral” because trees absorb CO₂ from the atmosphere in the first place, and you are simply releasing it back. When you combine this with a carbon capture facility, it is argued, you are therefore removing CO₂ from the atmosphere overall.    But this view considers the process of burning wood in isolation. It ignores, just as an example, a wider chain of activities including planting and harvesting the trees, converting the wood into chips and then shipping them to the power plant – not to mention storing and using the captured CO₂ once the wood has been burned.  There is also a misconception that burning wood produces only CO₂ – a BBC News reporter was saying as much the other day. But if this were the case, we would not need to separate CO₂ from other flue gases. Some of the carbon in the wood could become carbon monoxide, for instance, which, if not captured, would indirectly contribute to levels of greenhouse gases in the Earth’s atmosphere. The process also produces other noxious emissions, such as volatile organic compounds and oxides of nitrogen, which are responsible for acid rain. Too many people also tend to see wood as better than oil or coal because the amount of CO₂ produced by burning a given unit is much lower for wood. But this overlooks the fact that you get considerably more heat from burning a unit of oil or coal than from wood. In other words, you have to burn much more wood to produce the same amount of heat, so the carbon emissions are actually much more than they appear. This leads people to greatly underestimate the amount of land we will need for trees if biomass power is to become a much bigger part of the energy mix. The Drax plant alone uses more wood than the UK produces every year, for instance.  The blinkered thinking around carbon capture also goes way beyond biomass power plants. There are now 43 carbon capture facilities either operating or in development – ten in the US, followed by Canada and Norway. Very few are attached to power plants so far, with most instead removing CO₂ from oil fields or gas processing plants. But generous new subsidies in the likes of the US are making the industry optimistic about carbon capture in the power sector regardless of which feedstock is burned.  Across the board, there is the same tendency to ignore the carbon emissions in everything from coal/gas/oil extraction to CO₂ storage. We also hear very little about the solvents traditionally used to separate the CO₂ from the rest of the combustion gases. These amines are highly corrosive and bad for the environment, plus there are CO₂ emissions from producing them in the first place.  My point is not that we should be against carbon capture plants; the technology is much needed, and pilots like the one at Drax are important for possibly scaling up the process and measuring what is achievable. But when scientists conduct these measurements, they need to consider the complete chain to look at all of components involved – including, in the case of wood, the land used for the trees, and the consequences of deforestation.  We also need much more discussion and research into which solvents are the most environmentally friendly for gas separation: Drax claims to be using a new solvent with environmental benefits, so it will be interesting to see what the results look like down the line.  Clearly, our society needs energy. We would never be able to sustain ourselves if we eliminated fossil fuels completely. Capturing carbon dioxide emissions certainly has a role to play in the energy systems of the future, but it needs to be appraised in a way that looks at the whole picture.  The reality is that if the UK and EU are serious about being completely carbon neutral by 2050, it will have to use a mixture of methods and cut back more aggressively on the emissions being produced in the first place. This is always going to be more efficient than any attempts to put the genie back in the bottle afterwards. Regardless of what anyone says about technological solutions to the carbon problem, it is almost impossible to get away from this basic fact."
"**A man who killed and dismembered a retiree in a bid to steal his money dumped his victim's remains in a badger sett, a court has heard.**
Daniel Walsh, 30, is accused of murdering Graham Snell, 71, whose body parts were found in various locations around Chesterfield.
On the last day he was seen alive, Derby Crown Court heard, Mr Snell told police the defendant had been stealing from his bank account.
Mr Walsh denies murder.
Prosecutor Peter Joyce QC told jurors Mr Snell went to a police station on 19 June last year and told officers he had ""a problem with a man who comes and stays at my house without being invited"".
By the time officers went to his home in Marsden Street the next morning, Mr Joyce said, ""Graham Snell was lying dead in his house"".
""Also in his house but not answering that door was Daniel Walsh,"" he said.
The next day, the court heard, the defendant purchased 10 rubble sacks and two large saws ""to cut through the bones of Graham Snell"".
On 24 June, jurors were told, Mr Walsh loaded two or three large black bags containing ""many parts of Mr Snell's body"" into a taxi.
He then travelled to Barbon Close where he ""buried or pushed"" them into various parts of a badger sett, Mr Joyce said.
Three days later, he once again travelled by taxi to dump parts of the victim's torso in communal bins at a block of flats.
They were discovered on 2 July, the court heard.
Mr Snell's head and arms were eventually found in a wood ""a little way away"" in February this year, the prosecutor said.
""What he did was awful and what he did was murder,"" Mr Joyce said.
""He killed him, he chopped him up, he fed him to the badgers, he put... his torso in a communal bin.
""It was murder to get his hands on this old, retired man's money and just dispose of him as a piece of rubbish.""
The jury was also told Mr Walsh had previous convictions for stealing Â£5,000 from Mr Snell in 2009 and assaulting him in 2014.
Jurors were told the trial had originally begun in March but was stopped due to Covid-19.
_Follow BBC East Midlands on_Facebook _, on_Twitter _, or on_Instagram _. Send your story ideas to_eastmidsnews@bbc.co.uk _._"
"A new expansion has added environmental challenges to Sid Meier’s Civilization VI, the latest in a popular series of strategy video games that has been running since the 1990s. The expansion – called Gathering Storm – adds new features to the game, most notably anthropogenic climate change and natural disasters.  The game involves developing a civilisation from its humble beginnings in the Stone Age to nowadays and beyond, while choosing from a vast array of technologies and cultural policies. As the game and the ages progress, your energy choices become increasingly important. Indeed, Gathering Storm is based on a simple model of global warming wherein CO₂ emissions from energy sources induce sea level rise, as well as more frequent and intense extreme weather events such as droughts and storms. In turn, these can have potentially devastating effects on your cities and units, pushing the player to think about different adaptation strategies such as flood barriers for coastal cities.  The game even progresses into a “future era”, where players are offered options like carbon capture and storage technologies or “seasteads” to house segments of the population. From early on, this new expansion compels players to think about some of the potential long-term consequences of actions that may offer short-term benefits. One example would be chopping down forests to accelerate production or convert land for other uses which, in the long run, renders a city more vulnerable to flooding and reduces the carbon sink capacity of your civilisation. When asked about whether Gathering Storm was somewhat of a political statement, the lead developer, Dennis Shirk, remained largely agnostic: “No, I don’t think that’s about making a political statement. We just like to have our gameplay reflect current science.” It is certainly true that the game does not coerce players into taking any particular pathway, yet it does include a “World Congress” in which climate or deforestation treaties and humanitarian aid can be ratified. We would also argue that the very inclusion of anthropogenic climate change and an associated system of incentives and punishments is inherently a political act. Moreover, in the social studies of science, what one considers to be “current science” has political ramifications. In the case of Gathering Storm, for example, in most scenarios a player could probably continue to be a “free rider” and rely solely on technological solutions. That is only possible because those technologies are known in advance and players are given virtually perfect information on the different stages of climate change and its effects. One of the consequences is that the game essentially eliminates the very uncertainty which is inherent to the “current science” on climate change and conveys a sense of technological optimism whereby innovations alone can sustain human prosperity. We are not suggesting that the developers are necessarily liable or even responsible for promoting these views. Rather we wish to illustrate how different depictions of the future can restrict or encourage certain courses of action. The developers could have chosen to make the effects of climate change and access to mitigating technologies more random (although we do not know how difficult that would be to implement in practice nor its effects on gameplay). In contrast to this incidentally optimistic outlook, there is an interesting Polish video game by the name of Frostpunk. Frostpunk is set in a dystopian alternate reality in which a volcanic event has triggered a colossal global ice age. The game’s primary scenario consists of surviving the winter – which gets incrementally colder as time progresses – in “New London”: a settlement of survivors clustered around a large coal-powered generator. The player must choose between a number of difficult policies and options to ensure the survival of the population. These include 24 hour shifts, child labour, corpse disposal strategies and, more drastically, whether to welcome refugees or refuse them entry.  While Frostpunk does not directly address the issue of anthropogenic climate change, it evokes extreme scientific scenarios (from the 1970s and 1980s) of global cooling and nuclear winters. The game also takes place in what we understand is Victorian Britain, epitomising the industrial revolution and the onset of the new geological era we now live in: the Anthropocene.  Both these games go a long way in engaging and educating their players on climate change, forcing them to deal with the kinds of political and ethical trade-offs that exist in real world decision-making. We highly encourage these innovations, not just in video games but more broadly in bridging the gap between science and the digital arts.  In the academic journal Environmental Communication, we argue that science and the humanities (including the arts) need to work together in the case of complex issues such as climate change, so as to better communicate scientific thinking and its political ramifications. Video games – as interactive and playful products – offer truly exceptional opportunities to do just that. We welcome these initiatives with open arms, so long as they remain responsible and stimulate critical thinking."
"

Ultra‐​orthodox Jews in heavy beards and heavier black coats pray for hours each day at Jerusalem’s Western Wall, even under a sweltering summer sun. Each year, Shiite Muslims whip their backs bloody with chains during the religious holiday of Ashura. Religious vegetarians in Phuket, Thailand, similarly drive knives and skewers through their cheeks.



From an outsider’s perspective, religious displays of self‐​inflicted pain can seem pointlessly barbaric. But many anthropologists and evolutionary psychologists believe they have an important function: to facilitate collective action by requiring members to send a costly, hard‐​to‐​fake signal of commitment to the group’s common creed.



The same impulse, in a rather less impassioned form, seems to animate the Democrats’ climate change bill. Coordinating international political action to achieve significant reductions in carbon emissions is a collective action problem of grand, global scale. One way to achieve and maintain such coordinated effort is to detect and punish shirkers. (Governments keep money rolling into their treasuries by threatening tax dodgers with jail.) However, there is no world government with the power to bring wayward nations into line, no world‐​ranging whip to keep countries pulling in time.





The Democrats’ cap‐​and‐​trade bill is stalled in legislative limbo because Americans are far from united about its merits.



This is the glaring flaw in plans for carbon taxes and cap‐​and‐​trade regimes: The world’s wealthy nations may now be willing to paddle their boat upstream, but if the developing world won’t row along with them, if they insist on a free ride, the boat is going nowhere.



Yet there are other tricks for encouraging cooperation and weeding out “free‐​riders.” Consider the self‐​flagellating Shiites and face‐​piercing Thai vegans. These are extreme examples of a cooperation‐​enabling strategy that game theorists call “costly signaling.” Those who display an unflinching devotion to even the most burdensome rules of common life are more likely to pull their weight, to uphold their end of a deal. Talk is cheap, but the willingness to pay a price signals to others the commitment of a real team player.



President Obama would like to walk into the climate‐​change talks in Copenhagen this December flashing a clear signal that America is willing to pay a price in the fight against carbon and its depredations. Indeed, the best one can hope from the climate legislation languishing in Congress is that, if passed, it will put the world on notice that the United States, the Earth’s greatest per‐​capita carbon font, can be trusted to pull its weight in a global climate deal.



The signal would certainly be costly. According to the Congressional Budget Office, the Waxman‐​Markey cap‐​and‐​trade scheme passed by the House would reduce GDP growth between .03 percent and .09 percent per year for the next 40 years. That may not sound like much, but annual growth rates, like annual interest rates, are compounding, which means that the cost grows considerably over time. At the conservative .03 percent annual penalty, the CBO estimates the U.S. economy in midcentury will be short more than $300 billion a year compared with a future without Waxman‐​Markey.



What would Americans get in return? Nothing, nada, zip, zilch—unless most of the world plays along. As the CBO put it: “As long as a significant fraction of the world did not adopt similar policies, some of the reductions in the United States would probably be offset by increases in emissions elsewhere.” That is to say, if countries like India and China won’t agree to (and, more important, stick with) painful cuts that will slow their steady rise from poverty, American sacrifice will do next to nothing to combat the threat of melting ice caps and a more livable Canada.



Costly signals can make sense if they deliver the benefits of cooperation. Won’t proof of our faith help skeptical governments in the developing world see that international cooperation is possible after all? It’s unlikely.



The Democrats’ cap‐​and‐​trade bill is stalled in legislative limbo because Americans are far from united about its merits. It would be reasonable for international players to suspect that an American electorate unhappy with the costs of a future carbon cap might have a change of heart. And then there’s the bill itself: a patchwork of exemptions, subsidies, and special favors. If political horse‐​trading produced something so convoluted from the start, it is fair to assume that it will become even more compromised as time goes on, leaving the U.S. unable to actually meet the legislation’s aims. Most important, a costly signal clinches trust only among those on the same wavelength. Overheated ultra‐​orthodox Jews and lacerated Shiite Muslims probably don’t much impress each other. Likewise, the signal broadcast by the willingness of wealthy nations to cut their carbon emissions may fail to impress poorer counties with fundamentally different priorities. They are not free‐​riding if they never asked to be in the boat.



It is hard to see the point of legislation that promises certain costs and improbable benefits. Still, there could be a point. Many Americans would find profound meaning in passing legislation like Waxman‐​Markey and gladly bear its costs—even if it does little to secure international cooperation, and even if it does nothing to slow global warming. The law would nevertheless speak to what Americans value, what we aspire to, who we are, what we’re about. It would say that we’re not so bad, that we repent our industrial sins, that over here we know full well that green is the new black.



Alas, this is not a statement of faith most Americans are prepared to make, or a cost they are prepared to pay. They should not be asked to don a green hair shirt just to show the world that some of us care.
"
"The Arctic global seed vault has reached the milestone of having 1m varieties stored in its deep freeze. The new deposits are being made after unexpected flooding of its entrance tunnel in 2017 prompted an upgrade. Seeds from 60,000 crop varieties from across the world are being placed in the vault to back up those held in other seed banks.  The €9m (£7.5m) underground facility in the Norwegian archipelago of Svalbard opened in 2008 as a “failsafe” store. But the unexpectedly rapid pace of global heating led to melting of the permafrost that had encased it. Now, a €20m refurbishment by the Norwegian government means the vault is secure for the future and “absolutely watertight”, according to officials. The destruction of nature means vital diversity of crops and their wild relatives are being lost, at a time when the impact of the climate emergency means new varieties are needed to cope with changing weather and pests. Seed banks can also be destroyed by power loss and war, as happened in Aleppo, Syria, making the Svalbard vault crucial. Tuesday’s deposits, from 36 institutions, are the most diverse and include seeds of 27 wild plants from Prince Charles’s Highgrove estateas well as seeds of the candy roaster squash, which are being deposited by the Cherokee Nation in the US. Wild emmer wheat, known as the “mother of wheat” when it was discovered in 1906, is being deposited by Haifa University in Israel, alongside potato varieties from Peru and other crops from Mongolia, Morocco, Myanmar and New Zealand. Each sample contains roughly 500 seeds. The Svalbard vault, which is carved into solid rock, houses samples of about 1,050,000 crop varieties from 5,000 species. This represents two-fifths of the estimated 2.4m varieties in the world, and the vault has plenty of room to accommodate them. “Crop diversity is an essential basis of food production,” said Hannes Dempewolf, a scientist at Crop Trust, which operates the vault alongside the Nordic Genetic Resource Centre. “And the Svalbard vault is the essential backup facility for seed banks around the world, safeguarding the biodiversity they hold.” Many crop varieties have been lost, including 93% of fruit and vegetable varieties in the US. “The issue of some water intrusion in the entrance tunnel was certainly not foreseen during construction,” Dempewolf said. “No one thought summers would be so warm. The physicist Edward Teller tells the American Petroleum Institute (API) a 10% increase in CO2 will be sufficient to melt the icecap and submerge New York. “I think that this chemical contamination is more serious than most people tend to believe.” Lyndon Johnson’s President’s Science Advisory Committee states that “pollutants have altered on a global scale the carbon dioxide content of the air”, with effects that “could be deleterious from the point of view of human beings”. Summarising the findings, the head of the API warned the industry: “Time is running out.” Shell and BP begin funding scientific research in Britain this decade to examine climate impacts from greenhouse gases. A recently filed lawsuit claims Exxon scientists told management in 1977 there was an “overwhelming” consensus that fossil fuels were responsible for atmospheric carbon dioxide increases. An internal Exxon memo warns “it is distinctly possible” that CO2 emissions from the company’s 50-year plan “will later produce effects which will indeed be catastrophic (at least for a substantial fraction of the Earth’s population)”. The Nasa scientist James Hansen testifies to the US Senate that “the greenhouse effect has been detected, and it is changing our climate now”. In the US presidential campaign, George Bush Sr says: “Those who think we are powerless to do anything about the greenhouse effect forget about the White House effect … As president, I intend to do something about it.” A confidential report prepared for Shell’s environmental conservation committee finds CO2 could raise temperatures by 1C to 2C over the next 40 years with changes that may be “the greatest in recorded history”. It urges rapid action by the energy industry. “By the time the global warming becomes detectable it could be too late to take effective countermeasures to reduce the effects or even stabilise the situation,” it states. Exxon, Shell, BP and other fossil fuel companies establish the Global Climate Coalition (GCC), a lobbying group that challenges the science on global warming and delays action to reduce emissions. Exxon funds two researchers, Dr Fred Seitz and Dr Fred Singer, who dispute the mainstream consensus on climate science. Seitz and Singer were previously paid by the tobacco industry and questioned the hazards of smoking. Singer, who has denied being on the payroll of the tobacco or energy industry, has said his financial relationships do not influence his research. Shell’s public information film Climate of Concern acknowledges there is a “possibility of change faster than at any time since the end of the ice age, change too fast, perhaps, for life to adapt without severe dislocation”. At the Rio Earth summit, countries sign up to the world’s first international agreement to stabilise greenhouse gases and prevent dangerous manmade interference with the climate system. This establishes the UN framework convention on climate change. Bush Sr says: “The US fully intends to be the pre-eminent world leader in protecting the global environment.” Two month’s before the Kyoto climate conference, Mobil (later merged with Exxon) takes out an ad in The New York Times titled Reset the Alarm, which says: “Let’s face it: the science of climate change is too uncertain to mandate a plan of action that could plunge economies into turmoil.” The US refuses to ratify the Kyoto protocol after intense opposition from oil companies and the GCC. The US senator Jim Inhofe, whose main donors are in the oil and gas industry, leads the “Climategate” misinformation attack on scientists on the opening day of the crucial UN climate conference in Copenhagen, which ends in disarray. A study by Richard Heede, published in the journal Climatic Change, reveals 90 companies are responsible for producing two-thirds of the carbon that has entered the atmosphere since the start of the industrial age in the mid-18th century. The API removes a claim on its website that the human contribution to climate change is “uncertain”, after an outcry. Exxon, Chevron and BP each donate at least $500,000 for the inauguration of Donald Trump as president. Mohammed Barkindo, secretary general of Opec, which represents Saudi Arabia, Kuwait, Algeria, Iran and several other oil states, says climate campaigners are the biggest threat to the industry and claims they are misleading the public with unscientific warnings about global warming. Jonathan Watts “A major upgrade was the only right thing to do and the Norwegian government has certainly put the resources up to make sure that it is absolutely watertight now.” Hege Njaa Aschim, a spokeswoman for the government, which owns the vault, said: “The entrance tunnel and the upgrade will secure the seed vault for the future.” In 2017, she told the Guardian: “A lot of water went into the start of the tunnel … The vault was supposed to [operate] without the help of humans.” No water reached the seed vaults. The 130-metre entrance tunnel has been fully waterproofed and the cooling equipment that keeps the vault at -18C moved to a new service building, so heat from the machinery can be released outside. The vault is 130 metres above sea level and designed for a “virtually infinite lifetime”. “It is always dangerous to talk about something being completely failsafe and impregnable,” Dempewolf said. “In 20, 30, 40 years down the line, we will continue to monitor the situation to see whether any other upgrades are necessary.”"
"The legal case against Heathrow airport expansion pitted the need to tackle the climate crisis against economic arguments on behalf of UK plc, and for the first time in any major infrastructure project, the planet won. John Holland-Kaye, the chief executive of the airport, made a last-minute PR push on Radio 4’s Today programme on the eve of the appeal court decision, claiming expansion was the “key to delivering the prime minister’s vision of a global Britain” after Brexit.  But the impact on the climate of emissions from a third runway’s 260,000 extra flights a year ultimately held sway in the appeal court. The environmental groups Friends of the Earth and Plan B argued the expansion would jeopardise the UK’s ability to make the very deep reductions in greenhouse gas emissions that are necessary to stop global warming from causing catastrophic and irreversible impacts. What just happened? For the first time judges have said that plans for a major infrastructure project are illegal because they breach the UK's commitments to reduce greenhouse gas emissions to tackle the climate crisis. This is a groundbreaking legal decision that could effect future infrastructure developments and puts the UK’s commitment to cut emission to net zero by 2050 at the forefront of future policymaking. What will happen next? The government has been told by the court of appeal to declare its decision to allow Heathrow airport expansion - contained in its airline national policy statement - illegal. Ministers have two choices now. They can withdraw the whole policy statement or try to amend it to make it compatible with the UK’s commitments to reduce greenhouse gas emissions to net zero by 2050.  Will the runway be built? If the government can prove that expanding Heathrow is compatible with its commitments under the Paris agreement to very radically reduce greenhouse gas emissions, the runway may go ahead. But the prime minister has always been against the third runway, and the government has told the court it will not be appealing against its decision on Thursday.  There now hangs a very big question mark over whether the bulldozers will ever start work on the runway. They argued that the then transport secretary, Chris Grayling, acted unlawfully when he agreed to the expansion of Heathrow in the government’s airports national policy statement in June 2018 – he had failed to take into account the UK’s international obligations under the Paris agreement, and its own domestic law. The judges’ ruling in favour of the campaign groups puts both the need for the UK to make significant reductions in emissions and the requirements of the Paris agreement at the forefront of policymaking. It is a judgment that could have lasting implications for future infrastructure projects. The ruling strikes a warning note that the climate crisis means it can no longer be business as usual. In future, for economic and business decisions to have both legal standing and political credibility they must take into account the impact on global heating. This ruling comes in the year the UK’s trustworthiness on these issues comes under the global spotlight, as the government hosts the key UN international climate talks, Cop26, in Glasgow in November. In the long history of Heathrow’s planned expansion, during which both Labour and Conservative governments have backed a third runway, those protesting that its environmental impact is just too great have seemed to be the underdogs. Arguments about increased air pollution and noise pollution, and the severe negative consequences of, in effect, tacking a new airport with the capacity of Gatwick on to Heathrow, have been met by politicians and business leaders contending that expansion is vital to the UK’s economic prosperity. Nothing, Grayling said last year, must stop this “massive economic boost”. Now it seems the impact of human activity on the planet has outweighed the economic holy grail of Heathrow expansion. But campaigners – some of whom have been fighting for decades to stop the runway – will hold in the back of their minds the words of another Conservative prime minister, David Cameron, even as they celebrate the appeal court ruling. Cameron promised them in 2010: “No ifs, no buts, no new runway,” only to commission a report which paved the way for the runway to be approved. This latest legal challenge – at a time of acute and growing public concern about climate change – might just prove decisive for Heathrow Airport Ltd, and political supporters of its project. The make-up of parliament has also changed considerably since the Commons voted in favour of the third runway in 2018. The judges did not cancel the expansion, but they have thrown a big, perhaps insurmountable, obstacle in its path – and given Boris Johnson, who as a backbench MP threatened to lie down in front of the bulldozers to stop a third runway, a compelling reason to scrap it."
"**An agreement to relax Covid rules over Christmas is not ""an instruction to meet with other people"", Wales' First Minister Mark Drakeford has said.**
Three households from around the UK will be able to meet from 23 December until at least 27 December.
It follows an agreement between the UK government and ministers in Wales, Scotland and Northern Ireland.
Mr Drakeford said he believed people would be unwilling to stick to ""strict rules"" over the Christmas period.
It comes as Welsh ministers consider whether more restrictions will be needed in the run up to Christmas, as cases rise among the under-25s.
The first minister called for a ""common approach"" to dealing with the aftermath of Christmas - earlier he warned that relaxing rules would lead to an ""inevitable"" rise in coronavirus.
Welsh Conservatives welcomed the agreement, but Plaid Cymru warned that ""hard-gotten gains"" must not be lost ""for the sake of a few days"".
Mr Drakeford told BBC Wales: ""If we ask people just to stick to the strict rules we have now I'm afraid lots of people will not be prepared to do that.
""So it's not a choice between relaxation or no relaxation.
""It's having a form of relaxation where there are rules that people will recognise that will allow people to enjoy Christmas, but we'll do it in a controlled way.""
The Welsh Labour leader added: ""People will be allowed to do what the law will allow them to do, but this is not an instruction to travel, it's not an instruction to meet with other people.
""People should still use a sense of responsibility, should still ask themselves whether what they are doing is keeping themselves and other people safe.""
Under the agreement, made at a meeting of Cobra on Tuesday afternoon:
The agreement said that ""existing, more restrictive"" rules on pubs and restaurants, and meeting in other venues will be maintained.
Ministers have been considering tighter restrictions in Wales in the run up to the festive period.
""The cabinet will meet before the end of this week again,"" the first minister said. ""If we're in a position to make an announcement this week, then that's what we will do.""
He added: ""Where does coronavirus spread? It's spread in people's homes, it spreads in hospitals and it spreads in hospitality.
""We have to think about all three of those settings and do our very best to bear down on the virus, which spreads so fast if it's given an opportunity.""
Vaccines and mass testing are ""not going to come to our rescue in January"", the first minister added.
""There is still a pull through to the spring before we will see the real benefit of those things, and we are going to have to ask people to go on living with the virus, living with it sensibly, living with it in a way that limits the damage while we are bringing those new possibilities, fully on stream.""
Plaid Cymru leader Adam Price said a ""compassionate but responsible approach"" was ""sensible"", but said the Welsh Government ""has a responsibility to ensure clear communication over the festive period, encouraging people to follow the guidelines"".
Welsh Conservative Senedd leader Paul Davies welcomed the decision and said it showed ""what can be achieved when governments work together"".
Simon Hart, the UK government's Welsh Secretary, said he was delighted with the agreement, but urged people ""to continue to be careful and responsible over the Christmas period to keep themselves and their families safe"".
The broad questions about Christmas have been answered, but plenty of questions remain about what happens before and after.
Mark Drakeford has said repeatedly that the festive relaxation will lead to more Covid cases and ""payback"" will be needed.
Does that mean a tightening of the rules in the run up to Christmas? Probably.
Could they be introduced to coincide with the new regime starting in England on 2 December? That would continue the theme of a communal UK approach.
And while talk of vaccines and mass testing have given us hope for a better 2021, how quickly can they be rolled out to the general population?
Whatever the calendar says, January could still feel very 2020."
"
Share this...FacebookTwitterPortuguese skeptic site Ecotretas reports on the crashing carbon market:
It’s nothing new. It started over a year ago, when US
carbon trading crashed. Two weeks later, it
closed. Now, it’s our turn in Europe. It had already started
earlier this Summer. But now, as can be seen in the left graphic, obtained from
Bloomberg, carbon prices are diving even more! And this is yesterday’s
graph, as today, as I write, it is diving another -10.773% to € 7.040.
Charts and more here…
Share this...FacebookTwitter "
"**Scotland's local Covid-19 alert levels are to remain unchanged, with Nicola Sturgeon saying the government must follow a ""cautious approach"".**
Where each local authority area sits in the five-level system of measures is reviewed every Tuesday.
The first minister said restrictions were ""having an impact"", but that no changes would be made this week.
She said case numbers may be declining across Scotland, but it was important to ""keep the virus at bay"".
And Ms Sturgeon said she was ""hopeful"" that a deal will be agreed later on Tuesday to ease curbs over Christmas in a ""temporary and limited"" way to allow more people to meet up.
A group of 11 council areas in the west of the central belt are to remain in level four - the top tier of curbs - until 11 December.
East Lothian moved down to level two as of Tuesday morning, but plans for Midlothian to make the same move were scrapped amid concerns about a rise in infections.
Ms Sturgeon said she was ""hopeful"" that both Dumfries and Galloway and Argyll and Bute could drop to level one in the coming weeks.
She said there had been rises in case numbers in Aberdeen and Aberdeenshire, but that these had been linked to ""specific outbreaks"" meaning they would not trigger an increase in the local levels.
And the first minister said officials were monitoring Clackmannanshire and Perth and Kinross, currently in level three, ""particularly carefully"" in light of a recent increase in cases.
Scotland moved to a five-level system of localised restrictions earlier in November, with the aim of suppressing the virus in high-prevalence areas but allowing more freedom in places with fewer cases.
Significant changes have been phased in over the past week, with 11 councils around Glasgow and west and central Scotland moving to level four, the top tier of measures.
Ms Sturgeon said this was a bid to ""make sure cases in these areas fall more markedly"", saying people there should ""stay at home as much as possible"".
She said: ""The latest data shows that across the country as a whole and within most local authorities, the restrictions in place are having an impact.
""The number of new cases across the country has stabilised in recent weeks, and we have grounds for cautious optimism that numbers may be declining.
""There is also evidence that admissions to hospitals and intensive care units are declining too, although these do tend to fluctuate on a day to day basis. However the national picture - which is positive - masks some regional variations.""
Midlothian was originally meant to move to level two on Tuesday morning, but this was put on hold after fears about rising cases in the area.
Ms Sturgeon said the number of cases had risen from 61 per 100,000 people to 97, saying that while this was ""well below the national average"" the 50% increase was a concern.
Local council leader Derek Milligan said the move would be ""absolutely devastating"" for local businesses, and Scottish Labour leader Richard Leonard pressed the first minister about the ""eleventh hour decision"".
He said: ""Decisions like this one today need to be a genuine co-production involving the locally elected council and the local business community, workers and their trade unions.
""In Midlothian they have been told for the last week that they would move to level two, including as recently as last Friday. It was only at 10:45 yesterday morning they were told that they may not. And only at 16:29, they were told by the deputy first minister that they definitely were not moving to level two.
""So that means as a result, stock ordered by businesses will go to waste, investment in health and safety measures will lie idle, and staff re-hired will once again be laid off.""
The first minister said she knew the decision would be ""disappointing for individuals and businesses"" in Midlothian, but said it was better than risking having to move the area back up a level in a week's time.
Ms Sturgeon is due to take part in further talks later with ministers from the UK government and the other devolved administrations about easing measures at Christmas.
She told MSPs that she hoped a ""common framework"" would be agreed to allow more people to meet up over the festive period.
However she warned that this would be ""temporary and limited"", and said ""people should use any flexibility carefully and only if they believe it right and necessary for their personal circumstances"".
The Scottish Greens have called for the government to reveal ""how much of an increase in cases"" it expects after loosening the rules, with MSP Alison Johnstone saying ""it's not good enough for the first minister to fall back on personal responsibility"".
Meanwhile Scottish Lib Dem MSP Alex Cole-Hamilton asked Ms Sturgeon if she backed the ""vigilante action"" taken by the SNP's Westminster leader Ian Blackford when he accused a photographer of breaking travel rules.
The first minister praised the MP's ""grace and dignity"" for apologising for ""doing something he recognised he should not have done"".
And Scottish Conservative group leader Ruth Davidson pressed Ms Sturgeon for details about the rollout of Covid-19 vaccines, with the first minister saying there were still details to be ironed out about the certification of different immunisations and how they will be supplied."
"Melting glaciers, rising sea levels, global warming and violent storms: the effects of climate change are well documented. But a growing weather trend that has caused much concern is storm clustering – when three (sometimes more) hurricanes or typhoons group together in a short space of time, gathering strength and unleashing even greater devastation. The development of a tropical depression – a low pressure area with thunderstorms and winds below 39mph – to a tropical storm that attains hurricane strength in less than six hours, shows how quickly these things can intensify. But increased frequency is also a trend, as storms follow each other in quick succession. Those who question the existence of climate change should look at the global hurricane history, or even the hurricane pattern in their own country. If we look at these storms, patterns of increasing intensity and frequency clearly demonstrate how climate change is having a direct impact on the way hurricanes behave. In developed countries coastal residents in affected areas are keenly aware of these hazards and respond well during emergencies by liaising with local agencies and heading to designated shelters during evacuations. But this is not the case in developing and underdeveloped countries, although basic response awareness exists through devastating experience and a degree of public information. Thanks to advances in hurricane forecasting and hindcasting techniques, situations like the Galveston hurricane in 1900, which struck the Texas coast without any official warnings, are happily a thing of the past. The weather prediction models of the National Hurricane Center and the European Centre for Medium Range Weather Forecasting are able to plot the likelihood of impending hurricane paths – known as the “cone of uncertainty” – five days in advance, and are generally accurate. But the real issue is how prepared we are around the world for the increasing frequency of hurricanes and their terrifying “gang” version, hurricane trios. This violent onslaught of hurricane-strength storms batters communities and destroys buildings and infrastructure from the US to the Caribbean to South-East Asia. But should communities on the coast stay and defend, or retreat altogether? Hurricanes hammered the Atlantic from 2016 and 2018, including the Category 5 Matthew (2016), the Harvey-Irma-Maria trio (2017), which registered Category 4, 5 and 4 respectively, and Category 4 Florence and Michael (2018). This not only revealed the rising trend in intensity and frequency, but also alerted the world to the phenomena of clustering.  Critically, predicting the path of a hurricane depends on forecasting the dynamics of its intensity. Understanding the factors that contribute to the sudden changes in the strength (or weakening) of a hurricane is crucial. Changes in wind direction, interaction with the land at the coast, and ocean temperature and depth all play their part in altering the intensity of a hurricane that is highly sensitive to even slight changes. In general, the accuracy of predicting the way a hurricane intensifies and then re-intensifies in less than 24 hours is more challenging than predicting its path. But these dynamics are the underlying factors which compound the threat of hurricane frequency. These dynamics are also capable of further altering storm surge characteristics by triggering coastal and inland flooding – such as abnormal rises in water levels – which often result in shocking devastation. Hurricane Michael in 2018 was the perfect example of the importance of predicting how rapidly a hurricane has intensified before it hits the coast, in this case Florida. The predicted track of the storm was almost accurate but its intensity was more difficult to assess. The National Hurricane Center forecasted Michael’s path by issuing a five-day cone of uncertainty advising of sustained winds of 65mph. However, the sudden change in the storm’s dynamics changed a Category 1 hurricane to a Category 4 with winds of 155mph. This underscores the uncertain and variable nature of hurricane prediction. Despite these emerging and changing weather-related risks, residential and public  buildings are still going up on affected coastal areas. Recent research in China identified a tsunami that swept away the present-day coastal province of Guangdong in 1076AD. It means storm-related surges have been documented in the region for more than 1,000 years – yet still building and expansion goes on heedless of the risk. This is almost the same situation for all vulnerable coastal cities. For instance, Florida has hundreds of thousands of coastal residents living in Low Elevation Coastal Zones – land that is less than ten metres above sea level and within 200km of the coast – but yet again construction there continues despite the threat of hurricanes every season. Developers are already conceiving storm-resilient buildings that can withstand winds of at least 200mph – a Category 5 hurricane. But it’s unlikely many have considered the compounded stress effect on structures having to continuously withstand hurricane force winds more frequently and in quick succession. Building massive sea defences along vulnerable coastlines is practically impossible and isn’t a permanent solution to increasing coastal storm hazards. There is no point in risking lives by remaining, as storm clusters can be unpredictable. It is simply too dangerous, so evacuation is the only option. However, when it comes to coastal assets and investments, defending in a more appropriate and sensible way is required.  Some coastal cities are planning ahead. A recent development of extensive parks in Boston, US, aims to protect the urban shoreline infrastructure from flooding. And a 2009 study revealed the effectiveness of mangrove planting in coastal areas of India to protect the shoreline and reduce cyclone damage. But more practical solutions are needed, especially in more vulnerable developing regions, because cluster storms are not going away any time soon."
"
Now we know why they like global warming so much – it’s hot.

Excerpts below of the original story here: Porn surfing rampant at U.S. science foundation
Jim McElhatton
EXCLUSIVE:
Employee misconduct investigations, often involving workers accessing pornography from their government computers, grew sixfold last year inside the taxpayer-funded foundation that doles out billions of dollars of scientific research grants, according to budget documents and other records obtained by The Washington Times.
The problems at the National Science Foundation (NSF) were so pervasive they swamped the agency’s inspector general and forced the internal watchdog to cut back on its primary mission of investigating grant fraud and recovering misspent tax dollars.
“To manage this dramatic increase without an increase in staff required us to significantly reduce our efforts to investigate grant fraud,” the inspector general recently told Congress in a budget request. “We anticipate a significant decline in investigative recoveries and prosecutions in coming years as a direct result.”
The budget request doesn’t state the nature or number of the misconduct cases, but records obtained by The Times through the Freedom of Information Act laid bare the extent of the well-publicized porn problem inside the government-backed foundation.
For instance, one senior executive spent at least 331 days looking at pornography on his government computer and chatting online with nude or partially clad women without being detected, the records show.
When finally caught, the NSF official retired. He even offered, among other explanations, a humanitarian defense, suggesting that he frequented the porn sites to provide a living to the poor overseas women. Investigators put the cost to taxpayers of the senior official’s porn surfing at between $13,800 and about $58,000.
Read the rest of the article at the Washington Times here
Have a friend or acquaintance that works at NSF? Send them an Ecard here
h/t to Charles the moderator


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e932bb015',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The G20 group of the world’s wealthiest nations have agreed to collectively sound the alarm over the threat to the financial system posed by the climate emergency. Overcoming objections from Donald Trump’s US administration, G20 finance ministers and central bank governors meeting in Saudi Arabia over the weekend agreed to issue their first communique with references to climate change since the beginning of the Trump era, according to reports from Reuters. China produces the most heat-trapping pollution, followed by the US. But historically, the US has contributed more carbon dioxide to the atmosphere than any other nation. The US also has high emissions per capita, compared to other developed countries. And Americans buy products made in China, therefore supporting China's carbon footprint.  Sources told the news agency that the statement of priorities included the importance of examining the implications of global heating for financial stability, as part of the work of the G20’s Financial Stability Board, the steering group for international banking industry rules. The language represented a compromise to overcome opposition from US officials at the first major meeting of Saudi Arabia’s year-long presidency of the G20, according to the sources. An attempt to include references to the downside risks for global growth posed by the climate crisis was dropped. Concerns about the economic damage from rising global temperatures and extreme weather events have risen up the agenda among world leaders, central bankers and financiers in recent years. The financial system continues to fund activities that are inconsistent with meeting climate targets, paving the way for trillions of pounds of financial losses in the future and catastrophic environmental consequences should the world economy fail to adapt. The meetings in Riyadh were attended by Mark Carney, who has driven the climate emergency up the agenda among world leaders and financial regulators to stake a legacy at the Bank of England before he stands down as governor next month. The new chancellor, Rishi Sunak, stayed in London to continue preparing for next month’s budget, instead sending a senior civil servant from the Treasury. Reuters reported that the communique issued at the end of the meetings in the oil-rich Gulf state would be the first to include references to climate change since Trump became president in 2017. The International Monetary Fund included climate-related disasters in a list of the risks facing a highly fragile recovery in the global economy this year. However, the increasing focus comes as US officials resist naming global heating as an economic risk, following Trump’s move at the outset of his presidency to withdraw the world’s largest economy from the Paris climate accords. • This article was amended on 26 February 2020. The communique was the first during the Trump era to mention climate change, not the first since 1999 when the G20 was founded, as an earlier version had said. "
"**Here are five things you need to know about the coronavirus pandemic this Tuesday evening. We'll have another update for you tomorrow morning.**
Up to three households will be able to mix indoors during a five-day Christmas period of 23-27 December, under a plan agreed by the leaders of England, Scotland, Wales and Northern Ireland.
It comes after agreement was reached at a Cobra meeting bringing together the UK government and the devolved administrations.
People will be able to mix in homes but not pubs or restaurants.
Earlier, Transport Secretary Grant Shapps warned Christmas travellers should plan journeys carefully and prepare for restrictions on passenger numbers.
Wondering how Christmas might look this year? Here are a few things that may be different.
More than one in five secondary school pupils in England missed school last week, figures out today show.
The weekly data from the Department for Education shows overall attendance was down to 83% of pupils, with almost 900,000 children out of school because of either testing positive for coronavirus or coming into contact with a positive case.
The DfE says keeping schools open is a ""priority"". But Mary Bousted, co-leader of the National Education Union said the situation had ""reached crisis point"".
The rules for self-isolating mean pupils have to stay home for at least 14 days. For more on the rules for self-isolating click here.
Latest figures show there were almost 14,000 deaths in the week ending 13 November, with 2,838 of these involving Covid - 600 more than the previous week.
Data compiled by the Office for National Statistics show north-west England and Yorkshire were the worst hit for excess deaths - both regions were more than a third above expected levels.
By comparison, the number in south-east England was 2% above the five-year average.
But there is hope that the rise may soon start slowing as the daily figures show deaths are not rising as quickly as they were.
Click here if you want to know how many cases there have been in your area.
Organised crime groups nearly got away with benefit scams of as much as Â£1bn as they attempted to take advantage of looser benefit rules during the pandemic, BBC News has discovered.
The fraudsters were trying to exploit looser rules introduced to cope with a surge of universal credit claims.
Before the scam was spotted, officials unwittingly confirmed thousands of stolen identities. It was foiled when a junior civil servant working with High Street banks noticed dozens of claims for universal credit had been made asking for money to be paid into the same bank account.
BBC News has asked the Department for Work and Pensions for a response.
The etiquette of the video call has been a steep learning curve for some. Director Tristram Shapeero has had to apologise for failing to hit the mute button while commenting on an actor's ""tiny apartment"".
Shapeero, who has worked on Brooklyn Nine-Nine and Never Have I Ever among others, was auditioning Euphoria actor Lukas Gage via Zoom when he was heard saying: ""These people live in these tiny apartments. Like I'm looking at his, you know, background and he's got his TV... ""
Gage then interjects: ""I know it's a [rubbish] apartment that's why [you should] give me this job so I can get a better one.""
Get a longer daily news briefing from the BBC in your inbox, each weekday morning, by signing up here.
You can find more information, advice and guides on our coronavirus page.
And we've had a look at what England's new lockdown rules will be.
**What questions do you have about coronavirus?**
_ **In some cases, your question will be published, displaying your name, age and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read our**_terms & conditions _ **and**_privacy policy.
Use this form to ask your question:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or send them via email to YourQuestions@bbc.co.uk. Please include your name, age and location with any question you send in."
"
Share this...FacebookTwitterJosh weighs in on the sea level debate…

Thanks for making my day, Josh!  See all of Josh’s cartoons HERE.
Share this...FacebookTwitter "
"Two companies in Japan recently announced they are to begin building two huge solar power islands that will float on reservoirs. This follows Kagoshima solar power plant, the country’s largest, which opened in late 2013 and is found floating in the sea just off the coast of southern Japan. These moves comes as Japan looks to move on from the Fukushima disaster of 2011 and meet the energy needs of its 127m people without relying on nuclear power.  Before the incident around 30% of the country’s power was generated from nuclear, with plans to push this to 40%. But Fukushima destroyed public confidence in nuclear power, and with earthquakes in regions containing reactors highly likely, Japan is now looking for alternatives. Solar power is an obvious solution for relatively resource-poor nations. It is clean, cost-competitive, has no restrictions on where it can be used and has the capability to make up for the energy shortfall. A small fact that solar researchers love to trot out is that enough sunlight falls on the earth’s landmass around every 40 minutes to power the planet for a year. To put this another way, if we covered a fraction of the Sahara desert in solar panels we could power the world  many times over.  The technology already exists, so producing enough solar power comes primarily down to one thing: space. For countries such as the USA with lots of sparsely populated land this is not an issue, and there have already been a large number of “solar farms” installed around the country.  In places like Japan where space is limited, more inventive solutions are required. This is the principle reason behind the decision to move their solar power generation offshore. While the land is highly congested, and therefore expensive, the sea is largely unused. It therefore makes a good degree of sense to use this space for floating power plants. Somehow though the concept of floating solar plants initially does seem rather jarring. Water plus electricity? We’ve all been raised by popular culture and public safety films to know that those two really don’t mix.  But this isn’t some sort of vast technological challenge that mankind will struggle to overcome. The panels are designed to be waterproof and a number of these types of plants have been built in Japan already, including the large installation in Kagoshima.  Part of the beauty of solar power is how simple it is to use. At a basic level, once you buy the off-the-shelf photovoltaic module, it’s simply a case of plugging it in. The principle engineering challenge of offshore solar farming consists of little more than building a pier and covering it in solar panels.  This may be a slightly glib oversimplification, but consider the relative difficulties in comparison to the construction of an offshore oil drilling platform. These represent a true engineering challenge and a true risk when that challenge is failed, as we saw all too clearly with the Deepwater Horizon spill in the Gulf of Mexico in 2010. The risks and difficulty associated with off-shore solar are vanishingly small by comparison.  Floating solar also has some interesting fringe benefits. Solar modules function much better when cooler, so situating them near water actually helps performance. In India they have also been used as an interesting dual purpose solution. In the state of Gujarat, solar panels were installed atop the Narmada canal, serving to both generate power and prevent water evaporating from beneath.  There is also no reason why the design needs to be so functional. By far the most unique application is the concept of “energy ducks”, giant floating solar panel-coated water fowl which have been proposed to sit in Copenhagen harbour acting as both a tourist attraction and carbon-neutral power source. This may never happen unfortunately, but it is a rather wonderful demonstration of how solar power can be applied in many different ways.  Solar islands could certainly be a solution for other countries where space is an issue – it’s possible that one day a significant portion of Europe’s power could be generated by giant solar pontoons in the ocean. The technology exists and the engineering challenges are nothing that can’t be overcome. The only questions now are whether the will is there to push solar islands as a solution and more importantly do we make them duck shaped?"
"Unfurling across paintings, poems and carvings, Cymbidium orchids are more than just wild plants in China. They are inextricably linked with the country’s culture. But this rich blooming of human response to orchids that has endured for millennia is fragile, and as Cymbidium orchids increasingly vanish from the wild so too do the words and knowledge that humans have about them. Every year the Royal Botanic Gardens, Kew, and the New York Botanical Garden open their doors to thousands of visitors who flock to their orchid shows. Easily grown and long-lasting orchids such as generic Phalaenopsis form the bulk of these temporary displays. But behind the scenes these institutions engage in longer-term work to conserve not just living plants but also records of the culture attached to them.  Kew’s Spirit Collection contains ghostly flowers of Cymbidium kanran; their colour washed out but their three-dimensional shape preserved by immersion in a mixture of alcohol, glycerol, and water. In collaboration with the Institute of Medicinal Plant Development, Beijing, Kew has also developed a collection of plants – including orchids – in the forms they are used in traditional Chinese medicine – chopped, dried, fried and so on. Indexed with scientific botanical names, this collection is a repository of knowledge and a reference tool for authenticating botanical ingredients. Dr Barnabas Seyler, assistant researcher in the department of environment at Sichuan University, looked at the relationship between biodiversity and cultural diversity by examining changes in knowledge of Cymbidium orchids in Liangshan Yi autonomous prefecture, Sichuan. “As an ethnobotanist, I find all facets of biocultural diversity to be fascinating,” says Seyler.  “Many people, particularly in the west, but also in rapidly changing, urbanising, and modernising China do not fully appreciate the magnitude of symbolism and pride that Cymbidium have held throughout history in traditional Han Chinese culture,” he says. “Symbolism, metaphors, and poetry associated with Cymbidium are credited to have begun with Confucius’s own sayings and infuse traditional Confucian thought today. If you walk into any Chinese restaurant around the world, or into any tea parlour or salon in China, you will likely find paintings, furniture, place settings, or other material culture items depicting Cymbidium orchids.” Orchids, like other wild species, are vulnerable to the impact of climate change and habitat loss. But Cymbidium species native to Sichuan studied by Seyler have an additional vulnerability – their beauty. Between 2005–2008, when Cymbidium prices were at their peak, wild-collected rare forms could be sold for six-figure sums. In 2006 one was bought for 4.6m Chinese yuan (£511,000). Interviewing individuals in predominantly Yi and Han communities, Seyler and his colleagues assessed whether people could identify different Cymbidium species. Additionally, they asked about local ecological knowledge: how to find, harvest and grow orchids, as well as orchid business knowledge, and awareness of orchids in arts, academia and idiomatic expressions. In all categories of knowledge, they found that when species were locally extinct, knowledge about them had declined, except among individuals involved in their trade.  Seyler believes that botanical gardens contribute much towards conserving biocultural diversity, “through ex-situ collections, for example collecting and showcasing plants like Cymbidium, advocating for their conservation, and educating the general public, and documenting traditional knowledge, stories, and cultural traditions associated with these plants”. In the US, New York Botanical Garden is a designated plant rescue centre. When the US Fish and Wildlife Service finds shipments of orchids that lack the paperwork to prove they were either cultivated or sustainably collected from the wild, they are sent to a plant rescue centre. If the country of export does not request their return within 30 days, they are incorporated into the institution’s collection, but remain US government property. “Botanical gardens, properly functioning, serve as local drivers of economic development, education capacity building, and as mechanisms for community cohesion, showcasing the unique beauty and value of a region’s biocultural diversity to visitors,” says Seyler. Han Chinese culture won’t collapse if Cymbidium orchids become extinct in the wild in China. Culture doesn’t entirely disappear because of the loss of one plant or group of plants. But what happens if species loss continues? Seyler believes botanical gardens are playing a vital role in keeping the culture alive. “They can help to address some of the educational, social, and economic challenges that contribute to the unsustainable overharvest of biodiversity,” he says. • The orchid festival at Royal Botanic Gardens, Kew, runs until 15 March. The orchid show at the New York Botanical Gardens is open until 19 April. This article was edited on 6 March 2020 to reflect the announcement of extended opening dates for Kew’s orchid festival"
"**Chancellor Rishi Sunak is unveiling the government's spending plans for the coming year.**
The Spending Review will include details on public sector pay, NHS funding and money for the devolved administrations in Northern Ireland, Scotland and Wales.
Mr Sunak will also set out the extent of the damage done to the UK economy by the coronavirus pandemic.
A No 10 spokesman said the economic forecasts will be ""a sobering read"".
The government's Covid response has led to huge spending and borrowing rises.
The chancellor is expected to begin his statement at around 12:30 GMT following Prime Minister's Questions.
Some Spending Review announcements have already been trailed.
These include an extra Â£3bn for the NHS in England to help tackle the backlog of operations delayed due to Covid, an increase in defence spending and a Â£4.6bn package to help the unemployed back to work.
The government is expected to announce a cut in the UK's overseas aid budget to 0.5% of national income, down from the legally binding target of 0.7%.
There have also been reports that the chancellor is considering a pay freeze for all public sector workers except frontline NHS staff.
Plans to change the way big spending projects are analysed \- which the Treasury says is currently biased in favour of the south east of England - will be published alongside the Spending Review.
The chancellor may also choose to set aside money to tackle climate change and regional inequalities.
Devolved governments will receive money proportionate to any funding England gets in the Spending Review.
This is decided using the Barnett formula - devised by Lord Barnett, a Labour politician, in the 1970s.
Mr Sunak and Treasury Chief Secretary Stephen Barclay updated the Cabinet on Wednesday morning.
A Downing Street spokesman said: ""Cabinet was told the OBR forecasts will show the impact the coronavirus pandemic has had on our economy and they will make for a sobering read, showing the extent to which the economy has contracted and the scale of borrowing and debt levels.
""But - as the IMF (International Monetary Fund), OBR and others have pointed out - the costs would have been much higher had we not acted in the way we have done.""
""It's going to look horrible.""
The simple truth about the Spending Review according to a senior MP.
The chancellor will bang the drum for his plans to keep people in jobs, or help find new ones.
Rishi Sunak will take out the metaphorical megaphone to explain how he'll allocate billions of taxpayers' cash to spend on infrastructure in the coming months.
But the headlines of the Spending Review, when governments put their money where their mouths are, won't be in any rhetorical flourishes at the despatch box, nor likely in any surprise announcements kept back as goodies for the public.
The government had intended to use the Spending Review to set out its plans for the next three years, however this was reduced to just one year due to the economic turmoil caused by Covid.
The difficult financial backdrop will dominate this year's review with the economy projected to be 10% smaller than it was pre-virus.
Tax revenues have fallen as many businesses have been forced to close and government schemes to support furloughed workers have led to soaring levels of spending.
Public borrowing is expected to rise to Â£372bn - compared to the Â£55bn the government had originally expected to borrow.
The Spending Review will be accompanied by economic forecasts from the Office for Budget Responsibility - including predictions on how tax will be raised.
Labour's shadow chancellor Anneliese Dodds said the government's ""irresponsible choices"" during the pandemic had ""led to our country experiencing the worst downturn in the G7, and created a jobs crisis"".
""This prime minister and his government talk a good game but they haven't delivered on their promises - and regional inequality has got worse under their watch,"" she said.
""They clapped for key workers - but now they're freezing their pay, and looking to scrap planned minimum wage increases for the private sector.""
Unions called for Mr Sunak to maintain investment in the public sector, the TUC's deputy general secretary Paul Nowak telling BBC Breakfast ""now is not the time to make cuts to public services"".
And the SNP is calling for a huge stimulus package to support growth and jobs across the whole of the UK.
""The spending has to match the challenges we see in the economy,"" said its economic spokeswoman Alison Thewliss. ""At the moment interest rates are at a record low so the government should be borrowing."""
"It’s not news to observe that as soon as anyone mentions climate change policy in Australia, madness generally follows. A fresh outbreak of stupidity in the political debate has been triggered by Labor’s decision last week to sign up to a net zero target by 2050. Given the madness, let’s look at net zero, and establish some basic facts.  Achieving net zero emissions by mid-century is certainly a challenging task, requiring the transformation of Australia’s carbon-intensive economy. But increasingly, net zero is not a controversial ambition. More than 70 countries and 398 cities say they have adopted a net zero position. Every Australian state has signed up to net zero emissions by 2050, and these commitments are expressed either as targets or aspirational goals. Net zero has also been adopted by business groups in Australia who only a few years ago opposed Labor’s carbon pricing scheme. The Australian Climate Roundtable, which includes the Australian Industry Group, the Business Council of Australia, the ACTU, the National Farmers’ Federation and the Australian Council of Social Service, issued a statement late last year supporting policies requiring “deep global emissions reductions, with most countries including Australia eventually reducing net greenhouse gas emissions to zero or below”. The BCA, which represents Australia’s biggest companies, says Australia should legislate a target of net zero emissions by 2050. Just for context, this same organisation characterised Labor’s 45% reduction target by 2030 – the opposition’s policy for the last election – as “economy wrecking”. The point here is: attitudes are shifting, and rapidly. Because the government is speaking out both sides of its mouth, the position is confusing. Let’s step through the sequence. Both the trade minister, Simon Birmingham, and the emissions reduction minister, Angus Taylor, have acknowledged over the past couple of weeks that signatories to the Paris agreement (and that’s Australia) have already signed on to achieving carbon neutrality by mid-century, or not long after. As Taylor told Sky News on Monday: “There is that targeting built into the Paris agreement where the world has agreed to get to net zero emissions in the second half of the century. That’s already there.” And Birmingham, on 10 February: “In signing onto the Paris agreement, Australia has committed to a net zero target for the world by the second half of this century, and we have to then work towards that, and we do that in the bite-size pieces of what we can achieve to 2030.” But while acknowledging the commitment the government has already made for “the world” (and Australia was both a Paris signatory, and part of “the world”, last time I looked) – the government then also seems to suggest it hasn’t committed to anything, and won’t in the absence of a plan to get there – a plan it is yet to specify. Very good question. Last Friday the finance minister, Mathias Cormann, confirmed the government “will be finalising a longer-term target in time for Cop26” (which is the UN-led climate talks scheduled for the end of this year) but Taylor’s language is different. The emissions reduction minister talks about “a long-term strategy” rather than a target, which is highly suggestive that the government won’t adopt a formal target despite having already implicitly signed up to a net zero objective by signing and ratifying Paris. But just to be clear: nobody significant in the government is ruling out adopting a target over the course of the year. I’m sorry if your head is hurting. Bear with me. When the emissions reduction minister was asked about his position on targets on Monday, Taylor said the government would be “focusing on technology” – which translates as the government identifying new technologies than can help Australia’s export industries reduce emissions “without job destruction”. That language doesn’t rule out adopting a target, but suggests a target isn’t that likely. At the risk of making your head explode, Taylor disdained adopting a target in the same interview where he acknowledged that his own government had, in fact, voluntarily signed an international emissions reduction agreement with a net zero objective. Despite this obvious inconvenience, Taylor counselled: “You can’t pursue a target like this without clarity for what it means for Australia.” The government is also arguing at the moment it will not sign up to net zero without knowing what the costs are. Taylor indicated on the ABC on Monday that some work was “going on” to examine these questions – although it was entirely unclear what that work was, or whether it would also examine the cost of inaction. When some other work was put to the minister on Monday, work undertaken by the CSIRO, Taylor didn’t seem that interested. He noted he’d seen “lots of modelling” during the last election. It’s also passing strange that the government has been very attentive to the (absent) costs of Labor’s national commitment to a net zero target, but has not – at least in my hearing – pursued the premiers over the cost of their 2050 commitments. A recent bilateral energy and climate change agreement between the Morrison and Berejiklian governments expressly referenced the state’s “economy-wide target of net-zero emissions by 2050” without any criticism from Canberra."
"
Share this...FacebookTwitterHalloween is already in full swing at the Potsdam Institute For Climate Impact Reseaarch in Germany. Witches, goblins, flesh-eating zombies and scary monsters are stirring about and spreading disturbing horror visions. Right now Stefan Rahmstorf is frozen by such a vision like a deer in the headlights.
North Pole stop in the 1950s. (Photo US Navy)
Rahmstorf, who has the unscientific habit of taking cyclic graphs and drawing straight trend lines along their upward or downward flanks and extending them all the way out to doomsday, presents the latest horror vision that has been haunting his mind lately. Indeed the visions seem to be getting more intense and vivid. In a piece in the Gulf Times he warns of “grave warning signs”.
Obviously he appears unaware of the fact that the Arctic all but melted away in the 1950s (see photo above) before rebounding to the record high levels of the 70s. You can’t start at the top of a cycle a draw a straight line along its downward flank all the way to yonder (hat-tip DAmbler):
A grave warning sign in the Arctic
By Stefan Rahmstorf /Berlin
Largely unnoticed, a silent drama has been unfolding over the past weeks in the Arctic. The long-term consequences will far outstrip those of the international debt crisis or the demise of the Libyan dictatorship, the news stories now commanding media attention. The drama – more accurately, a tragedy – playing out in the North is…continue reading
Oh no! Here comes doomsday!!!

Share this...FacebookTwitter "
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
The good news: there’s new and exciting opportunities opening themselves to us.The bad news; some people are hilariously unquestioning.

It has been an even more entertaining than usual couple of days in the alarmosphere. I’d been traveling the last week, doing TV station work and station surveys. While on the road I discovered through an email that I was the subject of a YouTube Video called “Climate Crock of the Week”.
The video was about my surfacestations.org project and was titled “What’s up with Watts?”. It was sad and funny at the same time, and as is typically the case with our old friends it was directed at me personally, far more than it tried substance. Equally typically, and sadly, what substance it tried turned out to be wrong. I continued on my travels, my friend Dr. Roger Pielke Sr. posted an opinion on it last week to address some of the issues.
Little did I know bizarro land awaited upon my return home.
Sitting down Saturday night, to watch the video again, detecting through its exquisite subtleties and nuance, I couldn’t help but laugh, because once again I noticed that everything reported in it was just wrong.
In fact, it probably was the worst job of fact-finding I had ever seen, which as WUWT readers know, is a bold assessment. I’ve been involved in broadcast TV news for 25 years, and have seen some really bad work from greenhorns fresh out of reporters school. This video reminded me of those. It was as if whoever put it together had never researched it, but just strung together a bunch of graphics, video, photos, and a monotone voice-over track with ad hominems liberally sprinkled for seasoning. I figured it was probably just an overzealous college student out to save the world and this was some college project. It had that air of  radical burningman quality about it.
Curiosity piqued, I inquired into just who is this climate Solon? To my surprise, he turned out to be an “independent film producer” working out of his house in Midland, MI under the name “Greenman Studio”, one Peter Sinclair, a proud graduate of Al Gore’s Climate Camp. I still figured him to be a kid and imagined his mom was yelling down into the basement “Peter that’s too loud, turn it down!”.
I also wondered if it was the same “Green Man” that had once prompted surfacestations volunteer Gary Boden to create this nifty patch:

This came about because my now defunct local “Alternate Weekly” had a ghost writer named “green man” who penned an unintentionally (I think) hilarious editorial about me and the www.surfacestations.org project back in 2007 in which he wrote the famous line:
“The Reverend Anthony WTF Watts and his screeching mercury monkeys…”
…in response to our daring to survey the weather stations nationwide. The “mercury” is reference to thermometers.
What was funny is that in my original story, one of my commenters posted a silly comment about well, “green stuff” and the editor of the local “Alternate Weekly” went ballistic and demanded I remove it  and gave me a stern lecture on libel. I was happy to comply not out of legal obligation but courtesy and deleted the comment.
Is this Green Man the same guy? Inquiring minds want to know.
OK back to the present. I checked my email for some correspondence from Mr. Sinclair for the past week and found none, and looked back even further to see if he had contacted me about the surfacestations project weeks before in email or in my letters pile. I found nothing and was surprised that he had made a video using my work without at least a basic request or notice.  Normally when somebody wants to publish something in another media type (that is not a blog or webpage) from the surfacestations project or my blog, they contact me and ask permission to use the items. The word normal, however, upon scrutiny really doesn’t apply here.
I’ve gotten dozens of such requests from magazines, newsletters, book publishers, and TV stations. So far, I’ve never said no to any request for such materials or copyright waivers. I’ve filled out lots of forms granting my copyright waiver for the legally skittish that need more than an email or “sure, go ahead” over the phone.
click for PDF
 
But, in the video Mr. Sinclair produced and posted on YouTube, I noticed that he did in fact use photographs and graphics from my published book “Is The U.S. Surface Temperature Record Reliable?”.  I hold the copyright on this book. The notice for copyright is in the inside front cover.  © 2009 Surfacestations.org  All rights reserved, including the right to reproduce this report or portions thereof in any form.  ISBN 13: 978-1-934791-29-5  and ISBN 10: 1-934791-26-6.
There was also a Warner Brothers video clip from the movie “Anchorman” with a segment about the incompetent TV weatherman which I assume was added to portray me in my chosen career, and amazingly (and most amusingly) there was another video clip from the movie “The Adventures of Buckaroo Banzai” which is a campy sendup of “War of the Worlds”. Interestingly in  the credits, and I know this because I happened to watch the movie about two weeks before on Showtime, there is a “John Van Vliet” listed in the credits. It made me wonder if it is the same John Van Vliet that created the “opentemp” program launched just a couple of months after I first started the surfacestations project in an attempt to derail it early on. He made the mistake of using incomplete data. More on incomplete data later.
I noted that neither clip was from the trailers you could find on YouTube and were of high quality, so maybe they were cribbed from a DVD or perhaps an Apple video download, since I recognized from the editing effects that Mr. Sinclair owns a Macintosh. WB has some pretty stringent clip licensing requirements, which I know from doing TV news and a reporter wanting once to use part of a film from WB in a special news report. WB wanted our TV station to pay, but the cost was sky high for our small TV station. They finally whittled it down to something we could afford.
Doing a little more research, I found that Mr. Sinclair does a series of animated online greeting cards, which you can see here: http://www.care2.com/ecards/bio/1023
I thought this one was funny: http://www.care2.com/send/card/0840
The description portrayed him as a pretty nice guy with an alternate minded view of the world like a lot of college students have. He is not a college student, though he has a son who is of college age, a nice Ron Paul supporter, I am told from someone who has met him. His rather conservative son, contrasts the rather left-wing eco-activist ad hominem and rhetorically unrestrained father(see here). It is almost humorous greeting card-worthy, this role reversal.
But since he had used that © symbol, Mr. Sinclair demonstrated awareness of copyright protections, having availed himself of them, e.g., here, right below his own artwork.
With knowledge of this and ad hominem attacks made on me personally, I reasonably presumed his copyright violation on my part was likely intentional. I also figured that this might be a teachable moment, as I was still thinking this is a kid just out of college since there seems to be no business website for Greenman studio in operation yet, it is still “under construction”.
http://www.greenmanstudio.com/
And, I mused, by bringing the copyright issue to his attention, I’d probably be doing him a favor, since I surmised he’d be at risk for using the film clips. I figured anybody working a business out of a house without an operating web page probably can’t afford licensing fees. No deep pockets there. I certainly have no personal beef with Mr. Sinclair, it is just the copyright issue.
But my copyright had been ignored, with evidence that Mr. Sinclair as a publisher himself using the © symbol understands copyrights, and WB’s copyright also looked like it also had been ignored. And well, lets face it, he got the facts wrong about the project and never contacted or interviewed me to get any facts from my side (more on that later). So it could hardly be defined as “journalism” and the protections that such enterprise affords for “fair use”. So I filled out the form for copyright issues on YouTube, and pressed enter.
What I expected to happen is that I’d get an angry email or blog comment from the guy, I’d suggest to him (privately) to make a couple of modifications, grant him a copyright for the factual graphics from the surfacestations project, and tell him to put his video back up on the web. End of story, lesson learned.
What I didn’t expect was the alarmosphere going into berserk overdrive.
After all, this was not yet a “weekday” which it increasingly seems to be what we call those periods when our friends lapse into said mode. It turns out that YouTube put my name and the surfacestations.org URL up on the video pane for the former video, made me a target for hatred by the “scream first, ask questions later” types.
The first hint of this started on Sunday when I got a comment on my blog. The commenter, who obviously didn’t know the difference between copyright law and constitutional law wanted to know why I had “denied free speech” to Mr. Sinclair. Of course, “free speech” protections involve state infringement and,as powerful as our friends do apparently believe I have become, neither am I the state nor was the state involved here, so the angst was yet again rather misplaced. Regardless, I also thought this a pretty odd comment. Since Mr. Sinclair still hadn’t contacted me, I paid no attention to it.
Then I began receiving more odd comments, and I’m thinking; “why are these people making a private copyright dispute their personal business?”
Here’s sampling of  a  few comments I got that never made WUWT:
“Watts you are a coward chickesh** no good dumba** weatherman hiding behind a law that you’ve irrationally applied”
“You can’t handle the TRUTH, if I were Jack Nicholson I’d kick your a**”
“Wattsup, you and your stupid picture book project are toast!”
I even got comments from “Omar” in Finland:
“Looks like your attempt to smother and censor information has fired back badly on you Mr Watts: Do you have – how you say – the cahones to explain yourself? I think not. You appear to be a child coward man.”
Censoring huh? And around the alarmosphere all sorts of curious accusations of censorship — again, with the long arm of the state nowhere to be found, this seemed to be a variant of the Tim Robbins (see also “paranoid” and “uncomprehending”) School of Crying “Censorship”. Even more bizarre, were the demands. On the “DeSmog Blog”, Kevin Grandia lambasted me for not knowing anything about law, and then demanded I email him and explain myself and my reasons for filing a copyright complaint. I’m no lawyer, but clearly giving details of a dispute to an angry third party not involved isn’t right up there with sound legal advice.
Still apparently confused that his dispute lay not with me but with YouTube or the concepts of intellectual property, when that didn’t get the required response, Mr. Grandia posted another angry column over on the Huffington Post, and made the same demand. He’s wondering why I haven’t responded directly to him.
Really.
But being that guardian of smoggy freedom, Mr. Grandia took it a step further, and, in a rather ironic follow-up to his seizing of the mantle of all that conforms to the laws, somehow located the original YouTube video and reposted it to YouTube under the “DeSmog Blog” label:
You can watch it here:
http://www.youtube.com/watch?v=P_0-gX7aUKk 
So much for my “censorship”, feel free to view it. You see, I’ve had lots of angry criticism in the last two years, this is nothing new, so I’m not really concerned about the criticisms.
When viewing, note the graph from NCDC in the video which “proves” my surfacestations project is (choose your own derogatory word). More on that momentarily.
The alarmosphere was reaching a tipping point. I knew it was only a matter of time before somebody would blog the coup de grace, and yet; I still haven’t heard from Mr. Sinclair so I could tell him about what I’d like changed.
OK. But if Mr. Sinclair had contacted me (like a journalist would) before he made his video, instead of simply reading the NCDC Talking points memo (revised version seen here, PDF) he could have found out a few things, such as:

NCDC used an old outdated      version of my data set (April 2008) they found on my website and assumed      it was “current”. Big mistake on their part. Big admission of not overly      concerning himself with first-hand knowledge, or even substance, on his      part.
NCDC did not contact me about      use of the data. The data, BTW is not yet public domain, though I plan to      make it so after I’ve published my paper. So like Mr. Sinclair,      technically they are also in violation of copyright. Surfacestations is a private project, I emphasize, what with the public-private concept being one of      the major precipitors of the alarmosphere’s angst.
That data NCDC found had not      been quality controlled, many of the ratings changed after quality control      was applied, thus changing the outcome.
When notified of this, they      did nothing to deal with the issue, such as notifying readers.
NCDC published no      methodology, data or formula used, or show work of any kind that would      normally be required in a scientific paper.
The author is missing from      the document thus it was published anonymously. Apparently nobody at      NCDC would put his or her name on it.
When notified of the fact      that the author’s name Thomas C. Peterson (of NCDC) was embedded in the  properties of the PDF document (which happens on registration of the Adobe      Acrobat program, causing insertion in all output), NCDC’s only response      was to remove the author’s name from the document and place it back online. It is odd behavior for a scientist to publish work but not put your name on it.
NCDC got the number of USHCN      stations wrong in their original document document graph, citing 1228 when      it is actually 1218 I notified them of this and they eventually fixed it.
That NCDC original document      did not even cite my published work,  or even use my name to credit      me. I have the original which you can view here Note also the name in the document properties and      the number of USHCN2 stations above the graph.

I’m regularly lambasted for publishing things here that are not “peer reviewed”. But, when NCDC does it, and does it unbelievably badly, not only is the “talking points memo” embraced by the alarmosphere as “truth” and “falsification”, but NOT ONE of those embracing it show the remotest interest in questioning why it fails to meet even the basic standards for a letter to the editor of a local newspaper.
My own local paper wouldn’t publish a letter or memo where the author is not identified. Yet an anonymous NCDC memo the author won’t even own up to is considered “climate truth”.
Students of the alarmists may have noticed some time ago, how the burden of proof and quality of publication shifts when the other side of the aisle is doing the talking.  In fact, nobody who has jumped into the fray has asked me any questions, yet take as accurate our gift-card designer cum climate scientist Mr. Sinclair at his word, without asking me a single question.
I guess it doesn’t matter now, The Good Ship Teachable Moment has sailed, now that “Big Smog” has stepped in as the defender of freedom. I think Mr. Grandia is hoping that I’ll file a copyright complaint against him.
But here is the kicker. Once you sort through all the ad homs in the video, you find the nugget. It involves that graph that Mr. Sinclair cites from the NCDC Talking Points Memo. If he had asked, he would have found out that it has some pretty embarrassing flaws.

Figure 1. From the NCDC Talking Points Memo.
As referenced in the text of the NCDC  Talking Points Memo, the Figure1 graph compares two homogenized data sets, and demonstrates an uncanny correlation. Here is what they say:
Two national time series were made using the same homogeneity adjusted data set and the same gridding and area averaging technique used by NOAA’s National Climatic Data Center for its annual climate monitoring.
Seems reasonable, until you understand what “homgenization” really is.
What’s “homogenization” you say? Some kind of dairy product treatment?
Well no, not quite. It is data that has been put through a series of processes that render it so the end result is like comparing the temperature between several bowls of water that have been mixed together, then poured back into the original bowls and the temperature measured of each. What you get is an end temperature for each bowl that is a mixture of the other nearby bowl temperatures.
Here’s another way that is more visual. Think of it like measuring water pollution. Here’s a simple visual table of CRN station quality ratings (as used in my book) and what they might look like as water pollution turbidity levels, rated as 1 to 5 from best to worst turbidity:


In homgenization the data is weighted against the nearby neighbors within a radius. And so a station the might start out as a “1” data wise, might end up getting polluted with the data of nearby stations and end up as as new value, say weighted at “2.5”. Our contributing author John Goetz explains how even single stations can affect many many other stations in the GISS and NOAA data homogenization methods carried out on US surface temperature data here and here.

In the map above, applying a homogenization smoothing, weighting  stations by distance nearby the stations with question marks, what would you imagine the values (of turbidity) of them would be? And, how close would these two values be for the east coast station in question and the west coast station in question? Each would be closer to a smoothed center average value based on the neighboring stations. Of course this isn’t the actual method, just a visual analogy.
So, essentially, NCDC’s graph is comparing homogenized data to homogenized data, and thus there would not likely be any large difference between “good” and “bad” stations. All the differences have been smoothed out by homogenization  pollution from neighboring stations!
The best way to compare the effect of siting between groups of stations is to use the “raw” data, before it has passed through the multitude of adjustments that NCDC does. Admittedly, raw data can have its own problems, but there are ways my friends and I at the Pielke research team can make valid station trend comparisons without making numerical adjustments to the actual data raw data.
And finally for those who say “Watts doesn’t want you to see this video” or “he fears the science”, I direct you to this WUWT entry, dated June 26th, 2009:
NCDC writes ghost “talking points” rebuttal to surfacestations project
I was the first one to report on the NCDC Talking Points Memo. Fearing science, video and all that, I chose to publicly blog on a subject critical and even damaging to my own research, knowing full well others would pick it up, including those who would not treat this even-handedness kindly.
The document is an internal memo for NOAA. It didn’t get wide attention after it was first published on June 9th, in fact I don’t think it got any attention at all.
Without my pulling it out of internal memo obscurity and discussing it on WUWT, Dr. Pielke likely wouldn’t have commented on it, McIntyre wouldn’t have written about it,  twice, and thus from all the pickups from those articles, Mr. Sinclair probably wouldn’t have ever seen it. Surely there would not be this delightfully entertaining, rather revealing, and grade school caliber commentary had I not sought to publish it to a wide audience.
But that’s OK. The result is not something I fear, even if my final analysis shows the USA trends are unaffected. There are other things we know and will learn that are of significance.
In fact I’ve had some very positive things come out of this, both on the media and scientific side. Some offers and ideas have been floated.
But that’s a story that will have to wait. Maybe Mr. Grandia will place an online demand for it. Stay tuned. They rarely disappoint.
Oh, and I got to “meet” Mr. Sinclair, the father of a college-age kid though not quite  the young college kid I expected:

 On Climate, Comedy, Copyrights, and CinematographyThe good news: there’s new and exciting opportunities opening themselves to us.The bad news; some people are hilariously unquestioning.

It has been an even more entertaining than usual couple of days in the alarmosphere. I’d been traveling the last week, doing TV station work and station surveys. While on the road I discovered through an email that I was the subject of a YouTube Video called “Climate Crock of the Week”.
The video was about my surfacestations.org project and was titled “What’s up with Watts?”. It was sad and funny at the same time, and as is typically the case with our old friends it was directed at me personally, far more than it tried substance. Equally typically, and sadly, what substance it tried turned out to be wrong. I continued on my travels, My friend Dr. Roger Pielke Sr. posted an opinion on it last week to address some of the issues.
Little did I know bizarro land awaited upon my return home.
Sitting down Saturday night, to watch the video again detecting through its exquisite subtleties and nuance. I couldn’t help but laugh, because once again I noticed that everything reported in it was just wrong.
In fact, it probably was the worst job of fact-finding I had ever seen, which as WUWT readers know, is a bold assessment. I’ve been involved in broadcast TV news for 25 years, and have seen some really bad work from greenhorns fresh out of reporters school. This video reminded me of those. It was if whoever put it together had never researched it, but just strung together a bunch of graphics, video, photos, and the most monotone Pat Paulsen narration I’d ever heard. I figured it was probably just an overzealous college student out to save the world and this was some college project. It had that air of  radical burningman quality about it.
Curiosity piqued, I inquired into just who is this climate Solon? To my surprise, he turned out to be an “independent film producer” working out of his house in Midland, MI under the name “Greenman Studio”, one Peter Sinclair, a proud graduate of Al Gore’s Climate Camp. I still figured him to be a kid and imagined his mom was yelling down into the basement “Peter that’s too loud, turn it down!”.
I also wondered if it was the same “Green Man” that had once prompted surfacestations volunteer Gary Boden to create this nifty patch:

This came about because my now defunct local “Alternate Weekly” had a ghost writer named “green man” who penned and unintentionally (I think) editorial about me and the www.surfacestations.org project back in 2007 in which he wrote the famous line:
“The Reverend Anthony WTF Watts and his screeching mercury monkeys…”
…in response to our daring to survey the weather stations nationwide.
What was funny is that in my original story, one of my commenters posted a funny comment about well, “green stuff” and the editor of the local “Alternate Weekly” went ballistic and demanded I remove it  and gave me a stern lecture on libel. I was happy to comply not out of legal obligation but courtesy and deleted the comment.
Is this Green Man the same guy? Inquiring minds want to know.
OK back to the present. I checked my email for some correspondence from Mr. Sinclair for the past week and found none, and looked back even further to see if he had contacted me about the surfacestations project weeks before in email or in my letters pile. I found nothing and was surprised that he had made a video using my work without at least a basic request or notice.  Normally when somebody wants to publish something in another media type (that is not a blog or webpage) from the surfacestations project or my blog, they contact me and ask permission to use the items. The word normal, however, upon scrutiny really doesn’t apply here.
I’ve gotten dozens of such requests from magazines, newsletters, book publishers, and TV stations. So far, I’ve never said no to any request for such materials or copyright waivers. I’ve filled out lots of forms granting my copyright waiver for the legally skittish that need more than an email or “sure, go ahead” over the phone.
But, in the video Mr. Sinclair produced and posted on YouTube, I noticed that he did in fact use photographs and graphics from my published book “Is The U.S. Surface Temperature Record Reliable?”.  I hold the copyright on this book. The notice for copyright is in the inside front cover.  © 2009 Surfacestations.org  All rights reserved, including the right to reproduce this report or portions thereof in any form.  ISBN 13: 978-1-934791-29-5  and ISBN 10: 1-934791-26-6.
There was also a Warner Brothers video clip from the movie “Anchorman” with a segment about the incompetent TV weatherman which I assume was added to portray me in my chosen career, and amazingly (and most amusingly) there was another video clip from the movie “The Adventures of Buckaroo Banzai” which is a campy sendup of “War of the Worlds”. Interestingly in  the credits, and I know this because I happened to watch the movie about two weeks before on Showtime, there is a “John Van Vliet” listed in the credits. It made me wonder if it is the same John Van Vliet that created the “opentemp” program launched just a couple of months after I first started the surfacestations project in an attempt to derail it early on. He made the mistake of using incomplete data. More on incomplete data later.
I noted that neither clip was from the trailers you could find on YouTube and were of high quality, so maybe they were cribbed from a DVD or perhaps an Apple video download, since I recognized from the editing effects that Mr. Sinclair owns a Macintosh. WB has some pretty stringent clip licensing requirements, which I know from doing TV news and a reporter wanting once to use part of a film from WB in a special news report. WB wanted our TV station to pay, but the cost was sky high for our small TV station. They finally whittled it down to something we could afford.
Doing a little more research, I found that Mr. Sinclair does a series of animated online greeting cards, which you can see here:
http://www.care2.com/ecards/bio/1023
I thought this one was pretty funny: http://www.care2.com/send/card/0840
The description portrayed him as a pretty nice guy with an alternate minded view of the world like a lot of college students have. He is not a college student, though he has a son who is of college age, a nice Ron Paul supporter, I am told from someone who has met him. His rather conservative son, contrasts the rather left-wing eco-activist ad hominem and rhetorically unrestrained father(see here). It is almost humorous greeting card-worthy, this role reversal.
But since he had used that © symbol, Mr. Sinclair demonstrated awareness of copyright protections, having availed himself of them, e.g., here, right below his own artwork.  With knowledge of this and ad hominem attacks made on me personally, I reasonably presumed his copyright violation on my part was likely intentional. I also figured that this might be a teachable moment, as I was still thinking this is a kid just out of college since there seems to be no business website for Greenman studio in operation yet, it is still “under construction”.
http://www.greenmanstudio.com/
And, I mused, by bringing the copyright issue to his attention, I’d probably be doing him a favor, since I surmised he’d be at risk for using the film clips. I figured anybody working a business out of a house without an operating web page probably can’t afford licensing fees. No deep pockets there. I certainly have no personal beef with Mr. Sinclair, it is just the copyright issue.
But my copyright had been ignored, with evidence that Mr. Sinclair as a publisher himself using the © symbol understands copyrights, and WB’s copyright also looked like it also had been ignored. And well, lets face it, he got the facts wrong about the project and never contacted or interviewed me to get any facts from my side (more on that later). So it could hardly be defined as “journalism” and the protections that such enterprise affords for “fair use”. So I filled out the form for copyright issues on YouTube, and pressed enter.
What I expected to happen is that I’d get an angry email or blog comment from the guy, I’d suggest to him (privately) to make a couple of modifications, grant him a copyright for the factual graphics from the surfacestations project, and tell him to put his video back up on the web. End of story, lesson learned.
What I didn’t expect was the alarmosphere going into berserk overdrive. After all, this was not yet a “weekday” which it increasingly seems to be what we call those periods when our friends lapse into said mode. It turns out that YouTube put my name and the surfacestations.org URL up on the video pane for the former video, made me a target for hatred by the “scream first, ask questions later” types.
The first hint of this started on Sunday when I got a comment on my blog. The commenter, who obviously didn’t know the difference between copyright law and constitutional law wanted to know why I had “denied free speech” to Mr. Sinclair. Of course, “free speech” protections involve state infringement and,as powerful as our friends do apparently believe I have become, neither am I the state nor was the state involved here, so the angst was yet again rather misplaced. Regardless, I also thought it this a pretty odd comment, since Mr. Sinclair still hadn’t contacted me, and I paid no attention to it.
Then I began receiving more odd comments, and I’m thinking; “why are these people making a private copyright dispute their personal business?”
Here’s sampling of  a  few comments I got that never made WUWT:
“Watts you are a coward chickesh** no good dumba** weatherman hiding behind a law that you’ve irrationally applied”
“You can’t handle the TRUTH, if I were Jack Nicholson I’d kick your a**”
“Wattsup, you and your stupid picture book project are toast!”
I even got comments from “Omar” in Finland:
“Looks like your attempt to smother and censor information has fired back badly on you Mr Watts: Do you have – how you say – the cahones to explain yourself? I think not. You appear to be a child coward man.”
And around the alarmosphere all sorts of curious accusations of censorship — again, with the long arm of the state nowhere to be found, this seemed to be a variant of the Tim Robbins (see also “paranoid” and “uncomprehending”) School of Crying “Censorship”. Even more bizarre, were the demands. On the “DeSmog Blog”, Kevin Grandia lambasted me for not knowing anything about law, and then demanded I email him and explain myself and my reasons for filing a copyright complaint. I’m no lawyer, but clearly giving details of a dispute to an angry third party not involved isn’t right up there with sound legal advice.
Still apparently confused that his dispute lay not with me but with YouTube or the concepts of intellectual property, when that didn’t get the required response, Mr. Grandia posted another angry column over on the Huffington Post, and made the same demand. He’s wondering why I haven’t responded directly to him.
Really.
But being that guardian of smoggy freedom, Mr. Grandia took it a step further, and, in a rather ironic follow-up to his seizing of the mantle of all that conforms to the laws, somehow located the original YouTube video and reposted it to YouTube under the “DeSmog Blog” label:
You can watch it here:
http://www.youtube.com/watch?v=P_0-gX7aUKk 
Note the graph from NCDC in the video which “proves” my surfacestations project is (choose your own derogatory word). More on that momentarily.
The alarmosphere was reaching a tipping point. I knew it was only a matter of time before somebody would blog the coup de grace, and yet; I still haven’t heard from Mr. Sinclair so I could tell him about what I’d like changed.
OK Nut if Mr. Sinclair had contacted me (like a journalist would) before he made his video, instead of simply reading the NCDC Talking points memo (seen here, PDF) he could have found out a few things, such as:

NCDC used an old outdated      version of my data set (April 2008) they found on my website and assumed      it was “current”. Big mistake on their part. Big admission of not overly      concerning himself with first-hand knowledge, or even substance, on his      part.
NCDC did not contact me about      use of the data. The data, BTW is not yet public domain, though I plan to      make it so after I’ve published my paper. So like Mr. Sinclair,      technically they are also in violation of copyright. Surfacestations is a private project, I emphasize, what with the public-private concept being one of      the major precipitors of the alarmosphere’s angst.
That data NCDC found had not      been quality controlled, many of the ratings changed after quality control      was applied, thus changing the outcome.
When notified of this, they      did nothing to deal with the issue, such as notifying readers.
NCDC published no      methodology, data or formula used, or show work of any kind that would      normally be required in a scientific paper.
The author is missing from      the document thus it was published anonymously. Apparently nobody at      NCDC would put his or her name on it.
When notified of the fact      that the author’s name Thomas C. Peterson (of NCDC) was embedded in the      properties of the PDF document (which happens on registration of the Adobe      Acrobat program, causing insertion in all output), NCDC’s only response      was to remove the author’s name from the document.
NCDC got the number of USHCN      stations wrong in their original document document graph, citing 1228 when      it is actually 1218 I notified them of this and they eventually fixed it.
That NCDC original document      did not even cite my published work,  or even use my name to credit      me. I have the original which you can view here Note also the name in the document properties and      the number of USHCN2 stations above the graph.

I’m regularly lambasted for publishing things here that are not “peer reviewed”, but when NCDC does it, and does it unbelievably badly, not only is the “talking points memo” embraced by the alarmosphere as “truth” and “falsification”. Not ONE of those embracing it show the remotes interest in questioning why it fails to meet even the basic standards for a letter to the editor of a local newspaper. My own local paper wouldn’t publish a letter or memo where the author is not identified. Yet an anonymous memo the author won’t even own up to is considered climate truth.
Students of the alarmists may have noticed some time ago, how the burden of proof and quality of publication shifts when the other side of the aisle is doing the talking.  In fact nobody who has jumped into the foray has asked me any questions, yet take our gift-card designer cum climate scientist Mr. Sinclair at his word that what he reported, without asking me a single question, is accurate.
I guess it doesn’t matter now, The Good Ship Teachable Moment has sailed, now that “Big Smog” has stepped in as the defender of freedom. I think Mr. Grandia is hoping that I’ll file a copyright complaint against him.
But here is the kicker. It involves that graph that Mr. Sinclair cites from the NCDC Talking Points Memo. If he had asked, he would have found this out.

Figure 1. From Talking Points Memo.
As referenced in the text of the Talking Points Memo, the NCDC graph compares two homogenized data sets. What’s that you say? Some kind of dairy product?
Well no, not quite. It is data that has been put through a series of processes that render it
such that end result is like comparing the temperature of several bowls of water
[need work here and diagram to explain homgenization of data]
And finally for those who say “Watts doesn’t want you to see this video” or “he fears the science”, I direct you to this WUWT entry, dated June 26th, 2009:
NCDC writes ghost “talking points” rebuttal to surfacestations project
I was the first one to report on the NCDC Talking Points Memo. Fearing science, video and all that, I chose to publicly blog on a subject critical and even damaging to my own research, knowing full well others would pick it up, including those who would not treat this even-handedness kindly.
The document is an internal memo for NOAA. It didn’t get wide attention after it was first published on June 9th, in fact I don’t think it got any attention at all.
Without my pulling it out of internal memo obscurity and discussing it on WUWT, Pielke likely wouldn’t have commented on it, McIntyre wouldn’t have written about it,  twice, and thus from all the pickups from those articles, Mr. Sinclair probably wouldn’t have ever seen it. Surely there would not be this delightfully entertaining, rather revealing, and grade school caliber commentary had I not sought to publish it to a wide audience.
But that’s OK. The result is not something I fear, even if it shows the trends are unaffected. There’s other things we know and will learn.
In fact I’ve had some very positive things come out of this both on the media and scientific side. Some offer and ideas have been floated.
But that’s a story that will have to wait. Maybe Mr. Grandia will place an online demand for it. Stay tuned. They rarely disappoint.
Oh, and I got to “meet” Mr. Sinclair, the father of a college-age kid though not quite the young college kid I expected:

 



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93feb2d1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterA recently released BP report here shows that global coal consumption has risen over the last 10 years by almost 50%. So wouldn’t you think that all those millions of tons of emitted CO2 (food for plants) as a result would drive the global temperatures up? Have temperatures risen along with all that extra coal burning?
No they haven’t. In fact they’ve dropped slightly over the same period. So go figure!

The blue line shows skyrocketing global coal use, yet global temperatures have fallen. 
In the above chart the blue line shows global coal consumption, data taken here, Review of World Energy. According to the report, India and China alone are responsible for 90% of the world’s coal consumption increase, while renewable energy in the 2 countries plays nary a role. According to BP figures, global CO2 emissions rose 5.8% in the year 2010.
The International Energy Agency (IEA) says that China will add a whopping 600 gigawatts of coal power plant capacity by the year 2035, equivalent to the current capacity of the USA, EU and Japan – combined! So as China adds one coal power plant each week, Europe and the USA are lucky to get a single one approved during an entire year.
Demand for coal is not about to change directions any time soon. The IEA estimates that the global population will climb to 8.5 billion people by the year 2035. That means a huge growth in demand for power. Already today the sad truth is that 20% of the global population still has no access to electricity. Forcing the prices up with CO2 emission trading schemes and carbon taxes will only make the situation worse for the very poor.
But now that we know burning coal has hardly a noticeable impact on temperature and climate (zero-correlation), it’s high time to double our efforts in producing more coal so that the world’s demand can be satisfied so that bitter poverty may be alleviated once and for all.
Share this...FacebookTwitter "
"**Home Secretary Priti Patel was warned on several occasions to treat staff with respect, a former top official who resigned over the row has told the BBC.**
An inquiry found Ms Patel's behaviour broke the rules - although she was ""unaware"" of her conduct.
But ex-official Sir Philip Rutnam has contradicted this, saying Ms Patel was told not to shout and swear at staff a month after becoming home secretary.
The home secretary has apologised for any ""upset"" she caused people.
But she insisted the inquiry's findings had made clear that ""issues were not pointed out"" to her in the course of the department's ""deeply challenging"" work.
The PM has said he does not believe Ms Patel is a bully and regards the matter as closed despite the resignation of his adviser on ministerial standards, Sir Alex Allan, whose report found Ms Patel's conduct fell below the standards expected of government ministers.
Sir Philip Rutnam quit as permanent secretary at the Home Office in February, after complaining about Ms Patel's conduct, and is currently suing the government for unfair dismissal.
He said he was never asked to contribute to the bullying inquiry despite resigning over the matter.
Furthermore, he maintains that Ms Patel was ""advised"" on a number of occasions about the need to treat staff with respect, despite the report saying she had been ""unaware"" of her conduct.
""As early as August 2019, the month after her appointment, she was advised that she must not shout and swear at staff.
""I advised her on a number of further occasions between September 2019 and February 2020 about the need to treat staff with respect.""
Sir Philip said he respected Sir Alex and regretted his resignation but took issue with the ex-adviser's suggestions that the Home Office did not provide sufficient support to Ms Patel when she took on the job.
""Enormous efforts were made from top to bottom in the Home Office to support the new home secretary and respond to her direction and significant achievements have resulted. The advice does not fairly reflect this.""
In his report, Sir Alex said there was ""no evidence"" that Ms Patel was aware of the impact of her behaviour, and no feedback was given to her at the time"".
The BBC's political editor Laura Kuenssberg said Sir Philip's comments were important because they challenged this verdict.
Ms Patel has given an ""unreserved apology"" for any upset caused, which she said ""was completely unintentional"", but argued she was not ""supported"" by her department at the time when claims were made.
Mr Johnson has said he retains ""full confidence"" in Ms Patel while Tory MPs have rallied behind her.
Former minister Simon Clarke told the BBC she had been working ""under the most challenging circumstances to bring about major change in a department that needs it"".
But the head of the Committee on Standards in Public Life, Lord Evans, said there were ""serious questions"" about the process for investigating breaches of the ministerial code which must be ""urgently"" looked into."
"China will not be sending ducks to Pakistan to chomp through a plague of locusts after all, an expert from Beijing’s troubleshooting team has said. A report in the Ningbo Evening News had said 100,000 ducks would be sent from Zhejiang province to Pakistan to deal with its worst locust invasion in two decades, generating 520m views on China’s Weibo social media platform on Thursday and thousands of comments.  China deployed ducks, whose natural diet includes insects, to fight a similar infestation in the north-western Xinjiang region two decades ago, reportedly with considerable effectiveness. Despite the popular support for the idea in a country where cute duck memes have become hugely popular, Zhang Long, a professor from China Agricultural University told reporters in Pakistan the ducks would not be suited to the conditions there. “Ducks rely on water, but in Pakistan’s desert areas, the temperature is very high,” Zhang said. Zhang, part of a delegation of Chinese experts sent to help the south Asian country combat the locusts, advised the use of chemical or biological pesticides instead. The locusts have already caused extensive damage in east Africa and India. Locust swarms can fly up to 150km (90 miles) a day with the wind, and eat as much in one day as about 35,000 people. The Ningbo Evening News had quoted Lu Lizhi, a researcher from the Zhejiang Provincial Institute of Agricultural Technology, as saying the use of ducks was much less expensive and environmentally damaging than pesticides. “Ducks like to stay in a group, so they’re easier to manage than chickens,” he said. A duck is also capable of eating more than 200 locusts per day, compared to just 70 for a chicken, Lu said.  • This article was amended on 27 February 2020 after Zhang Long rejected the Ningbo Evening News report that China was going to dispatch ducks to Pakistan"
"
This makes a lot of sense if you are a rational thinking person. I thought I’d alert WUWT readers to it. Below is a table from the front page of Spaceweather.com today, operated by NOAA and Dr. Tony Phillips. 


And this week, we saw what can happen when PHA’s come calling:

So in light of that, I thought this article was rather interesting.
Death from the Skies = Boring, Sweat from GHGs = Sexy [Jonah Goldberg]
Published at The Corner, part of NRO
From a longtime reader:
Dear Jonah,
I thoroughly enjoyed your article today, and not just because you touched on an area where I worked – at least tangentially – for over a decade.  You are right, virtually nobody is doing the leg work on keeping track of all the debris and potentially nasty sized rocks out there compared to the number of people shrieking about our impending slightly warmer earth.  The big reason is that it isn’t very sexy work, unlike being a proponent of Anthropocentric Global Warming (AGW).  If you work on space debris, minor planet orbits and earth crossing orbits about the best you can hope for is getting to name a new rock nobody else saw, or maybe getting your name in the paper while being misquoted by some reporter who doesn’t have a clue about what preliminary results or margin of error means when he says that your recently discovered rock will destroy the earth in 2029.
By comparison if you use your computer model to predict that according to your model the earth might possibly warm by somewhere between 0.9 and 3.5 degrees Celsius by the year 2100 you get to hang out with Al Gore and Bono and morally scold the ignorant proles for driving their SUVs to pick up the kids from daycare as you jet off to Switzerland for another speaking engagement.  Of course there is one other distinction.  The guy cataloging rocks is actually doing science, and that’s hard work.
One of the problems many people, especially scientists, are starting to have with the AGW proponents is their use of shrill tone and authority of numbers to try to stifle debate.  Science is not consensus, and though there can be a scientific consensus that doesn’t constitute science either.  Computer models predicting conditions 50 years from now in a system as complex as the earth aren’t within spitting distance of science.  To be science something has to be testable and falsifiable. It must produce a predicted data point, interaction or outcome that is unique to the theory and can be verified or falsified.  Would you bet your future on the accuracy of day seven of a seven day weather forecast?  That is essentially what we are being told by the AGW proponents we absolutely must do without delay.  Of course I think the without delay part has more to do with “We must pass the stimulus without delay” or “We must pass healthcare without delay” considerations than any notion that waiting three or four years will actuall make any long term difference.
read the rest of the article at The Corner
h/t to Planet Gore


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94499df5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Jonathan Brearley might be the most important British climate pioneer you’ve never heard of. In the past 10 years, the former civil servant has quietly steered the UK to its first climate legislation, and then to the policy framework responsible for creating Britain’s cleanest electricity system since the 1880s. As the new chief executive of Ofgem, Brearley plans to inject climate action into the core of the UK’s energy regulation, too. This may prove his toughest challenge yet. Brearley’s first task as Ofgem’s new boss was to answer critics who claimed the watchdog was out of touch with the UK’s “net zero” agenda because its rigorous checks and balances were founded upon the duty to protect customers, not the environment. In the balancing act between ambitious green spending and home energy bills, many companies feared that Ofgem would err on the side of short-term savings. Brearley’s nine-point plan to prioritise climate action seeks to prove the doubters wrong. “We take net zero very seriously, and as seriously as our other objectives. What you will see is us making sure that this is factored into all our decisions in quite a fundamental way.” Brearley joined the energy regulator in April 2018 to lead Ofgem’s energy networks team after stints within government building the foundations of Britain’s clean energy system. First, as the lead director behind the UK’s first climate legislation, the 2008 Climate Change Act, then as the architect of the government’s clean energy reform, which ignited a swing from fossil fuels to green electricity. His wide-ranging green strategy aims to help get 10m electric vehicles on our roads by 2030 and support a fourfold increase in offshore wind generation and a shift towards low-carbon heating. Ofgem is still committed to keeping a lid on energy bills, but will also lead a crackdown on “greenwash” energy deals and push for new tariffs that encourage homes to help balance the energy system. Brearley plans a rewiring of the energy regulator itself, too. He says Ofgem’s 1,000-strong workforce plans to “get out of the building” and beyond the routine box-ticking required of every industry regulator. “We know we need to be faster moving, more engaged with the world that we’re working in, and these things will be part of what we’ll do as we change over the next few years.” He admits there are “absolutely” tensions between plans for a bold, green future and the need to safeguard vulnerable homes, which risk being left behind. This challenge may prove greater for Brearley than his predecessors. The pace of change in the energy market will mean more investment in a low-carbon energy system, paid for through energy bills, as well as a rush of disruptive new technologies and companies into the market. Brearley keeps one particular customer in mind. She is of retirement age, living in social housing, and was being overcharged hundreds of pounds by her existing energy supplier. It took almost six months to fix the problem with the help of an energy adviser at a charity. “Two things struck me when I met her,” says Brearley. “The first is how much of a struggle it was for her to fix something which represented an important amount of money in her life. But the second was that when I asked her if she planned to switch away from her supplier she said she wouldn’t – because she’s just not comfortable choosing a new supplier.” “There are people who are poised to benefit from changes in the energy system, but those who will find it difficult. That means making decisions which are based on the investment that we need to get to net zero, but also how we can share those costs out in a way that’s fair,” Brearley says. In the past, Ofgem has been criticised for overseeing “rip-off” energy deals and poor customer service from suppliers, while network companies profiteered at the expense of rising bills. Brearley says Ofgem “will be less afraid of stepping in where we feel that customers are being harmed” and will “be driving a very hard bargain” with energy network owners. “I’m sure at some point these companies will claim that they can’t make investments without higher rates of return. Our job is to stick to the evidence, to stand firm, and get what we think is a fair deal for customers,” Brearley says. “I am very clear that we won’t make this transition to net zero if customers feel ripped-off.”"
"

Circling Yamal 3 – facing the thermometers
Guest post by Lucy Skywalker

Let’s look closely and compare local thermometer records (GISS) with the Twelve Trees, upon whose treerings depend all the IPCC claims of “unprecedented recent temperature rise”.
For my earlier Yamal work, see here and here. For the                        original Hockey Stick story, see here and here.
Half the Hockey Stick graphs depend on bristlecone                        pine temperature proxies, whose worthlessness has already                        been exposed. They were kept because the other HS graphs, which depend on Briffa’s Yamal larch treering series, could not be disproved. We now find that Briffa calibrated centuries of temperature records on the strength of 12 trees and one rogue outlier in particular. Such a small sample is scandalous; the non-release of this information for 9 years is scandalous; the use of this undisclosed data as crucial evidence for several more official HS graphs is scandalous. And not properly comparing treering evidence with local thermometers is the mother of all scandals.

I checked out the NASA                        GISS page for all thermometer records in the vicinity of Yamal and the Polar Urals, in “raw”, “combined”, and “homogenized” varieties. Here are their locations (white). The Siberian larch treering samples in question come from Yamal and Taimyr. Salehard and Dudinka have populations of around 20,000; Pecora around 50,000; Surgut around 100,000; all the rest are officially “rural” sites. Some are long records, some are short.

Russia has two problems. First, many records stopped or became interrupted around 1990 after the ending of Soviet Russia; worst affected are the very telling Arctic Ocean records. Second, during Soviet Russia (and possibly now for all I know), winter urban records were “adjusted” downwards so that the towns could claim more heating allowances. Nevertheless, it will become clear that these issues in no way impede the evidence regarding treerings.
 
Click to enlarge these graphs. The first shows the 20 GISS stations closest to Yamal and the Polar Urals. The second shows treering width changes over time (only 10 of the 12 trees here). This was supposedly compared with local thermometer records, and used to calibrate earlier treering widths as temperature measurements to create a 1000-year temperature record. It was a pig to turn these graphs into a stack of transparent lines at the same scale as the GISS records for comparison, but finally, interesting material started to emerge.

I scaled all the GISS thermometer records to the same temperature scale, and ran them all from 1880 to 2020 at the same time scale (GISS graphs do not do this). I overlaid them as transparent lines along their approximate mean temperatures for comparison. Mean temperatures (visually judged) vary from around -2ºC (Pecora) to -13ºC (Selagoncy, Olenek, Hatanga, and Ostrov Uedine) and even -15ºC (“Gmo Im E.K. F”). The calibrations are degrees Centigrade anomaly, and decades.

Ha! Straightway we see clear patterns emerging.                      Let’s agree them:
Thermometer records: (1) time-wise, thermometers show temperatures rising from 1880 to 1940 or so; (2) temperatures fall a little from 1940 to 1970; (3) temperatures then rise a little but do not quite regain the heights of the 1940’s; (4) despite mean temperatures ranging from -2ºC to -15ºC (total means range 13ºC), and a range of temperature                      anomalies from each mean of only 9ºC from warmest year to coldest year, when mean temperatures are aligned, clear correlations emerge; (5) there are high variations between adjacent years. We shall investigate all this more closely in a minute.
Treering records: I’ve shown here the full records given for the 10 trees that runs from 1800 to 2000; but below, I use the same timescale as the thermometer records (1880-2020) for comparison. It is useful to see a few things here already: (6) treering sizes are increasing from 1830; (7) before that they show a decrease; (8) they do show correlation from 1880 on (this is NOT proof that the correlation is due to temperature).

Yamal area: (9) The 7 stations around Salehard seem to go in lock step with each other pretty well. (10) The five Yamal treering records (YAD) also correlate with each other, showing spikes around 1910, 1925, 1940, 1955, 1965, and 1980-1990. (11) But the treerings fall out with each other 1990-2000; and (12) these treering spikes do NOT correspond to the thermometer temperature spikes; but (13) there is a slight correlation with the longterm temperature; however, (14) crucially, there is no hockeystick blade in the thermometer record (15) nor is there one in the treering record if we remove the red YAD061 which is clearly an outlier – only a plateau’d elevation of the peaks throughout the 20th century starting before the real CO2/temp rise (and this is actually matched by pre-1800 values at times).
Excuse me for wondering if treerings beat to a different drum than temperature – it is certainly curious that there appears to be something causing correlations in the treerings. Wind? Sunspots? The moon? But let’s check by zooming in a little closer…

Salehard close-up: (16) all the nearby                      thermometer records mirror Salehard closely, although stations are up to 500 miles apart, the range of mean temperatures is -2ºC to -9ºC, and the range of annual temperatures at each station is up to about 9ºC – altogether a remarkable consistency. Click to see animated version of these records. (17) The close                      fit of Mys Kamennij (pale sea-blue) is particularly significant, since it is maritime and rural, and the same distance as Salehard from the treering site (some 120 miles), but in the opposite direction; (18) Ostrov Waigatz (Vaigach Island) shows the same pattern but with greater extremes; (19) in comparison with all this, the treering records show virtually no correlation at all – yet since treering differences between summer and winter exist at all, one would expect to see some correlation with warmer and colder years. (20) Perhaps if a far larger sample were used, a correlation might be detected, but clearly (21) we have trees here that are far too individual – especially YAD061.

Polar Urals: Here are seven station records around the Polar Urals site, compared with the five Taimyr (POR) treering records. (22) Mean temperatures are lower here – further North but also more continental, so perhaps the summers are as warm as Yamal, with similar near-treeline environment. (23) more noise in the temperature record, but the overall pattern is still the same; (24) 1943, 1967, 1983 are warm in common with the Salehard records, and 1940 is cold; other years are harder to compare. (25) The early fragmentary records for Dudinka and Turuhansk still fit together and overlay the Salehard records well, showing clear temperature rise between 1880 and 1940. (26) The treering records are fairly coherent, more so than the Yamal ones, and (27) they fit the Yamal records’ spikes in 1910, 1925, 1940, 1955, 1965, and 1980 on, but (28) again, this does not fit the temperature record.

The best of both record series: Really rural thermometer records from the maritime Arctic: (29) show the strongest pattern yet which (30) fits the other two sets of thermometer records but (31) does not fit the treering records even though (32) the treerings show coherent patterns within themselves, despite the two sites being some 800 miles apart.

Briffa’s full chronology: The Yamal chronology Briffa used (black) is compared with Polar Urals (green) and shows recent temperatures exceeding the Medieval Warm Period but (33) this is highly questionable, as is the recent final uptick. No MWP supports the alarmist “Unprecedented!” yet Polar Urals generally have been shown to fit local thermometer records better than Yamal for the period of overlap.

More GISS Arctic graphs: There are many serious problems with GISS but we can only take the evidence here. (34) GISS 64ºN+ shows a misleading trend line – temperatures rise to 1940, fall to 1970, rise to 2000 but not higher than 1940, then level off after 2000; (35) I don’t know what stations went into this composite – the final uptick alerts my suspicions to some UHI or other station problems; (36) Tamino takes the biscuit for cherrypicked trends in the GISS 80ºN+ North Polar winter record (sea green) – it clearly opposes the general worldwide fall in temperatures 1940-1970. However, it’s interesting to see such extremes.

GISS’ homogeneity adjustments: Thankfully, only a few of these Russian records are “adjusted”. But the alterations are telling. Surgut spikes upwards over Salehard from about 1960 on – but (36) the adjustment (probably UHI) is perversely done by truncating and moving earlier records upwards, instead of adjusting later records downwards. And (37) why were Salehard’s and Ostrov Uedine’s earlier “raw” records omitted in the adjusted records? I think every correction here will tend to amplify global warming trends.

GISS world temperatures, 2008: This map                      was shown in Tingley & Huybers’ latest Hockey Stick presentation                      at  PAGES                      conference. GISS’ own station records around Yamal and Polar Urals appear to show (38) this map is misleading, since according to GISS’ own records, above, averages local to Yamal / Polar Urals after 2000 are at the most 1.5ºC anomaly (above local mean).

CRU Arctic temperatures, seasonal anomalies:                      (graph by romanm) Since this is from uncheckable individual station records, (39) the figures could be contaminated by various “correction” factors, (41) UHI is especially likely in the winter. But note that (42) the difference in character between months, and between summers and winters, is striking – summers have hardly changed – and (43) still no definitive Hockey Stick as per illustrations and per Briffa’s Yamal treering record, nothing beyond the range of natural patterns clearly evidenced here. Even the known slight overall increase during the twentieth century takes place mainly earlier in the century.
Conclusions: There is no sign whasoever of a Hockey Stick shape with serious uptick in the twentieth century, in the thermometer records. Yet these records are clearly very consistent with each other, no matter how long the record or how cold, high, or maritime the locality, with a distance span of over a thousand miles. Neither does the Hockey Stick consistently show in the treerings except in the case of a single tree. Even with thermometer records that are incomplete and suffering other problems, the “robust” conclusion is –
“Warmist” treering proxy temperature evidence is                      falsified directly by local thermometer records.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92451de4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIt wasn’t long ago a PNAS study led by Stefan Rahmstorf had come out claiming sea level rise is “accelerating”. This of course was followed by the mainstream media jumping on the global warming bandwagon and trumpeting doom and gloom would strike sooner than we ever thought, maybe even before we die.
Unfortunately, the acceleration has been in the opposite direction, thus making the authors of the PNAS study look just a bit foolish.
The latest NASA satellite data show that sea levels have dropped 6 mm over the last year – the biggest drop ever recorded since satellite data has been taken. This is hardly the kind of acceleration Rahmstorf had in mind. You’d think the media would be falling all over themselves to report this good news. They have not. Only a tiny few German media outlets have reported the plummeting sea level news.
It’s due to a “weather shift”!
Der Spiegel rolled out a report called: Weather Shift Drops Global Sea Level, authored by Axel Bojanowski, hat-tip Dirk Maxeiner here. Caution: don’t be fooled into thinking Der Spiegel writers have become sceptical. To the contrary, they are cleverly, indirectly, blaming global warming for the “peculiar” sea level drop.
Global warming, you see, leads to weather shifts, which then leads to sea level drop. Hence global warming leads to sea level drop. Of course Der Spiegel will never admit this is what they are claiming, but they do indeed want you to believe it’s all because of “unusual freak weather” (which started when humans started driving SUVs).
The eastern Pacific heated by up to 10°C, huge quantities of water evaporated – and then later the mass of water fell to the ground via numerous storms over South America and later over Australia during the La Niña period.”
As is often claimed with temperature, sea level drop is now weather and sea level rise is climate. To Der Spiegel’s credit, Bojanowski at least admits that sea level rise has slowed down (emphasis added):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




However since 1993, the oceans have been measured by satellites. They have detected a rise of 3 mm per year. During the last eight years, the rate of increase has slowed down.”
Leading German tabloid Bild here also expressed shock that sea levels have dropped by more than half a centimetre over the last year. Here, Bild blames the ENSO (er, weather) for the sea level drop.
Over the last 12 months, more precipitation than usual poured down over the continents, for example the destructive flood in Australia. The blame for this: the especially pronounced weather phenomena El Niño and La Niña.”
German sceptics mock bogus “accelerating” sea level claim
Germany’s online auto-reporter.net expresses doubts about the coming climate catastrophe, citing that back in the 1980s Germans were projecting the end of the forests due to acid rain. 25 years later the forests are as healthy as they have ever been.  Auto.reporter.net questions the supposed sea level rise:
It is supposed to be rising rapidly and submerging many countries. Now scientists have determined that sea level is sinking. […] The causes have yet to be determined. Scientists had expected a continuous increase.
What can we learn from that? That scientists can never exactly know what is happening. And this is the case concerning alleged man-made climate change. It is foolhardy when people think they can impact the climate over 100 years. The political target of limiting the temperature increase to 2°C  is haphazardly selected. […]  We’ll probably laugh about the climate change discussion in 20 or 30 years just as we laugh today over forest die-off, which in reality never came to pass.”
Finally German science publicist Dirk Maxeiner here simply could not contain his urge to mock the alarmists:
Global sea level has dropped by more than half a centimetre over the last 12 months. That equals 5 metres of sea level drop over the next 1000 years – at least that’s what my computer simulation shows. Now how on earth are the island states supposed to cope with all this expanding land? What a catastrophe! We have to immediately form a special commission charged with the task of managing the great transformation of these regions and setting down ecological guidelines. Professor Schellnhuber – it’s up to you!”
Share this...FacebookTwitter "
"

While there’s been much carping about the pork‐​laden, recently enacted farm bill, it turns out to be small fry compared to current energy legislation. If passed intact, HR 4, the “Energy Policy Act of 2002,” will begin the stealth enactment of the infamous Kyoto Protocol on global warming, wisely canned by President Bush a year ago.



Of particular concern is Title X, which requires industry to “voluntarily” report its total emissions of greenhouse gases such as carbon dioxide. “Require” and “voluntary” can only coexist in the goofy world of Washington, as the reporting of carbon dioxide becomes mandatory for all industry under this bill if 60 percent of the nation’s total emissions aren’t “voluntarily” reported.



Who’s kidding whom? The purpose of Title X is to establish some type of baseline for carbon dioxide emissions so that some type of arbitrary “cap” can be legislated. Think of this as a Corporate Average Fuel Economy program for me, thee, and everything we own. This is the deceptive atmosphere that pervades HR 4, which is based upon misleading “findings.” If these “facts” are incorrect or incomplete, what does that say about the subsequent regulations? Let’s examine just two of the many “findings” in HR 4, and propose some modest, more factual revisions. Current “Finding #1”: “The Intergovernmental Panel on Climate Change (IPCC) has concluded…that most of the warming of the last 50 years is ‘attributable to human activities’ and that the Earth’s average temperature can be expected to rise between 2.5 and 10.4 degrees Fahrenheit this century.”



Missing Facts: The earth’s surface temperature has warmed a little over 1 degree in the last 100 years. Half of that warming took place before humans could have caused it, and an additional 10 percent or so of the more recent warming has been caused by changes in the sun. Most of that recent warming is in the coldest air of winter, as predicted by greenhouse theory. In other words, the total warming caused by people is a shade less than a mere half of a degree. 



The U.N. made 245 separate forecasts for the next 100 years, based on different assumptions about energy use. The one that warms over 10 degrees predicts unprecedented changes in both per‐​capita emissions of carbon dioxide and the number of people on the planet. Both fly in the face of reality: Carbon dioxide per capita has been basically constant since we started measuring it nearly 50 years ago, and population projections are being scaled down rapidly as the world’s economies develop. Most of the U.N.‘s other 244 forecasts are in the lower half of the predicted range. An extension of current emission trends produces a warming that is slightly beneath their lowest estimate.



Revised “Finding #1”: “The Intergovernmental Panel on Climate Change (IPCC) has concluded that human beings have contributed to a slight warming of planetary mean temperatures in the past 50 years, largely in the coldest air of the winter. Based upon extrapolation of current carbon dioxide emission trends and latest population projections, the warming of the next 100 years is likely to be around 2.5 degrees. Other, less likely assumptions could produce more warming.” 



Current “Finding #4”: “The IPCC has stated that … global average sea level has risen, oceanic heat content has increased, and snow and ice extent have decreased, which threatens to inundate low‐​lying island nations and coastal regions throughout the world.” Missing Facts: Recent studies of satellite data and submarine records reveal that the rise in sea level due to human‐​induced climate change is at best about 2.5 inches, approximately half of what the U.N. has estimated. This forces a halving of the IPCC’s previous 100‐​year average projection, down to nine inches. Much of the U.S. Atlantic coast has seen much larger sea level rises in the last 100 years because of geological activity. Pretty much no one but a few scientists have even noticed it as we happily adapted, building increasingly expensive beach houses.



Melting of sea ice does not change sea level: Pour yourself a drink and prove it. Melting of land ice does. The “ice balance” in Greenland, the largest ice mass in our hemisphere, is neutral. In Antarctica, the continental sheets are growing, not shrinking. Revised “Finding #4”: “Most recent findings reveal a slight rise in sea level as a result of human activities, but there is no evidence for an increasing trend. Observations indicate that sea level will continue to rise, at a rate that most developed economies have easily adapted to.”



Space doesn’t permit an expanded criticism of other findings, but one of them deserves a Dishonorable Mention: HR 4 cites a government report, which turns out to be the “U.S. National Assessment” of climate change. It is based upon two climate models that perform worse than a table of random numbers applied to U.S. temperatures. For this, and for all the other half‐​truths in HR 4, we’re supposed to start down the economically disastrous road to Kyoto?
"
"
Sometimes, seemingly innocuous posts can bring in some oddball commenters. Such is the case this week with a  post I did about a cloud (or lack thereof) spotted by former NWS Lead Forecaster for northern California, Jan Null.

That post brought out the chemtrailers, one of whom insisted that the “hole punch cloud” was not only a new phenonmenon (it isn’t) but made by (you guessed it) chemicals released from airplanes.
In the strictest sense, he’s right. It is caused by airplane exhaust:
This relatively rare occurenvce is the result of an aircraft flying through a  layer of high clouds that have precisely the right temperatre and moisture.  As the jet aircraft flies through the layer it contributes just enough additonal moisture and exhaust particles for the ice crystals in the cloud to grow large enough to fall out as ”fall streaks”.  This happens in a circular pattern around the path of the jet with a hole in the cloud layer being the result.
Jan Null
SF Weather Examiner
There’s nothing nefarious about it.
But when I didn’t allow the discussion of the ridiculous premise of chemtrails, that didn’t sit well.
Oh, and hey: If you people can’t handle the TRUTH about PICTURES >YOUYOU< can’t handle?!?!
.
That you have the freaking temerity to post BALD-FACED LIES from some JERK professing a ~new~ cloud formation, and thence declare that NOBODY may challenge that PROCLAMATION?
.
Science, you say?
.
Science which can’t =OR WILL NOT= be challenged?
.
YOU ARE NO BETTER THAN THE SNAKE OIL SALESMAN YOU PROFESS TO EXPOSE!
.
Go ahead, MR. WATTS, DELETE THIS POST TOO!
I decided to leave it up as an example.
Ric Werme really said it all with this comment about SHOUTING IN ALL CAPS.
Thus our quote of the week:
Good science doesn’t need all-caps.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93be510a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Environmental campaigners say a cocktail night involving the fossil fuel industry and federal politicians represents an “insidious” lobbying effort to undermine climate action. The pro-coal Liberal MP Craig Kelly and Labor’s Joel Fitzgibbon hosted a cocktail event at Parliament House to discuss carbon capture and storage with industry leaders on Wednesday night.  An invite seen by the Guardian was sent out by Kelly and Fitzgibbon, who chair the parliamentary friends of resources, together with representatives of Santos and the carbon capture body CO2CRC. The event is described as a “cocktail event to mark the inaugural meeting of the CO2CRC Carbon Capture and Storage Policy Forum”. That forum features companies such as BHP, Chevron, Coal21, ENI, Exxon, the Global Carbon Capture and Storage Institute, JPower, Shell and Woodside. The invite says the forum aims to “work with governments, industry and other stakeholders” to create “suitable policy settings and a regulatory framework to accelerate the development and deployment of CCS technology in Australia”. “CCS has the potential to create a new wealth-creating industry for Australia, breathe new life into existing industries by reducing carbon emissions, and underpin the development of new energies such as hydrogen,” the invite said. Environment group 350 Australia says the event shows the need to “crack down on the undue influence of lobby groups on our democracy”. The 350 Australia chief executive, Lucy Manne, said the event was an “insidious effort by the fossil fuel lobby to undermine action on the climate crisis”. Manne said carbon capture and storage had proven a “pipe dream of the coal and gas lobby” and diverted millions away from proven renewables. “The climate crisis has been felt across the country this past summer, with communities suffering due to extreme bushfires, drought, floods and heatwaves,” she said. “It’s outrageous that instead of working out how to rapidly transition to the renewable energy future the vast majority of Australians and businesses want, our elected representatives will tonight be sipping cocktails with the coal lobby and discussing how to extend the life of dirty coal-burning power stations.” Such lobbying is generally hidden from the public unless revealed by the media. The Fitzgibbon-Kelly cocktail event was reported in News Corp papers. It does not appear in any of the transparency measures governing lobbying. Federal ministers are also not required to disclose who they have met with, unlike in states like Queensland and New South Wales. “One of the things that we’re calling for is the politicians to pledge for a whole range of transparency reforms so we have much better transparency around donations,” Manne told the Guardian. “We also want more transparency around lobbying. We do have a lobbying register. But it really doesn’t give us a true picture of who’s lobbying our politicians.”"
nan
"
Share this...FacebookTwitterNTZ contributor and climate observer Matti Vooro provides evidence that the floods of North America are likely linked to cooling, and not warming.
===================================================
Global Cooling – The Real Cause Behind The Catastrophic North American Floods Of 2011
by Matti Vooro
The Canadian Prairies and the United States North Central regions are experiencing one of the worst flood seasons going back some 350 years, read here, here, here, and here. The reason for these floods is not well reported or researched by our media.
These floods stem from the extreme cold and the significant snow extent that fell in the central North America during the past winter. The very significant initial spring snow melt followed by a cool and very wet spring has resulted in more water than the ground could possibly hold. Some areas are having continuous flooding and have received two floods already and may get a third flood as well from new heavy rainfall.
US And Canadian temperatures have been falling 
Unreported by the media are the news that the annual and winter temperatures have been falling over most areas of US and southern Canada [excluding the north] since 1998, and more significantly during the last 4 years [see below]. So how can the floods be due to warming?
Winter temperatures in the USA have been plummeting:
Winter temperatures in the contiguous United States, 1998-2011 (NCDC)
Also in Southern Canada:
Winter temperature departures from 1948-2011 for the Canadian Prairies and Northwest Forest Regions for 1998-2011. Source: Environment Canada
Snow extent is also on the rise:
Snowfall extent is climbing. From Rutgers University
Snow depth in Northern US And Southern Canada in April according to the NOAA:
Source: http://climvis.ncdc.noaa.gov/cgi-bin/cag3/hr-display3.pl
As winters cool, more snow accumulates, which contributes to flooding. As temperatures cool, how can the flooding be due to warming?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




While AGW climate scientists and some world governments are mainly focused on the non-existing global warming, the globe is actually cooling and the impact of this cooling is far greater and more imminent than that of global warming 100 years from now, as these floods clearly illustrate.

This colder and wet weather means that crops cannot be planted and the entire season could be lost for the affected areas. These regions are the bread baskets of the globe and we should all be concerned. Over 1.2 million hectares of farming land may already be lost for farming this year in the Manitoba, and Saskatchewan region alone as recently reported on our news. Similar losses may happen in the United States.
More floods in the future – due to cooling
We are likely to see many more spring floods in the upcoming years like the 2011 floods because the cold winters are returning and could be with us for the next 20-30 years – like we had during the late 1970’s as the Pacific and Atlantic Oceans cool and lose their current heat content. These inland areas of North America are clearly cooling faster than the coastal areas since their climate is not moderated by still warm oceans. This will change in the future as the oceans start to cool and coastal areas will also get colder like Western Europe and Eastern North America.

These events, like the cold and snowy winters, extra flooding and severe tornadoes, have very little to do with man-made carbon dioxide or global warming as the global temperatures have been cooler than normal this winter and the early part of this year, and global and regional temperatures have actually been declining.
The expensive and misguided anti-global warming policies divert valuable funds from other vital areas of our global life, like helping nations who are suffering and experiencing natural disasters, improved flood control, rebuilding homes and infrastructure after tornadoes and major flooding and extra food storage for emergencies as well as job creation, better health care and poverty.
In my judgment this problem could get much worse in the coming years. Like the Pacific Ocean, the North Atlantic Ocean is also cooling again and by 2015 we could begin to feel even cooler weather during the winter and spring especially along the North American eastern coasts and Western Europe. Food and energy could be in short supply unless we all adjust our national and global focus from a non existing global warming threat to a much bigger and very current threat from global cooling for the next 20-30 years.
In summary, the message of this brief article is that we are totally focused on the wrong weather. The impact of global and regional cooling from our winters is much more severe and immediate than any minor impact of global warming 100 years from now, the effect of which may never materialize. Our limited financial resources are being channeled in the wrong direction with little benefit to the planet or to humanity.
Matt Vooro
Share this...FacebookTwitter "
"

My name is Tom Miller. I am director of health policy studies at the Cato Institute, a nonpartisan public policy research institution headquartered in Washington, D.C. I would like to thank Chairman Underheim and Rep. Weickert for inviting me to testify today about what can be done about rising health care costs and, in particular, the possible role of medical savings accounts in restraining the growth of health care costs and making health care more affordable and available.



This committee has already heard quite a bit of testimony regarding the various possible factors behind the return of annual double‐​digit percentage increases in health care spending and health insurance premiums. Some of those factors may remain outside the immediate reach of public policy, such as increased consumer demand for health services in an aging and relatively wealthier society. Other factors include a mix of both higher costs and even more highly valued benefits, such as more effective prescription drugs and innovative technological advances in medical diagnostics and treatment.



Indeed, our society may well decide to spend even higher and higher shares of our nation’s resources on health care in future years — as long as someone, somewhere can be found to foot the bills — but American consumers will receive more value for each dollar they spend only if the distorting effects of government’s multiple role as a regulator, purchaser, and subsidizer of health care are reduced. Our objective should be neither to artificially keep spending levels higher, nor lower, than their market‐​determined costs, but instead to allow individual consumers to seek the best value that balances their spending preferences and priorities with the resources that they can command. I’ll discuss a little later how best to sort out the respective roles of efficient market‐​based mechanisms for delivering health care and societal goals of safety net care, income security, and welfare assistance.



A number of the cost‐​drivers behind higher health care bills are related in part, if not entirely, to current public policies that are outside the primary scope of my remarks today. Mandated health benefits and excessive health services regulation at the state and federal levels unquestionably boost health care costs — estimates might range from a conservative 10 percent to as much as 25 percent, on average — and they contribute significantly to the level of Americans who cannot afford to purchase private health insurance. Medical malpractice costs, including defensive medicine, cannot be eliminated but they can be made better or worse by particular government policies.



Although adoption and dissemination of advanced health care technologies is frequently blamed as the key culprit in the long‐​term secular trend toward rising health care costs, that sort of oversimplified analysis often neglects to ask whether (1) our lives are better off, on balance, even after paying those higher costs, and (2) how the pattern and pace of such technological advancement might change if we relied less on third‐​party payers to fund it and increased the share of health spending that is paid out‐​of‐​pocket by individual consumers.



Another related set of governmental and non‐​governmental factors have combined to diminish the former role of managed care insurers in holding down the costs, if not improving the value, of private sector health care. The managed care backlash, in conjunction with provider pushback on prices, the pendulum swing of the insurance underwriting cycle, and employers’ competition for scarce labor, may have been inevitable to some degree — but public policy efforts to regulate, if not outlaw, many managed care practices and to encourage court challenges to third‐​party restrictions on access to care certainly contributed to a loosening of referral and authorization rules, as well as more inclusive provider networks. As a result, today we have more choice and access to care in many health insurance plans, as long as we (or our employers) still can afford to pay the higher premiums.



 **Medical Savings Accounts & Cost‐​Sharing Clones**



In the current environment of rising health care costs, the fading away of managed care’s third‐​party controls on the supply of health care, and (apparently) continued political resistance to even higher levels of government involvement in the regulation and financing of health care, more employers and their employees are turning to less comprehensive insurance coverage, greater individual cost‐​sharing, and reduced “insured” benefits as the most promising and, until now, relatively less‐​explored means to control health care costs.



Consumer‐​driven health care ranges across a wide continuum of health financing vehicles — from modest increases in cost‐​sharing under more conventional employer‐​sponsored health plans (higher deductibles, multi‐​tiered copayments for types of covered prescription drug purchases, higher out‐​of‐​network coinsurance percentages) to various types of two‐​tiered, defined contribution health plans (combining a higher‐​deductible group insurance policy with an individual health savings account) to the “real thing” — medical savings accounts (MSAs).



In a health care world where “nobody else” seems to managing either the cost or the quality of health care very effectively, MSAs and their cost‐​sharing cousins are picking up the baton and leading employers and their workers toward a consumer‐​driven model of health care purchasing.



Note that a recent Watson Wyatt Worldwide survey found that only 43 percent of workers were satisfied with the overall performance of their health plan. Less than half (48 percent) trust their employer to design a plan that will provide the coverage they need. Approximately the same percentage (47 percent) think that better health plans are available for the same cost. And almost four out of ten employees want their employer to contribute a fixed dollar amount toward the premium for any health plan — even if it means finding their own health plan.



The evidence is overwhelming that increased cost sharing reduces health insurance premiums substantially. For example, Jason Lee and Laura Tollen recently noted in a June 2002 article in Health Affairs that increasing cost sharing from a plan with $ 15 copays and no deductible to one with 20 percent coinsurance and a $ 250 deductible reduces premiums by about 22 percent; and a combination of 30 percent coinsurance and a $ 1000 deductible would reduce premiums by 44 percent. Offering less comprehensive insurance plans with greater enrollee financial responsibility is designed to encourage enrollees to be smarter consumers of health care services, limit demand for less beneficial “discretionary” care, seek out higher‐​value options, and save money for more critical medical needs in the future.



The recent revival of greater cost‐​sharing and so‐​called consumer‐​driven health plan options may provide a partial transition vehicle for employers who are rethinking their health benefits strategies but remain ambivalent about relinquishing most of their role in structuring employees’ choices and monitoring health plan vendors.



The “full‐​strength” version of consumer empowerment, of course, remains an MSA. MSAs provide a health care savings account in combination with a high‐​deductible health insurance policy. The savings account is controlled by the insured person and used to pay routine health care expenses. The accompanying catastrophic insurance policy covers more substantial health care costs. Because the cost of such a policy is usually significantly less than the cost of a low‐​deductible policy, the money saved may be used to increase contributions by an individual (or his employer) to an MSA administered by a designated trustee or custodian.



Unspent MSA funds, including any interest or investment earnings, accumulate from year to year, providing money to cover possible medical expenses in the future.



MSAs help control costs, improve access to health care, expand consumers’ choice in and control of health care, and increase savings.



By putting individuals back in control of more purchasing decisions, MSAs create incentives for individuals to purchase health care more prudently and reduce their overall health care spending in a given year.



Whereas out‐​of‐​pocket payments by individual consumers accounted for about 50 percent of total health care spending in 1960, the share of third‐​party payments (by private health insurers, employers, and government agencies) for health care has grown to about 80 percent. Third‐​party payment of health bills insulates individual consumers from the real cost of their health care decisions and treatment. Consumers have less reason to avoid unnecessary care, question costs, or shop around for the best treatment available at a reasonable price, but they have every incentive to demand more services.



Excessive third‐​party coverage with low deductibles increases administrative costs, because every small bill must be submitted for review and checking for accuracy.



Instead of limiting the supply of desired medical services, MSAs lower the demand for those services by requiring individuals to pay directly and up front for their discretionary health care choices.



The RAND Health Insurance Experiment, conducted from 1974 to 1982, demonstrated that the more people had to pay for medical are without insurance reimbursement, the less they would spend on total medical care.



Because MSA plans are linked to high‐​deductible insurance that covers health claims that are more catastrophic in nature, they make the cost of insurance coverage more affordable for most Americans. Less -comprehensive coverage will mean lower premiums for a larger fraction of people with low incomes. The majority of standardized insurance policies currently available are generous and expensive — making them unaffordable to low‐​income people. On the other hand, catastrophic insurance for very large, less‐​predictable health care expenses forces consumers to bear the full marginal costs of health care up to the point where their use of health care exceeds the deductible.



Under many third‐​party health benefit arrangements, consumers have little incentive or ability to become more knowledgeable about health care. MSAs stimulate consumer demand for information about the quality and price of health care.



A number of studies have illustrated that MSAs improve health plan options not just for affluent and health individuals, but for all Americans.



In April 2000, RAND Corporation researchers examined the effects of making MSA options available to small businesses. RAND rejected the assumption that MSAs appeal most to the wealthiest and healthiest workers. It found that HMOs would remain more attractive to higher‐​income workers, primarily for tax reasons, and exceptionally good health risks would be more likely to decline any insurance at all than to select the MSA option.



A 1996 study by National Bureau of Economic Research analysts concluded that most workers would end up retaining a substantial portion of the contributions they made to MSAs by the time they retired. Approximately 80 percent of employees would have retained over 50 percent of their MSA contributions by the time of retirement, and only 5 percent of workers would have saved less than 20 percent of their contributions. Although workers with high health care expenses in one year tend to have lower but still higher than average expenses in the next few years, the concentration of annual expenditures declines continuously as more and more years of expenditures are cumulated. High expenditure levels typically do not last for many years.



Another 1996 study of Ohio‐​based firms that offered MSAs that did not qualify for tax advantages under the Health Insurance Portability and Accountability Act (HIPAA) determined that the employer’s total cost for family coverage under those MSA plans averaged 23 percent less than traditional family coverage, yet the average employee with family coverage also would be $ 1355 better off under the worst‐​case (maximum out‐​of‐​pocket liability) scenario.



So, if MSAs are so great, why don’t we see more of them in the marketplace? Primarily because the federal MSA program authorized under HIPAA in 1996 has been unnecessarily handicapped, if not permanently crippled, by unreasonable restrictions on what still remains a “demonstration project.” Congress still needs to permanently authorize federally qualified MSAs; lift the enrollment cap and allow an unlimited number of people to have MSAs; expand MSA eligibility to include employees in businesses of all sizes, as well as employees without employer‐​sponsored insurance; allow MSA plans to offer a much wider range of deductibles; allow MSA holders to fund fully their MSAs each year, up to 100 percent of the insurance policy deductible; allow employers and employees to combine their contributions to MSAs at any time within a given year; and either preempt first‐​dollar state‐​mandated benefits or provide the flexibility for MSA plans to adjust to comply with those conflicting mandates.



In the case of MSAs, most of the problems have been caused at the federal policy making level. Aside from making equivalent state income tax benefits available to MSA owners and avoiding or eliminating restrictions on high‐​deductible insurance policies subject to state regulation, state policy makers should continue to press Congress to make MSAs universally available. They also might consider offering MSA‐​like health plan options to state employees as part of their benefits package, in order to boost demand in this currently thin market.



 **Remove Barriers to Growth of Defined Contribution Health Plans**



Despite the potential benefits of two‐​tiered defined contribution (DC) health plans, as well as the recent tax guidance issued by the Internal Revenue Service clarifying how accumulated balances in an individual employee’s health reimbursement accounts may be treated when rolled over at the end of a year, several regulatory barriers to the future growth of DC plans still need to be removed.



First, “pure” DC plans for fully insured employer groups, in which an employer distributes defined health benefits contributions to each eligible employee and allows them to purchase their own individual or non‐​employer‐​group insurance coverage, run the risk of being regulated inconsistently. They might be treated both as employee welfare benefit “group” plans and as “individual” health plans under state law.



To clarify the regulatory treatment of any plan or fund under which medical care is offered to employees by an employer solely through provision of a monetary payment or contribution to a participant or beneficiary and that is used exclusively to purchase individual health insurance coverage, state policy makers and other interested parties should press CMS and members of Congress to clarify that such plans or funds should not be considered an “employee welfare benefit plan” for regulatory purposes under the Employee Retirement Income Security Act (ERISA). However, such plans or funds would retain their “group” tax exclusion benefits under the Internal Revenue Code. Such hybrid treatment (group for tax purposes, individual for regulatory purposes) would be premised on the conditions that (1) only the employer, rather than individual employees, may decide to provide health benefits through defined contribution payments, and (2) such defined contributions must be provided to all employees or all members of a class of employees based on work‐​related distinctions.



Second, the defined contributions employers make to individual employees in pure DC plans, to be used to purchase individual health insurance coverage, should be allowed to vary on the basis of health status in the event the employer uses an approved risk‐​adjustment mechanism. That is, employers would be allowed to make larger contributions to workers with poorer health status to offset the higher premiums they would face when they seek to purchase individual coverage. State insurance regulators would need to approve this exemption from HIPAA non‐​discrimination rules for “group” plans, or press CMS to update and revise its past regulatory interpretations of this provision.



 **Lower Costs Trump Insurance Subsidies**



To achieve better health outcomes, we need to provide individual health care consumers with stronger incentives to be cost‐​conscious in using scarce medical resources. Making the market‐​based cost of care more transparent to all parties involved in health spending decisions will encourage its more efficient consumption and delivery. Reducing the long‐​term rate of growth in the cost of health care remains more important than (and, beyond a certain point, operates at cross‐​purposes to) expanding the scope and scale of subsidized health insurance coverage. Health insurance subsidies increase medical costs and the demand for health insurance, creating net welfare losses estimated at 20 percent to 30 percent of total insurance spending. In the opposite direction, access to “free” care dampens the demand for private health insurance. In striking the necessary balance, the effects of comprehensive third‐​party insurance on raising costs and limiting access to health care substantially outweigh any disincentives to obtain insurance protection that may be caused by direct provision of charity care. When rising health care expenditures outpace wage increases, their strongest effect is to reduce health insurance coverage for low‐​income workers. Hence, at the margin, increasing incentives to purchase less‐​comprehensive health insurance and filling in urgent gaps in direct delivery of health care through safety net mechanisms may produce more affordable and accessible health care.



 **Market‐​Driven Deregulation via Competitive Federalism**



Empowering consumers with a greater diversity of affordable health benefits choices will require exposing exclusive state health care regulation based on geography to competition from market‐​friendly regulation across state lines.



Lower‐​income workers in small firms bear the brunt of excessive state health insurance regulation, because their employers generally are unable to self‐​insure and, thereby, gain ERISA protection from state benefit mandates, restrictions on rating and underwriting, and other regulatory burdens. In general, increased state regulation has raised the cost of health insurance and limited the range of benefits package design. A wide assortment of small‐​group regulatory measures imposed by many states during the 1990s failed to improve levels of insurance coverage and, in some cases, priced low‐​risk consumers out of the small‐​group market. Various state government regulatory attempts to force low‐​risk insureds to subsidize high‐​cost insureds through devices like modified community rating and guaranteed issue often were counterproductive, because they triggered premium spirals that drove younger, healthier, and lower‐​income workers out of the voluntary insurance market. In other words, state health insurance regulation has been part of the problem, not part of the solution.



Rather than try to solve state‐​based regulatory failure with a new round of heavy‐​handed federal rule making or preemption, the better route to restoring a market‐​friendly, consumer‐​empowering environment at the state level is to facilitate competitive federalism‐​revitalized state competition in health insurance regulation that reaches across geographic boundary lines. (The closest successful model for such competitive federalism involves corporate law and the business of corporate charters, in which Delaware has specialized and excelled by consistently producing benefits to its “customers”-investors.) Such regulatory competition would limit the excesses of geographically based monopoly regulation. Currently, insurance consumers (at least in the non‐​self‐​insured market) are subject to a single state government’s “brand” of insurance product regulation. Solely by virtue of where they live, they are stuck with the entire bundle of their home state’s rules. Short of physically moving to another state, they are unable to choose ex ante the type of health insurance regulatory regime they might prefer and need as part of the insurance package they purchase.



Competitive federalism could facilitate diversity and experimentation in health insurance regulatory approaches. It would discipline the tendency of insurance regulation to promote inefficient wealth transfers and promote individual choice over collective decisions driven by interest group politics. In short, it would improve the quality of health insurance regulation, thereby enhancing the availability and affordability of health insurance products.



Insurers facing market competition across state lines would have strong incentives to disclose and adhere to policies that encouraged consumers to deal with them. Employers and individuals purchasing insurance would migrate to state regulatory regimes that did not impose unwanted mandates but, instead, fit the needs of their consumers. State lawmakers would become more sensitive to the potential for insurer exit. At a minimum, interstate regulatory competition would provide an escape valve from arbitrary or discriminatory regulatory policies imposed at either state or federal levels.



Key design requirements for regulatory competition in health insurance would include:



Several mechanisms or paths could lead to vigorous interstate competition in health insurance regulation. A more indirect, but sustainable, approach would involve strategic use of choice of forum clauses, and perhaps choice of law clauses, in health insurance contracts. Insurers would condition sales of a particular policy on a consumer’s consent to the designated litigation forum. That forum would be matched to the state whose regulatory law was selected. This choice of forum would need to be adequately disclosed and executed at the beginning of the contractual period, not just at the time of litigation. Insurers could increase the likelihood that the agreement would be enforced and regulatory competition enhanced by linking the designated forum to their company’s domicile‐​rather than to the site of the sales transaction.



Federal law could provide some shortcuts‐​such as a statute mandating enforcement of choice of forum contracts under the commerce or full faith and credit clauses of the Constitution. Congress also could provide uniform disclosure requirements for choice‐​of‐​forum and the insurer’s domicile in insurance contracts.



A more direct federal statutory approach might set an “insurer domicile” rule, in place of a “site of transaction” rule, for determining applicable state law and regulatory authority‐​at least as a default rule for multi‐​state transactions where the respective parties do not otherwise designate operative law. For example, Rep. Ernest Fletcher (R-KY) recently introduced the “State Cooperative Health Care Access Plan Act of 2002” (H.R. 4170), which would authorize a health insurer offering an insurance policy in one primary state (the primary location for the insurer’s business) to offer the same policy type in another secondary state. The product, rate, and form filing laws of the primary state would apply to the same health insurance policy offered in the secondary state.



Another route to interstate competition in insurance regulation might be built on decisions by individual states to grant regulatory “due deference” to determinations by out‐​of‐​state insurance regulators that a particular insurance company is qualified to conduct such business. Once an insurer submitted evidence of good standing in its domestic jurisdiction and (if different) in the jurisdiction where it conducts the largest share of its health insurance business, it would qualify for licensure in the state granting such regulatory deference. Regulators in secondary states would be most likely to treat proof of licensure and good standing in the primary state as prima facie evidence of qualification for licensure in the secondary state, while still requiring additional routine documents and fees and compliance of the primary states’ insurance department with broadly accepted accreditation standards, such as those maintained by the NAIC. Initially, an individual state’s decision to grant regulatory due deference would be similar to a declaration of unilateral free trade in health insurance products. The state would be eliminating or reducing its own regulatory restrictions on out‐​of‐​state insurance to benefit its citizens and to provide a model for other states to emulate.



 **A Real Safety Net for the Medically Uninsurable**



Medically uninsurable individuals represent a small percentage of the uninsured population (roughly no more than 1 percent to 2 percent of the uninsured have ever been denied health coverage for medical reasons). But they present the strongest case for public assistance. To some degree or another, at least 29 states currently operate high‐​risk pools that make insurance coverage available to the uninsured and subsidize their premiums. States with well‐​structured and adequately financed high‐​risk pools are more successful in keeping their individual health insurance markets competitive and insurance rates affordable. Such pools allow the individual insurance market to operate efficiently, while carving out for special treatment high‐​cost individuals who are beyond the capacity of the individual market to handle on an unsubsidized basis.



However, not all state high‐​risk pools are adequately financed (ideally, the funding should come from general revenues rather than through taxes on insurers within the state), and many states do not provide such subsidized coverage at all. Using the rationale that the “medically uninsurable” (at least to the extent that the unsubsidized price to insure them privately far outstrips their ability to pay) should be considered “medically needy,” mandatory Medicaid coverage and matching federal assistance should be extended to this class of beneficiaries, provided that the federal funds are channeled through state‐​operated high‐​risk pool programs that meet certain minimum criteria (for example, premium ceilings, waiting periods, rejection by at least one insurer, catastrophic conditions allowing automatic pool acceptance without prior carrier rejection) already in practice, but not “new” ones. The scope and scale of this Medicaid‐​financed high‐​risk pool coverage for the medically uninsurable would be capped at an upper ceiling that equals the higher amount of all individuals in a state facing private insurance premiums that are at least 200 percent of standard rates (plus those who cannot obtain any coverage at all, for medical reasons) or 2 percent of all people covered in a state’s individual insurance market.



 **Citizens’ Appropriations for Charity Care**



To bolster financing for charitable safety net care and ensure that it is delivered with private‐​sector efficiency, a new 100 percent, dollar‐​for‐​dollar federal income tax credit (above the line) should be provided for certain charitable contributions to provide health care services to the low‐​income uninsured. These “citizen appropriations” would be modeled in part on the Arizona tax credit for education “scholarships.” The maximum individual credit amount allowed would be no greater than 10 percent of an individual’s federal income tax liability in a given tax year. Eligible donations would have to be made to approved organizations that provide health insurance coverage, health care services, or payment of medical bills to uninsured individuals who are not eligible for optional federal health tax credits or Medicaid assistance. Organizations eligible to receive the donations must either be a non‐​profit, in accordance with section 501(c)(3) of the Internal Revenue Code, or, in the case of health care providers and that who wish to receive direct donations, they must create a separate non‐​profit subsidiary to receive and distribute such funding. Eligible organizations could spend only as much of their donations as they could document were directed toward paying the health care expenses of qualified uninsured individuals. Taxpayers could designate the institution to which their donation would be directed, but they could not pinpoint the individual beneficiary.



States could play a role in jump‐​starting this process, by providing their own state income tax credits along similar lines for such “citizen appropriations,” even in the absence of federal policy action.



 **“Getting Over” Adverse Selection**



Despite hypothetical concerns about adverse selection and risk segmentation in a more competitive, market‐​based private health insurance system, there actually is little evidence that individuals and families can identify and anticipate most of their future medical expenses in ways their potential insurers cannot. A recent study by Cardon and Hendel in The Rand Journal of Economics finds little empirical evidence of information asymmetries, market failure, and adverse selection in health insurance markets. Differences in health expenditures between the insured and uninsured are mostly due to observable differences in demographics (age, gender) and price sensitivities (higher‐​income workers capture more tax subsidies for insurance coverage), rather than unobservable factors related to health status.



Long, Marquis, and Rodgers also find little support for the hypothesis that people anticipate changes in their insurance status and arrange their health care consumption accordingly. The authors also find no evidence that people choose to purchase or drop insurance coverage in anticipation of change in their overall health care needs and conclude that insurer selection is an unlikely explanation for this failure to find quantitatively important transitory demand. However, they observe that recent state reforms aimed at eliminating or limiting some insurer restrictions on coverage of pre‐​existing conditions ironically might increase the ability of patients to adjust their treatment patterns for chronic conditions in anticipation of insurance changes.



Private insurers don’t need to remain helpless and clueless regarding potential adverse selection problems. In competitive markets, they may use a number of tools: set periodic limits on plan switching, vary premiums according to the amount of insurance purchased, underwrite and rate based on risk categories, create more homogeneous risk pools, or rely on the law of large numbers to diversify risks in large pools. Consumer inertia and individual differences in aversion to risk further limit the applicability of adverse selection theory to the real world.



Many difficulties we observe in health care insurance markets are due to government intervention rather than adverse selection or other market failures. If insurers are not allowed to charge different premiums to different risks, price predicted risk appropriately, and match their policy configurations to market demands, they will be more likely to resort to higher uniform prices, less savory practices like excluding or discouraging coverage of high risks, and, ultimately, market exit. Cream skimming (selecting only the best risks) becomes the insurers’ mirror image of adverse selection by insurance customers. Political interventions don’t alleviate underlying differences in risk across customers or eliminate insurers’ knowledge of such differences. They only force insurance companies to cope in inefficient ways and create new problems.



It is preferable to allow private insurers to do what they do best‐​evaluate risk and price it accordingly‐​and then deal with remaining outlier problems (for example, the medically uninsurable) through explicit, transparent public subsidies rather than more camouflaged regulatory cross‐​subsidies. We should separate support for societal objectives of income redistribution and protection against prohibitively expensive, but predictable, health risks from the competitive operations of commercial insurance markets.



Health status information is most likely to be asymmetric when it is scarce and costly. While government mechanisms prefer to ignore, hide, or shift those information costs, markets create proper incentives to discover efficient ways to signal relevant private information and put it to use.



Deregulating insurance choices and providing greater tax parity for all insurance purchasers can fill the real gaps in private insurance coverage, by providing breathing room for further market innovations, such as new forms of voluntary risk pooling and long‐​term insurance contracts. The growing availability of online health information and insurance products further strengthens the case for empowered consumers.



 **Summary**



Market mechanisms can’t eliminate every unfortunate human experience in health care access, affordability, and quality. Private charity and a backup safety net of transparent, direct subsidies have necessary roles to play. Unlike centralized government “solutions,” markets don’t promise perfect outcomes, just better ones.



The current climate of annual double‐​digit percentage increases in health care costs, dissatisfaction with the mature version of managed care, and remaining political resistance to centralized command‐​and‐​control mechanisms points to greater acceptance of the last remaining, relatively unexplored health care reform option‐​putting choices back in the hands of individual consumers and competitive free markets.



In a more market‐​based health care system, you would, to a large degree, get what you pay for, unless someone else wanted to pay for it voluntarily on your behalf. Income redistribution issues should be debated separately and resolved in the larger political arena, while we finally allow health insurance markets to operate more efficiently for the purposes for which they are best suited.
"
"Lizards from the deserts of Australia to the tops of mountains in Costa Rica have given us insights into how animals take advantage of their environment to be less cold-blooded.  Lizards seek out sunny patches or the warm underside of rocks where they can soak up the heat to enhance digestion or run faster.  When it gets too hot, they can escape the heat by finding shade or retreating to burrows underground.  In particular, tropical species, including lizards, are thought to be especially vulnerable to climate warming because they  already live at temperatures that can be dangerous. Without the sweat glands or metabolic control that mammals take for granted, lizards can heat up very quickly if they find themselves caught out in the sun for too long.   Species living in the tropics are also thought to have behavioural adaptations that are finely tuned to stable and predictable weather regimes, such as daily activity rhythms. Such behaviours that may be ill-suited to the increasing variability that is predicted with climate change where flexibility may be an asset.   In a world of greater climatic extremes, lizards may over-expose themselves to dangerous temperatures, or may find themselves with only a few opportunities to feed or find mates if their activity patterns are constrained to a particular window in temperatures. This question of how animals might respond to a warmer and more variable world is the focus of a new study aimed at understanding how evolution might come into play as our climate changes. Are species’ current tolerances and behaviours fixed – or can we expect scope for rapid evolutionary change through adaptation? Will some animals be saved by evolution? Michael Logan from Dartmouth College in the US conducted a clever experiment with his colleagues to test whether tropical lizards have the potential to change their physiology over generations to better adapt to a warmer environment, but also one that is less predictable.   The authors moved a population of brown anole lizards (Anolis sagrei) from a forested site in the Bahamas to a nearby open peninsula where daily temperatures on the ground were more than 2°C higher than the lizards were used to. They found that in a warmer and more variable climate, those lizards that survived functioned better in the heat, were fast and were also active over a broad range of temperatures.  The authors conclude that a new environment rapidly selected the lizards that were best suited to survival. They expect subsequent generations of peninsular lizards will continue with hard-wired evolutionary changes in their physiology and behaviour – eventually a new form of the species may emerge, tailored to towards the hotter peninsula.   Even so, the authors do not distinguish whether the shifts in the characteristics of the peninsular lizards are genetically based, which would be a prerequisite for evolutionary change – the traits need to be heritable and passed on to subsequent generations. Indeed, there isn’t much evidence for genetic change in response to climate change. Yet animals as diverse as pink salmon and soil mites have shown rapid evolutionary changes can occur in just a few generations, as opposed to the typical view that evolution takes hundreds of years to manifest itself. Yet even though evolution may rescue some species as the world warms, we don’t factor it in to our predictions of which plants and animals will be the most vulnerable to climate change. There are some obvious reasons for this.  While “rapid” evolution is possible, it still takes a while to unfold – longer than the duration of typical research grants and PhD programmes. For some slow-reproducing species, such as those from cold polar environments or large mammals, it may take decades or more to observe. Only the most patient evolutionary biologists would devote their life to investigating how each generation of elephants has further adapted to climate change. The capacity for evolutionary change is also tricky to predict as so much depends on context. Some populations of a particular species will contain individuals with certain characteristics – the capacity to tolerate extreme heat, say – that will allow those individuals best suited to a new environment to survive and the population to carry on, as seems to be the case in the brown anole lizards experiment. Other populations, without these adaptable individuals, will simply die out. We presently face the most exceptional extinction rates of modern times.  Rapid environmental change is already outpacing the capacity for many species to adapt and survive, but certainly some will beat climate change. One of the big questions this research poses relates to conservation: which species can we best assist through establishing new populations and supplementing declining populations with measures such as artificial breeding programmes? But this may be too narrow a focus. These fast-adapting lizards show that evolutionary change itself could yet be put to good use in conservation. While efforts have focused on saving threatened creatures by moving populations to safer places or trying to preserve their habitats, this study shows that moving them to more extreme environments can pre-adapt populations to a warmer world. Crucially, it suggests another tool to help identify those species that will have a chance under climate change and opens the possibility that we could give some species an adaptation head start."
"What a splendid irony it would be if the enduring legacy of Donald Trump’s presidency was the Green New Deal – a radical, government-directed plan to transition the US to a socially just society with a zero-carbon economy. Of course, it isn’t Trump’s idea. The Green New Deal was first proposed a decade ago, but has only recently captured the public imagination. Environmental activists from the “Sunrise Movement” protested in the office of House speaker, Nancy Pelosi, on November 13 2018, demanding the deal. And they were joined by recently elected congresswoman, Alexandria Ocasio-Cortez, who has argued passionately on behalf of the plan ever since.  Still, it’s partly thanks to Trump and the shock of his election that radical ideas are getting a hearing and his opponents are being forced to think bold. That’s just what is needed if the world is to get serious about tackling climate change. Alongside an aim for net-zero greenhouse gas emissions and 100% renewable energy, the Green New Deal demands job creation in manufacturing, economic justice for the poor and minorities and even universal healthcare through a ten-year “national mobilisation”, which echoes President Franklin Roosevelt’s New Deal in the 1930s. The UK has, for the past decade, thought of itself as a climate leader. It’s true that the 2008 Climate Change Act, which sets a legally-binding framework for carbon reduction, is ambitious compared to legislation in many other countries.  But the UK’s approach – like so many other countries – is based on quiet consensus. So far, climate politics has been a polite conversation between government, industry and researchers, not a subject of heated debate in parliament. My research with UK politicians shows a reluctance to speak out on climate change, as many prefer a low-key approach – dressing up climate action in the language of economic policy and market mechanisms to avoid confrontation with colleagues, the electorate or the industries that risk losing out in the shift to a low-carbon economy.  Some members of parliament even told me that they deliberately avoid mentioning climate change in speeches to the House of Commons or in their constituency, fearing it could backfire. One worried that he would be branded a “zealot”, and marginalised by his colleagues if he argued too vociferously in favour of climate action. This approach is severely limiting. Moving to a zero-carbon society will require changing the way that people live in their homes, travel around, shop, eat and source their food. It’s impossible to do all this without people noticing and attempting to impose change from above, without social consent, may also cause a backlash. The French president, Emmanuel Macron, found this to his cost when he tried to implement fuel tax rises which disproportionately affected poorer consumers. The result was the Gilets Jaunes protests which erupted in France in late 2018. 


      Read more:
      Emmanuel Macron's carbon tax sparked gilets jaunes protests, but popular climate policy is possible


 Climate policies should involve and excite people by addressing their concerns and aspirations. Climate policy proposals have typically centred around technically optimal solutions – trying to establish the least disruptive or costly approach, without paying attention to the question of whether people might vote for them. Barack Obama’s well-intentioned climate policies as US president fitted this mould. His Clean Power Plan, which sought incremental carbon reductions from existing power stations, was a pragmatic response to a divided political scene. After decades of technocratic and consensus-building climate politics, the Green New Deal swaggers onto the scene – an avowedly political and idealistic take on climate action. The Green New Deal was put forward as a Resolution to the House of Representatives, by Ocasio-Cortez and supporters from both houses on February 5 2019. It’s only a non-binding statement of intent at this stage and would require complex legislation. Bold political plans often founder on the rocks of implementation, especially when politics are as fractious as in the current Congress. But the Green New Deal has already succeeded in one important aspect: it puts climate policies on the agenda that are as ambitious as the science of climate change demands. This makes it impossible for opponents to stay silent. The Green New Deal is forcing Democrats and Republicans to consider their own stance on climate change. Some Democrats have branded the plan as unrealistic – a “green dream”, as Pelosi called it. Veteran senator, Diane Feinstein, was similarly dismissive, when young campaigners asked for her support. Republicans, meanwhile, have branded it a socialist takeover to rally their own supporters. But the Green New Deal’s opponents can’t simply criticise. They will need to find their own answer to the climate question. For the Republicans, denying or dismissing the science of climate change is becoming less tenable by the day. The impacts of climate change are mounting, public concern is rising, and schoolchildren are striking.  The Green New Deal has drawn attention to a gaping hole in right-wing politics – the confident articulation of a climate strategy. If you agree with the scientific consensus that rapid action is necessary, but you don’t like the strongly social flavour of the Green New Deal, what do you propose in its place? In the UK, the fog of Brexit has clouded any serious political debate on climate change, but when politicians manage to take a breath, they too will face the same challenge. The Labour Party has promised action but the Conservatives have been told that their own commitments aren’t compatible with the Paris Agreement and so they, too, need a plan. The fight is not nearly won. But the Green New Deal is already succeeding in putting climate action where it belongs, as the defining political issue of our time. How strange that we have dysfunctional US politics to thank for this huge step forward."
"
Today while looking for something else I came across an interesting web page on the National Climatic Data Center Server that showed a study from 2002
A continuous multimillennial ring-width chronology in Yamal, northwestern Siberia (PDF) by Rashit M. Hantemirov and Stepan G. Shiyatov
That study was tremendously well done, with over 2000 cores, seemed pretty germane to the issues of paleodendroclimatology we’ve been discussing as of late. Jeff Id touched on it breifly at the Air Vent in Circling Yamal – delinquent treering records?
A WUWT readers know, the Briffa tree ring data that purports to show a “hockey stick” of warming in the late 20th century has now become highly suspect, and appears to have been the result of hand selected trees as opposed to using the larger data set available for the region.
OK,  first the obligatory Briffa (Hadley Climate Research Unit) tree ring data versus Steve McIntyre’s plot of the recently available Schweingruber data from the same region.
Red = Briffa's 12 hand picked trees   Black = the other dataset NOT used
The Hantemirov- Shiyatov (HS) tree ring data that I downloaded from the NCDC is available from their FTP server here. I simply downloaded it and plotted it from the present back to the year 0AD (even though it extends much further back to the year 2067 BC) so that it would have a similar x scale to the Briffa data plot above for easy comparison. I also plotted a polynomial curve fit to the data to illustrate trend slope, plus a 30 year running average since 30 years is our currently accepted period for climate analysis.
Compare it to the Briffa (CRU) data above. 
Click for larger image
When I first saw this plot, I thought I had done something wrong. It was, well, just too flat. But I double checked my data import, the plot, the tools used to plot, and the output by running it 2 more times from scratch. Then I had Jeff Id over at the air vent take a look at it. He concurs that I’ve plotted the data correctly.
The trend is flat as road kill for the past 2000 years, though it does show an ever so slight cooling.
So the next task was to look at more recent times. Here’s the last 200 years of the data:
Zoomed to last 200 years - click for larger image
Still flat as road kill.
Finally, since Tom P made a big deal out of the late 20th century with his analysis where he made the mistake of combining two data sets that had different end points, I thought I’d show the late 20th century also:
Zoomed to last 50 years - click for larger image
Still flat.
Note that in the graph done by Steve McIntyre showing both Briffa and Schweingruber data, both of those data sets are also quite flat until we get into the late 20th century. So out of the 3 data sets we’ve looked at, the Briffa data, the data kept hidden for almost 10 years,  is the only one that shows any propensity for sudden 20th century warming.
But don’t take my word for it that this record is so flat. Look at the authors results. Their results seem identical to what I’ve plotted. Here is the last 2000 years of data charted taken from their paper:

Figure 8 Reconstructed southern Yamal mean June–July temperature anomalies relative to mean of the full reconstructed series.
But for those that want more close up views, I’ve done some additional graphs. Since the authors used a 50 year window in one of their graphs I did the same. I also changed the Y scale to show a zoomed in +/- 0.3°C as the range rather than the +/- 4.0°C the authors used in the plot above. Some details begin to emerge, but once again the trend is essentially flat, and slightly negative.
Click for a larger image
And here are the last 200 years zoomed
Click for a larger image
The period around 1800 was warmer than the late 20th century according to the data viewed this way, but we can see that slight rise in temperature for the 20th century. However compared to the rest of the Yamal HS data record it appears insignificant.
The authors insist that this wood contains a valid climatological record.
Holocene deposits in the southern Yamal Peninsula contain a large amount of subfossil tree remains: tree trunks, roots and branches. This is the result of intensive accumulation and the good preservation of buried wood in the permafrost. The occurrence of this material in the present-day tundra zone of the Yamal Peninsula was described for the first time by Zhitkov (1913). Later, Tikhomirov (1941) showed that, on the evidence of remains of trees preserved in peat, during the warmest period of the Holocene, the northern tree-line reached the central region of the Yamal Peninsula (up to 70°N), whereas today the polar timberline passes through the southernmost part of the peninsula at a latitude of 67°309 N.

By 1964, attention had been drawn to the potential significance of Yamal subfossil wood for reconstructing climatic and other natural processes over many thousand years, as a result of  fieldwork carried out within the valley of the Khadytayakha River in the southern part of the Yamal Peninsula (Shiyatov and Surkov, 1990).
I was impressed with the amount of field work that went into this paper. The authors write:
We travelled by helicopter to the upper reaches of the river to be sampled. Small boats were then used for locating and collecting cross-sections from wood exposed along the riverbanks. It was also possible, when going with the stream, to explore the nearest lakes. 
The best-preserved material from an individual tree is usually found at the base of the trunk, near to the roots. However, many of these remains are radially cracked and it is necessary to tie cross-sections, cut from these trunks or roots, using aluminum wire before sawing. This wire is left in place afterwards as the sections are air-dried.
Here’s how they got many of the tree samples using a rubber boat:

And here is how they sum up the last 2000 years from a tree line analysis they did:
From the beginning of the first century bc to about the start of the sixth century ad, generally warm conditions prevailed. Then began a quasi 400-year oscillation of temperature, cooling occurring in about 550–700, 950–1100, 1350–1500 and 1700–1900. Warming occurred in the intermediate periods and during the twentieth century. The more northerly tree-line suggests that the most favourable conditions during the last two millennia apparently occurred at around ad 500 and during the period 1200–1300. It is interesting to note that the current position of the tree-line in Yamal is south of the position it has attained during most of the last three and a half millennia, and it may well be that it has not yet shifted fully in response to the warming of the last century.
Interestingly while the authors note some warming in the last century, they don’t draw a lot of attention to it, or refer to it as being “unprecedented” in any way. There’s no graphs of nor mention of “hockey stocks” either.
Here’s the link to the source data:
ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/reconstructions/asia/russia/yamal_2002.txt
Feel free to make some plots of your own.
===
UPDATE: While I had originally surmised this data supported Steve McIntyre’s recent findings with respect to Briffa, Steve notes in comments that the methodology is different between the two data sets:
Steve McIntyre: I’ve made MANY references to Hantemirov and Shiyatov 2002 in my posts on Yamal. In my first post on Yamal after getting access to the data, I discussed the Hantemirov and Shiyatov 2002 reconstruction as archived at NCDC see http://www.climateaudit.org/?p=7142
In that post, I observed that the standardization method used in H and S 2002 was different than Briffa 2000, that the H and S method would be unable to recover centennial scale variability and that it was not relevant to the issues at hand.
The H and S reconstruction does not “support” my point in respect to Yamal. It’s irrelevant to it.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9285497b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Bamboo, a common grass which can be harder to pull apart than steel, has the potential to revolutionise building construction throughout the world. But that’s not all. As a raw material found predominantly in the developing world, without a pre-existing industrial infrastructure built to skew things towards the rich world, bamboo has the potential to completely shift international economic relations.  The past century has seen an unprecedented transfer of products and predefined solutions – instead of capacity-building programs – from the rich countries to poor, under the rubric of “development aid”. The economic incentives for the former are obvious: when developed nations introduce, for example, their reinforced concrete technology to developing nations, those countries must also acquire the proper machinery, the technical expertise to maintain them, and the building materials suitable for those machines, and they must buy all of those things from the developed countries.  This divides our planet between those who produce goods and services, and those who are meant just to consume. Unless new materials, developed from the resources available in developing territories, enter the market, the system will remain the same. Bamboo could be the material which turns this relationship on its head.  For an example of the exploitative trade system currently in place, look no further than steel. Steel-reinforced concrete is the most common building material in the world, and developing countries use close to 90% of the world’s cement and 80% of its steel. However, very few of these nations have the ability or resources to produce their own steel or cement, forcing them into an exploitative import relationship with the developed world. Out of 54 African nations, for instance, only two are serious steel producers. The other 52 countries all compete in the global marketplace for this ever-more-expensive, seemingly irreplaceable material. But steel is not irreplaceable. Bamboo provides a material alternative, and it grows in the tropics, an area that coincides closely with the developing world. One of nature’s most versatile products, bamboo belongs to the botanical family of grasses and is extremely hard to tear apart.  Its strength comes from the way the grass evolved, adapting to natural forces. In contrast to wood, the bamboo culm or haulm – botanical terms for the stem of a grass – is thin and hollow. This allows it to move with the wind, unlike a tree, which tries to simply withstand any natural forces it is exposed to. This adaptation for flexible movement required nature to come up with a very light but tension-resistant fibre in the bamboo culm which is able to bend in extreme ways without breaking. Bamboo is harder to pull apart than timber or even reinforced steel. Bamboo is also a highly renewable and eco-friendly material. It grows much faster than wood, and is easy to obtain in great quantity. It is also known for its unrivalled capacity to capture carbon and could therefore play an important role in reducing carbon emissions worldwide – another advantage for developing nations in light of the trade in carbon emission certificates. Simply from an economic perspective, most developing nations should be interested in the material. It could strengthen local economies and lower dependency on international markets. The great social, economic, and material benefits of bamboo and its widespread availability are not reflected in the demand for the material, however. Despite its strengths, bamboo has a number of weaknesses as a construction material. Water absorption, swelling and shrinking behaviour, limited durability, and vulnerability to fungal attacks have limited most applications of bamboo so far.  Today, bamboo usage is generally limited to being a structural component in regions where it reflects local architectural traditions; early attempts to use it as an untreated, non-composite reinforcement material in concrete were not successful.  But bamboo fibre could be extracted and combined with other materials to create a composite, harnessing its natural strengths as part of a viable building material, an alternative to steel and timber. Indeed, this is exactly what researchers at ETH Singapore’s Future Cities Laboratory are working on.  There are approximately 1,400 known species of bamboo, which comes in all sorts of shapes, sizes and strengths. Using new technologies, we’re studying which bamboo species are most suited for usage in construction and how we can overcome some of its limitations by combining bamboo with adhesive matter.  Bamboo composite material can be produced in any of the familiar shapes and forms in which steel and timber are produced. Like them, the material can be used to build wall structures for houses or any other buildings. More interestingly, it can be used for specific applications that best take advantage of the material’s tensile strength, such as reinforcement systems in concrete or beams for ceilings and roof structures. Today, bamboo costs less than a quarter as much, by weight, as steel reinforcement. And because steel is 15 times denser than natural bamboo, the figures by volume are even more extreme. In South-East Asia alone, there is enough bamboo already in cultivation to meet equivalent demand for construction steel 25 times over. Bamboo largely grows in developing countries which, with this new technology, could potentially develop substantial value chains. Farmers, collection centres, distributors and, finally, production facilities could form a strong economic power – so long as the bamboo is not simply exported as a raw material.  Developing countries must develop and sustain knowledge and industrial know-how in order to strengthen their economic capacities. The production of a high-strength building material could establish strong new rural-urban linkages and create an alternative source of revenue for farmers. Expanding cultivation would help farmers in other ways, too; due to its fast growth, bamboo can secure open soil and protect it against erosion. Being a grass, bamboo also keeps the water table high and therefore improves the productivity of adjacent fields planted with food crops.  Bamboo could play an important role not only as a traditional construction resource but also as the major component of an industrialised product, enabling the creation of a “smoke-free” industry in developing nations."
"A dozen countries have called for an EU climate target for 2030 to be drawn up “as soon as possible”, if the bloc is to galvanise the rest of the world before vital UN talks in Glasgow later this year. In a letter to the EU’s top official on climate action, Frans Timmermans, the dozen EU member states say “the EU can lead by example and contribute to creating the international momentum needed for all parties to scale up their ambition” by adopting a 2030 EU greenhouse gas emissions reduction target “as soon as possible and by June 2020 at the latest”.  This year’s UN talks in Glasgow are crucial, as the world is far adrift of goals set at the landmark 2015 Paris conference, including the aspiration to limit global heating to 1.5C above pre-industrial levels. Even half a degree higher will significantly increase the risks of drought, floods, extreme heat and poverty for millions of people. The letter piles pressure on Timmermans, who is due to unveil the EU’s long-awaited climate law on Wednesday. A leaked draft of the law shows Timmermans’ plans to propose an EU-wide 2030 target by September. The target would probably be an emissions reduction of 50-55% compared with 1990 levels, which green activists say is not enough to guarantee meeting the EU’s goal of net-zero emissions by 2050. EU officials think a couple of months’ difference in proposing the target makes little difference, but would allow them to bring onboard more reluctant countries, including Poland, which has not yet signed up to the EU-wide goal of net zero emissions by mid-century. The climate and environment ministers argue timing is crucial, as they want the EU to have a 2030 target, before an EU-China summit in September and well ahead of Glasgow climate talks in November. “No other major economy is prepared to take the lead to ensure an ambitious implementation of the Paris agreement,” they write. Wendel Trio, the director of the Climate Action Network Europe, said: “By proposing a 2030 target increase only in September, the commission will give member states no time to reach an agreement by Cop26 in November, the international deadline by which all countries must commit to new, ambitious climate pledges for 2030. The EU needs to have its own house in order, and quickly to push other countries to make substantial contributions well before the deadline.” The letter, organised by Denmark, was also signed by France, Italy, Spain and the Netherlands. Only two signatories – Slovenia and Latvia – are central and eastern countries that joined the EU after 2004. Germany is conspicuous by its absence. The EU climate law is the centrepiece of the European Green Deal, which aims to transform Europe’s economy to confront the climate emergency. The law could set Brussels on a collision course with populist governments in Poland, Hungary and the Czech Republic, which have been the slowest moving on the climate emergency. Putting the 2050 net zero target into law means Poland could be outvoted if it continued to refuse to sign up. The European Green Deal is the EU’s answer to what the European commission’s new president, Ursula von der Leyen, called the “existential issue” of the climate emergency. Most EU countries have signed up to goal of a climate neutral EU by 2050, a goal demanding dramatic change in energy use, farming, housing, transport, trade and diplomacy. The European Green Deal is only the start of the journey: laws will have to be drafted and agreed by EU ministers and MEPs; money will have to be raised; plans will have to be implemented. It is a route map, rather than a destination, and time is running out. Under the law, Brussels would also be able to take a government lagging behind on its climate target to the European court of justice, which can issue daily fines for failure to uphold EU law. Poland, which generates 80% of its electricity from coal, is seeking EU funds to help wean its economy from fossil fuels. The European commission has proposed a €100m (£87m) “just transition” fund to help countries with coal mining jobs adjust to a green economy, but Warsaw has yet to come onboard. Climate activists have accused the commission of lacking ambition, as the climate law gives scant detail on how the EU will meet the 2050 net zero target, either by phasing out fossil fuel subsidies, reforming the EU’s common agricultural policy or regulating industry. “With no 2030 climate target and no measures to end subsidies for fossil fuels, industrial farming and other destructive industries, the commission has left a big hole in what’s meant to be the flagship of the European Green Deal,” said Sebastian Mang, a climate and energy policy adviser at Greenpeace. The Swedish climate activist Greta Thunberg is likely to give her verdict on the EU climate plans when she meets MEPs on Wednesday. She will also lead a “European strike” in Brussels on Friday with Belgian campaigners."
"

It’s getting a little hard for the press to keep up with the required twice‐​weekly global warming scare story, so, a few days ago, NBC reached into the recycling bin for an old chestnut: Glaciers are melting at our national treasure, Glacier National Park, and it’s a result of pernicious human influence on the atmosphere. On the program, reporter Jim Avila claimed that “the temperature’s gone up an average 3 1/2 degrees in the park during the last 110 years.”



A closer, more scientific look at the data reveals a different picture. The way we take reliable estimates of temperature is to measure it over a broad area–usually several counties–with an array of thermometers. This is the methodology used by the National Climatic Data Center, the country’s “official” climate repository. They have divided the nation into a couple of hundred “Climatological Divisions” (CDs). Their professionals and volunteers monitor calibrated instruments within each division, and then monthly and annual temperatures are calculated by averaging the readings.



At our latitude, glaciers melt in the summer. Beginning in September and ending in May, it snows frequently at Glacier Park, which is one of the reasons for its glaciers, to begin with. You can download the history of the Western Montana Climatological Division (“Montana CD 01”) for yourself, at http://​www​.wrcc​.dri​.edu. This site is the academically lustrous Desert Research Institute at the University of Nevada and “wrcc” is the Western Regional Climate Center, headed by Dr. Kelly Redmond, one of America’s most careful and savvy climatologists.



All of the CD records have a common starting point in 1895. Inspection of the entire summer history yields no statistically significant warming whatsoever in Montana 01. Ditto for the annual record, although that is not as important as the summer readings. In other words, Glacier’s glaciers are being melted by temperatures that show no summer warming distinguishable from natural year‐​to‐​year variability over the last 107 years.



With climate data, it’s easy to play the standard game of picking a starting point in the record to prove a point. Precisely, one can come up with 3 1/2 degrees of warming by looking at data beginning in 1950, rather than considering the entire history.



The reality is that Glacier Park’s glaciers are melting today because the summer temperature is the same, on average, as it was when the record begins, 107 years ago. They were melting then, too, which was before people burned much fossil fuel, altering the earth’s natural greenhouse effect. They started to melt as the region emerged from what is sometimes called the “Little Ice Age,” a cold period that ended in the middle of the 19th century.



This incident recalls similarly breathless and shoddy reporting two years ago about the melting of the hemisphere’s largest ice complex, the Greenland ice cap, and its outflow glaciers. The overall ice balance turns out to be nearly neutral, but the southern end is melting. We also have a large number of surprisingly good climate records at and near the regions of maximum melting. There hasn’t been any warming for a long time. Every graduate student who has ever passed a comprehensive exam in climatology knows that temperatures have been going down in this region for about 70 years. Like Glacier National Park, southern Greenland also warmed in the 19th century with the climb‐​out from the Little Ice Age.



For what it’s worth, some knowledgeable people think that this decline in regional temperature is caused by global warming (and others do not). But it is getting colder up there, even as the glaciers retreat. As scientists who study Greenland speculated two years ago, the melting rate must have been prodigious in the early part of the 20th century (or nearly 100 years ago) when Greenland was warmer than it is now.



Why didn’t NBC check the regional temperature records for western Montana? Instead of recycling an old story, they could have produced a much better one–real news–by showing that Glacier’s’ glaciers have been melting like this for well over a century and are doing so without any net regional summer warming in the last 100 years.



It’s a fact that many other glaciers around the world are melting because of local and regional changes that correlate with human combustion of fossil fuels, but not the disappearing ice fields of Glacier National Park.
"
"**Testing has uncovered 106 cases of Covid-19 at a Cornwall meat processing factory.**
Cornwall Council said the employees of Kepak in Bodmin - of which 80-85% were asymptomatic - were now self-isolating at home.
In total, 271 staff members were tested over the past week after a small number initially reported positive tests.
Rachel Wigglesworth, the council's director of public health, said the situation would be monitored closely.
She said: ""Outbreaks such as this are always tough for everyone involved, but we have worked with Kepak and our health service partners in taking quick and robust action to help stop the virus spreading.""
The council said testing was continuing at the site and anyone identified as a close contact linked to the positive cases would be contacted by NHS Test and Trace.
A spokesperson for The Kepak Group said it was providing support and advice for staff who needed to self-isolate and was working closely with public health teams.
They said : ""The Group is working tirelessly to protect its staff as well as ensuring the continuity of secure food supply during this pandemic."""
"**Officials at the Department for the Economy believed wind turbines ""should be eligible"" for coronavirus payments.**
Fifty-two wind turbine owners in Northern Ireland received emergency coronavirus funding from Stormont.
The sector was subsequently ruled ineligible, after more than Â£500,000 was paid.
The Department of Economy said once new information came to light, ""eligibility was reconsidered"" to protect the public purse.
New papers obtained by the BBC Nolan Show reveal that officials ""could not decide"" on a way to withhold payments from owners already receiving the lucrative green energy subsidy.
The documents suggest that weeks after the initial 52 wind turbine payments were made in April, a further 328 payments were still on the table.
They suggest that the cost of the payments would have been over 10 times greater if officials from Land and Property Services (LPS) had not intervened.
On 5 May, the Department for the Economy told Land and Property Services that wind turbines should be eligible for the Â£10,000 Covid grants.
Keith Forster, director of the department's strategic policy division, said: ""We discussed this issue at length, and we could not decide on an approach that would enable us to meaningfully withhold payment to any business that meets the criteria of the scheme.
""On this basis, wind turbines registered as receiving small business rates relief should be eligible for the scheme.""
Ian Snowden, LPS chief executive, later warned the department that LPS had found ""another 300 previously unrated properties, all of them wind turbines.""
He was concerned about a potential payment of Â£6.28 million for wind turbine owners. At this stage, LPS was already withholding 328 turbine payments.
In a meeting with Department for the Economy officials on 19 May, LPS raised further concerns.
Minutes of the meeting state that ""many wind turbines are registered as individual limited companies, however deeper analysis of the initial assessment estimated that the maximum payments that could potentially be received by a director of multiple wind turbine companies would be between Â£40k- Â£80k"".
On 12 June, the Department for the Economy instructed LPS not to issue the payments.
Finance committee member Jim Wells MLA said the Department for the Economy initially made the wrong call on this issue.
""The Audit Office report indicated that due to the very high levels of subsidy, the owners of wind turbines did not need or deserve any form of financial aid.
""There should have been no question that they warranted Small Business Rates Relief or the coronavirus grants"".
He said payments should stop immediately, creating a ""windfall"" of extra money that ""could be put to good use elsewhere'"".
The Department of Economy said the initial decision on wind turbines ""was considered on the baseline eligibility criteria"".
A spokesperson for the department said: ""Decisions were made in the context of available information and when new information came to light, decisions were made on that basis.
""Once full details came to light, in regards wind turbines, the policy position regarding eligibility was reconsidered in order to protect the public purse.""
BBC's Nolan show also revealed that almost 2% of businesses in receipt of the small business rates relief scheme are wind turbine owners.
This cost the taxpayer Â£310,000 in 2020/21. Wind turbines have been receiving the rates discount since 2010, with the annual cost rising from Â£178,035 in 2015.
Many wind turbines in NI are already receiving ""excessive' subsidies"", according to the Audit Office.
The Department of Finance say the small business rate relief scheme for 2020/21 was approved without change by the Executive and Finance Committee."
nan
"

In an effort to justify its massive global warming regulations, the Obama Administration had to estimate how much global warming would cost, and therefore how much money their plans would “save.” This is called the “social cost of carbon” (SCC). Calculating the SCC requires knowledge of how much it will warm as well as the net effects of that warming. Needless to say, the more it warms, the more it costs, justifying the greatest regulations.   
  
Obviously this is a gargantuan task requiring expertise a large number of agencies and cabinet departments. Consequently, the Administration cobbled a large “Interagency Working Group” (IWG) that ran three combination climate and economic models. A reliable cost estimate requires a confident understanding of both future climate and economic conditions. The Obama Administration decided it could calculate this _to the year 2300_ , a complete fantasy when it comes to the way the world produces and consumes energy. It’s an easy demonstration that we have a hard enough time getting the next 15 years right, let alone the next 300.   
  
Consider the case of domestic natural gas. In 2001, _everyone_ knew that we were running out. A person who opined that we actually would soon be able to exploit hundreds of years’ worth, simply by smashing rocks underlying vast areas of the country, would have been laughed out of polite company. But the previous Administration thought it could tell us the energy technology of 2300. As a thought experiment, could anyone in 1717 foresee cars (maybe), nuclear fission (nope), or the internet (never)?   
  
On the climate side alone, there’s obviously some range of expected warming, often expressed as the probabilities surrounding some “equilibrium climate sensitivity” (ECS), or the mean amount of warming ultimately predicted for a doubling of atmospheric carbon dioxide. In the UN’s last (2013) climate compendium, their 100+ computer runs calculated an average of 3.2°C (5.8°F). A rough rule of thumb would be that this is also an estimate of the total temperature change predicted from the late 20th century to the year 2100.



That forecast is simply not working out. Since 1979, when global temperature-sensing satellites became operational, both satellite and weather balloon data show that the lower atmosphere is warming at about half the rate that was predicted. And in the area that is supposed to show the most integrated warming, in the tropics from about 15,000 to 45,000 feet, there’s two to three times less warming being observed than would be “forecast” by the UN’s models if they are run backwards from today. At the top of the active weather zone there, the forecast warming is a stunning seven _times_ more than has been observed.   
  
Since around the time that the last UN report was being written, a spate of scientific papers has been published showing that the ECS is quite a bit lower than the UN’s number, by 40-60 percent, depending upon the study.   
  
It seems like there’s quite a conspiracy of nature when it comes to observed versus predicted warming, with various measures all telling us that we’re seeing about half as much warming as we are supposed to in the bulk atmosphere. Further, the Obama Administration assumed a distribution of possible warming that was way to hot at the extreme end, 7. 1°C or 12.9°F, a number that _Science_ magazine recently said was “implausibly high” in a different model.   
  
On the economic side, how much something will cost by the year 2300 requires some estimate of economic growth between now and then. It’s called the discount rate, and there are actually guidelines for how to do this put out in 2003 by the Office of Management and Budget. The higher the discount rate, the less that warming costs that far out into the future. OMB says that “you should provide estimates of net benefits using both 3 percent and 7 percent.”   
  
( _Understanding discount rates_ : Imagine someone is going to give you $100 today (which you can invest), or a year from now, when that original $100 hasn’t been able to “work” for a year. If you’re OK either with $100 today, or $105 a year from now, your discount rate is 5 percent; you’re really expecting that much macroeconomic growth in a year. Over the long haul, average inflation-adjusted returns on equity investments are around the OMB’s 7 percent.)   
  
The latter figure drove the cost of warming down too far for the Obama Administration’s liking, and the cost actually went below zero assuming 7 percent and an ECS not far from what may be the most realistic value. That means it could be a net benefit, something the Netherlands’s Richard Tol has been saying for decades, as long as it doesn’t warm too much. The Administration wouldn’t go near that, so, in contravention of the OMB guidance, they simply did not use the 7 percent rate, as Kevin Dayaratna of the Heritage Foundation notes.   
  
For more information on the social cost of carbon, take a look at my testimony from earlier this week before the House subcommittees on the environment and on oversight. A lot came to light in the hearing, which will go a long way towards an EPA justification to cease and desist on its onerous Clean Power Plan and other Obama Administration climate regulations. 


"
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
From EurekAlert
International Greenland ice coring effort sets new drilling record in 2009
Ancient ice cores expected to help scientists assess risks of abrupt climate change in future
















 IMAGE: Atmospheric gases trapped in ancient ice recovered during the international North Greenland Eemian Ice Drilling, or NEEM, project are expected to help scientists better assess the risks of abrupt climate…
Click here for more information.















A new international research effort on the Greenland ice sheet with the University of Colorado at Boulder as the lead U.S. institution set a record for single-season deep ice-core drilling this summer, recovering more than a mile of ice core that is expected to help scientists better assess the risks of abrupt climate change in the future.
The project, known as the North Greenland Eemian Ice Drilling, or NEEM, is being undertaken by 14 nations and is led by the University of Copenhagen. The goal is to retrieve ice from the last interglacial episode known as the Eemian Period that ended about 120,000 years ago. The period was warmer than today, with less ice in Greenland and 15-foot higher sea levels than present — conditions similar to those Earth faces as it warms in the coming century and beyond, said CU-Boulder Professor Jim White, who is leading the U.S. research contingent.
While three previous Greenland ice cores drilled in the past 20 years covered the last ice age and the period of warming to the present, the deeper ice layers representing the warm Eemian and the period of transition to the ice age were compressed and folded, making them difficult to interpret, said White. Radar measurements taken through the ice sheet from above the NEEM site indicate the Eemian ice layers below are thicker, more intact and likely contain more accurate, specific information, he said.
“Every time we drill a new ice core, we learn a lot more about how Earth’s climate functions,” said White, “The Eemian period is the best analog we have for future warming on Earth.”
Annual ice layers formed over millennia in Greenland by compressed snow reveal information on past temperatures and precipitation levels and the contents of ancient atmospheres, said White, who directs CU-Boulder’s Institute of Arctic and Alpine Research. Ice cores exhumed during previous drilling efforts revealed abrupt temperature spikes of more than 20 degrees Fahrenheit in just 50 years in the Northern Hemisphere.
The NEEM team reached a depth of 5,767 feet in early August, where ice layers date to 38,500 years ago during a cold glacial period preceding the present interglacial, or warm period. The team hopes to hit bedrock at 8,350 feet at the end of next summer, reaching ice deposited during the warm Eemian period that lasted from roughly 130,000 to 120,000 years ago before the planet began to cool and ice up once again.
The NEEM project began in 2008 with the construction of a state-of-the-art facility, including a large dome, the drilling rig for extracting 3-inch-diameter ice cores, drilling trenches, laboratories and living quarters. The official drilling started in June of this year. The United States is leading the laboratory analysis of atmospheric gases trapped in bubbles within the NEEM ice cores, including greenhouse gases like carbon dioxide and methane, said White.
The NEEM project is led by the University of Copenhagen’s Centre of Ice and Climate directed by Professor Dorthe Dahl-Jensen. The United States and Denmark are the two leading partners in the project. The U.S. effort is funded by the National Science Foundation’s Office of Polar Programs.
“Evidence from ancient ice cores tell us that when greenhouse gases increase in the atmosphere, the climate warms,” said White. “And when the climate warms, ice sheets melt and sea levels rise. If we see comparable rises in sea level in the future like we have seen in the ice-core record, we can pretty much say good-bye to American coastal cities like Miami, Houston, Norfolk, New Orleans and Oakland.”
Increased warming on Earth also has a host of other potentially deleterious effects, including changes in ecosystems, wildlife extinctions, the growing spread of disease, potentially catastrophic heat waves and increases in severe weather events, according to scientists.
While ice cores pinpoint abrupt climate change events as Earth has passed in and out of glacial periods, the warming trend during the present interglacial period is caused primarily by human activities like fossil fuel burning, White said. “What makes this warming trend fundamentally different from past warming events is that this one is driven by human activity and involves human responsibility, morals and ethics.”
###
Other nations involved in the project include the United States, Belgium, Canada, China, France, Germany, Iceland, Japan, Korea, the Netherlands, Sweden, Switzerland and the United Kingdom.
Other CU-Boulder participants in the NEEM effort include INSTAAR postdoctoral researcher Vasilii Petrenko and Environmental Studies Program doctoral student Tyler Jones. Other U.S. institutions collaborating in the international NEEM effort include Oregon State University, Penn State, the University of California, San Diego and Dartmouth College.
For more information on the NEEM project, including images and video, visit http://www.neem.ku.dk.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e939bc9b1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The World Bank has been criticised for providing $55m (£43m) to aid fossil fuel extraction in Guyana, at the same time that it has pledged to stop direct funding of oil and gas production. The Washington-based institution, which provides loans and grants to aid the development of poorer countries, will provide $20m to pay for the training of Guyanese oil and gas officials, including those involved in the marketing of oil. It will also provide $35m to revamp the banking and insurance sectors in the country, in anticipation of the influx of billions of dollars of oil money from new oilfields. The World Bank gained plaudits from environmental groups in 2017 when it pledged to “no longer finance upstream oil and gas” after 2019. However, the pledge allowed it to finance the strengthening of governance and regulation in poorer countries, in the hope of avoiding the so-called resource curse in which oil wealth brings corruption and misuse of money. Campaigners said the World Bank’s focus on the oil and gas sectors clashed with its commitments “to help countries accelerate the transition to sustainable energy” and to support the 2015 Paris agreement goal of keeping global temperature increases to below 2C. “The World Bank’s public assistance to upstream oil development in Guyana is a blatant contradiction to Guyana’s climate change priorities and the bank’s commitment to the Paris climate agreement,” said Heike Mainhardt, a senior advisor at Urgewald, a German non-governmental organisation that has tracked the projects. “I am perplexed by the World Bank’s disregard for its own warning.” Mainhardt said the World Bank’s provision of assistance to the Guyanese government and general budget support allowed it to get around its pledge not to finance upstream oil and gas projects. The country’s rulers were then free to use the money to finance oil development directly, she said. Guyana is expected to become one of the world’s largest oil producers after US firm ExxonMobil, along with the consortium partners Hess and China’s state-owned CNOOC, found sites that could deliver 8bn barrels of oil, including in the offshore Stabroek block. The discovery could prompt massive change for a country of only 800,000 people and less than $5,000 in GDP per person in 2018. The consultancy Rystad Energy this month forecast that Guyana’s oil production could reach 1.2m barrels per day by the end of the decade, lifting total annual oil revenues well above $20bn at current prices. It said government income – projected to be about $270m in 2020 – could reach nearly $10bn annually within a decade, far outstripping Guyana’s 2018 GDP of $3.9bn. Melinda Janki, an international lawyer challenging oil development in Guyana, asked why the World Bank was not funding cheap renewable energy for the country. She said the institution was “pushing Guyana down a financially disastrous development path”. Oil production has become a significant issue in the run-up to Guyana’s election in March, after criticism of the incumbent government run by President David Granger. This month Global Witness, a corruption monitor, said a 40-year deal agreed between the government and ExxonMobil for drilling rights would deprive the country of $55bn. The Guyanese opposition has maintained that it would not renegotiate the ExxonMobil contracts. A World Bank spokeswoman said: “The World Bank has not provided any financing to develop the Stabroek Block oilfield.” She added that the resources governance project “is aligned with the World Bank’s 2017 One Planet summit announcement that the [World Bank Group] will no longer finance upstream oil and gas, but will continue to help client countries strengthen transparency, governance, institutional capacity, and the energy regulatory environment, including oil and gas.” • This article was amended on 28 February 2020 because the Guyanese opposition has not promised to renegotiate the ExxonMobil contracts, as an earlier version said. This has been corrected."
"

It’s a fact that TV meteorologists tend to be a bit more skeptical about the upcoming end of the world than some federal climatologists. One needs only to look at the bare ground in our Nation’s Capital to understand why.



Washington just witnessed the biggest busting of a major snow forecast in a long, long time; in fact, back to November 11, 1987, when double digits fell out of a sky that was supposed to drop maybe an inch. In the recent case, what would have been a record‐​or near‐​record snow was forecast by virtually everyone and for very good reasons.



It’s important to define a busted forecast. That’s not one where the forecaster changes his or her mind based upon new information prior to an event. To qualify as a disastrous forecast, a major event has to be predicted and held on to even as it fails to materialize.



Big busts are rare because our computer models are so darned good, lead by the flashy Euro, run twice a day out of the United Kingdom, which has developed a reputation for nailing disruptive Atlantic Coast cyclones days ahead of time. One of those was named Sandy, as it transitioned from a large but not particularly intense hurricane into a large and particularly vicious coastal storm. The U.S. models, especially the newer SREF (Short Range Ensemble Forecast) aren’t too shabby either, especially compared to what was online during the 1987 bust.





Our “best science” can be wrong.



The recent disaster also occurred on the watch of a collection of some of the world’s best forecasters, who, for a variety of reasons, are clustered in Washington.



Led by our modern computer output, all of their forecasts failed. The list includes Bob Ryan, a past‐​president of the 14,000-strong American Meteorological Society, who works for the local ABC outlet. Also included would be Jason Samenow who runs the very popular www​.cap​i​tal​weath​er​.com at the _Washington Post_ , and enjoys cult status in DC. When he walked into my classroom at UVA, I felt like Roy Hobbs’ (“The Natural”) high school coach must have.



The new head of the National Weather Service is the legendary Louis Uccellini. If young Samenow is The Natural, Louis is the Godfather, and his specialty is—wait for it—the Mid‐​Atlantic snowstorm. In fact, he and Paul Kocin literally wrote the book on them.



I’m pretty sure if Uccellini thought his troops were making a mistake on what was going to be a very public storm, he’d probably call someone and have a chat.



They had every reason to hold onto the snow forecast. Precipitation was moving in from the Southwest. As forecast, what began as rain in central Virginia transitioned over to snow in a couple of hours; even a bit earlier than it was supposed to. Charlottesville, about 80 miles southwest of DC, eventually racked up over 14 inches. The central Shenandoah Valley, thirty miles west of there, hit 20.



At 11:30pm on March 5, it began to rain in Washington, D.C. itself. Within a mere 20 minutes, it switched over to snow as the temperature began to drop through the upper 30s. The ground began to whiten. As the precipitation intensified and the low pressure system became more intense, basic physics said more cooling was on the way. Let the Big One begin!



Congratulatory emails went flying, and beer cans popped forth. Most forecasters went to bed expecting to wake to a winter wonderland.



My thermometer showed 35° and dropping. I set it to alarm at 37°, on the crazy, impossible, off‐​chance that something would go terribly awry. It went off two hours later, signaling that the bust had begun.



What happened is hardly an indictment, but rather a statement of the human condition. Our “best science” can be wrong.



So, in summary, not only were the best models and the best forecasters largely in agreement (there was one exception: the sophisticated Euro was somehow missing the heaps of snow that were already piling up down in Virginia), a _ll_ evidence indicated that the snowstorm was unfolding as planned. See any parallels with global warming? Sophisticated climate models and highly trained scientists, and a smart warming that began around 1975.



Oh, the lack of any warming for seventeen years now? My climate alarm just went off.
"
"Bovine tuberculosis (TB) is expected to cost British taxpayers nearly £100m in 2014. Scientific evidence is a vital weapon in the fight to protect cattle from TB. Why, then, has the government just fought and won a legal battle to avoid consulting independent scientists on its most high-profile TB control effort? Wild badgers play a role in transmitting TB to cattle, and culling badgers seems an obvious solution. A new round of badger culls is about to start, but it is risky . A complex interaction between badger behaviour and TB transmission means that the results of culling could, depending on various factors, increase TB levels, instead of reducing them. To add to that, badger culling is expensive.  This is why, in 2013, the government started a pilot that it hoped would be give them a cheap and effective way to control cattle TB. Farmers, rather than government, would pay for the culling. And, rather than being cage-trapped, badgers would be shot in the wild.  This pilot was started in just two areas – and for good reason: the whole approach was untested, and the stakes were high. Marksmen shooting at night might endanger public safety. Shooting free-ranging badgers might cause suffering. And, worst of all for the aims of the approach, failing to kill enough badgers, fast enough, would worsen the cattle TB situation that the culls were intended to control. In the face of such uncertainty, the government adopted a commonly used approach. It appointed an Independent Expert Panel to assess the safety, humaneness and effectiveness of the pilot project. The expectation was that this panel’s conclusions would reflect scientific evidence, whether or not they supported government policy. The Independent Expert Panel found that farmer-led culling was far from effective. Tasked with killing at least 70% of the local badgers within a six-week period, cull teams only managed to kill between 28% and 48%. Culling periods were extended, but still the total kill rose to only something between 31% and 56%, according to government figures. Unless more badgers could be killed, and faster, farmer-led culling risked worsening the problem it was intended to solve. The 2013 culls also failed to meet their targets for animal welfare. Between 7.4% and 22.8% of badgers were still alive five minutes after being shot and were assumed to have experienced “marked pain”. Despite facing these failures, the government decided to repeat culls in the same areas in 2014. If effectiveness and humaneness could be improved sufficiently, culling might be extended to more areas in 2015. If not, the government might need to reconsider their policy. One would think, then, that measuring effectiveness and humaneness would be a central goal of 2014’s culls. The Independent Expert Panel, together with government scientists, selected the most accurate and precise ways to estimate the effectiveness and humaneness of the 2013 culls. Measuring effectiveness is challenging because – being nocturnal and shy – badgers are hard to count. The panel overcame this problem by using genetic “fingerprints” to identify badgers from hair snagged on barbed wire. They measured humaneness primarily through independent observers recording the time that shot badgers took to die. The panel recommended that the same approaches be used for subsequent culls. But the government rejected this recommendation. This year there will be no attempt to count badgers in the cull areas, either before or after the culls. The time badgers take to die will not be recorded. There will be no oversight by independent scientists. Instead, the effectiveness of the culls which start tonight will be judged using a method so utterly inadequate it was barely considered in 2013. Key data will be collected by marksmen themselves: people with a vested interest in the cull being designated “effective” and “humane”, who in 2013 collected data so unreliable it was considered unusable by the panel. Available information suggests that any future claim that the 2014 culls have reduced badger numbers sufficiently to control TB will be completely baseless. Why the change in approach? Government cites cost, and hired some expensive lawyers to defend its position when the Badger Trust sought, and eventually lost, a judicial review of the decision to scrap independent scientific oversight of this year’s culls. Yet the cost of pushing forward with an ineffective culling policy would far outweigh the cost of properly assessing effectiveness and humaneness. Government has repeatedly referred to its programme of badger culling as “science-led”. One would expect a science-led policy to entail gathering reliable information on management outcomes, and using this and other evidence to inform future decisions. Choosing – against formal expert advice – to collect inconsistent, inadequate and potentially biased data is an insult to evidence-based policymaking. When ineffective culling can make a bad situation worse, failing to collect the evidence needed to evaluate future policy fails farmers, taxpayers and wildlife. Next, read this: Cattle herd model reveals best ways to halt spread of TB – and a badger cull isn’t one of them"
"By some strange quirk of fate, it is exactly 12 years to the day since I, alongside fellow climate activists, climbed on to the roof of the House of Commons to protest against plans for a third runway at Heathrow. Today’s high court judgment is a vindication of everything climate activists have been saying for more than a decade: Britain cannot honour its national commitment to tackle climate change at the same time as building a new runway at one of the busiest airports in the world. To be precise, the court did not quite say this. It ruled that ministers’ failure to take the UK’s climate change commitments into account rendered the Airports National Policy Statement (ANPS) – which effectively gave the green light to a third runway – unlawful. In order to be lawful, the ANPS would have to be rewritten to include a credible plan for squaring expansion with our commitment under the Paris Agreement to seek to limit global temperature rise to no more than 1.5C. The court was careful to clarify that it has no opinion on whether or not this is possible.  As someone who has been fighting these plans for 15 years I can confirm that it isn’t. If the third runway is built, there are only three possible outcomes: either we will fail on climate change; or we will have to constrain capacity elsewhere, roughly equivalent to closing Manchester airport; or we simply won’t be able to use the new runway at Heathrow, making it one of the most expensive white elephants in history. It is difficult to overstate the significance of this decision. Heathrow airport is a bastion of the global fossil fuel economy, so the symbolism alone of this defeat will resonate loudly around the world, giving courage to the movement fighting for a livable future – while striking fear in the hearts of the corporate fossil interests still determined to profit while the planet burns. It also sends a clear and timely message in advance of the UK hosting the most important UN climate summit since Paris, Cop26: Britain is prepared to lead the world in tackling the climate crisis. Scrapping Heathrow expansion is a surprise gift to climate diplomacy. But the mechanics of this decision could be even more important for the climate struggle. A British court has ruled, quite sensibly, that domestic policy decisions must be assessed against their impact on the UK’s ability to fulfil commitments under the Paris Agreement. The British judicial system remains incredibly influential globally, with courts around the world modelled on our own, so this means any high-carbon infrastructure project – from motorways to fracking wells to coal-fired power plants – could potentially now be blocked as unlawful in any of the 195 countries that are signatories to the Paris Agreement. So what happens next? Well, the airport will appeal, but it will lose – because the argument is unwinnable. Expanding Heathrow will have no positive impact on the UK’s economy. Pressure to expand Heathrow has nothing to do with increasing the number of international business flights, which are in sustained decline across all of London’s airports. In reality, the industry’s push for expansion is overwhelmingly about handling ever more international transfer passengers, alongside more and more outbound leisure flights by wealthy frequent flyers from London and the south-east. These are all journeys that cost the UK money rather than bringing it in. My own suspicion is that it may not even get as far as an appeal, as Heathrow’s investors will now get cold feet and find excuses to withdraw from the project. The third runway is dead, and cannot be resuscitated. More widely, today’s judgment marks a turning point in the climate struggle. It looks like the beginning of the British state taking the implications of the climate emergency as seriously as its citizens have now begun to. For UK air travel, this means there is turbulence ahead. We can no longer muddle on with the pretence that ever-increasing demand for flights can be met while also reducing emissions down to zero. The uniquely generous tax breaks that have kept air travel artificially cheap must end, but if the climate movement wants to maintain the support of the wider public, we must do this in the fairest way possible. That means bringing in a frequent flyer levy, which would protect access to some air travel for all, regardless of income, while still keeping aviation emissions within safe limits for the climate. Whether Boris Johnson’s government has the stomach for this kind of medicine remains to be seen. • Leo Murray is a co-founder and director at climate charity Possible"
"
Share this...FacebookTwitterAnd so how many more studies do we need to tell us the obvious?  There are so many studies out there that conclude renewable energy subsidies are a failure, yet you can be sure they will all be ignored by the next IPCC report, which instead will focus on some oddball quack paper by Ottmar Edenhofer.The University of  Witten/Herdecke has put out a press release here. Hat tip: oekowatch.de.
According to the press release, Prof. Dr. André Schmidt has drawn a harsh conclusion on the German EEG feed-in laws for renewable energies.
In his study of the economic and ecological impacts of the EEG Feed-in Act for favouring renewable energies for the Federal Office of Research, Prof. Dr. André Schmidt, economist at the University of Witten/Herdecke, has reached a devastating conclusion: they are counter-productive! “In Europe the feed-in act does not save a single microgram of CO2, subsidises carbon power plants in foreign countries, solar module  manufacturers in China, and so the German solar industry as a result gains no benefits on the market.“
Harsh words, and he has arguments behind them: “Through the EEG Act, power from solar cells has a price that is eight times higher (€ 0.34 /kwh) than conventionally produced power,“ he calculated. And he asked what do we get in return?
Carbon dioxide: When climate gases decrease because of the EEG, then the supply of of salable emission rights also goes down  (if  a functioning trading system indeed exists). ” The biggest polluters at home and abroad can cheaply purchase a free pass instead of thinking about filters.“
Employment: For the 48,000 German jobs (Source: Federal Association of Solar Economics for 2009) subsidies to the tune of €8.4 billion were forked out in 2008. “That comes out to €175,000 per job! When one compares this to coal mining subsidies, which are a relatively modest €75,000 per job, coal looks really good!“, Schmidt grumbles.
Competitiveness: 48% of all solar systems installed in Germany originate from China because German capacity simply cannot meet the demand. The global market share of German companies is at about 15%, and trending down: “When India and Thailand come onto the market soon, we’ll be at 8-10%. Here in Germany companies are investing too little in R&D, productivity advancement is sub-par, sales have stagnated. In 2010 there was a €4.3 billion trade deficit in solar modules.“. In Schmidt’s view, the inflated and guaranteed feed-in rates have paralyzed innovation in this industrial sector.“
In summary, the German EEG feed-in act is a flop.
 
 
Share this...FacebookTwitter "
"**Millions of Americans are already travelling home to celebrate Thanksgiving, despite warnings from health officials amid a significant wave of coronavirus cases and deaths.**
Thanksgiving, traditionally a large family get-together that rivals Christmas in size, is on Thursday.
Three million people are reported to have already travelled through US airports from Friday to Sunday.
But the number is around half the usual figure for Thanksgiving travel.
Dr Anthony Fauci, the country's top infectious diseases expert, told CBS News that people in airports ""are going to get us into even more trouble than we're in right now"".
The number of people flying in the US is the highest since mid-March, when the virus started to spread rapidly in the country.
But millions of Americans are also making huge personal sacrifices to stay at home this year. Some of our readers have been sharing their stories with us.
_We are used to family gatherings. Not being able to spend holidays with family - especially not visiting parents - is hard, but it's the right and responsible thing to do. We want them around._ **Dr Abdul Razzak, Ohio**
_Our families have been understanding about our desire not to travel this year. My husband and I will have just one friend over, who has been part of our 'bubble' all year._ **Meredith Power, Maryland**
_I am staying home and avoiding any unnecessary travel or contact with other people. Normally we travel to spend the Thanksgiving holiday with our family. I'm sad I will not be able to see my grandma. I miss being in their presence._ **Ryan Sedgeley, Wyoming**
Inevitably, however, much of the focus is on those who are travelling, and the fears that many are ignoring the health guidance.
Cleavon Gilman, an emergency doctor in Arizona, tweeted: ""Our pleas for help have fallen on selfish deaf ears.""
On Monday, the US - the worst-hit country in the world - recorded a further 150,000 cases of coronavirus, according to the Covid Tracking Project.
The number of people admitted to hospital with the virus has increased by nearly 50% in the past two weeks, while more than 257,000 have now died of Covid-19 nationwide.
Elsewhere in the US:"
"From unprecedented bushfires in forests that used to be too wet to burn to warming seas that have killed giant underwater forests, Australia is experiencing the effects of the global climate crisis more rapidly than much of the world. Over the past three weeks, Guardian Australia has told these stories in a major six-part series that was paid for by readers.  The Frontline: inside Australia’s climate emergency has also looked at what happens when towns run out of water, at the health effects of cities and towns being engulfed in smoke for weeks on end, and at extreme heatwaves that are killing people prematurely. On Monday, we publish the final episode in the series, The lost harvest. On Tuesday 3 March, readers will have the chance to ask experts in these fields questions about the series, what the science tells us and the impacts already being felt, in a Frontline live blog running from 10am-3pm. The scientists and professionals taking part are (all times AEDT): 10am-11am: Prof Lesley Hughes, ecologist, distinguished professor of biology and pro-vice-chancellor (research) at Macquarie University, and Climate Councillor. Hughes has expertise on the impact of climate change on species and ecosystems. 11am-12pm: Greg Mullins, former commissioner of Fire and Rescue NSW, volunteer firefighter and Climate Councillor. 12pm-1.30pm: Prof Michael Mann, climatologist, geophysicist and director of the Earth System Science Center at Pennsylvania State University. Mann is currently based in Sydney. 2pm-3pm: Assoc Prof Donna Green, from the University of New South Wales’ climate change research centre. Green has expertise in the health effects of climate change and air pollution. We will be taking questions in advance and during the blog. Please send them to frontline.live@theguardian.com or leave a comment here. The Twitter hashtag for questions and the discussion on Tuesday will be #frontlinelive See you then."
"

Using trade as a weapon of foreign policy has harmed America’s economic interests in the world without significantly advancing national security.



The proliferation of trade sanctions in the last decade has been accompanied by their declining effectiveness. From Cuba to Iran to Burma, sanctions have failed to achieve the goal of changing the behavior or the nature of target regimes. Sanctions have, however, deprived American companies of international business opportunities, punished domestic consumers, and hurt the poor and most vulnerable in the target countries.



According to the president’s Export Council, the United States has imposed more than 40 trade sanctions against about three‐​dozen countries since 1993.



The council estimates that those sanctions have cost American exporters $15 billion to $19 billion in lost annual sales overseas and caused long‐​term damage to U.S. companies–lost market share and reputations abroad as unreliable suppliers.



Economic sanctions are especially damaging when applied to “duel use” technology. U.S. companies face a web of controls that inhibit exporting high‐​speed computers and other high‐​tech goods that, while civilian in nature, could conceivably be used by a hostile regime for military purposes.



Export controls on high‐​tech goods suffer from two fatal flaws: The first is that similar technology can often be obtained off the shelf from foreign competitors. Export controls succeed only in cutting U.S. firms out of fast‐​growing foreign markets without enhancing national security one bit.



The second flaw is that whatever controls are written into law are quickly outdated by Moore’s law of technological advancement. Today’s “supercomputer” inevitably becomes tomorrow’s high‐​end PC.



As well as inflicting economic damage, trade sanctions have been a foreign policy flop. A comprehensive study by the Institute for International Economics found that sanctions have achieved their objectives in fewer than 20 percent of cases. For example, the Nuclear Proliferation Prevention Act of 1994 failed to deter India and Pakistan from testing nuclear weapons in May 1998.



Trade sanctions seldom work because of the competitive global marketplace and the nature of regimes most likely to arouse America’s ire. Although the United States is by far the world’s largest economy, its global economic leverage is limited. The United States accounts for only 13 percent of the world’s merchandise exports and 16 percent of its imports. If Washington seeks to punish another country by unilaterally withholding exports, such as farm products, computers, or oil‐​drilling services, other global suppliers stand ready to fill the gap.



Even if sanctions inflict some pain on the target country, they typically fail because of the nature of regimes most likely to become targets of sanctions. Human rights abuses tend to vary inversely with economic development. Governments that systematically deprive citizens of basic human rights typically intervene in daily economic life, resulting in underdeveloped and relatively closed economies. Such nations are the least sensitive to economic pressure. The autocratic nature of their governments also means that they are relatively insulated from any domestic discontent caused by sanctions. If anything, sanctions tend to concentrate economic power in the hands of the target government and reduce that of citizens.



America’s ongoing embargo against Cuba illustrates the failure of sanctions. When the United States first imposed a comprehensive trade embargo in 1961, Cuba was conducting most of its trade with the United States. Since then, sanctions have utterly failed to influence the government of Fidel Castro, which has used the embargo to excuse its own policy failures and gain international sympathy. Although the embargo once enjoyed a measure of international support, today no other nation stands behind it. The reason is obvious: nearly 40 years after its imposition, the embargo has only hurt American companies and the Cuban people, while leaving the Castro regime firmly entrenched with little prospect of change. The manifest failure of U.S. policy prompted Pope John Paul II during his historic visit to Cuba in January 1998 to declare that sanctions are “always deplorable, because they hurt the most needy.”



Defenders of sanctions often cite South Africa as a success, but sanctions were not the only reason apartheid fell; the fall of the Soviet Union contributed to the climate of reform. Moreover, sanctions against South Africa differed from most U.S. sanctions today in two key respects. One, they were multilateral, while the large majority of sanctions imposed by the United States since 1993 have been unilateral. Second, the apartheid government in South Africa was answerable to a limited but still sizable electorate of about 5 million whites, which made the government more sensitive to outside pressure. Given that multilateral sanctions against a semi‐​democratic government were not sufficient to force change, it is virtually guaranteed that unilateral sanctions against a dictatorship will fail.



U.S. influence around the world is strengthened by the presence of American multinational companies. Foreign direct investment is not only profitable for American shareholders; it also helps foster greater economic growth in less‐​developed nations. American companies introduce new technologies and production methods, while raising wages and labor standards. That creation of wealth helps to advance social, political, and economic institutions that are independent of the ruling authorities. Companies engaged in long‐​term investments in Burma and elsewhere also help to build schools, hospitals, and roads.



China offers a good example of how economic engagement can help to slowly but steadily change a country for the better. Over the past two decades, China has become America’s fourth largest trading partner and the world’s second largest recipient of foreign direct investment behind only the United States, and China will soon be a member of the World Trade Organization.



China’s internal market reforms and increasing openness have fostered rapid growth that has led to rising living standards and greater autonomy for citizens. The share of industry controlled directly by the government has fallen from almost 100 percent two decades ago to less than 50 percent today. Private ownership of homes and businesses is rising dramatically.



Continued economic engagement has also helped open the door to China for a growing number of organizations whose mission is to promote religious and political freedom. For example, East Gates Ministries International, headed by evangelist Ned Graham, has been able to distribute millions of Bibles to Chinese believers. More than a decade after the outrage of Tiananmen Square, the communist government has begun to release political prisoners and allow a small measure of internal criticism. As was the case in Taiwan and South Korea, China’s economic liberalization is creating a foundation for a more vigorous civil society independent of government control.
"
"

I’ve been warning for years of the dangers of the federal Racketeering Influenced and Corrupt Organizations law and how it gives prosecutors and enterprising private lawyers leverage to target above‐​board businesses in search of punishment or profit. Since the law’s passage in 1970, RICO has seldom been used against violent organized crime. Instead, it has been aimed at a wide array of white‐​collar defendants, as William Anderson noted in _Regulation_ six years ago, and especially at unpopular industries like gun and cigarette makers, as Cato’s Bob Levy noted in 2000. The latest fillip is Sen. Sheldon Whitehouse’s proposal to aim racketeering charges against groups that promote wrongful thinking on climate change. The civil side of the statute (“civil RICO”), which can be used in private litigation, is especially susceptible to tactical use by private lawyers who know that the vagueness of the law, the high cost of response, the triple‐​damages provisions, and the racketeering stigma especially are useful in forcing adversaries to the bargaining table. The more those adversaries value respectability, the more powerful the leverage.   
  
  
Now comes word that a Washington, D.C.-based tough‐​on‐​crime group calling itself the Safe Streets Alliance has filed suit seeking, in its words, “to hold those involved with Colorado’s recreational marijuana industry liable under federal racketeering statute and to have Colorado marijuana business licenses held invalid.” Its press release is at least honest enough to acknowledge that the targets include “the citizens of Colorado” for what it believes was their faulty decision to enact Amendment 64 in 2012. In one case SSA, representing a local Holiday Inn franchisee that didn’t care to have a medical marijuana shop near its business, succeeded in forcing owner Jerry Olson (no relation) out of business. A key tactic in the suit — one quite familiar to those of us who follow hardball civil litigation in general — was to name as racketeering co‐​defendants a variety of risk‐​averse, often respectable businesses that had in some way done business with the main target. Thus AP reports:   




…just last week, a bonding company in Des Moines, Iowa, paid $50,000 to get out of the lawsuit.   
  
  
“We are out of the business of bonding marijuana businesses in Colorado and elsewhere until this is settled politically,” said Therese Wielage, spokeswoman for Merchants Bonding Company Mutual.



Thus does the litigation accomplish its goal whether or not it ultimately prevails before a judge:   




“This lawsuit is meant more to have a chilling effect on others than it is to benefit the plaintiffs,” said Adam Wolf, Olson’s lawyer.



SSA lawyer Brian Barnes of Cooper & Kirk doesn’t seem to contradict that:    




“We’re putting a bounty on the heads of anyone doing business with the marijuana industry,” Barnes said.



I’m occasionally asked why I bother to worry about the legal woes of unpopular industries whose goods I don’t even care to consume. A different way to look at the question is that almost anyone’s line of business — whether it be soft drinks or accounting or putting up visitors in one’s home or charitable non‐​profit work or electioneering or employing entry‐​level workers at minimum wage — is one public‐​vilification campaign, or one round of lawsuits, away from becoming an unpopular industry. 
"
"Energy prices are rising, and it hasn’t gone unnoticed that the profits of the handful of large energy supply companies are rising too. While it can be argued that there is no direct causal relationship between the two, there is clearly a case for consumers to have access to better tools that help them access the best tariffs and lower their bills.  In particular, those consumers with energy consumption patterns that are more predictable or desirable for the companies that supply them should be able to demand better prices than others without. The problem is that a single individual customer doesn’t have much in the way of negotiation power in the market. One possible answer is for them to band together into groups, and engage in collective bargaining to get a better deal, a concept called collective energy purchasing. The trend of joining forces to negotiate better prices for their electricity started in continental Europe, particularly in Belgium and the Netherlands, but has gained considerable traction in the UK. The Department for Energy and Climate Change ran a £5m fund in 2012 to provide support to organise such group bargaining.  Schemes such as the Big Switch and the Big Deal provided thousands of consumers with a way to switch, saving up to 25-30% on their annual electricity bill. More recently, several start-ups firms and local initiatives have started offering similar schemes. The process of joining together is typically mediated by a third party, and a large part of it can be automated. For instance, given the adoption of smart meters, it is not hard to imagine a near future in which consumers can simply upload (or provide access to) information about their usage, and a web service can work out which is the optimal tariff and carry out the collective purchasing and switch on their behalf.  It’s a compelling idea, but there’s no guarantee an automatically selected tariff will always be the best choice for all the group’s customers. Buying as a group may provide an optimal result for the group, but this is an averaged result rather than one that applies with respect to each individual customer. Individual customers (or subgroups of customers) may be better off switching individually, or forming their own subgroup around another tariff. This type of phenomena is called coalitional stability, and has been long studied in coalitional game theory, and more recently, in distributed artificial intelligence. Recent research has started using AI techniques to address the challenge of designing more efficient group-buying aggregators. One central issue is modelling how predictable each customer is. Distribution companies must estimate how much electricity their consumers will use and buy long-term, forward contracts. Any shortfall of electricity has to be bought on the spot market or during balancing, typically at a higher price. The converse also holds, in that any electricity bought in excess has to be sold during balancing, usually at a loss. So having accurate data about consumption is important. A “prediction of use” tariff, which asks customers for a prediction of their electricity use (or estimates this from their past consumption records) and charges them accordingly would better match their cost to the supplier based on how predictable their energy use is. Crucially, while each consumer may be unpredictable, grouping them together in a collective reduces their aggregate uncertainty, making their consumption more predictable. In fact, a market could comprise a whole range of these prediction of use tariffs. Some of could be flat, like existing tariffs, where the utility company would carry the risk but charge higher prices. Others would encourage greater predictability from customers, lowering the risk to the supplier who in turn provides a much better price for electricity consumption within the predicted limits (and extra charges for use beyond the predicted amount). Different consumers with different requirements could be dynamically clustered, depending on how well they predict their consumption, with buying groups formed around particular tariffs. The sort of coalitional game theory that can help design software and tools to provide the best tariffs can also divide the bill in the most fair way. On way is the concept of marginal payment, where a customer pays the difference between what the group pays including him or her, and what the group would pay if he or she were not a member. It’s conceivable that better artificial intelligence techniques can help us provide incentives for people to form groups. A well known problem in electricity group buying is that people are reluctant to commit until a critical mass is reached. So marginal payments could depend not only on ease of predicting consumption but also on how early a member joined their collective purchasing group. Used wisely, such collective schemes can raise consumers’ awareness of their energy usage, lowering overall energy consumption, leading to less carbon emissions, lower costs for supplier and consumer, and less wastage. "
"**A deal to allow families to meet over Christmas has been reached by the leaders of all four UK nations.**
A source told the BBC that details on how Covid restrictions will be relaxed will be announced shortly.
Scotland's first minister said she would ""continue to ask people to err on the side of caution"".
BBC Scotland's chief political correspondent said three households will be allowed to meet indoors over five days between 23-27 December.
Glenn Campbell said they will be able to meet in each other's homes, at a place of worship and in an outdoor public space. But the groupings must be ""exclusive"", meaning you cannot get together with people from more than two other households.
He added the leaders of the nations are expected to urge Britons to use any new flexibility sparingly because public health officials are worried Christmas get-togethers could cause a January spike in Covid cases.
Speaking ahead of a meeting of the UK government's emergency committee Cobra, Welsh First Minister Mark Drakeford cautioned any extra freedoms would not be an instruction to do ""risky things"".
Scotland's First Minister Nicola Sturgeon also stressed any changes would be ""temporary"" and ""limited"".
She said that the ""details"" may differ ""to reflect different circumstances in each nation"", such as what the definition of a ""household"" might be.
She added: ""I know everyone has a desire to see loved ones over the festive period.
""However, there is also a very real and a very legitimate anxiety that doing so could put those we love at risk, set back our progress as a country and result in unnecessary deaths and suffering.""
Meanwhile, the government has recorded another 608 deaths of people in the UK who have died within 28 days of a positive Covid test. There were also a further 11,299 cases of people testing positive for coronavirus.
What to do about Christmas divides opinion.
Increased mixing indoors will certainly mean there is greater transmission of the virus.
But, as chief medical adviser Prof Chris Whitty said on Monday, there is a balance to be struck between the harm the virus can cause and the societal and economic impacts of trying to control it.
He called for a ""public-spirited approach"".
By that he means adhering to the restrictions in the lead up to Christmas, being responsible with the opportunity the relaxation gives people and then immediately switching back to compliance.
If that happens any impact could be minimised - and, of course, it will be up to individuals to decide just how much they mix within the rules.
These are very fine judgement calls by ministers.
They hope Christmas will provide respite and help steel the public for what is clearly going to be a long, hard winter.
They also feel they have little choice, believing large numbers of people would ignore pleas not to mix and this way they can provide advice on how to enjoy Christmas as safely as possible.
But there is also the risk by sanctioning it there will be more mixing than there would have otherwise been.
Prime Minister Boris Johnson has acknowledged there would be risks of letting people meet over Christmas but said families should have the chance to reunite.
Transport Secretary Grant Shapps earlier said Christmas travellers should plan journeys carefully and prepare for restrictions on passenger numbers.
Referring to domestic travel during the festive period, Mr Shapps urged those travelling on public transport to pre-book tickets as the capacity of services remains reduced to allow for social distancing and as a result of staff self-isolating.
Mr Shapps also highlighted Network Rail's plans for a series of upgrades and routine maintenance across Britain between 23 December to 4 January.
He told BBC Radio 4's Today programme: ""I would appeal to people to think very carefully about their travel plans and consider where they are going to travel and look at the various alternatives available.""
Mr Shapps added that people who live in areas placed in the highest tier of restrictions in England should avoid leaving their region entirely.
Mr Shapps said confirmation of the exact rules would come by Thursday - when people find out which tier their local area will be in - or potentially before then.
It comes after the prime minister confirmed tougher tier curbs once England's lockdown ends.
Gyms and non-essential shops in all parts of England will be allowed to reopen from 2 December under a strengthened three-tiered system.
Areas will not find out which tier they are in until Thursday - and the decision will be based on a number of factors including case numbers, the reproduction rate - or R number - and pressure on local NHS services.
At a Downing Street news conference on Monday to outline a ""Covid-19 winter plan"", Mr Johnson admitted Christmas this year would be very different to normal.
""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none,"" he said.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
Meanwhile, Mr Shapps announced people arriving in England from many countries will be soon able to reduce their quarantine period by more than half if they pay for a coronavirus test after five days.
The rules will come into force from 15 December and the tests from private firms will cost between Â£65 and Â£120.
Elsewhere, Health Secretary Matt Hancock told MPs the UK's new mass testing capacity could be used after the pandemic to diagnose a wider range of illnesses.
He said a British culture of ""soldiering on"" and going to work despite having symptoms of illnesses, including flu, ""should change"".
""In fact, I want to have a change in the British way of doing things where 'if in doubt, get a test' doesn't just refer to coronavirus, but refers to any illness you might have,"" he said.
Latest figures from the Office for National Statistics showed the total number of deaths occurring in the UK is nearly a fifth above normal levels."
"
Here’s an interesting story from the Times. One wonders if the Royal Society is ready to deal with all the unintended and unmodeled consequences of such actions? The last man-made volcano didn’t go over so well. – Anthony
A familiar man-made volcano - The Mirage in Vegas - Image courtesy PDphoto.org
 From The Sunday Times August 30, 2009
Man-made volcanoes may cool Earth
  



   Jonathan Leake, Environment Editor 

    

THE Royal Society is backing research into simulated volcanic eruptions, spraying millions of tons of dust into the air, in an attempt to stave off climate change.
The society will this week call for a global programme of studies into geo-engineering — the manipulation of the Earth’s climate to counteract global warming — as the world struggles to cut greenhouse gas emissions.
It will suggest in a report that pouring sulphur-based particles into the upper atmosphere could be one of the few options available to humanity to keep the world cool.
The intervention by the Royal Society comes amid tension ahead of the United Nations-sponsored climate talks in Copenhagen in December to agree global cuts in carbon dioxide emissions. Preliminary discussions have gone so badly that many scientists believe geo-engineering will be needed as a “plan B”.
Ken Caldeira, an earth scientist at Stanford University, California, and a member of a Royal Society working group on geo-engineering, said dust sprayed into the stratosphere in volcanic eruptions was known to cool the Earth by reflecting light back into space.
“If I had a dollar for geo-engineering research I would put 90 cents of it into stratospheric aerosols and 10 cents into everything else,” said Caldeira.
The interest in so-called aerosols is linked to the eruption of Mount Pinatubo in the Philippines in 1991, the second largest volcanic eruption of the 20th century. The explosion blasted up to 20m tons of tiny sulphur particles into the air, cooling the planet by about 0.5C before they fell back to earth.
The Royal Society is Britain’s premier science institution and its decision to take geo-engineering seriously is a measure of the desperation felt by scientists about climate change.
read the rest of the story here




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e938acb66',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI think I’ve found the root of Joe Romm’s problem. He needs to go back to school and learn more maths and natural sciences! At least that’s what a recent Yale University study shows.Somehow this paper got by me. Maybe this is old news, and so forgive me if this is already known. It’s nothing you’d hear about from the “enlightened” media, after all.
Recall how climate alarmists always try to portray skeptics as ignorant, close-minded flat-earthers who lack sufficient education to understand even the basics of the science, and if it wasn’t for them, the world could start taking the necessary steps to rescue itself.
Unfortunately for the warmists, the opposite is true. The warmists are the ones who are less educated scientifically. This is what a recent Yale University study shows. Hat tip: www.politik.ch.
Professor Dan M. Kahan and his team surveyed 1540 US adults and determined that people with more education in natural sciences and mathematics tend to be more skeptical of AGW climate science. Of course this means that people will less education are more apt to be duped by it.
Surprised? Here’s an excerpt of the study’s abstract (emphasis added):
The conventional explanation for controversy over climate change emphasizes impediments to public understanding: Limited popular knowledge of science, the inability of ordinary citizens to assess technical information, and the resulting widespread use of unreliable cognitive heuristics to assess risk. A large survey of U.S. adults (N = 1540) found little support for this account. On the whole, the most scientifically literate and numerate subjects were slightly less likely, not more, to see climate change as a serious threat than the least scientifically literate and numerate ones.
Time for you warmists to go back to school (though I seriously doubt many of you are capable of learning much of anything, on account of extreme cultural cognition disability).
To learn more, here’s a video on Cultural Cognition and the Challenge of Science Communication which looks at risk perception w.r.t. the issues of climate change, and here’s a video on Cultural Cognition Hypothesis.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterDer Spiegel reports here (Bloggers accuse IPCC of conflicts of interests) on Climategate 2.0, where the IPCC just published a huge new report on renewable energy, claiming that within 40 years nearly 80% of the world’s energy needs could be met, mostly through a massive expansion of wind and solar power. The report, it turns out, was hardly scientific, and was based a lot on a paper co-authored by an employee (Sven Teske) of Greenpeace International and the European Renewable Energy Council.It means the report’s main message came from a full-time environmental activist and that the data underpinning the science was hardly more than a load of activism, and not real data.
Like much climate scandal news, it first got media traction in the Anglo-regions of the world. Now some of the German media are timidly weighing in. Der Spiegel writes:
So far the debate has made waves in the Anglo-Saxon media. That is probably due to the fact that the first to have noticed the possible conflict of interest is Steve McIntyre.  The Canadian is considered as one of the main critics of the established climate sciences. Last week he let loose at his climate blog Climateaudit.org, and spoke of “Greenpeace-Karaoke” in the IPCC report.”
IPCC Report “completely without partisanship”
Much of McIntyre’s critic was aimed at Sven Teske of Greenpeace and Ottmar Edenhofer of the Potsdam Institute for Climate Impact Research (PIK), who is also co-chairman of the WG3 of the IPCC. But Edenhofer has defended the report in comments to Der Spiegel. Der Spiegel writes:
The climate-economist told SPIEGEL ONLINE he defends the entire report. It is “balanced” and referees the state of scientific knowledge ‘without any partisanship’. He points out that the now much criticized study was subjected to ‘a strict scientific reveiw process’. The underlying assumptions of the concerned study have been clearly named. Moreover the IPCC-Report did not  ‘present any special report about renewable energies as being predominant’.”
As a blogger that follows German developments in climate science and politics, it is well known that the cast of characters at the PIK, including Edenhofer himself, are often more than loose with the truth. The PIK is well-known for its activism, let alone its shady brand of science.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But that doesn’t stop Der Spiegel from presenting a defence for Edenhofer and the report, writing that Edenhofer has a point, and even admitted that the costs of renewable energies are fraught with uncertainties, also see original Der Spiegel story .
Teske also offered a defence, telling Der Spiegel that he was not involved in writing up the press release itself. Der Spiegel quotes Teske:
 I first saw it when it was presented the first time at the press conference in Abu Dhabi.”
Teske adds he doesn’t see any problem being involved it the report, claiming that representatives of the US fossil fuels and nuclear industry were there too. Der Spiegel adds:
Greenpeace represents 3 million member worldwide. ‘Why shouldn’t we be involved in such a report?’, asks Teske.”
Der Spiegel then ends the piece by writing about the new IPCC codex that was agreed on, and quotes the last sentence that appears:
To prevent situations in which a conflict of interest may arise, individuals directly involved in or leading the preparation of IPCC reports should avoid being in a position to approve, adopt, or accept on behalf of any government the text in which he/she was directly involved.”
Share this...FacebookTwitter "
"For hundreds of thousands of years humans lived in hunter-gatherer societies, eating wild plants and animals. Inequality in these groups is thought to have been very low, with evidence suggesting food and other resources were shared equally between all individuals. In fact, in the hunter-gatherer societies that still exist today we see that all individuals have a say in group decision making. Although some individuals may act as leaders in the sense of guiding discussions, they cannot force others to follow them. But it seems that with the beginning of agriculture around 10,000 years ago, this changed. An elite class began to monopolise resources and were able to command the labour of others to do things, such as build monuments in their honour. So how was it that egalitarian societies, where all men were equal, transitioned into hierarchical societies where despots reigned? In recent years archaeologists have tended to focus on the means by which would-be leaders could coerce other individuals into following them (so-called theories of agency). But while leaders probably did coerce their followers once they were in power, it is difficult to see how they could do so at the outset. After all, if all individuals started out with equal resources and equal status, how could one individual force 30 others to do their bidding? This problem forces us to examine the benefits that would-be leaders could provide to their followers – and this is where agriculture comes in. While hunting wild game did not involve much co-ordination beyond placing traps and positioning hunters, agriculture presented an opportunity to massively increase the amount of food that could be produced. A classic example is the development of irrigation systems, which allowed crops to be grown further away from rivers and water sources. Although the role of irrigation systems in creating despotic states has been overstated in the past, they certainly would have created an opportunity for would-be leaders to behave entrepreneurially by managing their construction. Those that chose to follow their agricultural-technologist leader would then benefit from access to irrigation. This would provide the benefit of increased food production, enhancing both their quality of life and the number of surviving offspring they could produce. In this way, social hierarchy could initially arise voluntarily – because individuals that chose to follow the leader were materially better off than those that did not. But under what conditions does this voluntary leadership, where everybody benefits, turn into despotism? I tried to answer this question with a new computational model, which has highlighted two key linked factors. The first is population growth. When populations are small it’s relatively easy for individuals to go back to a leaderless way of life, for example by moving to a new patch of land. This seems to happen in modern hunter-gatherer groups, where people may simply walk away from a bullying leader in the middle of the night. But as population density increases, it becomes harder and harder to find free land to move to that is not controlled by the leader and their followers. Model simulations demonstrate that positive feedback between leaders increasing resource production and population growth can create an obligatory hierarchy, destroying the viability of leaderless life in the area. And empirically, hierarchy formation most often co-occurs with an increase in food production that drives population growth.  The second factor is the cost of changing the leader. Even if individuals are locked into a hierarchy, despotism is not inevitable if individuals can readily choose to follow a different leader. For example, by moving to a different group with a different leader. Group membership in hunter-gatherer societies is quite fluid, so this is relatively easy. But with agriculture, individuals would have become tied to a plot of land in which they had invested, making leaving the group very costly. This would become even more extreme with irrigation farming, where farmers would be tied to the system. Indeed, the most despotic early states arose in locations such as Egypt, where agriculture had to happen in a narrow valley along the Nile, making dispersal very difficult. So the use of agriculture established human societies and provided for them in some ways that improved over hunter-gathering. But it shattered the social norm and facilitated the rise of despotism by attracting followers to entrepreneurial leaders that could provide them with benefits, by increasing population density which reduced the ability for others to survive outside the hierarchical group and by making it so costly to leave the group that to do so was unattractive even when faced with despotic leaders. Even in ancient times at the dawn of agriculture there was, it seems, no such thing as a free lunch."
"
Spurious SST Warming Revisited
Dr. Roy Spencer August 31st, 2009 

My previous post described what I called “smoking gun” evidence of a spurious drift in the NOAA sea surface temperature (SST) product when compared to SSTs from the TRMM satellite Microwave Imager (TMI). The drift seemed to be mostly confined to 2001, almost a ’step’ jump. The moored buoy validation statistics of the TMI sea surface temperatures from Frank Wentz’s web site (SSMI.com) suggested that the TMI SSTs had good long-term stability.
But 2001 was also the year that the TRMM satellite was boosted into a higher orbit, which concerned me. I asked Frank about the effect of this event on the TMI SSTs (which also come from his web site). Frank couldn’t remember the details, but said he spent quite a bit of time correcting for the altitude change on the retrieved SSTs since the microwave emission of the sea surface depends upon the TMI instrument’s view angle with respect to the local vertical.
I know from our many years of work together on the AMSR-E Science Team that Frank is indeed a careful researcher, yet it seemed like more than a coincidence that the TMI and NOAA sea surface temperatures diverged during the same year as the orbit boost. So, I went back to see what might have caused the problem. I went back and thought about the different ways in which one can compute area averages from satellite data.
To make a long story short, because the orbit boost caused the TMI to be able to “see” to slightly higher latitudes, the way in which individual latitude bands are handled has a significant impact on the resulting temperature anomalies that are computed over time. The previous results I presented were for the 40N to 40S latitude band, which is nominally what the TMI instrument sees today. But before 2001, the latitudinal extent was slightly smaller than it was after 2001.
As shown in the following figure, if I restrict the latitude range to 38N to 38S, which was always covered during the entire TRMM mission, I find that the divergence between the TMI and NOAA average SST measurements essentially disappears.
Click for larger image
Even though I was processing the NOAA and TMI datasets in the same manner, I should NOT have been. This is because there were not as many gridpoints over cooler SST regions going into the ‘global’ averages before the satellite altitude boost as after the boost. So, for example, one must be very careful in computing a latitude band average, say from 39N to 40N, to make sure that there has been no long-term change in the sampling of that band.
Based upon the above comparisons, I would now say there is no statistically significant difference in the SST trends since 1998 between TMI, the NOAA ERSSTv3b product, and the HadSST2 product. And it does look like July 2009 might well have experienced a warmer SST anomaly than July 1998, as was originally claimed by NOAA. (Remember, TMI can not see all of the global oceans, just equatorward of about 40 deg. N and S latitude.)
In the bottom panel of the above figure, I also have a comparison between the TMI and AMSR-E sea surface temperatures, which are available only since June of 2002 from the Aqua satellite. As can be seen, there is no evidence of a calibration (or sampling) drift in that comparison either.
So, what’s the moral of this story? Always question your results…even after finding the obvious errors. And maybe I should eliminate the term ’smoking gun evidence’ from any results I describe in the future.
Oh…and don’t believe everything you read on the internet.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e937cacbd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
You won’t want to miss Lord Christopher Monckton (Former advisor to UK Prime Minister Thatcher) on Glenn Beck  – Today Friday, October 30th!
Monckton as many WUWT readers know, is a prominent skeptic and has been making presentations around the USA at college campuses, similar to what Al Gore does. Monckton recently criticized the Copenhagen Treaty and the potential for President Obama to sign it as possibly ceding US sovereignty to the UN on the issue.
Times below:

Monckton will be on Fox News Glenn Beck Show, with former UN Ambassador John Bolton, for the full hour. The topic will be all aspects of the Global Warming Scare and the push for a “new world order” to “deal” with it.
Expect fireworks!
FOX cable news Glenn Beck Show 
Time:  5:00pm Eastern time zone
For viewers that don’t have Fox News, check this page afterwards and we’ll put up links to the recoreded video when it is available.
For now, this video of his recent presentation can be seen here

UPDATE:
Video of the interview is now available  here

 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e922c930d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
In Debunking National Wildlife Federation Claims – Part 2 some commenters claimed that the snow data cited from WRI “was not good enough”. OK then, on to a bigger catchment. Steve Goddard replies in this brief essay.
From Wikimedia: Lake Powell from above Wahweap Marina. July 2004, by Dave Jenkins
Lake Powell (Arizona and Utah) provides a good proxy for western slope snowfall, because much of the snow in Wyoming, Colorado, Utah and Northwestern New Mexico drains into the lake via the Colorado, Green and San Juan Rivers.  The lake currently contains more than 4.5 trillion gallons of water and is 490 feet deep at the dam.

http://en.wikipedia.org/wiki/File:Greenutrivermap.png
Between 2000 and 2005, drought conditions (combined with greatly increased water usage in Arizona, California, Nevada and Colorado) caused Lake Powell levels to drop nearly 120 feet. This prompted a considerable consensus of global warming hysteria.
Every scientific study confirms that global warming will cause the amount of water in the Colorado River to decline
 http://www.ucpress.edu/blog/?p=151 
But a strange thing happened in 2006 – the lake level stopped declining and instead started increasing rapidly.  As you can see in the table below from lakepowell.water-data.com, since 2005 the lake elevation has increased by more than 60 feet above the 2005 low of 3562 ft.  As of January 29, at 3622 ft. the lake is within three feet of the January 29 average of 3625 feet elevation.  The volume of water in the lake has increased by 65% in the last five years to 4.5 trillion gallons. (At movie theater prices for bottled water, that could almost erase the US National Debt.) 














DATE MEASURED


ELEVATION


CONTENT


INFLOW


OUTFLOW


HIGH TEMP


LOW TEMP


WATER TEMP




Wed       Jan 29, 1964


3414.40


1108997


4979.00


1140.00


51.0


24.0


--




Fri       Jan 29, 1965


3491.52


4198819


10445.00


9730.00


54.0


32.0


--




Sat       Jan 29, 1966


3535.41


6797291


8500.00


11800.00


46.0


22.0


--




Sun       Jan 29, 1967


3517.50


5641536


7591.00


6570.00


53.0


29.0


--




Mon       Jan 29, 1968


3525.19


6108008


6872.00


8700.00


47.0


31.0


--




Wed       Jan 29, 1969


3540.30


7097730


15583.00


13700.00


49.0


32.0


--




Thu       Jan 29, 1970


3569.60


9303379


8521.00


10500.00


43.0


23.0


--




Fri       Jan 29, 1971


3600.37


12052938


12221.00


3920.00


58.0


29.0


--




Sat       Jan 29, 1972


3608.01


12794734


12475.00


11800.00


51.0


34.0


--




Mon       Jan 29, 1973


3601.37


12112452


10572.00


20900.00


37.0


23.0


--




Tue       Jan 29, 1974


3647.08


17150100


9599.00


8940.00


50.0


25.0


--




Wed       Jan 29, 1975


3645.75


16958398


9145.00


8620.00


41.0


24.0


--




Thu       Jan 29, 1976


3666.82


19671442


7622.00


9700.00


53.0


28.0


--












All Lake Powell water data for January 29th




Sat Jan   29, 1977


3651.91


17675554


9198.00


19000.00


51.0


27.0


--




Sun Jan 29,   1978


3625.36


14502612


5437.00


6660.00


48.0


32.0


--




Mon Jan   29, 1979


3629.33


14921634


8785.00


20200.00


28.0


14.0


--




Tue Jan   29, 1980


3673.18


20435414


10583.00


17100.00


52.0


25.0


--




Thu Jan   29, 1981


3679.78


21351354


7612.00


12000.00


52.0


30.0


--




Fri Jan   29, 1982


3663.93


19101078


7252.00


16900.00


52.0


32.0


--




Sat Jan   29, 1983


3683.31


21807370


13109.00


10600.00


51.0


39.0


--




Sun Jan   29, 1984


3680.79


21404754


13849.00


24300.00


51.0


26.0


--




Tue Jan   29, 1985


3680.74


21366038


16915.00


25400.00


44.0


31.0


--




Wed Jan   29, 1986


3685.73


22105498


13913.00


20100.00


55.0


30.0


--




Thu Jan   29, 1987


3679.39


21169534


15213.00


26200.00


52.0


39.0


--




Fri Jan   29, 1988


3683.04


21703966


9039.00


11700.00


62.0


31.0


--




Sun Jan   29, 1989


3676.87


20805426


5984.00


13600.00


50.0


8.0


--




Mon Jan   29, 1990


3655.74


17940848


5025.00


9510.00


n/a


22.0


--




Tue Jan   29, 1991


3630.86


14949762


5525.00


10200.00


46.0


26.0


--




Wed Jan   29, 1992


3621.42


13914222


6840.00


12700.00


48.0


22.0


--




Fri Jan   29, 1993


3613.74


13110446


6142.00


13400.00


52.0


28.0


--




Sat Jan   29, 1994


3657.30


18141516


7817.00


11300.00


46.0


28.0


--




Sun Jan   29, 1995


3647.12


16859904


6100.00


9580.00


51.0


30.0


--




Mon Jan   29, 1996


3678.07


20978340


8740.00


15600.00


57.0


28.0


--




Wed Jan   29, 1997


3671.32


20021628


11037.00


18400.00


49.0


33.0


--




Thu Jan   29, 1998


3679.18


21138864


10921.00


19900.00


56.0


26.0


--




Fri Jan   29, 1999


3680.71


21361650


7397.00


14395.00


49.0


n/a


--




Sat Jan   29, 2000


3679.33


21160650


8314.00


13156.00


55.0


24.0


--




Mon Jan   29, 2001


3666.53


19363092


7456.00


14876.00


44.1


28.0


--




Tue Jan   29, 2002


3652.58


17539014


3868.00


13424.00


39.9


28.0


47.0




Wed Jan   29, 2003


3615.58


13300186


4328.00


12787.00


55.0


37.0


50.0




Thu Jan   29, 2004


3592.09


11010776


4858.00


12815.00


50.0


28.0


49.0




Sat Jan   29, 2005


3562.14


8486755


8181.00


14100.00


50.0


37.9


49.0




Sun Jan   29, 2006


3594.59


11241152


6438.00


11534.00


53.1


30.9


47.0




Mon Jan   29, 2007


3599.72


11723383


8253.00


13519.00


48.9


30.9


47.0




Tue Jan   29, 2008


3590.80


10893087


9906.00


12773.00


35.1


19.9


46.0




Thu Jan   29, 2009


3614.36


13174179


7893.00


13360.00


48.9


28.9


48.0




Fri Jan   29, 2010


3622.33


14011695


6974.00


14437.00


39.9


32.0


--




Source:
http://lakepowell.water-data.com/
The yearly change in volume is determined by the formula :
delta H = inflow – outflow – evaporation – seepage
Evaporation is relatively constant from year to year as is seepage, so the formula can be written as:
delta H = inflow – outflow – K
Outflow (water usage) has greatly increased over the last few decades due to massive population increases in Phoenix, Las Vegas and Southern California – not to mention the large and ever increasing amount of water being used by the biofuels industry.  (It has been estimated by the  University of Twente in The Netherlands that the manufacture of one liter of biodiesel requires 14,000 liters of  water).
The point being that despite large  increases in outflow, the lake level has been rapidly recovering. This could be  due to only one explanation – lots and lots of snow in the Rocky Mountains  during the last five years.
And an  extra bonus from the “weather is not climate” department – January 29, 2010 at  39.9 degrees was ten degrees below normal and the second coldest on  record.
  

Lake Powell (Arizona and Utah) provides a good proxy for western slope snowfall, because much of the snow in Wyoming, Colorado, Utah and Northwestern New Mexico drains into the lake via the Colorado, Green and San Juan Rivers.  The lake currently contains more than 4.5 trillion gallons of water and is 490 feet deep at the dam.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e5a1597',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
How has an image of a reservoir in a desert come to be the best, strongest, and most scientific indication of climate change?
A few months back, I posted a critique titled: Gavin Schmidt’s new climate picture book: Anti-Science?
I found it ironic that Dr. Schmidt used photos to depict climate change, while at the same time promoting open criticism of my surfacestations.org project on realclimate.org.  That project also uses photography, combined with measurements and a NOAA sanctioned rating system, to gauge thermometer siting issues. Oddly, there seems to be no complaints from the usual suspects when Dr. Schmidt uses artistic composition photography to illustrate climate change issues.
It is only fair then that since Dr. Schmidt has responded to the original author of that critical piece, Harold Ambler, that I repost Dr. Schmidt’s response here. Harold has invited me to republish that piece here.
A note to readers, Harold is going through a rough patch financially while waiting for his new book, Don’t Sell Your Coat, is to be published in November 2009. Royalties from it won’t come in until mid-2010. So if anyone is so inclined, please visit his web page and give him a  boost in the tip jar. – Anthony
More About Anti-Science
Guest post by Harold Ambler
As most of my readers know, I posted a critique of Gavin Schmidt’s book, Climate Change: Picturing the Science, not quite three months ago. Dr. Schmidt has responded in the last few days:

The point of a photo is always the context in which it’s seen. Lake Powell is a long way below it’s 1990’s peak, and that is due to a combination of reductions in rainfall upstream and additional demands on it’s water downstream. The last two years have seen a small rise in water level, and as you state correctly, it is important not to read too much into a short term record.
However, the real point of the photo (and as we discuss in the chapter that uses it), is that climate change is really only an issue because of the impacts – whether on human society or ecosystems. Areas that are already under water stress, such as the American South West are very vulnerable to changes in rainfall regime. And in fact, there is some evidence that long-term trends in precipitation in this region are already being affected by ongoing changes.

We have a long discussion in the book about being careful with the problem of attribution in imagery and we try to make that clear in the captions.”
The science concurs:
http://www.csmonitor.com/2008/0213/p25s05-usgn.html
“Last week, Dr. Barnett published additional work in the journal Science attributing 60 percent of the reduction in snowpack, rising temperatures, and reduced river flows over the past 50 years to global warming.
The latest work “not only shows that climate change is a real problem. It also shows it has direct implications for humans – and not just in the third world,” says Peter Gleick, president of the Pacific Institute in Oakland, Calif.”
So yes, it’s a combination of things, as stated in the book (if you bother to read past the cover photo) and in the scientific literature.
My Response to Dr. Schmidt (Plus a Note to Readers):
I grew up in the San Francisco Bay Area and lived through a few droughts, including the very serious one of 1976 to 1978. Again and again, my family and I saw water levels in the local reservoirs (and others in the state) decline to worrisome levels before they were, thankfully, replenished. One perspective on the phenomenon of alternating drought and wet in the West is that it is terrifying, and should be brought to as many people’s consciousness as possible as a new menace, part of global warming, etc. Another, more like my own, would point out that the astonishing agricultural productivity and explosion of population throughout the Southwest are proofs of humanity’s ability to adapt to its natural surroundings in very effective ways.
=====
Please read the remainder of the story at Talking About the Weather and don’t forget the tip jar 😉 – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93dbfa62',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Former vice president and Oscar winner Al Gore is scheduled to testify to both House and Senate committees today about global warming. For the past few years Gore has traveled across America speaking to audiences that range from friendly to worshipful, from journalists in New York and Washington to actors in Hollywood. If he has ever faced skeptical questions, it hasn’t been reported.   
  
  
We have several times invited the former vice president to present his famous slide show at the Cato Institute, in conjunction with a slide show prepared by Patrick J. Michaels, who takes a more benign view of climate change. Michaels is senior fellow in environmental studies at the Cato Institute and research professor of environmental sciences at the University of Virginia. He is the state climatologist of Virginia, a past president of the American Association of State Climatologists, and an author of the 2003 climate science “Paper of the Year” selected by the Association of American Geographers. His research has been published in major scientific journals, including _Climate Research, Climatic Change, Geophysical Research Letters, Journal of Climate, Nature, and Science_. He received his Ph.D. in ecological climatology from the University of Wisconsin at Madison in 1979. His most recent book is _Meltdown: The Predictable Distortion of Global Warming by Scientists, Politicians, and the Media, _which has been number one on Amazon’s global warming bestseller list for months at a time and has been reprinted twice this year.   
  
  
Gore’s office has declined our invitations. If Vice President Gore is committed to public understanding of climate change, why will he not demonstrate to a Washington audience composed of both supporters and skeptics that his ideas can carry the day in a dialogue with a leading critic? He wiped the floor with Ross Perot; does he fear that the case for catastrophic climate change is not as strong as the case for NAFTA?   
  
  
The invitation is still open. Mr. Vice President, please come to the Cato Institute and present your slide show to an audience of journalists and scholars with a knowledgeable climate scientist also on the dais.
"
"
Orland CA and the New Adjustments
by Steve McIntyre on June 29th, 2009
In my last post, I observed that NOAA’s Talking Points applied their new “adjustments” to supposedly prove that NOAA’s negligent administration of the USHCN network did not “matter”.
In order to illustrate the effect of the new methods in this post, I’ll compare the new adjustments (post-TOBS) to the old adjustments (post-TOBS) on a “good” station – Orland CA, a prototype “good” station, discussed at the outset of surfacestations.org, discussed at WUWT here and CA here in early 2007.

The station history for Orland (at CDIAC) says that it has been in its present location for (at least) most of the 20th century and has had minimal changes during that time, other than perhaps time-of-observation (TOBS). The TOBS adjustment is carried forward into USHCN-v2. As I understand it, NOAA’s New Adjustment Method replaces station-history based adjustments for instrumentation changes and station location (the latter formerly done in FILNET).
As a benchmark, here is the difference between FILNET (adjusted) and TOBS for Orland in the “old” USHCN. Adjustments in the 20th century are negligible – in keeping with station history information that indicates no changes in location.

Figure 1. “Old” USHCN Adjustments for Station Location and Instrument Changes
Now here is the net adjustment in the “New” USHCN.
Two points jump out. Look first at the monthly adjustments at the right hand side. In the “old” method, there weren’t any adjustments to recent data – where metadata did not indicate any relevant change. In the “new” method, there are all sorts of jittery little adjustments. They seem to average out, but why introduce these jitters in the first place? It’s starting to look like a pointless Hansen-esque (ROW-style) adjustment that simply distorts the underlying data.
On a larger scale, the new adjustment noticeably increases the 20th century trend at Orland.

These graphics strongly indicate to me that the effect of the algorithm – regardless of whatever good intentions may underlie it – is that data from lower quality stations is being blended into the presently archived Orland data. I presume that something similar is happening to other “good” stations (though I’ve only examined one example so far.) (Note that Orland is a CRN3 station. However, its excellent continuity makes it a pretty attractive station for benchmarking and visually it doesn’t look a “bad” CRN3 station).
Based on this example, it looks like NOAA’s Talking Points comparison is between the overall average and 70 “adjusted” stations – AFTER the good stations have been adjusted. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e94cd2f83',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Britons should stop ""soldiering on"" by going to work when sick and making others ill, the health secretary says.**
Matt Hancock said people in the UK were ""peculiarly unusual and outliers"" for still going to work when unwell.
He made the comments in a joint session of the Health and Social Care and the Science and Technology committees.
He also told MPs he would like to see the diagnostic capacity built for Covid used to test for other illnesses like flu once the pandemic had passed.
The UK now has the capacity to carry out over 500,000 tests a day, with new labs to be opened next year to double that number.
He said he wanted to see the ""global-scale diagnostics capability"" continued to be used.
""Afterwards we must use it, not just for coronavirus, but everything,"" he told MPs.
""I want to have a change in the British way of doing things where 'if in doubt, get a test' doesn't just refer to coronavirus but refers to any illness that you might have.
""Why in Britain do we think it's acceptable to soldier on and go into work if you have flu symptoms or a runny nose, thus making your colleagues ill?
""I think that's something that is going to have to change.
""If you have, in future, flu-like symptoms, you should get a test for it and find out what's wrong with you, and if you need to stay at home to protect others, then you should stay at home.
""We are peculiarly unusual and outliers in soldiering on and still going to work, and it kind of being the culture that 'as long as you can get out of bed you still should get into work'. That should change.
""This year there's been far fewer respiratory and other communicable diseases turning up in the NHS.
""I want this massive diagnostics capacity to be core to how we treat people in the NHS so that we help people to stay healthy in the first place, rather than just looking after them when they're ill."""
"
Share this...FacebookTwitterHere’s a must read.
German blogsite Ökowatch here brings our attention to a report appearing in the Smithsonian: America’s First Great Global Warming Debate.
Even Thomas Jefferson was worried about man-made climate change. The Smithsonian writes:
The date was 1799, not 1999—and the opposing voices in America’s first great debate about the link between human activity and rising temperature readings were not Al Gore and George W. Bush, but Thomas Jefferson and Noah Webster.”
Thomas Jefferson, we find out, was a  warmist (who probably had not yet figured out how to make tons of money like Al Gore has done). According to the Smithsonian, Jefferson wrote:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Snows are less frequent and less deep….The elderly inform me the earth used to be covered with snow about three months in every year. The rivers, which then seldom failed to freeze over in the course of the winter, scarcely ever do so now.”
The cause of the climate change back then was man, though not from CO2 emissions, but through deforestation (ironically, today’s efforts to regulate climate are resulting in accelerated deforestation).
Author Samuel Williams claimed  climate change back then was “so rapid and constant.” Unfortunately Williams’ observations are not reflected by Mann’s Hockey Stick chart, which indicates very little climate change back then.
Like today, there were skeptics too, with Noah Webster being among the most vocal  in claiming that the conclusions were mainly based on anecdotes. The Smithsonian writes that Webster eventually prevailed, and quotes Kenneth Thompson, a modern environmental scientist from the University of California at Davis, who praises Websters saying:
,,, ‘the force and erudition’ of Webster’s arguments and labels his contribution to climatology ‘a tour de force.’
The same can be said about today’s skeptics.
Share this...FacebookTwitter "
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
nan
"Anthony Albanese will travel to New South Wales coal country over the weekend in an effort to persuade regional Australians that net zero by 2050 means opportunity for blue-collar workers and for farmers. In an address to the country Labor conference in Singleton on Saturday, the Labor leader will blast the Nationals for engaging in “lazy cynicism” and for selling out regional communities by opposing action to reduce greenhouse gas emissions.  Federal Nationals have been out since Labor announced it would adopt the mid-century net zero target, claiming that shift would spell the death of agriculture. But a former deputy leader of the NSW Nationals, Niall Blair, said this week the target Labor adopted would provide a great opportunity for the agricultural sector in Australia to diversify and thrive. Albanese will tell Saturday’s conference the global community wants Australia to reduce emissions, but it is not demanding Australia stop exporting coal. “In fact, the global steel and aluminium industries – all needed to build solar panels and wind towers – will continue to demand high-quality Australian coal for decades to come”. Labor suffered negative swings in the Hunter Valley region during last year’s election because of a backlash in coal communities to the party’s climate policies. Albanese will say coal will remain part of such regional economies, but also point out regional economies are increasingly diversified. “The Hunter doesn’t have all its eggs in the coal basket,” Albanese will say. “Contrary to Nationals’ rhetoric, regional Australia is more than resources alone. Look around you. The mighty Hunter is Australia’s largest regional economy, with an output of more than $47bn each year”. “Could you imagine Australia without Hunter wine? Could you imagine horse-racing without Hunter thoroughbreds? Down the road we have the University of Newcastle. Up the road, the University of New England. Two of the jewels of our tertiary sector. “To ignore the diversity of regional Australia makes no sense.” Albanese will also argue new technologies will create new jobs. “With the development of an Australian hydrogen industry, regional Australia would be a natural home for expanded industries in aluminium, steel, silicon and ammonia.” He will say there are huge opportunities in regional Australia to contribute to the abatement task and gain new incomes streams through carbon farming. “Australia has the potential to capture 1bn tonnes of carbon dioxide a year, generating a new source of income for our farmers in the process.” Albanese will also nominate forestry and rare earths as blue-collar regional industries that will continue through the transition to decarbonisation. “Just as coal and iron ore fuelled the industrial economies of the 20th century, [rare earth and forestry] will fuel the clean energy economies of the 21st,” the Labor leader will say. “If we leave it to the Nationals, we will drift back towards the 19th century. They would rather cling to yesterday and run scare campaigns than embrace the opportunities of tomorrow. “This lazy cynicism is shameful. They sell out their own communities and our full potential as a nation. To deny energy alternatives as the Nationals do is to rob regional communities of their future”."
"**The government is planning to pass new laws to cut Britain's overseas aid budget, the BBC has learned.**
It has raised fears among MPs that the reduction could be permanent.
There had been speculation the chancellor was proposing a temporary, one-off cut to help pay down the government's record deficit.
Foreign Secretary Dominic Raab said the UK is ""a leading, if not one of the leading, countries on aid"" and ""that will continue"".
The idea behind a temporary cut was to reduce aid spending next year to just 0.5% of national income, down from the legally binding target of 0.7%.
But the BBC understands that Rishi Sunak's reforms will require new legislation to be passed by Parliament, which MPs believe implies a permanent cut to the aid target or even its abolition.
The issue is that the 0.7% baseline for Britain's aid budget is enshrined in law by the International Development (Official Development Assistance Target) Act.
This does allow the government to miss the target in certain circumstances, such as if there is a substantial change in the country's national income.
Foreign and Development Secretary Dominic Raab is simply required to lay a report before Parliament explaining why he has missed the target.
But there is a growing belief at Westminster that this exemption can apply only retrospectively.
The act places a duty on Mr Raab to ensure the 0.7% target is met. If he misses it, the act requires him to describe what steps he has taken to ensure the target is met the following year.
Some MPs and charities believe these two provisions mean the government cannot declare in advance its intention to miss the target without breaking the law.
To cut the aid budget without fresh legislation might lay the decision open to judicial review.
MPs also believe that a one-off cut to the aid target - a saving of about Â£4bn - would hardly touch the sides of the Â£350bn deficit projected for this year. They say it only makes sense for the Treasury if the cut is permanent.
They also believe that if the government is going to reduce aid spending and face significant political and international criticism, ministers will be tempted to go the whole hog and scrap the target entirely.
Almost 200 charities, two former prime ministers, opposition parties, church leaders, ex-heads of the armed forces, global philanthropists have all come out against the cuts.
The Archbishop of Canterbury Justin Welby has urged Prime Minister Boris Johnson not to go ahead with cuts, saying that ""helping the world's poorest is one of the great moral and ethical achievements of our country"".
The risk for the government is that passing new legislation would give critics of the aid cut the chance to oppose and potentially block the changes in Parliament.
Although the government has a working majority of more than 80, it has seen a number of rebellions of late. One senior Tory MP said defeating the government on this ""would be entirely doable"".
Such is the scale of the reforms to the aid target that Mr Raab is expected to make a statement to MPs about it on Thursday."
"
Share this...FacebookTwitterBy Matt Vooro
The current year 2011 is a good example of what happens when global temperatures drop and we have cold and snowy winters that stretch well into spring. The current La Nina and the cold PDO brought colder temperatures and extra amount of snow during the past winter to many parts of North America, which means significant spring flooding like we just had in Central US and many parts of Canada.
Even Hadley shows cooling over the last 10 years. Yet policymakers all believe it's getting warmer!
The late and extra snow pack in the Rockies and the colder Pacific air, generally due to the colder North Pacific Ocean surface temperatures as measured by the PDO, also caused extra spring rain and even more flooding as well as severe tornadoes. As the quite cold air from the Rockies (due to the significant snow pack still in the mountains) meets the warm moist air coming from the Gulf of Mexico, severe and frequent tornadoes are spawned in the US tornado alley. This was also the pattern in the 1970’s.
Moreover, there could also be a loss of annual crops this year due to a shortened growing season from the extra wet soil and late planting because of the flooding and cold spring, This pattern could repeat itself many times in the decades ahead similar to the climate we had in the 1970’s. Both the Atlantic and Pacific oceans show strong indications of heading for cooler SST levels.
These alternating cold and warm phases typically last for 20-30 years, as reflected in our past climate records. We just came out of the warm phase and appear to be now headed for a cooler phase. The winters started to get cooler for some parts of the Northern Hemisphere already after 1998, but more noticeably and significantly in many regions including Europe and Asia during the last for 4 years. 
Wasteful anti-global warming policies have resulted in badly needed funds being diverted towards very expensive and unsustainable green energy projects and carbon dioxide storage or sequestering. Without major government subsidies most of these projects would not be viable. Indeed their implementation worsened the financial situation of several poorer European nations. In some countries, fossil fuel plants are being shut down prematurely rather than being converted to cleaner fossil energies resulting in 100 % increase in energy costs due to the more expensive green energy replacements.
These very expensive policies to fight global warming have little effect on global temperatures, if any. These misguided policies divert valuable funds from other vital areas of our global life, like helping nations experiencing natural disasters, job creation, better health care, improved flood control, rebuilding homes and infrastructure after tornadoes and major flooding and extra food storage for emergencies. In my judgment this problem could get much worse in the coming years.
Like the Pacific Ocean, the North Atlantic Ocean is also cooling again and by 2015 we could begin to feel even cooler weather during the winter and spring especially along the North American eastern coasts and western Europe. Food and energy could be in short supply unless we all adjust our national and global focus from a non existing global warming threat to a much bigger and very current threat from global cooling for the next 20-30 years.
The events like cold and snowy winters, extra flooding and severe tornadoes have very little to do with man generated carbon dioxide or global warming as, to the contrary, the global temperatures have been cooler than normal this year and global temperatures have actually been declining since 2001 (see chart above).
Share this...FacebookTwitter "
"A development of 750 new homes in the small town of Ilfracombe on England’s north Devon coast has been approved by the local council. The news would be unremarkable if it weren’t for the identity of the project’s backer – the artist Damien Hirst. One wonders if the “artist’s impressions” of the prosaically titled Southern Extension are by the man himself. I suspect not. The new-town’s name is matched by a series of rather sparse, equally prosaic sketch-renders of predictable developer mass housing. The only hint we get at an artist’s vision behind Southern Extension is a statement from the architect: Hirst, we are told, has a “horror of ‘anonymous, lifeless buildings’ and wants ‘the kind of homes he would want to live in’”, and is intent on developing a thoroughly sustainable, wind and solar powered-eco town. New towns are nothing new. And neither are housing developments led by social and cultural visionaries, and industrialists. From the model villages of mustard-magnates in the early 19th century (Trowse, in Norfolk, was expanded by the Coleman family in 1805); Titus Salt’s Saltaire in Yorkshire (built by the industrialist to replace slum conditions for workers at his mills in 1851); to Tomáš Baťa’s shoe company town in Essex (effectively a communist enclave in south-eastern England, built in 1933), and Prince Charles’ Poundbury (built from 1993 onwards, on the Prince’s Duchy of Cornwall land in Dorset), industrial (and agricultural) endeavour, social ambitions and architectural desires have been the drivers behind the building of villages and towns with utopian aspirations. What is new, however, is the involvement of artists as the force behind these future imaginings. Southern Extension is to be partially built on land owned by Hirst, adjacent to one of his residencies in the north Devon town, with the development backed by the artist. In the model villages of old, wealthy, socially minded industrialists wished to provide accommodation for their workers that bettered the social ills that they saw. They established communities around the mills, factories and fields that needed supplies of well-rested, well-fed and reasonably happy workers to generate their wealth. But while they were indeed socially minded, they were very much industrial capitalists at heart. These villages and towns were industrial utopias – work and production were at the core of their ambitions, as well as social well-being. In 21st century Britain, we no longer work in the traditional industries. Model towns and villages are not built by the Salts and Baťas of our day. But what we most certainly do have is a culture industry. And any artist worth their salt (and certainly those like Hirst) are adept business people. They are entrepreneurs, and within the culture industry, they can be seen as the industrialists of our time.  But today’s culture industry is markedly different from the industries of the past: the primary commodity is cultural capital, and the most important producers in the chain are not the artists, or even the artist’s assistant’s who produce the works, but the consumers who give the art its value in the first place. The consumer, by endowing the objects of art (and their prints, and coffee-mugs, and dot-painted bins that are associated with Hirst’s art) with cultural capital, produces the value of the art in itself. Hirst already has a strong, and controversial, presence in Ilfracombe. He has established a gallery, a restaurant, a café and plans for further businesses in the area. Verity, a 20m tall statue created by Hirst of a naked pregnant woman, looks out over the town’s harbour. So why would an artist build a new town next to his adopted home-town, and use his own capital to drive the development? We can look to the prosaic, un-visionary “artist’s impressions” of Hirstville for our answer. These drawings belie the bottom-line economics at the heart of Hirst’s drive to develop this corner of England. For an artist who operates on shock value, there is little shocking (expect perhaps for the lack of shock) in this vision of a future English town. The imaginings and associated descriptions (“sustainable”, “eco” and the like) and the sparseness of the drawings allow the local council to pass the planning application without undue concern: this town is much like that of any new development – unthreatening, free of any substantial critical vision, and capable of being delivered at high margins, turning a tidy profit for the artist-cum-developer.  Where Hirst’s first stroke of genius comes into play, however, is simply through his own involvement. The Hirst brand will convert this town into more than housing: it will attain cultural capital though association and consumption (and not, judging by the “artist’s impressions”, through any form of design), and therefore command higher prices for the real estate. Economically, the development almost can’t fail to be a success for Hirst – for who wouldn’t want to live in a house that shared the magic of the man who put a shark in a tank of formaldehyde? But perhaps we can also see faint echoes of the model-village aspirations of yore? For the important point to notice here is the association between the Hirst-ville development and Ilfracombe – or Hirst-on-Sea as it’s become known by some. Like the industrialists of the 19th and 20th centuries, the model village of Southern Extension will provide a steady stream of productive employees in the culture industry, ready to work hard by eating, drinking and buying in the cafes, restaurants, and art galleries of Hirst-on-Sea: the hard graft of productive consumption.  And what better way to secure both your future workforce and consumer base, than by creating the brand convergence between the home (the Hirst-house) and the place of work and play (the Hirst-café/restaurant/gallery)?  With several thousand new residents housed in the latest eco-homes, and imbued with the cultural capital of The World’s Most Famous Living Artist, the business interests (the culture factories) of Hirst’s Ilfracombe are almost guaranteed to remain productive for many years to come, through the dedicated service of those “creatives” to whom the development will no doubt be marketed."
"
Share this...FacebookTwitterMuch of the German media have been screeching and hyperventilating today about CO2 emissions reaching a record high, see FOCUS or TAZ here or Die Zeit here or Der Spiegel here, to name a few.
All the dregs are at it, making dire 100-year predictions based on silly climate models that have been proven to be wrong time and again. Warmists are gasping in panic screaming “time is running out and they we’ve got to act now!” Where’s the Valium? Take a look at the global temps:
Hadley shows cooling over the last 10 years, even though CO2 emissions have been climbing!
The International Energy Agency (IEA) reports that CO2 emissions rose 1.6 billion tons in 2010, the highest since record keeping began. Total CO2 emissions last year were 30.6 billion tons globally, up 5% from the previous record set in 2008.
Face it – the record emissions are good news and are an indicator of global economic growth and vitality. That’s what normally happens when the economy grows – more energy gets consumed to do more work. Let’s hope the trend continues. Don’t worry, CO2 will not cause the temperature to go up that much. The science behind global warming and tipping points is JUNK.
Indeed the temperatures are not cooperating, and they are not about to for a couple more decades. Time to go back and redo the science.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIf you support your enemies, then you ought not wonder why they grow stronger and soon defeat you.
The early results of the Bremen state elections are in, and once again the conservative CDU Party took one on the chin, and in the gut, and again the Greens soared, read here. The German state of Bremen will now be governed by a solid majority coalition of socialist Reds and environmental Greens. As for the conservative CDU party, it has tumbled behind the Greens to 3rd place, picking up only a measly 20% of the vote.
A similar election debacle took place just weeks earlier in the German industrial state of Baden Wurttemberg, where the Greens, who were once just a fringe party, swept to power after 60 years of conservative CDU rule. Angela Merkel’s conservative CDU party is now a collapsing house of cards.
Even the CDU’s coalition partner, the business-friendly FDP Free Democrats, have been reduced to a mere asterisk in the polls, falling well below the 5% hurdle and are now an insignificant political force.
Why are the Greens flying high and the Conservatives and Free Democrats plummeting?
To answer that question, it is helpful to play out a scenario in your mind. Imagine if the Angela Merkel’s conservative CDU one day adopted a new plank in its platform: “Jobs for Germans” and “Foreigners stay out!” What would be the result?
This would tantamount to a mainstream party endorsing and legitimising an extreme fringe ideology, an ideology that deserves defeat and not support. Such an endorsement however would be an immediate boost for Germany’s far-right brownish parties, who would be propelled and zoom in the polls overnight. The CDU on the other hand would deservedly go into a tailspin.
This is what happened with the CDU and the Greens. The Greens in Germany 15 years ago were a just minor party that struggled to reach the 5% hurdle in state and national elections. But over the years, the big parties like the conservative CDU began adopting and endorsing politically-correct green positions on energy and climate change rather than opposing them. The green political-correctness, they thought, would make them more appealing. The result: green fringe positions became viewed by the public as having been legitimised, and so people began to view the Greens as mainstream. Acceptance grew.
Today many people are green to a certain extent – it’s hip after all. So when going to the polls, why vote for the CDU when you can vote for the real deal: the Greens!
Successful politics is not about supporting the kooky fringe positions of your opponents and so legitimising them. No – it is about exposing them for what they really are, and then hanging them around the necks of your opponents and parading them through the public. But if you stupidly support the positions of your opponents, then you ought not wonder why they they keep getting stronger.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The CDU naively believed green voters would gravitate to their ranks if they adopted fuzzy green positions. But that didn’t happen. Indeed the opposite occurred in that the green positions looked more attactive, and so voters migrated to the Greens instead.
The same happened in the USA when Newt Gingrich gave the climate movement his stamp of approval by cozying up with Nancy Pelosi on a sofa. Republican Arnold Schwarzenegger did the same by adopting extreme green beliefs, and even became more green than a number of Democrats. In the end, along with a host of other RINOs, they only became extremely valuable useful imbeciles for the Democrats. The GOP paid a heavy price.
Endorsing the kooky climate positions espoused by Nancy Pelosi and the Democrats neither strengthened Newt nor the Republicans. To the contrary, it glorified the freak positions of the Democrats, who only grew stronger at the GOP’s expense.
“Thanks for the climate support Newt!” —- “Thanks for all the help Arnie!”
German useful political imbeciles for the Greens
Chancellor Angela Merkel took her endorsement of green to an extreme by having Hans Schellnhuber of the crackpot Potsdam Institute for Climate Impact Research (PIK) act as her close advisor on climate change. Merkel even once said that climate change is humanity’s greatest threat and challenge.  Merkel’s Environment Minister Norbert Röttgen (CDU) is also a huge proponent of going green – in order to save the planet.
What better way to make the Greens look like genuine heroes?
Today CDU leaders are scratching their heads wondering why the Greens have passed them in the polls. How much more clueless can one get?
Even the FDP’s new party leader Philipp Rösler has been exposed as a political punk with no qualms about prostituting himself and his FDP party to green industry lobbyists. Where is the FDP today? The party is reduced to just an asterisk in the polls – a political laughing stock. They can’t figure it out.
It’s your constant endorsement of the opposition, stupid!
Getting back to success
To get back to successful politics, the first step is to stop endorsing the absurd positions of your opponents. Then you have reject them and expose the fraud and the corruption that is the science of climate change, and the utter folly of controlling climate by fiddling with one single trace gas. Now is the time to expose the green filth, the junk science, the web of cronyism, and the dead-end it is all taking us to. The first party that does that will be the first to climb back to prominence.
Share this...FacebookTwitter "
"**Personal protective equipment (PPE) stockpiles in England were inadequate for the Covid pandemic and price rises earlier this year cost taxpayers about Â£10bn, the spending watchdog has said.**
The National Audit Office said there had been a particular shortage of gloves and aprons.
The government said the NAO's report recognised that NHS providers had been able to get what they needed in time.
Almost Â£12.5bn was spent on 32bn items of PPE between February and July 2020.
During the same period in 2019, 1.3bn items were bought at a cost of Â£28.9m.
Each item had been ""substantially"" more expensive in 2020, because of very high global demand, the NAO said, from almost triple the cost for respirator masks to more than 14 times as much for body bags.
Had the government been able to pay 2019 prices, it would have spent Â£2.5bn on PPE in 2020.
In reality, it had spent Â£12.5bn, including hundreds of millions on ""unsuitable"" items that could not be used.
Some had ""passed its expiry date or did not meet current safety standards"", the watchdog said, with ""insufficient checks"" meaning Public Health England had had to recall eye protectors that did not meet standards.
In Parliament, on Wednesday, Labour leader Sir Keir Starmer accused Prime Minister Boris Johnson of ""wasting"" taxpayer's money on equipment that ""can't be used"".
But Mr Johnson replied ""99.5%"" of the 32 billion items of PPE bought between February and July 2020 ""conform entirely to our clinical needs"".
Earlier, the Department of Health and Social Care (DHSC) said ""only 0.49% of all the purchased PPE tested to date"" had not been fit for purpose.
NAO head Gareth Davies said: ""As PPE stockpiles were inadequate for the pandemic, government needed to take urgent action to boost supplies.
""Once it recognised the gravity of the situation... the price of PPE increased dramatically, and that alone has cost the taxpayer around Â£10bn.""
Before the Covid-19 pandemic, there were two emergency stockpiles of PPE:
But the NAO said: ""The EU exit stockpile held few items of PPE other than a large number of gloves.""
Meanwhile, the flu stockpile, as well as having shortages of some key items, did not include any gowns or visors despite the fact they had been ""recommended for inclusion in June 2019 by the New and Emerging Respiratory Virus Threats Advisory Group (Nervtag)"".
Public Health England told the NAO it had been analysing the market to work out which gowns to buy, when the pandemic had begun, which it said was the ""normal approach"" to find a lower price.
In mid-March, the government had still believed its two stockpiles would provide ""most of the PPE needed to manage a Covid-19 pandemic"" and so focused on distributing this PPE rather than buying more, the NAO reported.
The situation had become ""precarious"" in April and May, with stocks threatening to run out.
At one point, only 3% of the required number of gowns had been available.
But the nation did not at any point run out of any type of PPE.
The scramble for PPE in the early stages of the pandemic was not confined to the UK. Every healthcare system was desperate to secure protective equipment and prices soared. But the National Audit Office lays bare how the UK was at the back of the queue, having failed to spot the warning signs and how woefully inadequate the stockpiles were.
A failure to anticipate what might be needed for anything other than a flu pandemic in essence cost the taxpayer Â£10bn - the extra money needed to secure supplies such as gowns and visors during the Covid crisis.
The report highlights poor distribution of PPE with many staff saying they did not have the right equipment. The NAO notes starkly that health and care employers have reported more than 100 deaths among staff because of exposure to coronavirus.
An official inquiry, when it happens, will look hard at many aspects of the UK's preparedness and handling of the crisis and the PPE issue will be central. With a series of reports, the NAO has now done important groundwork but there is much still to find out.
A DHSC official said: ""As the NAO report recognises, during this unprecedented pandemic all the NHS providers audited 'were always able to get what they needed in time' thanks to the Herculean effort of government, NHS, armed forces, civil servants and industry"".
But the NAO heard feedback from care workers, doctors and nurses that showed ""significant numbers of them considered that they were not adequately protected during the height of the first wave of the pandemic"".
Employers have reported 126 deaths among health and care workers linked to exposure at work.
And there were concerns about training and whether the equipment was appropriately fitted, particularly from women and people belonging to ethnic minorities.
In a Royal College of Nursing survey of 5,000 NHS staff, 49% of respondents belonging to ethnic minorities said they had been adequately ""fit tested"" for a respirator, compared with 74% of white nurses.
The DHSC said it was ""listening to the reported practical difficulties with the use of some PPE experienced by women and black, Asian and minority ethnic (BAME) individuals, among others, and... taking action to make sure user needs are adequately addressed in future provisions"".
In a separate report, the Public Accounts Committee, a parliamentary body which works closely with the NAO, said it was ""concerned that the department had no plan before the pandemic for how it might increase critical care equipment in the event of an emergency"".
""This lack of preparedness was exacerbated by the fact that it did not know how many ventilators were available to the NHS to begin with,"" the PAC said.
But it added the government had managed to buy an additional 26,000 ventilators for use in the NHS, a ""significant achievement""."
"**A former Manx commissioner has died after testing positive for Covid-19 while receiving treatment in England.**
David McWilliams, who was in his mid 70s, had been flown to north-west England for spinal surgery.
His grandson Marcus Taylor paid tribute on Facebook to his ""hero"", adding that the Ballasalla resident had been ""my role model, my guide, my best friend"".
Health Minister David Ashford said the former Malew commissioner's death was ""tragic news"".
He said it was still safe to travel to the UK for treatment and people should not be ""reconsidering their medical treatment"".
Mr McWilliams, who was registered blind, served with Malew Commissioners between 2001 and 2004 and also campaigned for better provisions and benefits for those with disabilities.
He was also a former chairman of Sailing for the Disabled on the island.
A Malew Commissioners spokesman said Mr McWilliams would be ""greatly missed"".
""He was well known and respected around Ballasalla Village and the wider community,"" he said.
""David took a great interest in local politics and cared much for the people of the island.
""He always wanted to ensure that people were treated fairly and with respect.""
_Why not follow BBC Isle of Man on_Facebook _and_Twitter _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
"
I’ve mentioned problems with airports as climate stations in the past, mostly that they are pockets of UHI that have grown with the 20th century aviation boom. A good example is Chicago O’Hare airport. I’ll bet that many of you don’t know that the ICAO ID for O’Hare, is KORD, and FAA uses ORD which is what you see on airline luggage destination tags. “ORD” has nothing to do with the name O’Hare, which came after the airport was established. It has everything to do with the name “Orchard Field” which is what the airport started out as, which at the time was far more rural than it was now. You can read about its early history here.


Here is what it looked like in the 1940’s:
Looking down runway 22 at Orchard Field - photo circa 1943 - Image courtesy of the Bensenville Community Public Library O'Hare collection
Here’s that same view today from Google Earth:

Looking down runway 22 today - click for larger image
Look at O’Hare today, a sprawling megaplex of concrete and terminals surrounded by urbanization:
Click for interactive view
The weather station location above is designated by the orange pushpin. Here’s a closeup view:
Click for larger image
Note that there’s two electronics equipment buildings nearby with industrial sized a/c exhaust vents. While not USHCN, NCDC metadata lists O’Hare as a Class “A” station, which means it does in fact record climate. Data from O’Hare can be used to adjust other stations with missing nearby data.
The point I’m making with all the photos is that airports are far from static, especially since airline deregulation in the 1980’s. The are just as dynamic as the cities they serve. We measure climate at a great many airports worldwide. E.M. Smith reports that the majority of the GHCN record is from airports.
Even NOAA meteorologists admit that airports aren’t necessarily the best place to measure climate. In a series of stories I did…
How not to measure temperature, part  88 – Honolulu’s Official Temperature ±2
..about the failure of the aviation weather station at Honolulu causing unparalleled record highs, the NOAA Meteorologist there had this to say:
“ASOS…placed for aviation purposes…not necessarily for   climate purposes.” 

The key issue here is “aviation purpose, not climate purposes”. The primary mission is to serve the airport. Climate is a secondary or even tertiary consideration. And that’s exactly what happened in the story from the Baltimore Sun below. The observer used FAA guidelines rather than NOAA guidelines to measure snow for the climate record. NOAA doesn’t like the record because he didn’t follow their procedures, so they toss it out.
However, when a new high temp record is set in Honolulu due to faulty equipment, NOAA thinks THAT’s alright to keep in the records:
NOAA: FUBAR high  temp/climate records from faulty sensor to remain in place at Honolulu
A nearby station shows the error:
This is your Honolulu  Temperature. This is your Honolulu Temperature on ASOS. Any questions?
So it is with some disgust that I provide an excerpt of this article on NOAA rejecting a record snowfall at the BWI airport, where they set up a snow measuring board, but didn’t follow through on procedure. Again, the airport was doing measurements to serve the airport interests, not NOAA.
=====================================

Sat 20 Feb 2010
By Frank D. Roylance
Shawn Durkin, weather station manager who has worked for Pacific Weather Inc. for 16 years, stands on the rooftop location at BWI where Pacific Weather takes its snow measurements, using a snow board, mounted on the bench to his left, and an 8-inch rain gauge, at right. Baltimore Sun photo by Amy Davis / February 18, 2010



…
A contractor working for the  Federal Aviation Administration at BWI Thurgood Marshall Airport, paid  to measure the snow for the aviation industry’s needs, did not follow a  separate protocol required by the National Weather Service and the  National Climatic Data Center for valid climate data.
So while  the contractor measured 28.8 inches of snow during that storm, the  National Weather Service has thrown out the reading. Instead,  climatologists will rank the storm as “only” 24.8 inches – a number that  almost surely understates the “true” total.
Worse, for  climatologists, it now appears the weather service’s rules for snow data  had been ignored for years at BWI, throwing a cloud over the validity  of snow totals as far back as 1998, when the FAA took the job over from  the weather service.
Only BWI’s data are known to be affected,  but the problem could be more widespread. That possibility has caught  the attention of top officials at the FAA.
“We plan to meet with  the National Weather Service next week to begin a discussion on making  sure that we’re all on the same page in terms of measuring snow  accumulations at our airports,” FAA spokesman Jim Peters said. “There  will be a national discussion.”
In the meantime, the weather  service’s Baltimore- Washington Forecast Office in Sterling, Va., is  preparing to convene a committee of climatologists and other experts to  review Baltimore’s snowfall records from the 2010 and 2003 storms, and  perhaps back to 1998.
“I feel very strongly about historical  records and getting the climate data correct,” said James E. Lee, the  meteorologist-in-charge at Sterling. “Obviously, with the increased  media attention and political attention to climate, it is really up to  NOAA [the National Oceanic and Atmospheric Administration, of which the  National Weather Service is an agency] to make sure … the climate  record is a genuine one, and consistent to the best of our ability.”
The  problem at BWI came to light Feb. 6, as snow accumulations reported at  the airport passed 26 inches. They seemed poised to break the record set  in February 2003 – the storm listed on Sterling’s Web site as  Baltimore’s biggest.
But when reporters called asking about a new  record, Lee said that because of measurement errors by an FAA  contractor at BWI, the two-day storm total would be pegged at “only”  24.8 inches. He had discarded a 28.8-inch measurement from BWI because  it was the sum of hourly measurements throughout the storm – a method  invalid for climatological data.
Even at 24.8 inches, Lee said,  the storm total beat the previous two-day record of 24.4 inches, set at  BWI during two days of the four-day 2003 event. “I’m convinced that was  the most amount of snow Baltimore has seen [from a two-day storm] in  recorded history.”
But Lee had to use the most conservative  reading from the airport – a “snow depth” measurement of the total on  the ground when the storm ended, after hours of compaction.
The  FAA requires its observers to take hourly snow measurements and wipe the  boards clean after each hour, adding the totals as they go. That  provides pilots with better real-time information about changing  conditions. But it virtually eliminates compaction and so inflates  accumulation. Climatologists require measurements every six hours,  striking a balance between the hourly and snow depth readings. Some  airports maintain separate snow boards for the different protocols. But  not BWI.
Richard Carlson, vice president of Pacific Weather Inc.,  said his company has experienced weather observers at 20 U.S. airports,  including eight at BWI. Pacific has held the contract there since 2008.
“We  follow the FAA manual … and that is the guide book on how these  meteorological observations are to be taken,” Carlson said. “We had  heard about the six-hour measuring thing, but … if you have high winds  at all, this really is not going to work.”
…
Read the full article at the Baltimore Sun

Read Frank Roylance’s blog  on MarylandWeather.com



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8d774233',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Coronavirus cases in one of the country's worst affected areas have more than halved in two weeks.**
Scarborough's infection rate has fallen to 296 per 100,000, with 17 new Covid-19 cases recorded in the past 24 hours.
On 11 November the town's infection rate stood at 611, the second-highest in England.
Cases across North Yorkshire have also declined with the average for the county now 196 cases, lower than the national average of 207.
According to the Local Democracy Reporting Service, health officials hope the county could avoidÂ the toughest tier of coronavirus restrictions as the national lockdown ends next week.
Richard Webb, the county's corporate director of health and adult services, told a briefing that falling infection rates had given the region some optimism that it could escape the tightest controls set to be announced by the government on Thursday.
However, he warned Covid cases were still 10 times higher than summer, and urged residents toÂ fight downÂ any over-optimism.
Mr Webb said: ""We are still seeing people dying from Covid and behind all the numbers are human stories.""
The county was under Tier 1 restrictionsÂ as it entered the national lockdown which will now end on 2 December.
_Follow BBC Yorkshire on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to_ yorkslincs.news@bbc.co.uk _or_send video here _._"
"

If we want to understand the current advance of global capitalism, it is worth remembering that a liberal international economic order has actually arisen twice, both at the end of the nineteenth century and now, at the end of the twentieth. 1 In many ways, the world economy has simply caught up to where it was 100 years ago, prompting prominent economists to question whether the level of international integration is as high now as it was before the interruptions of two world wars and the Great Depression.



In a recent study, economists Michael Bordo, Barry Eichengreen, and Douglas Irwin ask whether globalization today is really different from globalization a century ago. 2 Answering that question can help us determine whether we are living in unprecedented times, whether the nation‐​state is becoming obsolete, and whether the new liberal international economic order promises to endure. Indeed, as various observers have noted, the mere fact that the first episode of global capitalism met such a cataclysmic end should force us to reflect on the current features of commercial, financial, and labor integration.



One area in which the world is decidedly less liberal than it was under the Pax Britannica is that of immigration. Although technological advances have made travel far more affordable and convenient than in the nineteenth century when restrictions on immigration were minimal or nonexistent, today hardly a country in the world–certainly not a rich country–does not have an array of labor and immigration regulations. As economist Deepak Lal convincingly explains, such restrictions on the movement of people exist today because citizenship concedes rights to the services provided by the welfare state. 3 Yet even as the welfare state has grown, so have migration flows. From 1965 to 1990, the foreign‐​born population rose from 75 million people to 120 million–with flows from poor to rich countries accelerating the most. The number of people emigrating to the United States has grown about ten times since 1945, for example, but as a percentage of the U.S. population immigration still represents only about one‐​third of its peak at the turn of the century.4



By contrast world merchandise trade reached its 1913 level by the 1970s.5 Global exports as a percentage of global output stood at about 12 percent in the early 1970s and has since risen to about 18 percent. U.S. exports as a share of the country’s economy are only slightly higher today at 8 percent than they were in the late nineteenth century. Including trade in services, however, the U.S. export figure rises to about 11 percent. Indeed, the rise of trade in services such as tourism, finance, insurance, and technical assistance has become far more pronounced today than it was previously. Thus taking into account world exports of both goods and services, global exports rise to about 23 percent of world output today. 6



Two other features distinguish commerce in the new liberal international economic order: the rise of the multinational corporation and the change in the composition of trade. Much trade today involves corporations “slicing up the value chain” and engaging in intra‐​industry trade. As a result, manufactured goods are increasingly exported from developing countries to developed countries. That contrasts sharply with the nineteenth century experience when countries on the periphery exported primary goods to rich countries, which exported manufactured goods made primarily in the center countries to the periphery. Bordo and his coauthors note, for example, that 80 percent of U.S. imports from Mexico are manufactured products while 100 years ago that figure was only 10 percent. And although U.S. exports of goods have not increased dramatically as a share of its economy, a much higher percentage of the production of the United States’ tradable goods is now exported. 7



**Financial Integration**



Is the world more financially integrated than it was in the past? A look at net capital flows suggests that the answer is no. The outflow of capital from Great Britain reached 9 percent of its GDP during the Victorian era with similar figures in Germany, France, and the Netherlands. No country today even comes close to those levels of net flows. In the 1990s, the average capital outflow for leading economies was slightly above 2 percent of GDP. 8



Other differences in capital market integration suggest that the world is indeed more globalized than ever. Gross flows of capital, at about $1.5 trillion per day are much larger than at any time in history, and much of that money represents short‐​term investment. Investors can today react at a moment’s notice to economic and political developments around the world in a market that offers a wide and growing range of international financial instruments. Investors, moreover, are almost equally putting their funds into equity instruments as into debt instruments, which predominated 100 years ago. At that time, international finance concentrated on funding certain sectors, principally railroads and government bonds. 9 Today, with more rapid and perhaps more reliable information about investment opportunities, international funds flow into virtually every sector of countries’ economies. In short, although net capital flows are not as large as those of the nineteenth century, gross flows are unprecedented in size, as are the extent and sophistication of capital markets, suggesting that financial integration is greater today than in the first episode of world capitalism.



 **The Role of Technology and Politics**



Has globalization come about because of political or because of technological change? Here again, Bordo and others suggest important differences between the two episodes of world capitalism. During the last century, technological changes clearly led to globalization. By the 1860s the political bases for a liberal international economic order were already in place. Great Britain repealed the Corn Laws and established its presence in China in the 1840s; it conquered India by 1857, and, with France, defeated Russia in the Crimean War by 1856. Contemporaneous and subsequent advances in technology led to a 40 to 50 percent drop in the cost of shipping in the latter part of the nineteenth century and early part of the twentieth. The transatlantic cable was laid in the 1860s; use of railroads and the telegraph proliferated; the Suez Canal was completed in 1869; and the radio telephone linked Europe and North America by 1900. Those and other innovations led to the first rise of world capitalism. 



Globalization today has benefited from technological improvements but has been almost entirely due to dramatic political changes. Countries around the world have lowered their trade barriers and opened their economies since the 1980s and especially so since the fall of the Berlin Wall. (That actually contrasts with the nineteenth century experience as nations were actually raising tariffs incrementally as the century wore on.) New technology such as air transport may have helped propel today’s globalization, but the role of such change in leading to globalization should not be overstated. On the other hand, technology in the information age may make it more difficult for politicians to reverse the course of world capitalism. 



Although globalization is often said to create inequality and economic volatility, historical evidence points, in fact, to economic convergence in living standards among countries that open their economies. Studies have shown tendencies to converge among countries in the Organization of Economic Cooperation and Development, among U.S. states, and among Japanese prefectures. 10 Significantly, economist Jeffrey Williamson found a decreasing gap in living standards among people living in countries participating in the international economy during both periods of global capitalism. “History offers an unambiguous positive correlation between globalization and convergence. When the pre‐​World War I years are examined in detail,” Williamson adds, “the correlation turns out to be causal: globalization played _the_ critical role in contributing to convergence.” 11



 **The Financial System**



The incidence of financial crises in Asia, Russia, and elsewhere in recent years has also often been treated as novel and as a consequence of globalization. Yet economists have examined the causes of the recent economic turmoil and have formed a consensus that the causes included pegged‐​exchange rates, government‐​directed credit, protected financial systems, moral hazard at the national and international level, and the lack of transparency in official accounts. Despite that consensus, the crises are used by critics of globalization to advocate _moving away from_ a more market‐​based system and toward _more_ interventionism. Thus India is cited as having followed more prudent policies than its East Asian neighbors since its more closed system allowed it to avoid succumbing to the regional financial crisis. The price it has paid for stability, of course, has been enduring poverty. By contrast, Hong Kong has had a volatile economic history but has become one of the wealthiest places on earth. Indeed, even after the financial turmoil, East Asian crisis countries are still eight to 15 times richer than India.



Financial crises also occurred in the first era of world capitalism. One common feature between the two eras is that banking and currency crises occurring at the same time tend to be more common in the periphery, or less‐​developed, countries than in the rich countries, where currency but not banking crises are more common. However, under the gold standard of the Victorian age, crises were resolved differently from how they are resolved under the current system of adjustable exchange rates based on fiat money.



One significant difference was that financial rescues 100 years ago were undertaken by the private sector, while today they are official, usually led by the International Monetary Fund. British Foreign Secretary Lord Palmerston summed up the attitude that prevailed for the rest of the century when U.S. states defaulted in the 1840s: “British subjects who buy foreign securities do so at their own risk and must abide the consequences.” 12 Largely as a result of that approach, economic recovery was more rapid in crisis countries than it is today and crisis countries then did not experience wealth losses as large as those experienced by crisis countries today. 13



 **Liberalism from above or Liberalism from below?**



The distinct institutional framework under which liberalization is taking place worldwide—including the prominence of supranational governmental organizations like the IMF, the World Bank, the World Trade Organization, and the United Nations, and the prominence of welfare and regulatory states—causes both enemies and proponents of globalization to attribute the market revolution to the efforts of those institutions, and to recommend that further developments be managed by international world bodies.



Yet the evidence indicates that such international organizations have at best been marginal to the globalization process and at worst have caused disruptions or delays along the way. Decades of World Bank and IMF lending to inward‐​looking regimes, for example, have certainly slowed the move to world capitalism. 14 Yet countries have _unilaterally_ undertaken economic restructuring, trade and capital account liberalization, and other policy reforms as past policies failed. That is even so of countries that have entered into multilateral free‐​trade agreements, like Mexico, which reduced its trade barriers for years before proposing the North American Free Trade Agreement. China, in its bid to join the WTO, is following the same course. Thus while aid agencies are likely to cause more harm than good in the globalization process (lending to Russia, for example), free‐​trade agreements such as the WTO are likely to be helpful. However, they serve more to preserve trade liberalization reforms than to promote them. 15



In short, the world economy has evolved as a result of changes coming from the national level rather than changes directed at the international level—what German liberal Wilhelm Röpke called an international order “from within and beneath” rather than the “false internationalism” that characterizes supranational organizations. The danger of the constructivist approach to achieving a liberal economic world order is that it may lead to discretionary and arbitrary use of power. Razeen Sally of the London School of Economics describes those hazards. 



Neoliberal institutionalists do not portray international policy coordination in the frame of limiting general rules at the international level that proscribe discretionary government action; rather, they think of it as an apparatus of complicated negotiations on particularistic policies intended to achieve specific results. This is the hallmark not of limited government under the Rule of Law, but of unlimited and discretionary government in an international public policy cartel, avoiding both domestic political accountability and market disciplines. In this context, international regimes are manifestations of government failure transplanted to the international level. Intergovernmental cooperation and international agreements, far removed from public scrutiny and the control of national legislatures and judiciaries, supply extra room for arbitrary activity by politicians and bureaucrats. They exacerbate the malaise of Big Government and political markets within nation‐​states. 16



We have already seen some of that dynamic at work. For example, through international forums, rich countries have pressured poor countries to adopt labor and environmental regulations that did not exist in rich countries at a similar stage in their development. Those impositions have come about against the wishes of developing countries and the vast majority of consumers in rich countries. 



Examples of arbitrariness and lack of transparency are amply provided by the IMF. For instance, the Fund does not tolerate the current account deficits of its member countries exceeding 4 or 5 percent of GDP even though large deficits are beneficial in many cases. Indeed, Australia, Canada, and Argentina had current account deficits greater than 10 percent for decades before 1913. The process by which the IMF decides the bailout amounts nations receive is also unclear. Why did the IMF put together a $57 billion rescue package for Korea as opposed to, say, a $30 billion package? We may never know the criteria or the rationales used in that case or many others. 



In the end, globalization may make such international bureaucracies irrelevant. And efforts to promote international liberalism from above may prove futile. In the meanwhile, we can come to some tentative conclusions. The world has seen global capitalism before; what is unprecedented is not globalization per se, but the extent to which the world is more globalized today than it was 100 years ago. That is especially so in terms of trade and finance. Moreover, no matter how much international agencies would like to take credit for the worldwide market revolution, those changes have emerged at the national level and have not been imposed from above. In that sense, the nation‐​state remains quite relevant. But a backlash against global liberalism is in fact more likely to occur if international agencies increasingly manage the world economy to the detriment of what poor countries consider most important, namely, economic growth.



Happily, one of the biggest differences between the two periods of world capitalism—the ideological environment—portends well for the 21st century. At the end of the nineteenth century, the wave of the future was socialism and its variants, which intellectuals considered held great promise for humanity. That belief system helped destroy the first era of globalization. Today, with socialism thoroughly discredited, basic liberal principles are generally accepted. That current climate of opinion does not make continued globalization inevitable, but it removes a major obstacle on the path toward prosperity that the world has recently resumed. 



**Notes**



1. Jeffrey Sachs and Andrew Warner, “Economic Reform and the Process of Global Integration,” Development Discussion Paper no. 552, Harvard Institute of International Development, September 1996, p. 5.



2. Michael D. Bordo, Barry Eichengreen, and Douglas Irwin, “Is Globalization Today Really Different than Globalization a Hundred Years Ago?” in _Brookings Trade Forum 1999_ , ed. Susan M. Collins and Robert Z. Lawrence (Washington: Brookings Institution Press, 1999), pp. 1–72.



3. Deepak Lal, “The Challenge of Globalization: There Is No Third Way,” in _Global Fortune: The Stumble and Rise of World Capitalism_ , ed. Ian Vásquez (Washington: Cato Institute, 2000), p. 29.



4. Julian L. Simon, _The Economic Consequences of Immigration_ (Ann Arbor, Mich.: University of Michigan Press, 1999), p. 28.



5. Paul Krugman, “Growing World Trade: Causes and Consequences,” _Brookings Papers on Economic Activity_ , vol. 1, 1995, p. 331 and International Monetary Fund, _World Economic Outlook_ (Washington: IMF, May 1997), p. 112.



6. _Economic Report of the President_ (Washington: Government Printing Office, 2000), pp. 306–7, 422 and International Monetary Fund, _World Economic Outlook_ (Washington: IMF, May 2000), pp. 203, 232.



7. Bordo et al.



8. _World Economic Outlook_ , May 1997, p. 114.



9. Bordo et al.



10. Sachs and Warner, p. 39 and Kevin H. O’Rourke and Jeffrey G. Williamson, _Globalization And History: The Evolution of a Nineteenth‐​Century Atlantic Economy_ (Cambridge, Mass: MIT Press, 1999).



11. Jeffrey G. Williamson, “Globalization, Convergence, and History,” _Journal of Economic History_ , June 1996, p. 277.



12. Cited in Harold L. Cole, James Dow, and William B. English, “Default, Settlement, and Signalling: Lending Resumption In A Reputational Model of Sovereign Debt,” _International Economic Review_ , May 1995, p. 369.



13. Bordo, et al., and Michael Bordo and Anna J. Schwartz, “Measuring Real Economic Effects of Bailouts: Historical Perspectives on How Countries in Financial Distress Have Fared With and Without Bailouts,” paper presented at the Carnegie Rochester Conference on Public Policy, November 19–20, 1999.



14. See Doug Bandow and Ian Vásquez, eds., _Perpetuating Poverty: The World Bank, the IMF, and the Developing World_ (Washington: Cato Institute, 1994).



15. Brink Lindsey, “Free Trade From the Bottom Up,” _Cato Journal_ 19, no. 3 (Winter 2000): 363. 



16. Razeen Sally, _Classical Liberalism and International Economic Order: Studies in Theory and Intellectual History_ (London: Routledge, 1998), pp. 196–97.
"
"

The national media have given tremendous play to the claims of Vice President Al Gore, some federal scientists, and environmental activists that the unseasonably warm temperatures of this past summer were proof positive of the arrival of dramatic and devastating global warming. In fact, the record temperatures were largely the result of a strong El Niño superimposed on a decade in which temperatures continue to reflect a warming that largely took place in the first half of this century. 



Observed global warming remains far below the amount predicted by computer models that served as the basis for the United Nations Framework Convention on Climate Change. Whatever record is used, the largest portion of the warming of the second half of this century has mainly been confined to winter in the very coldest continental air masses of Siberia and northwestern North America, as predicted by basic greenhouse effect physics. The unpredictability of seasonal and annual temperatures has declined significantly. There has been no change in precipitation variability. In the United States, drought has decreased while flooding has not increased. 



Moreover, carbon dioxide is increasing in the atmosphere at a rate below that of most climate‐​change scenarios because it is being increasingly captured by growing vegetation. The second most important human greenhouse enhancer — methane — is not likely to increase appreciably in the next 100 years. And perhaps most important, the direct warming effect of carbon dioxide was overestimated. Even global warming alarmists in the scientific establishment now say that the Kyoto Protocol will have no discernible impact on global climate. 
"
"
Share this...FacebookTwitterIn the wake of the massive earthquake and tsunami that devastated Japan, a tsunami of hysteria has swept the German Green Party to a stunning victory in elections in the state of Baden-Württemberg.The Greens also piled on in the Rhineland Palatinate elections as well. One thing is clear:  Angela Merkel’s coalition government with the FDP liberal democrats were punished for their horrendous handling of energy policy in Germany.
One-year ago Merkel and her government approved operating lifetime extensions of nuclear power plants in Germany, giving 8 older reactors the seal approval, certifying they were safe and reliable. That, they were. Indeed German reactors and their management are among the best and safest in the world.
But then came Fukushima, followed by the tsunami of panic generated by an irresponsible media, anti-nuclear activists and opportunistic Greens, who fanned the flames of hysteria with vigor.
But rather than standing up and putting her full faith in German nuclear reactor technology and management, Angela Merkel panicked, blinked and turned her back on the industry, ordering the shutdown of the 8 nuclear reactors that were built before 1981. By turning her back on the nuclear industry, she in the end will have turned her back on the country. The days of cheap, competitive and reliable energy are coming to an end in Germany.
Turning her back on the nuclear industry also made her government’s earlier approval of the reactors appear like a farce. Angela Merkel only succeeded in showing where her priorities are – political survival. Her stance with regards to the Libya war was also pure political calculation.
Now it’s time to pay the piper. Angela Merkel’s government is now collapsing faster than Japan did under it’s 9.0 March 11 earthquake. Rarely does anyone witness ineptitude of this magnitude. She deserved to fail. All of this now clears the way for the SPD socialists and environmental Greens to sweep into power in 2013 in the national elections.
This all clears the way for the German renewable energy experiment, as the reds and the greens are eager to go full throttle with the renewable energy madness, which in the end will make Germany vulnerable and dependent on its neighours. Thanks in part to Merkel, the way is clear to embark on something that has never been tried: an experiment to see if it is possible to power an industrialised and developed country without nuclear and carbon-based energy.
Share this...FacebookTwitter "
"Greta Thunberg, the 16-year-old Swedish climate activist, is calling for system change. At a press conference in Brussels, she told the European Commission that in order to fight climate change we need to change our political and economic systems – a message that has been repeated on signs and in chants in the student climate strikes around the world. The school climate strikes, which she started alone in August 2018, have become a social movement with 1,659 strikes planned for March 15 in 105 countries. But what is system change? How do entire systems change? When we see “save the planet” initiatives, they often look like individual decisions that don’t cost much, like switching to a bamboo toothbrush or washing containers before you recycle them. By all means, do these things, but don’t confuse them with system change. Most people don’t know how to change political, economic and social systems. They end up making token gestures instead that may even perpetuate the problem. There’s also the question of how to overcome powerful vested interests that benefit from the current system. But there is research that can help us understand system change. Neo-institutional theory is one approach to understanding how and why people organise collectively. People create meaning, follow rules and reproduce structures – such as classrooms, businesses, offices and community halls – based on assumptions of what is right and proper. Classrooms look similar, not because each time we set one up we rationally decide how to do so, but because we make assumptions about what a classroom is supposed to look like.  Because we are part of these meaning structures, we reproduce existing norms and beliefs and resist change. System change happens when we don’t take our assumptions for granted, which allows more and more people to question the status quo. Thunberg is telling us that our current political and economic systems are no longer fit for purpose. She is pointing out that the emperor has no clothes.  Changing a system takes time. My research on the LGBT movement in Ireland documented efforts and achievements over 40 years. Homosexuality went from being a crime, to being celebrated in a progressive movement. While the referendum on marriage equality took one day in 2015, the efforts of many to change the system took decades. The Three Horizons Framework can help explain the different factors that lead to changing systems. Horizon one is business as usual – the status quo – and the outgoing institution in times of change. Horizon three is the new institution – with newly legitimised structures and beliefs. The space between them is horizon two, which is occupied by people focused on social change – who lead the transition from an old system to the new.  Most people recognise the problems with the present system and want to help society move to something more sustainable. Products like bamboo toothbrushes exist to monetise that concern, but because they’re sold in plastic and shipped around the world, their production and distribution still consumes fossil fuel and does nothing to change the existing economic or political system that is fuelling climate change. A collective challenge to political and economic elites is likely to be more effective in forcing this transition.  When aspects of horizon three appear – glimpses of a more sustainable system – they are usually rejected as illegitimate or too radical. When Rosa Parks sat down at the front of the bus in a move to promote civil rights in America in 1955, she was condemned. Looking back after system change has happened, these people are seen as leaders. The system that needs to be changed to avert climate disaster is capitalism, which is losing its legitimacy largely due to the system’s failure to respond effectively to climate change. Applying all I’ve learned about how systems change, it’s possible to imagine that the current system which sustains business-as-usual capitalism – horizon one in the framework – is occupied by those who continue to produce, sell and consume products and services that rely on fossil fuels. That’s most of us, but horizon one is also maintained by climate deniers and investors in fossil energy, who, despite the scientific evidence, keep chugging along.  A more sustainable system could include policies we might currently consider “extreme”, like universal basic income. This is a guaranteed payment for all people regardless of their wealth which could help break the cycle of production and consumption that pollutes the atmosphere and fills the ocean with discarded plastic. Evidence suggests there is growing support for this, particularly among young people. Extending human rights to non-humans and even to ecosystems is another idea that seems radical today but is gaining traction and could define an alternative system in future. One thing is for sure, we’ll look back in horror one day at how humans treated the natural world, as many already do in the present. If the climate strikers can continue to grow their movement and sustain momentum, their leadership could be an important part of society’s transition to a more sustainable system in horizon three. Capitalism may seem permanent, but research shows that systems inevitably change over time, and are ultimately created and reinforced by us. But in order to change anything, people must question their own role in the system first. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

If you’ve watched any television in your lifetime, chances are you’ve seen more than a few beer ads. In fact, some of the most memorable advertisements in the history of the medium have been produced by beer makers, as they vigorously compete for customer allegiance. It’s just another part of doing business for beer companies, which depend on TV ads to build brand name recognition.



But if you’re a consumer who enjoys other spirits besides beer, you might be wondering why you never hear anything on TV about your favorite brands, or even competing liquor products. The reason you do not is because, for the past 50 years, the spirits industry has lived under a voluntary ban on the placement of liquor ads on TV. But as revenues have declined gradually over the past two increasingly health‐​conscious decades, the industry has rethought the wisdom of the ban and began cautiously testing the regulatory climate by placing ads on some local TV or cable stations. The debate over the wisdom of this reversal has been heating up nationally since NBC announced recently that they would allow liquor commercials to run during late‐​evening programming, making them the first national network to do so.



Not surprisingly, a lot of social do‐​gooders are up in arms over this and are demanding that federal policymakers take action to halt the practice. NBC “is shirking its public interest responsibility as a broadcaster by putting its bottom line ahead of the health and safety of young people,” says George A. Hacker, director of the Alcohol Policies Project at the Center for Science in the Public Interest. And Joseph Califano, director of the National Center on Addiction and Substance Abuse at Columbia University, told _The Wall Street Journal_ last week, “The only solution now is for federal regulation, just as we have federal regulation prohibiting tobacco ads on television.”



From a public policy perspective, the fear seems to be puritanical in character: If people see booze ads on TV, they will drink more. Such post hoc reasoning could be challenged on a number of grounds. Specifically, it is difficult to believe that Americans are a mindless herd of robots who will make a mad dash to their local liquor stores just because they see a few TV ads. In fact, Dr. Morris E. Chafetz, president of the Health Education Foundation and author of _The Tyranny of Experts_ , argues that “the claim that advertising can lead anyone down the bottle‐​strewn garden path not only to drink alcohol but to abuse it, is pure hokum.” In the mid‐​90s, Dr. Chafetz conducted a review of academic research for the _New England Journal of Medicine_ on the question of how advertising affected alcohol use. His conclusion: “I did not find any studies that credibly connect advertising to increases in alcohol use (or abuse) or to young persons taking up drinking. The prevalence of reckless misinterpretation and misapplication of science allows advocacy groups and the media to stretch research findings to suit their preconceived positions.”



So even though academic evidence suggests that exposure to advertising is unlikely to increase consumption, liquor companies are still willing to run ads, perhaps in an attempt to build brand recognition or attract beer and wine consumers. The question is, is there anything wrong with that?



The answer, of course, is all a matter of personal opinion. In a free society, however, people should be at liberty to make such choices without government entering the picture. Adults should be responsible for their decisions in this regard and they should exercise authority over their children until they reach an age when they can be trusted to make such decisions on their own. Employing the old “It’s about the children!” defense to support an ad ban doesn’t make sense for other reasons. As my Cato Institute colleague Doug Bandow noted in a 1997 _Wall Street Journal_ editorial, “almost every good advertised on the airwaves may have some inadvertent adverse effect on the young,” whether it is cars, riding mowers, high‐​fat food, or computers. “But that’s no excuse for banning ads,” concludes Bandow.



Moreover, children can see liquor ads in magazines and newspapers too, so should we ban liquor ads in print? Which raises another important question: Why is it that we continue to tolerate an artificial regulatory distinction between print and electronic media? For decades, policymakers have imposed the equivalent of second‐​class citizenship on electronic media (television, radio) in terms of First Amendment protections. Unlike their print counterparts, which receive substantial free speech protections, electronic media face numerous speech restrictions that would be unthinkable for newspapers or magazines. So the next time you see a newspaper editorializing about the need to ban liquor ads on TV, fire off a letter to the editor and ask them how they’d feel about a federal ban on all those liquor ads that appear in the paper’s pages and provide them with substantial revenues.



Anyway, a federal ban on televised liquor advertising would probably not pass First Amendment muster today. In the important 1996 decision _Liquormart, Inc. v. Rhode Island_ , the Supreme Court struck down a Rhode Island ban on the advertisement of retail liquor prices outside of the place of sale since such a blanket prohibition against truthful speech about a lawful product betrayed the First Amendment. As Thomas A. Hemphill, a fiscal officer for the New Jersey Department of State, noted in _Regulation_ magazine in 1998: “That landmark decision makes it much more difficult for legislators to restrict truthful commercial speech, thus establishing a precedent for more stringent evidentiary requirements underlying future advertising regulations. Therefore any new law that imposes a comprehensive ban on television or radio liquor commercials will probably not survive First Amendment judicial review.” The Court bolstered this line of reasoning in the subsequent 1999 decision _Greater New Orleans Broadcasting Assn., Inc. v. United States_, which declared that the FCC could not ban casino advertising in states where gambling was legal. The Court declared, “the speaker and the audience, not the Government, should be left to assess the value of accurate and nonmisleading information about lawful conduct.” These decisions also suggests that the Court may finally be getting serious about affording commercial speech the same protections granted to political speech, a move that is long overdue.



A final concern about a federal regulatory response to TV ads relates to its potential applicability to the Internet. As television and the Internet increasingly converge and more Americans gain access to broadband connections, it is likely that more and more television programming will be made available over the Net. So any ban on liquor ads on TV would likely have threatening implications for Internet Webcasting in the long run.



In conclusion, there has never been any logic behind the artificial distinction between liquor and other products, such as beer and wine, when it comes to promotional activities. Alcohol is alcohol. Why should the form in which it is delivered change its legal status? And why place advertising restrictions on _lawful_ products at all? If someone was trying to sell crack cocaine or cruise missiles on TV, it might make for a more interesting debate. But alcohol is a legal product that manufacturers have every right to promote. Policymakers need to take a sober look at these realities before they rush headlong into needless and unconstitutional restrictions on liquor advertisements on TV.
"
"
Still Hiding the Decline
by Steve McIntyre
Even in their Nov 24, 2009 statement, the University of East Anglia failed to come clean about the amount of decline that was hidden. The graphic in their statement continued to “hide the decline” in the Briffa reconstruction by deleting adverse results in the last part of the 20th century. This is what Gavin Schmidt characterizes as a “good thing to do”.
First here is the Nov 2009 diagram offered up by UEA:

Figure 1. Resized UEA version of Nov 2009, supposedly “showing the decline”. Original here ,
Here’s what UEA appears to have done in the above diagram. 
While they’ve used the actual Briffa reconstruction after 1960 in making their smooth, even now, they deleted values after 1960 so that the full measure of the decline of the Briffa reconstruction is hidden. Deleted values are shown in magenta. Source code is below.

Figure 2. Emulation of UEA Nov 2009, using all the Briffa reconstruction.
 
R SOURCE CODE:
##COMPARE ARCHIVED BRIFFA VERSION TO CLIMATEGATE VERSION
#1. LOAD BRIFFA (CLIMATEGATE VERSION)

 # archive is truncated in 1960: ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/reconstructions/n_hem_temp/briffa2001jgr3.txt”
loc=”http://www.eastangliaemails.com/emails.php?eid=146&filename=939154709.txt”

 working=readLines(loc,n=1994-1401+104)

 working=working[105:length(working)]

 x=substr(working,1,14)

 writeLines(x,”temp.dat”)

 gate=read.table(“temp.dat”)

 gate=ts(gate[,2],start=gate[1,1])
#2. J98 has reference 1961-1990

 #note that there is another version at  ftp://ftp.ncdc.noaa.gov/pub/data/paleo/contributions_by_author/jones1998/jonesdata.txt”
loc=”ftp://ftp.ncdc.noaa.gov/pub/data/paleo/contributions_by_author/jones2001/jones2001_fig2.txt”

 test=read.table(loc,skip=17,header=TRUE,fill=TRUE,colClasses=”numeric”,nrow=1001)

 test[test== -9.999]=NA

 count= apply(!is.na(test),1,sum)

 test=ts(test,start=1000,end=2000)

 J2001=test[,""Jones""]
#3. MBH :  reference 1902-1980

 url<-""ftp://ftp.ncdc.noaa.gov/pub/data/paleo/contributions_by_author/mann1999/recons/nhem-recon.dat""

 MBH99<-read.table(url) ;#this goes to 1980

 MBH99<-ts(MBH99[,2],start=MBH99[1,1])
#4. CRU instrumental: 1961-1990 reference

 # use old version to 1997 in Briffa archive extended

 url<-""ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/reconstructions/n_hem_temp/briffa2001jgr3.txt""

 #readLines(url)[1:50]

 Briffa<-read.table(url,skip=24,fill=TRUE)

 Briffa[Briffa< -900]=NA

 dimnames(Briffa)[[2]]<-c(""year"",""Jones98"",""MBH99"",""Briffa01"",""Briffa00"",""Overpeck97"",""Crowley00"",""CRU99"")

 Briffa= ts(Briffa,start=1000)

 CRU=window(Briffa[,""CRU""],start=1850)

 tsp(CRU) #  1850 1999  #but starts 1871 and ends 1997

 delta<-mean(CRU[(1902:1980)-1850])-mean(CRU[(1960:1990)-1850]);

 delta  #   -0.118922

 #used to get MBH values with 1961-1990 reference: compare to 0.12 mentioned in Climategate letters
#get updated version of CRU to update 1998 and 1999 values

 loc=""http://hadobs.metoffice.com/crutem3/diagnostics/hemispheric/northern/annual""

 D=read.table(loc) #dim(D) #158 12 #start 1850

 names(D)=c(""year"",""anom"",""u_sample"",""l_sample"",""u_coverage"",""l_coverage"",""u_bias"",""l_bias"",""u_sample_cover"",""l_sample_cover"",

 ""u_total"",""l_total"")

 cru=ts(D[,2],start=1850)

 tsp(cru) #  1850 2009
#  update 1998-1999 values with 1998 values

 CRU[(1998:1999)-1849]= rep(cru[(1998)-1849],2)
#Fig 2.21 Caption

 #The horizontal zero line denotes the 1961 to 1990 reference

 #period mean temperature. All series were smoothed with a 40-year Hamming-weights lowpass filter, with boundary constraints

 # imposed by padding the series with its mean values during the first and last 25 years.

 #this is a low-pass filter

 source(""http://www.climateaudit.org/scripts/utilities.txt"") #get filter.combine.pad function

 hamming.filter<-function(N) {

 i<-0:(N-1)

 w<-cos(2*pi*i/(N-1))

 hamming.filter<-0.54 – 0.46 *w

 hamming.filter<-hamming.filter/sum(hamming.filter)

 hamming.filter

 }

 f=function(x) filter.combine.pad(x,a=hamming.filter(40),M=25)[,2]
## WMO Figure at CRU

 #http://www.uea.ac.uk/mac/comm/media/press/2009/nov/homepagenews/CRUupdate

 #WMO: http://www.uea.ac.uk/polopoly_fs/1.138392!imageManager/1009061939.jpg

 #2009: http://www.uea.ac.uk/polopoly_fs/1.138393!imageManager/4052145227.jpg
X=ts.union(MBH=MBH99+delta,J2001,briffa=briffa[,""gate""],CRU=cru )  #collate

 Y=data.frame(X); year=c(time(X))

 sapply(Y, function(x) range(year [!is.na(x)]) )

 #      MBH J2001 briffa  CRU

 # [1,] 1000  1000   1402 1850

 # [2,] 1980  1991   1994 2009
smoothb= ts(apply(Y,2,f),start=1000)
xlim0=c(1000,2000) #xlim0=c(1900,2000)

 ylim0=c(-.6,.35)

 par(mar=c(2.5,4,2,1))

 col.ipcc=c(""blue"",""red"",""green4"",""black"")
par(bg=""beige"")

 plot( c(time(smoothb)),smoothb[,1],col=col.ipcc,lwd=2,bg=""beige"",xlim=xlim0,xaxs=""i"",ylim=ylim0,yaxs=""i"",type=""n"",axes=FALSE,xlab="""",ylab=""deg C (1961-1990)"")

 usr 1960

 points( c(time(smoothb))[temp],smoothb[temp,""briffa""],pch=19,cex=.7,col=”magenta”)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9075ad31',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The London Times is reporting:
“The chairman of the leading climate change watchdog was informed that claims  about melting Himalayan glaciers were false before the Copenhagen summit, The  Times has learnt.
Rajendra Pachauri was told that the Intergovernmental Panel on Climate Change  assessment that the glaciers would disappear by 2035 was wrong, but he  waited two months to correct it. He failed to act despite learning that the  claim had been refuted by several leading glaciologists.”
See the Times article here
And from Richard North at The EU Referendum, this video news report link and his commentary:

Less than a week after he claimed the IPCC’s credibility had increased as a result of its handling of the “Glaciergate” scandal, Pachauri’s own personal credibility lies in tatters as The Times accuses him of a direct lie.
This is about when he first became aware of the false claim over the melting glaciers, Pachauri’s version on 22 January being that he had only known about it “for a few days” – i.e., after it had appeared in The Sunday Times.  
However, Ben Webster writes that a prominent science journalist, Pallava Bagla – who works for the Science journal (and NDTV as its science correspondent) – claims that last November he had informed Pachauri that Graham Cogley, a professor at Ontario Trent University and a leading glaciologist, had dismissed the 2035 date as being wrong by at least 300 years. Pachauri had replied: “I don’t have anything to add on glaciers.”
Bagla interviewed Dr Pachauri again this week and asked him why he had decided to overlook the error before the Copenhagen summit. In the taped interview, he asked: “I pointed it out [the error] to you in several e-mails, several discussions, yet you decided to overlook it. Was that so that you did not want to destabilise what was happening in Copenhagen?”
Dr Pachauri replied: “Not at all, not at all. As it happens, we were all terribly preoccupied with a lot of events. We were working round the clock with several things that had to be done in Copenhagen. It was only when the story broke, I think in December, we decided to, well, early this month — as a matter of fact, I can give you the exact dates — early in January that we decided to go into it and we moved very fast.”
According to Pachauri, “… within three or four days, we were able to come up with a clear and a very honest and objective assessment of what had happened. So I think this presumption on your part or on the part of any others is totally wrong. We are certainly never — and I can say this categorically — ever going to do anything other than what is truthful and what upholds the veracity of science.”
Without even Bagla’s input, we know this to be lies.  Apart from anything else, there was the crisis meeting under the aegis of UNEP – which we reported on Thursday – which concluded that the 2035 claim “does not appear to be based upon any scientific studies and therefore has no foundation”.
Read his complete essay here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8edca52b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The discovery of a 4,000-year-old wine cellar in Israel has provided the best direct evidence yet of the raucous, boozy celebrations that were a key part of the region’s culture at the time.  The cellar was found during a recent excavation at Tel Kabri, Israel, described in a new paper in the journal PLOS ONE. In the remains of a palatial storage complex, archaeologists uncovered ceramic jars and fragments of other vessels dating from the Middle Bronze Age, a period which ran from roughly 1900 BC to 1600 BC. Analysis of ceramic samples showed the presence of tartaric and syringic acids, which are standard wine identifiers in archaeological contexts. Tests also revealed various combinations of herbal additives such as honey, juniper, mint, myrtle, and cinnamon. Wine was an essential part of Bronze Age life in Canaan (as well as the larger Levantine area) – the region roughly corresponding to modern day Israel-Palestine. This is partly due to the fertile soil and warm climate, which made the area ideal for vines to thrive. The abundance of grapes and therefore wine explains its significance and presence in a variety of events. The test results match with what we know from historical sources such as a list of wine types and additives from Mari, Northern Mesopotamia, in 18th century BC, or the Ebers Papyrus from Egpyt. Dating from 1550 BC, the Ebers Papyrus was a predominantly medical text, and included such prescriptions as a remedy involving powdered dung mixed with wine and administered orally. The vessels were found in a storeroom off the main hall at Tel Kabri, and could have held 2,000 litres of wine. Though this seems like a sizeable stockpile, it could have been consumed fairly quickly and must have been replenished every season. Wine is known to have a longer shelf life than other alcoholic beverages (such as beer) in the Bronze Age, but even a large cellar such as this wouldn’t have lasted forever. So how could one court get through around 3,000 modern day wine bottles in one year? Large feasts involving the ritual consumption of alcohol are one reason. In the Levant, such celebrations were known as marzeah, involving an elite cadre of well-heeled people, generally men, who threw parties characterised by heavy wine drinking. Marzeahs could be held to venerate a god (though they were not directly religious), remember a deceased ancestor, to celebrate hunting, warfare, or a certain rite of passage.  The alcohol consumption was key, especially wine. The reasons for such excess are unclear: perhaps to commune with the gods, to demonstrate their own form of divine qualities, or simply social drunkenness among men in a scene reminiscent of an ancient fraternity party. The overall concept of the marzeah was likely one that affirmed social status and hierarchy. The celebrations included young, nubile girls; at one point even two unmarried daughters of the god El attended a marzeah (they were apparently drunk and badly behaved, but the surviving texts make no mention of sexual activity). Overall the evidence on women’s role is mixed.  Excluded wives had their own marzeahs, or similar events, but some sources note that men and women did drink together, both under watchful eyes and at weddings. Wine was also used in religious contexts, where it was often used as an offering to the gods, or in wakes where mourners played music and drank wine. In festivals, it used in sacrificial rituals or mythological battle re-enactments, where wine often took the place of blood.  Wine served as thanksgiving and a symbol of the payment system, which was essential in the economic system.  It was so commonplace in the Levant that it was included in lodging rations, and possibly used to pay tithes, as one would with grain. Even the army were sometimes paid in wine. But it is the big parties that are most relevant to this newly discovered cellar. Ultimately what was going on here was conspicuous over-consumption of an essentially unproductive/luxury commodity – wine. Thorstein Veblen, an early analyst of such indulgence among the upper classes, once described this as lending a “means of reputability to the gentleman of leisure”. Veblen was writing in 1899 about capitalist society, but his point was such consumption at the top was a hangover from feudalism and before.  The finding at Tel Kabri is a case in point – who needs 3,000 litres of wine? By using wine in this way, the elite of the Bronze Age were signifying and justifying their power."
"**More than one-third of jobs in arts, culture and heritage are vulnerable as a result of coronavirus restrictions.**
That is according to a new study from Ulster University's Economic Policy Centre (UUEPC).
The report suggests a high proportion of jobs in museums, galleries, theatres and music are particularly at risk.
In general, the creative sector is ""more exposed to the challenges arising from Covid-19 than other sectors and occupations.""
The main factors for that are social distancing measures limiting capacity and a reluctance among audiences to return even when venues can re-open.
The UUEPC report estimated that there are 39,100 people employed in the arts, creative, culture and heritage sector in Northern Ireland.
However, the type of occupation that figure includes is very wide.
It ranges from people working in Information Technology (IT) and architecture to those employed in music, crafts, the performing arts, museums, galleries and libraries.
The study said that while a high proportion of jobs in areas such as IT were not vulnerable, jobs in the other sectors were ""much more at risk.""
It estimated that more than 60% of jobs in museums, galleries and libraries were vulnerable, along with almost half of jobs in music, theatre and visual art.
A significant number of jobs in film and TV production were also at risk.
""The pandemic has caused the immediate closure of non-essential business including the Arts, Culture and Heritage industries resulting in cancelled work and events such as large music events like Belfast Vital and Belsonic which attracted thousands of people to Belfast annually,"" the report said.
""The healthcare situation in NI will be more important in this sector than in the average NI occupation, given the interactive nature of work and a dependence on discretionary consumer spending.""
The authors of the report also make a number of suggestions on how venues and visitor attractions could be helped to recover from the impact of the pandemic.
They include ""visitor vouchers,"" which would subsidise 30-50% of the cost of tickets to venues to encourage audiences to return.
Venues would also be compensated if they had to cancel events at short notice due to new or changing restrictions.
The authors of the study also suggest a bursary of Â£1,000 a month for arts workers who have not been able to benefit from other job support schemes.
Northern Irish artists could also be commissioned to create new public art, the UUEPC report said.
A number of emergency funding schemes for arts and heritage have been opened by the Department for Communities.
The Northern Ireland executive received Â£33m from Westminster in July as part of a UK-wide support package for arts and culture."
"**People from three households in Northern Ireland will be allowed to meet indoors for five days over the Christmas period, the first and deputy first ministers have said.**
The decision will apply to all four devolved nations.
Three households will be allowed to bubble from 23 to 27 December.
NI is due to begin a two-week lockdown from this Friday until 11 December, in a bid to curb the spread of the virus before Christmas.
The UK government has said anyone travelling to or from Northern Ireland can travel on 22 and 28 December, but ""only meet with their Christmas bubble"" between 23 and 27 December.
The Stormont Executive will meet on Thursday to consider the arrangements for Christmas in more detail.
First Minister Arlene Foster said she hoped the announcement would give people space to plan over the holiday period.
She added that it was difficult to ""balance"" Christmas festivities with managing the spread of the virus.
Deputy First Minister Michelle O'Neill described Tuesday's announcement as a ""message of hope"", but urged people to be responsible, safe and mindful of healthcare workers.
""There is a risk associated with allowing people to come together,"" she said.
Northern Ireland's Chief Medical Officer Dr Michael McBride said he had ""no doubt"" that Northern Ireland would see increased cases after the Christmas period.
However, he said the authorities needed to ""balance"" that risk with recognising that Christmas is ""a very important time of year"" for families who have had a difficult time in 2020.
He added they recognised ""that people will come together irrespective of the advice that we give and it's important therefore that we give advice to the population to ensure that they can come together in as safe a way as possible"".
Dr McBride urged the public to follow the rules, saying that ""what none of us wants, is to be in the very difficult situation - which we can't rule out - of advising the executive that a further period of restrictions is required in the new year.""
Politicians like announcing good news, which might explain why we got our first joint press appearance of Arlene Foster and Michelle O'Neill since tensions between their two parties over the Covid-19 lockdown erupted several weeks ago.
The announcement will come as a relief to many, but the relaxation is not designed to be a blank cheque.
Ministers are relying on public buy-in and compliance with these festive rules.
There are still plenty of questions about how this will work - particularly with an influx of students expected home in the coming weeks.
Some people will likely already have booked their flight or boat to Northern Ireland for Christmas, probably not within the tight 22-28 December travel window being mooted.
Then there's the added complexity of choosing who makes a bubble and the worry of having to cut out some family or friends, at a time which is traditionally all about coming together.
Each Christmas bubble will be allowed to meet at home, at a place of worship or in an outdoor public space.
It will also mean families can travel from one part of the UK to another without any sanctions.
But the four governments have said existing, more restrictive rules on hospitality and meeting in other venues will be maintained over the Christmas season.
The first minister said she recognised many people were looking forward to Christmas ""get-togethers"" as it had been a difficult year due to the pandemic.
""We hope that this clarity from ourselves today will give people that space to do a little bit of planning,"" added Mrs Foster.
The Republic of Ireland is due to set out its plans for socialising and travel over Christmas later this week.
Ms O'Neill said the executive also wanted to look to the Irish government for a ""common approach"" to managing the situation together.
""It's important to be honest - in a pandemic there's so little to be certain about but it's our intention to allow families to have some space over Christmas,"" she added.
Eleven further coronavirus-related deaths were reported in Northern Ireland on Tuesday, bringing the Department of Health's overall death toll to 947.
Six more deaths were reported in the Republic of Ireland, bringing its death toll to 2,028.
When a Christmas bubble is formed it must remain fixed and must not be changed; households within it have to be exclusive.
People can gather in private homes and overnight stays will be permitted.
People sharing a bubble can also meet up in places of worship or in an outdoor public space.
But people will not be allowed to meet with their Christmas bubble in hospitality settings or other entertainment venues.
You can meet people outside your bubbles, but this must be done in line with existing regulations.
At present, six people from two households in Northern Ireland can meet in a private back garden, while no more than 15 people can gather in a public space.
A support bubble counts as one household - so for this Christmas period this bubble can join with two other households.
Those households can be any size.
The executive has already said this will not be a normal Christmas, and recognises that that the arrangements will not work for everyone.
But the first and deputy first ministers said they presented an opportunity to allow people in Northern Ireland to have some more contacts with family and friends over the Christmas season.
The UK government has advised that in the two weeks that follow an individual's last meeting with a Christmas bubble, people should reduce their contact with others as much as possible.
Students planning to return to their family home for Christmas are not counted as people from a different household, according to Dr McBride.
""For students travelling home, that will be their household so they will become a member of that household,"" he said.
""So then they can combine with two other households.""
However, speaking on BBC's News NI's Coronavirus Catch-up, Dr McBride warned the the virus ""doesn't go away for Christmas"".
He urged students who had access to Covid-19 testing at their universities to avail of it and advised everyone travelling home for Christmas, not just students, to limit their contacts in the 10 to 14 days leading up to their journey.
He warned the public that older and extremely vulnerable people are ""at no less risk from this virus in the Christmas period than they were at any other period""."
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
In the past decade, since the release of the flawed 1998 study by Michael Mann, now known as MBH 98, the phrase “hockey stick” has been used to describe a certain shape of a graph. It has also become synonymous with poor data selection and  bad statistical procedure.
Yet again and again we see climate studies pushing this hockey stick shape as a way of saying we are “living in the worst time period of the data”.
Here, without statistics, without bristlecone pines, inverted lake sediments, midge larvae carcasses, larch trees in Yamal, or convoluted never before seen statistical methods, I present a directly measured data set that produces a real “hockey stick” shape.
Graph: It's worse than we thought
The data is directly measured and not a proxy, the plot is real. There’s no data adjustment or statistical manipulation. Care to know what it is?
From the website “Calculated Risk“

Here is the monthly Fannie Mae hockey stick graph …
 
Click on graph for larger image in new window.
Fannie Mae reported today that the rate of serious delinquencies – at least 90 days behind – for conventional loans in its single-family guarantee business increased to 4.45% in August, up from 4.17% in July – and up from 1.57% in August 2008.
“Includes seriously delinquent conventional single-family loans as a percent of the total number of conventional single-family loans. These rates are based on conventional single-family mortgage loans and exclude reverse mortgages and non-Fannie Mae mortgage securities held in our portfolio.”
Just more evidence of the growing delinquency problem, although these stats do include Home Affordable Modification Program (HAMP) loans in trial modifications.
Now that’s a hockey stick to be worried about.
It hardly is a surprise then that when we see that sort of graph of actual data in the American economy, we start to see graphs like this one depicting confidence in climate change as an important issue:

Source: Pew Poll,  story here
(h/t to WUWT reader Michael)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91ce35e9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Chile is the first South American nation to sign, with complete bipartisan support within the country, a Free Trade Agreement with the United States. Now, why do Chilean workers support free trade policies? Because while the nation in the 1970s was initiating its free‐​trade development strategy it was also establishing a pioneering system of personal retirement accounts as the foundation of its Social Security policy. 



The connection between the two is important. All around the world, trade liberalization is cast as a battle between capitalists and workers, between “global elites” and the “common man.” In Chile, however, market‐​invested retirement funds mean that every worker is a capitalist and has a visible stake in an internationally competitive economy. In Chile, to be anti‐​globalization is to be both anti‐​capitalist and anti‐​worker.



A vast majority of Chileans benefits from free trade not just as consumers, but also as owners of the productive assets of the economy through their retirement accounts. Free trade is good for the economy, and what’s good for the economy is good for investors. Thus there is a virtuous cycle of trade liberalization that has so far thrived regardless of the political party in power.



Chile already has a 6 percent flat tariff rate that is low compared to the rates of most countries and, more importantly, it is applied equally to all imports. The flat tariff decision of the 1970s was critical. A differentiated tariff not only creates economic distortions that slow economic growth, but it continually generates special interest pressures and opportunities for corruption. With a flat rate, a politician can’t be bought on trade issues… because he has nothing to sell.



Before this latest deal, Chile signed an FTA with the European Union and another with Korea. It also has several bilateral free trade agreements with countries like Canada and Mexico. Complete free trade is at hand. The mere possibility of zero import tariffs is stunning in an economy that in the 1960s was one of the most protectionist in the world. In those days, Chile was a devoted follower of the misguided import‐​substitution proposals of the Santiago‐​based United Nations Economic Commission for Latin America. 



But in the mid‐​1970s the country’s trade policy changed radically. Not only did Chile completely dismantle the system of quotas and other trade barriers, but also under the so‐​called Chicago boys’ liberal economic policies, the low flat tariff policy was adopted. The end result of all these reforms: more than a decade of economic growth at an “Asian tiger” level of 7 percent a year that doubled the size of the Chilean economy and led, for the first time in history, to the highest income per person in the whole region.



Trade liberalization does not take place in a vacuum; the proper overall economic and cultural climate is essential. Social Security choice as implemented in Chile, between the government‐​run pay‐​as‐​you‐​go system and one of personal retirement accounts has solved the retirement crisis and delivered enormous benefits to workers. It also has made trade and economic liberalization more possible by linking the interests of workers to that of the overall economy.



U.S. Trade Ambassador Robert Zoellick, a real world hero of trade liberalization, courageously stated that “one of the nice things in this agreement is we have some additional access in terms of pension fund management with a Social Security system that I wish we could imitate.” 



I hope this FTA is only “the end of the beginning.” There are innumerable initiatives that could spring from greater trade integration. By a kind of intellectual osmosis, we Chileans can integrate into our own reality the basic economic and political concepts of a country “conceived in liberty” by its incomparable Founding Fathers‐​just as North Americans may benefit from learning about our culture and way of life, a process that, with 37 million people of Hispanic origin in the United States, is well under way.



My dream is an “American Community” of independent nations, cherishing their own cultural identities but joined together in a common market for trade and investment, and with free movement of people and ideas. An American Community would comprise 830 million people and a gross domestic product of $13 trillion.



I salute and join Walt Whitman who once said: “The spirit of the tariff is malevolent. It flies in the face of all American ideals. I hate it root and branch. It helps a few rich men to get rich, it helps the great mass of poor men to get poorer. I am for free trade because I am for anything that will break down the barriers between peoples. I want to see the countries all wide open.” 
"
nan
"A fresh legal challenge to HS2 has been launched by the naturalist and broadcaster Chris Packham, arguing that the UK government’s decision to approve the high-speed rail network failed to take account of its carbon emissions and climate impact. Packham and the law firm Leigh Day said the Oakervee review, whose advice to proceed with HS2 in full was followed by Boris Johnson last month, was “compromised, incomplete and flawed”.  The crowdfunded legal challenge comes in the wake of a court of appeal ruling on Heathrow, which declared that the government’s planning statement allowing a third runway at the London airport was unlawful for not referencing the Paris climate agreement. Packham will likewise argue that the Oakervee report failed to quantify and address the full impact of HS2’s likely carbon emissions. The initial environmental statement for the high-speed rail network was made in 2013, before the government signed the Paris agreement and committed to achieving net zero carbon emissions by 2050. Packham said: “Every important policy decision should now have the future of our environment at the forefront of its considerations. But in regard to the HS2 rail project I believe our government has failed. I believe that essential submissions regarding environmental concerns were ignored by the review panel. “As a consequence, the Oakervee review is compromised, incomplete and flawed and thus the decision to proceed based upon it is unlawful.” Tom Short, a solicitor at the law firm Leigh Day, said the “environmental impacts relevant to the decision whether to proceed have not been properly assessed”. He added: “In a time of unprecedented ecological catastrophe, [Packham] is clear that the law, and moral logic, require the government to think again.” The Department for Transport said it was considering the claim and would respond in due course. A DfT spokesperson added: “We understand campaigners’ concerns, and have tasked HS2 Ltd to deliver one of the UK’s most environmentally responsible infrastructure projects. When finished, HS2 will play a key part in our efforts to tackle climate change, reducing carbon emissions by providing an alternative to domestic flights and cutting congestion on our roads.” HS2 Ltd will build a new high-speed rail line linking London with Birmingham, and later Manchester and Leeds. The Oakervee review, commissioned by Johnson after becoming prime minister, said the costs of the project could rise to more than £106bn. Campaigners say the line will damage or destroy almost 700 wildlife sites, including more than 100 ancient woodlands. HS2 disputes the figure and says only 62 ancient woodlands will be affected, and most of those would remain intact."
"
Share this...FacebookTwitterWhat follows in English is a summary version of a piece appearing here at the European Institute for Climate and Energy (EIKE) based in Germany, written by retired meteorologist Klaus-Eckart Puls.
=================================
PIK report: “Sea level rising fastest in 2000 years” turns out to be a quack! Data shows no change!
Sea levels are now rising faster than at any time in the last 2000 years claims a new hockey stick manufactured by Michael Mann and Stefan Rahmstorf. But that claim has already turned out to be bogus.
As nobody cares much about so-called climate change anymore, the Potsdam Institute For Climate Impact Research (PIK) had to come up with another scare story: rapidly rising sea levels. That claim is supported by a whopping 2 (cherry-picked) North Carolina coastal sediment cores, which the authors claim reflect sea level behavior for the entire globe.  Other scientists have already poured cold water on the paper, like Jens Schröter of the Alfred Wegener Institute, who says Mann’s and Rahmstorf’s paper is “unsuitable for making predictions”.
The opposite is the reality
The new predictions of catastrophe are not based on actual MEASUREMENTS. Actual measurements made by coastal tide gauges and satellites show the opposite is likely happening, i.e. sea level rise is actually decelerating. Presented are 7 datasets that contradict the latest Mannian hockey-stick fantasy.
(1) The US-Coastal Journal reports that sea level rise rate is clearly slowing down – based on tide gauge measurements, full publication here:

It is essential that investigations continue to address why this worldwide-temperature increase had not produced acceleration of global sea level over the past 100 years, and indeed why sea level has possibly decelerated for the last 80 years.”
(2) EUMETSAT recently made public the GLOBAL sea level data/measurements. Result: No trace of an acceleration! See the following graphic:

3) The GFZ Potsdam reached the same result, showing that there is no global uniform trend. Moreover, many locations show a huge sea level drop!

(4) Norderney and Cuxhaven German coastal locations have records going back over 100 years, and so does the NLWKN Lower Saxony State Association and the state of Lower Saxony for the North Sea coast. These MEASUREMENTS too show no acceleration in sea level increase (text translated below):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The text in English:
NLWKN (Annual Report 2005)
“All the discussions and horror scenarios for nothing:
There is no scientific basis for a massive increase in sea level by 2100. The NLWKN has an objective witness for saying this: the tide gauge of Nordeney. It provides a consistent recording of the water level for 100 years. From this data series you can read it: The increase for the time period from 1906 to 2005 is exactly 24.3 cm.
The state government of Lower Saxony:
“Climate change is not detectable:
The state government sees no signs of an increasing sea level at the North Sea coast as a result of climate change. Also the trend of more frequent storm surges is not detectable, says Minister of the Environment Hans-Heinrich Sander (FDP) in the state parliament. The trend remains unchanged at 25 cm per century. A more rapid increase is not observed.”
It needs to be pointed out that since the last Ice Age, North Germany’s coastal land is sinking, while Scandinavia’s is rising.
5) Works from the Institute for Historical Coastal Research Wilhelmshaven also show completely different results from those of PIK and Mann:

The above chart shows that for the period of 1600 to 2000 sea level rose on average 35 cm/century. But from 1900 – 2000, it rose only 25 cm. That is a clear deceleration. Afterwards: The sea level in the recent years has risen even more slowly than in the 20th century! …there is no trace of any acceleration whatsoever.
(6) A NASA team of authors recently published a report that clearly highlighted two points:
(a) No acceleration in sea level exists,
(b) The sea temperature shows a declining trend,
consequently there exists no thermally accelerated sea level increase:
Blue line: global sea level as to AVISO,
Red line: Sea surface temperature since 2004 ARGO (3000 bouys).
(7) Even the IPCC came to the same conclusion that there is no acceleration. With each report the projected sea level for the year 2100 was revised downwards every time – now it is projected to be only 40 cm:
IPCC prognoses: Step-by-step returning to reality.
Finally, when one takes into account that the PIK and the Mann author-team are known worldwide as alarmists, e.g. Michael Mann (hockey stick inventor), Stefan Rahmstorf (PIK) et al., then considerable doubt on the credibility of this doomsday paper is in complete order.
A recent critique of the sea level alarmism can be found here.
Klaus-Eckart Puls – EIKE
(Translation/editing by P. Gosselin)
=======================================================
Share this...FacebookTwitter "
"**The world's largest maker of latex gloves will shut more than half of its factories after almost 2,500 employees tested positive for coronavirus.**
Malaysia's Top Glove will close down 28 plants in phases as it seeks to control the outbreak, authorities said.
The company has seen a huge surge in demand for its personal protective gear since the start of the pandemic.
However, there have been concerns about the working conditions of the low-paid migrant workers it relies upon.
On Monday, Malaysia's health ministry reported a sharp rise in Covid-19 cases in areas where Top Glove factories and dormitories are located.
Nearly 5,800 workers have been screened so far with 2,453 testing positive, it said.
Top Glove operates 41 factories in Malaysia, with many of its workers coming from Nepal and living in crowded dormitory complexes.
""All those who tested positive have been hospitalised and their close contacts have been quarantined to avoid infecting other workers,"" Director-General of Health Noor Hisham Abdullah told Reuters news agency.
It is unclear when the factory closures will begin but they are scheduled to take place in stages.
Top Glove has been in the global spotlight for its record high profits this year, but also over allegations of exploitative labour practices at the firm.
In July, the United States banned the import of gloves from two of the company's subsidiaries following forced labour concerns.
A recent report from the US Department of Labour raised the same issue, pointing to the high recruitment fees overseas migrant workers must pay to secure employment in the rubber glove industry which often results in debt bondage.
In September migrant workers told the Los Angeles Times about difficult working conditions at Top Glove factories, describing 72-hour work weeks, cramped living conditions and low wages.
A few weeks later, Top Glove said it had raised remediation payments to compensate workers for recruitment fees after recommendations from an independent consultant.
Glorene Das, executive director of Tenaganita, a Kuala Lumpur-based NGO that focuses on labour rights, said some Malaysian firms that depend on a migrant workforce were ""failing to meet the basic needs of their workers"".
""These workers are vulnerable because they live and work in congested shared quarters and do work that does not make it possible to practice strict social distancing,"" she told the BBC.
""During these times employers have a huge responsibility towards them but we are hearing of cases where they are not providing workers with sufficient food or even withholding their wages,"" she added.
Shares in Top Glove fell by 7.5% on Tuesday after the factory closures were announced. But despite the slump the company's shares have surged over four fold this year, reported Reuters."
"

Whoever is elected president, global warming legislation is going to be passed in Washington next year. 



Legislation proposed by both John McCain and Barack Obama will require that the cost of energy to become so high that people will avoid using it. The serious question is: why would we do this in the current economic environment? Why would we take away capital that people would otherwise use to invest in companies that produce efficient things when that capital is already being destroyed at an alarming rate?



Other nations that embraced the abject environmental failure known as the Kyoto Protocol and imposed higher energy costs are fleeing from climate change policies as their economies implode. Only the U.S. seems eager to commit economic suicide over global warming. 



Kyoto did nothing measureable about climate change. Global carbon dioxide emissions rose by the same amount they were supposed to fall because of it. All it cost was money. Germany ‘s Chancellor Angela Merkel, who is probably the woman most responsible for the Protocol itself, now calls drastic cuts in carbon dioxide emissions, “ill‐​advised climate policy”. Her foreign minister, Frank‐​Walter Steinmeier, who last year trotted the globe pronouncing global warming a grave threat to world peace, now says that “this crisis changes priorities” and that “interest in protecting the climate will change because of such a crisis”.



A trip around the world (or around the country, or, for that matter, around your city) will demonstrate that economic vitality and environmental protection are highly correlated. The ritzy part of town is neat as a pin, where residents smugly buy (unverifiable) “carbon offsets” to assuage guilt about the four‐​wheel‐​drive behemoth, while bathed in compact‐​fluorescent light. In the poor neighborhoods of the world? Well, they’re cooking indoors with wood or dung, they don’t have a clue what a “carbon offset” or a compact‐​fluorescent is, and the power is out.



As economies suffer increasingly from global warming taxes and regulation, nations can descend from first‐​world energy infrastructure and supply to banana‐​republic like conditions, even without the current economic contraction. 



The first place where this hell is likely to freeze over is going to be in Great Britain this winter. Residential energy costs average $600 per year over where they were a year ago. Britain’s National Housing Administration estimates that 5.7 million British households will spend more than 10% of their income on fuel and energy next year. 



Right now, wholesale power prices in Britain are four times what they are in France. Older coal and nuclear power plants have to be taken out of production for repair and refit. How do energy‐​intensive industries, such as cement, steel, and brickmaking compete in such an environment? They don’t. 



Green policies are sure to make this much, much worse. In large part because of European Union environmental directives, a full 37% of the U.K.‘s electrical generation capacity will be lost by 2015, most of that from mandatory reduction of coal‐​fired plants. Imagine what percent of households will be spending 10% or more of their income on energy 2015. 



Nor will the shortfall to be taken up by solar energy and windmills. Britain is a pretty cloudy place, it isn’t all that big, and windmills produce no power when there is no wind. Last month, Cambridge Econometrics projected that less than 5% of Britain’s total energy will come from these so‐​called “renewables” in 2020.



Before the current financial uncertainty, European governments and the EU environmental bureaucracy thought they could get away with all of this expensive unreality. But, as Angela Merkel and her Foreign Minister now admit, this is beginning to seem “ill advised.”



All of this flags a much larger problem. The only way to reduce emissions enough to have a significant effect on our modest warming trend is to make energy so expensive that people can’t afford it. But, as the current economic situation shows, when people can’t afford it, these policies become “ill advised”. Among other reasons, they are not advisable because they take away capital that is necessary for environmental protection. 



The solution is obvious. Only when technologies are available that produce lower carbon dioxide emissions at a competitive price, will people and politicians really buy in. This requires investment—by individuals—of real money that is currently being confiscated and tilted at windmills. Expensive energy and a financial contraction can only delay this investment, perhaps forever. The United States and the United Kingdom would do well to pay attention to Germany’s newly‐​found realism about global warming policy. 
"
"**""At the moment we're not able to budget for Christmas or anything because we had to buy more all year round.""**
Kirsty Clayton, 24, from Milford Haven, is one of the 112,700 people in Wales out of work and on Universal Credit - almost double the 57,400 last October.
Like other claimants, mother-of-three Ms Clayton has received a Â£20-a-week top up during the pandemic.
But that is due to end in March and there are calls for it to be extended in the chancellor's Spending Review.
Neither Ms Clayton nor her partner are currently employed, and they are struggling to bring up three children aged two, one and three months.
She said she has had to buy three sizes of nappies and because she has had to buy more from local shops since the start of the pandemic, they are Â£4 more expensive per pack than supermarkets' own-brand versions.
Ms Clayton has been helped with parcels from Patch, a Pembrokeshire charity which donates food, gifts and toys at Christmas, and said she would ""struggle quite a bit"" if the extra Â£20 a week comes to an end as planned in March.
She explained: ""It's taken its toll on me. I've been depressed as well - post-natal depression - it's been really hard.""
There are calls, including from within his own party, for Chancellor Rishi Sunak to extend the payment beyond March when he announces his annual Spending Review later.
Like other charities in Wales, Patch has experienced a large increase in the numbers of people it is helping.
The charity, which gives food parcels with five days' worth of food to those in need, has handed out 70% more meals this year than it did last year.
Between January and the end of September, it provided 78,435 meals compared with 46,080 meals during the same period in 2019.
""Even though I have been doing this for more than 20 years, I am always shocked by the different needs, and this year, oh my goodness, I have never seen anything like it,"" said Patch co-ordinator Tracy Olin.
""The stories are completely different from anything I have seen before.""
Ms Olin spoke about a self-employed couple whose incomes had both stopped because of Covid and had been forced to use the foodbank.
""People have got used to that Â£20 and taking it away would be huge,"" said Ms Olin.
""These are the most vulnerable people. If I had my wages cut by Â£20 a week I would really notice a difference, but these people have so little income already, Â£20 a week is going to be devastating.""
Ms Clayton's Conservative MP, former Welsh Secretary Stephen Crabb, said the payment had been particularly important in Wales because wages are lower than the rest of the UK.
""Increasing the allowance by Â£20 has been so important to so many families, not just the unemployed who've lost their jobs but people who are working irregular hours,"" he said.
""I think the Treasury can afford to keep this uplift in payments - it will cost around Â£7bn next year.
""I think the idea that we would now reverse the Â£20 increase to Universal Credit next March, at a time when many families would be facing an increase in unemployment, for me that is just out of the question."""
"
Share this...FacebookTwitterIt was supposed to be USA’s Fukushima – an environmental disaster of Biblical dimensions – one that would serve as a watershed in USA’s energy policy development. The nation would now finally start to wean itself off oil and switch to renewables in earnest. When the BP Deepwater Horizon drilling platform exploded, killing 11 workers, and began releasing million of gallons of crude oil into the Gulf of Mexico, outrage spread across the USA and soon worldwide.
Thanks to brave men (and nature) there is little trace of the Deepwater Horizon tragedy today. Thanks to the workers who risked life and limb in doing their jobs honourably. They will not be forgotten. (Photo credit: US Coast Guard)
Like Angela Merkel declaring a moratorium on nuclear energy in Germany after Fukushima, President Barack Obama also declared a moratorium on offshore oil drilling. Indeed it looked as if Deepwater Horizon was the environmental disaster that environmentalists had been waiting for. Environmental horror stories spread like wildfire. Calls to get away from oil grew shrill.
But fortunately for the environment (and unfortunately for the environmentalists) the Deepwater Horizon disaster turned out to be all hype. Today, just a year later, it is difficult to find any of the spilled oil. Within just a few months the true scale of the spill became known, read here, for example. When compared to the shear volume of the Gulf of Mexico, the 800 million liters of spilled oil turn out to be comparable to a few drops of oil in an Olympic swimming pool. Indeed concentrations of greasy, yukky sunscreen lotion in a public pool are much higher.
Now scientists have discovered that bacteria (nature) are devouring the crude. It’s almost all gone.
The German online FOCUS magazine has written the epitiaph on the gravestone of the “Deepwater Horizon Disaster”, which actually died at birth. Focus writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Even the optimists among the biologists were surprised at the speed at which microbes in the water gobbled up the oil in the water.
‘We  misjudged the Gulf ecology’s capability and bacterial ability to process hydrocarbons’, emphasized William Reilly, a leader of the investigation commission that analyzed the oil disaster for US President Barack Obama’s government.”
FOCUS reminds us that “oil spills” are also a natural ongoing phenomena.
160 million litres of oil leak out each year from hundreds of natural fissures in the ocean sea floor. It’s residence time according to studies is anywhere between 10 hours and 5 days. The microbes work so effectively that only a fraction of the oil reaches the sea surface. Obviously crude oil-eating bacteria got along amazingly fine with the oil from Deepwater Horizon.”
Today the stuff is gone, eaten by nature. People are bathing at the beach, and life goes on.
Let’s recall that tens of thousands of underwater volcanoes globally spew out all sorts of nasty things into the oceans. One quickly realises that Mother Nature is in fact a long way from being clean and gentle. When you get down to it, Mother Nature is a cocktail of physical, biological, and chemical weapons – she is always trying to kill us. She is not this silly paradise of a picture that Google uses for Earth Day.
Happy animals living in a tropical paradise with no disease or danger. This is what the planet would be like if we switched off all the lights. 
Share this...FacebookTwitter "
nan
nan
"
Share this...FacebookTwitterPeople are discussing the instructions made by CERN Head Rolf-Dieter Heuer not to interpret the results of the CLOUD experiment on cloud formation, read here.
What follows is a translation of the relevant text in the article that appeared in DIE WELT here.
Welt Online: Also the results of the so-called Cloud experiment are being awaited with much excitement, where cloud formation is being researched. These results could indeed be important for understanding global climate change?
Heuer: Indeed this is about better understanding cloud formation. In nature there are many parameters that influence this, among them temperature, air humidity, aerosols and cosmic rays. The Cloud experiment examines the effects of cosmic rays on cloud formation. The rays for studying this come from the accelerator. And inside the experimentation chamber it can be examined under controlled conditions how the formation of droplets depends on radiation and floating particles. The results will be published soon. I’ve requested my colleagues to clearly present the results, but not to interpret them. That would mean immediately entering the highly political arena of climate change discussion. You have to be clear that concerning cosmic rays it is only one parameter of many.”
Personally I agree with Dr. Heuer. CERN is not a climate research institute, and so the interpretation of results (with regards to impact on climate) ought to be left to experts of atmospheric and climate sciences. Last I heard is that CERN is a research institute for particle physics, and not climate. And we’ve all seen what happens when other kinds of physicists start acting like climate experts and start modelling future climates.
If they do not wish to interpret the results, other scientists from the appropriate fields certainly will. And if they don’t, the bloggers will.
Heuer also admits that climate science is highly politicized, and so one can’t blame them for not entering this poisoned arena. After all, telling the truth would mean a cut-off in funding. Finally it doesn’t hurt to remind some us that a vast number of parameters are involved in cloud formation and climate change, and that it doesn’t all get boiled down to a single dominant factor (like CO2).
Thanks for reminding us of that, CERN.
Share this...FacebookTwitter "
"We are living through a period of unprecedented environmental breakdown which is increasingly being referred to as “the Anthropocene”. As the term becomes more and more pervasive, I want to explain why, as a psychologist and a committed environmentalist, I think it is a highly problematic way of framing our predicament.  Originally proposed by atmospheric scientists and then geologists, the Anthropocene has come to the fore as a powerful if perplexing way of talking about our current era. This is a period in which, for the first time in its history, the Earth is being deeply transformed by one species – humans. The word Anthropocene refers to the idea that the Earth’s geological record has been transformed by humanity: Anthropos is Greek for human and -cene is a substantial geological time period within the current 65 million year old Cenozoic era.  It is remarkable how quickly this idea has become ubiquitous. It is now the subject not just of academic texts and conferences, but art, fiction, magazines, travelogues, poetry, even an opera.  While I agree that this is an important and timely provocation, I want to pause here for a moment, and consider whether the Anthropocene narrative really does capture our predicament and our prospects.  There is already plenty of criticism of the Anthropocene idea. Alternative terms like Capitalocene (which attempts to highlight the detrimental forces of capitalism), and Plantationocene (which emphasises the role of colonialism, the plantation system and slave labour) have been offered as a way of doubling down on the elements of human history responsible for environmental crises, rather than lumping all humans, and their responsibility, together. But I want to concentrate on the idea of time itself. “Deep time” is the concept of geological time that is used “to describe the timing and relationships between events that have occurred throughout Earth’s history”. That’s a 4.54 billion year history. We struggle to grasp the huge scale of a sense of time that is so, well, deep. There are numerous analogies for helping us comprehend this enormity, like the 24-hour clock – that humans have only been on the planet for 19 seconds of it. I like the one below, as you can visualise it simply enough by holding out your arm.  If the Earth formed about 4.54 billion years ago at the shoulder, animals of any kind appear within the palm, and more familiar (to us) lifeforms originate at the first knuckle. Movement along the fingers represent the periods that followed, incorporating, for example, the Jurassic. And humans? The 11,700-year-old Holocene marks the start of a global spread of homo sapiens – “a microscopic sliver at the tip of a fingernail”. The beginning of the proposed Anthropocene, whether we go with a starting point of a mooted 400 years, 70 or somewhere in between, is a tiny speck within this sliver. So, have homo sapiens created a new geological era? In simple terms, there is something of a case here – there’s plenty of evidence for human impact in the geological record, from signatures of human-induced climate change, atomic testing, and much more. But a fuller appreciation of deep time should actually make us wary of the Anthropocene label, maybe even shift our image of ourselves and what it means to inhabit the Earth at this time. Here’s why.  


      Read more:
      A glass of whisky could help you get your head around deep time


 Around 66m years ago, a mass extinction event took place, wiping out around three quarters of all species. This was most likely the result of an enormous asteroid impact – a conclusion reached after the discovery of a thin but distinct layer of sediment in the geologic record from this time, containing elements abundant in asteroids.   Mass extinction offered an opportunity for the rise of mammals as dominant lifeforms – ushering in the Cenezoic (“new life”) era. This thin layer of comet dust in the rock record represents a brief but vital transition between much thicker preceding and subsequent layers. But no one refers to what followed the mass extinction event as the “Cometocene”. That just wouldn’t make sense – the impact was a one-off event, significant in the context of deep time only in that it ushered in new foundations for life that then stretched out for millions of years into the far future.  What if the same could be said of our influence? What if, even with the well-documented effects of an Anthropocene still accumulating, we are talking about human impacts as a mere blip in the context of deep time? This is likely true. The spread of industrialism has aggressively and rapidly extracted and used up a finite supply of resources. The fact of finiteness, coupled with unprecedented environmental breakdown, fundamentally circumscribes the long-term viability of any possible era of human dominance.  This is what the American writer John Michael Greer claims when he says that all forms of industrial civilisation combined, in the context of geological time, are unremarkably short-lived and “self-terminating” – simply a transition between eras. This is why he considers the Holocene-Neocene transition, H-N transition for short, as a more accurate term, with Neocene being a placeholder name for whatever emerges next.  Our geological legacy will probably be like the comet dust – “a slightly odd transition layer a quarter of an inch thick”. As a remarkably adaptive species, humans may find ecological niches to survive and flourish in this far future, but we will not be dominant.  This does not mean we are heading towards some kind of one-off cataclysm – another extinction event. It means we are already living through one. But rather than being remembered as something grandiloquent and portentous – like the Anthropocene – it is more likely that some far future species would think of us as what historian Stephen Kern calls “a parenthesis of infinitesimal brevity”. In the context of deep time, the Earth will continue to meander on without us, and it will hardly notice we’re gone, just as it hardly knew we were here. This sojourn into deep time is not intended to be depressing or defeatist, certainly not to rule out hope, or to avoid acknowledgement of the damage humans can do. I think its psychological relevance is to offer a reminder of life itself as something to approach with reverence and awe; our species as interdependent and interconnected, not somehow apart; and to chip away at any residual hubris in the idea of the Anthropocene. Locating humanity in an even deeper story can seem scary. But it might also be liberating. For countless cultures around the world of course, this is nothing new – many Indigenous worldviews embrace nature, have a reverence for it and a deep sense of time and place. While being historically displaced from those places by the forces of colonialism and industrialism, these voices are often neglected.   The history of our far future, if we have one, will be one where we learnt to recognise interdependence with nature, with other species. In the end, it is about what it means to be human. As the late environmental philosopher Val Plumwood warned: “We will go onwards in a different mode of humanity, or not at all.”"
"Scotland might traditionally be known for its North Sea gas reserves but it also leads the way in renewable power. The current devolved Scottish government wants 100% of the nation’s electricity generation to come from renewables by 2020. But the wind industry that may power Scotland towards the target developed while part of the UK. How might renewables fare after independence? The short answer is that, in the event of a Yes vote, renewable energy would go from strength to strength. The rest of the UK would have no choice other than to co-operate with Scotland on energy matters. The negative attitude of the coalition government towards renewable energy has long been evident and also includes plans for a significant reduction in renewable energy subsidies. In the event of a No vote it is highly unlikely that the UK government would change its position and encourage investment in renewable power. Wind energy is doing well, for instance. According to RenewableUK wind energy set new UK records during August 2014, surpassing the amount of energy generated by coal power in the UK on five different days that month and overtaking nuclear generation on one occasion.  The latest government polling put public support for renewable energy sources at a very high 79%. But despite this the UK government continues to support unsafe and costly new nuclear stations at the expense of wind, solar or tidal power. Its position could only worsen in the event of a No vote, and would represent a disaster for Scotland’s renewables ambitions. A Yes vote, on the other hand, would surely place the future of renewable power in the hands of the Scots, who are committed to a future powered by wind and sea. The more optimistic reading of Scotland’s reserves, one that echoes the estimates of industry lobbyists Oil and Gas UK, would ensure energy security for Scotland. This would in turn provide revenue which could be invested in the further development of renewables. The more pessimistic version, where Scotland’s oil and gas reserves are declining, means developing renewables becomes an even more important aspect of energy security.  The Scottish government’s own report Energy Regulation in an Independent Scotland declares the country should position itself as the best place to generate renewable power and it seems set to follow up this plan in the event of a Yes. Whichever way you look at its oil reserves, Scottish renewables would grow stronger under independence. Scotland has the wind, and it knows how to use it. The renewables industry is clearly concerned about the effect of a Yes vote on subsidies currently regulated by UK bodies. However, Scotland can deal with regulatory uncertainty by relying on its clear long-term policy direction and the introduction of new subsidies that will focus on specific growth opportunities for renewables. Whatever the outcome of the referendum, the UK still has its climate change targets to meet – and wind energy from Scotland would help. As a result, it is highly likely that the rest of the UK would have no choice but to continue importing electricity from Scotland. This would place independent Scotland in a better position to negotiate an arrangement for an integrated energy market, in line with the stated policy goal of forming an effective energy partnership with the rest of the UK. In summary, a Yes vote would definitely be the better choice for meeting the aspirations of renewable energy generation in Scotland and to ensure security of supply in the long term. The rest of the UK would have no choice but to co-operate with an independent Scotland under these circumstances to meet its climate change targets and to avoid blackouts."
"The author of a groundbreaking report on the economic impact of climate change has called on Rishi Sunak to spend more than £8bn in his first budget next week to kickstart a “massive and long-term” boost to “zero-carbon infrastructure, new skills and sustainable innovation”. Lord Stern said the new chancellor had a unique opportunity to address regional inequalities and invest to meet the government’s target for net-zero emissions with measures already highlighted in the Conservative party manifesto. Stern, who runs the London School of Economics’ Grantham Research Institute on Climate Change, told Sunak to focus his efforts on sectors that are “difficult to decarbonise”, such as transport, property and industry. Sunak is known to be hurriedly rewriting his budget speech for 11 March and earmarking funds to tackle the coronavirus outbreak, possibly delaying measures to improve the UK’s infrastructure. But he is expected to signal extra spending in the regions over the life of the parliament to 2024, to support Britain reaching net-zero carbon emissions by 2050. The report recommends the government use £6.3bn committed for energy efficiency in the 2019 election manifesto to reduce energy waste in buildings, which are responsible for 17% of the UK’s greenhouse gas emissions. It also recommends that £1bn committed in the manifesto for vehicle charging points should be focused on rural parts of the UK that would otherwise miss out on the electric car revolution. Stern said £800m promised to limit emissions through initiatives to capture carbon and store it should be used as an incentive to generate private sector investment. “The budget should mark the start of a decade of massive investment in accelerating the transition of the UK economy to zero-carbon growth,” said Stern, whose report for former chancellor Gordon Brown in 2005 was one of the first to show the economic challenges from climate change and how they could be met in the UK and globally. “This would mean that by 2030 the UK could have higher living standards, and better health and wellbeing, underpinned by UK businesses innovating and adopting cutting-edge zero-carbon technologies and practices fit for the mid-21st century,” he added. Stern’s warning against inaction by the Treasury came as 101 climate campaigners and former government and Bank of England advisers wrote to the incoming central bank boss, Andrew Bailey, urging him to force firms to disclose their climate risks “as soon possible”. The central bank should also exclude fossil fuel assets from both future rounds of quantitative easing (QE) and the assets the Bank accepts as collateral, so as to “lead by example”. An earlier report by the Grantham Institute found that the bank’s £10bn purchases of corporate bonds, part of a £435bn stimulus programme, was heavily skewed towards oil and gas companies. Among the 101 signatories were Sir David King, a former government chief scientific adviser, ex-Citigroup chief economist and former Bank of England monetary policy committee member Willem Buiter and primatologist Jane Goodall. The signatories said they wanted Bailey to recognise the severity of the climate emergency by going beyond the initiatives put in place by his predecessor Mark Carney. Fran Boait, executive director of Positive Money, one of the sponsors of the letter, said: “The investments made by our financial system today determine whether we will be able to keep global temperature rises below the 1.5C upper safe limit. Finance is currently funding warming of more than 4C, which represents an existential threat not only to finance and the economy, but to life on earth. “With less than a decade to drastically cut emissions and avoid irreversible climate breakdown, Andrew Bailey must ensure that climate remains high on the Bank’s agenda. These steps are a necessary starting point.”"
"When they start mining the seabed, they’ll start mining part of me. These are the words of a clan chief of the Duke of York Islands – a small archipelago in the Bismarck Sea of Papua New Guinea which lies 30km from the world’s first commercial deep sea mine site, known as “Solwara 1”. The project, which has been delayed due to funding difficulties, is operated by Canadian company Nautilus Minerals and is poised to extract copper from the seabed, 1600m below the surface. Valuable minerals are created as rapidly cooling gases emerge from volcanic vents on the seafloor. Mining the seabed for these minerals could supply the metals and rare earth elements essential to building electric vehicles, solar panels and other green energy infrastructure. But deep sea mining could also damage and contaminate these unique environments, where researchers have only begun to explore. The industry’s environmental impact isn’t the only concern. It’s been assumed by the corporate sector that there is limited human impact from mining in the deep sea. It is a notion that is persuasive especially when compared with the socio-ecological impacts of land-based mining.  But such thinking is a fallacy – insights from my research with communities in Papua New Guinea over the past three years highlight that the deep sea and its seabed should be thought of as intimately connected to humanity, despite the geographical distances involved. For the people of the Duke of York Islands, deep sea mining disturbs a sense of who they are, including the spirits that inhabit their culture and beliefs.  In Western thought, the sea has not only been considered to be marginal to politics, but also as entirely distinct from the land. Separating nature from humanity has proved useful in enabling exploitation of the natural world for human means. Deep sea mining, with all its material connections between a dynamic seabed and sites of consumption on land, provokes new questions. If humanity can’t physically encounter the deep seabed, then how are we to treat it ethically?. By conceptually “distancing” the deep ocean, who is being marginalised? For the people who live close to Solwara 1, the answer is pointed. These communities have long understood the world as a connection between “nature”, “spirits” and “beings”. Central within this cosmology are the spirits – masalai – some of which are understood as guardians of the seabed and its resources. Masalai are a fundamental part of the islanders’ world. Thus, the prospect of deep sea mining means not just social and economic disruption, but spiritual turmoil. The digging up of the seabed and the extraction of its resources cuts through the very fabric of their spiritual world and its sacred links to the sea and land.  As the historian Neil Macgregor put it in the Radio 4 series “Living with the Gods”, masalai are not  out there… [like] tourists in the human realm, from somewhere else … but in a world in which we co-inhabit.  The political implication for island communities here is clear. The copper which might be mined from the seabed is effectively constituted by these spirits. Thus, as copper “resurfaces” in the objects and technologies of the future – in batteries and wiring – it also carries a spirituality from the region where it originated. Spirits infuse the traditions and everyday practises of the people on the Duke of York Islands. “Shark calling” is one such example which is practised along parts of the west coast of New Ireland Province – the closest point on land to Solwara 1.  Every few weeks, when the sea conditions allow, “shark callers” attempt to attract sharks to their hand-carved wooden canoes by rattling a mesh of coconut shells in the water, before capturing them by hand. Shark meat is a key part of local diets that generally lack protein. 


      Read more:
      Deep sea mining could help develop mass solar energy – is it worth the risk?


 Shark callers communicate with spirits which are “resident” in stones found on local beaches prior to their expeditions. It’s no surprise then, that these communities fear noise pollution generated by deep sea mining and the physical disturbance of the seabed which could sever the cultural connections they have with the ocean. Deep sea mining companies should consider the spirituality of the people their work affects and other kinds of environmental knowledge as important in their own right. As this new industry collides with cultural belief systems in different parts of the world, it will be essential to understand the complex ways in which deep sea mining does have “human” impacts after all. Culture is a key part of any understanding of environmental politics, no matter how extreme the environment in question."
"
Share this...FacebookTwitterNow that the science behind the threat of polar bear extinction has fallen to pieces too, it’s worth looking at how the German elite media has approached the story, at least those who have not chosen to ignore the inconvenient embarrassment altogether, as most have done.
Der Spiegel, to their credit, has given the story online, front page treatment here. But one notices that Der Spiegel couldn’t help making it sound like an intrigue involving environmentalists, power politics and oil companies lurking in the background pulling strings:
It is a mysterious story that a research suspense story needs: Powerful oil companies in the background, dead polar bears as icons of climate change – and a scientist under suspicion.”
Der Spiegel then goes on to explain why Monnett was suspended, first explaining how Monnett sees it:
In Monnett’s view, scientists like himself are standing in the way of the Obama-government to open up the ocean area off the coast of Alaska for oil drilling. That’s what the government of the state under Republican Sean Parnell wants. That’s what the oil companies, foremost Shell, want. And that’s what the White House wants.”
According to Der Spiegel, Monnett views himself as a lone victim of a conspiracy for having played a major role in getting the polar bear on the list of endangered species. Finally, past the half way point of the piece, Der Spiegel points out that Monnett’s scientific work was indeed sloppy and grossly lacked data:
Indeed the hated scientist had to admit last winter in a hearing that hardly any documentation for viewing the dead polar bears exists. There were no clear photos. The animals also did not show up in any official datasets of the expedition.”
Der Spiegel also noticed that the peer review was everything but rigorous, the paper sailed through the process with hardly any scrutiny:
In the publication of the Polar Biology article, it appears no one was disturbed by this. Even in-house reviewers in Monnett’s office as well as three anonymous peer reviewers of the journal simply waved the paper through with only slight modifications.”
Share this...FacebookTwitter "
"Algae isn’t just found in your garden pond or local river. Sometimes it explodes into vast “blooms” far out to sea, that can be the size of a small country. Such algal blooms can match even a rainforest at taking carbon out of the air. And then, in just a week or two, they are gone – sometimes consumed by viruses.  Given the scale of blooms and their vital role in both marine ecology and climate regulation we must know more about these viruses. Research conducted with our Weizmann Institute colleague Yoav Lehahn and others and published in the journal Current Biology, is the first attempt to quantify the affect of viruses on large scale algal blooms. Algae in this context refers to tiny sea organisms known as phytoplankton which exist right at the bottom of the marine food web, providing the ultimate source of all organic matter in the sea. They do this by consuming carbon dioxide during photosynthesis, “fixing” this carbon into organic matter (themselves) in the same way trees take carbon out of the air.  Therefore phytoplankton serve as a major sink, or “biological pump”, of atmospheric CO2. Some studies show that, although they account for less than 1% of the photosynthetic biomass on Earth, phytoplankton fix almost half of the world’s total organic carbon. Blooms in the ocean are controlled by two processes. First, the biological or ecological factors – the interplay between nutrients, predators and pathogens. More nutrients means more algae; more algae-eating fish, or algae-attacking viruses means fewer blooms. And then you have the physical processes – the stability of the upper ocean and the temperature and salinity of the water and the speed of its currents.  In our study, we developed an approach to trace and follow unique patches of plankton blooms in which the physical conditions are almost constant. Such patches can be viewed as unique ecosystems in which any changes can be attributed mainly to biological or ecological processes. We used satellite data over such a patch in the North Atlantic to track the whole life cycle of a bloom in a phytoplankton species called Emiliania huxleyi. We are able to do this as satellites are able to detect the presence in water of pigments such as chlorophyll used for photosynthesis. The physical properties of this patch were stable throughout the life of the bloom – the sea didn’t suddenly change temperature, get more salty or drift in an unusual direction. Therefore we know that most changes in the algae were caused by biological processes. Recent studies of marine microbiology showed viruses play a key role in regulating algae populations, killing off algal blooms in the same way viruses regulate human populations during an epidemic. Such processes were documented in the lab and in local measurements in confined zones in the ocean. But what was missing was real-world verification over large scale natural bloom. We wanted to find a mass viral-induced phytoplankton demise in nature. We set out to find a unique fingerprint of a virus-driven demise and to use this to measure the impact of viruses on large scale oceanic blooms. Combining satellite data with field measurements of viral activity during an algal bloom in the North Atlantic ocean allowed us to conclude that the demise of this specific bloom was due to viral infection.  We used this newly established method of following the complete life cycles of a distinct bloom to estimate the amount of carbon that is turned over by viruses that infect this bloom. We were able to calculate the life-span of this phenomenon and how it has been affected by viral infection. An algal patch with an area of around 1,000 km2 can “fix” around 24,000 tons of organic carbon throughout its life time. This is equivalent to a similarly sized patch of rain forest. And this is, of course, only one patch out of many.  The big impact viruses have on entire ecosystems was already well known. But now, for the first time, we can quantify their immense impact on open ocean blooms: in just two weeks such viruses can “consume” a huge algal bloom that harbours many tons of carbon. The exact fate of the organic carbon that is realised by viral attack is not clear. Most of it is probably recycled back into the atmosphere by bacteria that use the organic carbon as substrates for respiration (a process called The Viral Shunt).  Another option is that the viral infection release sticky molecules like sugars and lipids which make organic carbon sink faster to the ocean bed. If the latter scenario is true it will have a profound impact on the efficiency of CO2 “pumping” from the atmosphere to the deep ocean. This carbon will  have a better chance of being buried in the ocean sediment. The ability to quantify viral infections from space, and the rapid timing of this bloom demise on such large scales holds a promise for understating the fate of carbon in the oceans."
"

Everyone who reads _Science_ — the journal of the lobbying organization the American Association for the Advancement of Science (AAAS) — knows that it only accepts one side of the global warming story in its “Compass” and “Perspectives” sections, and in its more opinionated, mainline articles. Anyone who writes otherwise for those sections gets a quick rejection. That’s understandable because global warming is scheduled to pay U.S. scientists about $4.2 billion next year, and the AAAS is just doing its job keeping the customers happy.



But sometimes they go a little overboard in their one‐​sided zeal, particularly when they schedule so‐​called bombshell articles to coincide with the periodic meetings of the signatories to the United Nations’ Climate Change treaty, discussing implementation of the (dead?) Kyoto Protocol. The most recent case of this funereal dance just ended in Milan, Italy.



For Milan, _Science_ published, and then heavily publicized, an article by federal climatologists Tom Karl and Kevin Trenberth, entitled “Modern Global Climate Change.” This reveals that _Science,_ in its plumping for Kyoto, is now publishing material that is decades behind the global warming power curve. 



Karl and Trenberth repeat the usual United Nations saw that there’s “a 90% probability interval for warming from…1.7° to 4.9°C” in the next century.” In fact, the 21st century warming rate is now well‐​known to be confined to a much lower and smaller range, about 0.75 +/- 0.25°C per 50 years, and may be lower than that. 



You can’t even generate a constant rate of global warming unless carbon dioxide goes up exponentially. In other words, a constant increase in carbon dioxide must lead to a damped (slowing) response in warming. This has been known since 1872.



Karl and Trenberth give the impression that this exponential increase is happening. It’s not. But, they write: “Recent greenhouse gas emission trends in the United States are upward, as are global emission trends, with increases between 0.5 and 1% per year over the past few decades.”



The problem here is one of purposeful imprecision, as in “past few decades.” In reality, data from the Energy Information Administration show that there was some substantially exponential growth in emissions, but since 1980 it’s been much closer to a simple linear change. Twenty‐​four years of recent linearity comprises “a few decades,” doesn’t it?



This change in emissions is reflected in changes in the growth rate of atmospheric carbon dioxide, which stabilized nearly 30 years ago. That’s right. While all scientists have glibly assumed an exponential increase in atmospheric carbon dioxide, that stopped, in the statistical sense, three decades ago. But an exponential increase is required to generate a constant rate of warming.



What happened? Per capita emissions of carbon dioxide peaked around 1980 and have been in statistically significant decline ever since. 



What perpetuates the tired myth of exponentially increasing carbon dioxide? It’s the oft‐​repeated saw that “Everyone in the world aspires to a U.S. lifestyle.” Since we used to emit about 30 percent of the world’s industrial belching of CO2, the math becomes obvious if everyone emulates us.



People who assumed increases in per capita carbon dioxide were wrong 25 years ago and they are wrong now. But this is precisely what is input into every general circulation climate model, and these models serve as the basis for Karl and Trenberth’s projections for warming. They’ve been run with the wrong data for a quarter century! 



If you put in the right data, warming drops dramatically, to about 1.6°C in the next 100 years. A while back, in a statement he would probably like to have back, Robert Watson, then head of the U.N.‘s Intergovernmental Panel on Climate Change, allowed that such a small warming might actually be beneficial.



Why was everyone wrong? Well, it turns out that the world is largely emulating the United States. Per capita incomes are increasing. As they increase, per capita emissions drop because people can invest in more efficient technology. In what large nation did the drop first take place? The good ‘ole USA.



How on earth did _Science_ become so derriere in the face of so much reality? Perhaps that’s what happens when one’s political goals get in the way of one’s science.
"
"
Share this...FacebookTwitterTwo short items today. One a winter forecast for Central Europe and the other is a look back at Germany’s recent wet and “warm” summer.2011/2012 Winter forecast
Another forecast for Germany (Central Europe) for the 2011/12 winter is out. This one is from Dominik Jung, a young whippersnapper meteorologist for Germany’s leading daily tabloid Bild, read here in German in a piece called: Weather Expert Expects A Shivering Winter, h/t: Reader Ike
Jung believes the 2011/2012 Winter, i.e. December, January and February, will be colder than the mean temperature for the period of 1960-1990, which has been designated as being “normal”. Bild quotes Jung:
Already during the last three years it was up to 2°C colder than the average. If that happens again this year, which we believe it will, then it would be the fourth cold winter in a row and so a small sensation.”
Jung believes that it will be especially cold in Southern Germany with lots of snow and ice. I remind you that Jung is a warmist, and so his words need to be taken with caution when assessing the quality of his science. After all, warmists do believe CO2 is the major driver of climate, and that other factors like the sun and oceans are irrelevant.
Claim that German summer 2011 was too warm is “stupidity”
We just heard that Great Britain had its coolest summer in 20 years and how a number of butterflies died off as a result. Things were not much better in Germany, which just had one of its coolest summers in 20 years. But that didn’t keep the warmstream media from declaring it as “too warm”.
Readers Edition has a short piece here and brings our attention to a video clip of a discussion on German NDR public TV.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The video is some sort of talk show where guest meteorologist Stefan Kreibohm explains why Germany’s summer was a washout this year. At the 2.20 mark the question comes up on whether or not this was an unusual event and if this is the sort of thing we will all have to start getting used to in the future and if we are in a “death spiral”.
Quite surprisingly Stefan Kreibohm answers “no” and says:
It is simply nature’s mood. It happens. There have been summers where it hardly rained and there have been summers where it has rained a lot. This year we happened to have a summer where it rained a lot.”
and over the last 300 years, he adds:
There have always been warmer phases and colder phases.”
The lady then brings up an interesting point at the 3:31 mark and remarks that she perceived the summer to be cool, and so asks how come we are told it was warmer than normal, with a gentleman adding that in June parts of Germany even saw a frosty night. At the 4:05 mark Kreibohm explains:
There’s actually a dispute among meteorologists. The German Weather Service always uses the period of 1960 to 1990 as a reference. This was a time that was a little bit colder than today. So if you compare this summer with that, we see that it was warmer then normal. That’s nonsense – no one understands that. So if you tell someone the summer was too warm, it’s of course pure stupidity. If you compare it to the last 30 years, it was completely normal.”
Well shiver me timbers! A bit of reality here, and on German public television no less! Expect the producers to be reprimanded and Kreibohm never to be invited again.
 
Share this...FacebookTwitter "
"
If you are just joining us, first you should read about what started it all here.
While Realclimate.org continues deleting the ongoing river of comments posted on their threads ( Note: Any of you who find that your posts to those sites are being rejected {as usual without any explanation} can keep a copy of the post, and post it at http://rcrejects.wordpress.com if you want. Keep those screencaps going folks) asking about the McIntyre Yamal data development, Jennifer Marohasy of Australia is drawing a bit of a line in the sand. Given the churlishness of the Team and the blockades put up by Hadley, I can’t say that I blame her stance. – Anthony

Leading UK Climate Scientists Must Explain or Resign
By Jennifer Marohasy
MOST scientific sceptics have been dismissive of the various reconstructions of temperature which suggest 1998 is the warmest year of the past millennium.    Our case has been significantly bolstered over the last week with statistician Steve McIntyre finally getting access to data used by Keith Briffa,  Tim Osborn  and Phil Jones to support the idea that there has been an unprecedented upswing in temperatures over the last hundred years –  the infamous hockey stick graph.
Mr McIntyre’s analysis of the data – which he had been asking for since 2003 – suggests that scientists at the Climate Research Unit of the United Kingdom’s Bureau of Meteorology  have been using only a small subset of the available data to make their claims that recent years have been the hottest of the last millennium.   When the entire data set is used, Mr McIntyre claims that the hockey stick shape disappears completely. [1]
Red - before new data Black - after new data
Mr McIntyre has previously showed problems with the mathematics behind the ‘hockey stick’.   But scientists at the Climate Research Centre, in particular Dr Briffa, have continuously republished claiming the upswing in temperatures over the last 100 years is real and not an artifact of the methodology used – as claimed by Mr McIntyre.     However, these same scientists have denied Mr McIntyre access to all the data.    Recently they were forced to make more data available to Mr McIntyre after they published in the Philosophical Transactions of the Royal Society  –  a journal which unlike Nature and Science has strict policies on data archiving which it enforces.   
This week’s claims by Steve McInyre that scientists associated with the UK Meteorology Bureau have been less than diligent  are serious and suggest some of the most defended building blocks of the case for anthropogenic global warming are based on the indefensible when the methodology is laid bare.
This sorry saga also raises issues  associated with how data is archived at the UK Meteorological Bureau with in complete data sets that spuriously support the case for global warming being promoted while complete data sets are kept hidden from the public –  including from scientific sceptics like Steve McIntyre.
It is indeed time leading scientists at the Climate Research Centre associated with the UK Meteorological Bureau explain how Mr McIntyre is in error or resign.
***********
Notes and Links
[1] Yamal: A “Divergence” Problem, by Steve McIntyre, 27 September 2009
http://www.climateaudit.org/?p=7168
The above chart shows the difference when the entire data set (black line) as opposed to a subset (red line) is used to reconstruct temperature.   The chart is accompanied by the following comment from Mr McIntyre:  “The next graphic compares the RCS chronologies from the two slightly different data sets: red – the RCS chronology calculated from the CRU archive (with the 12 picked cores); black – the RCS chronology calculated using the Schweingruber Yamal sample of living trees instead of the 12 picked trees used in the CRU archive [leaving the rest of the data set unchanged i.e. all the subfossil data prior to the 19th century]. The difference is breathtaking.”
Mann, Michael E.; Bradley, Raymond S.; Hughes, Malcolm K. (1998), “Global-scale temperature patterns and climate forcing over the past six centuries” (PDF), Nature 392: 779–787, doi:10.1038/33859, http://www.caenvirothon.com/Resources/Mann,%20et%20al.%20Global%20scale%20temp%20patterns.pdf
Wikipedia http://en.wikipedia.org/wiki/Hockey_stick_controversy#cite_note-17
CRU Refuses Data Once Again
http://www.climateaudit.org/?p=6623
http://climateresearchnews.com/2009/09/the-hockey-stick-is-dead/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92d8c348',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Brazil’s Atlantic forest – Mata Atlântica – is one of the world’s great biodiversity hotspots, rivalling even the Amazon. Running on and off for several thousand kilometres along the coast, the forest is home to 10,000 plant species that don’t exist anywhere else, more bird species than the whole of  Europe, and more than half of the country’s threatened animal species. Today, the ecosystem it sustains is under threat: trees have been cleared for farms, houses and roads, big cities such as Rio de Janeiro and São Paulo have grown in the region, and just 15% of the original forest remains. But it’s not all doom and gloom for the Atlantic forest. Myself and collaborators from Universities of São Paulo, Michigan, Toronto, and UNESP, have published a study in the journal Science which shows that paying farmers to conserve areas of forest within their property is good value for money. The key message from our work is that it is possible to protect native species, maintain a healthy ecosystem and potentially reduce poverty, all for less than US$200m each year. These results are interesting for at least two reasons. The first is very simple, our conclusions are not calamitous – instead of showing that it is the end of world as we know it, we show that human welfare and conservation needs can both be satisfied for a reasonably small amount of money. The headline number seems large, but it only represents less than 0.01% of Brazil’s GDP. The second reason is much more complicated, however. Brazil is heading for an election in October, and as it stands anything could happen. This is important as the current government is in the process of relaxing the Brazilian Forest Code which would, among other things, allow farmers to set aside a smaller proportion of their land to native habitat. This has sparked a fiery discussion among conservationists, scientists, politicians and farmers. Previously, the code required farmers living within the Atlantic forest to set aside 20% of their land for native habitat. Farmers were prevented from designating land they couldn’t use anyway such as particularly steep terrain or areas close to rivers, so many had to set aside more than 20% of their land. In theory, this was great for biodiversity; in practice it never worked as farmers didn’t respect the law, often because they couldn’t afford the economic costs of setting aside productive agricultural land for conservation. The other side of the coin is that the new code is unlikely to protect the Atlantic forest’s species. But, up until now, nobody knew how much habitat really was needed. What our results show is that at least 30% of the forest area needs to be set aside for conservation if we are to preserve a healthy ecosystem. This is great news for Brazilian scientists and conservationists, because now they can use a number to base their arguments while discussing the changes to the Forest Code, instead of saying the usual “more the merrier”. But the suggestion that more forest is needed to preserve biodiversity doesn’t mean the battle between farmers and conservationists has to continue. This is because the onus of increasing forest cover from 20% to 30% doesn’t have to solely fall on farmers. There are already some schemes that pay farmers to set aside part of their land for the protection or restoration of native habitats. These schemes are usually run by local governments or NGOs and the rationale is that they are paying people to protect crucial ecosystem services, such as carbon storage, watersheds (or water quality) and the functions provided by a healthy spread of plants and animals such as pollination, or pest control. We show that if the Brazilian government expands these schemes, then we can have both happy farmers and happy biodiversity. This does not mean that every single farm in the Atlantic forest would have to set aside 30% of its land for conservation. It also does not mean that we would protect all species from extinction, as some need 100% of pristine forest.  But if priority areas were restored to at least 30% native habitat cover, the price to pay would be less than 6.5% of what Brazil currently spends on agricultural subsidies. Farmers willing to set aside land for conservation would receive regular payments, local communities would receive the benefits of enhanced ecosystem services, and native species would be protected. Sounds like a good deal. Whoever wins Brazil’s election will have to deal with either angry farmers or angry scientists and conservationists. Our study thus reveals a promising light at the end of the tunnel – the suggestion that the new government might not need to make compromises; potentially this is a battle everybody can win."
"
Share this...FacebookTwitterh/t: Dirk H


Note how the reporter is upset about people going out in the storm when he himself is outside right smack in it. Then again, the winds are only 50 mph (and not the 500 mph the media was telling us yesterday :)). 
I noticed the report is from “The Weather Channel”. Isn’t that Heidi Cullen’s warmist outfit?
Share this...FacebookTwitter "
"
Guest Post by Willis Eschenbach
One of the arguments frequently applied to the climate debate is that the “Precautionary Principle” requires that we take action to reduce CO2. However, this is a misunderstanding of the Precautionary Principle, which means something very different from the kind of caution that makes us carry an umbrella when rain threatens. Some people are taking the Precautionary Principle way too far …

Figure 1. Umbrella Exhibiting an Excess of Precaution
The nature of the Precautionary Principle is widely misunderstood. Let me start with the birth of the Precautionary Principle (I’ll call it PP for short), which comes from the United Nations Rio de Janeiro Declaration on the Environment (1992). Here’s their original formulation:
“In order to protect the environment, the precautionary approach shall be widely applied by States according to their capability. Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation.”
This is an excellent statement of the PP, as it distinguishes it from such things as carrying umbrellas, denying bank loans, approving the Kyoto Protocol, invading Afghanistan, or using seat belts.
The three key parts of the PP (emphasis mine) are:
1)  A threat of serious or irreversible damage.
2)  A lack of full scientific certainty (in other words, the existence of partial but not conclusive scientific evidence).
3)  The availability of cost-effective measures that we know will prevent the problem.
Here are some examples of how these key parts of the PP work out in practice.
We have full scientific certainty that seat belts save lives, and that using an umbrella keeps us dry. Thus, using them is not an example of the PP, it is simply acting reasonably on principles about which we are scientifically certain.
There are no scientific principles or evidence that we can apply to the question of invading Afghanistan, so we cannot apply the PP there either.
Bank loans are neither serious nor irreversible, nor is there partial scientific understanding of them, so they don’t qualify for the PP.
The Kyoto Protocol is so far from being cost-effective as to be laughable. The PP can be thought of as a kind of insurance policy. No one would pay $200,000 for an insurance policy if the payoff in case of an accident were only $20, yet this is the kind of ratio of cost to payoff that the Kyoto Protocol involves. Even its proponents say that if the states involved met their targets, it would only reduce the temperature by a tenth of a degree in fifty years … not a good risk/reward ratio.
Finally, consider CO2. The claim is that in fifty years, we’ll be sorry if we don’t stop producing CO2 now. However, we don’t know whether CO2 will cause any damage at all in fifty years, much less whether it will cause serious or irreversible damage. We have very little evidence that CO2 will cause “dangerous” warming other than fanciful forecasts from untested, unverified, unvalidated climate models which have not been subjected to software quality assurance of any kind. We have no evidence that a warmer world is a worse world, it might be a better world. The proposed remedies are estimated to cost on the order of a trillion dollars a year … hardly cost effective under any analysis. Nor do we have any certainty whether the proposed remedies will prevent the projected problem. So cutting CO2 fails to qualify for the PP under all three of the criteria.
On the other side of the equation, a good example of when we should definitely use the PP involves local extinction. We have fairly good scientific understanding that removing a top predator from a local ecosystem badly screws things up. Kill the mountain lions, and the deer go wild, then the plants are overgrazed, then the ground erodes, insect populations are unbalanced, and so on down the line.
Now, if we are looking at a novel ecosystem that has not been scientifically studied, we do not have full scientific certainty that removing the top predator will actually cause serious or irreversible damage to the ecosystem. However, if there is a cost-effective method to avoid removing the top predator, the PP says that we should do so. It fulfils the three requirements of the PP — there is a threat of serious or irreversible damage, we have partial scientific certainty, and a cost-effective solution exists, so we should act.
Because I hold these views about the inapplicability of the precautionary principle to CO2, I am often accused of not wanting to do anything about a possible threat. People say I’m ignoring something which could cause problems in the future. This is not the case. I do not advocate inaction. I advocate the use of “no-regrets” actions in response to this kind of possible danger.
The rule of the no-regrets approach is very simple — do things that will provide real, immediate, low-cost, tangible benefits whether or not the threat is real. That way you won’t regret your actions.
Here are some examples of no-regrets responses to the predicted threats of CO2. In Peru, the slums up on the hillside above Lima are very dry, which is a problem that is supposed to get worse if the world warms. In response to the problem, people are installing “fog nets“. These nets capture water from the fog, providing fresh water to the villagers.
In India’s Ladakh region, they have the same problem, lack of water. So they have started building “artificial glaciers“.These are low-cost shallow ponds where they divert the water during the winter. The water freezes, and is slowly released as the “glacier” melts over the course of the following growing season.
These are the best type of response to a possible threat from CO2. They are inexpensive, they solve a real problem today rather than a half century from now, and they are aimed at the poor of the world.
These responses also reveal what I call the “dirty secret” of the “we’re all gonna die in fifty years from CO2” crowd. The dirty secret of their forecasts of massive impending doom is that all of the threatened catastrophes they warn us about are here already.
All the different types of climate-related destruction that people are so worried will happen in fifty years are happening today. Droughts? We got ’em. Floods? There’s plenty. Rising sea levels? Check. Insect borne diseases? Which ones would you like? Tornados and extreme storms? We get them all the time. People dying of starvation? How many do you want? All the Biblical Plagues of Egypt? Would you like flies with that?
Forget about what will happen in fifty years. Every possible climate catastrophe is happening now, and has been for centuries.
So if you are truly interested in those problems, do something about them today. Contribute to organizations developing salt resistant crops. Put money into teaching traditional drought resisting measures in Africa. Support the use of micro-hydroelectric plants for village energy. The possibilities are endless.
That way, whether or not the doomsayers are right about what will happen in fifty years, both then and now people will be better prepared and more able to confront the problems caused by the unpleasant vagaries of climate. Fighting to reduce CO2 is hugely expensive, has been totally unsuccessful to date, will be very damaging to the lives of the poorest people, and has no certainty of bringing the promised results. This is a very bad combination.
Me, I don’t think CO2 will cause those doomsday scenarios. But that’s just me, I’ve been wrong before. If you do care about CO2 and think it is teh eeeevil, you should be out promoting your favorite no-regrets option. Because whether or not CO2 is a danger as people claim, if you do that you can be sure that you are not just pouring money down a bottomless hole with very poor odds of success. That’s the real Precautionary Principle.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f1dbead',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels and Paul C. “Chip” Knappenberger, from Cato’s Center for the Study of Science, review interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
\---------   
  
  
When it comes to global warming, facts often take a back seat to fiction. This is especially true with proclamations coming from the White House. But who can blame them, as they are just following the lead from Big Green groups (aka, “The Green Blob”), the U.S. Climate Change Research Program (responsible for the U.S. National Climate Assessment Report), and of course, the United Nations' Intergovernmental Panel on Climate Change (IPCC).   
  
We have documented this low regard for the facts (some might say, deception) on many occasions, but recently we have uncovered a particularly clear example where the IPCC’s ideology trumps the plain facts, giving the impression that climate models perform a lot better than they actually do. This is an important façade for the IPCC to keep up, for without the overheated climate model projections of future climate change, the issue would be a lot less politically interesting (and government money could be used for other things … or simply not taken from taxpayers in the first place).   
  
The IPCC is given deference when it comes to climate change opinion at all Northwest Washingon D.C. cocktail parties (which means also by the U.S. federal government) and other governments around the world. We tirelessly point out why this is not a good idea. By the time you get to the end of this post, you will see that the IPCC does not seek to tell the truth—the inconvenient one being that it dramatically overstated the case for climate worry in its previous reports. Instead, it continues to obfuscate.   
  
This extracts a cost. The IPCC is harming the public health and welfare of all humankind as it pressures governments to seek to limit energy choices instead of seeking ways to help expand energy availability (or, one would hope, just stay out of the market).   
  
Everyone knows that global warming (as represented by the rise in the earth’s average surface temperature) has stopped for nearly two decades now. As historians of science have noted, scientists can be very creative when defending the paradigm that pays. In fact, there are already several dozen explanations.   
  
Climate modelers are scrambling to try to save their creations’ reputations because the _one_ thing that they do not want to have to admit is that they exaggerate the amount that the earth’s average temperature will increase as a result of human greenhouse gas emissions. If the models are overheated, then so too are all the projected impacts that derive from the model projections—and that would be a disaster for all those pushing for regulations limiting the use of fossil fuels for energy. It's safe to say the number of people employed by creating, legislating, lobbying, and enforcing these regulations is huge, as in “The Green Blob.”   




In the Summary for Policymakers (SPM) section of its _Fifth Assessment Report_ , the IPCC pays brief attention to the recent divergence between model simulations and real-world observations:   




There are, however, differences between simulated and observed trends over periods as short as 10 to 15 years (e.g., 1998 to 2013).



But, lest you foolishly think that there may be some problem with the climate models, the IPCC clarifies:   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



Whew! For a minute there it seemed like the models were struggling to contain reality, but we can rest assured that over the long haul—say, since the middle of the 20th century—according to the IPCC, model simulations and observations “agree” as to what is going on.   
  
The IPCC references its “Box 9.2” in support of the statements quoted above.   
  
In “Box 9.2” the IPCC helpfully places the observed trends in the context of the distribution of simulated trends from the collection of climate models it uses in its report. The highlights from Box 9.2 are reproduced below (as our Figure 1). In this figure, the observed trend for different periods is in red and the distribution of model trends is in grey.   






  
  
_Figure 1. Distribution of the trend in global average surface temperature from 114 model runs used by the IPCC (grey) and the observed temperatures as compiled by the UK’s Hadley Center (red). (Figure from the IPCC Fifth Assessment Report.)_   
  
As can be readily seen in Panel (a), during the period 1998-2012, the observed trend lies below almost all the model trends. The IPCC describes this as:   




111 out of 114 realizations show a GMST [global mean surface temperature] trend over 1998–2012 that is higher than the entire HadCRUT4 trend ensemble.



That gives rise to the IPCC SPM statement (quoted above) that   




There are, however, differences between simulated and observed trends over periods as short as 10 to 15 years (e.g., 1998 to 2013).



No kidding!   
  
Now let’s turn our attention to the period 1951-2012, Panel (c) in Figure 1.   
  
The IPCC describes the situation depicted there as:   




Over the 62-year period 1951–2012, observed and CMIP5 [climate model] ensemble-mean trends agree to within 0.02°C per decade.



This sounds like the model are doing pretty good—only off by 0.02°C/decade. And this is the basis for the IPCC SPM statement (also quoted above):   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



Interestingly, the IPCC doesn’t explicitly tell you how many of the 114 climate models are greater than the observed trend for the period 1951-2012. And it is basically impossible to figure that out for yourself based on their Panel (c) because some of the bars of the histogram go off the top of the chart and the x-axis scale is so large as to bunch up the trends such that there are only six populated bins representing the 114 model runs. Consequently, you really can’t assess how well the models are doing and how large a difference of 0.02°C/decade over 62 years really is. You are left to take the IPCC’s word for it.   
  
Don’t.   
  
The website Climate Explorer archives and makes available the large majority of the climate model output used by the IPCC. From there, you can assess 108 (or the 114) climate model runs incorporated into the IPCC graphic—a large enough majority to quite accurately reproduce the results.   
  
We do this in our Figure 2. However, we adjust both axes of the graph such that all the data are shown and you can see the inconvenient details.   






  
  
_Figure 2. Distribution of the trend in the global average surface temperature from 108 model runs used by the IPCC (blue) and observed temperatures as compiled by the UK’s Hadley Center (red) for the period 1951-2012 (the model trends are calculated from historical runs with the RCP4.5 emissions scenario results appended after 2006). This presents the nearly identical data in Figure 1 Panel (c)._   
  
What we find is that there are 90 (of 108) model runs that simulate more global warming to have taken place from 1951 to 2012 than actually occurred and 18 model runs simulating less warming to have occurred. Which is another way of saying the observations fall at the 16th percentile of model runs (the 50th percentile being the median model trend value).   
  
So let us ask you this question, on a scale of 1 to 5—or rather, using the descriptors, “very low,” “low,” “medium,” “high,” or “very high”—how would you describe your “confidence” in this statement:   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



OK. You got your answer?   
  
Our answer is, maybe, “medium”, and there is plenty of room for improvement.   
  
The model range should be much tighter, indicating that the models were in better agreement with one another as to what the simulated trend should have been. As it is now, the model range during the period 1951-2012 extends from 0.07°C/decade to 0.21°C/decade (with the observed trend at 0.107°C/decade). And this is from models that were run largely with observed changes in climate forcings (such as greenhouse gas emissions, aerosol emissions, volcanoes, etc.) and for a period of time (62 years) during which short-term weather variations should all average out. In other words, they are all over the place.   
  
Another way the agreement between model simulations and real-world observations could be improved would be if the observed trend fell closer to the center of the distribution of model projections. For instance, the agreement would be better if, say, 58 model runs produced more warming and the other 50 produced less warming.   
  
What would lower our confidence?   
  
The opposite set of tendencies. The model distribution could be even wider than it is currently, indicating that the models agreed with each other even less than they do now as to how the earth’s surface temperature should evolve in the real world (or that natural variability was very large over the period of trend analysis). Or, the observed trend could move further from the center point of the model trend distribution. This would indicate an increased mismatch between observations and models (more similar to what has taken place over the 1998-2012).   
  
Unfortunately, that’s what is happening.   
  
Figure 3 shows at which percentile the observed trend falls for each period of time starting from 1951 and ending each year from 1980 through 2013.   








_Figure 3. The percentile rank of the observed trend in the global average surface temperature beginning in the year 1951 and ending in the year indicated on the x-axis within the distribution of 108 climate model simulated trends for the same period. The 50 th percentile is the median trend simulated by the collection of climate models._   
  
After peaking at the 42nd percentile (still below the median model simulation, which is the 50th percentile) during the period 1951-1998, the observed trend has steadily fallen in the percent rank, and currently (for the period 1951-2013) is at its lowest point ever (14th percentile) and is continuing to drop. Clearly, this “tendency within a trend” (as Casey Stengel or Yogi Berra might have called this “trendency”) is looking bad for the models, as the level of agreement with observations is steadily decreasing with time.   
  
In statistical parlance, if the observed trend drops beneath the 2.5th percentile, it would be widely considered that the evidence was strong enough to indicate that the observations were not drawn from the population of model results. In other words, statisticians would say the models disagree with the observations with “very high confidence.” Some researchers use a more lax standard and would say that falling below the 5th percentile would be enough to consider the observations not to be in agreement with the models. We could consider that case to be described as “high confidence” that the models and observations _disagree_ with one another.   
  
So, just how far away from either of these situations are we?   
  
It all depends on how the earth’s average surface temperature evolves in the near future.   
  
We explore three different scenarios between now and the year 2030:   
  
Scenario 1: The earth’s average temperature during each year of the period 2014-2030 remains the same as is average temperature observed during the first 13 years of this century (2001-2013). This scenario represents a continuation of the ongoing “pause” in the rise of global temperatures.   
  
Scenario 2: The earth’s temperature increases year-over-year at a rate equal to the observed rise in temperature observed during the period 1951-2012 (a rate of 0.107°C/decade). This represents a continuation of the observed trend.   
  
Scenario 3: The earth’s temperature increases year-over-year during the period 2014-2030 at a rate equal to that observed during the period 1977-1998—the period often identified as the 2nd temperature rise of the 20th century. The rate of temperature increase during this period was 0.17°C/decade. This represents a scenario in which the temperature rises at the most rapid rate observed during the period often associated with an anthropogenic influence on the climate.   
  
Figure 4 shows how the percentile rank of the observations evolves under all three scenarios from 2013 through 2030. Under Scenario 1, the observed trend (beginning in 1951) would fall below the 5th percentile of the distribution of model simulations in the year 2018 and beneath the 2.5th percentile in 2023. Under Scenario 2, the years to reach the 5th and 2.5th percentiles are 2019 and 2026, respectively. And under Scenario 3, the observed trend would fall beneath the 5th percentile of model-simulated trends in the year 2020 and beneath the 2.5th percentile in 2030.   








_Figure 4. Percent rank of the observed trend within the distribution of model simulations beginning in 1951 and ending at the year indicated on the x-axis under the application of the three scenarios of how the observed global average temperature will evolve between 2014 and 2030. The climate models are run with historical forcing from 1951 through 2006 and the RCP4.5 greenhouse gas scenario thereafter._   
  
**It is clearly not a good situation for climate models when even a sustained temperature rise equal to the fastest yet observed (Scenario 3) still leads to complete model failure within two decades.**   
  
So let’s review:   
  
1) Examining 108 climate model runs spanning the period from 1951 to 2012 shows that the model-simulated trends in the global average temperature vary by a factor of three—hardly a high level of agreement as to what should have taken place among models.   
  
2) The observed trend during the period 1951-2012 falls at the 16th percentile of the model distribution, with 18 model runs producing a smaller trend and 90 climate model runs yielding a greater trend. That's not particularly strong agreement.   
  
3) The observed trend has been sliding further and further away from the model median and toward ever-lower percentiles for the past 15 years. The agreement between the observed trend and the modeled trends is steadily getting worse.   
  
4) Within the next 5 to 15 years, the long-term observed trend (beginning in 1951) will more than likely fall so far below model simulations as to be statistically recognized as not belonging to the modeled population of outcomes. This disagreement between observed trends and model trends would be complete.   
  
So with all this information in hand, we’ll give you a moment to revisit your initial response to this question:   
  
On a scale of 1 to 5, or rather, using these descriptors “very low,” “low,” “medium,” “high,” or “very high,” how would you describe your “confidence” in this statement:   




The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend.



Got your final answer?   
  
OK, let’s compare that to the IPCC’s assessment of the situation.   
  
The IPCC gave it “very high confidence”— _the highest level of confidence that they assign_.   
  
Do we hear stunned silence?   
  
This, in a nutshell, sums up the IPCC process. The facts show that the agreement between models and observations is tenuous and steadily eroding and will be statistically unacceptable in about a decade. And yet the IPCC tells us with “very high confidence” that models agree with observations and therefore are a reliable indicator of future climate changes.   
  
Taking the IPCC at its word is not a good idea.   
  
_[This is a major revision of a post that first appeared at_ _Watts Up With That_ _on April 16, 2014.]_


"
"
Excerpts: from the Sunday Times: Polar bear is a ‘new’ species
by Jonathan Leake
Polar bears may  have come into existence only 150,000 years ago, when trapped brown  bears had to adapt to an ice age
Kissing Cousins? Oreo the brown bear and Ahpun the polar bear play at the Alaska Zoo. Photo from the Alaska Daily News by BOB HALLINEN / Daily News archive 1998
Polar bears may have come into existence only 150,000 years ago, when  brown  bears were trapped by an ice age and had to adapt quickly to survive,  scientists have found.
The suggestion follows the discovery of the jawbone of an animal that  died up  to 130,000 years ago, making it the oldest polar bear fossil found. The  bone  has yielded new insights into the origins of Earth’s largest land  predator.
One is the possibility that polar bears owe their existence not only to  past  climate change, including ice ages, but have also survived at least one  long  period of global warming.
The bone was discovered at Poolepynten on the Arctic island of Svalbard  by  Professors Olafur Ingolfsson, of the University of Iceland, and Oystein  Wiig, of the University of Oslo.
…
In a paper they said: “Brown bears of the ABC islands may be descendants  of  ancient ursids [bears] that diverged from other lineages of brown bears  and  subsequently founded the polar bear lineage.” This view is expected to  get  support from new research, out this week, based on DNA extracted from  the  Poolepynten jawbone.
It means polar bears have already survived a global warming that  affected the  northern hemisphere from 130,000 to 115,000 years ago, when the  Greenland  ice sheet and the Arctic ice cap were smaller than now. Professor Chris  Stringer, of the Natural History Museum in London, an expert in ice  ages,  said: “Early polar bears would not have had all the specialisations of  modern animals and we know nothing about their behaviour.
“Living through a warm period back then does not mean they are resilient  to  climate change now.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8d8ce264',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The building of tens of thousands of homes on flood-prone land is worsening the damage to surrounding areas, Conservative MPs have said, as the head of the Environment Agency warned against new developments on floodplains. Tory backbenchers called on Boris Johnson to review the government’s housing policy over concerns that new homes were either not flood-proof or were exacerbating issues in neighbouring communities.  John Redwood, the MP for Wokingham, said building on land most at risk of flooding was “a very foolish thing to do and it’s obviously making the problem considerably worse”. He said the risk to residents had been “greatly increased” by building on floodplains in his Berkshire constituency, and added: “I think [the government] should certainly review their planning policy and I think they should take the Environment Agency’s advice more seriously on appeal and regard it as a very important factor.” Two severe flood warnings remained in place on Tuesday night, in Hereford and Ironbridge where homes were evacuated as the River Severn was expected to reach near-record levels. Residents were also evacuated in the town of Snaith, east Yorkshire, after the River Aire burst its banks. There were 250 flood warnings and alerts in place across England, from Devon to Cumbria, with a further 10 in Wales following one of the wettest Februarys in 254 years of records. The government has come under pressure over its aims to build 300,000 homes a year by the mid-2020s to help ease a chronic shortage across the UK. Local authorities say they are struggling to meet these demands because of a shortage of available land, leading to one in 10 of new homes in England being built on high-risk flood sites since 2013. The Guardian revealed on Sunday that more than 11,000 homes were planned in areas the government considers a high flood risk in the seven English regions swamped by Storms Ciara and Dennis. The government says its planning policy is clear that housing should be located in the areas least at risk of flooding and, when development in a risk area is absolutely necessary, “sufficient measures should be taken to make sure homes are safe, resilient and protected from flooding”. However, a series of experts, MPs and local authorities have said that these new developments often increase the flood risk to surrounding areas because water that would be otherwise absorbed by the land instead runs off more quickly into rivers that then burst their banks. The Tory MP Laurence Robertson said two huge housing developments were under construction in his Tewkesbury constituency, comprising 2,500 homes, and that one of them was currently under water. “All of that [new housing] is just going to make [the flooding] so much worse,” he said. “I don’t think there’s been any adequate demonstration that they can contain the water in the new buildingwork. I’ve got other examples in my constituency where houses have been built, particularly on slightly elevated land, which throw the water downhill. They suffer [in surrounding areas] and that’s what I fear.” Kieran Mullan, the newly elected Conservative MP for Crewe and Nantwich, said housebuilding on floodplains was “asking for trouble and I would need a lot of convincing to find that is ever justified”. He said a recently built housing estate had caused a road in his constituency to flood and that residents’ concerns about the dangers had been ignored. “It is no surprise concreting over fields can make localised water drainage worse,” he said. Ahead of a speech in London on Tuesday, the head of the Environment Agency, Sir James Bevan, said properties should not be built on the floodplain “as far as possible” and that some developments should never have been approved. Speaking on BBC Radio 4’s Today programme, Bevan also raised the possibility that some vulnerable communities on the coast and in river valleys may have to move to avoid repeated flooding. He said those communities should not be “forced out” but that there needed to be a conversation about how they can be protected in the long term. In a speech at the World Water-Tech Innovation summit in central London, Bevan said it was unrealistic to ban all housebuilding on floodplains given England’s geography. However, he added: “The clue is in the name: floodplain. So we can and should insist that development only happens there if there is no real alternative, that any such development doesn’t increase other people’s flood risk … and that properties built on the floodplain are flood resilient, for example with the garages on the ground floor and the people higher up.” Labour has called for an immediate end to building on land considered to be at high risk of flooding, which equates to 10% of land in England. Analysis by the Guardian found that more than 84,000 homes had been built in these high-risk flood zones between 2013 and 2018, with the annual total having doubled in that time. The government also came under fire from the National Farmers’ Union, which blamed a “third world” approach to water management for the devastation. The NFU president, Minette Batters, said the government had done “nothing” in the last eight years to act on its 2012 manifesto involving more reservoirs and a national plan to transport water elsewhere to the country to meet needs. “Years of neglect has created an urgent problem,” she said, adding that farmers’ efforts with wood dams or the introduction of beavers to naturally manage water flow were part of the solution."
"**""It's going to look horrible."" The simple truth about the Spending Review according to a senior MP.**
The chancellor will bang the drum for his plans to keep people in jobs, or help find new ones. Rishi Sunak will take out the metaphorical megaphone to explain how he'll allocate billions of taxpayers' cash to spend on infrastructure in the coming months.
But the headlines of the Spending Review, when governments put their money where their mouths are, won't be in any rhetorical flourishes at the despatch box, nor likely in any surprise announcements kept back as goodies for the public.
What may shock, is the cold reality of the cost of the coronavirus, which will be laid bare in the tables and charts published at the same time, presenting to the country in the shape of statistics from the Office for Budget Responsibility how much damage the pandemic has really done to how we make a living.
Without poring over the spreadsheets, the ""horrible"" will mean a massive gap between what the government takes in tax and what is has been spending, a deficit more than ten times what it was last year.
There will be an estimate of the number of people who may end up unemployed, perhaps nearing three million before too long.
It's likely to mean a freeze on pay for much of the public sector; a cut, even if temporary to the amount of cash the UK spends on foreign aid; tight spending limits for government departments on their day-to-day spending and eye-watering levels of debt and borrowing.
One former Treasury minister, who is not prone to hyperbole (unusually for a politician you might wonder) described it as a ""multigenerational debt which will have implications for the rest of our lives in terms of what the British state can afford"".
We will on Wednesday, they suggest, ""learn a great deal about the problem"", what months of emergency spending has done to the economy. But before we go on, don't hold your breath to learn much about any solutions.
The chancellor and prime minister have decided politically that while budgets will be tight (and let's see the black and white to assess this for real) there can't be a return to the kind of squeeze of the Cameron and Osborne era.
No one in government would pretend in private there is any way to avoid tax rises at some point. But Mr Sunak is not going to announce any of that on Wednesday - any big ways of raising money to fill the hole won't come until the Budget next year at the earliest, and perhaps not until after that.
But Wednesday's review will sketch out the very, very serious challenge for the country's finances that is on the way.
Most importantly of course that will be reflected in the number of people who might lose their jobs with all the distress that entails, all the business that could be lost, and the impact on people's pay packets. But it also sets the backdrop for the decisions that our politicians have to make, and will be confronted with for many, many years to come.
It's notable that while there have been some skirmishes around the edges in the last nine months, there has been very little tension over the government and the Bank of England's central actions to write enormous cheques, and keep the signatures coming as the pandemic has progressed.
And it's far from over. But as time goes on the exit from the emergency leaves the government with extremely difficult political decisions.
There is no appetite to break any of the prime minister's expensive manifesto promises.
New Tory MPs, particularly in new Tory seats are chomping at the bit for evidence to show to their constituents they made the right decision.
One former minister said, ""our voters want something tangible they can see at the end of their street,"" and they want it fast.
But the chancellor also, according to his allies, says ""we have to be the party of looking after people's money - he says, if we lose that, why don't you just vote Labour?""
The argument works the other way too, to an extent. If the Tories are racking up levels of public spending that are previously unimaginable, the traditional gap in economic vision doesn't leave Labour with that much space.
How and when will either of the main parties try to confront what has really gone on as the cost of trying to deal with the pandemic has gone up and up and up?
Some ministers worry even many MPs haven't yet understood the real consequences for how we make a living - the damage the decisions made to protect the country during the emergency of the pandemic have had on the economy.
But Wednesday will be the first time, eight months in, when we will be confronted with the size of the likely bill. The argument about who and how to pay will dominate for many years to come."
"
Share this...FacebookTwitterThe warmist German klimaretter.info site has a piece about sequestraton of carbon dioxide, which reports that Swedish power company Vattenfalls plans have a CCS plant near Berlin ready by 2015.
The plant would remove CO2 from Vattenfall’s brown coal power plant and pump it into the earth for high-pressure underground storage. But an expert geological assessment shows that could lead to problems. According to klimaretter.info:
Storing carbon dioxide underground could however have negative impacts beyond Brandenburg. A geological expert assessment for the community of Barnim-Oderbruch made available to klimaretter.info states that because of the overpressure in the bedrock strata, a salinisation of groundwater has to be expected within a radius of 100 km from the injection borehole. That would affect Mecklenburg Western Pommerania and Poland.
That means the entire Berlin metropolitan area would be impacted. Geology expert Ralf Krupp studied the underground geology in the area and concludes that the ground structure may not be able to securely store the CO2 because the 20-meter salt layer is not thick enough, and so fears that the high pressure could lead salt water carrying strata to mix in with drinking water – causing it to become saline. Kilmaretter also writes:
Especially problematic for Krupp is that saltwater probably is laden with heavy metals. ‘This could be an acute hazard for many water utilities,’ the geologist descríbes.”
In the meantime Vattenfalls calls such scenarios “purely speculative” and that there a number of technical factors that have to be considered. Water utility companies, however, find the scenarios plausible and not without risk.
In the meantime, the uncertainty is already having a powerful impact on public opinion. Activist and cititens groups are already mobilising to stop the CCS technology from being employed not only near Berlin, but at a number of locations throughout Germany. So add another technology that is too risky to be used – along with nuclear power, GMO’s, high speed trains, coal power plants, shale gas, oil, internal combustion engines, bottled water, fireplaces, toilets…
Reading up on CCS technology, I find that it involves a lot work (consumption of energy) and will provide no benefit. Seems to be yet another superstition-driven folly. Watch this Alberta video on how it works: http://www.youtube.com/watch?v=R0i6dhEPSwU.
Share this...FacebookTwitter "
"

One of the oft‐​encountered talking points offered by the Left is the extent to which the Bush administration has alternatively ignored, intimidated, and done violence to the scientific community. The picture being painted is that of a know‐​nothing Christian fundamentalist in the thrall of corporate America waging unremitting war against the Enlightenment.   
  
  
While there is enough truth to this charge to give it legs, the “science” lobby is scarcely blameless. For all the moral and ethical posturing surrounding the sanctity of “the scientific process” and the need to keep the same safe from assaults by power‐​hungry politicians and ignorant political mob action, climatologist James Hansen’s recent call to literally criminalize disagreement with him about climate change is a more radical assault on the the scientific process and the scientific method than anything forwarded by the Bush administration.   
  
  
Now, James Hansen would probably argue that he’s not interested in criminalizing disagreement _per se_ ; he’s interested in criminalizing dangerous, life‐​threatening speech that the speaker _knows_ is fraudulent. Perhaps. But exactly what is the nature of this special mind‐​reading power that allows James Hansen to determine that Rex Tillerson, head of ExxonMobil, believes X but says Y? Is it so beyond the realm of possiblity to think that Rex Tillerson actually _believes_ what he says (pace, say, commentary by our own Pat Michaels on the subject)? Or does James Hansen presume to know Pat Michaels’ true and secret thoughts as well?   
  
  
To the extent that James Hansen’s views are embraced by the self‐​appointed gendarmes of science, politicians are right to suspect that climate change alarmism is heavily influenced by the lust for power, the demands of ego, and the pursuit of political agendas that go far beyond a disinterested search for scientific truth. Moreover, one can’t help but wonder about the strength of an argument that requires the threat of force to silence critics.   
  
  
Call me an idealogue, but criminalizing skepticism about scientific theories is probably not the best way to facilitate the quest for scientific truth.
"
"The amount of water at the Earth’s surface is pretty constant, but in many parts of the developed world we are running out of the right sort of water, and our ability to access it. The severe water shortages experienced in California and the southwestern US, in Australia, and even parts of the UK show we need new methods for ensuring a clean water supply. One is to produce high quality water from wastewater, something that is improving all the time. While this could help relieve the strain on water supplies, public attitudes to the idea of using water that is recycled from sewage and other wastewater streams for drinking and domestic use is the more significant barrier. The treatment and reuse of “grey” water (waste from baths, showers, washing machines and so on) for non-drinking uses such as irrigation is already widespread. But as the demand for water grows and supplies continue to dwindle, more and more attention is being paid to “black” water – in simple terms, sewage.  Technological advances and environmental regulations have made the production of very high quality water from black wastewater streams not just feasible, but increasingly an economic and political necessity. The challenge facing water engineers now is arguably just as significant: convincing the public to accept sewage water recycled in this way for mainstream domestic consumption. Let’s be clear. Untreated sewage is dangerous stuff, responsible throughout history (and all-too-often still today for many communities worldwide) for more deaths, disease and misery than pretty much any other single cause.  Industrial wastewater treatment is rightly considered one of the wonders of the modern world. Customers of modern water utilities companies expect reliable, high-quality water supply and removal as a given, to the extent that the majority have no idea where their water comes from, or goes to. In practice, of course, wastewater discharged into the environment from one community has long become the source water for another community downstream – think Oxford, to Reading, to London in a chain along the river Thames. Urban myths about the number of people who have already tasted a Londoner’s tap water are deeply ingrained and somehow accepted. But when asked directly about the acceptability of recycled wastewater as a direct feed into potable supplies, attitudes harden. In an Oregon State University survey in 2008, while a majority supported a specific water recycling proposal in principle, the percentage of people strongly agreeing with potential applications dropped to as low as 13% for uses associated with human contact or consumption, from around 55% for other industrial and municipal uses.  In a 2013 poll for The Guardian newspaper, 63% of respondents claimed they would drink recycled sewage water, but the context was broader and the question more hypothetical than in the Oregon study.  This psychological factor is important: like the fly in your soup, we are put off when a problem is placed close at hand. The key is to add steps in the process – discharging treated wastewater to the river before abstracting it again for drinking. A 2012 Southern Water study suggests this approach would be acceptable, if the quality could be guaranteed. Recent evidence on the prevalence of antibiotic-resistant microbes in treatment plants highlights the need for ongoing technical development to combat emerging threats to health and environment. Other concerns lie around persistent organic pollutants such as pharmaceuticals, which may be concentrated by repeated recycling of black wastewater. In striving to introduce recycled water systems, water engineers face the challenge of tackling real and perceived threats to water quality, mistrust of commercial utilities and government authorities, and a deep-rooted fear of contaminated water. Ironically, climate change could be part of the answer. Wichita Falls, Texas, became in July 2014 the first place in the world to implement 50:50 mixing of directly recycled wastewater in domestic supplies. Residents are largely philosophical about their “potty water”, but then they’re experiencing the worst drought in 70 years with extreme restrictions on water use. In Wichita Falls, it’s state politicians and regulators rather than consumers that are the largest hurdles the scheme must jump. Water resource managers occupy a shifting landscape between technological capability, political precaution, and public attitudes which can swing strongly and quickly. Navigating this difficult terrain while introducing engineering answers that work is complex, but the evidence suggests that trust is key to public acceptance.  In California, Israel, Australia and Singapore, environmental concerns, price incentives, fines and even national security have been used to convince people of the need to adopt wastewater recycling. Information campaigns, celebrity endorsements, aggressive branding and collaboration with trusted independent organisations are designed to reduce the yuck factor.  In the final analysis however, necessity and urgency are the most effective levers of opinion, as Wichita Falls appears to prove. Perhaps the real challenge for water engineers is to find a way to secure the infrastructure for resilient, sustainable water supplies almost behind the scenes, ready to press the button when circumstances drive public and politicians to accept the unacceptable."
"“Nurdles” may sound cute but they pose a huge risk to the marine environment. Also known as “mermaid tears”, these small plastic pellets are a feedstock in the plastic industry. Instead of being converted into household items, many end up in the ocean, collecting toxins on their surfaces and being eaten by marine wildlife. Not so cute now, are they? Nurdles are the building blocks for most plastic goods, from single-use water bottles to televison sets. These small pellets – normally between 1mm and 5mm – are classed as a primary microplastic alongside the microbeads used in cosmetic products – they’re small on purpose, as opposed to other microplastics that break off from larger plastic waste in the ocean. The small size of nurdles makes them easy to transport as the raw material which can be melted down and moulded into all kinds of plastic products by manufacturers. Unfortunately, mismanagement of these little pellets during transport and processing leads to billions being unintentionally released into rivers and oceans through effluent pipes, blown from land or via industrial spillage. “Mermaid tears” is an appropriate nickname when we consider the potential harm that nurdles have on marine life. Their small size, round shape and array of colours make them attractive food – easily mistaken for fish eggs and small prey. This “food” has an extra problem – it comes with a side of noxious chemicals.  The large surface area to size ratio and polymer composition of the nurdle pellets allow persistent organic pollutants (POPs) in seawater to build up on their surfaces. These toxins then transfer to the tissue of organisms which eat them.  The problem is in the name – POPs are “persistent”, meaning they don’t go away easily and can remain on the surface of nurdles for years. Nurdles can also be colonised by microbes that are dangerous to humans. A study investigating nurdles on bathing beaches in East Lothian, Scotland, found that all five beaches tested had nurdles that were covered with E. coli – the bacterium responsible for food poisoning.  Nurdles can be so noxious that people cleaning beaches or recording pellets in scientific surveys are advised not to touch them with their bare skin – which makes sun bathing on many beaches in the summer an unattractive prospect. So how many nurdles are out there in the ocean and on coastlines? It’s estimated that up to 53 billion nurdles are released annually in the UK from the plastic industry. That’s the same amount of nurdles that it would take to make 88m plastic bottles. So why are nurdles rarely discussed in the plastic pollution debate? Luckily, there are organisations raising awareness of nurdles and their prevalence in marine pollution. The Great Global Nurdle Hunt started by Fidra – a charity based in Scotland that addresses environmental issues – and the Marine Conservation Society encourages people to become citizen scientists and gather data on how common these pellets are on beaches around the world.  Data collection helps identify the main sources of this pollution from the plastic industry, which can use the information to improve management of the problem. As there are so many nurdles present in the environment, it takes an army of people to gather information about them. The Hunt takes place over ten days in February each year.  Citizen scientists log their nurdle findings onto a global map that shows the extent of nurdle pollution worldwide and how it’s changed over time. Since 2012, the number of beaches being searched has reached 1610 across six continents, 18 countries and with over 60 organisations involved. This year, Staffordshire University’s Microplastic and Forensic Fibre Research Group took part in efforts to estimate the concentration of nurdles on Hightown beach in Liverpool, UK. An average of 139.8 nurdles per square metre were found. That’s around 140,000 nurdles over 1km of hightide line.  If you’d like to become a citizen scientist and collect nurdle data at your local beach, there are a few useful tips. Have a look at one of the online nurdle ID guides online so that you don’t mistake a polystyrene ball, BB gun pellet or ancient fossil for a nurdle. Make sure to check seaweed and other marine debris when on the beach – these act like large nurdle nets. Once you’ve collected data, don’t forget to submit your findings to a suitable survey so that that they can be used to fight the pollution problem. And if you don’t live near the coast, don’t worry – nurdles have been found in most environments, including rivers, lakes and even far inland and away from water. We even found them in soil in our campus. So let’s get nurdle hunting – but don’t forget your gloves."
"**As much as Â£1bn in benefit fraud has been prevented from being paid to organised-crime groups in recent months, BBC News has learned.**
But before the scam was spotted, officials unwittingly confirmed thousands of stolen identities.
Fraudsters took advantage of looser rules introduced to cope with a surge of universal credit claims during the pandemic.
BBC News has asked the Department for Work and Pensions for a response.
In May, a junior civil servant working with High Street banks noticed dozens of claims for universal credit had been made asking for money to be paid into the same bank account.
Further investigation identified more than 100,000 fraudulent claims.
And officials admit they had confirmed thousands of people's identities to the gangs that had stolen them - and passed on their National Insurance numbers.
The Department for Work and Pensions wants to write to those whose data has been compromised.
But BBC News has learned it is struggling to identify many of them and is wary of sending out letters to last known addresses in case they end up in the wrong hands, exacerbating the data breach.
Claimants whose identities have been stolen can face real hardship, as it can be months before their accurate benefits are paid.
Currently, 5.7 million people receive universal credit, almost double the figure for March.
To cope with the surge, identity checks were processed online, rather than face-to-face, and information such as the cost of rent and whether someone had been self-employed taken on trust.
Criminal gangs have attacked several government Covid-19 schemes.
DWP officials have asked the Treasury for Â£200m over three years, in this spending round, calculating it would enable it to prevent such mass scams and save taxpayers about Â£500m each year.
It is estimated more than a million claims for universal credit have still to be properly checked, with additional rising concerns tens of thousands of people may have claimed the benefit without declaring they had received government grants to help the self-employed.
However, the Treasury has turned down the request."
"
Here is the current Pacific satellite image, note the lower right.
Click for a larger image
I might add that the likelihood of a hurricane strength storm striking Southern California is low. Since 1900, only four tropical cyclones have brought gale-force winds to the Southwestern United States. They are an unnamed tropical storm that made landfall near San Pedro in 1939, the remnants of Hurricane Joanne in 1972, the remnants of Hurricane Kathleen in 1976, and Hurricane Nora in 1997 which entered California as a tropical storm.
The storms that do make it close enough to be a threat are often weakened by two facts: cold sea surface temperatures and upper level steering winds that tend to take them away for SoCal. But it’s a fun exercise to discuss the possibility. – Anthony
Hurricanes in Los Angeles?
Guest post by Roger Sowell

A hurricane hitting Los Angeles. No, it hasn’t happened yet, but it could. I am using the same reasoning as the Carbon Is Going to Kill Us crowd, where it is deemed prudent and even mandatory that we take action now to prevent a future catastrophe. AGW believers insist that all mankind (well, except for developing countries, of course!) curtail or stop altogether emitting carbon dioxide, as that may cause ice caps to melt and oceans to rise and population disruption.
There is a hurricane in the Pacific Ocean, headed directly toward Los Angeles. It’s name is Jimena (pronounced him -ay – nuh, accent on the ay). Jimema currently has winds of 135 miles per hour, and is just south of the tip of Baja, California. Its course is to the northwest, up the Baja peninsula.
Judging from the mass confusion a couple of years ago when Houston evacuated ahead of hurricane Rita, Los Angeles might want to start packing and driving today. Houston only had around 1 million people exiting the city, and had at least five freeways on which to do it. Los Angeles has approximately 3 million people, probably closer to 4 million, but the metropolitan area has 18 million, and only three ways out. There is the Interstate 10, going due East; Interstate 5 going North; and highway 101, also going north. I-5 also goes south, but little good that will do since one runs into San Diego and the hurricane.
A hurricane hitting Los Angeles. We must take prudent steps to avoid the certain disaster and destruction from a hurricane. We will not be required to wait 100 years for the results to be in. This hurricane will be here in less than 10 days. We must act today, while there is still time. The science is settled. Hurricanes hitting major population centers are a serious threat. Remember New Orleans and Hurricane Katrina. Houston and Hurricane Rita. We must mobilize FEMA so they can get their red tape all in order, ready to send trailers and water and food packs to Los Angeles.
The low-lying areas of Southern California are at risk of inundation from the storm surge. Ports and river basins will be swamped with seawater, causing un-told devastation to precious seashore that is a national treasure, as the California Coastal Commission regularly reminds us. A storm surge from a hurricane can be several feet. The California Coastal Commission was in a tizzy recently over the prospect of the ocean rising just one foot, in the next century. Where is the alarm, the hysterical and frantic activity, over a storm surge of 5 to 10 feet in the space of 24 hours?
Where is the clarion call to action from our state and city leaders? Governor Schwarzenegger, Mayor Villaraigosa, are you watching this hurricane? Have you prepared the state and city and county to deal with this?
Or, are you hoping the hurricane does arrive, and right away, so that the wildfires will finally be put out and the firefighters get some much-needed rest?
Stay tuned, sports fans.  This is about to get interesting.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e93cef8c5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Congress finally passed a stimulus package after making sure to strip out elements that would benefit the economy over the long run. The plan includeed an accelerated depreciation provision that will be beneficial if it is later made permanent. But a new corporate tax survey by KPMG makes clear that this is only the first of many needed business tax reforms in the United States.



KPMG found that the United States has the fourth highest corporate income tax rate in the 30‐​nation Organisation for Economic Co‐​operation and Development (OECD). The combined U.S. federal and average state rate of 40 percent is almost 9 percentage points higher than the average OECD top corporate rate of 31.4 percent.



This is a dramatic reversal of the U.S. tax situation. After cutting the federal corporate rate from 46 percent to 34 percent in 1986, policymakers fell asleep at the switch, perhaps assuming that we had claimed a low‐​tax advantage permanently. But most industrial countries followed the U.S. lead and cut tax rates in the late 1980s. Then another round of tax rate cuts began in the late 1990s, with the result that the average OECD corporate rate fell from 37.6 percent in 1996 to just 31.4 percent by January 2002 (see chart). The average corporate rate in the European Union is now 32.5 percent, down from 38.2 percent in 1996.





We sometimes write‐​off European economies as uncompetitive welfare states, and ignore that many countries have improved their business climates. In fact, a recent study by the Economist Intelligence Unit placed the United States second, behind the Netherlands, for the “best place in the world to conduct business.” And a study by GrowthPlus, a European think tank, compared 10 major countries to determine which had the best environment for entrepreneurial growth companies. Again, the United States finished second, this time behind Britain. 



In the last few years, the corporate tax rate was cut in Denmark, France, Ireland, Germany, Poland, and Portugal, and in many countries outside of Europe. Even socialist Sweden has a top corporate tax rate of just 28 percent. It is certainly true that overall European taxes, as a share of gross domestic product, are much higher than in the United States. But Europe has shifted about one‐​third of its overall tax burden to less distortionary consumption taxes.



What the Europeans and others are realizing is that countries shoot themselves in the foot by imposing high tax rates on mobile capital. IMF data show that annual global portfolio capital flows rose six‐​fold during the past decade. United Nations data show that global direct investment also rose six‐​fold during this period. The U.S. attracts a big share of these flows because of its large economy, stable currency, and strong growth. But investment flows are increasingly sensitive to taxes, so it makes less and less sense to have a high corporate rate. After all, last year’s recession, the Enron collapse, and the high‐​tech bust all show that the U.S. business sector is not as invincible as it seemed in the late 1990s. 



A high statutory rate isn’t the only aspect of U.S. business taxation that needs reform. Aside from making the new depreciation rules permanent, we need to switch to a territorial tax system from the complex worldwide system that makes U.S. firms less competitive abroad. Glenn Hubbard, chairman of the Council of Economic Advisers, has noted that “from an income tax perspective, the United States has become one of the least attractive industrial countries in which to locate the headquarters of a multinational corporation.” As a consequence, there has been a “marked increase” in the number of U.S. firms reincorporating abroad, according to a new U.S. Treasury analysis. Shouldn’t the U.S. be trying to attract business rather than drive it away?



The critics of course will say that big corporations and their shareholders should pay their “fair share” of taxes, and that the government needs to crack down on tax‐​avoiders like Enron. Such views ignore big picture realities. First, the huge rise in global capital flows means that the corporate tax burden probably falls more on immobile workers, and less on the mobile capital income it is ostensibly placed on. Second, the high corporate tax rate is the reason why Enron and other firms go to such wasteful lengths to avoid and evade taxes. If the U.S. cut its corporate rate to say 20 percent, not only would real capital investment increase, but firms would financially restructure in order to shift more of their global tax base into this country.



As the world economy changes, so must U.S. tax policy. Pressures to attract mobile capital through international “tax competition” will continue to increase. These trends dictate that we reform our tax system by moving away from a high‐​rate income tax system to a low‐​rate consumption‐​based tax system. 
"
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
Share this...FacebookTwitterPerhaps the images of angry mobs in Greece and forecasts of a bitter cold winter for Europe are having a sobering effect on our CO2-drugged up politicians here in Europe.
And as other countries line up behind Greece on the path to discontent, perhaps its a good idea, after all, to stop raking honest citizens with the global warming scam.
At least EU Energy Commissar Günther Oettinger may be getting it.
The online Stern magazine reports here that Oettinger has a warning on the dangers of rising energy costs.
Higher energy costs do not only threaten the businesses here, but also the social peace when a part of the population is not able to afford paying the electric bills.”
Yeah, it can really piss people off when the power gets shut off in the middle of winter. Oettinger, at an international business conference last Saturday, added:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I’m surprised at how thoughtlessly the development of the electric prices have been taken. This is one reason for the steady de-industrialisation – energy intensive production is being sent overseas.”
And most the production that is done overseas uses much more energy and entails higher greenhouse gas emissions. But who cares, Germany can pat itself on the back for its contribution in rescuing the planet from greenhouse gases.
We need to recall here that Oettinger is likely only speaking the words his audience wants to hear, and that tomorrow he’ll be at some environmental conference and will be spewing just the opposite-  on how “green” jobs are the future. It’s this kind of dishonesty and insincerity that really fuels the mobs out there. So let’s look at what Oettinger will say tomorrow.
Oettinger also provided a few other wise words during his rare few minutes of good sense. Stern writes:
For this reason Germany has to reconsider its subsidies in solar energy. ‘The Energy Feed-in Act was clever, but there comes a time when we have to wake up and realize that the sun shines brighter and longer in other countries,'”
Just come out and say it, Herr Commissar, solar energy was a flop.
 
Share this...FacebookTwitter "
"

News loves hurricanes. They usually form far, far away, providing at least a week of stories. And they often start with a bang. Down in the tropical Atlantic, young ones bomb out to amazingly low barometric pressures and outrageous sustained winds. Hurricane Ivan’s lowest pressure, for example, would cause the needle on you home barometer to spin around twice. The resultant “eyewall” winds were a 20‐​mile wide tornado.



It’s incredible stuff. But they usually weaken considerably by the time they get to the states, owing to our more northerly latitude and the fact that hurricanes don’t do well when much of their circulation is over land, which has to happen when they approach North America.



That doesn’t stop the hype machine. While we like to count up property damage and losses, no one mentions the fantastic revenue that these storms generate for the media, or that the constant drumbeat of Charley‐​Frances‐​Ivan, Charley‐​Frances‐​Ivan must have political repercussions.



And so, Tony Blair was just in Washington to visit John Kerry, where he conflated Hurricane Ivan with dreaded global warming. 



I like just about everything about Tony Blair. He’s smart, affable, and a real friend to a nation that needs some. But he’s way off on global warming, and advising Kerry to bail out his campaign with apocalyptic climate hype invites a grilling by the climate truth squad, a rather large body of weather nerds in a weather‐​fixated country.



Blair’s problem is that he listens to his science adviser, Sir David King, who is one of the most ill‐​informed hawks on climate change on this greening planet. King actually pronounced the goofy global warming flick “The Day After Tomorrow” as scientifically plausible, which should have completely blown away his credibility. Now he claims that this year’s hurricane activity is a product of global warming and that warming will make hurricanes worse.



Here’s the simplistic argument. Hurricanes require warm water. Global warming means more of that. Therefore, more hurricanes. 



The fact is that there’s plenty of warm water for hurricanes every year–virtually the entire tropical ocean is hot enough, and yet there are only about 10 per year in the Atlantic. The real research question on these storms is not why there are so many but, rather, why there are so few, given the massive expanse of warm water available to them. 



And here’s the real scientific inconvenience in Blair’s story. The planet warmed slightly–much less than forecast by people like King–in the last half of the last century, but while that happened, maximum winds in Atlantic hurricanes DECLINED significantly.



Yep. As shown by scientist Chris Landsea of the National Oceanic and Atmospheric Administration, maximum winds measured by hurricane‐​hunter aircraft over the last 50 years have declined significantly.



Further, there’s a logical (if lawyerly) argument that pins this salutary change on global warming. It goes like this: Atlantic hurricanes are much more delicate than their destruction suggests. One thing they cannot tolerate is a west wind blowing into them because it wrecks their symmetry. As a result, their maximum winds decline.



El Niño–another climate hype machine–generates precisely this type of wind over the Atlantic. That’s why, in El Nino years, the forecast is for a weak hurricane season.



In the latter part of the last century, there were an unusual number of El Niño years compared to previous decades. Some scientists (like David King) claim that global warming is increasing the frequency of El Nino. But if that’s the case, then global warming would be responsible for the decline in maximum hurricane winds.



How much could that be worth? The decline has been about 15 mph since 1950. That’s not a small number because the force of a hurricane’s wind goes up with the square of the velocity. In the high Category Three/​low Four range, this change reduces the power by 25 percent. Given that the U.S. experiences about 15 strong hurricanes every decade, and that the average cost is now about $5 billion for one of those hits, you could, if you buy the El Niño argument (I don’t but some others do), thank global warming saving about $13 billion per decade.



These numbers won’t stop the hype machine on hurricanes. But you’d think that Great Britain’s science adviser would have been sufficiently well informed that he would have kept his prime minister from asking John Kerry to sow the whirlwind.
"
"
Share this...FacebookTwitterBy Ed Caryl
Pierre and I have both written about the effects of soot on Arctic and sub-Arctic ice and glaciers, read here Glaciers – The Dark Side and Half Of Arctic Warming Caused By Soot. Scientists are recognising that CO2 is becoming less of a factor and that black carbon soot instead is being recognised increasingly as the a driver of warming in the Arctic. Time to go back and revamp the models – again.
Source: http://atmoz.org/blog/2007/06/12/global-melting-big-thaw/
Researchers from the Arctic Council, representing the eight countries that border the Arctic, are now seriously studying the subject. See the Associated Press article here. The photo and caption from the article reads (emphasis added).
This undated handout photo provided by NOAA-STAD, Soot Transport and Deposition Study, shows Trish Quinn of NOAA in a first snow pit. An international research team is in the land of snow and ice in search of soot. Though the Arctic is often pictured as a vast white wasteland, that can be deceiving. And carbon deposited there as a result of activities elsewhere can have a long-term impact on climate (AP Photo/NOAA-STAD).
‘The Arctic serves as the air conditioner of the planet,’ explained Patricia Quinn of the National Oceanic and Atmospheric Administration, one of the research participants. Heat from other parts of the Earth moves to the Arctic in the circulating air and ocean water, and at least some of that warmth can radiate into space.”
At the same time, some of the incoming heat from the sun that tends to be absorbed in other locations is reflected by the ice and snow, allowing the polar regions to serve as cooling agents for the planet.”
Of course they need to insert the obligatory nod to CO2.
Cutting carbon dioxide and other greenhouse gases is the backbone of any effort to combat warming, both globally and within the Arctic, Quinn said.”
The group has not published yet, as the research will not end until the end of May, and to have any hope of getting their paper or papers past “peer review” they need to get CO2 into the mix. But this study is a few steps in the right direction.
Share this...FacebookTwitter "
"Curious Kids is a series by The Conversation, which gives children of all ages the chance to have their questions about the world answered by experts. All questions are welcome: you or an adult can send them – along with your name, age and town or city where you live – to curiouskids@theconversation.com. We won’t be able to answer every question, but we’ll do our best. What forms a current under water? – Natalie, age 11, Melksham, UK. Thanks for your question, Natalie. Underwater currents can form in lakes, rivers and oceans, and there are many reasons why they happen. Since I’m an ocean scientist, I’m going to explain the currents you find in the sea.  Some ocean currents are very large, and the biggest one – called the “global conveyor belt” – moves water very slowly all the way around the world. In fact, it takes water in the global conveyor belt about 1,000 years to get right around the planet.  Because the global conveyor belt and other big ocean currents move so slowly, we don’t notice them when we go to the beach. But we might feel some other types of currents when we go for a swim. When ocean waves get to a beach, they turn white at the top and crash onto the sand – this is called “breaking”. Swimming or surfing in breaking waves can be good fun, but we need to remember that these waves cause currents to form.  When waves break on the shore, the sea water in them gets pushed up against the beach. This water must get back out to sea somehow, otherwise we’d expect the water level at the beach to rise and rise forever. Of course, the water can’t get back out to sea near the surface, because that’s where the breaking waves are busy moving water toward the shore. So, two different currents form, to help take the water back out.  One of these currents is called the “undertow”. It forms beneath the breaking waves, and pulls the water back toward the sea, across the sandy seabed, out past where the waves are breaking.  Though the undertow helps to get some of the water back to sea, it’s not usually very strong. So, some of the work has to be done by another type of current, called a “rip” current. Rips are much stronger, narrow currents that run straight out to sea. Rip currents don’t happen all the way along the beach. They only form at certain “weak spots” along the beach where waves are not breaking, and the water is a bit deeper. This makes it easier for the water to flow back out to sea.  Here’s how it works: after water is brought in toward the shore by breaking waves, it can’t turn around and go straight out again, so it runs sideways along the beach in what we call a “feeder current”. As soon as it finds a weak spot, where the waves aren’t breaking, the water flows back out to sea in a rip current.  It’s very handy to know how to spot a rip current when you go to the beach, because they are much stronger than undertow currents and can sweep people out to sea. 


      Read more:
      Rip currents are a natural hazard along coasts – here's how to spot them


 When there are lots of waves breaking on the beach, it’s tempting to swim in places where the water looks calmer. But we know that rips form at the places where the waves aren’t breaking – so this is actually the worst place to swim!  Rip currents sometimes leave another tell-tale sign: because they’re so strong, they can churn up the sand on the seabed, making the water look brown and murky. Even if we know how to spot a rip current, it is always best to swim at beaches where there is a lifeguard, because they’re specially trained to know the best places to swim, and will always be on the look out to make sure everyone is safe. More Curious Kids articles, written by academic experts: How does heat travel through space if space is a vacuum? – Katerina, age ten, Norwich, UK. What makes a shooting star fall? - Katelyn, age seven, Adelaide, Australia. Is there a place in the middle of the English Channel where the waves change direction? – Sebastian, age 12, Kent, UK."
"
Share this...FacebookTwitterI’ve been writing lately on the folly that is the Green Economy, citing a number of examples here in Germany, read here for example.
Walter Russel Mead at The American Interest has a must read on how the once much ballyhooed Green Jobs Initiative stands in the USA today. It isn’t pretty.
Feeding The Masses On Unicorn Ribs
by Walter Russell Mead
Besides healing the planet and returning the rising seas to their natural beds, then-Senator Obama promised that his administration would create beautiful green jobs: well paid, stable, abundant jobs, unionized, with full benefits and making the earth healthier and the American people richer. As President, he stayed on message: even after the truther-enabling “green jobs czar” Van Jones left the administration, green jobs have been one of the President’s signature policies for putting the American people back to work. Continue reading here…
Some excerpts from Mead’s article:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Obama promised to create 5 million green jobs within ten years.”
and
…New York Times has also figured it out that the administration’s green jobs initiative is an embarrassing mess.”
and
What worries me is that they didn’t understand that making something this bogus a central plank of his actual governing plan on an issue as vital as jobs would have serious costs down the road. Many liberals want green jobs to exist so badly that they don’t fully grasp how otherworldly and ineffectual this advocacy makes the President look to unemployed meat packers and truck drivers.”
Read the entire article for more.
Share this...FacebookTwitter "
"

“Donald Trump is undermining the rules‐​based international order.” _The Economist_ ’s headline last summer summarized a common refrain within America’s foreign policy establishment. Trump “wants to undo the liberal international order the United States built,” Thomas Wright of the Brookings Institution warned on Inauguration Day in 2017. Trump could “bring to an end the United States’ role as guarantor of the liberal world order,” Princeton professor G. John Ikenberry wrote.



Trump is certainly hostile to what he sometimes refers to as “globalism”: multilateralism, free trade agreements, international institutions, and any international legal regime that could impose constraints on U.S. power. He is antagonistic toward allies and treaties, withdrawing the U.S. from the Paris climate agreement, the Trans‐​Pacific Partnership (TPP), the Iran nuclear deal, the Intermediate Nuclear Forces Treaty (INF), the UN Educational, Scientific and Cultural Organization (UNESCO), and the UN Human Rights Council.



But those excoriating Trump for his disregard for rules and norms rarely mention similar, routine violations of this rules‐​based order by his predecessors. And while the foreign policy establishment is firm in its condemnation of Trump’s “turning away from global engagement,” as Richard Haass of the Council on Foreign Relations put it, their harshest criticisms seem reserved for those few sporadic instances in which Trump tries to jettison lengthy and failed military deployments, as in Syria and Afghanistan, or expresses insufficient enthusiasm for permanent overseas garrisons.





President Trump is not the first president to weaken the international liberal order.



The pundits, practitioners, and politicians that make up the foreign policy establishment have rarely respected the non‐​interventionist principles at the core of the United Nations, an institution exemplifying the liberal rules‐​based international order that the United States helped establish following World War II. Article 2(4) of the UN Charter says “All Members shall refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state…” According to the Charter, which American post‐​war planners helped write, the use of force is illegal and illegitimate unless at least one of two prerequisites are met: first, that force is used in self‐​defense; second, that the UN Security Council authorizes it.



This prohibition against war is not some trivial aspiration. Non‐​intervention is the centerpiece of international law and the United Nations has repeatedly sought to underline its significance. In 1965, the General Assembly declared “No state or group of states has the right to intervene, directly or indirectly, for any reason whatever, in the internal or external affairs of any state.” Again in 1970, it unanimously reaffirmedthe illegality of “armed intervention and all other forms of interference or attempted threats.” In 1981, the General Assembly further specified that the Charter’s “principle of non‐​intervention and non‐​interference” prohibited “any … form of intervention and interference, overt or covert, directed at another State or group of States, or any act of military, political or economic interference in the internal affairs of another State.”



The United States is currently engaged in active military hostilities in at least seven countries, namely Afghanistan, Iraq, Syria, Yemen, Somalia, Libya, and Niger. That tally doesn’t include drone strikes in Pakistan, combat operations in Kenya, Cameroon, and Central African Republic, or other interventions of unknown magnitude. The true number might be closer to 14 countries. The White House is also explicitly threatening U.S. military action to change the regime in Venezuela and against Iran for a host of spurious reasons. Not one of these cases meets the prerequisites for legal military intervention (a plausible self‐​defense case can be made for the war in Afghanistan, but it expired a long time ago).



 **No other state in the international system uses force more than** the U.S. has **.** Throughout the Cold War, the United States used military means to interfere in other countries about twice as often as did the Soviet Union. This doesn’t include interventions below the threshold of military action: from 1946 to 2000, Washington meddled in foreign elections more than 80 times (compared to 36 by the Soviet Union or Russia over the same period). Covert operations to overthrow democratically elected governments, as in Iran, Guatemala, and Chile, were a stapleof U.S. conduct in this period, and according to the Rand Corporation, “the number and scale of U.S. military interventions rose rapidly in the aftermath of the Cold War.” The Congressional Research Service lists more than 200 individual U.S. military interventions from 1989 to 2018, a rate that no other country even comes close to matching.



It’s hard for America to act as the guarantor of a rules‐​based order that it consistently violates. When President Obama condemned Russia’s annexation of Crimea in 2014, saying international law prohibits redrawing territorial borders “at the barrel of a gun,” it was somewhat awkward: The United States did exactly that in the 1999 Kosovo war, which lacked Security Council approval, and successive administrations have similarly supported Israel as it annexes and occupies territory in violation of international law. Secretary of State John Kerry castigated Russia’s territorial grab this way: “You just don’t in the 21st century behave in 19th century fashion by invading another country on completely trumped up pretext.” As it happens, that’s a rather apt description of the Bush administration’s brazenly illegal invasion of Iraq in 2003.



Washington often appeals to international law to justify military action against despots who commit atrocities, as it did when it secured UN Security Council approval in 2011 to bomb Libya. But even there, when the initial use of force was authorized, the Obama administration rapidly exceeded the mandate of the resolution by pursuing what amounted to a regime‐​change strategy. And such appeals to humanitarianism are highly selective: U.S. military power has also been used to assist Saudi Arabia, one of the world’s most regressive authoritarian regimes, commit war crimes and keep an impoverished and largely defenseless population in Yemen under siege.



America’s delinquency isn’t restricted to the use of force. Though 139 other countries have done so, Washington has refused to sign on to the Rome Statute, which established the International Criminal Court. And although the United States has badgered China for violating the UN Convention on the Law of the Sea, which defines maritime rights and responsibilities, the U.S. refuses to ratify the treaty itself. For all the talk of China’s unfair trade practices, the only country that receives more formal complaints about WTO violations than China is the United States—and China does a better job of complying once complaints are made.



The political establishment in Washington has always accepted this unique role for the United States. We’re the policeman of the world. We enforce the rules and therefore assert the right to violate them, even as we (often violently) deny others that same prerogative.



 **Any claim to special privileges rests to some extent on whether** the international community sees it as legitimate.The problem is that America’s increasing disregard for the rules has undermined its legitimacy and that of the order itself: More than any other single nation, its actions determine the basis of international norms. As U.S. foreign policy becomes more transparently lawless, the power of international law to constrain state behavior weakens accordingly. To legitimize the Russian annexation of Crimea, President Vladimir Putin actually citedthe “Kosovo precedent.” In 2016, Chinese officials dismissed U.S. criticisms of Beijing’s human rights record by citing the “notorious…prison abuse at Guantanamo.” The United States, Chinese diplomat Fu Cong told the UN Council on Human Rights, “conducts large‐​scale extra‐​territorial eavesdropping, uses drones to attack other countries’ innocent civilians, its troops on foreign soil commit rape and murder of local people. It conducts kidnapping overseas and uses black prisons.” And when American officials lambaste Iran for backing the Syrian regime of Bashar al‐​Assad despite his use of chemical weapons, Iranian officials frequently remind the world that the United States aided Saddam Hussein while he deployed chemical weapons on a much larger scale.



Our hypocrisy has always been a threat to our legitimacy, but in the past it was often managed with careful rhetoric and diplomatic maneuvers designed to conceal the discrepancy between our words and our deeds, to camouflage our violations in language that reinforced the order or appealed to higher values. Trump is distinct from his predecessors not because his foreign policy is a radical departure, but because he is carrying out similar policies without the moralistic righteousness of his predecessors .



Saving the liberal order means adhering to the UN Charter’s prohibition on the use of force except in self‐​defense or unless authorized by the Security Council. It means rolling back our global military footprint and adopting a more restrained foreign policy that at least approximates the manner in which we expect other nations to behave. It means recognizing that the United States is not exempt from the rules and norms it often punishes others for transgressing, and it means acknowledging that the foreign policy establishment has done at least as much damage to the rules‐​based order as has President Trump.
"
"**An outbreak of Covid-19 has taken place at a meat processing factory.**
Cornwall Council said it was aware of ""a number of confirmed cases"" at the Kepak meat processing factory in Bodmin.
Additional testing is now taking place on site for Kepak employees and the factory on the Cooksland Industrial Estate remains open.
Neither Cornwall Council nor Public Health England would say how many cases had been confirmed.
Rachel Wigglesworth, Cornwall Council's director of public health, said employees who needed to isolate had already been excluded from work and given support to isolate.
She said: ""We have been discussing their ongoing measures to ensure it is safe for staff to attend work as normal.""
Kepac said: ""The group is working tirelessly to protect its staff as well as ensuring the continuity of secure food supply during this pandemic."""
"**Deaths involving Covid-19 have risen again in Wales to the highest weekly total since early May.**
A total of 190 deaths were registered in the week ending 13 November, according to the Office for National Statistics (ONS).
This is 24 more than the previous week and account for a quarter of all deaths in Wales.
Meanwhile, 19 more deaths have been reported linked to hospital infections at Cwm Taf Morgannwg health board.
There have been 10 more deaths at the Princess of Wales hospital, Bridgend, five at the Royal Glamorgan hospital and four deaths at Prince Charles Hospital in the last week.
It takes the total deaths to 177. That includes six deaths at Maesteg hospital reported a few weeks ago.
The number of cases linked to the outbreaks has slowed down, rising from 597 to 628.
The ONS figures show deaths in care homes involving the virus have also risen to their highest total - 36 - for five months.
The proportion of Covid deaths compared to all deaths is higher in Wales than in England in this latest week.
There were 56 deaths registered across the Cwm Taf Morgannwg health board area, which covers Rhondda Cynon Taf, Bridgend and Merthyr. Of those, 44 were in hospital.
There were also 51 deaths in the Aneurin Bevan health board area, across all settings, 29 deaths in Swansea Bay, 27 in Betsi Cadwaladr and 14 in Cardiff and Vale.
There were 10 deaths in Hywel Dda and three hospital deaths involving Powys residents.
The figures show:
So-called 'excess deaths', which compare all registered deaths with previous years, are above the five-year average.
Comparing current figures with the number of deaths normally seen at this point in the year is regarded as a useful measure of how the pandemic is progressing.
In Wales, the number of deaths fell to 742 in the latest week, but this was still 84 deaths (12.8%) higher than the five-year average.
In the latest week, England had 2,274 deaths involving Covid, followed by Scotland (278), Wales (190) and Northern Ireland (96)."
"Converting renewable energy into electricity is one thing; converting it into fuel is quite another. The vast majority of global energy demand is for fuel, and a renewable source could help us heat our houses and travel efficiently long into the future. It might even mean we could avoid the conflicts that will arise while competing for the last remaining fossil fuels. Today, we are a step further towards this goal after engineering the gut bacteria E. coli, most famous for the strain of it that causes food poisoning, to make it generate renewable propane. My colleagues and I detail our work in a study published in the journal Nature Communications. Scientific advances now mean we can make microbes churn out useful energy, by changing the way they process energy. These microbes can then convert the “renewable” sunlight (and carbon dioxide) into fuel, either directly or using sugar as an intermediate stop-over.  Although the technology for renewable conversion of solar energy into electricity already works well, this isn’t quite the same as being “renewable energy”. Approximately 85% of total energy demand is actually for fuels, as it is far easier to store energy in fuel rather than as electricity.  Industrial scale production of cheap renewable fuel therefore runs into a big problem. It needs to out-compete fossil fuels – an alternative technology that only needs to pump out the ready product. In searching for a renewable fuel process that could be economically sustainable we focused on propane, a bulk component of liquid petroleum gas. Propane is an attractive target for several reasons.  It’s a gas, which means you could immediately separate the finished product. The microbes who produced the propane would be left behind and the new fuel will escape as a gas. No need for a messy separation.  That said, propane also requires little energy to liquefy, thereby enabling the high-energy density storage that is required for cost-effective usage. There’s a reason your car’s gas tank is actually full of liquid – gas simply takes up too much room. The fact propane is already in commercial use also helps. It’s used as a fuel in rural areas or in industry, and sometimes also for transport. In Italy, for example, thousands of stations sell propane-containing mixtures under the label “Autogas”. You can’t make renewable propane through natural reactions – no organisms naturally pump out propane in the way humans breathe out CO2 or trees exhale oxygen. We therefore turned to synthetic biology, where biology meets engineering, in order to create such a capability. We chose E. coli because it is easy to engineer. Left to its own devices, E. coli takes glucose from its surroundings and breaks it down into smaller carbon molecules, electrons and “internal” chemical energy. These smaller parts are used only as building blocks for cellular growth – to reproduce. In the engineered cells, however, we hijack the assembly line for one of those building blocks known as “fatty acid synthesis”. Fatty acids are normally synthesised mainly in order to generate cell membranes but, by introducing a special enzyme, we can redirect it to instead release butyric acid, the precursor for propane. From there, only two more enzymes were needed in order to convert this smelly fatty acid into propane. All in all, this was achieved by introducing only five genes — a very, very tiny fraction of the more than 4,000 genes present in the entire genome of E. coli. Our work represents a proof-of-concept for renewable fuel development as we deliberately selected a process that considers all steps of the pathway from production to utilisation. All in order to maximise chances of commercial production. At the end of the day, that is what is most important – to enable sustainable and renewable conversion of sunlight and CO2 into fuel, with minimal impact on the environment."
"
Share this...FacebookTwitterAxel Bojanowski of the German flagship news magazine Der Spiegel blasted the UN for its Extreme Weather report saying the:
UN presentation was dubious: research results are ignored, the report remains secret.”
and
Only the summary of the report was officially made public on Friday. This is a document that politicians and lawyers of the community of countries negotiated this week in Kampala.”
Even though the sensationalist mainstream media have been writing about predictions of dire events coming in the future, Bojanowski writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Neither the new climate report nor its summary can be read as a sheer warning of increasing weather catastrophes. The real message is: We simply know too little about the most weather catastrophes in order to predict their development.”
There’s simply not enough data out there to draw conclusions. Der Spiegel also seems frustrated that politicians are not taking scientists seriously.
The presentation of the new climate report however shows the opposite, that it appears to not matter what scientists find out through their years of hard work – the message is always the same: ‘Everything is going to get worse.'”
With the conference in Durban coming up, little seems to have changed. Politicians automatically use every climate report for the sole purpose of driving the agenda, without even knowing what’s in it. Der Spiegel writes:
The climate report is being sold simply as a wake-up call – the work of the scientists is simply being ignored. ‘Unfortunately we still don’t have many answers to the questions of climate forecasts,’ says Lisa Schipper of the Stockholm Environment Institute, a lead author of the new IPCC Report, to SPIEGEL ONLINE. ‘There is no black and white scenario’.”
The circus that was Copenhagen is moving to Durban, and is about to start soon.
Share this...FacebookTwitter "
"Scott Morrison says he’s happy to work with the New South Wales government on its ambition to hit net zero emissions by 2050 because the premier, Gladys Berejiklian, “has a plan” – although the plan the prime minister referenced on Tuesday ends in 2030. Despite blasting Labor federally for adopting the same net zero target as the Berejiklian government, the prime minister told parliament circumstances were different in NSW because there was a strategy in place to deliver a transition.  “We have a plan,” Morrison told parliament on Tuesday. “That’s what the leader of the opposition doesn’t understand. He doesn’t have a plan, he just has some sort of vague commitment to something 30 years from now.” In declining to criticise NSW, while criticising Labor federally, Morrison cited a recent $2bn agreement his government signed with NSW as evidence there was a plan in the state. “Just a few weeks ago, the premier and I stood together and we agreed a plan, some $2bn, to invest in what is happening in New South Wales and in Australia, to achieve important targets,” the prime minister said. But the text of the agreement says the memorandum of understanding between the two governments commences on the day of signing, which was 31 January 2020, and continues until 30 June 2030. The text does not purport to be an agreement covering actions to 2050, although it does reference the NSW commitment. Morrison said on Tuesday the agreement with NSW was “about getting access to the gas that this country needs to ensure that we can firm up renewable investments in this country, which is at record levels, to put stability into our electricity grid”. “The problem with what the Labor party is proposing with their 2050 commitment, net zero target of 2050, is they have no plan.” While blasting Labor for adopting net zero, the government continues to leave its options open about whether to adopt a 2050 target down the track. Earlier in the day, the prime minister told colleagues in their weekly joint party room meeting it wasn’t about “being for or against the target”. “I won’t commit to anything that I don’t know the cost of, if I don’t know the impact on jobs,” Morrison told colleagues. “The leader of the opposition has got no plan he has got no clue what the impact would be.” Morrison also used a warning about the economic consequences of the rapidly-spreading coronavirus to criticise Labor for signing up to net zero. “Given the economic challenge, Labor’s reckless approach on the economy is particularly troubling, signing up to a target with no knowledge of what it would cost,” he said. While the political focus remained on 2050, Labor on Tuesday signalled it would adopt a more ambitious emissions reduction target for 2030 than the Coalition’s. Labor took a 45% emissions reduction target by 2030 to the last federal election, and is yet to determine its new medium term target. The process of revising the medium term target will be contentious in Labor ranks. The NSW right winger Joel Fitzgibbon has already argued publicly that the ALP should consider adopting a bipartisan position with the Coalition. The shadow climate change minister, Mark Butler, has rejected Fitzgibbon’s proposition, and told reporters on Tuesday Labor would base all of its proposed emissions reduction targets, including a new medium-term target, on scientific advice. He said the government’s 2030 target was consistent with 3C warming “which would be catastrophic for a vulnerable continent like Australia, and for the rest of the world”. “That’s why we have consistently opposed Tony Abbott and now Scott Morrison’s target for 2030,” Butler said. “The government’s current target, we still don’t know where that came from. “Tony Abbott plucked it out of the air. He’s never produced scientific evidence or modelling to show it’s consistent with what we need to do on climate change. “That will be our approach though, to take scientific advice, to work with the community, to work with businesses, on a proper pathway to net-zero emissions by 2050.”"
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
Share this...FacebookTwitterDietmar Doering of the think tank Denken für die Freiheit (Thinking for Freedom) brings our attention to this exposé of green energy in Germany.Germany is a country completely obsessed with saving the planet from man-made CO2, and has been pursuing renewable energies with unmatched abandon, pouring tens of billions into green energy. Lots of countries use Germany as a model for the direction of the future of energy.
But before other states and countries decide to follow Germany’s lead, they need to take a closer look and to think again.
Is the German story a success? Far from it.
The reality, which is rarely revealed by the public media, is that the country is bordering on an environmental and energy debacle.
The normally very green German state television ARD presented a shocking piece on where the blind, ideological and zealous charge to renewable energy is taking the country.
ARD video here
The mad, blind rush to renewable energies
0:00 – 1:55: German pols are falling all over themselves, trying to take credit for the country’s booming renewables business. Billions of euros in subsidies are flowing everywhere and to anything that sounds renewable. It’s a runaway gravy train – one that is headed for a cliff. As the moderator says; “It’s an energy transformation gone amok”. The struggle to take credit for it likely will not last very long, once the damage gets tallied.
Thousands of micro hydro-power plants everywhere along rivers and streams are now chopping up the nation’s fish stock, mountains of corn for ethanol or driving up food prices, wind turbines are idled and don’t deliver power to the markets because of a lack of transmission lines – all generously subsidized. Professor Olav Hohmeyer of the University of Flensburg:
There’s no plan. There’s no idea of when, how much, and where the power is needed.”
Everything is just being thrown up randomly, without a plan. If it’s renewable, it gets built.
7000+ small-scale hydro power plants chopping up salmon and eel
1:55 mark – Germany has 7700 hydroelectric plants, most are very small scale and massively subsidized by the government. Germany’s vast network of rivers are now so dense with small-scale hydro plants that all paths for eel, salmon and other fish are cut off. A huge proportion of the fish and eel end up getting chopped up by the turbines. To solve the problem the government hired fishermen to try to help the fish bypass the turbines. With luck they are able to save one or two salmon…a week! “An enormous expense,” the ARD says. The fish that do make it upstream get chopped to pieces later on the way back down, see at 3:28 mark (warning – graphic!). Germany’s rivers have become the rivers of blood and chopped fish.
Oh! But isn’t that a small price to pay for rescuing the planet?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Winfried Klein, 350 of these 7700 hydroplants produce 95% of the hydro power, while the other 7350 small-scale plants produce only 5% of the hydro power, but kill most of the fish and eel. Klein says at the 4:10 mark that if these 7350 micro-plants were shut down, the power grid would not even notice it. Klein adds:
But oh no! Every little puny project has to get subsidized because of ‘ecological reasons’.”
The ARD questions a top environment bureaucrat (Jürgen Becker – state secretary of the Federal Environment Ministry) at the 4:40 mark, but just the looks of the guy tells the whole story. Indeed they’ve known about the problem for years…but these fools have been drugged up on CO2 propaganda for too long, and are literally brainwashed and think the survival of the planet is at stake. Fish dying off is a small price to rescue it.
Of course efforts are being generously paid for preventing the turbine-fish-chopping problem. But bypasses consisting of 6-inch plastic pipes have saved very few fish, the video shows. For the mini hydroplant operator it’s however lucrative (5:54 mark). He gets paid 20,000 euros ($28,000) per year by the government for the next 20 years just for having the bypass tube (most fish never find it). “A lot of money for a senseless solution,” the ARD comments.
So if by now you’re thinking this is approaching Soviet-scale mass mismanagement, you’re not too far from the truth. This is what happens when a senseless hysteria grips a country and sends it charging blindly into the dark.
The ARD says at the 6:20 mark that the government is in a frenzy to switch over to renewable energy, no matter the cost.
Biogas – food for fuel
7:07 mark – Biogas is also massively subsidized in Germany, and consequently the country has become overgrown by millions of monoculture acres of cornfields. Dairy farmers can no longer afford to feed their cattle because feed prices and land lease prices have shot up. All land is now being committed to saving the planet. For biogas plant operators, it’s a real cash-cow. Initially the biogas plants were intended to convert waste materials into gas, but instead farmers twisted the arms of politicians and are now paid handsomely to dump mountains of corn into their biogas plants. Little wonder that food prices are surging worldwide today – that’s the price of rescuing the planet.
Paying windparks NOT to produce
10:00 mark – In Germany some wind power does not even make it to the consumer’s electrical outlet. As Germany approves every windpark at every and any location, many areas are not adequately connected to the grid, meaning they are turning for nothing, or not turning at all. For example windpark operator Reinhard Christiansen near the Danish border is often ordered to shut down his turbines even under ideal wind conditions. This is because the power grid gets overloaded and there is still no possibility of transmitting the wind power from northern Germany to the industrial and populated areas in the south.
ARD reports that Christiansen has had to shut down his park 70% more often than last year. But for Christiansen it doesn’t matter. The state jumps in and pays for the power that otherwise would have been produced – money for nothing! “Absurd,” says the ARD. What’s needed are huge transmission lines to transport the power to the south. But this will cost billions more and have to go through lots of private property. And the government never reckoned that citizens would protest vehemently.
ARD also reports at the 12:00 mark that offshore parks too “face a debacle” because “the government has no plan for the power system”. They’re building the windparks, but they’re forgetting the transmission lines to bring the green power to the markets. As it is, the gigantic offshore windparks risk becoming monuments dedicated to the colossal energy stupidity of the government. The government is simply subsidizing the windmills, whether they can be used or not.
For countries and states thinking about copying the German model, you may want to think again.
 
Share this...FacebookTwitter "
"

In less than two years, North Carolina’s governor and legislature have helped to revive the state’s economy. The economy is growing and adding jobs, improving the well‐​being of North Carolina residents.



Governor Pat McCrory took office in January of 2013, joining a Republican legislature. For the first time since Reconstruction, North Carolina’s executive and legislative branches were controlled by Republicans, and they had a large mandate for reform. In 2011, the state’s economy grew at an anemic 0.3 percent. It was well below the national rate of 1.6 percent and one of the lowest in the Southeast. The state’s growth lagged many of its peers in 2012 as well. Individuals wanted change.



The new government took action to repair the state. The biggest item on the agenda was tax reform. McCrory and the legislature’s plan passed one of the most impressive tax reform packages in any state in years.





Limiting the growth of spending and passing tax reform is putting the state on a path of fiscal responsibility.



First, the plan consolidated brackets and cut the individual income tax rate. The overhaul replaced three individual income tax rates ranging from 6.0 to 7.75 percent with a single rate of 5.8 percent. In 2015, the rate will be cut again to 5.75 percent. Cutting taxes returns money to the pocketbooks of individuals and small businesses in the state.



Larger businesses also gained from corporate tax reforms. The corporate income tax rate was cut from 6.9 to 6.0 percent in 2014, and is scheduled to fall to 5.0 percent in 2015. The rate will continue to drop over the next several years if budget targets are met.



The tax reforms also increased the income tax standard deduction, repealed the estate tax, and expanded the sales tax based to cover more services, eliminating favoritism among industries.



In totality, this package puts North Carolina on a pro‐​growth trajectory with a low, broad tax structure. The reforms will vault North Carolina from 44th to 17th in the Tax Foundation’s State Business Tax Climate Index. All told, these tax cuts reduced the burden of taxation on North Carolina residents by $700 million annually, or 3 percent of state tax revenues.



Tax reform did not stop there, with more tax cuts passed in 2014. The state eliminated the local privilege tax. The burdensome tax added unnecessary complexity to North Carolina’s tax code. Three hundred of North Carolina’s 540 cities charged the tax, but it was calculated differently across the state. Some localities assessed a flat fee, others charged a tax that varied by the business’s size or employment structure. Eliminating the tax freed North Carolina firms from needless paperwork allowing each firm to concentrate on their businesses core function.



To complement the tax reforms, the state has also controlled spending growth. Actual general fund spending increased at half the national average in 2014. The state also reformed unemployment insurance and Medicaid, and eliminated several thousand state employees too.



All of these reforms seem to be making a difference and North Carolina’s economy is responding. In 2013, North Carolina’s economy grew 30 percent faster than the national average. The state added jobs at a quicker rate than the national average too. During the same time period, private‐​sector jobs grew by 4 percent, compared to 3.4 percent nationally.



For his efforts, Governor Pat McCrory received an “A” in the Cato Institute’s newest edition of its “Fiscal Policy Report Card on America’s Governors.” The report card assigns grades of “A” to “F” to the nation’s governors based on their efforts to restrain government and cut spending. McCrory tied for the highest score of any governor.



Under the leadership of Governor McCrory and the state legislature, North Carolina is poised for economic success. Limiting the growth of spending and passing tax reform is putting the state on a path of fiscal responsibility.
"
"
Bloomberg: U.S. Northeast May Have Coldest Winter in a Decade
By Todd Zeranski and Erik Schatzker
The SOI has gone weakly positive again - click for larger
Sept. 28 (Bloomberg) — The U.S. Northeast may have the coldest winter in a decade because of a weak El Nino, a warming current in the Pacific Ocean, according to Matt Rogers, a forecaster at Commodity Weather Group.
“Weak El Ninos are notorious for cold and snowy weather on the Eastern seaboard,” Rogers said in a Bloomberg Television interview from Washington. “About 70 percent to 75 percent of the time a weak El Nino will deliver the goods in terms of above-normal heating demand and cold weather. It’s pretty good odds.”
Warming in the Pacific often means fewer Atlantic hurricanes and higher temperatures in the U.S. Northeast during January, February and March, according to the National Weather Service. El Nino occurs every two to five years, on average, and lasts about 12 months, according to the service.
Hedge-fund managers and other large speculators increased their net-long positions, or bets prices will rise, in New York heating oil futures in the week ended Sep. 22, according to U.S. Commodity Futures Trading Commission data Sept. 25.
“It could be one of the coldest winters, or the coldest, winter of the decade,” Rogers said.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92eb6f17',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Britain’s oil industry watchdog plans to overhaul its mission to wring as much value from the North Sea’s oil reserves as possible before the UN climate talks this year. The Oil and Gas Authority (OGA) was due to meet a government minister on Wednesday afternoon to discuss how the regulator, which was set up to extend the life of the North Sea, could play a part in tackling the climate crisis.  Andy Samuel, the OGA’s chief executive, told delegates at an industry conference that the watchdog was considering how it could help the UK meet its climate targets and would open a consultation on how to redefine the OGA’s strategy within the coming months. “We will be discussing this with a minister this afternoon,” he said. The overhaul follows a pledge by Ofgem, the energy regulator, to play a bigger role in helping to meet the government’s climate targets, after coming under fire for failing to prioritise the climate emergency. The OGA was established five years ago by the government to extend the life of the North Sea, after the oil price crash cast doubt over the future of the UK’s ageing oil basin. Since then, the statutory duty has raised questions over whether it is compatible with the government’s commitment to tackling the climate crisis. The regulator’s new strategy is expected to align its work safeguarding North Sea jobs and investment with the UK’s legally binding ambition to cut carbon emissions virtually to zero by 2050. This is likely to pile pressure on North Sea firms to shrink their carbon footprints by reducing flaring and methane leaks, and even use renewable energy to run the rigs. The OGA is also likely to push for companies to collaborate on big investments in carbon capture technology and clean hydrogen production, which could help the UK meet its net zero targets. The strategy may also mean the North Sea produces fewer barrels of oil if companies shift their portfolios from oil towards gas in line with future demand forecasts. Bob Ward, a director at the Grantham Research Institute on Climate Change at the London School of Economics, welcomed the OGA’s attempt to “reconcile the UK’s target for net zero emissions by 2050 with its strategy for maximising economic recovery” from the North Sea. “If the world is serious about achieving the Paris Agreement’s goal, it should mean a global decline in demand for oil and gas. The North Sea has relatively high operating costs compared with other basins, so its oil and gas might be among the first to be priced out of global markets if overall demand falls,” he said, speaking on the sidelines of the International Petroleum Week conference. The Department for Business, Energy and Industrial Strategy did not respond to a request for comment."
"
Spurious Warming in the Jones U.S.  Temperatures Since 1973
by Roy W. Spencer, Ph. D.
INTRODUCTION
As I discussed in my last  post, I’m exploring the International Surface Hourly (ISH) weather  data archived by NOAA to see how a simple reanalysis of original weather  station temperature data compares to the Jones CRUTem3 land-based  temperature dataset.
While the Jones temperature  analysis relies upon the GHCN network of  ‘climate-approved’ stations whose number has been rapidly dwindling in  recent years,  I’m using original data from stations whose number has  been actually growing over time.  I use only stations operating over the  entire period of record so there are no spurious temperature trends  caused by stations coming and going over time.  Also, while the Jones  dataset is based upon daily maximum and minimum temperatures, I am  computing an average of the 4 temperature measurements at the standard  synoptic reporting times of 06, 12, 18, and 00 UTC.
U.S. TEMPERATURE TRENDS, 1973-2009
I compute average monthly temperatures in 5 deg. lat/lon grid squares,  as Jones does, and then compare the two different versions over a  selected geographic area.  Here I will show results for the 5 deg. grids  covering the United States for the period 1973 through 2009.
The following plot shows that the monthly U.S. temperature anomalies  from the two datasets are very similar (anomalies in both datasets are  relative to the 30-year base period from 1973 through 2002).  But while  the monthly variations are very similar, the warming trend in the Jones  dataset is about 20% greater than the warming trend in my ISH data  analysis.

This is a little curious since I have made no adjustments for  increasing urban heat island (UHI) effects over time, which likely are  causing a spurious warming effect, and yet the Jones dataset which IS (I  believe) adjusted for UHI effects actually has somewhat greater warming  than the ISH data.
A plot of the difference between the two datasets is shown next,  which reveals some abrupt transitions. Most noteworthy is what appears  to be a rather rapid spurious warming in the Jones dataset between 1988  and 1996, with an abrupt “reset” downward in 1997 and then another  spurious warming trend after that.

While it might be a little premature to blame these spurious  transitions on the Jones dataset, I use only those stations operating  over the entire period of record, which Jones does not do. So, it is  difficult to see how these effects could have been caused in my  analysis.  Also, the number of 5 deg grid squares used in this  comparison remained the same throughout the 37 year period of record (23  grids).
The decadal temperature trends by calendar month are shown in the  next plot.  We see in the top panel that the greatest warming since 1973  has been in the months of January and February in both datasets.  But  the bottom panel suggests that the stronger warming in the Jones dataset  seems to be a warm season, not winter, phenomenon.

THE NEED FOR NEW TEMPERATURE RENALYSES
I suspect it would be difficult to track down the precise reasons why  the differences in the above datasets exist.  The data used in the Jones  analysis has undergone many changes over time, and the more complex and  subjective the analysis methodology, the more difficult it is to ferret  out the reasons for specific behaviors.
I am increasingly convinced that a much simpler, objective analysis  of original weather station temperature data is necessary to better  understand how spurious influences might have impacted global  temperature trends computed by groups such as CRU and NASA/GISS. It  seems to me that a simple and easily repeatable methodology should be  the starting point.  Then, if one can demonstrate that the simple  temperature analysis has spurious temperature trends, an objective and  easily repeatable adjustment methodology should be the first choice for  an improved version of the analysis.
In my opinion, simplicity, objectivity, and repeatability should be  of paramount importance.  Once one starts making subjective adjustments  of individual stations’ data, the ability to replicate work becomes  almost impossible.
Therefore, more important than the recently reported “do-over”  of a global temperature reanalysis proposed by the UK’s Met Office  would be other, independent researchers doing their own global  temperature analysis.  In my experience, better methods of data analysis  come from the ideas of individuals, not from the majority rule of a  committee.
Of particular interest to me at this point is a simple and objective  method for  quantifying and removing the spurious warming arising from  the urban heat island (UHI) effect.  The recent paper  by McKitrick and Michaels suggests that a substantial UHI influence  continues to infect the GISS and CRU temperature datasets.
In fact, the results for the U.S. I have presented above almost seem  to suggest that the Jones CRUTem3 dataset has a UHI adjustment that is  in the wrong direction.  Coincidentally, this is also the conclusion of a  recent  post on Anthony Watts’ blog, discussing a new  paper published by SPPI.
It is increasingly apparent that we do not even know how much the  world has warmed in recent decades, let alone the reason(s) why.  It  seems to me we are back to square one.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8da3a513',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"School students across the UK (and the world) went on strike on February 15, leaving their lessons to protest the lack of effective action on climate change. Coordinated school strikes may be a novel tactic, but mass environmental activism isn’t. So will things be any more successful this time around? The first big global wave of ecological concern began in the late 1960s and involved fears of overpopulation, air and water pollution and the extinction of species. It peaked with the 1972 Stockholm Conference on the Human Environment, which kicked off international environmental politics. The next mass movement began in the late 1980s with concerns over the ozone hole, Amazonian deforestation and newly-voiced fears of climate change – then known as the “greenhouse effect”. That wave peaked with the 1992 Rio Earth Summit, which sought to tackle both global warming and biodiversity, and marked the beginning of coordinated climate action through the UN. That conference was addressed by a passionate and articulate young woman representing “ECO” – the Environmental Children’s Organization: From about 2006 to 2010 there was another, climate specific wave, beginning with Al Gore’s An Inconvenient Truth documentary, and groups like Climate Camp in the UK. It climaxed (or fizzled out) with the 2009 UN climate summit in Copenhagen. This wave saw the creation of various “Youth Climate Coalition” organisations in Australia and the UK. In academic terminology these periods of concern and relative indifference are known as the “Issue Attention Cycles”. This latest wave of climate action emerged in 2018, in the shape of Extinction Rebellion and its French cousin (or inverse) the gilets jaunes. Earlier in the year, Swedish schoolgirl Greta Thunberg had begun her solo “school strike” in Stockholm while, more or less simultaneously, activists in America launched the “Zero Hour” youth climate march. Alongside this activism, the IPCC released its report on what it would take to keep global warming below 1.5℃, and Mother Nature lent a hand with blistering hot summers in the UK, California and (more recently) Australia.  Previous bursts of environmental activism occurred before climate breakdown had been quite so obvious and severe. This time round, the heatwaves, hurricanes and floods will keep coming, perhaps making the latest wave of enthusiasm last longer. But what goes up must come down, and the students will find that it is very hard indeed to sustain emotional and physical mobilisation for a prolonged period. Right now, this issue is roughly where the Parkland shooting protests were last year – newsworthy for now, but the media caravan will inevitably move on.  That has consequences: when protests and actions stop getting the same amount of attention, and it seems that momentum is stalling, internal disagreements as to what is the best way forward, beyond a cycle of marches and symbolic strikes, will emerge, and will need to be managed skilfully. Some will want to work “within the system” and get invited onto advisory panels and into consultative processes. Others will have to get on with real life (university, paying the rent, working on, ah, zero-hour contracts). On one front, the young are lucky – their age means it is hard to see any direct infiltration and “strategic incapacitation” by undercover police. But the flip side is that social media offers virtually limitless surveillance possibilities. One possibility is an attempt to discredit and demoralise those who seem vulnerable. Elements of special interests like the oil and gas industry often try to “pick off” individual scientists or activists rather than take on a whole field – climate scientist Michael Mann has dubbed this the Serengeti Strategy as it resembles lions hunting the weakest zebras. We are already seeing this strategy in the latest wave of climate activism: recently Greta Thunberg had to address some rumours being circulated about her. Youth activists also face the problem that they may annoy their parents and grandparents. Yet before offering advice to the young, we older people have to ask ourselves, why should they listen to us? We’ve known about the problem and either been ineffective or done nothing. It is children who are owed an enormous apology and expression of humility. So for the latest generation of climate campaigners, my top four pieces of advice (see here for a longer list), based on both my activism and my time in academia, are as follows: Be aware of emotions. People won’t be persuaded just by being given more information on global temperatures or carbon budgets – psychological skills will matter too. Your parents are probably wrestling with fear (aren’t we all?) and guilt for not having sorted this out before you had to. Fear and guilt make can make people oscillate from action to inaction, pessimism to optimism. Traditional “social movement” activities (marches, petitions, protests, camps) have a short shelf-life. The media gets bored and stops reporting. Meanwhile, those in power learn how to cope with the pressure. Be very careful about getting drawn into the Big Marches In London syndrome. You’re going to need to innovate, repeatedly. Even though time is short, this is still a marathon, not a sprint. But what would you say? How should we older people offer advice, when, who to, and about what? Suggestions in the comments please."
nan
"
Share this...FacebookTwitterIt’s a slow climate news day here in Germany, and this story at chinadaily.com happened to catch my eye.
Russian scientists expect to meet aliens by 2031
So there you are. Kook scientists (funded by the poor taxpayers) are also to be found in fields other than climate science. To be honest, I’d say the odds these aliens being discovered are likely greater than some of the goofy climate predictions we’ve heard from GISS or the PIK coming true. The China Daily writes:
Russian scientists expect humanity to encounter alien civilizations within the next two decades, a top Russian astronomer predicted on Monday.
‘The genesis of life is as inevitable as the formation of atoms… Life exists on other planets and we will find it within 20 years,’ Andrei Finkelstein, director of the Russian Academy of Sciences’ Applied Astronomy Institute, was quoted by the Interfax news agency as saying. Speaking at an international forum dedicated to the search for extraterrestrial life, Finkelstein said 10 percent of the known planets circling suns in the galaxy resemble Earth.
Where does he come up with 10%? Fat chance of that being true. And planets that resemble earth? Right!



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




If water can be found there, then so can life, he said, adding that aliens would most likely resemble humans with two arms, two legs and a head. “They may have different colour skin, but even we have that,” he said.
Gee, and I thought alarmist climate scientists were kooks and losing it completely.
I agree that there is a chance that there is “other life” out there – maybe way way out there, like 100 million light years away, and in some “weird” form. But the chances that there is a planet with creatures with “two arms, two legs and a head” living on it, and close enough to be discovered, and that in the next 20 years, is statistically zero. That would require a planet whose numerous physical features would be incredibly similar to the earth’s, and one that would also have followed a similar geological, climatic multi-million year history and evolution. We’re talking zillions of factors here.
Forget it.
But let’s say we did miraculously discover such creatures. Then the planet on which they live would very very very likely be thousands if not millions of light years away, meaning the images and signals that we would be receiving would be thousands or millions of years old, meaning the creatures would be long dead anyway.
Sorry, but we’re on our own here, and our stay is temporary. Just be glad you even got the precious chance to know the earth.
Share this...FacebookTwitter "
"
Here’s something rather astonishing.
The Institute of Physics, has made a statement about climate science.
 
The Institute of Physics is a scientific charity devoted to increasing  the practice, understanding and application of physics. It has a  worldwide membership of over 36,000 and is a leading communicator of  physics-related science to all audiences, from specialists through to  government and the general public. Its publishing company, IOP  Publishing, is a world leader in scientific publishing and the  electronic dissemination of physics.
IOP issued a no holds barred statement on Climategate to the UK  Parliamentary Committee. Here’s the key passages:

What are the implications of the disclosures for the  integrity of scientific research?
1. The Institute is concerned that, unless the disclosed e-mails are  proved to be forgeries or adaptations, worrying implications arise for  the integrity of scientific research in this field and for the  credibility of the scientific method as practised in this context.
2. The CRU e-mails as published on the internet provide prima facie  evidence of determined and co-ordinated refusals to comply with  honourable scientific traditions and freedom of information law. The  principle that scientists should be willing to expose their ideas and  results to independent testing and replication by others, which requires  the open exchange of data, procedures and materials, is vital. The lack  of compliance has been confirmed by the findings of the Information  Commissioner. This extends well beyond the CRU itself – most of the  e-mails were exchanged with researchers in a number of other  international institutions who are also involved in the formulation of  the IPCC’s conclusions on climate change.
3. It is important to recognise that there are two completely  different categories of data set that are involved in the CRU e-mail  exchanges:
· those compiled from direct instrumental measurements of land and  ocean surface temperatures such as the CRU, GISS and NOAA data sets; and
· historic temperature reconstructions from measurements of  ‘proxies’, for example, tree-rings.
4. The second category relating to proxy reconstructions are the  basis for the conclusion that 20th century warming is unprecedented.  Published reconstructions may represent only a part of the raw data  available and may be sensitive to the choices made and the statistical  techniques used. Different choices, omissions or statistical processes  may lead to different conclusions. This possibility was evidently the  reason behind some of the (rejected) requests for further information.
5. The e-mails reveal doubts as to the reliability of some of the  reconstructions and raise questions as to the way in which they have  been represented; for example, the apparent suppression, in graphics  widely used by the IPCC, of proxy results for recent decades that do not  agree with contemporary instrumental temperature measurements.
6. There is also reason for concern at the intolerance to challenge  displayed in the e-mails. This impedes the process of scientific ’self  correction’, which is vital to the integrity of the scientific process  as a whole, and not just to the research itself. In that context, those  CRU e-mails relating to the peer-review process suggest a need for a  review of its adequacy and objectivity as practised in this field and  its potential vulnerability to bias or manipulation.
7. Fundamentally, we consider it should be inappropriate for the  verification of the integrity of the scientific process to depend on  appeals to Freedom of Information legislation. Nevertheless, the right  to such appeals has been shown to be necessary. The e-mails illustrate  the possibility of networks of like-minded researchers effectively  excluding newcomers. Requiring data to be electronically accessible to  all, at the time of publication, would remove this possibility.
8. As a step towards restoring confidence in the scientific process  and to provide greater transparency in future, the editorial boards of  scientific journals should work towards setting down requirements for  open electronic data archiving by authors, to coincide with publication.  Expert input (from journal boards) would be needed to determine the  category of data that would be archived. Much ‘raw’ data requires  calibration and processing through interpretive codes at various levels.
9. Where the nature of the study precludes direct replication by  experiment, as in the case of time-dependent field measurements, it is  important that the requirements include access to all the original raw  data and its provenance, together with the criteria used for, and  effects of, any subsequent selections, omissions or adjustments. The  details of any statistical procedures, necessary for the independent  testing and replication, should also be included. In parallel,  consideration should be given to the requirements for minimum disclosure  in relation to computer modelling.
Are the terms of reference and scope of the Independent  Review announced on 3 December 2009 by UEA adequate?
10. The scope of the UEA review is, not inappropriately, restricted  to the allegations of scientific malpractice and evasion of the Freedom  of Information Act at the CRU. However, most of the e-mails were  exchanged with researchers in a number of other leading institutions  involved in the formulation of the IPCC’s conclusions on climate change.  In so far as those scientists were complicit in the alleged scientific  malpractices, there is need for a wider inquiry into the integrity of  the scientific process in this field.
11. The first of the review’s terms of reference is limited to:  “…manipulation or suppression of data which is at odds with acceptable  scientific practice…” The term ‘acceptable’ is not defined and might  better be replaced with ‘objective’.
12. The second of the review’s terms of reference should extend  beyond reviewing the CRU’s policies and practices to whether these have  been breached by individuals, particularly in respect of other kinds of  departure from objective scientific practice, for example, manipulation  of the publication and peer review system or allowing pre-formed  conclusions to override scientific objectivity.
How independent are the other two international data sets?
13. Published data sets are compiled from a range of sources and are  subject to processing and adjustments of various kinds. Differences in  judgements and methodologies used in such processing may result in  different final data sets even if they are based on the same raw data.  Apart from any communality of sources, account must be taken of  differences in processing between the published data sets and any data  sets on which they draw.

Clearly a sleeping giant has awakened.
Andrew Bolt muses:
This submission in effect warns that this recent warming may not be  unprecedented, after all, and those that claim it is may have been  blinded by bias or simply fiddled their results and suppressed dissent.
I’ll repeat: Climategate reveals the greatest scientific scandal of our  lifetime.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e129716',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Personal protective equipment (PPE) stockpiles in England were inadequate for the Covid pandemic and price rises earlier this year cost taxpayers about Â£10bn, the spending watchdog has said.**
The National Audit Office said there had been a particular shortage of gloves and aprons.
The government said the NAO's report recognised that NHS providers had been able to get what they needed in time.
Almost Â£12.5bn was spent on 32bn items of PPE between February and July 2020.
During the same period in 2019, 1.3bn items were bought at a cost of Â£28.9m.
Each item had been ""substantially"" more expensive in 2020, because of very high global demand, the NAO said, from almost triple the cost for respirator masks to more than 14 times as much for body bags.
Had the government been able to pay 2019 prices, it would have spent Â£2.5bn on PPE in 2020.
In reality, it had spent Â£12.5bn, including hundreds of millions on ""unsuitable"" items that could not be used.
Some had ""passed its expiry date or did not meet current safety standards"", the watchdog said, with ""insufficient checks"" meaning Public Health England had had to recall eye protectors that did not meet standards.
In Parliament, on Wednesday, Labour leader Sir Keir Starmer accused Prime Minister Boris Johnson of ""wasting"" taxpayer's money on equipment that ""can't be used"".
But Mr Johnson replied ""99.5%"" of the 32 billion items of PPE bought between February and July 2020 ""conform entirely to our clinical needs"".
Earlier, the Department of Health and Social Care (DHSC) said ""only 0.49% of all the purchased PPE tested to date"" had not been fit for purpose.
NAO head Gareth Davies said: ""As PPE stockpiles were inadequate for the pandemic, government needed to take urgent action to boost supplies.
""Once it recognised the gravity of the situation... the price of PPE increased dramatically, and that alone has cost the taxpayer around Â£10bn.""
Before the Covid-19 pandemic, there were two emergency stockpiles of PPE:
But the NAO said: ""The EU exit stockpile held few items of PPE other than a large number of gloves.""
Meanwhile, the flu stockpile, as well as having shortages of some key items, did not include any gowns or visors despite the fact they had been ""recommended for inclusion in June 2019 by the New and Emerging Respiratory Virus Threats Advisory Group (Nervtag)"".
Public Health England told the NAO it had been analysing the market to work out which gowns to buy, when the pandemic had begun, which it said was the ""normal approach"" to find a lower price.
In mid-March, the government had still believed its two stockpiles would provide ""most of the PPE needed to manage a Covid-19 pandemic"" and so focused on distributing this PPE rather than buying more, the NAO reported.
The situation had become ""precarious"" in April and May, with stocks threatening to run out.
At one point, only 3% of the required number of gowns had been available.
But the nation did not at any point run out of any type of PPE.
The scramble for PPE in the early stages of the pandemic was not confined to the UK. Every healthcare system was desperate to secure protective equipment and prices soared. But the National Audit Office lays bare how the UK was at the back of the queue, having failed to spot the warning signs and how woefully inadequate the stockpiles were.
A failure to anticipate what might be needed for anything other than a flu pandemic in essence cost the taxpayer Â£10bn - the extra money needed to secure supplies such as gowns and visors during the Covid crisis.
The report highlights poor distribution of PPE with many staff saying they did not have the right equipment. The NAO notes starkly that health and care employers have reported more than 100 deaths among staff because of exposure to coronavirus.
An official inquiry, when it happens, will look hard at many aspects of the UK's preparedness and handling of the crisis and the PPE issue will be central. With a series of reports, the NAO has now done important groundwork but there is much still to find out.
A DHSC official said: ""As the NAO report recognises, during this unprecedented pandemic all the NHS providers audited 'were always able to get what they needed in time' thanks to the Herculean effort of government, NHS, armed forces, civil servants and industry"".
But the NAO heard feedback from care workers, doctors and nurses that showed ""significant numbers of them considered that they were not adequately protected during the height of the first wave of the pandemic"".
Employers have reported 126 deaths among health and care workers linked to exposure at work.
And there were concerns about training and whether the equipment was appropriately fitted, particularly from women and people belonging to ethnic minorities.
In a Royal College of Nursing survey of 5,000 NHS staff, 49% of respondents belonging to ethnic minorities said they had been adequately ""fit tested"" for a respirator, compared with 74% of white nurses.
The DHSC said it was ""listening to the reported practical difficulties with the use of some PPE experienced by women and black, Asian and minority ethnic (BAME) individuals, among others, and... taking action to make sure user needs are adequately addressed in future provisions"".
In a separate report, the Public Accounts Committee, a parliamentary body which works closely with the NAO, said it was ""concerned that the department had no plan before the pandemic for how it might increase critical care equipment in the event of an emergency"".
""This lack of preparedness was exacerbated by the fact that it did not know how many ventilators were available to the NHS to begin with,"" the PAC said.
But it added the government had managed to buy an additional 26,000 ventilators for use in the NHS, a ""significant achievement""."
"
Share this...FacebookTwitterTuvalu is saved! What follows is a press release from the Leibniz Institute for Marine Science (IFM-GEOMAR) on a new paper appearing in the GRL, which shows sea level changes are far more complex than first thought. It’s back to the drawing board for climate and sea level modellers. (Hat-tip Science Skeptical)
==================================================
Quo Vadis Sea Level? New Study Shows Ocean Currents Lead To Strong Regional Fluctuations
Dr. Andreas Villwock
Scientists of the Leibniz Institute for Marine Science (IFM-GEOMAR) have now shown that there are large regional variations when it comes to sea level change. The causes are due to changes in ocean currents, which lead to varying sea levels, especially in the tropical Pacific and Indian Oceans.



Chart above: Sea level fluctuations caused by wind and ocean currents (relative to mean global sea level rise) for the period 1958-2007 (in cm). The model simulation shows regions with sunken sea level (blue) in the tropical Pacific and Indian Ocean. Graphic from IFM-GEOMAR.
Why has the sea level in some regions of the tropical Indian Ocean and Pacific risen strongly over the last 15 years, while in the decades before the sea levels at these locations dropped? The ocean scientists from Kiel are uncovering why by using computer simulations. A paper now appearing in the Geophysical Research Letters shows that fluctuations in ocean currents, caused by trade winds in the tropical Pacific, play an important role.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The impact of wind and ocean currents are prevalent in the tropical Pacific especially in the wake of the El Niño phenomena. “The associated swashing back and forth of the warm surface water leads to a continuous rise and drop in sea level of up to 20 cm within just a few years“, explains oceanographer Franziska Schwarzkopf of the Leibniz Institute for Marine Science (IFM-GEOMAR) and author of the study.
While these short term fluctuations are well documented by modern satellite measurements, little was known about the long-term pattern of changes. “Our computer simulations which use current models show that regional water levels also over time periods of several decades are affected by wind changes and ocean currents“, says Professor Claus Böning, director of Kiel Ocean-Modelling and co-author of the study. A surprising finding from the scientists in Kiel:
In the middle of the last 50 years, some areas in the tropical Pacific and Indian Ocean experienced a drop in sea levels, contrary to the global trend.”
These new results on sea level rise of the last decades mean an additional challenge for climate modeling. “Whether a group of islands has to reckon with a greater increase in sea level with respect to the average, or can reckon with a temporary drop over the next decades depends decisively on the development of the wind systems and ocean currents“, says Böning. “Future research programs will put increasing focus on the regional fluctuations in the oceans.“
The paper: Schwarzkopf, F.U. and C.W. Böning, 2011: Contribution of Pacific wind stress to multi-decadal variations in upper-ocean heat content and sea level in the tropical south Indian Ocean. Geophysical Research Letters, 38, L12602, doi: 10.1029/2011GL047651.
Contact:
Prof. Dr. Claus Böning, Tel: (+49) 431 600-4003, cboening@ifm-geomar.de
Dr. Andreas Villwock (public relations), Tel: (+49) 431 600-2802, avillwock@ifm-geomar.de
================================================
Other reading (h/t: Krishna Gans): Meteorologically driven trends and worldclimatereport.


Share this...FacebookTwitter "
"
One of the most ridiculous claims recently related to Menne et al 2010 and my surfacestations project was a claim made by DeSmogBlog (and Huffington Post who carried the story also) is that the “Urban Heat Island Myth is Dead“.
To clarify for these folks: Elvis is dead, UHI is not.
For disbelievers, let’s look at a few cases showing UHI to be alive and well.

CASE 1: I’ve measured it myself, in the city of Reno for example:

The UHI signature of Reno, NV  – Click for larger image
Read the story of how I created this graph here The procedure and raw data is there if you want to check my work.
I chose Reno for two reasons. It was close to me, and it is the centerpiece of a NOAA training manual on how to site weather stations to avoid UHI effects.

CASE 2: NOAA shows their own measurements that mesh well with mine:
To back that up, the NOAA National Weather Service includes the UHI factor in one of it’s training course ( NOAA Professional Competency Unit 6 ) using Reno, NV. 
In the PCU6 they were also kind enough to provide a photo essay of their own as well as a graph. You can click the aerial photo to get a Google Earth interactive view of the area. The ASOS USHCN station is right between the runways.

This is NOAA’s graph showing the changes to the official climate record when they made station moves:

Source for 24a and 24b: NOAA Internal Training manual, 2004-2007
Oops, moving the station south caused a cooling. Fixed now, all better.
What is striking about this is that here we have NOAA documenting the effects of an “urban heat bubble” something that DeSmog Blog says ” is dead”, plus we have NOAA documenting a USHCN site with known issues, held up as a bad example for training the operational folks, being used in a case study for the new USHCN2 system.
So if NOAA trains for UHI placement, I’m comfortable in saying that DesmogBlog claims of UHI being “dead” are pure rubbish. But let’s not stop there.

CASE 3: From an embattled scientist.
A paper in JGR that slipped in 2007 without much notice (but known now thanks to Warwick Hughes) is one from Phil Jones, the “former” director of the Hadley Climate Center in the UK. The paper is titled:  Urbanization effects in large-scale temperature records, with an emphasis on China
In it, Jones identifies an urban warming signal in China of 0.1 degrees C per decade.  Or, if you prefer, 1 degree C per century. Not negligible by any means. Here is the abstract:
Global surface temperature trends, based on land and marine data, show warming of about 0.8°C over the last 100 years. This rate of warming is sometimes questioned because of the existence of well-known Urban Heat Islands (UHIs). We show examples of the UHIs at London and Vienna, where city center sites are warmer than surrounding rural locations. Both of these UHIs however do not contribute to warming trends over the 20th century because the influences of the cities on surface temperatures have not changed over this time. In the main part of the paper, for China, we compare a new homogenized station data set with gridded temperature products and attempt to assess possible urban influences using sea surface temperature (SST) data sets for the area east of the Chinese mainland. We show that all the land-based data sets for China agree exceptionally well and that their residual warming compared to the SST series since 1951 is relatively small compared to the large-scale warming. Urban-related warming over China is shown to be about 0.1°C decade−1 over the period 1951–2004, with true climatic warming accounting for 0.81°C over this period. 
Even though Jones tries to minimize the UHI effect elsewhere, saying the UHI trends don’t contribute to warming in London and Vienna, what is notable about the paper is that Jones has been minimizing the UHI issues for years and now does an about face on China.
Jones may have tried to hide CRU data, but he’s right about China.

CASE 4: From “The Dog ate My Data” who writes:
 The Australian Bureau of Meteorology (BOM) blames Melbourne’s equal warmest overnight temperature of 30.6 degrees, on January 12 on the heat island effect. The previous time the city was that hot overnight was February 1, 1902.

 The Age newspaper cites a meteorologist at the bureau, Harvey Stern,
Melbourne recorded its equal warmest overnight temperature, 30.6 degrees, on January 12. The previous time the city was that hot overnight was February 1, 1902.
A meteorologist at the bureau, Harvey Stern, said that Melbourne suffered from a heat island effect, in which a city is warmer than the surrounding countryside.
This was the case especially at night, because of heat stored in bricks and concrete and trapped between close-packed buildings.
I am stunned if that is correct firstly because BOM isn’t blaming Global Warming and secondly that the urban heat island effect directly receives the blame. With faults in the 2007 IPCC’s AR4 now pouring out I guess it is not suprising that attributions of weather events are now, shall we say, possibly becoming more circumspect.

CASE 5: Heatzilla stomps Tokyo
From the website “science of doom” who writes:

New Research from Japan
Detection of urban warming in recent temperature trends in Japan by Fumiaki Fujibe was published in the International Journal of Climatology (2009). It is a very interesting paper which I’ll comment on in this post.
The abstract reads:
The contribution of urban effects on recent temperature trends in Japan was analysed using data at 561 stations for 27 years (March 1979–February 2006). Stations were categorized according to the population density of surrounding few kilometres. There is a warming trend of 0.3–0.4 °C/decade even for stations with low population density (<100 people per square kilometre), indicating that the recent temperature increase is largely contributed by background climatic change. On the other hand, anomalous warming trend is detected for stations with larger population density. Even for only weakly populated sites with population density of 100–300/km2, there is an anomalous trend of 0.03–0.05 °C/decade. This fact suggests that urban warming is detectable not only at large cities but also at slightly urbanized sites in Japan. Copyright, 2008 Royal Meteorological Society.
Why the last 27 years?
The author first compares the temperature over 100 years as measured in Tokyo in the central business district with that in Hachijo Island, 300km south.
Tokyo –               3.1°C rise over 100 years (1906-2006)
Hachijo Island –  0.6°C over the same period

This certainly indicates a problem, but to do a thorough study over the last 100 years is impossible because most temperature stations with a long history are in urban areas.
However, at the end of the 1970’s, the Automated Meteorological Data Acquisition System (AMeDAS) was deployed around Japan providing hourly temperature data at 800 stations. The temperature data from these are the basis for the paper. The 27 years coincides with the large temperature rise (see above) of around 0.3-0.4°C globally.
And the IPCC (2007) summarized the northern hemisphere land-based temperature measurements from 1979- 2005 as 0.3°C per decade.
How was Urbanization measured?

The degree of urbanization around each site was calculated from grid data of population and land use, because city populations often used as an index of urban size (Oke, 1973; Karl et al., 1988; Fujibe, 1995) might not be representative of the thermal environment of a site located outside the central area of a city.

What were the Results?

Mean temperature anomaly vs population density, Japan

The x-axis, D3, is a measure of population density. T’mean is the change in the mean temperature per decade.
Tmean is the average of all of the hourly temperature measurements, it is not the average of Tmax and Tmin.
Notice the large scatter – this shows why having a large sample is necessary. However, in spite of that, there is a clear trend which demonstrates the UHI effect.
There is large scatter among stations, indicating the dominance of local factors’ characteristic to each station. Nevertheless, there is a positive correlation of 0.455 (Tmean = 0.071 logD3 + 0.262 °C), which is significant at the 1% level, between logD3 and Tmean.
Here’s the data summarized with T’mean as well as the T’max and T’min values. Note that D3 is population per km2 around the point of temperature measurement, and remember that the temperature values are changes per decade:

The effect of UHI demonstrated in various population densities

Note that, as observed by many researchers in other regions, especially Roger Pielke Sr, the Tmin values are the most problematic – demonstrating the largest UHI effect. Average temperatures for land-based stations globally are currently calculated from the average of Tmax and Tmin, and in many areas globally it is the Tmin which has shown the largest anomalies. But back to our topic under discussion..
And for those confused about how the Tmean can be lower than the Tmin value in each population category, it is because we are measuring anomalies from decade to decade.
And the graphs showing the temperature anomalies by category (population density):

Dependence of Tmean, Tmax and Tmin on population density for different regions in Japan

Quantifying the UHI value
Now the author carries out an interesting step:
As an index of net urban trend, the departure of T from its average for surrounding non-urban stations was used on the assumption that regional warming was locally uniform.
That is, he calculates the temperature deviation in each station in category 3-6 with the locally relevant category 1 and 2 (rural) stations. (There were not enough category 1 stations to do it with just category 1). The calculation takes into account how far away the “rural” stations are, so that more weight is given to closer stations.

Estimate of actual UHI by referencing the closest rural stations – again categorized by population density

And the relevant table:


Temperature delta from nearby rural areas vs population density

Conclusion
Here’s what the author has to say:
On the one hand, it indicates the presence of warming trend over 0.3 °C/decade in Japan, even at non-urban stations. This fact confirms that recent rapid warming at Japanese cities is largely attributable to background temperature rise on the large scale, rather than the development of urban heat islands.
..However, the analysis has also revealed the presence of significant urban anomaly. The anomalous trend for the category 6, with population density over 3000 km−2 or urban surface coverage over 50%, is about 0.1 °C/decade..
..This value may be small in comparison to the background warming trend in the last few decades, but they can have substantial magnitude when compared with the centennial global trend, which is estimated to be 0.74°C/century for 1906–2005 (IPCC, 2007). It therefore requires careful analysis to avoid urban influences in evaluating long-term temperature changes.
So, in this very thorough study, in Japan at least, the temperature rise that has been measured over the last few decades is a solid result. The temperature increase from 1979 – 2006 has been around 0.3°C/decade
However, in the larger cities the actual measurement will be overstated by 25%.
And in a time of lower temperature rise, the UHI may be swamping the real signal.
The degree of urbanization around each site was calculated from grid data of population and land use, because city populations often used as an index of urban size (Oke, 1973; Karl et al., 1988; Fujibe, 1995) might not be representative of the thermal environment of a site located outside the central area of a city.

Case 6: California Counties by population show a distinct UHI signature.
My friend Jim Goodridge, former California State Climatologist identified the statewide UHI signature issues way back in 1996. This graph had a profound effect on me, becuase it was the one that really made an impact on me, switching my views to being skeptical. Yes, I used to be a warmer, but that’s another story.
Goodridge, J.D. (1996) Comments on “Regional Simulations of Greenhouse Warming including Natural Variability” . Bull, Amer. Meteorological Society 77:1588-1599.
Goodrich (1996) showed the importance of urbanization to temperatures in his study of California counties in 1996. He found for counties with a million or more population the warming from 1910 to 1995 was 4F, for counties with 100,000 to 1 million it was 1F and for counties with less than 100,000 there was no change (0.1F).

He’s been quietly toiling away in his retirement on his computer for the last 15 years or so making all sort of data comparisons. One plot which he shared with me in 2003  is a 104 year plot map of California showing station trends after painstakingly hand entering data into an Excel spreadsheet and plotting slopes of the data to produce trend dots.
He used every good continuous piece of data he could get his hands on, no adjusted data like the climate modelers use, only raw from Cooperative Observing Stations, CDF stations, Weather Service Office’s and Municipal stations.
The results are quite interesting. Here it is:

I’ll have more interesting revelations from Jim Goodridge soon.

Case 7: NASA JPL’s climatologist says UHI is an issue
This press release from NASA Jet Propulsion Lab says that most of the increase in temperature has to do with ubanization:
[NASA’s JPL Bill] Patzert says global warming due to increasing greenhouse gases is responsible for some of the overall heating observed in Los Angeles and the rest of California. Most of the increase in heat days and length of heat waves, however, is due to a phenomenon called the “urban heat island effect.”
Heat island-induced heat waves are a growing concern for urban and suburban dwellers worldwide. According to the U.S. Environmental Protection Agency, studies around the world have shown that this effect makes urban areas from 2 to 10 degrees Fahrenheit (1 to 6 degrees Celsius) warmer than their surrounding rural areas.
Patzert says this effect is steadily warming Southern California, though more modestly than some larger urban areas around the world. “Dramatic urbanization has resulted in an extreme makeover for Southern California, with more homes, lawns, shopping centers, traffic, freeways and agriculture, all absorbing and retaining solar radiation, making our megalopolis warmer,” Patzert said.

CASE 8: You can see it from space. NASA (not the GISS division) measures it. Here’s a report they presented at the last AGU meeting in December 2009. Gee, that curve below looks like Reno, NV, doesn’t it?
  
The urban heat island effect can raise temperatures within cities as much as 5 C higher than the surrounding countryside. New data suggests that the effect is more or less pronounced depending on the type of landscape — forest or desert — the city replaced. Credit: NASA
› Larger image


 
NASA researchers studying urban landscapes have found that the intensity of the “heat island” created by a city depends on the ecosystem it replaced and on the regional climate. Urban areas developed in arid and semi-arid regions show far less heating compared with the surrounding countryside than cities built amid forested and temperate climates.
“The placement and structure of cities — and what was there before — really does matter,” said Marc Imhoff, biologist and remote sensing specialist at NASA’s Goddard Space Flight Center in Greenbelt, Md. “The amount of the heat differential between the city and the surrounding environment depends on how much of the ground is covered by trees and vegetation. Understanding urban heating will be important for building new cities and retrofitting existing ones.”
Goddard researchers including Imhoff, Lahouari Bounoua, Ping Zhang, and Robert Wolfe presented their findings on Dec. 16 in San Francisco at the Fall Meeting of the American Geophysical Union.
Satellite imagery of suburban (top) and urban Atlanta shows the differences in daytime heating, as caused by the urban heat island effect. Credit: NASA Goddard’s Scientific Visualization Studio
› Larger image (suburban)
› Larger image (urban)

Yep, UHI is alive and well. Anybody with an automobile dashboard thermometer who drives a commute from country to city can easily measure UHI, and you don’t have to be a climate scientist to prove it to yourself.
UPDATE: For a primer on how UHI is not dealt with by NOAA and CRU, have a look at this Climate Audit post:
Realclimate and Disinformation on UHI


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e415395',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

According to David Rind from NASA’s Goddard Institute for Space Studies (GISS), 2005 is going to set the all‐​time record for global warmth. He told Juliet Eilperin of the _Washington Post_ (October 13, 2005) only a major volcanic eruption could intervene. But Eilperin also interviewed Oregon State Climatologist George Taylor, who told her that Goddard’s findings were “mighty preliminary.” 



That’s because there’s more than one history of global temperature. Three receive the most citations. NASA’s record begins in 1880, as does another history from the U.S. Department of Commerce, developed at the Department’s National Climatic Data Center (NCDC). But the most widely referenced history (and the one primarily used by the U.N.‘s Intergovernmental Panel on Climate Change (IPCC)) is compiled by the Climate Research Unit (CRU) at England’s University of East Anglia. It goes back to 1856.



The vast majority of the underlying temperature observations that go into each of these compilations is the same, but each organization has developed its own techniques for how the raw observations are geographically combined and adjusted for confounding factors such as urbanization, missing values, etc. As a result, annual values in each temperature history differ slightly.



So let’s take a look at where the average temperature in each stands through September 2005, and what the prospects are for setting a record for the year as a whole, given that there are still three months of data to be added.



In the table below are all of the relevant numbers.







The GISS anomalies are calculated from the average for the period 1951 through 1980, the NCDC anomalies are relative to the average from 1880 through 2004, while the CRU temperatures are the departures from the 1961–1990 mean. But this matters little in our analysis. In each history, the record‐​warm year is 1998. The September anomaly in the CRU data is not yet available but we have estimated it based upon the values reported by GISS and NCDC. 



The “Additional Monthly Anomaly” is the increment relative to the January‐​September average that each month in the period October through December must average in order to have 2005 become the warmest year. It is negative for the GISS temperatures because they are currently above the record value. 



The “Number of Observed Exceedences” is the number of times during the period of record that the average anomaly (relative to the first 9 months of the year) during last three months of the year reached or exceeded the value required to have 2005 set the record. 



The “Percent Chance” is “Number of Observed Exceedences” divided by the “Total Number of Observations”. 



This last column is where the rubber meets the road. Based upon the previous behavior of the climate system (as captured by the global average temperature in each compilation), there is a nearly three‐​in‐​four chance that 2005 will finish as the warmest year in the NASA GISS global temperature history, but less than a one‐​in‐​five chance that the NCDC record will be set, and virtually no chance that the CRU will report 2005 as the hottest year measured. Both Rind and Taylor are going to turn out to be correct. 



The significance of all of this depends on whom you talk to. The press, as already foreshadowed by Eilperin’s Washington Post article, will surely trumpet the record‐​setting values from the GISS data, while noting that the other datasets (probably) will have placed 2005 as the second warmest year on record. The various scientists interviewed will point out that this occurred even in the absence of a strong El Niño (the primary reason 1998 was so hot) and that this is further evidence that the earth is warming from an enhancing greenhouse effect. 



So, what else is new? We already know that the world is warming and that it will continue to do so for the foreseeable future (with or without any greenhouse gas emission controls). Record temperatures will continue to be set every couple of years or so. In fact, if it weren’t for the 1998 El Niño, a new record high global average temperature would have been established in 4 of the last 5 years (including 2005). The big news is that 2005 will further establish that the rate at which temperatures have been rising during the past 30 years or so has been remarkably constant with a value of about 0.17ºC per decade, and it shows no sign of speeding up. Climate models share this constancy of warming; they just predict different rates. Unless that behavior is wrong, the additional warming until 2100 will be about 1.6°C, near the low end of projections made by our friends at the United Nations, and, frankly, too small to worry about, given that the energy structure of our society is likely to change dramatically in 100 years’ time. We’ll bet that no one points that out in December, when the warmth‐​of‐​2005 stories will proliferate like Santas. 



References:



Goddard Institute for Space Studies Temperature Data  
http://​data​.giss​.nasa​.gov/​g​i​s​temp/



National Climatic Data Center Temperature Data  
http://​www​.ncdc​.noaa​.gov/​o​a​/​c​l​i​m​a​t​e​/​r​e​s​e​a​r​c​h​/​a​n​o​m​a​l​i​e​s​/​a​n​o​m​a​l​i​e​s​.html



Climate Research Unit Temperature Data  
http://​www​.cru​.uea​.ac​.uk/​c​r​u​/​d​a​t​a​/​t​e​m​p​e​r​a​ture/
"
"
Share this...FacebookTwitterCFACT was one of the sponsors of the 4th International Climate Conference, which took place in Munich last weekend. What follows is a worthwhile interview with the Chairman of the European Institute for Climate and Energy (EIKE), Holger Thuss, presented by CFACT.
4th Climate Conference in Munich, 2011
=================================================
On Friday and Saturday, November 25
& 26, as the UN prepared to kick off COP17, the UN Conference
on Climate Change, in Durban, South Africa, CFACT, the European Institute for
Climate and Energy (EIKE) and others co-sponsored a climate conference of our
own in beautiful Munich, Germany. EIKE was launched in 2007 in Berlin. CFACT is
a proud founding member.
CFACT: What has been the public mood in Germany about global
warming?
For years, large parts of the population were buying into the
IPCC’s position without question. Others had some doubt, but could not find any
reliable source in their own language to educate themselves about anything
about climate that did not favor the Green’s narrative. It was terribly
one-sided.
Are things changing?
The media, while still often hostile, now takes serious interest
in our activities and today is willing to openly question whether the IPCC and
climate campaigners are always right. Today, more and more people in Germany
realize they have been misinformed and I can say without exaggerating that our
work played an important part in this. We see a real change, not just among
ordinary citizens, but also business leaders, lawmakers, media executives and
the consulting industry (which is very influential in Germany). People from
every sector of society are giving positive feedback, attending events, buying
our publications and are now not shy about giving us advice. People read the
newspaper and decided for themselves that something didn’t smell right. EIKE
with the help from CFACT gave them the facts and a way to meet one another,
organize and communicate.
Even more attended than last year!
What was it like organizing the first real organization in Germany to
question climate orthodoxy?
We had resistance from all sides. We needed 1 1/2 years to register
and organize – it took that long to overcome bureaucratic resistance. It was
even worse to deal with the hostile media which didn’t hesitate to tell
outright lies about our funding, research and the qualifications of our experts
– who are distinguished academics from prestigious universities! Some climate
radicals tried to make a big deal about the relationship between EIKE and CFACT
and just can’t seem to grasp that organizations can be friends too and
cooperate together to do great work. Today EIKE is proud to welcome cooperation
and support from many diverse and talented people and organizations. The
momentum is on our side.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Tell us about the climate conference program.
We have now conducted two workshops and four major conferences.
Every one has been a success and each was larger than the one before. There is
real excitement. We just completed our first conference in Munich, which was
timed to take place at the start of the UN climate conference in Durban. I was
very pleased to realize that 90 percent of the people attending were new. They
heard we were going to be in their region, wanted to be with us and were
willing to reach into their own pockets to support the conference financially.
They came not only from Southern Germany, but from Austria, Switzerland and
even South Korea and Paraguay.  We are rather proud to be able to offer
attendees simultaneous German and English translations throughout the
conference.
What were some highlights of this weekend’s conference?
Almost every presentation was a highlight because we heard state of the art science
from such distinguished researchers as Henrik Svensmark from Denmark, Nir Shaiviv
from Israel, Gernot Patzelt from Austria and Jan Veizer from Canada. We also
covered policy and legal aspects related to the downsides of renewable energy
and of course the Climategate scandal parts one and two from fascinating
speakers such as Donna Lafromboise from Canada and Chris Horner from the U.S.
We had so many more truly brilliant presenters that I hesitate to mention any
names at all, because each is worthy and we are extremely grateful to them all.
What message can CFACT carry from Munich to the UN’s COP17 in Durban?
We want the UN and everyone reading this to know that the IPCC
reports have been proclaiming their conclusions as unquestionable scientific
facts, when actually their science is faulty and Climategate shows the UN has
placed its trust in the wrong people. We call for open minds and vigorous
debate without fear. The IPCC should be replaced by a more credible
institution, perhaps one uniting research institutions from around the world,
free from bias and advocacy.
================================================
Visit CFACT here: http://www.cfact.org/
 
 
Share this...FacebookTwitter "
"The Beast from the East, a polar vortex which brought freezing conditions to the UK, arrived on February 26 2018. Two days later there was a minimum temperature of -11.7°C (10.9˚F) at South Farnborough in Hampshire, and a maximum of only -4.8°C (23.4˚F) at Spadeadam in Cumbria. In sharp contrast, on February 26 2019, temperatures reached 21.2˚C (70.2˚F) at Kew Gardens in south-west London – the warmest winter day since records began. In February 2019, bumblebee queens were out looking for nest sites, adult butterflies were emerging from their winter hibernation and blossom appeared on some trees and shrubs. But what will be the long-term effects of 2019’s early spring? The relatively new science of phenology examines the timing of the seasons by plotting the calendar records of first plant bud, first flower, first nesting behaviour and first migrant arrivals. Over the past three decades, these records have confirmed that spring temperatures are generally arriving earlier in the year. As the days get longer and warmer in the northern hemisphere, bird species such as the barn swallow follow these natural cues to depart for British habitats, where they nest and rear their young. These insectivorous migratory birds time their breeding season to coincide with insects being present in sufficient numbers to feed their young.  While the main trigger for birds to migrate from their overwintering grounds to Britain is the length of daylight, temperature will also fine-tune the arrival date. An early spring means that insects could emerge and breed before migratory birds arrive. Once in the UK, the birds may find there are fewer insects to eat and this results in fewer chicks fledging, which leaves their predators, including the sparrowhawk and the stoat, with less to eat. The disconnect between the arrival of insectivorous birds and the abundance of insects ripples through the ecosystem, affecting other animals and plants that at first sight may not seem linked to this seemingly benign change. Of course, much depends on how long the warm weather lasts and what follows. The mild conditions in 2019 have prompted buds to burst on some plants – this makes the flowers and leaves vulnerable if the weather reverts to colder conditions. If the temperature drops to low single figures in degrees Celsius but remains above freezing, growth rate will slow and the plant’s growth will be stunted. A hard frost, on the other hand, would damage or even kill any of the flowers and leaves that have emerged during the warm spell. At the end of twigs is the apical meristem – the site where rapid cell division generates new plant material which results in the twigs growing longer each year. No longer protected from frosts within a bud, apical meristems are vulnerable to damage by frost. The result is that twigs will stop growing. Rather, a new apical growth point will establish itself from a bud nearer to the main trunk of a tree or the main stem of a bush. The long-term effect is that a twig develops in a different direction, and the plant carries this signature of frost damage for the rest of its life. An early spring might also accelerate climate change. Scientists at the Vienna University of Technology and the University of Leeds studied satellite data for the northern hemisphere – from southern Europe and Japan up to and including the Arctic tundra – and demonstrated that in many regions of the Earth, an early spring leads, counter-intuitively, to less plant growth.  This may be because certain plants have a predetermined growth period. Growth in early spring means an early cessation of growth later in summer or early autumn. Greater plant growth in the spring could also result in increased transpiration – the process by which moisture is drawn through plants from roots to small pores on the underside of leaves, where it becomes vapour which is released to the atmosphere. This causes a higher demand for water during the growing season which cannot be met if the summer and autumn are also dry. The result of this early growth is limited plant growth overall through the entire year. Plants that do not grow as big as they might absorb less carbon, so reduced plant growth means less carbon stored in vegetation, and that in turn means more carbon dioxide in the atmosphere, more warming and even earlier springs – a positive feedback loop. 


      Read more:
      The Arctic is turning brown because of weird weather – and it could accelerate climate change


 Many people have worried about the unseasonable warmth and spring-like conditions of February 2019. As unseasonably mild weather brings about changes in plant growth that could accelerate climate change and widen the disconnect between elements of ecosystems, this unusual week may leave an even more worrying legacy. However, even in the midst of a winter warm spell, the Met Office forecast predicts less mild and more changeable conditions to come – showery rain in some parts and stormy conditions elsewhere, with overnight frosts still possible. There is even a mention of snow on high ground. Such are the vagaries of the British weather."
"**Ministers are going to be asked to re-think the plan for gym classes in England from next week.**
Under the new three-tier proposals announced yesterday, they won't be allowed in the very high alert areas.
From 2 December gyms can re-open no matter what tier they're in - but if you're a fan of HIIT or yoga, they'll be banned in tier three.
UKactive, which represents a lot of the fitness industry, says it's ""disappointed"" and wants this changed.
Ministers say group classes will have to close in tier three because worries coronavirus could quickly spread in them.
This announcement shows a positive shift in the Government's desire to strengthen our nation's physical and mental resilience,"" says Huw Edwards, CEO of UKactive.
""However, we are disappointed to note that indoor group exercise is included in tier three restrictions and will not be permitted unless in household groups or bubbles.""
The body says it wants the government to change its decision and carry out a review into the role physical activity plays in society.
""The sector has proven this activity can be undertaken in a manner that is safe,"" says Huw.
""Using a combination of social distancing, sanitisation and increased ventilation.""
The government has yet to respond to this call by the fitness industry, but the Prime Minister Boris Johnson has said he's ""very sorry"" for the problems these measures are causing for business owners.
But he added that ""things will look and feel very different"" after Easter as a vaccine is rolled out.
Measures in Scotland, Wales and Northern Ireland will continue to be decided by its local governments, but a joint approach to Christmas is set to be announced.
_Follow Newsbeat on_Instagram _,_Facebook _,_Twitter _and_YouTube _._
_Listen to Newsbeat_live _at 12:45 and 17:45 weekdays - or listen back_here."
"
Share this...FacebookTwitterNot even CO2 can stop it!
The German online Bild newspaper here projects another bitter cold winter ahead for Central Europe. Early November will be on the mild side, and the real cold will not arrive until the end of November – but then look out! The Bild article starts with:
Shiver-Alarm: In four weeks the Horror-Winter begins!
Bild quotes meteorologist Dominik Jung of www.wetter.net/:
This snowy winter will extend over three months and spread into the flatlands. And it is going to be bitter cold. For Germany the fourth colder-than-normal winter in a row lies ahead. The fourth below normal winter in a row would be a small sensation.”
Wetter.net predicts December in Germany should see a fair amount of snow with good chances for a white Christmas. Temperatures will be around normal.. January will see temperatures plummet and the month will be considerably colder than normal. Central Europeans can expect to see temperatures to fall as low as -25°C.
Whatever happened to the balmy winters that were predicted by the models of CO2-drugged up climatologists not long ago? Remember being told that snow in the future would be “rare and exciting” and how we’d all would have to move to Antarctica to escape the heat?
Jung also says February will also be colder than normal and that snow will fall until the end of the month, and possibly drag on into March. So forget about an early spring. Who knows, maybe we’ll be shoveling snow for Easter too.
Note that these forecasts predict snow and ice for the normally temperate lowlands. Just a few years ago the drugged-up climatologists were predicting the end of skiing in the Alps.
 
Share this...FacebookTwitter "
"**A high street voucher scheme will see pre-paid cards issued to individuals in Northern Ireland, not to each household as previously thought.**
The scheme is still being worked on, but BBC News NI understands the amount given to each qualifying person could be between Â£75 and Â£100.
It was previously suggested that Â£200 would be awarded to each household.
The Department for the Economy has now provided more detail on the scheme that it expects to roll out early next year.
The voucher scheme was announced as part of a raft of financial measures outlined on Monday.
The aim of the scheme is to support bricks and mortar retail which has been adversely affected as a result of coronavirus restrictions.
Non-essential retail in Northern Ireland will be shut from Friday as part of a two-week circuit breaker.
The budget for the vouchers is Â£95m and that will also cover any administration fees a chosen provider may charge.
About 1.1m people could be issued a pre-paid card, which suggests there will be some sort of age limit on who will be eligible.
The money can be spent in shops, but not online.
It is based on similar schemes in Jersey and Malta.
Earlier this year, Jersey introduced an Â£11m scheme through which residents received Â£100 each to stimulate the island's economy.
Northern Ireland's First Minister Arlene Foster has previously said she was impressed by the initiative.
Murray Norton, chief executive of Jersey's Chamber of Commerce, told BBC NI's Good Morning Ulster the initiative had been ""good news for the consumer and the High Street"".
""Jersey is an island, so you can't get off it to go and spend it anywhere else for starters, but there were limits on the pre-paid card that everyone received,"" he said.
""Yes it did help consumers and yes it did help shopkeepers locally, by how much compared to how much was spent?
""The jury is still out on that.""
Northern Ireland's Finance Minister Conor Murphy said on Monday that the pre-paid card issued through the voucher scheme would be worth Â£200 per household.
Clarification that it would be paid per individual was issued on Tuesday by Diane Dodds' Department for the Economy."
