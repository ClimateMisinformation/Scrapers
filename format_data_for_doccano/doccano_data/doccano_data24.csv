"
539 new snowfall records were also set.
Since we are often treated to lists of record high temperatures when heat waves occur and they are improperly linked to global warming (like in Russia’s heat wave this summer), I thought it only fair that I show the number of record cold and snow records around the USA for the past week that aren’t linked to global warming.
Record low temperatures, low max, and record snowfall plotted - click for interactive graph
Of course it wouldn’t be fair to show just the lows temperatures and snow, so here are the high temperature records for the USA in the past week. 
Reord high temperatues for the past 7 days - click for interactive map
And here’s just the lows:

The summary of new records of interest for the past week in the USA :



Snowfall:
539


High Temperatures:
18


Low Temperatures:
336


Lowest Max Temperatures:
278



Lows outnumbered highs by a factor of 19 (336/18=18.6 ~19). That’s quite the cold snap.*
The coldest?
Deadhorse, Alaska, 	on Sunday, 26 Dec 2010	at -40°F beating -38°F set in 1984
*Note: some people clicking on the interactive map will see different numbers, since that map will record new highs and lows as this post ages. The headline was originally based on 16 highs during the week (see the highs map for a ratio of 21 to 1) then by the time the post editing was completed and the post made, the number of highs was up to 18, giving an 18.6 to 1 (~19 to 1 in the title) ratio. Later in the day the number of record highs in the one week period increased as new weather occurred (on Dec 31) and reports came in. The numbers were accurate at the time the post started. Weather records, like weather itself are dynamic with the forward moving one week period the interactive map generator uses, so please don’t assume error if you click on the interactive map and the numbers don’t match now, or in the future. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e85a95bd3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

This passage from the EPA’s “Regulatory Impact Analysis for the Proposed Standards of Performance for Greenhouse Gas Emissions for New Stationary Sources: Electric Utility Generating Units” sums up the intent and justification of the proposed standards.



“While sector‐​wide modeling does not project any new coal‐​fired EGUs [electric generating units] without CCS [carbon capture and storage] to be built in the absence of this proposal, we recognize that a few companies may choose to construct coal or other solid fossil fuel‐​fired units. In Chapter 5 of his RIA we present an analysis of the project‐​level costs of a new coal‐​fired unit with and without CCS, and estimate the social benefits of requiring CCS on a new uncontrolled unit. We also present a sensitivity analysis indicating that even in the unlikely event that market conditions change sufficiently to make the widespread construction of new conventional coal‐​fired units economical from the perspective of private investors, this rule would result in net benefits from avoided negative health and environmental effects.”



As we will show, this justification fails in virtually all of its aspects:



1) the social cost of carbon (SCC) estimates used by the EPA to compare project‐​level costs of new coal‐​fired power plants with and without CCS technology are overinflated and thus wrongly favor the adaptation of CCS;



2) the Rule would not result in net benefits from avoided negative health effects as human health is improving — partially as a result of climate change; and



3) the Rule would not result in net benefits from avoided negative environmental effects as the environmental impacts of the Rule are negligible and scientifically undetectable.



As a consequence, this proposed standard should be withdrawn and not revisited.
"
"Q: Can you recommend some climate crisis fiction? The nonfiction is too depressing and fiction often helps the heart cope with the worldPeggy Duesenberry, 61, Massachusetts, US A: Melissa Harrison is a novelist and nature writer whose books include At Hawthorn Time and All Among the Barley. She writes: There are plenty of dystopian cli-fi novels out there, designed to jolt us out of our current complacency – but it doesn’t sound as though that’s what you need. The American poet and climate activist Kate Schapira believes we must “imagine – and learn about! there are precedents! – the structures that would allow us to live well enough without hurting ourselves and each other, and without helping the people currently hurting us”. Fiction can help us do that imaginative work. The brilliant Jenny Offill’s new novel Weather is a great place to start, as it explores what it’s like for ordinary people to move from fear and denial to concrete action. Emily St John Mandel’s haunting Station Eleven (2014) takes us into a near future where disease has led to a breakdown of society, but not a world devoid of hope, for Shakespeare’s plays survive, and art and love remain central to the human experience. Set in an Australia ravaged by climate change, Alexis Wright’s richly strange, genre-bending The Swan Book (2016) is a reminder that other, older cultures may have healthier and more connected relationships to the natural world than the destructive western capitalism currently in the ascendant. Since writing The Dispossessed (1975), Ursula K Le Guin has concluded that an “anarchist utopia” such as the one she describes would eventually destroy itself – but as a way of envisioning a society organised on different principles to ours, it continues to inspire. Finally, Tove Jansson’s gorgeous, sparklingly simple The Summer Book (1972), in which a little girl and her grandmother spend a season on a Finnish island, has two vital lessons for today: how to live a rich, creative life with very few resources, and how to remain clear-eyed and full of courage in the face of grief and loss. The work we need to do now is as much moral and imaginative as it is practical. These are novels that can shift our values and priorities, if we allow them to. Submit your question for bookclinic below or email bookclinic@observer.co.uk"
"Earthquakes threaten to be a show-stopper for fracking. In the Netherlands, the largest gas field in Europe will be shut down by 2030 after sustained damage to homes from earthquakes became too severe. In Oklahoma, US officials have severely curtailed operations after injection of waste water underground caused several earthquakes above magnitude five – one nearly 180,000 times stronger than the 2.3 magnitude earthquake that brought a seven-year pause on fracking in the UK. While operations have since resumed in Britain, the practice still remains a political battleground, with earthquakes at the centre. The UK government’s fracking commissioner, Natascha Engel, recently resigned, claiming that an [unreasonably low] magnitude 0.5 threshold for tolerated earthquakes amounted, in effect, to a ban on fracking. Residents, on the other hand, largely oppose fracking near their homes. Fears of damage to property and the well itself at a fracking location in Lancashire, in the north of England, notably lowered house prices in the area. In the absence of a known mechanism by which fracking could cause earthquakes more than a mile or two from drilling sites, operators have often denied responsibility for such quakes. However, new research has now linked distant earthquakes to fracking, providing evidence that much larger areas surrounding sites may be at risk from drilling operations than previously demonstrated. This is a critical problem not only for fracking, but for cleaner energy solutions too. 


      Read more:
      Fracking causes earthquakes by design: can regulation keep up?


 Fracking involves injecting a high-pressure mixture of water, sand, and chemicals into shale layers to create fractures, opening pathways along which trapped gas in the shale can be extracted. Once this waste water has served its purpose, it can be reused for fracking injections at another site. By design, the breaking of rock that inevitably accompanies both waste water disposal and fracking produces small, usually imperceptible earthquakes. Occasionally though, the injection of fracking fluid or waste water can cause movements in natural pre-existing geological faults – large cracks that already exist in the rock. This can trigger the release of loaded energy stored in the fault, in much the same way a skier can trigger the release of an avalanche. If sufficiently severe, the resulting earthquake can cause damage to houses, threatening local communities. Some of these earthquakes occur very near the fracking site itself, but others have been reported as far as 50 kilometres away, making it difficult to guarantee the safety of surrounding areas. The new study, published in Science, takes a significant step forward in understanding this phenomenon. Experimenting in shallow geological faults, the researchers found that pumping water into these areas caused the rock along the fault lines to slowly slip. These “silent” movements didn’t produce earthquakes at the initial point of slippage, but gradually increased the pressure on more distant parts of the faults, inducing earthquakes much further away from the borehole than the injected fluid could reach. The research shows that by this mechanism, fracking can induce earthquakes tens of kilometres away. In Oklahoma, where fracking is an established practice, millions are at risk from property damage. This, of course, is not good news – but the first step in assessing whether a problem can be solved is understanding it. Setting the wider debate over the legitimacy of fracking to one side, the results are an important step forward in determining whether the key safety concern with fracking can be resolved. For example, we may soon be able to make accurate calculations of the extent of vulnerable areas, and the timescales on which earthquakes could occur. Being able to provide reliable information to residents and authorities would tackle the unknown in what is often an emotionally charged debate, and allow all involved to make an informed decision on whether fracking should be allowed.  It’s important to note that the problem of induced earthquakes is not just reserved to fracking. Several potential sources of clean energy and carbon dioxide removal technologies are also prone to inducing earthquakes. For example, most geothermal power stations re-inject the hot water extracted for electricity generation back into the ground to prevent reservoirs from running dry. Dry rock geothermal power stations also inject high-pressure water into deep wells to extract heat from fractured rock near the Earth’s core, causing earthquakes in a similar way to fracking. Underground storage of captured carbon dioxide – likely to be key in supporting the transition towards clean energy – can also induce earthquakes. An earthquake-induced rupture of an artificial carbon dioxide reservoir would nullify costly efforts to keep the gas out of the atmosphere, as well as posing health risks to local residents – so understanding how to manage such risks is imperative in the development of such technology. Much work is still required, and it’s not yet certain whether there is a way to stop underground fluid injections from causing earthquakes. But at the very least, we are one step closer to finding out. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterVeteran meteorologist Klaus-Eckart Puls made a presentation on sea level at the 4th climate conference in Munich at the end of last year. The data are clear: sea level rise is slowing down.

Here are the data Puls presents:
0:35 – Not everyone is convinced sea levels will rise quickly. Qatar just built a stadium on a man-made island.
1:30 – Sea level is complex and ranges on the planet from 110 meters below and 85 meters above the average due to gravitational variations. Geophysics is the main factor in sea level, and not climate. From all the physical factors, the talk in the media is only about the relatively minor climate component.
3:40 – Global mean trend from 60°N to 60°S is 2.8 mm. (Topex, Jason 1 and Jason 2, 1993 – 2010).
4:20 – 10,000 years ago the sea level was over 100m lower than today.
5:00 – The North German sea level has risen 1.35 m over the last 400 years, i.e. 35 cm per century. But from 1900 to 2000 it rose only 25 cm – a slowdown even though CO2 and temperature increased.
5:50 – It’s not the climate that’s a catastrophe – it’s the media.
6:40 – The sea level at the North German bight measured by 14 tide gauges shows a deceleration in sea level rise, 1843-2008. German authorities “see no signs of any climatic related sea level acceleration”.
8:30 – International tide gauges also show: No acceleration in sea level rise. In fact tide gauges show a deceleration. Puls asked the Potsdam Institute for Climate Impact Research for an answer, but they have yet to reply.
9:00 – EUMETSAT shows a deceleration over the last few years.
9:30 – “The measured data show us completely different results from the models, and that for 20 years.”
9:40 – The last two years show a distinct sea level drop.
10:30 – Envisat ESA satellite also shows a clear sea level drop since 2009.
11:10 – Puls compares tide gauge data (1.7 mm / year) and satellite measurements (3.27 mm / year). Scientists are baffled by the disagreement. Puls says, “It is obviously a measurement system problem.” Simon Holgate: “It is improbable that the sea level rise accelerated in the same year satellites began to operate.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




12:30 – John Church: It is unclear if there has been an acceleration since 1993.
12:55 – Tuvalu 1992 – 2009 data show that variations are tied to the Southern Oscillation. Moreover, Tuvalu is located on a tectonic fault and measured sea level changes there are due to tectonics, and not climate change.
16:00 – Quotes Mörner: “A 2°C warming of the upper 100 meters of the ocean would lead to only a 35 mm rise”. Thermal expansion of the ocean is completely exaggerated. It would take thousands of years to do that because to warm 1 meter of water 1°C, you need to cool 3000 meters of air 1°C.
17.20 – Quotes Trenberth: “No ocean temperature increase in the last 10 years. Moreover there seems to be a slight cooling”. Clearly if there is no ocean warming, then there can be no thermal expansion.
19:15 – Quotes Hans von Storch: “A statistical relationship between air temperature and change in sea level cannot be postulated.”
20:00 – Puls reminds us that there are still some journalists who keep saying the North Pole is melting and so the sea level is rising.
20:30 – The Antarctic is getting colder, and sea ice extent is growing. So is the thickness.
20:56 – Quotes the Alfred Wegener Institute (AWI), “A warmer climate in the Antarctic could however lead to more snowfall. The Antarctic ice sheet would tend to thicken rather than melt in the event of warming.”
21:50 – Quotes Prof. Dr. Heinz Miller of the AWI. “According to our calculated scenarios, we come to the conclusion that changes in the huge ice mass cannot contribute to sea level rise.”
22:20 – The AWI also said: “Although Greenland will likely lose ice mass, the loss of mass resulting from the melting in Greenland will be compensated by an ice gain in Antarctica.” No wonder the IPCC is continuously revised its sea level projections downwards.
23-20: Rahmstorf’s projection of 1.40 meters sea level rise by 2100 has nothing to do with reality.
23:40 – The newest panic is ocean acidification. Puls concludes: “Ocean acidification is an artifact!”
25:00 – Shows slides(in English of quotes by Nils-Axel Mörner.
27:00 – Puls concludes: “There is not going to be any acceleration of sea level”.
 
Share this...FacebookTwitter "
"Polluting the environment is a crime which can have countless victims – of numerous species and future generations. Whether it’s an oil spill in the sea, a release of raw sewage into a river, or a cloud of toxic gas into the air, the public has a clear interest in seeing criminal acts of pollution punished.  For a long time, courts have often been seen as soft on polluters, hesitating to penalise environmental criminals harshly. Yet recently, it seems that large fines against corporations have become increasingly common in the UK. In March 2019, Severn Trent Water was fined £500,000 for spilling thousands of gallons of raw sewage in a Birmingham park. It was the latest in a series of expensive court appearances for water companies over the last half decade. In 2014, the Court of Appeal handed a £250,000 fine to Thames Water, following the illegal discharge of untreated sewage materials into a stream in the North Wessex Downs. A year later, United Utilities was fined £750,000 after a pumping failure resulted in a raw sewage spill into the protected Duddon Estuary in Cumbria.  In 2016, Thames Water found itself in the dock again for illegally emitting sewage debris and sludge into the Grand Union Canal. It received a record £1m fine – a record which lasted little more than a year. In 2017, the same company was charged with emitting an estimated 1.4 billion litres of raw sewage into the river Thames and fined £20m. These large fines represent a visible change in the way courts have responded to environmental crimes committed by large corporations. It is a change which followed the introduction in 2014 of specific sentencing guidelines for environmental offences, which provide a set of considerations a judge must take into account when sentencing an environmental offender.  They include the culpability of the offender, the level of environmental harm caused, and the offender’s financial means. Evidence from the Sentencing Council indicates that the median fine levied against corporate offenders has more than doubled since they were introduced. So does this mean environmental criminals can now expect hard justice when committing environmental crimes? Sometimes, yes – at least if you are a large utility company causing significant environmental harm. But there are several reasons to think the impact of the fines might be limited.  First, the evidence compiled by the Sentencing Council suggests no spike in the level of fines handed down to individual (non-corporate) criminal offenders. This could mean that severe penalties are not applied to every transgressor.  Second, the increase in severity of fines has taken place against a drop in the overall number of prosecutions brought by the Environment Agency.  Third, many of the companies in charge of Britain’s crumbling water utility infrastructure are large companies which generate hefty profits. The £20m fine against Thames Water amounts to less than two weeks’ worth of the company’s profits. Is that an inadequate deterrent? Yet on the positive side, a move towards stricter penalties does create an incentive for polluters to come forward and report accidental, yet criminal, environmental harm to the Environment Agency. By doing so, they can enter into a so-called “enforcement undertaking”, a legally binding agreement between an offender and a regulator. In these, an offender aims to take certain steps to cease illegal activities which cause environmental harm, and promises to make specific changes to its operations.  Since being made available in 2011, the Environment Agency has accepted over 300 enforcement undertakings, and collected more than £13m in payments to environmental organisations and community groups. Importantly, almost all enforcement undertakings agreed by the Environment Agency include provisions for compensation to third parties affected as a result of the crime or for charitable donations to be made to environmental organisations. An example of this was when after the spillage of raw sewage in County Durham, Northumbrian Water paid £135,000 to three local environmental charities. From an offender’s perspective, there is much to like in an enforcement undertaking, which can avoid the stigma and reputational damage of a criminal sentence. Similarly, they are popular with the Environment Agency which is able to save on the costs of a criminal prosecution. The undertakings also allow for the community affected by pollution to receive some kind of financial compensation.  These positive developments notwithstanding, there are risks associated with the widespread use of enforcement undertakings. We don’t know, for example, what the £13m in payments to environmental organisations is actually spent on, as there is no public scrutiny or accountability mechanism overseeing this. Nor do we know how negotiations between the Environment Agency and a polluter are conducted.  These limitations, however, might not be reason enough to limit the use of enforcement undertakings which are generally seen as a cost-effective and informal way of securing compliance.  Importantly, as Brexit will fundamentally alter the legal landscape for environmental protection, the need for innovative and effective action is likely to increase. The combination of heavy fines and negotiable enforcement undertakings provides a solid foundation from which to respond to those changes."
"The chancellor missed a key opportunity to take leadership on the climate crisis with his budget, leaving the UK with little leverage to persuade other countries to join in with tougher targets on carbon, green campaigners said on Wednesday. Incentives for electric vehicles, as well as £300m to tackle air pollution, a £640m fund for nature and climate, cash for carbon-capture technology and a tax on plastic, were outweighed by a massive road-building scheme, no change to taxation for the oil and gas industries, and continuation of the 10 years-long freeze on fuel duty.  The fuel duty freeze alone costs about £9bn a year, and according to Carbon Brief that means emissions are 5% higher than they would be if duty were unfrozen. Also missing was any attempt to fix Britain’s heat-leaking housing, despite a £9.5bn pot for affordable housing. In the Conservative party manifesto last year there was a commitment to spend £9bn on domestic energy efficiency, which would insulate homes, making them warmer and lowering household energy bills, while also reducing greenhouse gas emissions. The UK is to host the vital UN COP26 climate talks, in Glasgow, this November, and green groups were hoping the chancellor would set out clear signals as to how the UK would meet its target of net zero emissions by 2050. Doing so is seen as essential in getting other nations to sign up to the tough new carbon commitments needed to drive through COP26 decisions. “This budget fails to put the UK on track to net-zero emissions, which is a major concern ahead of COP26,” said Ed Matthew, COP26 director of the Climate Coalition. “If the UK cannot get its own house in order it is at risk of crashing the climate talks before they have begun.” Youth climate activists tried to challenge the chancellor on his way to deliver the budget in parliament this morning, urging him to do more to stimulate the economy through tackling the climate emergency. But they were disappointed. Fatima Ibrahim, co-executive director of Green New Deal UK, which led the protest, said: “Ahead of COP26 we were hoping today would see a credible and comprehensive plan set out for how the British economy could reach net-zero as soon as possible. Instead, it’s been pushed back – an indication of how seriously this government is taking the climate crisis.” The government’s commitment to scrapping tax breaks on red diesel, a particularly polluting form of the fuel, was set against road-building – which includes among many schemes “improving traffic flows” on the A303, “unclogging Manchester’s arteries”, and investment in the A46 in the Midlands – and a house-building programme without clear accompanying net-zero emission commitments. Some green groups had urged the chancellor to devote at least 5% of public expenditure to the climate. Rebecca Newsom, head of politics at Greenpeace UK, said: “Far from ‘getting it done’ for climate and nature, the chancellor has completely missed the opportunity to address the climate emergency. Instead, by announcing £27bn for new roads it seems he’s driving in the opposite direction. Ending the red diesel tax break, and the Nature for Climate Fund announcements, are important steps. But they are just a fraction of what is needed to get the UK on track to delivering net zero before COP26.” The chancellor was to have announced a new national infrastructure strategy, encompassing hundreds of billions of pounds in public and private sector spending on low-carbon energy, transport, communications. But shortly before the budget that was put off to an undisclosed date later this spring. Whitehall sources said it was because some spending would have to be reviewed in light of the court ruling that ministers should have taken the Paris agreement on climate change into account when deciding on the Heathrow expansion plan. Lord Stern, one of the UK’s leading climate economists, said the national infrastructure strategy would mark another key opportunity to put the UK back on track. “The strategy must embody the commitment to net zero emissions and should lead to the rapid expansion in zero-carbon electricity that is required over the coming decades. The UK has a chance to lead the world at COP26. It must lead strongly by example now, and act in the best interests of the whole world as a key element in ‘global Britain’.”"
"

On May Day, Noah Keenlyside of Germany’s Leipzig Institute of Marine Science, published a paper in _Nature_ forecasting no additional global warming “over the next decade.”



Al Gore and his minions continue to chant that “the science is settled” on global warming, but the only thing settled is that there has not been any since 1998. Critics of this view (rightfully) argue that 1998 was the warmest year in modern record, due to a huge El Nino event in the Pacific Ocean, and that it is unfair to start any analysis at a high (or a low) point in a longer history. But starting in 2001 or 1998 yields the same result: no warming.





Science no longer provides justification for any rush to pass drastic global warming legislation. 



The Keenlyside team found that natural variability in the Earth’s oceans will “temporarily offset” global warming from carbon dioxide. Seventy percent of the Earth’s surface is oceanic; hence, what happens there greatly influences global temperature. It is now known that both Atlantic and Pacific temperatures can get “stuck,” for a decade or longer, in relatively warm or cool patterns. The North Atlantic is now forecast to be in a cold stage for a decade, which will help put the damper on global warming. Another Pacific temperature pattern is forecast not to push warming, either.



Science no longer provides justification for any rush to pass drastic global warming legislation. The Climate Security Act, sponsored by Joe Lieberman and John Warner, would cut emissions of carbon dioxide — the main “global warming” gas — by 66 percent over the next 42 years. With expected population growth, this means about a 90 percent drop in emissions per capita, to 19th‐​century levels.



Other regulatory dictates are similarly unjustified. The Justice Department has ruled that the Interior Department has until May 15 to decide whether or not to list the polar bear as an endangered species.



Pressure to pass impossible‐​to‐​achieve legislation, like Lieberman‐​Warner, or grandstanding political stunts, like calling polar bears an “endangered species” even when they are at near record‐​high population levels, are based upon projections of rapid and persistent global warming.



Proponents of wild legislation like to point to the 2007 science compendium from the U.N. Intergovernmental Panel on Climate Change, deemed so authoritative it was awarded half of last year’s Nobel Peace Prize. (The other half went to Al Gore.) In it there are dozens of computer‐​driven projections for 21st‐​century warming. Not one of them projects that the earth’s natural climate variability will shut down global warming from carbon dioxide for two decades. Yet, that is just what has happened.



If you think about it, all we possess to project the future of complex systems are computer models. Therefore, if the models that serve as the basis for policy do not work — and that must be the conclusion if indeed we are at the midpoint of a two‐​decade hiatus in global warming — then there is no verifiable science behind the current legislative hysteria.



What does this mean for the future? If warming is “temporarily offset” for two decades, does all the “offset” warming suddenly appear with a vengeance, or is it delayed?



Computer models, like the one used by Keenlyside, et al., rely on “positive feedbacks” to generate much of their warming. First, atmospheric carbon dioxide warms things up a bit. Then the ocean follows, raising the amount of atmospheric water vapor, which is a greater source of global warming than carbon dioxide. When the ocean does not warm up, it seems that the additional warming is also delayed.



All of this may mean that we have simply overestimated the amount of warming that results from increases in atmospheric carbon dioxide.



That final point has been a subject of debate for a long time. Several recent publications in the peer‐​reviewed literature argue that observed changes in temperature show the “sensitivity” of temperature to increasing carbon dioxide is lower than earlier estimates.



All of this suggests a 21st‐​century warming trend that will be lower than the average value calculated by the climate models in the IPCC compendium.



But who really knows? Before Keenlyside dropped his bombshell, few scientists would have said publicly that global warming could stop for two decades. Anyone raising that possibility would doubtlessly have been treated to the smug reply that “the science is settled,” and that only the most bumptious ignoramus could raise such a question.



One final prediction: The teeming polar bear population will be listed as “endangered,” and in the next year or two, Congress will pass a bill mandating large and impossible cuts in carbon dioxide.



What is “settled” is the politics, not the science.
"
"Early spring temperatures in the town of Deadhorse, on the north coast of Alaska, average -17°C. But with global warming affecting the Arctic more than anywhere, things are changing fast. At the end of March 2019, temperatures in Deadhorse hit 3°C, a whole 20°C warmer than the long-term seasonal average. Such huge variations are not normal or natural, and it is important we understand their long term environmental impacts. Now, scientists working on an island off the north coast of Canada, relatively near to Deadhorse, have discovered evidence that warming in the Arctic is triggering thousands of landslides that could reshape the landscape for good. The landslides are found in those high latitude areas where the ground consists of frozen soil and rock, known as “permafrost”. Closer to the poles, and at higher elevations, the surface can be frozen all year. Where temperatures are slightly less cold in the summer, perhaps because of a lower altitude or a greater distance from the pole, the surface of the permafrost melts most years. This melted layer is often saturated with water and thus is very weak, creating boggy conditions when the thaw develops. And where the permafrost is on a slope, melting often rapidly leads to instability, resulting in landslides.  As temperatures rise, those “summer months” above freezing can stretch into spring and autumn, the hottest days become hotter, and large areas of permafrost are melting. One potential impact is an increase in landslides, and recent studies have suggested that there is some evidence that this might be occurring on some high altitude slopes.  But the latest research from Canada, published in the journal Nature Communications, has demonstrated a dramatic increase in landslides in recent years – even at sea level. The researchers focused on Banks Island, an expanse of treeless tundra about the size of Sri Lanka or Ireland, with 68,000 muskoxen and just 112 humans.  They looked at a particular type of landslide known as “retrogressive thaw slumps” which can occur on comparatively gentle slopes when permafrost thaws. These landslides generally move slowly downhill, but what makes them particularly problematic is that once they start they tend to grow, and it is very difficult to arrest their development.  The team looked for signs of these landslides in archive imagery of Banks Island between 1984 and 2015. The results were dramatic: in 1984 only 63 active retrogressive thaw slumps could be identified, but by 2013 this number had increased to 4,077 – a 60-fold increase. Mapping the number of new landslides occurring each year over the study period, scientists realised that many more landslides developed in years with particularly warm summers. In the four warmest summers – 1999, 2011, 2012 and 2013 – almost 3,900 landslides developed. The study has significant implications. First, it shows a dramatic increase in the rate of permafrost degradation through landsliding. Most of the new landslides are now causing substantial erosion as the released sediment moves into watercourses and will cause changes in lakes and rivers even away from the landslides.   Second, it is clear that these changes are associated with years with high summer temperatures. While this study only looked at a single – albeit very large – island in Canada, there is nothing exceptional about this region climatically or geologically, which suggests that something similar will be happening in many other permafrost areas. And finally, rising temperatures in the Arctic mean permafrost is likely to degrade at a dramatically increased rate. Using the IPCC climate projections, in this area alone the researchers expect 10,000 or more new landslides per decade by 2075. If this is extended across other permafrost areas in Canada, Siberia and beyond, high latitude climate change will have a major impact on the landscape. This is likely to alter the ecosystems in these areas, but as yet it is impossible to estimate the ways in which they will change. Through time we are getting a better sense of how landscapes are responding to changes in climate. It is clear that permafrost is extremely sensitive to change – and the effects are likely to be profound. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterGerman geologist Dr. Sebastian Lüning, co-author of the best-selling skeptic book Die kalte Sonne, which has created a controversy in Germany, is speaking today at the Heartland Climate Conference in Chicago.
Click below for a live feed:

Title of Dr. Lüning’s presentation:
The Medieval Warm Period within the Context of Millennial Scale Climate Cycles
12:45 p.m. – 2:30 p.m local time, which is 19.45 – 21.30 CET. (You may want to check in one hour early to make sure).
Content:
– German reaction to the book
– How solar cycles have impacted climate during the Holocene
– Temperature reconstructions worldwide
– Problems with the IPCC models
– Little Ice Age caused by volcanoes?
– Warming or cooling ahead?
I hope as many readers as possible will join in, watch and comment.
 
Share this...FacebookTwitter "
"Britain’s farmers are almost 18 times more likely to be killed on the job than the average industrial worker, and the fatality rate is increasing. Look through the government’s summary of the 33 fatal farm, forestry and fishing accidents in 2017/18 and there were a number of types of fatalities such as falls, crushes, electrocutions and equipment malfunctions. Most people (but not farmers) might be surprised to learn that work with cows is particularly dangerous – “crushed by a bull” was the single most common cause of death. So what can be done? There’s no doubt that farming is a hard and relentless occupation and workers endure long hours and heavy workloads. For many farmers, it is more a way of life than a hobby or a job, and farms are often passed down from generation to generation, with families following the customs and traditions of their ancestors.  With increasing financial pressures, weather considerations and hard workloads, farmers are often forced to work long hours and cut corners in relation to safety. A number of jobs are required to be carried out on the farm and it has long been tradition among farmers to be a “Jack of all trades” instead of wasting money on specialist tradesmen or contractors. As such, it has become part of the culture of farming to turn your hand to anything, which begs the question of whether farmers actually realise farming is unsafe. This traditional mindset, combined with ever-increasing fatalities from farming incidents, demonstrate an urgent need for change. The sector does not appear to be learning from mistakes made. If farmers themselves won’t take the lead then the government’s Health and Safety Executive (HSE), which regulates safety at work, must change its approach to the industry. Things aren’t simple enough to be solved by top-down action alone, however, and change will also require a more collegiate approach across the sector. Either way, cultural change will be hard and will take time. Farming is one of the most dangerous industries, yet was a latecomer compared to other industries in terms of regulation. Although regulation and enforcement has increased over time, accidents and fatalities continue to rise and the same types of incidents reoccur time after time, demonstrating a failure in the system.  While we understand that agricultural incidents can happen and will continue to happen, there is no legal requirement for farm workers to undertake any form of health and safety course or training. Regulations are in place, but they will only work if farmers understand them and take their time to put measures in place to prevent or at least reduce such horrific incidents. People have been highlighting these issues for a long time. Back in the 1960s, a government researcher named GS Wilson compiled a report on farm safety and concluded that, “although men have readily adapted themselves to new machines and methods, they have not proved as able to recognise new dangers and learn how to guard against them”. The types of agricultural accidents Wilson looked at are still the main causes of injury and death today. It seems there has been little progress over the past five decades, and lessons are not being learned.  The HSE does provide a number of leaflets and booklets to help agricultural workers understand their obligations to comply with the law and work safely. The problem is ensuring that all those who are working on farms, whether that be employed, self-employed or the employer, have engaged with such advice. The HSE and the National Farmers Union are both trying to raise awareness of safety issues, and run various education campaigns. These initiatives are important as there is no legal requirement for farmers to attend any health and safety courses. Farmers do require certification for insurance purposes and for compliance with hazardous substances. Perhaps implementing similar certification for farm safety would be a good step forward. At the turn of the century, it was the construction industry that had more fatalities per worker than any other sector. It then implemented a continued professional development programme, which ensures that workers undertake training at set intervals to enable them to carry on working on sites. If it worked for construction, then surely it could also work in the sector that has now adopted the “most fatal” mantel – farming."
"Those least responsible for global warming will suffer the most. Poorer countries – those that have contributed far less to climate change – tend to be situated in warmer regions, where additional warming causes the most devastation. Extreme weather events such as Syria’s prolonged drought, South Asia’s catastrophic monsoon floods, and Cyclone Idai in South-East Africa, the third deadliest cyclone on record, are becoming more likely and more severe. These events are disproportionately bringing death, displacement, and crop failure. As a result of this, projections estimate that the economies of poorer, warmer countries will be gravely harmed by climate change over coming decades, while the cooler, richer countries responsible for the vast majority of the extra CO2 in the air may even benefit in the short term. But as new research reveals, this is not just a future concern – the economic injustice of climate change has already been operating for 60 years. The study, published in the Proceedings of the National Academy of Sciences, compared different countries’ GDP per capita – a measure of the average person’s economic standard of living – between 1961 and 2010. It then used climate models to estimate what each country’s GDP would have been without the effects of climate change. The findings are stark. Many poorer countries’ economies have rapidly grown in the last 50 years, albeit often at great social and environmental cost and to the benefit of the globalised economy. But even that growth has been held back substantially by climate change – the gap in GDP per capita between richer and poorer countries is 25% higher than it would have been in a climate-stable world. And with most richer countries sitting below and poorer countries above the 13℃ average annual temperature at which economic productivity peaks, global temperature rise is an immediate driver of this inequality. Of the 36 countries with the lowest historical carbon emissions, which are also some of the poorest and hottest countries in the world, 34 have suffered an economic hit compared to a world without warming, losing on average 24% of GDP per capita. The poorest 40% of countries, much of which are located in sub-Saharan Africa, Asia, and Central and South America, have lost between 17 and 31% of GDP in the last half century. India, one of the lowest emitters per capita, has been regarded as an economic growth champion in recent decades – but climate change has slowed its progress by 30%. While the country’s services sector has boomed, the agricultural sector – which employs half of India’s total workforce – has suffered greatly. A three-fold rise in extreme rainfall events and increased severe droughts have reduced crop yields and cause between $9 and 10 billion in damage per year to the agricultural industry alone. The same events also regularly bring India’s urban economic hubs to a standstill. With 12m inhabitants, Mumbai has the world’s largest population exposed to coastal flooding. Deluges in 2005 and 2014 forced the city’s international airport and roads to close, and cost millions in property damage.  Increasingly intense Indian summers that now regularly hit above 45℃ reduce productivity, kill thousands, and cause thousands more to commit suicide.  Add to this the multi-billion pound costs of rescue and rebuilding from cyclones such as 1999’s Odisha storm, which left two million homeless, and it’s easy to see how climate change can stunt the economic growth of India and similarly affected countries. For the world’s wealthiest countries however, climate change has added to the coffers – 14 of the 19 highest-emitting countries now find themselves in a better economic position than they would have been if the planet’s temperature had stayed constant, with an average boost of 13%. The US economy has suffered, but by a miniscule 0.2%, while the UK finds itself 10% better off. The 2018 heatwave there posed its own risks to health and crops, but it also provided huge boosts to ice cream sales and tourism. As is becoming increasingly clear, there are no quick fixes or easy solutions to climate change or inequality. Reducing emissions is, sadly, not enough, and providing yet more high-interest loans to “help” poorer nations adapt to a warmer world will only deepen global inequality. Alongside radically changing the economies of the world’s wealthiest nations, we must demand that reparations for past injustices be paid, that the debts of the Global South be cancelled, that privatisation of local industries and lands be reversed, and that the brutal border regimes surrounding the world’s wealthy nations be torn down. Only then can global inequality truly be tackled. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"**A coronavirus vaccine will be ready to be used in Wales within a week of getting the go ahead, the first minister has said.**
Mark Drakeford said Welsh ministers are ""working on the capacity now. A lot of work has been done already"".
England and Scotland have already made vaccine rollout announcements.
Plaid Cymru called for an ""urgent, clear and comprehensive vaccination plan"", urging the Welsh Government not to be ""vague"" on such a key matter.
Speaking to BBC Radio Cymru's Dros Ginio programme, Mr Drakeford said: ""What other places have done is shown an ambition. What we're doing here in Wales is to plan first before we make an announcement.""
A number of vaccines have recently reported successful trials but none have yet gained safety approval.
Scottish First Minister Nicola Sturgeon has said her government hopes to vaccinate a million people by the end of January, while in England they hope to have all vulnerable people vaccinated by Easter.
Mr Drakeford said:""We don't know yet, and people in Scotland don't know yet, how much of the vaccine will be available because the system hasn't yet been set up.""
On Wednesday, he told the BBC Wales Live programme that If Wales were to use the Pfizer vaccine, which has to be stored at -70 degrees, that the plan is to use equipment from the Welsh Blood Service.
""We can use the equipment the Wales Blood service already has to store material at that temperature and we can make it available for this vaccine,"" he said.
""The vaccine will have limitations, it will be difficult to transport but we will find ways of doing it.""
Plaid Cymru's health spokesman Rhun ap Iorwerth said that ""seeing the kind of roll-out plans in Scotland just reinforces the need for such a clear plan in Wales"". ""We need to know how - once approval is given for the vaccine - the vaccine will be rolled out in Wales, including timings, recipients and logistics,"" he said.
Asked about plans to tighten restrictions before Christmas, Mr Drakeford insisted the ""firebreak was successful"" and that ""it did everything we expected it to do"" but acknowledged the numbers of people with coronavirus were increasing.
He said the Welsh Government was looking at what is happening in other parts of the UK to ""see if there are things we can learn and if there are things we can put in place to help us with the figures we're seeing now"".
""We accept that if we're going to do something in the hospitality sector then we're going to have to find more support for the sector and its supply chain,"" he said.
Mr Drakeford said conversations were ongoing about how to find funds to support the hospitality sector and the best way to distribute it.
He told Dros Ginio: ""We're working on things today- what money can we bring together and to what purpose can we use to help things but we haven't come to the end of those discussions yet."""
"
I have a few very important (and personal) announcements to share with the WUWT community because they will impact content and moderation over the next few weeks. Please take a moment to read this.

1. There is a pressing and very serious medical challenge in my family that will require surgery and rehabilitation time. I can’t and won’t go into details. This will require me to take periods of time away from WUWT over several weeks if not months. I don’t know the full time impact yet. This coming Friday and perhaps Saturday I’ll be offline.
2. That said, I welcome the help of any who wish to help with moderation or who wish to submit stories and essays. We could use the help of a moderator from midnight to 4AM PST. I know we have a few regulars that post during that time, and if you’d like to help, leave a comment please. WUWT has always had guest posts, and they will continue to be welcome. Leave a comment if you have an idea.
3. In case you missed it yesterday, the new ENSO/SST page is up and running. Be sure to bookmark it. This graphic on the sidebar is a direct link:

…plus it is also available in the header menu under Reference Pages. Other pages of reference graphics pages will be coming soon, more details here.
4. Steve Goddard told me several months ago that he  would like to start his own blog, covering a wider range of scientific, social,  economic and political topics than I wish to cover here at WUWT. You can see his blog at http://stevengoddard.wordpress.com
Thanks to Steve for  all his contributions to WUWT over the last two years.
I’ll be continuing the Sea Ice News feature every Sunday.
5. For those of you wondering about the upcoming surfacestations project paper, let me say that it is now mostly out of my hands and will not be impacted by item 1. I have a team of co-authors with far greater skills than I working on it. I’m pleased with the current draft having survived a critical review designed to strengthen it. When I have more, I’ll pass it along and of course at the appropriate time the surfacestations main page will get an update.
Thank you all for your consideration.  – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8982dc23',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Along with the keys to No 11 Downing Street, the chancellor is given the job of doling out hundreds of billions of pounds of taxpayer money every year.**
In fact, for the first time this year, government spending will top a massive Â£1 trillion. But where does it actually go?
Like most of us, the chancellor has priorities, necessities, a wishlist - and unforeseen bills
For every Â£100 the central government spends next year, the biggest slice - more than Â£20 - will go on welfare payments such as pensions and universal credit. Other includes the money which goes to smaller government departments as well as contingency spending, including some of the government's virus-related costs.
Much of the welfare spending is dictated by factors such as an ageing population or unemployment. These fluctuate and so are hard for the government to control.
The next biggest chunk - Â£17.50 - goes on health. Education accounts for a further Â£7, while defence covers Â£4.50.
We've heard a lot about how the government has to borrow to help fund the spending bill this year. But as part of that debt has been picked up by the Bank of England, and interest rates are so low, that slice accounts for just Â£2 of every Â£100 - the smallest in decades.
That interest too is hard to predict. And with spending already sketched out for some big departments, such as health and defence, the chancellor's announcement only revealed plans for about Â£35 of every Â£100 the government will be spending in the next year.
Foreign aid has grabbed headlines but accounts for just 70p; the cuts announced now reduces that to 50p.
Be it fixing potholes or extra cash for the armed forces, government departments have to plead their case with the chancellor. And there are always winners and losers.
In the past couple of years, the government has claimed that austerity is over, the spending tap has been reopened and every department has been bestowed with more cash.
But over the past decade or so, the cost of living has risen and the population has grown. So money has to stretch further. And that money is split between day-to-day spending - from salaries to operations - and investment in the likes of roads, or capital investment.
Strip out that investment spending, and allow for inflation and the growing population, and while the health service will be better off in the next few years, defence actually won't be. In other words, it is more of a stretch to maintain day-to-day public services
One in every Â£4 the government spends, goes towards paying our 5.5 million public sector workers.
Frontline staff from nurses to police officers were awarded inflation-busting pay rises in the summer as they battled in the face of the virus.
But they were also warned not to expect more. Now 1.3 million people will have their pay rises ""paused"" for a year, saving the chancellor a billion or two.
His argument is that it's not fair to give wholesale rises when so many private sector workers have seen their incomes shrink or been laid off - and it is they who foot some of the public sector pay bill.
Exempt from the freeze will be the 31% who work in the health service - and anyone whose pay is under Â£24,000.
Also exempt will be those employed by local government and the devolved administrations, whose pay will be determined there. But the employers of those two groups could decide to impose curbs themselves.
The curb on public sector pay may feel like that Rishi Sunak is playing Scrooge.
But the cost of fighting the spread of coronavirus, and limiting the economic fallout has soared. It has now hit Â£280bn for this year - accounting for about Â£25 in every Â£100 the government is spending.
Much of that has gone on health and other services. About Â£40bn has gone on test and trace, PPE and vaccine implementation - the equivalent of more than Â£1,500 per household. Questions are already being asked if those sums have been spent wisely or effectively.
And then there's the support to the economy. The bill for furlough is expected to reach almost twice as much, at Â£70bn, with a further Â£20bn going to help the self-employed.
Most of such schemes will end come next March. But the health response won't - Mr Sunak expects that he'll have to fork out another Â£55bn, over 5% of the spending pot, on that in the next financial year.
In normal years, the government covers the vast majority of its spending through the tax it takes in - from income tax to VAT to Air Passenger Duty - around Â£94 of every 100 last year.
But this year those sources of income have suffered - just as outgoings have soared. For the first time, government spending will top Â£1 trillion.
This year the government will only fund about two-thirds of spending through taxation. That's equal to the biggest shortfall since World War Two.
At present, the government can borrow cheaply to plug the gap. But not forever. Rishi Sunak has already indicated he'll be looking to raise taxes - not yet, (for it's more than the economy could stand ) but in the years ahead.
The chancellor may still be doling out cash - but payback is coming."
"It is easy to get nostalgic for the era when most music lovers bought LPs. They would save their pennies for a Saturday trip to the local record store, before heading home clutching their glorious new vinyl in a plastic bag to drop the needle on the turntable and listen on repeat. This anachronistic ritual will be resurrected on International Record Store Day on Saturday April 13, as consumers queue to buy exclusive limited edition vinyl releases from their favourite artists. Launched a decade ago, this annual event is an industry drive to boost ailing independent record stores in an age when most people stream music online.  But is it actually true that earlier generations placed a greater value on recorded music than music fans in the present day? We are loath to succumb to the mythology of a “golden age” for music and lend credence to baby boomers moaning of bygone days when music somehow mattered more than it does now. We decided to investigate the numbers to see if they told a different story. As it turns out, they do – and it’s far worse than we expected. We conducted archival research on recorded music consumption and production in the US, comparing the economic and environmental costs of different formats at different times. We found that the price consumers have been willing to pay for the luxury of owning recorded music has changed dramatically.  The price of a phonograph cylinder in its peak year of production in 1907 would be an estimated US$13.88 (£10.58) in today’s money, compared to US$10.89 for a shellac disc in its peak year of 1947. A vinyl album in its peak year of 1977, when The Sex Pistols’ Never Mind The Bollocks came out, cost US$28.55 in today’s money, against US$16.66 for a cassette tape in 1988, US$21.59 for a CD in 2000, and US$11.11 for a digital album download in 2013.  This fall in the relative value of recorded music becomes more pronounced when you look at the same prices as a proportion of weekly salaries. Consumers were willing to pay roughly 4.83% of their average weekly salary for a vinyl album in 1977. This slips down to roughly 1.22% of the equivalent salary for a digital album during its 2013 peak.  With the advent of streaming, of course, the business model of consuming recorded music changed: what used to be a commodity industry, where people bought copies to own, is now a service industry in which they buy temporary access to a music experience stored in the cloud. For just US$9.99 – barely 1% of the current average weekly salary in the US – consumers now have unlimited ad-free access to almost all recorded music ever released via platforms such as Spotify, Apple Music, YouTube, Pandora and Amazon.  Yet if consumers are paying an ever lower price for their music, the picture looks very different when you start to look at environmental costs. Intuitively you might think that less physical product means far lower carbon emissions. In 1977, for instance, the industry used 58m kilograms of plastic in the US. By 1988, the peak year for cassettes, this had dipped slightly to 56m kg. When CDs peaked in 2000, it was up to 61m kg of plastic. Then came the big digital dividend: as downloading and streaming took over, the amount of plastics used by the US recording industry dropped dramatically, down to just 8m kg by 2016.  But if these figures seem to confirm the notion that music digitalised is music dematerialised – and therefore more environmentally friendly – there’s still the question of the energy used to power online music listening. Storing and processing music in the cloud depends on vast data centres that use a tremendous amount of resources and energy.  It is possible to demonstrate this by translating plastic production and the electricity used to store and transmit digital audio files into greenhouse gas equivalents (GHGs). This shows that GHGs from recorded music were 140m kg in 1977 in the US, 136m kg in 1988, and 157m kg in 2000. By 2016 it is estimated to have been between 200m kg and over 350m kg – and remember that this is only in the US. Obviously this is not the last word on the matter. To truly compare past and present, if it were even possible, you would have to factor in the emissions involved in making the devices on which we have listened to music in different eras. You would need to look at the fuel burned in distributing LPs or CDs to music stores, plus the costs of distributing music players then and now. There are the emissions from the recording studios and the emissions involved in making the musical instruments used in the recording process. You might even want to compare the emissions in live performances in the past and the present – it starts to look like an almost endless enquiry.  Even if the comparison between different eras ultimately came out looking different, our overriding point would be the same: the price that consumers are willing to pay for listening to recorded music has never been lower than today, yet the hidden environmental impact of that experience is enormous.  The point of this research is not to ruin one of life’s greatest pleasures, but to encourage consumers to become more curious about the choices they make as they consume culture. Are we remunerating the artists who make our favourite music in a way that accurately reflects our appreciation? Are streaming platforms the right business model to facilitate that exchange? Is streaming music remotely from the cloud the most appropriate way to listen to music from the perspective of environmental sustainability? There are no easy solutions, but taking a moment to reflect on the costs of music – and how they have changed over history – is a step in the right direction. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"**People coming to the UK from Estonia and Latvia will need to quarantine from 04:00 GMT on Saturday.**
The two Baltic states have been taken off of the UK government's travel corridor list.
At the same time, Aruba, Bhutan, East Timor, Mongolia and some Pacific islands have been added, meaning travellers from those places will not need to self-isolate.
However, current rules ban travel abroad unless for specific reasons.
The UK government has also changed its rules on Denmark, Transport Secretary Grant Shapps said.
While travellers from Denmark to the UK will still need to self-isolate, the government is lifting the ""total travel ban"" on Saturday.
The Foreign Office currently advises against all but essential travel to Denmark amid concerns over a new coronavirus strain that has spread from mink.
Anyone arriving into the UK from most destinations must quarantine for 14 days.
But there are a list of countries exempt from the rules, meaning returning travellers do not need to self-isolate, called the travel corridor list.
From 15 December, people who need to quarantine will only need to do so for five days \- if they pay for a private Covid test and are virus free.
Making the announcement on Thursday, Mr Shapps said latest data means Estonia and Latvia must be taken off the list.
There has been a sharp rise in the number of Covid-19 cases in Latvia in recent weeks, according to the Foreign Office. The Latvian government has announced a state of emergency lasting until 6 December.
Estonia's government has also introduced extra restrictions from 24 November.
Mr Shapps said Bhutan, East Timor, Mongolia, Aruba and six Pacific islands (Samoa, Kiribati, Micronesia, Tonga, Vanuatu and Solomon Islands) had been added to the list, effective from 04:00 on Saturday.
In England until 2 December, foreign travel is currently only permitted for work, education or if someone has another valid reason.
People can only travel in and out of Wales with a reasonable excuse, such as going to work or school.
In Northern Ireland, people are advised to only travel for necessary reasons and to ""carefully consider"" their holiday and travel options, in light of the pandemic.
In Scotland, people living in higher risk areas should avoid unnecessary travel to other places."
"
Image: National Science Foundation
Guest post by Indur M. Goklany
In the earlier post reporting on the recent greening of the Arctic, some commentators — Crispin in Waterloo, BillD, Jimbo — have alluded to the notion that Arctic thawing could lead to positive feedback by adding to methane emissions to the atmosphere.
This global warming bogeyman is founded on the plausible notion — plausible, at least at first blush — that warming might release methane from methane clathrates (or hydrates) stored in the Arctic permafrost which would increase its concentration in the atmosphere.

But methane has a “global warming potential” averaged over 100 years of 25, that is, methane, ton-for-ton, is 25 times more powerful a greenhouse gas than carbon dioxide (AR4WG1 Technical Summary: 33). Thus, such releases of methane would constitute a positive feedback for global warming.
The initial concerns about methane stemmed from the fact that by the 1990s the atmospheric concentration of methane, which had been growing rapidly, had exceeded 1,730 parts per billion (ppb), almost twice the maximum amount measured over the past 650,000 years in ice cores (AR4WG1: 3).
Concern of runaway methane feedback was also stoked by a number of modeling studies which suggested rapid disintegration of the permafrost with global warming (e.g., Lawrence and Slater 2005, Zimov et al. 2006).  However, in a modeling study which took into consideration the thermal profile of the permafrost, and the fact that the melting effect of warm air surface temperatures on the upper layers of permafrost would be countered by cooling due to colder deeper layers of permafrost, Delisle (2007) showed that “massive releases of methane in the near future are questionable.”
Even more compelling is that the growth in atmospheric concentrations has slowed substantially. As noted by the IPCC AR4WG1 (p. 796):
Recent measurements show that CH4 growth rates have declined and were negative for several years in the early 21st century … The observed rate of increase of 0.8 ppb yr–1 for the period 1999 to 2004 is considerably less than the rate of 6 ppb yr–1 assumed in all the [IPCC] SRES scenarios for the period 1990 to 2000.”
The latest observations indicate that the rate of change is not increasing, and that they “are not consistent with sustained changes … yet” (Dlugokencky et al. 2009: 4). [Dlugokencky’s “yet” seems gratuitous — no matter, I’ll give it a pass.] They also indicate that the geographical pattern and the isotopic signature of methane increases suggests that the major sources are wetlands — probably tropical wetlands —rather than Arctic permafrost.
Petrenko et al. (2009) examined the source of isotopic methane in a glacial ice core from West Greenland to determine the probable source of the large increase in methane during the abrupt warming of +10±4°C that occurred during the transition from the Younger Dryas to the Preboreal (~11,600 years ago) (Grachev and Severinghaus 2005).  They concluded that “wetlands were the likely main driver of the [methane] increase and that clathrates did not play a large role,” a finding they noted “is in agreement with findings from previous ice core CH4 isotopic studies” (Petrenko et al. 2009: 508). This study essentially reiterated the results of another paper by many of the same researchers that appeared in Nature the previous year (Fischer et al. 2008). Notably the Petrenko et al. study’s publication was accompanied by an announcement titled, “Ancient Greenland methane study good news for planet, says CU-Boulder scientist” (Eureka Alert 2009).
So it seems that while methane emissions might increase if there is warming, there is no evidence of catastrophic releases from clathrates.
References
1. The above is, for the most part, extracted from:
Goklany, Indur M. (2009). Trapped Between the Falling Sky and the Rising Seas: The Imagined Terrors of the Impacts of Climate Change. Prepared for University of Pennsylvania Workshop on Markets & the Environment, draft, 13 December 2009.
2. Specific references follow:
AR4WG1 ≡ IPCC’s Fourth Assessment Report for Work Group 1 ≡ IPCC (2007). Climate Change 2007: The Physical Science Basis. Cambridge: Cambridge University Press.
Delisle, G. (2007), Near-surface permafrost degradation: How severe during the 21st century?, Geophys. Res. Lett., 34, L09503, doi:10.1029/2007GL029323.
Dlugokencky, E. J., et al. (2009). Observational constraints on recent increases in the atmospheric CH4 burden. Geophysical Research Letters, 36, L18803, doi:10.1029/2009GL039780.
Eureka Alert. 2009. Ancient Greenland methane study good news for planet, says CU-Boulder scientist. PR announcement, 23 April 2009. Available at http://www.eurekalert.org/pub_releases/2009-04/uoca-agm042109.php.
Fischer, H., Melanie Behrens, Michael Bock, Ulrike Richter, Jochen Schmitt, Laetitia Loulergue, Jerome Chappellaz, Renato Spahni, Thomas Blunier, Markus Leuenberger  &  Thomas F. Stocker (2008). Changing boreal methane sources and constant biomass burning during the last termination. Nature 452: 864 -865.
Grachev, Alexi M.  and Jeffrey P. Severinghaus (2005).  A revised +10±4 °C magnitude of the abrupt change in Greenland temperature at the Younger Dryas termination using published GISP2 gas isotope data and air thermal diffusion constants. Quaternary Science Reviews 24 ( 5-6): 513-519.
Lawrence, D. M., and A. G. Slater (2005). A projection of severe nearsurface permafrost degradation during the 21st century, Geophys. Res. Lett., 32, L24401, doi:10.1029/2005GL025080.
Petrenko, Vasilii V.; Andrew M. Smith, Edward J. Brook, Dave Lowe, Katja Riedel, Gordon Brailsford, Quan Hua, Hinrich Schaefer, Niels Reeh, Ray F. Weiss, David Etheridge, and Jeffrey P. Severinghaus. 14CH4 Measurements in Greenland Ice: Investigating Last Glacial Termination CH4 Sources. Science 324: 506-508.
Zimov, S. A., E. A. G. Schuur, and F. S. Chapin III (2006). Permafrost and the global carbon budget, Science, 313, 1612–1613.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e86d6d8a1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGeologist Dr. Sebastian Lüning’s and Prof. Fritz Vahrenholt’s website looks at the NOAA’s and NCDC’s relentless search for new climate records with which to scare the public. But as they show, despite what these alarmists conjure up, the warming is not getting faster, period.

Figure 1: Temperature development of the last 33 years based on UAH satellite data. Source: climate4you.
We keep hearing scary claims like: “May 2012 was one of the hottest the globe has seen since records began!” or “CO2 in the Arctic hits 400 ppm level!” or “2011 was the warmest year with a cooling La Nina-effect!” are just some recent examples. In the USA, the picture looks the same, where the warmest May since 1895 was recorded (see Climate Central and WUWT).
But as Lüning and Vahrenholt explain, many of these records use arbitrary startpoints in the statistical record and are thus designed to create “a record”. It’s time to get back to science, they say.
A look at the global satellite temperature series shows that May 2012 is in no way anything unusual and fits right into the ongoing temperature plateau the Earth has been stuck at for quite some time now (see Figure 1). Lüning and Vahrenholt write:
It is quite amazing how stubborn this warming stop has been. Not one of the IPCC models had predicted this plateau. Also the hyped up temperature prognoses made by Hartmut Graßl and James Hansen have been shown to be far from reality, [read here, for example.]”
Ok, one could say that the temperature plateau is only 14 years long, and shouldn’t longer periods be considered? Here often the ominous 30-year rule gets applied. Somehow that’s considered okay. But luckily we also here have a development that’s good news. With each year the temperature plateau extends, the 30-year window shifts a step out of the strong warming period of 1977-1998. As a result a greater part of the plateau enters the calculation. Year by year the warming rates decreases.
But for many, 30 years are impractical. Satellite data have been around for only 33 years, for example, and here not much can be statistically calculated. Therefore, a group led by IPCC lead author Ben Santer once checked over which intervval length actually makes sense in order to find the man-made impact on temperature. They reached the result that it has to be at least 17 years.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And lo and behold we find a world record of a completely different type: Currently we are experiencing the lowest temperature rise of a 17-year series since satellite temperature data began. At the moment the warming rate is a minsicule 0.04°C per decade; this is an absolute record low. A few years ago the figure was up to 0.26°C per decade, i.e. more than six times higher.”
Gee, I wonder if we’re going to read that in the newspaper or any NOAA press release? Don’t hold your breath.

Figure 2: Currently we are experiencing the lowest temperature increase of a 17-year series since satellite data began. Source: dh7fb.
While some places like the USA are enjoying warm weather, the opposite is true in great Britain, which has seen weeks of cool, rainy weather. Lüning and Vahrenholt write: “The forecasts project that there will not be any noteworthy warm spell until at least September. Daily Express writes ‘Summer starts in September’.”
That takes us to another new record: the most missed “barbecue summers”!
Lüning and Vahrenholt conclude:
While alarmist pseudo-records can help to fill newspaper space and attract funding, the sense behind this selective approach has to be regarded with great skepticism.”
 
Share this...FacebookTwitter "
"Patrick Barkham asks if nature really can cure us of our mental health problems (Green Prozac, Review, 14 March). He eloquently describes how reconnecting with the natural world can help us at least recuperate and find solace in a way that our urban environments cannot. However, as Richard Mabey says in the article, it is not a panacea and nature itself is going through its own crisis; no amount of walking through silent woodlands and desertified fields will provide the sought-after cure. For those of us who have loved and immersed themselves in the countryside over many decades and written about it, our closeness to it can only add to our stress and worry. When I return to the lanes, hedgerows and woods I loved as a boy, where I found birds’ nests every 50 yards or so, watched field mice clambering with acrobatic agility up the stems of swaying wheat and heard the mournful piping of curlews and redshank from the marshy fields, today there is only an eerie stillness and scarcely a fluttering wingbeat over the monotone greenery.  When, in 2000, I wrote and illustrated my book Wings Over the Valley: A Birdwatcher’s Wales Diary, about the joys of birdwatching and the countryside, communicating that exhilaration gave me renewed strength and optimism, but those feelings have been shattered in the face of the continued destruction of our countryside.John GreenLondon • It’s good to read that highway authorities and the construction industry are taking the greening of our roadsides seriously (Flower power: the route map to a roadside revolution, 14 March), but we also need to get to grips with the huge increase in paved-over front gardens. We may need to find a place to park cars, but the case for having a green space around our homes is very persuasive. Not only do plants and trees help keep our homes cool in summer and warm in winter, but every blade of grass also absorbs carbon dioxide and emits oxygen, thus playing a part in stopping climate change. Green spaces in residential areas are an essential stopping-off point for butterflies, birds and bees. The Royal Horticultural Society reports that almost a third of front gardens are now paved over. The RHS Greening Great Britain campaign gives lots of hints to people who could be encouraged to leave space for flowers, shrubs or trees. If, like me, you’d like to congratulate those who have a green front garden, visit Healthy Life Essex at https://bit.ly/2WeFH6L, where you will find a downloadable thank-you card to pop through the letterbox.Eileen PeckBenfleet, Essex • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition  "
"The data tells us that climate change is real, but sometimes the feeling on the ground is far from convincing. A spell of hot weather can always be compared to a similar spell of hot weather from way back in the past. Similarly a big storm, intense rain and flooding often have parallels from long ago. Now a study in Geophysical Research Letters reveals that the signs of climate change are harder to spot in mid-latitude countries such as the UK and US, and easier to see in the tropics. Ed Hawkins, from the University of Reading, and colleagues compared global mean temperature fluctuations with local temperature fluctuations for different parts of the world. They show that while the global average temperature has wiggled smoothly upwards, a mid-latitude location such as Oxford in the UK has always had wild swings in temperature. “Climate change impacts in some countries are being hidden by their own changeable weather,” writes Hawkins. By contrast increasing temperatures in tropical regions are more obvious because the background climate is steadier in these regions.  When it comes to rainfall, the data reveals a clear signal of increasing extremes, masked in mean rainfall plots. Large parts of the UK have now experienced exceptional deluges, incomparable to historic records."
"Wherever you go in Britain – in city, town or country – you can come across a hidden wildlife haven. It may be home to sand lizards and stoats, adders and orchids, butterflies and bush-crickets, water voles, peregrine falcons, or great crested grebes. Yet often these oases are not official nature reserves, but little scraps of land we rarely consider important for nature. Churchyards, roadside verges, railway cuttings and disused quarries may not appear to have much in common. But they were all originally created for humans’ needs, before becoming places where wild creatures thrive. Together, they add up to an area larger than all our official nature reserves combined.  Not that these spots are unimportant to people. Often on the edge of urban areas, they are accessible to more people than rural nature reserves – especially to those who, by an accident of birth, background or geography, do not have access to “real” countryside. These places also matter for another, even more urgent reason. Since the second world war, a continuous drive towards more intensive farming has turned much of our countryside into a wildlife-free zone. That’s where the sites featured in this book come in. They provide a much-needed refuge for otherwise scarce species. Without them, some of our most vulnerable wild creatures would already have disappeared. I don’t need to tell you that Britain’s wildlife is under threat. Loss of habitat, pollution, persecution and, above all, the global climate emergency, mean that even our once common and widespread species are now struggling. So, at the start of this make-or-break century, as our wildlife enters one of the most challenging periods in its history, I have travelled round the country to visit these unheralded sanctuaries. Without them, nature in modern Britain might not be able to survive at all.  My taxi driver looked puzzled. ‘No, fella – there’s nothing down there except an industrial estate. I’ve never even heard of the “Window on Wildlife”!’ Fortunately, Google Maps came to our aid, and we headed through the rush-hour traffic towards Belfast docks. Twenty minutes later, we arrived outside “WoW” – the RSPB’s Window on Wildlife. The first thing that struck me was the strident calls of black-headed gulls, floating en masse over the lagoon before drifting down to land on artificial nesting rafts. They were accompanied by more delicate, wraith-like birds – common and Arctic terns – and dozens of black-tailed godwits, feeding voraciously. During the 1960s, when the port was being regularly dredged to permit the passage of large ships, the mud was dumped into three large pools. The plan was that eventually the mud would settle, and the land could then be reclaimed for building. But nature had other ideas. The fertile combination of water and mud created the ideal habitat for waterbirds such as ducks and waders, and they came here to feed in their thousands. Gradually, it became clear that the area was a real hotspot for birds: not just migrants and winter visitors, but breeding species too. Local people began lobbying to save this precious place from development. After long negotiations with the port authorities, the middle of the three lagoons was set aside for wildlife. Back at the visitor centre I had a chat with two regular volunteers, Ken and Phyllis. Ken reflected on the irony that somewhere that looks so natural is entirely created by humans, while Phyllis had noticed that, as the nearby industrial estate and business park grows, more and more workers are dropping in, lured by the sign outside. Belfast WoW is, to be honest, a bit out of the way to attract casual passers-by, as my taxi driver confirmed. But that’s all the more reason why places like this need championing. Phyllis told me she loves the reaction from visitors when they enter the observation area for the first time. ‘They just say, “Wow!”’ The southern migrant hawker is one of the most attractive of all our two dozen dragonfly species, and not only because it is so rare. The male is a dazzling cerulean shade, each azure segment interspersed with jet black. Close up, through my zoom lens, I could see the pale blue eyes and the lattice-like wings, laced with tiny shards of gold. This was just one of the amazing insects I saw on a recent visit to Canvey Wick, Britain’s first “brownfield” nature reserve. It is also home to a plethora of birds such as the whitethroat and stonechat as well as reptiles including adders and common lizards. In the early 1970s, this site was earmarked for an oil refinery, but the oil crisis led to the cancellation of the entire project. Afterwards, it was simply allowed to return to the wild. Had someone wanted to design a nature reserve for insects, they could hardly have done better – yet Canvey Wick is perhaps the most genuinely accidental habitat in this book. The poor, sandy soils only allow vegetation to grow slowly, so the wildflowers and grasses are not swamped as they might be elsewhere. And the large, circular stands of asphalt where the oil tanks would have stood built retain heat, creating a microclimate ideal for continental, warmth-loving species such as the shrill carder bee. Then there is the location: Canvey Island is one of the sunniest, warmest and driest places in the whole of the UK. Canvey Wick has been described as “England’s rainforest”, but as the Guardian’s wildlife writer Patrick Barkham pointed out, “England’s savannah” is a better description, given the absence of large trees. Like so many other wildlife-rich sites I have visited, it is a mosaic of mini-habitats: birch and willow scrub, brambles, dry and damp reedbeds, long grass and earth banks, which between them create exactly the right mix of ecological niches. Canvey Wick may not look special, but for invertebrate life it rivals well-known nature reserves such as Minsmere, Wicken Fen and Dungeness. As ecologist Dr Sarah Henshall notes, its unusual history gives it a special place in our natural heritage: “Canvey Wick is wild, it’s different, it’s rough around the edges. Wildlife thrives in the untidy messiness – that’s what makes the site unique.” I am reminded of my childhood, when I played for hours in places like this. Canvey Wick is the clearest possible evidence why the label “brownfield site” is so unhelpful – indeed positively detrimental. To those who care about Britain’s wildlife, it’s these messy corners that need to be prioritised, not the green swathes of agri-desert that make up so much of our lowland countryside. In May 1974, I cycled from my home to the village of Datchet, where the Queen Mother Reservoir was being built. I walked slowly through the heat-haze towards a distant strip of water, where a slim bird took off a few yards in front of me, giving a persistent, high-pitched whistle I now know was a sign of alarm. I lifted my binoculars to see a small, long-winged wader circling low over the gravel. When it landed, I could see the plain, brownish back, black mask and, most importantly, a thin, lemon-yellow eye-ring: my first ever little ringed plover. This was a classic example of a species adapting to an “analogue habitat”. On the continent, little ringed plovers nest on the shingle banks of rivers, swept clean of vegetation by winter floods. The bare shingle allows them to disguise their eggs, especially from aerial predators such as kestrels. Gravel pits and reservoirs provided an ideal substitute for riverbanks, and during the post-war years they allowed the little ringed plover to gain a foothold this side of the Channel. About this time, I came across Adventure Lit Their Star, by Kenneth Allsop. The name will be familiar to readers of a certain age, for Allsop was a familiar face on TV during the 1960s. This was a Boys’ Own adventure story, in which an airman recovering from tuberculosis joins forces with two young lads to foil attempts by an egg-collector to steal a precious clutch of little ringed plovers’ eggs. The message is that birds need to be protected and welcomed and, more importantly, that nature can offer a form of therapy. This was something Allsop, who had lost a leg in the war and suffered periodic bouts of depression, understood only too well. Little ringed plovers went against expectations to breed in what Allsop described as “the messy limbo that is neither town nor country”. As a definition of the Accidental Countryside, this could hardly be bettered. It took me five minutes to walk round. At the edge of the peat diggings a few miles from Glastonbury I saw 10 birds, of just four species. And yet, apart from the five loitering mallards, the others confirmed these changing times. The first bird was a little egret. I am old enough to remember when one of these impossibly white birds made any birding trip a red-letter day. Even now, I still feel a jolt of pleasure whenever I see this little heron, which when I was growing up was still confined to the area around the Mediterranean. Moments later I saw a buzzard, another bird that wouldn’t have been here 20 years ago. Whereas the little egret extended its range northwards thanks to climate change and habitat restoration, the buzzard benefitted from an end to persecution by gamekeepers, enabling it to recolonise its former haunts. The next bird was one of my favourites: the green sandpiper. I half expected to see it here on this warm, early-August evening, as they drop in to feed on their journey south to Africa. For me, they are the first sign of autumn, despite appearing at the height of summer. The final pair of birds would once have been a very rare sight here. But there are now several dozen great white egrets on the Somerset Levels, and recently they have begun to visit my local patch. Birds like the great white, little and cattle egrets (another recent colonist), give the lie to the idea that all our wildlife is in decline. It’s not, but it wouldn’t take much to destroy these temporary, liminal habitats on which so many wild creatures depend. Before I left, I heard the piping call of that lone green sandpiper, and watched it rise into the sky. As it disappeared, I wished it good luck. It would need it, for just as places like this are being destroyed, so its stop-over points are also under threat, from wetlands being drained, or drying up because of climate change. One day, I fear, I will watch one disappear over the horizon, and that will be the last time I ever see a green sandpiper; not just here, but anywhere. Birds are resilient creatures, for sure, but are they resilient enough? The Accidental Countryside: Hidden Havens for Britain’s Wildlife, by Stephen Moss, is published by Guardian Faber (£16.99). To buy for £11.99 go to bookshop.theguardian.com "
"
Share this...FacebookTwitterA couple of days ago I wrote a piece about a paper by some Australian shrinks claiming that skeptics are prone to believe conspiracy theories, like the 1969 moonlanding being staged in Hollywood.
But Marc Morano reminds us that there is a small problem with their assertion. Some of America’s most prominent skeptics are former astronauts who actually walked on the moon: Jack Schmitt and Buzz Aldrin. Gee, do you think they believe it was all done in Hollywood, too?
Please read Marc’s piece from 2009.
==========================================
Oops! Shades of Gore: Joe Romm’s research comes up short: Unknowingly Uses Skeptical NASA Moonwalker Schmitt to Rail on Global Warming Skeptics 
 By Marc Morano
Former Clinton Administration official and climate fear promoter Joe Romm — followed in the footsteps of former Vice President Al Gore — by linking skeptics of man-made global warming fears to those who believe the 1969 moon landing was staged. (Note: Romm and Climate Depot’s Morano debated global warming in March 2009. See: Morano debates former Clinton Official Romm – April 6, 2009).
The embarrassing problem for Romm is that he — unknowingly — used one of the most prominent global warming skeptics, NASA moonwalker Harrison ‘Jack” Schmitt, in an attempt to “prove” climate skeptics are akin to those who believe the moonlanding was staged.
Schmitt, who flew on the Apollo 17 mission, declared in 2008:
“The ‘global warming scare’ is being used as a political tool to increase government control over American lives, incomes and decision making. It has no place in the Society’s activities.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But Romm, in his July 20, 2009 article railing on climate skeptics, failed to do basic research on moonwalker Schmitt’s skeptical climate views. Romm approvingly cited Schmitt’s scientific views in an article at Climate Progress that was intending to smear climate skeptics.
Romm asserted: ‘I’m just drawing the painfully straight line from the moon hoax people to the climate hoax people.’ (Romm followed in Gore’s footsteps: See: Moonwalkers Defy Gore: NASA Astronaut Dr. Buzz Aldrin and Jack Schmitt reject global warming fears: Defy Gore’s Claim That Climate Skeptics Are Akin To Those Who Believe Moon Landing was ‘Staged’ – July 3, 2009).’
Romm approvingly quoted Schmitt rejecting moon landing conspiracy theories.
‘If people decide they’re going to deny the facts of history…(continue reading…)
=====================================
A final note: Reader Paul Matthews left a comment listing the 8 blogs the Australian shrinks used to get participants in their survey:
http://www.skepticalscience.com
http://tamino.wordpress.com
http://bbickmore.wordpress.com
http://www.trunity.net/uuuno/blogs/
http://scienceblogs.com/illconsidered/
http://profmandia.wordpress.com/
http://scienceblogs.com/deltoid/
http://hot-topic.co.nz/
Need we say more? Now we really know who belongs on the couch.
 
Share this...FacebookTwitter "
"
As shown by the indicators on WUWT’s new ENSO/SST page there is a deeping of the La Niña that is starting to rival 2008 in depth. While it hasn’t yet reached the level of the 2008 event, indications are that it is possible to match or even exceed it.

The graph above from Australia’s BoM took a dip just today, going from last week’s value of approximately -0.9 to -1.4C.
Other NINO index indicators show similar recent drops:





For those unfamiliar with what these index graphics represent, here is a map that shows the regions covered:

The combined 3.4 index has been deemed a useful metric to gauge El Niño and La Niña events and thus you’ll see it more commonly referenced than the other indices.
Of course a picture is worth a thousand words:



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e89739bda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIt’s great to see what all those tens of billions of euros spent on renewable energies and the skyrocketing costs of electricity have accomplished. Nothing!
According to the German Press Agency, dpa, European greenhouse gas emissions jumped 2.4% in 2010. The figures were released by the European Environment Agency (EEA) in Copenhagen yesterday. Read about it here.
Officials blame economic recovery in many countries and the harsh winter for the jump. Actually, the tens of billions of euros did have an impact. According to EEA Director Jacqueline McGlade:
…the increase could have been even higher without the fast expansion of renewable energy generation in the EU.”
Boy, I feel a lot better already. Actually I don’t. Even if emissions had gone down, it still would have been a complete waste of money, and is not going to change the climate. And whatever Europe manages to save in emissions over the next decade will simply be offset by China’s explosive growth in just matter of weeks.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to the EEA: Germany, Poland and Great Britain are responsible for 56% of the increase. Finland, Sweden and Austria also posted large emission increases. Shame on you.
But there were some big successes in Europe. Large emission savings were accomplished in Greece, Spain and Portugal. Congratulations! Of course, these happen to be the countries in Europe that have crashed economically. The media just forgot to mention that.
The Express (link above) writes:
Despite the increase in 2010, the 27-nation bloc is on track to meet its emissions targets under the Kyoto protocol, a 1997 climate accord limiting the emissions of most industrialised countries, the EEA said.”
What they don’t mention is that a large part of those cutbacks was achieved by the collapse of the former East Block and the really dilapidated factories of communist central planning.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterScientists keep finding major knowledge gaps in their “science-is-settled” field of climatology.
The latest gap is revealed by an experiment by an international team of scientists that shows evidence of a new mechanism where light causes atmospheric aerosols to increase in size.
 
Aerosol pollution over India and Bangladesh, 2001. (Photo source: NASA)
The results of the research by a team led by Maria Eugenia Monge et al have been published by the PNAS. Title: Alternative pathway for atmospheric particles growth.
“The new and up to now unknown processes may be the reason why the atmospheric chemistry and physics of aerosol concentrations are often underestimated in models. This photo-induced processes first will be characterized experimentally and then introduced to tropospheric models,“ recommends Hartmut Herrmann of the German Leibniz Institute for Tropospheric Research (IfT) in Leipzig, a member of the team.
The paper’s abstract underscores that major gaps exist in the understanding of the physicochemical pathways that lead to aerosol growth in the atmosphere and that these pathways need to be considered by models.
So once again it’s back to the drawing board for our habitually lost climate modellers.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to a Leibniz Institute for Tropospheric Research press release, light causes the aerosols to grow in size and have an impact on clouds and climate. Photocatalytic reactions can lead to a rapid formation of non-condensing volatile organic compounds (VOCs) on the surface of particles. They found that light can trigger chemical reactions between gaseous bonds and chemicals on the surface of organic particles, which ultimately allows them to increase in size says Dr. Maria-Eugenia Monge of IRCELYON and the University of Lyon.
Experiments showed that the particles under the influence of light can grow about 50 to 65 nanometers, which corresponds to about a doubling of their weight. The intensity of the light was of lesser importance. Already very weak UV radiation is enough to break the chemical bonds of dissolved organic material (DOM) and form free radicals.
The experiments were conducted at the IRCELYON in Lyon under the supervision of Dr. Christian George. Also participating were scientists of the French CNRS research association, the Israeli Weizmann Institute with Prof. Yinon Rudich, and Prof. Hartmut Herrmann of the German Leibniz Institute for Tropospheric Research (IfT) in Leipzig.
The Leibniz press release adds that aerosol particles in the atmosphere influence global climate because they reflect sunlight. They are also a factor in the global water circulation, which effects cloud formation and precipitation. They also impact human health as well. Despite their impacts, the processes that are responsible for the creation and growth of these particles are among the least understood fields of atmospheric science. At the IfT in Leipzig, the development chain of atmospheric particles from fine particulates to the formation of clouds and precipitation in natural areas as well as areas burdened by humans, i.e. large cities, are being researched.
So much for the new study on light and aerosols, which we know have a cooling effect on the planet. But with the last sentence in italics by the IfT, why do I get the feeling they are gearing up (again!) for man-made global cooling? Human activity is throwing up lots of aerosols (industry, agriculture, transportation, etc.) into the atmosphere and so are contributing to blocked sunlight. Time to start curtailing human aerosols!
And one more question: if low intensity light can cause major aerosol growth, wouldn’t it be very plausible that high energy cosmic rays could do the same?
 
Share this...FacebookTwitter "
"

Blame California’s mega‐​fires on global warming. Or at least that’s what Senate Majority Leader Harry Reid (D-NV) said last week in the _Hill_.



Global warming affords endless opportunities to test glib hypotheses by politicians who have no training whatsoever in fields of which they claim pontifical knowledge. And Reid’s statement is easy to test.



By the end of each and every summer, Southern California is drier than the world’s best martini. A couple weeks ago, I took a drive up San Gabriel Canyon, an arroyo typical of the mountains surrounding the Los Angeles basin. The steep hillsides were studded with crackling‐​dry vegetation, and it was obvious that the area was sitting on the precipice of a massive fire season.



California’s big wildfires are, ironically, caused by excessive winter rains. Normally, the region that’s been ablaze averages about a foot from December through March. Owing to the fact that just about every day after the rainy season is warm and sunny, it’s only a matter of a month or two before the surface dries out to the point that there’s not enough water to support additional plant growth. The more it rains in the winter, the more vegetation grows, and the more there is to burn in the summer, which is invariably hot and dry.



The distribution of rainfall between years is a bit unusual. The vast majority of the years have below normal precipitation — about four or so inches below the average of a bit over a foot, as shown in our attached graph. In the fewer years that are above average, when it rains, it pours, with rainfall often 100% (one foot or more) above the mean.



Some of the very wet years are caused by El Nino, a reversal of winds over the Pacific Ocean that has been going on every few years ever since there was a Pacific Ocean. People like Senator Reid (and Vice President Gore) will cite computer models predicting that El Ninos should become stronger or more frequent with global warming, but there are an awful lot of other models showing that they won’t change or that they might even lessen in frequency. The Nobel Prize‐​winning United Nations Intergovermental Panel on Climate Change says “There is no consistent indication of discernible future changes in ENSO [an acronym for El Nino] amplitude and frequency.”



When things get very wet, there’s plenty more time for the soil to remain moist, producing a much longer growing season in the hills where suburbs and very expensive homes are proliferating. The problem is that these rooms‐​with‐​a‐​view are also houses‐​with‐​a‐​risk; i.e., they’re in the path of wildfires. Rain adds fuel to the fire, by bulking up the vegetation mass.



If Senator Reid is right, then rainfall, or the frequency of rainy years, must be increasing in the fire zone. Here is the total December‐​March precipitation for the California South Coast Drainage Climatological Division from 1895 through 2007. Data are from the National Climatic Data Center, a part of the U.S Department of Commerce.



What’s noted in the graph is pretty obvious. Most of the years are below the long‐​term average of about 12 inches, but the relatively few that are above the mean are often way above it. If global warming is causing the increase in Southern California wildfires, then the frequency of very wet years has to be increasing in a significant fashion, because excessive moisture is required to create excessive vegetation.



Obviously it is not. In fact, the biggest agglomeration of far above‐​normal years was a 12‐​year period beginning in 1905.



Ironically it was these rains that prompted some of the massive westward migration of U.S. population, as both California and Arizona were touted as green paradises, which they were, thanks to all that vegetation. Sure, there were wildfires then, but very few people lived within their reach.



Now that the paradise of the Los Angeles Basin is home to so many more people, whenever we have a very wet year (2005 being the last big one), it’s only a matter of time before thousands of homes get torched.



But don’t blame this on global warming. There’s no trend whatsoever in the frequency of heavy‐​rainfall years that would promote wildfires. And our officials should especially avoid making untested statements on global warming to papers like the _Hill_ , which other Senators and Congressmen accept as gospel.
"
"

Despite indications that much of President Obama’s agenda is meeting intra‐​party skepticism all over Capitol Hill, there is one policy nexus where congressional leaders are still doggedly determined to move the country left: energy and the environment. Speaker Pelosi will reportedly allow a vote on the controversial Waxman‐​Markey “cap‐​and‐​trade” legislation at the end of this week.



And it gets even better. Not content to tempt political fate by imposing huge carbon taxes on the American middle class, Democrats have added a provision which imposes stiff tariffs on our trading partners if they don’t adopt aggressive carbon restrictions of their own.



You heard correctly: progressives have authored a bill that earns the mortal enmity of domestic energy consumers and our most crucial trading partners at the same time. Economy‐​killing climate policies and a trade war — together at last!



What happened is this: An early draft of Waxman‐​Markey already contained triggers that gave the president the choice to introduce carbon tariffs if jobs and industry “leak” overseas to countries that don’t constrain emissions so dramatically. (China and India come to mind.) The original version empowered the president to impose the carbon‐​linked tariffs beginning in 2025.



But though the language is not public yet, the House Ways and Means Committee is reportedly considering provisions that will give extra comfort to protectionists. Leaks from Hill offices indicate that the president would now be forced to impose the carbon tariffs — and could only opt out of doing so with permission from both chambers of Congress. Carbon‐​intensive imports would be subject to penalties at the border unless the country of origin requires emission reduction measures at least 80 percent as costly as ours. (The original Waxman‐​Markey bill had a threshold of 60 percent.) 



Unfortunately for the amendment’s authors, World Trade Organization rules make fairly clear that trade‐​limiting measures imposed to protect the environment should have the purpose of protecting the environment, and not to address any adverse competitiveness effects on domestic industry. Break that connection between measure and purpose, and you’ve got yourself a problem. The result could be litigation, retaliatory tariffs, or both. Does anyone really expect China to stand idly by in 2025 as their trade is embargoed?



And just for the sake of discussion, exactly how much global warming will be prevented by this assurance of future trade turmoil? Well, let’s use the federal government’s own model which — we are not making this up — is called MAGGIC (Model for the Assessment of Greenhouse‐​gas Induced Climate Change). It comes from the National Center for Atmospheric Research in Boulder, Colorado.



Let’s compare the effects of Waxman‐​Markey to the United Nations’ “business‐​as‐​usual” emissions scenario that’s in their big 2007 climate change compendium. If the U.S. only adopts Waxman‐​Markey, global warming would be reduced by a grand total of 0.2ºF by 2100. This is too small to even detect, because global temperatures bounce around by about this amount every year. For those who like to think more near‐​term, the amount of warming prevented by 2050 would be 0.07 of a degree.



According to the UN, without Waxman‐​Markey the warming from 1990 to 2050 would be 2.8ºF, and 5.3º by 2100. (Of course, observed warming since 1990 is running about 40 percent below the expected rate, largely because there hasn’t been any net warming since the very warm year of 1998.)



Now, let’s be completely unrealistic and assume that every nation that has “obligations” under the (failed) Kyoto Protocol cuts emissions as much as we do. Then the saved warming balloons all the way to 0.14ºF by 2050 and 0.4º by 2100, or 5 and 7 percent, respectively, of the “business‐​as‐​usual” total.



Let’s add it all up. We don’t do anything measurable to reduce global warming, we alienate some of our biggest trade partners, we risk a trade war, and Americans are allowed to emit the same carbon volumes as the average citizen did in 1867. What’s not to hate?



All of which explains why Waxman‐​Markey is being rushed to the floor. If people find out what is really in it, how risky it is and how small the purported benefits, it is hard to believe that it will pass.
"
"

The U.S.-China trade relationship, one of the world’s largest, is a flashpoint for concern over the U.S. trade deficit, China’s currency valuation, and Chinese intellectual property regulation. This relationship is under especially keen scrutiny as the 2008 presidential campaign heats up in the United States, with Democratic front‐​runners favoring punitive duties against China if it does not act to revalue its currency. Robert E. Scott, senior international economist at the Economic Policy Institute, and Daniel J. Ikenson, associate director of the Cato Institute’s Center for Trade Policy Studies, debate whether the next U.S. president should get tougher with China on trade.



 _Weigh in on this debate hosted by the **Council on Foreign Relations** by emailing the editors at letters@​cfr.​org. To view other online debates click here._



April 4, 2008(Most Recent)



 **Daniel J. Ikenson**



Space constraints preclude my rebutting each ad hoc, amorphous assertion made in Rob’s last post, but I want to address some of the most outlandish. 



If the current trading system encourages a race to the bottom, how does one explain the large and increasing foreign direct investment flows into the United States? Why is ThyssenKrupp building a $3.7 billion green field steel production facility in Alabama? Why do foreign nameplate automakers continue to invest in U.S. manufacturing? Why do the 5.1 million Americans employed by U.S. subsidiaries of foreign‐​owned companies earn on average 32 percent higher wages than workers at U.S.-owned companies? 



Because there is no race to the bottom, that’s why. There is a race to the top‐​for skilled workers, for access to production facilities closer to markets, for investment in countries where the rule of law is clear and abided, where there is greater certainty to the business climate, where the specter of asset expropriation is negligible, where physical and administrative infrastructure is in good shape, and so on. Seems odd how the same sirens who decry the race to the bottom spend the rest of their day opposing foreign direct investment in the United States.



Are we to believe that America’s elites are behind Walmart’s success? Seems to me Walmart and other retailers have been a conduit of the benefits of trade, allowing ordinary Americans to tap into the division of labor, extend their budgets, and increase their families’ access to clothing, food, and other everyday products. And American manufacturers and their workers are the beneficiaries of huge increases in exports to China‐​our fastest growing large export market since 2001. Beyond question, vast swaths of Americans and Chinese are benefitting from the expanding trade relationship.



With respect to U.S.-China trade, the next president should continue the tradition of this administration, which is to engage in quiet dialogue where there are issues to resolve and to resort to the WTO dispute settlement system when the facts support doing so.



As he or she reflects on the bilateral trade dialogue of the recent past, the next president should recognize that it has been more a litany of U.S. gripes than a dialogue, and that the time has come to start considering carrots to accompany the sticks. The next president should grant China market economy treatment in antidumping cases. While such a reform would take very little out of petitioning industries’ hides, the gesture would win vast sums of goodwill from the Chinese, which will be needed to resolve more important issues going forward.



April 3, 2008



 ** **Robert E. Scott****





If four low‐​wage workers are riding an elevator, the door opens, and Bill Gates gets on, everyone on that elevator becomes, on average, a billionaire. The economic benefits of U.S.-China trade, like the average net worth on that elevator, look good until you consider their distribution.



The gains from trade between the United States and China have flowed to a very small segment of both countries’ elites. The gap between the value of what U.S. workers produce and what they receive has widened dramatically, partly because deregulated trade has suppressed the real wages of all non‐​college educated workers (about 70 percent of the labor force). Between 1980 and 2005, U.S. productivity rose 71 percent while real compensation (including benefits) of non‐​supervisory workers rose just 4 percent. This measure includes all the benefits of globalization received by these workers. Most of the benefits of growth since 1980 have been captured by the top 10 percent, especially the top 1 percent, of U.S. workers. The problem is not trade, _per se,_ but the current trading system which has encouraged a race‐​to‐​the bottom in wages and labor standards. 



The systematic suppression of workers’ rights has reduced Chinese wages by 47 percent to 85 percent according to a recent  labor‐​rights petition, and the problems are worsening. Occupational illness and injury rates have never been higher in Chinese manufacturing. Workers are frequently forced to go unpaid, and their complaints and protests are often met with violent government responses. 



Fueling China’s vast trade surplus with the United States is its very high savings rates, nearing 50 percent of GDP in recent years. Conventional wisdom is that Chinese workers save excessively because China’s pension and public health systems are so poor. However, household savings recently declined to 16 percent of GDP. Business savings, on the other hand have soared to nearly 24 percent of GDP and government savings exceeded 10 percent  according to the IMF.



Thus, globalization’s benefits in China are reaped by an elite cadre who own and operate private and public enterprises. Their savings are piling up in the net worth of the rapidly expanding business empires under their control.



Getting tough with China about international trade and labor rights violations would help workers in both countries. The Bush administration has rejected two labor rights petitions submitted by the AFL-CIO and U.S. Representatives Ben Cardin and Chris Smith, and workers in both countries have suffered as a result. 



China needs more spending on infrastructure, environmental clean‐​up, and public health and other social services. It needs fundamental improvements in labor rights and enforcement, which will raise wages, increase private consumption and reduce China’s need to export. These win‐​win policies can help workers in the United States, China and all our trading partners.



April 2, 2008



 **Daniel J. Ikenson**



The only substantive point of agreement between Rob and me about China is that it makes for a nice wedding gift.



To embrace Rob’s perspective, one must assume away the reality of how the economy actually works and how it is structured. If the U.S. economy comprised only producers who were self‐​sufficient for their material inputs and who had no interest in selling products abroad, Rob’s prescriptions, which subordinate the multitude of individual U.S. economic interests to manufacturers’ interests, might garner some sympathy. But it is fantasy to characterize international trade as a contest between “our” producers and “their” producers. Not only has that line been blurred (thankfully) by foreign direct investment, cross‐​ownership, equity tie‐​ins, and transnational supply chains, but the fact is that the economy is composed of consumers, retailers, importers, shippers, designers, engineers, marketers, financiers, and producers who have great stakes in an open world economy, and who would be hurt by Rob’s proposals.



The currency issue is far more complicated‐​and far less insidious‐​than Rob implies. Is a more‐​weakened dollar what America really needs as prices for essentials like oil and food continue to rise? Do we really want China to have 40 percent more spending power on account of Yuan appreciation when China’s growing demand with a lower‐​valued currency explains much of the world’s commodity price increases? Other countries are looking for ways to bolster their citizens’ purchasing power by suspending tariffs and other import restraints, yet Rob thinks it’s wise to reduce Americans’ purchasing power by rendering dollars worth less.



I strongly disagree with Rob’s assertion that the trade deficits are a major cause of the loss of 3.4 million jobs. Manufacturing jobs are in decline worldwide, even in perennial trade surplus countries like Japan and Germany, as well as in China.



It is worth noting that between 2001 (Rob’s demarcation) and 2007, the increasing bilateral trade deficit has been accompanied by a 20 percent increase in real GDP, a 15 percent increase in manufacturing output, and the creation of 9.1 million net new jobs.



With respect to trade remedies, let’s summon the violins! Out of 263 U.S. anti‐​dumping and countervailing duty orders, there are 62 (24 percent) in place against Chinese imports. And the extremely prosperous U.S. steel industry‐​the victim in Rob’s last post‐​accounts for 126 of the 263‐​nearly half!



Rob’s prescriptions are not only unnecessary; they are particularly ill‐​suited for the twenty‐​first century global economy.



April 1, 2008



 ** **Robert E. Scott****





I’m glad that Dan thinks we need to get tough with China. We have ignored these problems for far too long. China provides vast and extensive subsidies to businesses making goods for export in many industries, artificially reducing the cost of their products. The U.S. government needs to develop new policies and institutions to enforce our fair trade laws and to ensure that the system delivers broadly shared benefits to U.S. workers and businesses producing goods and services in the United States. 



A  recent study by Prof. Usha Haley at the University of New Haven estimated that energy subsidies to the Chinese steel industry alone exceeded $27 billion between 2000 and mid‐​2007. China went from being a net steel importer a few years ago to the world’s largest steel producer and exporter. China’s share of U.S. steel imports increased six‐​fold.



Energy subsidies are rampant in China, and yet the U.S. Commerce Department refused to authorize countervailing duties in recent trade complaints involving tires and coated paper imports, both energy intensive products. U.S. trade laws need to be toughened in this area to ensure that _systematic_ subsidies that benefit all exporters are countervailed.



Beginning in 2001 with their tenth five‐​year plan, China targeted the auto and parts industries for rapid growth, and they have poured subsidies into this industry. U.S. auto parts imports soared from $1 billion in 2001 to $7 billion in 2007, resulting in massive layoffs and plant closures. These cases illustrate two key weaknesses in the U.S. trade policy enforcement system. 



First, our system depends on manufacturing firms and agricultural producers to initiate the vast majority of all U.S. unfair trade complaints. The system makes it hard for them to win cases until they are on their last legs so many cases are never filed. The U.S. government has the right to initiate complaints, but rarely does. Second, only the USTR, which is part of the President’s Executive Office, has the right to file trade complaints with the WTO. It often fails to do so for political reasons. For example, the big‐​three U.S. auto companies benefit from subsidized Chinese auto parts imports, and the USTR has refused to bring a WTO complaint until 2006, five years too late.



Congress should create an independent government agency with the resources and authority to file fair trade cases in the United States and at the WTO. We must insist that Chinese producers compete on a level playing field, and if we do, U.S. workers and businesses can win.



March 31, 2008



 **Daniel. J. Ikenson**





If a tougher stance means using the WTO Dispute Settlement Body [DSB] more systematically to achieve greater Chinese compliance with the vast obligations to which China agreed upon joining the WTO in 2001, the answer is “yes.” If it means supporting or encouraging provocative legislation or taking unilateral administrative actions to compel or punish China in a manner that would violate our own WTO obligations or would benefit a few litigious industries at the expense of broader economic interests, the answer is “no.”



In 2006, the USTR (Office of the United States Trade Representative) published its “Top‐​to‐​Bottom Review” of U.S.-China trade relations, in which it proclaimed the beginning of a new phase in the relationship, stating, effectively, that the honeymoon period (of reform implementation) was over and foreshadowing greater resort to the WTO dispute settlement system to achieve further compliance.



One month after publication of that report, USTR filed a WTO complaint alleging that certain Chinese policies discriminate against imported automobile parts. Very recently, the dispute panel established to hear that case ruled in favor of the United States.



Before the auto parts case, only one complaint about Chinese practices had been lodged with the DSB. It concerned a value‐​added tax on integrated circuits that was allegedly applied in full to imports only. During the consultation phase of the dispute (and without need of formal adjudication), the Chinese agreed to change their practice and the dispute was resolved.



In 2007, the USTR filed three WTO cases against China. The first involved certain tax provisions that allegedly amounted to subsidization of Chinese exporters. In response to the allegations, China changed its tax rebate practices (although the dispute is not completely resolved yet). The second concerned enforcement of intellectual property rights. The third concerned alleged barriers facing foreign traders and distributors of copyrighted materials like books, videos, and DVDs. A dispute panel was recently composed for the IP case, and the distribution barriers case is still in the consultations phase. Earlier this month, USTR brought a sixth case, alleging discrimination against U.S. providers of financial services information in China. 



Since the USTR’s 2006 review, five cases have been filed with positive outcomes achieved in two (the others are pending). It is important to recognize that our trade relationship with China is mutually beneficial, and that unnecessary provocation could open a Pandora’s Box of economic problems. There is no good reason to jettison a process that is working.



March 31, 2008



 ** **Robert E. Scott****



China is a protectionist state that has used all of its powers and resources to build an artificially competitive export powerhouse. The United States is the most important market for its exports. Growing U.S. trade deficits with China and other countries are a major cause of the loss of 3.4 million U.S. manufacturing jobs since 2001, when China entered the WTO. China’s export‐​led growth strategy is also very costly for its people.



We have been down this road before, and know how to deal with such situations. Two decades ago, Japan built an export powerhouse behind an artificially cheap currency and protected home markets. This continued until 1985, when it began to threaten the stability of the world financial system. The problem then, as now, was the U.S. trade deficit.



The Reagan administration, much like the current White House, doggedly ignored the over‐​valued dollar through its first term while millions of jobs disappeared and thousands of factories closed. Finally, Congress acted and passed a measure (HR 3035) which hit countries like Japan, Brazil, and Korea, that maintained large U.S. trade surpluses, with a 25 percent tariff.



In a complete about‐​face, Treasury Secretary James Baker then negotiated the Plaza Accord with the G-5 (Japan, Germany, France and the U.K.), on September 22, 1985. The next day, the Federal Reserve and Central banks in Japan and Europe executed coordinated currency interventions that began to drive the dollar down. The dollar continued to fall until the Louvre Accord 16 months later, which stabilized its level again. The dollar fell 29 percent to 46 percent against the G-5 currencies in this period.



The U.S. never imposed a tariff in the Plaza era‐​HR 3035 never even became law. The mere threat, combined with concerns about a potential financial crisis, were enough to get the deal done. 



China has invested over $1.5 trillion in foreign exchange reserves in order to keep the yuan artificially cheap. Economists estimate than its currency, too, needs to rise by about 40 percent. Other Asian export economies, such as Japan, are following similar strategies and also need to revalue, but they can’t do it alone. While the dollar has fallen sharply against the euro and other freely traded currencies over the past five years, it has barely budged against the yuan and yen. But this won’t happen until we get tough with Beijing. We need to put some backbone in our trade policy to get multilateral currency talks started now.
"
"Almost all parts of England will face tough coronavirus curbs with a ban on households mixing indoors and restrictions on hospitality after December 2.
The full list of tiers and areas published by the government is online - but the site is experiencing difficulties. If you can't access it, they are:
**South East**
Isle of Wight
**South West**
Cornwall
Isles of Scilly
**North West**
Cumbria
Liverpool City Region
Warrington and Cheshire
**Yorkshire**
York
North Yorkshire
**West Midlands**
Worcestershire
Herefordshire
Shropshire and Telford & Wrekin
**East Midlands**
Rutland
Northamptonshire
**East of England**
Suffolk
Hertfordshire
Cambridgeshire, including Peterborough
Norfolk
Essex, Thurrock and Southend on Sea
Bedfordshire and Milton Keynes
**London**
All 32 boroughs plus the City of London
**South East**
East Sussex
West Sussex
Brighton and Hove
Surrey
Reading
Wokingham
Bracknell Forest
Windsor and Maidenhead
West Berkshire
Hampshire (except the Isle of Wight), Portsmouth and Southampton
Buckinghamshire
Oxfordshire
**South West**
South Somerset, Somerset West and Taunton, Mendip and Sedgemoor
Bath and North East Somerset
Dorset
Bournemouth
Christchurch
Poole
Gloucestershire
Wiltshire and Swindon
Devon
**North East**
Tees Valley Combined Authority:
Hartlepool
Middlesbrough
Stockton-on-Tees
Redcar and Cleveland
Darlington
North East Combined Authority:
Sunderland
South Tyneside
Gateshead
Newcastle upon Tyne
North Tyneside
County Durham
Northumberland
**North West**
Greater Manchester
Lancashire
Blackpool
Blackburn with Darwen
**Yorkshire and The Humber**
The Humber
West Yorkshire
South Yorkshire
**West Midlands**
Birmingham and Black Country
Staffordshire and Stoke-on-Trent
Warwickshire, Coventry and Solihull
**East Midlands**
Derby and Derbyshire
Nottingham and Nottinghamshire
Leicester and Leicestershire
Lincolnshire
**South East**
Slough (remainder of Berkshire is tier 2: High alert)
Kent and Medway
**South West**
Bristol
South Gloucestershire
North Somerset"
"**All of Essex will be in tier two when England's second lockdown ends on 2 December, it has been announced.**
People in this tier cannot socialise with other households indoors.
The rule of six will apply to gatherings outdoors and pubs and restaurants will shut at 23:00 GMT and only be allowed to serve alcohol as part of meal.
Before the second lockdown, most of the county was in tier two but Thurrock and Southend-on-Sea were in tier one.
Spectators will be allowed to sporting and live events, but numbers will be more limited than those allowed in tier one areas.
Conserrvative county councillor John Spence, cabinet member for health and adult social care, said: ""We understand that going back into tier two will be hard for many, but we must all work together, follow the restrictions in order to save further lives and continue to protect our NHS.""
He said there was now ""a clear incentive"" to bring case rates down so the county could be placed in tier one when the decision was reviewed on 16 December.
Mr Spence said the tier two restrictions before the second lockdown ""had a positive impact"" on the rates of Covid-19.
Conservative MP for Harwich and North Essex, Bernard Jenkin, said: ""Obviously I'm as disappointed as anybody could be that we're not in a fit enough state to go into tier one.""
His colleague, MP for Southend West, David Amess, said he was ""very disappointed... because I did lobby for tier one"".
""I'm just hoping they will have another look at it.""
The government said the rationale for putting Essex into tier two was that cases in the county were at 159 per 100,000 people but the rate in over-60s was 100 cases per 100,000 and falling.
Mark Cory, leader of Colchester Borough Council, said the town had been ""lumped together"" with parts of the county which have higher rates of cases.
The Liberal Democrat said: ""We wouldn't be in this mess with death rates as high as the first peak if government had got test and trace working.""
Labour leader of Southend Borough Council, Ian Gilbert, said he was ""not surprised"" Essex was placed in tier two.
He said: ""Clearly the government's come to a decision that tougher restrictions are still needed in almost of the country.""
The case rate of Covid-19 in Basildon was above the England-wide average in the week to 21 November and has risen week-on-week.
Neighbouring Brentwood, where the rate has fallen, is also above the England average, as is Thurrock where the rate has risen.
Epping Forest is also above the England average and the rate there has stayed the same week-on-week.
Tier two rules mean football clubs can have up to 2,000 spectators at matches.
Colchester United owner and chairman Robbie Cowling said bringing back supporters ""must be safe and as I have said that will mean taking a cautious first step"".
He said the club would have up to 1,000 fans at its home game against Grimsby on 5 December.
Matches at elite level in England has been played behind closed doors since the return of football after the first lockdown.
Southend United, in League Two alongside Colchester, said it was delighted ""with the prospect of 2,000 fans returning"".
It said it would be working ""to ensure the safe return of supporters to the stadium as soon as possible"".
_Find BBC News: East of England on_Facebook _,_Instagram _and_Twitter _. If you have a story suggestion email_ eastofenglandnews@bbc.co.uk"
"
I missed doing a Sea Ice News last week due to being a bit discombobulated with family health issues which have now thankfully been resolved, so I’ll pick up here with a new report.
The news this week is that Arctic sea ice formation has slowed:
click for a larger image
As you can see above, after making a very fast recovery during most of October, it is now pacing the 2007 rate. This isn’t terribly unusual, as you can see a “choke point” beginning in early November where the rates of formation start to converge. Right now the JAXA daily data report is passing the 8 million square kilometer mark a value of:
10,31,2010,8038906
Earlier this week, there was some concern that there may be a sensor issue of some sort, particularly when comparing and I asked NSIDC’s Dr. Walt Meier about it, see:
NSIDC -vs- Cryosphere Today – a visual discrepancy
Compare this NSIDC Arctic Sea Ice extent chart…

…with this from Cryosphere Today:

It certainly appears that there is more ice in 2010 than 2007 on the Cryosphere Today page. Dr. Meier seems to think that the 2007 map from CT is missing some ice, as NSIDC’s comparison between the dates doesn’t appear off as much as the CT images. Walt’s point is:
There is more ice in the central Arctic this year, but less in the Beaufort Sea, Canadian Archipelago, and Baffin Bay. These areas roughly balance each other out.
Reader Lee Kington provides this blink comparator version of NSIDC’s images:

In other news, Antarctic ice continues to be significantly above normal:
Antarctic Graphs: 
For more maps and graphs, see the WUWT Sea Ice Page


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e876652a3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Indonesia is a land in turmoil, home to massive volcanoes, tsunamis, and earthquakes. On Monday, January 14, it experienced a brand new type of disturbance, the world’s first food riot caused by another nation pandering to the global warming mob. Indonesians took to the streets, demanding that their government to do something about the price of soybeans, a dietary staple.



All over the world, food prices are on the rise. For most of the late 1990s and up until 2005, the price of beans on the Chicago Board of Trade had remained stable at about $5 a bushel. Since then, they have shot up over 150 percent, to around $13. Corn has doubled, to $5. Wheat prices have tripled.



It all started with the 2005 Energy Policy Act, passed by a Republican congress and signed by a Republican president, mandating that an increasing amount of ethanol be admixed with gasoline. The bill was sold as a road to “energy independence” and as lowering the amount of carbon dioxide we emit, reducing dreaded global warming.



By now, 15 percent of our corn crop is being distilled, diverted from the proper purpose for such distillates (i.e. drinking), combusted, and sent out your car’s tailpipe.



The Act required production of four billion gallons of ethanol in 2006, increasing by approximately 700 million gallons each succeeding year. Enter those familiar characters supply, demand, and price. Supply tightens, prices escalate, and more and more farmers divert cropland from other crops (mainly soybeans and wheat) to corn. In the U.S., most crops are turned into animal feed, but in poorer countries, such as Indonesia (soybeans) or Mexico (corn for tortillas) they are consumed directly.



The ethanol malaise has also hit here at home, as a trip to the grocery store will reveal that the price of just about everything containing corn, wheat, or soybean products, or parts of animals fed on those crops, is skyrocketing. It’s hard to find a decent steak for under $12 a pound these days.



It’s only going to get worse. As if to add more 200‐​proof to the fire, President Bush, citing global warming in his 2007 State of the Union speech, called for production of 35 billion gallons of ethanol by 2017, displacing 20 percent of our current gasoline consumption with this intoxicating elixir. This is _five times_ the amount mandated in the 2005 Energy Act. He claimed that this would help us get off Middle Eastern oil.



I’ll leave the hocus about energy independence to my fellow energy wonks, because the pocus about global warming is an even easier kill.



Let’s stipulate that, indeed, 20 percent of our current gasoline consumption is somehow replaced. Transportation accounts for roughly one‐​third of our national emissions of carbon dioxide, so this would reduce our total emissions by 6.7 percent. That’s today’s emissions. Based upon recent data, the number of cars on the road will rise by this percent in about four years.





It’s hard to find a decent steak for under $12 a pound these days.



What does that do about global warming? It prevents .02º F worth of warming in the next century, based upon a formula published by the National Atmospheric Research Center in 1998. You experience this ambient temperature change every second of your life.



Now, suppose this policy were extended to all the nations of the world in which there are appreciable numbers of cars (called “Annex 1” countries by the United Nations), and the amount of warming that doesn’t occur is .05º F. No one will ever be able to detect these temperature changes in global records, which vary naturally by about .15º from year to year.



Displacing 20 percent of gasoline consumption is probably impossible. The U.S. produces more than half the world’s corn, and if we turned every kernel of it into ethanol, we’d still be 40 percent short of the President’s target.



To get there, we would have to find an economic way to make ethanol from cruder plant materials — so‐​called “cellulosic” ethanol. No matter how much money governments throw at this (including a lot from the 2005 energy bill), no one has figured out how to do this economically, and people have been at it for decades.



Of course, we won’t completely burn up our corn. We’ll incrementally ratchet it up until the inflation in food prices becomes politically untenable. Don’t be surprised, one day, if there’s a March for Food down Constitution Mall.



In other countries, there will be more riots, perhaps a coup or two, some pretty hungry people, maybe some genocide. And everywhere not a dram of a change in climate owing to ethanol will ever be measured.



The sad fact is that Indonesia’s unrest is only the slightest foreshock preceding the massive civil earthquake that is going to be unleashed as more and more absurd policies are mandated by the global warming mob.
"
"

From its peak on May 19 to its lowest point on Sept. 17, the Russian stock market has fallen by almost 58 percent. This is its largest decline since the crash of 1998. What is the cause of the current cataclysm? 



The Kremlin has been quick to blame the West, and primarily the United States, for the country’s troubles. Prime Minister Vladimir Putin blamed Western “speculators” who pulled out their investments en masse at the first sign of trouble. He also denied that Russia’s aggression toward Georgia played any role in the market’s fall. Putin suggested that the crisis is connected “not with the problems of the Russian economy, but with problems of the West’s economy.” In recent comments, he even referred to it as the “American contagion.” 



President Dmitry Medvedev concurred, saying, “The United States caused the whole crisis with its own financial market. … Regarding the factors that were responsible for the drops in the Russian stock market, I would estimate it as follows: 75 percent of the fall in the stock market is connected with consequences of the global financial crisis and 25 percent due to our internal problem, including consequences of the war in the Caucasus.”



How accurate are these assessments? 



After May 19, stock markets in almost every country of the world started to decline, and Russia followed this downward trend. Therefore, in the first stage of the global crisis, the financial problems were by no means specific to Russia. This global decline continued for almost two months. During this period, the U.S. stock market fell by 11.5 percent, the global market by 12.9 percent and the Russian market by 13.1 percent. Because the Russian market’s decline was slower in comparison to emerging markets, which fell by 17.5 percent overall, this may have confirmed for many observers what Finance Minister Alexei Kudrin claimed in January — that Russia had become an “island of stability.” 



But all of that changed on July 18, when the RTS fell by 4.5 percent, while there was little or no change in the markets in the United States, Europe or in emerging markets. What happened on that day? The Federal Migration Service granted TNK-BP chief Robert Dudley a “temporary visa” — valid for 10 days only. This was followed by a prolonged harassment campaign aimed at Dudley and other top managers from the British side of the joint venture. It became clear that the government was favoring the Russian shareholders in the conflict. In 2003, when TNK-BP was formed, the company had been touted as the crown jewel of successful joint ventures between Russian shareholders and a venerable foreign multinational corporation. It had the support at the highest level of both countries, including then‐​British Prime Minister Tony Blair and Putin. Thus, when this much‐​celebrated joint venture deteriorated into a nasty, underhanded shareholder battle, many investors drew their own conclusions about the unstable, unpredictable and arbitrary investment climate in the country. They started to pull their money out of Russia. 





The Kremlin has been quick to blame the West, and primarily the United States, for the country’s troubles.



In the following weeks, Russia’s stock market continued to drop. These declines were deeper than in other emerging markets, and this was because of the following additional events: Putin’s promise to “send a doctor” to Mechel’s director on July 24; the start of Russia’s intervention in Georgia on Aug. 8; the Kremlin’s unilateral recognition of independence for South Ossetia and Abkhazia on Aug. 26; and a whole series of official, inflammatory statements directed against the West from Sept. 3 to Sept. 17, which were accompanied by the decision to send strategic bombers to Venezuela and the announcement of naval maneuvers in the Caribbean Sea.



The authorities managed to achieve the impossible: In less than two months, Russia’s stock market declined by 51.8 percent. To be sure, stock markets in other countries also declined during this period, but it was nothing like what happened in Russia. The U.S. stock market fell by only 8.5 percent, the global market by 12.4 percent and the overall market of developing countries by 25.4 percent.



Given those dynamics, it would be difficult to blame outside influences — especially the United States — for causing Russia’s financial crisis. Had investors appraised the risk‐​return ratio for their Russian investments to be the same as that in the crisis‐​stricken United States, then the Russian stock market would have lost no more than the U.S. market did. And even if we were to take seriously what was reported in the Western media — that top Russian officials believed that the U.S. government had instructed U.S. banks not to lend to Russian companies — then why didn’t investors from Europe, Asia and the Arab states happily rush in to snatch up those greatly devalued shares at bargain prices? On the contrary, they stayed far away from the Russian market because they also evaluated it as a high‐​risk investment. 



The drop in world oil prices has also been cited as a contributing factor to the country’s stock market crisis. Indeed, oil prices fell by 38 percent over a two‐​month period — from July 17 to Sept. 17. But Russia’s market — which includes far more than just oil company shares — fell even further. In fact, the main evidence against the “oil factor” argument is the performance of markets in other leading oil‐​exporting countries. Even in these countries, where the share of oil production in their gross national product is generally higher than Russia’s, markets fell by only 20 percent. 



In the end, only a small portion of the 51.8 percent decline in Russia’s stock market can be attributed to the “American contagion.” In reality, the “U.S. factor” could not have accounted for more than a 17 percent decline. But by insisting on blaming the United States for its financial woes, the Kremlin is trying to trick the Russian people, as well as themselves. Foreigners, however, would never fall for that nonsense. 



As it turns out, at least half of the market’s fall is attributable to domestic causes. Foremost among them were the Kremlin’s attacks on Russian and foreign businesses, its aggression against Georgia followed by its recognition of independence for South Ossetia and Abkhazia, and the subsequent fears of investors and the international community as a whole that a new Cold War was about to start. Unlike Russia’s leaders, investors are scared off by any form of war — whether it is “hot” or “cold.” 



The total capitalization of Russian companies with shares traded on the stock market has fallen by almost $800 billion since May 19. Capitalization losses since July 17 alone account for more than $600 billion. Half of those losses — more than $300 billion — were the direct result of factors originating from Russia. 



The Russian government pretended to be mitigating the effects of the crisis by pumping more than $100 billion of state funds into the stock market, but nearly all of these funds went to state‐​owned companies and to other businesses that have close ties to the government. 



But Russia’s stock market crisis was brought on by more than just superficial causes. The drop in global oil prices to $90 or $100 per barrel — a symptom of the shifting winds of the global economy — was not damaging enough to trigger a crisis of this depth. Neither could the liquidity crisis be the reason, with Russia currently awash in petrodollars. Systemic, institutional problems are the real cause of this crisis. There is a fundamental incompatibility between open global markets and the universal principles of tolerance and respect that govern it in the West, on the one hand, and the paranoia and aggressiveness of Russia’s current leadership, with its cult of isolation and militarism and the modus operandi of street gangsters, on the other hand. 



In Korney Chukovsky’s classic children’s poem, “Putanitsa” (“The Muddle”), a crocodile is unable to put out a fire with pirogi and blini. Similarly, the fire of Russia’s deep and long‐​term institutional crisis cannot be extinguished with the financial “pirogi and blini” offered by Russian leaders — particularly when these goodies are divvied out only to their close friends. 
"
"

THIS WEEK:
By  Ken Haapala, Executive Vice President Science and Environmental Policy Project  (SEPP)
On Thursday, The French  Academy of Sciences released a report declaring the global warming exists and is  unquestionably due to human activity. The academy president declared the debate  is over. Former education minister Claude Allegre, who questioned the orthodoxy,  signed off on what he considered a compromise report stating: “I have not  evolved, I still say the same thing, that the exact role of carbon dioxide in  the environment has not been shown.”

The report  recognized uncertainties in solar influence, clouds, oceans and atmosphere.  Those who believe that human carbon dioxide emissions may have some warming  effect, but are not the dominant driver of climate change, may find the report  acceptable except that it gives carbon dioxide a principal role in climate  change. We await the translation of the full report, but apparently there is no  precision in the report. A vague statement, no matter how forcefully made,  remains vague. Please see Article # 1
***************************************
In an  article published on October 12, Bjorn Lomborg discusses the change in the  vocabulary of the global warming alarmists. No longer is global warming, or  climate change, the major theme. Instead, it has been replaced by clean energy,  clean jobs – a green economy. Lomborg also discusses how much a green economy is  costing his native country, Denmark. He believes that drastic carbon cuts are a  poor response to global warming. Please see Article # 2.
In another article for the Investors’ Business Daily (IBD), Lomborg  advocates committing streams of money to technical improvements in new wind and  solar energy, as well as other technical innovations. Lomborg’s comments are  rebutted in a follow-up article in IBD by Willie Soon, Bob Carter, and David  Legates who bring up a seldom mentioned issue: the benefits of increased CO2  Much is made of what economists call the external costs of carbon dioxide  emissions, namely global warming which is always considered bad. But increased  CO2 in the atmosphere stimulates more vigorous growth of plant life that  benefits humanity and the environment.
***************************************
The  Department of Interior has approved the building of what is called the world’s  largest solar-thermal power plant on 7,000 acres of Federal land in the desert  of Southern California. The project is a venture by two German companies. The  first half of the project could be eligible for a cash subsidy of $900,000,000  from the stimulus bill. The cash subsidy program ends on December 31, 2010.  Also, the companies are seeking Federal loan guarantees and, no doubt, an array  of benefits from the state.
To put the cash subsidy  perspective, it is useful to calculate the employment benefits. The  administration claims this project will provide up to 300 new permanent jobs.  This calculates out to $3,000,000 per permanent job. At that rate it would cost  about $20.27 Trillion to reduce the current unemployment rate (9.2% est. by US  Bureau of Labor Statistics) to the rough average over the past 15 years of 5%.  $20.27 Trillion is about 1.4 times the entire gross domestic product of the US  in 2009 (estimated to be $14.26 Trillion by the US Bureau of Economic Analysis).  The expenditure is enormous, but does it benefit the citizens of California by  providing affordable electricity?
As seen in other  reports (Article # 3 and articles under California Dreaming) there are  additional solar projects in California which promoters are trying to start  before December 31. These stories indicate that even after subsidies, the cost  of the electricity generated will be 30 to 70 percent more expensive than  electricity generated by natural gas, the dominant electricity generating fuel  in California. The promoters of the projects consider a 30 to 70 percent  increase in cost to be competitive – a clear consequence of the state’s  renewable energy mandates. Only in California!
***************************************
THE NUMBER  OF THE WEEK: 24 to 1 – the number of nuclear power plants under construction  in China (as reported by the World Nuclear Association) compared to the number  of nuclear power plants under construction in the US.
Green energy promoters stridently insist that we are in a race with China  to develop green energy, namely solar and wind. Spain and Germany were in the  race but dropped out and their green energy firms are suffering as the subsidies  stopped.
The question seldom asked is China really  in the same race? Over the next several weeks, The Number of the Week will  explore that question. If China is in a nuclear power race it is clearly  winning. Please see Nuclear Power in China under Energy Issues.
[Please note that the 104 nuclear power plants in  the US have a very high average capacity factor of over 90%.]
——————–
SEPP  SCIENCE EDITORIAL #32-2010 (Oct. 30, 2010)
S Fred Singer Chairman, and President, Science and  Environmental Policy Project (SEPP)
Why the Confusion about Global  Warming?
No one denies that the Earth  has warmed in the past century. So of course, the past decade must be the  warmest – even though there has been no upward trend since the 1998 temperature  peak. [Note the important distinction between temperature level (measured in deg  C or deg F) and trend (expressed in deg C per year).] The dispute is (and always  has been) about the cause of the warming. In fact, the major warming during the  first 50 years of the 20th century and the latter part of the 19th century is  generally accepted to be natural – a recovery from the Little Ice Age. But  there’s no credible evidence that identifies the most recent warming as  human-caused. On the contrary, while the UN’s IPCC claims to be quite certain  that it is anthropogenic, the independent NIPCC (Non-governmental International  Panel on Climate Change) concludes that “Nature – Not Human Activity – Rules the  Climate.” See  http://www.sepp.org/publications/NIPCC_final.pdf
In  this connection note the obfuscatory language used by the EPA in turning down  all of the ‘Petitions for Reconsideration’ of its Endangerment finding on CO2:  “The scientific evidence supporting EPA’s finding is robust, voluminous, and  compelling. Climate change is happening now, and humans are contributing to it.  Multiple lines of evidence show a global warming trend over the past 100 years.  Beyond this, melting ice in the Arctic, melting glaciers around the world,  increasing ocean temperatures, rising sea levels, altered precipitation  patterns, and shifting patterns of ecosystems and wildlife habitats all confirm  that our climate is changing.”
Yet there is no  evidence at all that humans are indeed contributing to warming in a significant  way. We’ll see you in court, dear EPA, and gladly examine your “compelling”  evidence!
– – – – – – – – – – – – – – – –  –
ARTICLES: For the numbered articles below please see:
The Week That Was 
1. Global warming ‘unquestionably’ linked to humans:  France
By Claire Snegaroff, APF, Oct 28,  2010
http://www.google.com/hostednews/afp/article…
2. What Have Climate Activists Learned
By Bjorn Lomborg, Project Syndicate, Oct 12, 2010 [H/t Berol  Robinson]
http://www.project-syndicate.org/commentary/lomborg6….
[SEPP Comment: The new hype is green energy, green jobs but the purpose  is the same – control of carbon dioxide emissions.]
3. Huge Solar-Plant Project Approved
By Cassandra Sweet and Siobhan Hughes, WSJ, Oct 26, 2010
http://online.wsj.com/article/SB10001424052702303467….
4. Disputing The Skeptical  Environmentalist
By Willie Soon, Robert  Carter, and David Legates, IBD, Oct 29, 2010
http://www.investors.com/NewsAndAnalysis/ArticlePrin….
5. Observe Other’s Past Energy  Experiences
By Charles Battig, Letter,  Richmond Times Dispatch, Oct 21, 2010
http://www2.timesdispatch.com/news/2010/oct21/ed-bat….
– – – – – – – – – – – – – – – –  –
NEWS  YOU CAN USE:
Challenging the Orthodoxy
Cabal of climate  skeptics to descend on parliament
By Leo  Hickman, Guardian, UK, Oct 26, 2010
http://www.guardian.co.uk/environment/blog…
Defending the  Orthodoxy
Climate Change May  Alter Natural Climate Cycles of Pacific
Science  Daily, Oct 18, 2010 [H/t Toshio Fujita]
http://www.sciencedaily.com/releases/2010/10/1010171….
[SEPP Comment: The IPCC and other advocates have ignored the influence of  natural cycles in the Pacific on global warming. Now some claim global warming  will change these cycles.]
Why Can’t We Innovate  Our Way To A Carbon-Free Energy Future?
By Bjorn  Lomborg, IBD, Oct 22, 2010
http://www.investors.com/NewsAndAnalysis/Article/551….
Weather  Extremes
Arctic Temperatures  and Ice – Why it is All About Natural Variability
By Joseph D’Aleo, ICECAP, Oct 24, 2010
http://www.icecap.us/…
Warmer Arctic Temps  Tied to U.S. Snowstorms
CBS News, Oct 22, 2010,  [H/t Joe D’Aleo ICECAP]
http://www.cbsnews.com/stories/2010/10/22/tech/main6….
NOAA: “Arctic Report  Card: Update for 2010”
By Arnd Bernaerts,  Digging In the Clay, Oct 25, 2010 [H/t ICECAP]
http://diggingintheclay.wordpress.com/2010/10/27/noa….
2010 Hurricane  Factoids
Roger Pielke, Jr, Blog, Oct 25, 2010  [H/t Marc Morano, Climate Depot]
http://rogerpielkejr.blogspot.com/2010/10/2010-hurri….
[SEPP Comments: Another disappointing season for those hyping  hurricanes.]
BP Oil  Spill and Aftermath
Panel Says Firms Knew  of Cement Flaws Before Spill
By John Broder,  NYT, Oct 28, 2010
http://www.nytimes.com/2010/10/29/us/29spill.html?_r….
Another Drilling  Smackdown
Editorial, WSJ, Oct 25,  2010
http://online.wsj.com/article/SB10001424052702304741….
Energy Issues
Nuclear Power in  China
World Nuclear Association, Oct 22,  2010
http://www.world-nuclear.org/info/inf63.html…
Half The  Productivity, Twice The Carbon
By Staff Writers,  Energy Daily, Oct 26, 2010 [H/t Catherine French]
http://www.energy-daily.com/reports/Half_The_Product….
[SEPP Comment: The IT industry needs affordable, reliable electricity.  This is news?]
Can Solar Shield  Protect The North American Power Grid
By Tony  Phillips, Science News, [H/t Toshio Fujita]
http://www.spacedaily.com/reports/Can_Solar_Shield_P….
Is Wind the Next  Ethanol?
By Ben Lieberman, CEI, Oct 26, 2010  [H/t Cooler Heads Digest]
http://cei.org/studies-point/wind-next-ethanol…
German grid aching  under solar power
UPI, Oct 19, 2010  http://www.upi.com/Science_News/Resource-Wars/2010/10/19/German-grid-aching-under-solar-power/UPI-13471287518368/
Time To Remove The  Roadblocks To A National Transmission Grid
By  Gilbert Metcalf, IBD, Oct 26, 2010
http://www.investors.com/NewsAndAnalysis/Article/551….
Hydrogen-generating  technology might power boats, store energy from wind, solar  sources
By Emil Venere, Press Release, Purdue  University, Oct 7, 2010 [H/t Toshio Fujita]
http://www.purdue.edu/newsroom/research/2010/101007W….
Subsidies and Mandates  Forever
Spending Review:  Honesty is the best policy before the bigger fuel bills start to  bite.
By Charles Moore, Telegraph, UK, Oct 22,  2010 [H/t Bob Ferguson, SPPI]
http://www.telegraph.co.uk/comment/columnists/charle….
Perplexing energy  policy
By Steen Syre, Boston Globe, Oct 26, 2010  [H/t Randy Randol]
http://www.boston.com/business/articles/2010/10/26/p….
Remember Renewable  Energy?
Editorial, NYT, Oct 27,  2010
http://www.nytimes.com/2010/10/28/opinion/28thurs1.h….
The Race for Future  Clean-Energy Jobs
By Terry McAuliffe, Richmond  Times Dispatch, Oct. 27, 2010
http://www2.timesdispatch.com/news/oped/2010/oct/27/….
California  Dreaming
Solar Power Project  Face Potential Hurdles
By Todd Woody, NYT, Oct  28, 2010
http://www.nytimes.com/2010/10/29/business/energy-en….
DOI Approves 1,000-MW  Rated Parabolic Trough Project
Power News, Oct  27, 2010
http://www.powermag.com/POWERnews/3127.html?hq_e=el&….
EPA and other Regulators On the  March
NERC: EPA Regulations  Could Impact System Reliability
Power News, Oct  27, 2010
http://www.powermag.com/POWERnews/3125.html?hq_e=el&….
Oh, Mann!
Cuccinelli Demands  Called ‘Governmental Intrusion’ Into Climate Science
By Eli Kintisch, Science Insider, Oct 21, 2010 [H/t Toshio  Fujita]
http://news.sciencemag.org/scienceinsider/2010/10/cu….
[SEPP Comment: Climate science is largely dependent on government  support. Now an investigation of possible inappropriate application of such  funds is a governmental intrusion?]
Review of Recent Scientific Articles by  NIPCC
For a full  list of articles see NIPCC Report
Flocks of Birds  Coping with Climate Change
Reference: Van  Buskirk, J., Mulvihill, R.S. and Leberman, R.C. 2010. Declining body sizes in  North American birds associated with climate change. Oikos 119:  1047-1055.
http://www.nipccreport.org/articles/2010/oct/27oct20….
Amphibian Population  Declines
Reference Rohr, J.R., Raffel, T.R.,  Romansic, J.M., McCallum, H. and Hudson, P.J. 2008. Evaluating the links between  climate, disease spread, and amphibian declines. Proceedings of the National  Academy of Sciences USA 105: 17,436-17,441.
http://www.nipccreport.org/articles/2010/oct/28oct20….
Effects of Elevated  CO2 on Longevity and Fecundity of an Invasive Weevil Feeding on Aspen, Birch and  Maple Foliage
Reference: Hillstrom, M.L., Vigue,  L.M., Coyle, D.R., Raffa, K.F. and Lindroth, R.L. 2010. Performance of the  invasive weevil Polydrusus sericeus is influenced by atmospheric CO2 and host  species. Agricultural and Forest Entomology 12: 285-292.
http://www.nipccreport.org/articles/2010/oct/28oct20….
Unexpected Biological  Resilience to Climate Change
Reference: Bell,  R.C., Parra, J.L., Tonione, M., Hoskin, C.J., Mackenzie, J.B., Williams, S.E.  and Moritz, C. 2010. Patterns of persistence and isolation indicate resilience  to climate change in montane rainforest lizards. Molecular Ecology 19:  2531-2544.
http://www.nipccreport.org/articles/2010/oct/28oct20….
Other Scientific  Issues
Introducing the  A-Train
By Adam Voiland, NASA Press Release, Oct  27, 2010 [H/t Anthony Watts, WUWT]
http://www.nasa.gov/mission_pages/a-train/a-trainht….
[SEPP Comment: An explanation of a train of satellites measuring the  earth’s changes.]
Changing Our  Understanding Of Atmospheric Aerosol Properties And Climate  Effects
By Staff Writers, Terra Daily, Oct 18,  2010 [H/t Toshio Fujita]
http://www.terradaily.com/reports/Changing_Our_Under….
[SEPP Comment: The influence of aerosols on the earth’s climate is  largely unknown. Better understanding of the physical nature of some aerosols is  an important step.]
Bees’ tiny brains  beat computers, study finds
Bees can solve  complex mathematical problems which keep computers busy for days research has  shown
Guardian, UK, Oct 24, 2010 [H/t A.J.  Meyer]
http://www.guardian.co.uk/world/2010/oct/24/bees-rou….
– – – – – – – – – – – – – – – –  –
BELOW  THE BOTTOM LINE:
Al Gore compares  human heart to hydrological cycle
By Rance  Leroy, French Tribune, Oct 21, 2010 [H/t Best on the Web]
http://frenchtribune.com/teneur/101652-al-gore-compa….
Space tourism to  accelerate climate change
By Adam Mann, Nature  News, Oct 22, 2010 [H/t A.J. Meyer]
http://www.nature.com/news/2010/101022/full/news.201….
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e87a86e29',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In his State of the Union address, President Bush pressed Congress to quickly pass legislation to make permanent the sweeping spying powers that Congress granted last August. Those powers, which include the ability to eavesdrop on foreign‐​to‐​domestic communications without meaningful judicial oversight, were due to expire last week. Congress has passed a two‐​week extension of the law, but that barely gives Congress time to catch its breath before the White House resumes its campaign to make it permanent.



Bush’s predecessor was also an ardent supporter of increased wiretapping authority. For example, on July 29, 1996, Bill Clinton unveiled a proposal to expand government surveillance by permitting the use of “roving wiretaps.” The nation was still reeling from terrorist attacks on the Atlanta Olympics and American barracks in Saudi Arabia, and many suspected that the explosion of TWA Flight 800 was also the work of terrorists. Clinton argued that these tragedies highlighted the need for legislative changes, and he pressed Congress to act before its August recess.



But Congress had a bipartisan tradition of its own to defend. As they had done since Watergate, Congressional leaders raised concerns about civil liberties. Then‐​Speaker Newt Gingrich said he was willing to consider changes to the law, but vowed to do so “in a methodical way that preserves our freedoms.” Senate Majority Leader Trent Lott vowed that Congress would not “rush to a final judgment” before going on vacation. In the end, the 104th Congress finished its term without giving President Clinton the wiretapping authority he sought.





Why are today’s Democrats less concerned with civil liberties than Republicans were a decade ago?



Today’s Democratic Congress has been far less protective of Americans’ privacy rights. Last August, in a virtual repeat of the events of 1996, Bush demanded that Congress approve expanded wiretapping powers before going on vacation. This time, Congressional leaders showed few qualms about “rushing to judgment.” Indeed, both houses of Congress approved the White House’s preferred legislation with minimal changes within three days of its introduction.



Why are today’s Democrats less concerned with civil liberties than Republicans were a decade ago? Democratic leaders would doubtless point to the 9/11 attacks. Those attacks have certainly contributed to a changed political climate, but they don’t justify Congress’s panicky reaction to the president’s demands. Congress had already expanded eavesdropping powers several times since 9/11. Congress approved new wiretapping authority with the Patriot Act in 2001, and approved further expansions later in 2001 and in 2002, 2004, and 2006. If the new powers the president was seeking weren’t urgent enough to include in those revisions to the law, it’s hard to believe they were an emergency in August 2007.



Moreover, the powers Congress granted last summer are far broader those sought by the Clinton administration in 1996. The “roving wiretaps” Clinton requested in 1996 and finally received in 1998 merely allowed investigators to obtain a single warrant to bug multiple phones used by a specific individual. In contrast, the Protect America Act completely eliminates the warrant requirement for surveillance “concerning persons reasonably believed to be outside of the United States” — even if one party to a call is an American citizen and the wiretap occurs on American soil. The attorney general is required to disclose to a secret court the general procedures used to choose wiretapping targets, but no judge reviews the list of specific targets to verify that the law is being followed. This evisceration of judicial review is an invitation to future abuses.



The lone virtue of the Protect America Act is that the powers it granted are now set to expire in mid‐​February. As this revised deadline approaches, Speaker Nancy Pelosi and Majority Leader Harry Reid will once again face pressure to rush the White House’s preferred legislation out the door. The president will claim that failure to act before the Protect America Act sunsets will undermine the government’s ability to eavesdrop on terrorists.



It’s an ominous claim, but it’s not true. The Protect America Act allows the administration to “authorize” eavesdropping programs for a year at a time. That means that the government’s various warrantless surveillance activities will continue to operate at least through August. And of course, if the need for new wiretaps arises after the act sunsets, the administration still has the opportunity to file for warrants under the Foreign Intelligence Surveillance Act (FISA). FISA even allows the government to begin surveillance first and apply for an emergency warrant after the fact.



In short, the administration will have ample authority to intercept terrorist communications for at least the next six months. As they shepherd FISA reform through Congress, Pelosi and Reid would do well to heed the advice of one of Pelosi’s predecessors: “The goal here is not to allow the terrorists to pressure us into suspending the very freedoms that make America precious.” Those words are as true today as when Newt Gingrich said them in 1996.
"
"
Share this...FacebookTwitterGerman daily Bild, number one by circulation in Europe, plants more seeds of man-made global warming skepticism in an article today about this year’s really crappy wet and cool summer.

“And just like the guy whose feet are too big for his bed, nothing seems to fit…”
While Bild asks how long the Azores high pressure system “Xerxes” will hold up and bring Germany the much welcome relief from all the cold and rain, the report also looks into long-term climate trends.
Bild poses the question: “Were summers of the last years really cooler, wetter and less freindly than before?”
To answer that question, Bild consulted wetter.net meteorologist Dominik Jung (a global warming believer). His answer:
It really is so. We took a look at the summer of the last 9 years. The tendency is that they were cooler and rainier than the long-term average.“
Normally a newspaper reporting on the summer’s weather would just leave it at that. But Bild takes the extra step and looks back at the summers over the last 2000 years and brings up Jan Esper’s recently published tree-ring study: Bild writes:
If you don’t believe Domink Jung, then we take a look at the longterm.analysis of an international tean of scientists. In it the scientists measured the wood density of trees and reconstructed the weather of the last 2000 years. Fact is: There is a cooling trend for the season that we call summer.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It has gotten about 0.3°C cooler each millenium. And we are feeling that trend today as well.”
So what is Bild telling its readers? I’d interpret it as: There’s nothing unusual about the currrent climate and over the long-term we are cooling.
Glad to see that a major media outlet in Germany is open to the broader climate picture. Very very few other media outlets have reported on the implications of Esper’s 2,000-year reconstruction.
Looks like Germany’s über-alarmist, closed-minded institutes, like Jochem Marotzke’s Max Planck Institute for Meteorology and John Schellnhuber’s Potsdam Institute for Climate Impact Research are going to have to go back to the drawing board and completely revamp their “climate models”. So far their projections have completely and massively diverged away from recorded observations.
And once again:

“It doesn’t matter how smart you are, or what your name is – it’s wrong. It’s that simple.”
Also very worthwile watching…on “vague theories”.

 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWhat does Hans von Storch’s “most effective climate policy” look like?


The 1936 German sign reads “We don’t stand alone.” Well, in 2012 add China’s flag to the poster for countries that practice forced sterilizations.
Hat tip a reader/blogger:

Breaking China’s One-Child Law
In an unprecedented crackdown, Chinese officials set out to sterilize 10,000 women — by jailing their relatives until the women submitted.
A dozen Chinese officials had beaten down the man’s door and dragged him away. “What has he done wrong?” Wei asked in alarm. “Nothing,” her husband replied. “He has been jailed because he is related to us.”
Wei, a bird-thin woman with bobbed hair, let lunch burn on the stove as she heard more. “My husband said we had broken the law by having two children. The authorities were imprisoning his brother until we were punished,” she says. “As soon as I learned it was about birth control, I began to cry and shake.” Family-planning officials in the southern county of Puning, in Guangdong province, were going to shocking new extremes to catch and punish violators of the country’s infamous one-child policy: They were seizing family members of women who had given birth illegally and were holding them hostage. The aim? To coerce the women into submitting to sterilization. Says Wei, “The officials said there was only one way to get my brother-in-law released: I had to undergo forced sterilization.”
As Wei panicked in her kitchen, the same scene was playing out in households all over Puning, a region of 2.2 million people, about six hours by bus from the provincial capital of Guangzhou. In early April, the local Family Planning Bureau, which oversees population control, launched what it termed an “Iron Fist Campaign,” targeting 10,000 women who had more than one child…continue reading…
I’m stunned that von Storch actually published that comment. I can’t believe it. Surely he was being cynical. The world is on its head today.
 

Share this...FacebookTwitter "
"

Well the Telegraph article on rationing our modern lifestyle to reduce CO2 has made some waves since I covered it yesterday, fortunately it has not affected the attendees at Cancun. See the video below.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e86a78c53',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI’d like to recommend the German Energy Blog to readers who’d like to know the latest about German energy news and issues. You may want to bookmark it.

German Energy Blog
Their latest article presents a resource called 50Hz:
Friday last week 50Hertz Transmission GmbH, the transmission system operator (TSO) whose grid covers large parts of Eastern and Northern Germany, started publishing the load flow data for its grid on the internet. It is the first TSO in Germany to do so.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Visitors of the web pages shall find an up-to-date map of the grid showing all power lines and interconnectors with other transmission grids. Network conditions shall be shown for the full hour. For a better understanding the lines shall be coloured depending on the specific load flow. Upon clicking on a power line, the load shall be given in megawatt (MW). The information is supplemented by data regarding special measures to secure system stability, i.e. …” (Continue reading)
It would be nice if all grid operators offered this service.
Especially the 50HZ Grid Data page offers up-to-date, interesting data on energy feed-in, etc.. For example here is photovoltaic feed-in for the last 24 hours:

Source: http://www.50hertz.com/en/2805.htm.
 
Share this...FacebookTwitter "
"
You know, in science, there was once this thing we called the Theory of Multiple Working Hypotheses.  Anathema (a formal ecclesiastical curse accompanied by excommunication) in modern climate science.  So, in juxtaposition to the hypothesis of future global climate disruption from CO2, a scientist might well consider an antithesis or two in order to maintain ones objectivity.
One such antithesis, which happens to be a long running debate in paleoclimate science, concerns the end Holocene.  Or just how long the present interglacial will last.
Looking at orbital mechanics and model results, Loutre and Berger (2003) in a landmark paper (meaning a widely quoted and discussed paper) for the time predicted that the current interglacial, the Holocene, might very well last another 50,000 years, particularly if CO2 were factored in.  This would make the Holocene the longest lived interglacial since the onset of the Northern Hemisphere Glaciations some 2.8 million years ago.  Five of the last 6 interglacials have each lasted about half of a precession cycle.  The precession cycle varies from 19-23k years, and we are at the 23kyr part of the range now, making 11,500 years half, which is also the present age of the Holocene.
Which is why this discussion has relevance. 
But what about that 6th interglacial, the one that wasn’t on the half-precessional “clock”.  That would be MIS-11 (or the Holsteinian) which according to the most recently published estimate may have lasted on the order of 20-22kyrs, with the longest estimate ranging up to 32kyrs.
Loutre and Berger’s 2003 paper was soon followed by another landmark paper by Lisieki and Raymo (Oceanography, 2005), an exhaustive look at 57 globally distributed deep Ocean Drilling Project (and other) cores (Figure 1), which stated:
“Recent research has focused on MIS 11 as a possible analog for the present interglacial [e.g., Loutre and Berger, 2003; EPICA community members, 2004] because both occur during times of low eccentricity.  The LR04 age model establishes that MIS 11 spans two precession cycles, with 18O values below 3.6o/oo for 20 kyr, from 398-418 ka.  In comparison, stages 9 and 5 remained below 3.6o/oo for 13 and 12 kyr, respectively, and the Holocene interglacial has lasted 11 kyr so far.  In the LR04 age model, the average LSR of 29 sites is the same from 398-418 ka as from 250-650 ka; consequently, stage 11 is unlikely to be artificially stretched.  However, the June 21 insolation minimum at 65N during MIS 11 is only 489 W/m2, much less pronounced than the present minimum of 474 W/m2.  In addition, current insolation values are not predicted to return to the high values of late MIS 11 for another 65 kyr.  We propose that this effectively precludes a ‘double precession-cycle’ interglacial [e.g., Raymo, 1997] in the Holocene without human influence.”
 


Figure 1.  The past 5 million years of climate from 57 globally distributed sediment cores.  (a general definition of an interglacial since the MPT is the oxygen 18/oxygen 16 isotope ratio must drop to 3.6 parts per mil)

To bring this discussion up to date, Tzedakis (Figure 2, his figure 3), in perhaps the most open peer review process currently being practiced in the world today (The European Geosciences Union website Climate of the Past Discussions) published a quite thorough examination of the state of the science related to the two most recent interglacials, which like the present one, the Holocene (or MIS-1) is compared to MIS-19 and MIS-11, the other two interglacials which have occurred since the Mid Pleistocene Transition (MPT) and also occurred at eccentricity minimums.  Since its initial publication in 2009, and its republication after the open online peer review process again in March of this year (2010), this paper is now also considered a landmark review of the state of paleoclimate science.  In it he also considers Ruddiman’s Early Anthropogenic Hypothesis, with Ruddiman a part of the online review.  Tzedakis’ concluding remarks are enlightening:
“On balance, what emerges is that projections on the natural duration of the current interglacial depend on the choice of analogue, while corroboration or refutation of the “early anthropogenic hypothesis” on the basis of comparisons with earlier interglacials remains irritatingly inconclusive.”
Figure 2. Tzedakis (2010) comparing the Holocene with the previous 4 interglacials.
An astute reader might have gleaned that even on things which have happened, the science is not that particularly well settled.  Which makes consideration of the science being settled on things which have not yet happened dubious at best.
As we move further towards the construction of the antithetic argument, we will take a closer look at the post-MPT end interglacials and the last glacial for some clues.
Higher resolution proxy studies from many parts of the planet suggest that the end interglacials may be quite the wild climate ride from the perspective of global climate disruption.
Boettger, et al (Quaternary International 207 [2009] 137–144) abstract it:
“In terrestrial records from Central and Eastern Europe the end of the Last Interglacial seems to be characterized by evident climatic and environmental instabilities recorded by geochemical and vegetation indicators.  The transition (MIS 5e/5d) from the Last Interglacial (Eemian, Mikulino) to the Early Last Glacial (Early Weichselian, Early Valdai) is marked by at least two warming events as observed in geochemical data on the lake sediment profiles of Central (Gro¨bern, Neumark–Nord, Klinge) and of Eastern Europe (Ples).  Results of palynological studies of all these sequences indicate simultaneously a strong increase of environmental oscillations during the very end of the Last Interglacial and the beginning of the Last Glaciation. This paper discusses possible correlations of these events between regions in Central and Eastern Europe.  The pronounced climate and environment instability during the interglacial/glacial transition could be consistent with the assumption that it is about a natural phenomenon, characteristic for transitional stages. Taking into consideration that currently observed ‘‘human-induced’’ global warming coincides with the natural trend to cooling, the study of such transitional stages is important for understanding the underlying processes of the climate changes.”
Hearty and Neumann (Quaternary Science Reviews 20 [2001] 1881–1895) abstracting their work in the Bahamas state:
“The geology of the Last Interglaciation (sensu stricto, marine isotope substage (MIS) 5e) in the Bahamas records the nature of sea level and climate change.  After a period of quasi-stability for most of the interglaciation, during which reefs grew to +2.5 m, sea level rose rapidly at the end of the period, incising notches in older limestone.  After brief stillstands at +6 and perhaps +8.5 m, sea level fell with apparent speed to the MIS 5d lowstand and much cooler climatic conditions. It was during this regression from the MIS 5e highstand that the North Atlantic suffered an oceanographic ‘‘reorganization’’ about 118.73 ka ago.  During this same interval, massive dune-building greatly enlarged the Bahama Islands.  Giant waves reshaped exposed lowlands into chevron-shaped beach ridges, ran up on older coastal ridges, and also broke off and threw megaboulders onto and over 20 m-high cliffs. The oolitic rocks recording these features yield concordant whole-rock amino acid ratios across the archipelago.  Whether or not the Last Interglaciation serves as an appropriate analog for our ‘‘greenhouse’’ world, it nonetheless reveals the intricate details of climatic transitions between warm interglaciations and near glacial conditions.” 
See Figure 3 (also figure 3 in their study)
Figure 3. Rapid Sea Level Spike at the end of MIS-5, the Eemian.
and Figure 4 (figure 5 in their study).



Figure 4. The MIS-5e notch (photo A) and modern notch (photo B) (Hearty and Neumann, 2001, figure 5).


The picture which emerges is that the post-MPT end interglacials appear to be populated with dramatic, abrupt global climate disruptions which appear to have occurred on decadal to centennial time scales.  Given that the Holocene, one of at least 3, perhaps 4 post-MPT “extreme” interglacials, may not be immune to this repetitive phenomena, and as it is half a precession cycle old now, and perhaps unlikely to grow that much older, this could very well be the natural climate “noise” from which we must discern our anthropogenic “signal” from.
If we take a stroll between this interglacial and the last one back, the Eemian, we find in the Greenland ice cores that there were 24 Dansgaard-Oeschger oscillations (Figure 5, originally figure 1. Sole et al, 2007), or abrupt warmings that occurred from just a few years to mere decades that average between 8-10C rises (D-O 19 scored 16C).  The nominal difference between earth’s cold (glacial) and warm (interglacial) states being on the order of 20C.  D-O events average 1470 years, the range being 1-4kyrs.
Figure 5. Dansgaard-Oeschger oscillations with their cycle designations. (Sole et al, 2007)
Sole, Turiel and Llebot writing in Physics Letters A (366 [2007] 184–189) identified three classes of D-O oscillations in the Greenland GISP2 ice cores A (brief), B (medium) and C (long), reflecting the speed at which the warming relaxes back to the cold glacial state:
“In this work ice-core CO2 time evolution in the period going from 20 to 60 kyr BP [15] has been qualitatively compared to our temperature cycles, according to the class they belong to.  It can be observed in Fig. 6 that class A cycles are completely unrelated to changes in CO2 concentration.  We have observed some correlation between B and C cycles and CO2 concentration, but of the opposite sign to the one expected: maxima in atmospheric CO2 concentration tend to correspond to the middle part or the end the cooling period.  The role of CO2 in the oscillation phenomena seems to be more related to extend the duration of the cooling phase than to trigger warming. This could explain why cycles not coincident in time with maxima of CO2 (A cycles) rapidly decay back to the cold state. ”
“Nor CO2 concentration either the astronomical cycle change the way in which the warming phase takes place.  The coincidence in this phase is strong among all the characterized cycles; also, we have been able to recognize the presence of a similar warming phase in the early stages of the transition from glacial to interglacial age.  Our analysis of the warming phase seems to indicate a universal triggering mechanism, what has been related with the possible existence of stochastic resonance [1,13, 21].  It has also been argued that a possible cause for the repetitive sequence of D/O events could be found in the change in the thermohaline Atlantic circulation [2,8,22,25].  However, a cause for this regular arrangement of cycles, together with a justification on the abruptness of the warming phase, is still absent in the scientific literature.”
Figure 6. Sole et al (2007) D/O oscillation classes.
In their work, at least 13 of the 24 D-O oscillations (indeed other workers suggest the same for them all), CO2 was not the agent provocateur of the warmings but served to ameliorate the relaxation back to the cold glacial state, something which might have import whenever we finally do reach the end Holocene.  Instead of triggering the abrupt warmings it appears to function as somewhat of a climate “security blanket”, if you will.
Therefore in constructing the antithesis, and taking into consideration the precautionary principle, we are left to ponder if reducing CO2’s concentration in the late Holocene atmosphere might actually be the wrong thing to do.
The possibility consequently exists that at perhaps precisely the right moment near the end-Holocene, the latest iteration of the genus Homo unwittingly stumbled on the correct atmospheric GHG recipe to perhaps ease or delay the transition into the next glacial.  Under the antithesis “Skeptics” and “Warmists” thus find themselves on the mutual, chaotic climate ground where the efficacy of CO2 as a GHG had better be right.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e85f9cb45',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I don’t actually have this title category, I just invented the title in honor of what I just stumbled across flipping through channels on DirecTV. I landed on the History Channel. Egads! Some diving guys on a boat haul around some scientist with a “magnetic anomaly detector”, which looks like a Radio Shack electronics kit gone bad, and are looking for black holes (yes the gravitational kind) in the Bermuda Triangle. Yes, really.
Here’s the DVD you can buy from the History Channel.

And here’s the program description:
Explore with us the wonders and mysteries of the Black Holes in our  universe. Is it possible that areas on earth might, in fact, show black  hole like tendencies?

We take a hard scientific look at an area  known as the Bermuda Triangle to see if there are indeed any  similarities between the supposed forces in the triangle and the  destructive force of a black hole.
From a research boat trip  through the triangle to interviews with scientists at the US Geological  Survey, Harvard University, and the UK’s Cardiff University, we go far  beyond the event horizon to explore the dangers in this area and what  relation they might indeed have with its counterpoint in space.
===================================
There’s a line in the TV show where they say “…there’s no question that the climate can change suddenly around the Bermuda triangle”…so for these folks, I guess weather is climate. *Sigh* God help us.
The poor chumps at these prestigious organizations they brought in as experts probably had no idea that they’d appear in a dreckumentary that has the crew of the Minnow looking for black holes under the sea in the Bermuda triangle.
Of course the History Channel also shows “Life after people” and Gore’s “An Inconvenient Truth“…so I suppose crap like black hole hunting in the Bermuda Triangle fits right in.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8815c7b5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Police have been granted extra powers, including carrying out random vehicle checks, to ensure people are not breaking Covid-19 rules in Cardiff.**
South Wales Police said it would increase the numbers of officers on duty to encourage people to adhere to the rules.
The powers will be in place from 09:00 on Friday until 17:00 on Sunday.
People breaching regulations could be fined and told to leave the city, the force said.
Wales' ""firebreak"" lockdown ended on 9 November, allowing pubs, restaurants and non-essential retailers to reopen, with social-distancing measures in place.
People living in Wales could also travel anywhere in the country from this date, but travel to and from England, which is still in lockdown until Wednesday, is banned.
Supt Jason Rees said: ""The past few months have been difficult for us all, and with the rules having relaxed slightly, non-essential business reopening and Christmas just around the corner, it's understandable that people will want to get out and about and enjoy all our city has to offer.
""The vast majority are doing so with caution and within the confines of the existing regulations, but those not adhering to the rules are continuing to put others at increased risk.
""We are anticipating another busy weekend in our city centre, and while we will continue to adopt the policing style we have throughout the pandemic - working with the public to encourage voluntary compliance - we are committed to enforcing where blatant and flagrant breaches occur.""
North Wales Police said it had no special operation in place for the coming weekend but had an ""ongoing dedicated operation, which targets unlawful travel movement amongst other types of Welsh Government regulation breaches such as unlawful gatherings"".
In the run up to Christmas many of us would be heading out for drinks and catch-ups with friends and preparing for parties.
But under current coronavirus rules in Wales it is illegal to meet up with too many people, in a bid to curb the spread of the virus.
Mass gatherings and house parties are banned, while only groups from four different households are allowed to meet indoors at pubs, cafes and restaurants.
Pubs, bars and restaurants, gyms, and other businesses are also allowed to reopen, but with last orders at 22:00 GMT, there have been fears people may start going to each other's homes to drink.
There are no travel restrictions in Wales, as local lockdowns have ended - but people are being urged to consider whether they need to travel before setting off.
While England remains in lockdown - until 2 December - it is against the law to travel across the border, unless you have a reasonable excuse, such as providing care or for study or work (if you cannot from home).
Under the coronavirus fixed penalty notices system, people can be fined Â£60 for a first offence, which increases to Â£120 for a second offence and continues to double for repeated offences, up to a maximum of Â£1,920.
If prosecuted, however, a court can impose an unlimited fine."
"

How much in additional taxes are you willing to pay now in order to ensure that the Earth would not be 3 degrees warmer 100 years from now (assuming the science is even possible) — $100 or $1,000 or $10,000 or more? Should the government prevent us from selling some of our body parts to allow others to live or have better lives?



Are we likely to get better health care in the future with more or less government involvement? Are the advances in information technology, such as the Internet, increasing or reducing the power of governments to monitor and control our lives? Is the current global financial crisis the result of too little or misguided government regulation of the financial industry? Does globalization increase or reduce income inequality?



The above questions and many others were the subject of learned discussion at the 60th anniversary meeting of the Mont Pelerin Society (MPS) that just concluded here in Tokyo.



The Society was established in Mont Pelerin, Switzerland, by the late economist/​philosopher and Nobel Laureate F.A. Hayek, with the objective of facilitating “an exchange of ideas between like‐​minded scholars in the hope of strengthening the principles and practice of a free society, and to study the workings, virtues, and defects of market‐​oriented economic systems.”



The Society does not seek “to create an orthodoxy, to form or align itself with any particular party or parties, or conduct propaganda,” nor does it take in the name of the Society positions on public policy issues.



Members come from many countries and include notable economists (including many Nobel prize winners) and other scholars representing the humanities, the law, and the natural sciences, as well as a few business people, high ranking government officials, and journalists. Even though the Society is not an activist organization, many of its members have gone on to create think tanks and other market‐​oriented public policy organizations across the globe.



In contrast to much of the mindless sloganeering that characterizes most political campaigns these days, the members and guests of the Mont Pelerin Society seriously discuss and debate issues with a genuine attempt to understand the costs, benefits and consequences of alternative approaches.



As an example, global warning is an issue where members have different beliefs as to how real a threat it is or is not. The Czech president and MPS member, Dr. Vaclav Klaus, presented a paper in which he argued his very well‐​known public position (he has written a book on the matter) that the science behind global warming is highly suspect, and that many of those who propose expensive solutions for what he believes is a nonproblem are self‐​interested individuals who hope to share in the government booty spent on global warming.



Others had some disagreement with his views, but engaged in a lively discussion of how much should be spent, if any, on a problem whose negative effects are likely to be experienced by future generations.



For instance, assume you believe global warming is both real and man‐​made, but you also understand that expensive actions taken now to deal with a future problem may not be cost‐​effective.



Technologies are improving rapidly so it might be far cheaper to wait until the new technologies become available before taking action. It also might be less expensive to find ways to adapt to climate change (either cooler or warmer) than try to change the climate — people in Minnesota adapt to cooler climates and do not suffer lower incomes than those in warmer Florida.



Finally, people living 100 years from now are likely to be perhaps 10 times richer than those living now (which was roughly the experience of the last 100 years in many parts of the globe). Therefore, does it make sense to tax the poor (those living today) to benefit the rich (those living 100 years from now)?



In sum, when the issue of global warming is looked at dispassionately, both those who see it as a problem and those who do not might conclude it makes sense to wait before taking any expensive action, when normal discount rates — e.g. the cost of capital — are properly taken into account.



Another issue discussed was that of global financial regulation. The common belief, at least in the press and political classes, is that the current financial crisis has stemmed from too little bank regulation. However, knowledgeable and thoughtful scholars among MPS members provided evidence — which is counterintuitive to many — showing the present international bank regulatory standards may have been the problem rather than the solution.



When good scholars and other smart people come together from many countries and professions to present evidence and discuss issues without a narrow political agenda or government sponsorship, it is surprising how often sensible and cost‐​effective solutions can be found that enhance rather than diminish human liberty. The founders of the MPS, such as F.A. Hayek and Milton Friedman, were for the most part optimists. Their vision of a freer and more prosperous future for most of mankind was realized in their lifetimes.



Yet, as they well understood, the threat of oppressive governments and ideologies is never‐​ending and thus requires people of good will to be forever vigilant for freedom and prosperity to continue.
"
"We are living in an era of intense marine urbanisation. More and more artificial structures are being built in the seas and oceans and, while many aim to combat climate change through exploiting wind or tidal energy, there are concerns over what this means for marine ecosystems. Offshore wind farms can present barriers to bird migration, while for seals, tidal turbines present an increased level of underwater noise or collision risk.   On the other hand, there are some indications that introducing man-made structures leads to new opportunities for some animals. Marine life can settle on old oil rigs, turning them into artificial reefs. Fish can seek refuge in their wake, while seals use structures as foraging sites. All of these environmental interactions need to be considered. That’s why we decided to investigate how turbulent wakes generated downstream from man-made structures can act as a form of “conveyor belt” for surface foraging seabirds, serving up a continuous stream of fish. Our results are published in the journal Communications Biology. Let’s rewind. Coastal waters are already very dynamic and complex marine habitats, characterised by headlands, islands or submerged rocks. Where strong tides rush past these features it can lead to turbulent swirls or eddies, including wakes left behind small islands, pier pillars or floating buoys.  Placing large offshore structures into such energetic environments can therefore change local flows, ranging from large sediment wakes behind offshore windfarms, to the emergence of distinct eddy-dominated wakes spanning tens to hundreds of meters behind tidal turbines. Where you see such turbulence, the chances are you see seabirds too. Seabirds scan the waters for conspicuous turbulent patches as there could indeed be something in the water. In fact, these turbulent flows can break up shoals of small fish or even displace individual fish. This can make small prey items available to surface-foraging seabirds such as gulls and terns.  Often nicknamed “sea swallows”, terns are rather slender and elegant seabirds and will only skim the surface or perform very shallow plunges to access their fishy prey. They rely on strong physical processes, such as turbulence or upwelling, to bring prey towards the surface. To understand how terns associate with both natural and man-made wake structures, we set up a study in a highly dynamic tidal channel, linking Strangford Lough in Northern Ireland with the Irish Sea. This is where the UK’s first grid-connected tidal turbine energy structure, SeaGen, was installed in 2008. At the time of our study in the summer of 2018, SeaGen was being decommissioned and its turbine blades had been removed. But the remaining structure still generated a large, turbulent downstream wake, known as a von Kármán vortex street. We focused our observations around SeaGen’s wake as well as two nearby natural dynamic sites, an island wake and a whirlpool, with the latter characterised by powerful eddies and localised up-wellings. Over the next several weeks, we then sat on the shore and counted the numbers of seabirds we saw foraging over each of the three wake features across different times of day and tidal states. We found that, on average, there were three times more terns foraging over the flood wake left by SeaGen than over the nearby natural wake sites. This suggests that the former tidal power plant acted as the most reliable conveyor belt for their prey. What makes the wake a conveyor belt? Imagine tidal flows reaching five meters a second (that is double the top speed of the Olympic swimmer Michael Phelps) rushing past SeaGen. Any small fish caught in these currents would no longer be able to swim against the tide.  To observe this we mapped out the turbulent flows of the wake from below using sound waves, and then used drones to track individual tern flight trajectories from above. Acoustic visualisations showed that SeaGen’s eddy-dominated wake mixes material across the entire water column, lifting potential small prey items towards the surface. Using drones, we could record the highly localised foraging movements of terns directly over the turbulent wake, with the incoming tide constantly replenishing the conveyor belt. The remaining SeaGen tower is set to be removed this month. It may be replaced by a new generation of tidal devices currently being tested in the lough. But in the meantime, these birds will have to stick to natural foraging sites."
"**Hundreds of residents have joined together to support independent businesses with a social media click-and-collect service.**
Bristol's Gloucester Road is famous for its long strip of independent businesses which include greengrocers, butchers and toy shops.
The Shop Local Gloucester Road Facebook group was set up by mum Alice Darley and has become a ""lifeline"".
One business owner said the group has boosted her shop during lockdown.
Miss Darley opened the Facebook group to allow others to order items from the shops that didnât already have an online service.
""When lockdown came it became clear that there must be another way to shop locally,"" she said.
âIt's been a lifeline for traders and the community.â
Laura Sharp, assistant manager of zero-waste shop Preserve, said: âOur customers feel more comfortable shopping here than large supermarkets during the pandemic because it feels safer.
âWeâve had more customers during lockdown because of the Facebook group.
âIt also definitely brings Gloucester Road together as a community.â
Amy Osborne, the manager of Dave Giles Butchers which has been trading on Gloucester Road for 30 years, said the group had become essential to the strip of businesses.
She said: âThe Facebook group is becoming a lifeline for a lot of independent business here during the lockdown.
âFor a lot of businesses here this lockdown is going to be difficult so the Facebook group is important.â
Bristol wine shop Grape and Grindâs owner, Daren Willis, closed his shop temporarily in March.
He said: âWe were able to tick along online and on Facebook whilst the shutters were down.
âThe support for Gloucester Road is extraordinary.""
_Follow BBC West on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to:_bristol@bbc.co.uk"
"
Share this...FacebookTwitter21:04 It’s there!
21:03: No earthquake so far here in Germany, no bright flashes – though we are having one heck of a shower right now – hopefully lightning will not knock out the power!
21.00 CET: Still no press release. Speculation has run wild since Friday.
20:57 CET: A few more minutes. Many of the climate science followers and bloggers are commenting over at Lucia’s Blackboard and McIntyre’s CA. McIntyre comments at LB that everyone should dial back expectations.
Share this...FacebookTwitter "
"

When I say “amyloid,” of course, almost everyone thinks of _beta‐​amyloid protein_ (also called “amyloid beta”), which accumulates as the waxy “senile plaques” that cluster around the brain cells of people with Alzheimer’s disease. 



–Aubrey de Grey, _Ending Aging: The Rejuvenation Breakthroughs  
That Could Reverse Human Aging in Our Lifetime_, p. 134



Aubrey de Grey is so deep into geek biogerontology that using “almost everyone thinks of” in the sentence quoted above does not strike him as rather generous. In reality, most of us are thinking “amyloid…amyloid…you’re talking about the singer, right? No, no…what am I saying…Wasn’t she the actress in that movie…?”



Four years ago, I reported that de Grey foresees a not‐​too‐​distant future in which humans can reverse the effects of aging, raising the possibility of living healthy lives for hundreds of years. He has not backed away from that position, and this book, written by de Grey and his research associate Michael Rae, represents an update from his perspective. In brief, he says that



As an economist, I am most interested–and most qualified to form an opinion about–the second point.



 **Accessible Metaphors**



De Grey makes state‐​of‐​the‐​art scientific issues accessible to an intelligent layman. He uses metaphors, as when he describes the role of mitichondria in terms of a power plant analogy (p. 53).



But while hydroelectric dams are (for the most part) environmentally benign, mitochondria are in one key aspect more like conventional power sources [in that they] create toxic wastes during the conversion of energy from one form to another…oxygen is also the sink for the electrons that are not fumbled–that are properly processed by the mitochondria–but that process loads _four_ electrons onto each oxygen molecule…Adding _one_ electron, by contrast, transforms benevolent oxygen into a particularly important free radical, superoxide. With your mitochondria generating ATP day and night continually, the ongoing formation of _superoxide_ is like having a constant stream of low‐​grade nuclear waste leaking out of your local reactor.



De Grey’s overarching metaphor is that the body is like a machine that, if properly maintained, can be kept running forever (p. 21).



we have hundred‐​year‐​old cars and (in Europe anyway!) thousand‐​year‐​old buildings still functioning as well as when they were built–despite the fact that they were not designed to last even a fraction of that length of time…the precedent of cars and houses gives cause for cautious optimism that aging can be postponed indefinitely by sufficiently thorough and frequent maintenance. 



However, maintenance of a car or a building often consists of replacement of components at a macro level. You replace whole tires and lightbulbs. You rip out a transmission or a kitchen and put in a new one.



What de Grey is talking about for humans is not macro replacement–giving you new organs or giving your cardiovascular system the equivalent of a transmission overhaul. Instead, he is talking about maintenance at a molecular level. For a car, it would be like having nanobots that repair corroded parts by reversing rust molecule by molecule. For a house, it would be like having shingles that when damaged by wind or wear are able to grow back to their original shape. 



**Internal Evolution at Work**



De Grey sees aging as a byproduct of an evolutionary process that takes place within the body. Mutations occur over time within your cells, sometimes randomly and sometimes stimulated by external events. This evolutionary process changes the balance between what I might call “good stuff” and “bad stuff” (here I am taking the technical caliber of the scientific exposition down several levels). Sometimes, the “good stuff” gets stronger, as when we develop an immunity to a disease. More often, however, the “good stuff” gets weaker and the “bad stuff” (like arterial plaque or pre‐​cancerous cells) gets stronger. It is this shift in the balance that leads to the symptoms of aging, including susceptibility to disease, which ultimately proves fatal.



Because aging is a natural outcome of the body’s internal evolutionary processes, de Grey argues that the standard paradigm for fighting the diseases of aging one by one is flawed. Prevention of one disease, in the form of slowing the processes that cause it, is a doomed strategy.



First, there is the fact that the processes that cause disease are the very processes that make life possible and enjoyable. The prevention paradigm amounts to lengthening the life of a car by keeping it in a dry, climate‐​controlled garage forever without ever driving it.



Second, there is the fact that if one age‐​related disease does not get you, then another one will. In my research into the causes of rising health care spending, I learned the sad truth that as we have achieved success in the battle against heart disease, we are increasing the chances that people will die of diseases like Parkinson’s or Alzheimer’s, with the result that expenditures on institutional care and full‐​time home care are soaring.



 **Only Seven Types of Damage**



De Grey argues that there are only about seven generic forms of damage, in which our body’s evolutionary processes cause it to lose “good stuff” or produce more “bad stuff” (again, those are my dumbed‐​down expressions). De Grey’s approach to reversing aging is to stimulate the body to throw out the bad stuff and grow more good stuff, maintaining a youthful balance.



For example, his approach to eradicate cancer is particularly radical. Cancer consists of “bad stuff” that has won the evolutionary battle and is now reproducing like gangbusters. He wants to make it impossible for _any_ type of cell to take over the internal ecosystem, so his proposal amounts to programming all cells to self‐​destruct after they reach an expiration date. Obviously, this creates a problem in that we need some of our cells to be able to last longer, or we will run out of “good stuff.” His solution is to use periodic stem cell implants to replenish our inventory of “good stuff.” Along the way, he makes a clear and compelling case that we need embryonic stem cell research.



(As an aside, I did not find his argument against the conventional cancer‐​fighting paradigm fully convincing. I find it more appealing to hope that there is a way to give every cell a “suicide pill” that it takes only if it recognizes that it is about to be captured by the cancer‐​enemy. Instead, killing off every cell, good or bad, and then trying to add new good cells strikes me as inelegant.



De Grey expresses the concern that when a conventional therapy goes after cancer (and the approach that appeals to me is more conventional), the laws of evolution suggest that a few cancer cells are likely to mutate and survive. Those that survive will be drug‐​resistant and therefore much more dangerous. However, I think that this is not like the mutation of germs, where a drug‐​resistant bacteria or virus can get started in one person’s body and continue to evolve somewhere else. The fact that a cancer cell evolves in my body to evade a particular drug does not make that drug any less effective in _your_ body. If your body is going to develop a drug‐​resistant form of that cancer, it is going to have to start from scratch. As a result, there may be a limited number of such mutations, and therefore we may need only a finite set of anti‐​cancer drugs. Of course, I have absolutely no expertise in this area. It is more likely that I misunderstand de Grey’s argument than that he is wrong.)



 **A Crash Course**



Too often, academics use their credentials to spit out biased polemics dressed up as science. _Ending Aging_ is the opposite. It is a crash course in state‐​of‐​the‐​art science dressed up as a polemic. De Grey wears his passion for undertaking a war on aging on his sleeve, yet most of the book consists of scientific analysis that, although simplified to enable a layman to follow, is conscientious in reporting doubts and objections to the author’s point of view.



I would recommend giving _Ending Aging_ to any scientifically‐​inclined youngster. It gives a sense of the possibilities, drama, and frustration of scientific inquiry. Also, it might inspire some young geniuses to undertake the sort of investigations and experiments that de Grey thinks will help win the war against aging.



 **New Institutions**



The polemical component of de Grey’s book is aimed primarily at the institutions and incentives that currently govern the medical research process. Some of the changes that he proposes are radical, and some are subtle.



The first institutional problem, from de Grey’s perspective, is that the incentives lead researchers to focus on specific diseases rather than on general‐​purpose technologies to fix cell damage. In scientific research, the usual distinction is between “basic” research and “applied” research. Almost everything that De Grey is talking about is in the “applied” arena. We can always use more basic research, but I think he would regard the basic research that we have today as sufficient in many respects.



The distinction between disease‐​specific and general‐​purpose fits under applied research. Within the category of applied medical research, there are discoveries that attempt to treat specific diseases, such as prostate cancer or Parkinson’s. However, the technologies that de Grey advocates developing might reverse the processes that are implicated in many diseases.



Today, the incentives to experiment with general‐​purpose anti‐​aging technologies are limited. Only if a technique can be demonstrated as helping to treat a specific disease can its development be funded and its efficacy tested in humans. Of course, many of the techniques necessary to achieve de Grey’s vision can be shoehorned into a disease‐​fighting agenda somewhere, which is why he can report results that justify his belief in the potential to conquer aging. However, there remains the fact that the current system gives too much incentive to find stopgap solutions to specific diseases and too little incentive to develop general‐​purpose anti‐​aging technologies.



The second institutional barrier is risk aversion, which is hard‐​coded into regulations pertaining to research and to clinical trials. De Grey writes (p. 323–324),



Regulation of experimental drugs and therapies…is based on one abiding principle above all others: the minimization of risk that the therapy might make the patient worse…



I take the view, quite simply, that Hippocrates has had his day…the _psychological_ effect of possibly causing harm…skews the objective cost‐​benefit analysis of a given treatment…I believe that the 10:1 (at least) ratio of lives lost through slow approval of safe drugs to lives lost through hasty approval of unsafe drugs is no longer acceptable. 



…[Laws and regulations will change.] People will die as a result; the 10:1 ratio mentioned above will probably be reduced to 2:1. And people will be happy about this change, because they’ll know it’s wartime, and the first priority–even justifying considerable loss of life in the short term–is to end the slaughter as soon as humanly possible.



What de Grey is saying is that today’s cautious approach to experimental medical testing significantly slows the rate of progress, which means that many people will suffer and die unnecessarily. However, those people are unseen and unknown, whereas those who suffer and die as a _result_ of medical experiments are identifiable and visible. I think that trying to sell people on the idea of taking more risks in order to advance medical progress is not as straightforward as de Grey makes it sound.



As an economist, I immediately think in terms of paying people to undergo risky therapies. For better or worse, this might appeal more to people who are very poor–perhaps even people living in other countries. However, those citizens who are squeamish about de Grey’s proposal to expose more people to harm now in order to reduce harm to others in the near future probably would not feel any less squeamish just because those who undergo the experiments are well paid.



At a more subtle level, de Grey wants institutional changes that wrest control of the research agenda from the medical establishment, which is vested in the existing paradigm. Here, the fact that so much medical research is under government auspices makes the outlook discouraging, in my view. If there is one thing that you can count on government to do, it is to protect incumbents and move with great reluctance to support upstarts and innovators. 



My guess is that de Grey will have better luck if he tries to mobilize wealthy philanthropists. If instead of donating buildings to universities our billionaires would donate money for prizes that reward general‐​purpose medical technologies, we might not have to wait for government research to adopt a paradigm shift, which is almost surely not going to happen. Wealthy (and not‐​so‐​wealthy) philanthropists who are reading this should check out de Grey’s organization SENS and look for ways to contribute both to his institute and to a prize fund.



Let me give de Grey the last word (p.328–329):



Just as people were wrong for centuries about how hard it was to fly but eventually cracked it, we’ve been wrong since time immemorial about how hard aging is to combat, but we’ll eventually crack it, too. But just as people have been pretty reliably correct about how to make better and better aircraft once they had the first one, we can expect to be pretty reliably correct about how to repair the damage of aging more and more comprehensively once we can do it a little.
"
"Over six decades, Sir David Attenborough’s name has become synonymous with high-quality nature documentaries. But while for his latest project, the Netflix series Our Planet, he is once again explaining incredible shots of nature and wildlife – this series is a little different from his past films. Many of his previous smash hits have portrayed the natural world as untouched and perfect, Our Planet is billed as putting the threats facing natural ecosystems front and centre to the narrative. In the opening scenes we are told: “For the first time in human history the stability of nature can no longer be taken for granted.”  This is a very significant departure – and one which is arguably long overdue. Those of us who study the pressures on wild nature have been frustrated that nature documentaries give the impression that everything is OK. Some argue that they may do more harm than good by giving viewers a sense of complacency. Conservation scientists were expecting that the new series wouldn’t shy away from the awful truth: the wonders shown in these mesmerising nature programmes are tragically reduced – and many are at risk of being lost forever. I had the privilege of seeing the One Planet team at work back in 2015 (these films take years to make). I spent three weeks at the camp in western Madagascar where they were working on their forest film. While the camera crew were working day and night filming fossa (lemur-hunting carnivores), and trying to get the perfect footage of leaf bugs producing honeydew (the series is worth watching for this sequence alone), the team was also digging deep into the complex issues of what is happening to this wondrous biodiversity. Their researcher spent many hours with Malagasy conservation scientist Rio Heriniaina talking to local community leaders about the challenges they face and the reasons for the very rapid rate of forest loss in the region. However, none of that fascinating footage made the final cut. Following a scene showing fossa mating, we are told that their forests have since been burnt. This was already happening in 2015. As Heriniaina told me:  Madagascar’s dry forests are vanishing before our eyes. Every burning season large areas of forest go up in flames to clear space for peanuts and corn. There is no simply answer to as why, and no simple solutions. Poverty plays a role but so does corruption and the influence of powerful people who profit from the destruction.  This is my main critique of Our Planet. Despite being billed as an unflinching look at the threats facing the intricate and endlessly fascinating ecosystems being depicted, it actually tends to shy away from showing these threats or, even more importantly, addressing the question of what can be done to resolve them. Like previous documentaries, shots have been carefully positioned to cut out evidence of human influence.  In my three decades of watching wildlife documentaries, I remember only one moment which broke from this tradition. In Simon Reeves’ 2012 series about the Indian Ocean, he showed people living in and around the habitats he was filming. He humanised them. He was also honest about how limited the picturesque natural habitats he was filming were. In a memorable sequence showing a sifaka leaping between trees, he asked the camera man to turn around, revealing the miles of sisal plantation which surround the tiny remnant of forest where endless crews go to film these charismatic lemurs. When Planet Earth II came out in 2016 I was disappointed to see a return to more of the same – that same remnant forest in southern Madagascar appeared, but without the context. As with previous documentaries, you could come away from Our Planet thinking the places being portrayed are completely separate from people. Human presence in and around many of these habitats has been erased. However to be successful, conservation can’t ignore people.  Maybe it is churlish to complain that Our Planet, like other such films, avoids showing the uncomfortable truth about just how threatened so much of nature really is. Perhaps the pure and unsullied vision is what makes them so popular. So many of us working in conservation were drawn in through watching Sir David Attenborough’s other films as children. By introducing viewers to fascinating facts about ecology (who knew that winds blowing across deserts feed life in the ocean?) and the mind-boggling behaviours of birds (such as the manakins shown doing a shuffle dance), Our Planet will engage a whole new generation.  Researchers have shown time and time again that knowledge isn’t enough to change people’s behaviour. However feeling connected with nature does matter. One thing the series will certainly do is make people fall in love with the planet. That is certainly a good thing. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
In bureaucracy, truth is often stranger than fiction. A non polluting electric car company gets slammed with fine for “non compliance” for a car that can’t produce any emissions.

That’s weird enough by itself, but even weirder is what else is in the company’s Securities and Exchange Commission report under what they cite as “risks”.
Here’s the relevant page of the report where they talk about risks, including the $275,000 fine from the EPA. Note what is highlighted under that. 
click to enlarge
They headline that with:
We are subject to substantial regulation, which is  evolving, and unfavorable changes or failure by us to comply with these  regulations could substantially harm our business and operating results.
That’s right, a zero emissions “green” electric car company cites this as a risk to the company’s business future:
the  imposition of a carbon tax or the introduction of a cap-and-trade  system on electric utilities could increase the cost of electricity;
You can see the Telsa SEC 10Q report for yourself at:
http://www.faqs.org/sec-filings/100813/TESLA-MOTORS-INC_10-Q/#ixzz0yDhK9ON3
Tesla’s crime? Failing to file for a 2009 emissions “Certificate of Conformity” from the EPA to comply with the “Clean Air Act.” until late in the year. Wait, I thought electric cars were supposed to help clean the air?
Damned if you do, damned if you don’t. It is a wonder that anybody would bother even trying to do business anymore where the minefield of bureaucracy looms even for popular and politically correct green companies in California.
h/t to autoblog.com

Sponsored IT training links:
The 642-374 study pack also includes 1Y0-A05 dumps and 350-018 practice exam so you will pass your certification exam on first try.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e893974a7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

It’s that time of year in North Carolina, when everyone from Bird Island to Carova Beach fears the Big One that washes away the beach house. According to the state Coastal Resources Commission, each passing year brings an even greater threat, thanks to rising sea levels.



The CRC scared everybody by predicting a median of 38 additional inches of sea‐​level rise in the next 86 years. For comparative purposes, global warming was associated with an 8‐​inch rise since 1900.



It’s enough to make people panic before the apocalypse. So sell me your beach house. Please.



I’d like to offer you bottom dollar because a) everyone thinks your house is doomed and b) because everyone is probably wrong. 



Note: Global warming is real and is caused, in part, by increasing atmospheric carbon dioxide, and a warmer world has higher sea levels.



So what? In climate, it’s not whether things change (they will, with or without our help), but how much they change.



Remember the adage, “It’s not the heat, it’s the humidity”? With regard to climate, that should be “It’s not the heat, it’s the sensitivity.”



“Sensitivity” is the amount of warming one gets for a doubling of carbon dioxide. Thanks to other greenhouse gas emissions, like methane, we’re already way past halfway there. The sensitivity is also a pretty good estimator of the change in surface temperature expected in this century.



Generally speaking, the ballpark “sensitivity” is estimated around 3.0°C (5.4°F). But the earth has been very reluctant to get with such a rapid warming program.



Global warming first burst onto the political scene on June 23, 1988, when NASA astrophysicist (and political activist) James Hansen testified that there was “a strong cause and effect relationship between the [then] current climate and human alteration of the atmosphere.” He also made a forecast, based upon two plausible scenarios for future carbon dioxide emissions. He predicted twice as much warming as has been observed.



In climate change, that’s the difference between something pretty substantial and something that is easily (and often unconsciously) adapted to. In North Carolina, there’s been an additional 10 inches of sea‐​level rise (for a total of 18 inches, since 1900) at Duck, thanks to the fact that the land there is sinking. People slowly adapted by stabilizing the dunes and building new homes on pilings.



No regulatory fiat, by the CRC or anyone else, is going to stop the coast from subsiding. But Duck is kind of an outlier. Further south, by Wilmington and Southport, the rise of the ocean is pretty much in step with the global average.



Other forecasts of warming are also too high. For example, the “midrange emissions” suite of models used by the United Nations Intergovernmental Panel on Climate Change appears to be predicting about a third more warming than we are getting.



CRC forecasts curiously use a much higher emission scenario than this, and it is clearly wrong. It was made before the discovery of hundreds of years supply of shale gas, which emits about half of the carbon dioxide of coal when used for electrical generation. Adjusting for CRC’s forecast errors (as well as for observed warming trends) yields a sea‐​level rise of about 13 inches from now to 2100.



But won’t the temperature rise accelerate and sea‐​level rise quicken?



Not if we believe the mathematical form of the Intergovernmental Panel’s midrange computer models. Contrary to talk about “ever‐​increasing warming rates,” these models in fact tend to predict a constant rate of warming. Indeed, a graph of the globe’s temperature history since the second warming of the 20th century began, over a third of a century ago, shows a remarkably constant rate of about 1.6°C (2.9°F) per century.



The CRC’s 38‐​inch projection is based upon almost three times as much warming as is being observed. For CRC to be right, the rate of warming will have to increase dramatically, but that would violate the U.N.‘s models showing a constant rate.



Which reminds me, here’s my offer for your beach house.…
"
nan
"Electric vehicles produce less carbon dioxide than petrol cars across the vast majority of the globe – contrary to the claims of some detractors, who have alleged that the CO2 emitted in the production of electricity and their manufacture outweighs the benefits. The finding is a boost to governments, including the UK, seeking to move to net zero carbon emissions, which will require a massive expansion of the electric car fleet. A similar benefit was found for electric heat pumps.  In the UK, transport is now the biggest contributor to the climate crisis and domestic heating has been stubbornly stuck on natural gas for much of the country. Across the world, passenger road vehicles and household heating generate about a quarter of all emissions from the burning of fossil fuels. That makes electric vehicles essential to reducing overall emissions, but how clean an electric vehicle is also depends on how the electricity is generated, the efficiency of the supply and the efficiency of the vehicle. That has made some individuals and governments question whether these technologies are worth expanding. The study, published on Monday in the journal Nature Sustainability, produced a decisive yes. Scientists from the universities of Exeter, Nijmegen and Cambridge conducted lifecycle assessments that showed that even where electricity generation still involves substantial amounts of fossil fuel, there was a CO2 saving over conventional cars and fossil fuel heating. They found that in 53 out of 59 regions, comprising 95% of the world, electric vehicles and domestic heat pumps generate less carbon dioxide than fossil fuel powered cars or boilers. The only exceptions are heavily coal-dependent countries such as Poland. In countries such as Sweden, which gets most of its electricity from renewable sources, and France, which is largely powered by nuclear, the CO2 savings from using electric cars reach as high as 70% over their conventional counterparts. In the UK, the savings are about 30%. However, that is likely to improve further as electric vehicles grow even more efficient and more CO2 is taken out of the electricity generating system. Heat pumps use electricity and heat exchange systems – similar in principle to those found in fridges – to take advantage of the difference in temperature underground and at the surface, in the case of ground source heat pumps, and between the outdoor air and indoors in the case of air source heat pumps. If they were widely used, the study found, they could reduce global carbon emissions by up to 0.8 gigatons a year by 2050, or the equivalent of Germany’s emissions today. “The idea that electric vehicles or heat pumps could increase emissions is essentially a myth,” said Florian Knobloch of Nijmegen University in the Netherlands, the lead author of the study. “We’ve seen a lot of disinformation going around. Here is a definitive study that can dispel those myths.” Jean-Francois Mercure, of Exeter University, a co-author of the study, added: “The answer is clear: to reduce carbon emissions, we should choose electric cars and household heat pumps over fossil fuel alternatives.” Among the detractors has been Bjørn Lomborg, the climate controversialist, who argued in a column published in newspapers around the world last week that electric cars were “simply expensive gadgets heavily subsidised for the wealthy to feel good while doing very little for the planet”. Mike Childs, head of science at Friends of the Earth, said: “Electric vehicles and heat pumps are absolutely critical for meeting climate goals so it’s good to see this favourable report. In the UK, both technologies will continue to make big carbon savings alongside our switch from fossil fuels to renewable energy to power the electricity grid.” But he warned that insulating homes and improving public transport remained important goals, alongside electric vehicles and heat pumps, and called for much more government action to realise the benefits. “Where the UK is dragging its feet is supporting the necessary rapid rollout of electric cars and heat pumps as well as the infrastructure to support them,” he said."
"I was invited to speak to a group of teenagers on climate strike in Oxford recently. Like many scientists, I support the strikes, but also find them disturbing. Which I’m sure is the idea.  Today’s teenagers are absolutely right to be up in arms about climate change, and right that they need powerful images to grab people’s attention. Yet some of the slogans being bandied around are genuinely frightening: a colleague recently told me of her 11-year-old coming home in tears after being told that, because of climate change, human civilisation might not survive for her to have children. The problem is, as soon as scientists speak out against environmental slogans, our words are seized upon by a dwindling band of the usual suspects to dismiss the entire issue. So if I were addressing teenagers on strike, or young people involved in Extinction Rebellion and other groups, or indeed anyone who genuinely wants to understand what is going on, here’s what I’d say. My biggest concern is with the much-touted line that “the Intergovernmental Panel on Climate Change (IPCC) says we have 12 years” before triggering an irreversible slide into climate chaos. Slogan writers are vague on whether they mean climate chaos will happen after 12 years, or if we have 12 years to avert it. But both are misleading. As the relevant lead author of the IPCC Special Report on Global Warming of 1.5°C, I spent several days last October, literally under a spotlight, explaining to delegates of the world’s governments what we could, and could not, say about how close we are to that level of warming. Using the World Meteorological Organisation’s definition of global average surface temperature, and the late 19th century to represent its pre-industrial level (yes, all these definitions matter), we just passed 1°C and are warming at more than 0.2°C per decade, which would take us to 1.5°C around 2040. That said, these are only best estimates. We might already be at 1.2°C, and warming at 0.25°C per decade – well within the range of uncertainty. That would indeed get us to 1.5°C by 2030: 12 years from 2018. But an additional quarter of a degree of warming, more-or-less what has happened since the 1990s, is not going to feel like Armageddon to the vast majority of today’s striking teenagers (the striving taxpayers of 2030). And what will they think then? I say the majority, because there will be unfortunate exceptions. One of the most insidious myths about climate change is the pretence that we are all in it together. People ask me whether I’m kept awake at night by the prospect of five degrees of warming. I don’t think we’ll make it to five degrees. I’m far more worried about geopolitical breakdown as the injustices of climate change emerge as we steam from two to three degrees. So please stop saying something globally bad is going to happen in 2030. Bad stuff is already happening and every half a degree of warming matters, but the IPCC does not draw a “planetary boundary” at 1.5°C beyond which lie climate dragons. What about the other interpretation of the IPCC’s 12 years: that we have 12 years to act? What our report said was, in scenarios with a one-in-two to two-in-three chance of keeping global warming below 1.5°C, emissions are reduced to around half their present level by 2030. That doesn’t mean we have 12 years to act: it means we have to act now, and even if we do, success is not guaranteed. And if we don’t halve emissions by 2030, will we have lost the battle and just have to hunker down and survive? Of course not. The IPCC is clear that, even reducing emissions as fast as possible, we can barely keep temperatures below 1.5°C. So every year that goes by in which we aren’t reducing emissions is another 40 billion tonnes of CO₂ that we are expecting today’s teenagers to clean back out of the atmosphere in order to preserve warm water corals or Arctic ice. Assuming people will still want to feed themselves and not turn the world over to biofuels, then scrubbing CO₂ out of the atmosphere currently costs £150-£500 per tonne, plus the cost of permanent disposal. So those 40 billion tonnes of CO₂ represent a clean-up liability accumulating at a cool £8 trillion per year, which is more or less what the world currently spends on energy. So here is a conversation young activists could have with their parents: first work out what the parents’ CO₂ emissions were last year (there are various carbon calculators online – and the average is about seven tonnes of fossil CO₂ per person in Europe). Then multiply by £200 per tonne of CO₂, and suggest the parents pop that amount into a trust fund in case their kids have to clean up after them in the 2040s. If the parents reply, “don’t worry, dear, that’s what we pay taxes for”, youngsters should ask them who they voted for in the last election and whether spending their taxes on solving climate change featured prominently in that party’s manifesto. Get angry by all means, but get angry for the right reasons. Action is long overdue, but to a British public sunbathing in February, weird though that was, it doesn’t feel like an emergency. Middle-aged critics would much rather quibble over the scale of climate impacts (as if they have any right to say what climate young people should have to put up with) than talk about the clean-up bill.  Climate change is not so much an emergency as a festering injustice. Your ancestors did not end slavery by declaring an emergency and dreaming up artificial boundaries on “tolerable” slave numbers. They called it out for what it was: a spectacularly profitable industry, the basis of much prosperity at the time, founded on a fundamental injustice. It’s time to do the same on climate change. Read more: School climate strikes: why adults no longer have the right to object to their children taking radical action or view this years’ Great Debate on Planetary Boundaries and 1.5°C at the European Geophysical Union. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Earl looks to have a path grazing the East Coast of the USA. Cape Hatteras, Long Island, and Cape Cod may be in a portion of the projected path.

Satellite image follows along with a recent bulletin. 

Animate the sat loop – click here
BULLETIN

HURRICANE EARL INTERMEDIATE ADVISORY NUMBER  27A

NWS TPC/NATIONAL HURRICANE CENTER MIAMI FL   AL072010

200 AM AST WED SEP 01 2010

...POWERFUL HURRICANE EARL BEGINNING TO MOVE AWAY FROM THE TURKS AND

CAICOS ISLANDS...

SUMMARY OF 200 AM AST...0600 UTC...INFORMATION

----------------------------------------------

LOCATION...23.5N 70.7W

ABOUT 145 MI...235 KM NNE OF GRAND TURK ISLAND

ABOUT 860 MI...1385 KM SSE OF CAPE HATTERAS NORTH CAROLINA

MAXIMUM SUSTAINED WINDS...135 MPH...215 KM/HR

PRESENT MOVEMENT...NW OR 315 DEGREES AT 15 MPH...24 KM/HR

MINIMUM CENTRAL PRESSURE...942 MB...27.82 INCHES

WATCHES AND WARNINGS

--------------------

CHANGES WITH THIS ADVISORY...

NONE.

SUMMARY OF WATCHES AND WARNINGS IN EFFECT...

A HURRICANE WATCH IS IN EFFECT FOR...

* NORTH OF SURF CITY NORTH CAROLINA TO THE NORTH CAROLINA/VIRGINIA

BORDER...INCLUDING THE PAMLICO AND ALBEMARLE SOUNDS

A TROPICAL STORM WARNING IS IN EFFECT FOR...

* TURKS AND CAICOS ISLANDS

A TROPICAL STORM WATCH IS IN EFFECT FOR...

* SOUTHEASTERN BAHAMAS

* CAPE FEAR TO SURF CITY

A HURRICANE WATCH MEANS THAT HURRICANE CONDITIONS ARE POSSIBLE

WITHIN THE WATCH AREA.  A WATCH IS TYPICALLY ISSUED 48 HOURS

BEFORE THE ANTICIPATED FIRST OCCURRENCE OF TROPICAL-STORM-FORCE

WINDS...CONDITIONS THAT MAKE OUTSIDE PREPARATIONS DIFFICULT OR

DANGEROUS.

INTERESTS FROM VIRGINIA NORTHWARD TO NEW ENGLAND SHOULD MONITOR

THE PROGRESS OF EARL.

FOR STORM INFORMATION SPECIFIC TO YOUR AREA IN THE UNITED

STATES...INCLUDING POSSIBLE INLAND WATCHES AND WARNINGS...PLEASE

MONITOR PRODUCTS ISSUED BY YOUR LOCAL NATIONAL WEATHER SERVICE

FORECAST OFFICE. FOR STORM INFORMATION SPECIFIC TO YOUR AREA OUTSIDE

THE UNITED STATES...PLEASE MONITOR PRODUCTS ISSUED BY YOUR NATIONAL

METEOROLOGICAL SERVICE.

DISCUSSION AND 48-HOUR OUTLOOK

------------------------------

AT 200 AM AST...0600 UTC...THE CENTER OF HURRICANE EARL WAS LOCATED

NEAR LATITUDE 23.5 NORTH...LONGITUDE 70.7 WEST.  EARL IS MOVING

TOWARD THE NORTHWEST NEAR 15 MPH...24 KM/HR.  THIS GENERAL MOTION IS

EXPECTED TO CONTINUE ON WEDNESDAY WITH A GRADUAL TURN TO THE

NORTH-NORTHWEST THEREAFTER.  ON THE FORECAST TRACK...THE CORE OF

THE HURRICANE WILL BE PASSING WELL EAST AND NORTHEAST OF THE TURKS

AND CAICOS ISLANDS TONIGHT AND NORTHEAST OF THE BAHAMAS TOMORROW.

MAXIMUM SUSTAINED WINDS ARE NEAR 135 MPH...215 KM/HR...WITH HIGHER

GUSTS.  EARL IS A CATEGORY FOUR HURRICANE ON THE SAFFIR-SIMPSON

HURRICANE WIND SCALE.  LITTLE CHANGE IN STRENGTH IS EXPECTED

THROUGH WEDNESDAY.

EARL IS A LARGE HURRICANE.  HURRICANE FORCE WINDS EXTEND OUTWARD UP

TO 90 MILES...150 KM...FROM THE CENTER...AND TROPICAL STORM FORCE

WINDS EXTEND OUTWARD UP TO 200 MILES...325 KM.  NOAA BUOY 41046

RECENTLY REPORTED SUSTAINED WINDS OF 72 MPH...115 KM/HR...WITH

GUSTS TO 85 MPH...137 KM/HR.

THE MINIMUM CENTRAL PRESSURE JUST REPORTED BY AN AIR FORCE RESERVE

HURRICANE HUNTER AIRCRAFT IS 942 MB...27.82 INCHES.

HAZARDS AFFECTING LAND

----------------------

WINDS...TROPICAL STORM CONDITIONS ARE PROBABLY OCCURRING IN THE

VICINITY OF THE TURKS AND CAICOS ISLANDS.  WEATHER CONDITIONS WILL

LIKELY IMPROVE IN THESE ISLANDS TODAY.

STORM SURGE...ABOVE NORMAL TIDES...ACCOMPANIED BY LARGE AND

DANGEROUS BATTERING WAVES...ARE POSSIBLE IN THE TURKS AND CAICOS

ISLANDS AND THE SOUTHEASTERN BAHAMAS THIS MORNING.

RAINFALL...RAINFALL ACCUMULATIONS OF 1 TO 3 INCHES...WITH ISOLATED

MAXIMUM AMOUNTS OF 6 INCHES ARE EXPECTED FOR THE SOUTHEASTERN

BAHAMAS AND FOR THE TURK AND CAICOS ISLANDS.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e890352b2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The hunger riots arising from soaring food prices are a terrible human drama. The world price of wheat has nearly tripled in three years, and doubled in the last year; the price of rice has increased by more than 50 per cent in three months.



The cost of a meal is 40 per cent higher than one year ago in many poor countries. When food purchases can represent 75 per cent of a household’s budget, those price increases become a nightmare.



This could mean the return of millions under the poverty threshold, wiping out several years’ development efforts.



What are the reasons for this situation? Given that the economy is made of complex interconnections, we shall seek the real reasons for the crisis in more depth than is usually done.



The crisis can be interpreted as the unintended consequence — the perverse effect — of several policies which prevent many decision‐​makers from acting responsibly.



First though, let us recall that each year millions can pull themselves out of poverty — thanks to the integration of their economies into globalisation. This is clearly good news but has induced an increasing demand for food, and especially for meat (and cattle needs cereal).



Now we need to understand why food supply does not follow demand. While we do need to take into account climate hazards (like drought in Australia), we mainly need to focus on the direct or indirect political elements that prevent markets from adjusting quantities and maintaining relatively stable prices where possible.



Subsidies in rich countries are sometimes rightly pointed at. They generate two types of effects. First, subsidies divert producers toward subsidised crops.



And when governments subsidise crops for biofuel production, it means less food crops and higher cereal prices.



The second effect is that subsidies combined with tariffs in rich countries close the door to poor countries, preventing them from competing on the markets of rich countries and developing their competitive advantage.



However, it should also be remembered that protectionism amongst African countries especially, is stifling their economies by reducing the possibilities of having productivity‐​enhancing regional markets. Protectionist policies are thus partly responsible here.



Institutional arrangements for agricultural land property are fundamental to the understanding of food problems today. In Africa, the main problem of agriculture is that poorly‐​defined property rights over the land are disincentives for investment.



The major challenge of development policy is to foster agricultural expansion in some countries by allowing institutional arrangements — integrated to local traditions — enabling the responsibility of “farming entrepreneurs.” Only with such incentives will the supply of agricultural production really emerge in some countries.



Interventionist policies from the World Bank and the IMF brought nearly no positive results compared to the massive amounts swallowed up in the aid process. However, they made several poor economies “sustainably dependent.”



Huge debts were created from counterproductive “Big push” utopian programmes, logically followed by structural adjustment programmes. Hence the obligation in aided countries to sometimes develop non‐​food export crops such as cotton in order to reimburse loans. These aid policies thus disturbed the food security of some poor countries.



But they also enriched and maintained irresponsible or corrupted government cliques. International agencies are still lending them money whilst they are oppressing their peoples and preventing them from enjoying economic freedom.



The recent financial crisis was somewhat deflected on the food market, as rice and corn are now investment shelters for speculators. We therefore must seek an indirect explanation of the food crisis behind the financial crisis. What we find are American banks granting too many risky loans. But few people realise that this is strongly linked to the Federal Reserve policy.



First, the Fed kept interest rates low for far too long in the early 2000’s, artificially feeding the housing boom. Second, what is often called the “Greenspan doctrine” produced at least one major perverse effect: that the Fed in a way “insures” losers (banks) who deliberately take risks.



This policy of wrong incentives led to banks acting irresponsibly and eventually degenerated into a global crisis. There is a striking parallel here with the behaviour of the IMF and World Bank: top organisations render other organisations under their control simply irresponsible. However, States and markets work efficiently only if the responsibility of decision‐​makers is sanctioned.



Finally, the oil price surge has an impact on transportation costs, and thus on the final price of food. The increase in oil prices is explained by an increase in demand at the world level with a supply which is essentially a rigid cartel that has not invested to adapt its production.



Besides, oil also represents an investment shelter for speculators in this period of crisis: the price increase can thus partially and indirectly be attributed to the people responsible for the financial crisis.



Emergency aid is necessary to relieve people from disaster. However, after treating the symptom, we will have to deal with the deep structural causes of these disequilibria.



Yet, “responsible” representatives of various organisations and States do not seem to hold a responsible discourse.



We heard references to the New Deal, to lasting supplementary aid by the president of the World Bank, to new market distortions, and even to lasting European protectionism by the French Agriculture Minister.



These are bad omens. This crisis should be an opportunity to promote the idea of responsibility in the system and to rethink development policies in the widest sense. This means changing attitudes within international aid agencies, governments in poor countries, and, of course, in wealthy countries as well. 
"
"So which Easter tradition came first? The packaging or the egg? The answer is of course not that surprising (it’s the egg). The tradition of giving people eggs at spring time has roots in ancient pagan festivals and exists in the history of a range of religions.  It is only in recent decades that the amount of packaging around a hollow chocolate egg has become a noticeable problem – partly because of a rise in the number of eggs sold. It’s true that some manufactures have made progress in reducing packaging, with a big focus on reducing plastics. Many popular eggs are wrapped in just a layer of foil and a card box (plus any wrappers that come on accompanying confectionary). But this does not mean the problem has gone away.  A report by Which? revealed that around a quarter of the total weight of Easter eggs sold in the UK is taken up by the plastic and cardboard packaging they are wrapped up in. The outer packaging of one of the top-ten selling brands tipped the scales at 152g of a 418g product (36.4%).  According to the environmental charity Friends of the Earth, Easter egg makers are still failing when it comes to plastic waste. This leads to some 3,000 tonnes of packaging waste each year. But it is too easy to blame the manufacturer – after all, we buy the eggs. 


      Read more:
      How to defuse the Easter egg 'arms race'


 And the packaging does play some role in protecting the chocolate from damage and contamination – otherwise you may end up with food waste (which is actually far worse).  We appear to be at a stalemate – manufactures do not want to change the big, bright packaging in fear of losing sales. Customers still want to present their friend or relative with a pristine, attractive, traditional gift.  So how can food providers and consumers help to reduce packaging waste? 
Here are a few options (although some may not be so sweet). Make the eggs flat. A two dimensional egg can be packaged far more easily and is less prone to damage than a 3D egg which requires additional packaging to protect those thin chocolate walls around a hollow space. Flat eggs could be made just as attractive and would certainly taste the same. They would also improve logistics efficiency by not having to transport so much air.  “Build your own” Easter egg kits - packs could include everything you need to produce a bespoke egg (including two egg halves) for your loved one. There would be no need for plastic packaging and you would be giving a personalised, hand-crafted gift. Opt for cardboard and items wrapped in packaging that can be recycled – such as cardboard and foil. Typically, it is the more luxurious brands that want to show off their extravagant produce in-store who still use large amounts of plastic.  Avoid getting drawn in by the additional items or “gifts” that may come with eggs. These are the kind of gifts that nobody really wants, such as a low-quality mug or plastic toy – and the negative environmental impact of producing those could be much greater than that of the chocolate egg and packaging combined. And you will pay a premium for them.  Ignore chocolate this Easter and opt for something more meaningful. Regardless of your religion (or lack of), Easter is about new life, not new waistlines. Bake or make something (egg shaped if you like) that your family will really like and will mean much more to them than manufactured chocolate. And which the planet will thank you for too. Whichever way you choose to cut down on packaging this Easter, remember that this is just one of many ways you can reduce your household waste. The world it seems is in the midst of a packaging crisis. Together we can (ahem) crack it."
"When Chitty Chitty Bang Bang was released 50 years ago, flying cars were a flight of fancy. Now, these futuristic vehicles are entering the outer fringes of reality. According to a new study published in Nature, for some journeys flying cars could eventually be greener than even electric road cars, cutting emissions while also reducing traffic on increasingly busy roads. However, gaps in necessary technology and practical uncertainties beyond the cars’ promising physics mean that they may not arrive in time to be a large-scale solution to the energy crisis and congestion – if at all. It might at first seem crazy that a flying car could be more efficient than a road car, especially when conventional planes have such a reputation as gas guzzlers.  But flying isn’t inherently inefficient – after all, birds can fly between continents without eating. Of course, a small, four-passenger car isn’t an albatross, but it isn’t a Boeing 737 either. There are many ways to make a car fly, but most are too problematic to get off the ground. Perhaps the most promising option is that taken in this study, based on the physics of vertical take-off and landing (VTOL) aircraft. They’re pretty amazing beasts. If you’ve heard of VTOL, something like a Harrier Jump Jet probably springs to mind, with two huge engines directing thrust that can be tilted vertically or horizontally. But these much smaller and lighter flying cars operate differently, with lots of tiny electric fans blowing air from many places. This fast-developing distributed electric propulsion (DEP) technology is key for efficiency when cruising, and it also creates possibilities for quieter take-off and hovering, as multiple small noise sources can be better managed. Wing and propeller design can also be optimised to be long, thin, and have lots of moving surfaces, just as birds do to make their flying efficient. The aim of all of these technical enhancements is to achieve maximum lift for minimum drag – the force that opposes an object’s motion through air and slows it down. A better lift-to-drag ratio means lower power consumption, and therefore lower emissions. These energy-saving innovations make cruising a breeze – but they don’t help much with take-off, hovering, or landing, which are still inherently inefficient. So while VTOL flying vehicles are still viable for short intra-city travel and pizza deliveries, they will not solve the energy crisis. For 100km journeys, electric flying vehicles could be 35% more efficient than a petrol-powered car – although, assuming the same number of passengers, still less efficient than an electric road car. However, it’s fair to assume that flying cars will serve primarily as taxi services in pre-defined air corridors, and are therefore likely to consistently carry more people. Taking this into account, for a 100km journey flying car emissions could be 6% less than those of electric road cars. As journey distance increases, so too do the efficiency gains over stop-start road cars, which have to deal with rolling resistance and less efficient airflow. But unfortunately, range is the Achilles heel for electric aviation.  The study looks at a range of up to about 200km and here flying cars could perform well. But while jet-fuelled planes can lose as much as 70% of their weight during flight (albeit at a cost of 100kg of CO₂ per passenger per hour), batteries don’t get lighter as they discharge. This means that beyond 200km or so, carrying batteries becomes a distinct disadvantage. The accepted view is that electric planes will only ever be viable for short-haul flights. It’s energy density that matters, measured in watt-hours per kilogram. Right now, the best batteries provide around 250 W-h/kg, a mere shadow of jet fuel and gasoline’s 12,000 W-h/kg. Batteries could creep up to 800 W-h/kg by the middle of this century, increasing their feasible range to 700 miles – half of all global flights fall within this distance. But without more dramatic innovation in battery technology, biofuels and liquid fuel from air-capture of CO₂ will likely need to play a substantial role in long-haul air travel. In focusing entirely on the physics of flying cars, the paper steers clear of a number of practicalities that must be considered before we embrace VTOL flying cars as a sustainable form of transport for the future. For example, it is important to consider the carbon costs of production, maintenance and down time, known as Life-Cycle Analysis (LCA). Electric vehicles have been criticised for both the energy and environmental costs of mining primary materials for batteries, such as lithium and cobalt. Added infrastructure required for flight may worsen the problem for flying cars. And of course, a grid powered by low-carbon sources is essential to make battery-powered vehicles part of the solution to our climate crisis. Aircraft also have highly stringent criteria for maintenance and downtime, which can often offset gains in performance and emissions. As an entirely new breed of planes, it’s impossible to predict how much it might cost to keep them air-worthy. Unforeseen maintenance complications can cost billions – just ask Boeing.   Finally, weather matters. A tailwind of 35mph reduces power use and emissions by 15%, but a 35mph headwind increases them by 25%. Having to carry heavy extra batteries to avoid the potential catastrophe of running out of charge before encountering a suitable landing place could offset emissions savings. Road cars, by contrast, can easily pull over to the side of the road when needed, without consequence. So when it comes down to CO₂ emissions per passenger kilometre, at present these advanced DEP flying cars are at best comparable to their road-going electric equivalents, and, at worst, little better than conventional combustion cars. With technology and safety improvements, they could yet play a part in our fossil-fuel-free future, taking short-haul planes out of our skies and freeing up fume-filled roads. The question on everyones’ lips is whether these flying cars will be ready in time to make a jot of difference to our very pressing energy crisis. Can we wait 30 years? Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"The coronavirus pandemic is shutting down industrial activity and temporarily slashing air pollution levels around the world, satellite imagery from the European Space Agency shows. One expert said the sudden shift represented the “largest scale experiment ever” in terms of the reduction of industrial emissions. Readings from ESA’s Sentinel-5P satellite show that over the past six weeks, levels of nitrogen dioxide (NO2) over cities and industrial clusters in Asia and Europe were markedly lower than in the same period last year. Nitrogen dioxide is produced from car engines, power plants and other industrial processes and is thought to exacerbate respiratory illnesses such as asthma. While not a greenhouse gas itself, the pollutant originates from the same activities and industrial sectors that are responsible for a large share of the world’s carbon emissions and that drive global heating. Paul Monks, professor of air pollution at the University of Leicester, predicted there will be important lessons to learn. “We are now, inadvertently, conducting the largest-scale experiment ever seen,” he said. “Are we looking at what we might see in the future if we can move to a low-carbon economy? Not to denigrate the loss of life, but this might give us some hope from something terrible. To see what can be achieved.” Monks, the former chair of the UK government’s science advisory committee on air quality, said that a reduction in air pollution could bring some health benefits, though they were unlikely to offset loss of life from the disease. “It seems entirely probable that a reduction in air pollution will be beneficial to people in susceptible categories, for example some asthma sufferers,” he said. “It could reduce the spread of disease. A high level of air pollution exacerbates viral uptake because it inflames and lowers immunity.” Agriculture could also get a boost because pollution stunts plant growth, he added. The World Health Organization describes NO2 as “a toxic gas which causes significant inflammation of the airways” at concentrations above 200 micrograms per cubic metre. Pollution particles may also be a vector for pathogens, as well as exacerbating existing health problems. The WHO is now investigating whether airborne pollution particles may be a vector that spreads Covid-19 and makes it more virulent. One of the largest drops in pollution levels could be seen over the city of Wuhan, in central China, which was put under a strict lockdown in late January. The city of 11 million people serves as a major transportation hub and is home to hundreds of factories supplying car parts and other hardware to global supply chains. According to Nasa, nitrogen dioxide levels across eastern and central China have been 10-30% lower than normal. NO2 levels also dropped in South Korea, which has long struggled with high emissions from its large fleet of coal-fired power plants but also from nearby industrial facilities in China. The country has avoided putting entire regions under lockdown but is meticulously tracing and isolating suspected coronavirus cases. The changes over northern Italy are particularly striking because smoke from a dense cluster of factories tends to get trapped against the Alps at the end of the Po Valley, making this one of western Europe’s pollution hotspots. Since the country went into lockdown on 9 March, NO2 levels in Milan and other parts of northern Italy have fallen by about 40%. “It’s quite unprecedented,” said Vincent-Henri Peuch, director of the Copernicus Atmosphere Service. “In the past, we have seen big variations for a day or so because of weather. But no signal on emissions that has lasted so long.” The source is not yet clear. One possibility is a slowdown of activity in Italy’s industrial heartland. Another factor is likely to be a reduction in road traffic, which accounts for the biggest share of nitrogen dioxide emissions in Europe. Peuch said satellites were now starting to pick up similar signals in other European cities that are entering into lockdowns, though the data needs to studied over over a longer period to confirm this is a pattern. Although the UK is more than a week behind Italy in terms of the spread of the disease and the government’s response, roadside monitors already show significantly reduced levels of pollution at hotspots such as Marylebone in London. Road traffic accounts for about 80% of nitrogen oxide emissions in the UK, according to Monk. For the average diesel car, each kilometre not driven avoids 52 milligrammes of the pollutant entering the air. “What I think will come out of this is a realisation - because we are forced to - that there is considerable potential to change working practices and lifestyles. This challenges us in the future to think, do we really need to drive our car there or burn fuel for that,” said Monk."
"Everybody seems to be talking about climate change again. This time, a great deal of the coverage has been sympathetic to the idea that we are facing an emergency that demands drastic action.  Extinction Rebellion’s protests caused some outrage, but also some surprising support. Swedish campaigner Greta Thunburg has been widely admired, David Attenborough has been spreading the word with urgency, and primetime programming has led to serious discussions about climate change across living rooms, offices and social media.  So is this the fabled tipping point in public opinion which will see widespread support for radical changes? That is a question that can only be answered in hindsight.  Yet despite the significant surge in interest and concern, most people are probably unaware of what climate change really means: that it’s not just about nudging our emissions a bit lower or taking incremental action generally. This is a challenge that is perhaps unprecedented in all of human history.  Given that I teach climate change to university students, I can (and do) talk for hours about the importance of global temperature change, or ecological impacts.  But these are academic concerns in the sense that they are almost completely separated from what climate change means to me, my family, friends and pretty much everything else I care about. It’s taken me some time to realise that I was in a sort of denial about climate change. I was able to compartmentalise it.  Reflecting on this led me to take a step over the line that separates academia from activism. I have colleagues and friends who are strict observers of this separation of states. Some of them have deeply principled concerns that advocating for particular climate related policies could undermine their professional objectivity.  Others have little desire to be the subject of the online abuse which often comes with sticking your head above the parapet and into the public debate.  I had these same reservations. But over time they have been gradually worn down by the steady drum beat of bad news and insufficient action. My personal tipping point was an otherwise unremarkable lecture to one of my undergraduate classes.  I was discussing atmospheric concentrations of carbon dioxide in the atmosphere over time, and pointed out that this has been increasing ever since they were born. On each one of their birthdays, there was more CO₂ in the atmosphere than on the same day the previous year. Every additional birthday cake candle celebrated another one, two, or even three per cent annual increase.  As I spoke, I looked into the faces of a generation that had been completely failed by their predecessors. It is a failure which came despite two decades of the science being perfectly clear that increasing CO₂ concentrations would produce further warming, and that dangerous changes to the global climate were lurking.  That was when I realised that the positive professional and personal changes I had managed to make were hopelessly inadequate. Yes, I avoided flying where possible, and yes, I had largely eliminated meat and dairy from my diet.  I cycle rather than drive. I had switched to a green energy supplier. All that was good. All that was important. But I keenly felt the need to do more.  So I decided to make a documentary about climate change – about what drives it and what we can do individually, and together, to ensure a stable natural world for our children and future generations.  Why a film? It was a chat with a good friend, film maker Paul Maple of Global Documentary, about our shared frustrations over the lack of climate change programmes being broadcast which led to plans to make our own.  I had no idea what would be involved, and Paul didn’t tell me – perhaps from fear of scaring me off. That was over three years, a thousand miles of travel around the UK, terabytes of data, and countless coffee-fuelled hours in the editing suite, ago.  All of that work has now been rendered down to the 39 minutes of The Race Is On: Secrets and Solutions of Climate Change. In making the film, we were extremely fortunate to be able to interview leading figures in climate change science, economics and activism. I wouldn’t be able to name them all here without also naming the 67 people who contributed to the crowdfunding of the project and so help turn our initial sketchy plans into reality.  Early on, we agreed that a film, no matter how slick, could only be one part of an engagement strategy. So we planned community screenings, in which the film would be followed by panel discussions and town hall style meetings. We also produced a companion website containing information on practical steps we can all take to reduce our climate impacts.  The journey from academic to film maker activist is not something I can unreservedly recommend. I’ve had to park aspects of my professional and personal life, given how all consuming the project was. And now I seem to have taken up a new role as distributor and promoter, as the film will have no value unless people watch it.  But while I hope that this will be more than offset by generating positive impact, it’s also true that on a personal level it’s been worth it. I’ve met some incredible people, been allowed to go places and do things that otherwise would have been out of bounds (it’s amazing what you can get away with when accompanied by a film crew), and learnt new skills that have helped both my teaching and research.  The film project has been a labour of love. At times, a stress test, and finally a ragged race to deadlines – so something like a microcosm of the civilisation-scale climate challenge we all now face.  Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
nan
"**The number of cases in a Covid cluster linked to an Aberdeenshire food plant has risen to 86.**
NHS Grampian said a further eight cases were now associated with the Kepak McIntosh Donald plant in Portlethen.
An incident management team has been set up to monitor the situation involving the premises.
More than 200 workers at the site took up the offer of testing after the first cases were detected.
NHS Grampian previously said that given the large numbers of people being tested, it expected the number of confirmed infections to rise.
In a statement on Thursday the health board said: ""All are being supported to self-isolate as required.
""We continue to work closely with Kepak McIntosh Donald and are very grateful for their co-operation."""
"
Share this...FacebookTwitterChris Horner posted a video on Facebook a few a hours ago where he reviews and brings us up to date on the efforts by the University of Virginia faculty to defy the law of the land by refusing to release taxpayer-owned documents surrounding Climategate.

There must be some something really embarrassing that needs to stay hidden, no matter the cost, as Mann and the faculty appear awfully desperate in their attempts to defy the Freedom of Information Act. Chris Horner states at the 12:18 mark:
Those people who don’t like us asking for the records have a problem with the law, and they have a problem with the University of Virginia complying.”
Mann and the faculty are applying a pressure campaign against the University in order to keep the public records under wraps. Horner:
The UVA said to us that they find themselves in a very difficult position with their faculty. And we pointed out to them that this difficult position they find themselves in is between their faculty and the law.”
And judging from Steve McIntyre’s latest post, Mann’s desperation is visible throughout his book, and reveals a person on the verge of losing it. Whenever you start getting paranoid ideas that the fossil fuel industry is and is out to get you by orchestrating a few blog sites, then you really have got to wonder. Not only has Mann run out of lies, but he is resorting to re-lying, as McIntyre points out. Mann s becoming a serious embarrassment to Jefferson’s university.
 
Share this...FacebookTwitter "
"
No Swearing sign on Atlantic Avenue, Virginia Beach, VA Photo: Steph Doyle
Gosh, I try to keep a semblance of decorum here at WUWT. I get upset when name calling starts and moderators are trained to clamp down on this sort of thing. That being said, can you imagine the caterwauling that would ensue if I wrote something like this piece below?
Andrew Revkin and I disagree on climate, but we maintain what I deem to be a civil, professional tone when we correspond. That’s how it should be. Foul language isn’t needed to get points across.
Joe Romm at Climate progress just showed his true colors by not only allowing such foul behavior, but actually encouraging it in the form of a guest post that he edited. I don’t buy Romm’s excuse that he was trying to “show some of the real anger over Revkins column”.
In my view, profanity is the last refuge of the disingenuously desperate.
Warning – foul language follows

Here’s the guest piece from Climate Progress, the last few paragraphs follow:
So, here’s a challenge for Andy Revkin: Do not write another word about climate science until you have spent one whole month as a visitor in a climate research institute. Attend the seminars, talk   to the PhD students, sit in on meetings, find out what actually goes  on  in these places. If you can’t be bothered to do that, then please shut the fuck up.
Update: On reflection, I think I was too generous to Revkin when I   accused him of making shit up, so I deleted that bit. He’s really just   parroting other people who make shit up.
Update #2: Oh, did I mention that I’m a computer scientist? I’ve   been welcomed into various climate research labs, invited to sit in on   meetings and observe their working practices, and to spend my time   hanging out with all sorts of scientists from all sorts of disciplines.   Because obviously they’re a bunch of tribalists who are trying to hide   what they do. NOT.
– Steve Easterbrook
=================================================
Gosh.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e899a4ec3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Madame Chairman, Distinguished Members of the Committee:



My name is Michael Tanner and I appreciate the invitation to appear today and the opportunity to share my perspective on the vital issue of reforming health care and what Wisconsin should and should not do to help resolve this issue.



For the past 14 years, I have been director of health & welfare studies for the Cato Institute in Washington, DC. Before that I served as legislative director for the Georgia Public Policy Foundation and as legislative director for health & welfare with the American Legislative Exchange Council. In all, I have spent more than 20 years studying the American health care system and am the author of five books on health care reform, most recently _Healthy Competition: What’s Holding Back American Health Care and How to Free It_.



During my time studying this issue, I have concluded that, in developing health policy it is vital to keep in mind one pertinent fact: for all its problems, the United States offers the highest quality health care in the world. Most of the world’s top doctors, hospitals, and research facilities are located in the United States. Eighteen of the last 25 winners of the Nobel Prize in Medicine either are U.S. citizens or work in this country.1 U.S. companies have developed half of all the major new medicines introduced worldwide over the past 20 years.2 In fact, Americans played a key role in 80 percent of the most important medical advances of the past 30 years.3 Nearly every type of advanced medical technology or procedure is more available in the United States than in any other country.4 By almost any measure, if you are diagnosed with a serious illness, the United States is the place you want to be. That is why tens of thousands of patients from around the world come to this country every year for treatment.



Of course, I’m aware that, as critics of American health care often point out, other countries have higher life expectancies and lower infant mortality rates, but those two indicators are not a good way to measure the quality of a nation’s health care system. In the United States, very low‐​birth‐​weight infants have a much greater chance of being brought to term with the latest medical technologies. Some of those low‐​birth‐​weight babies die soon after birth, which boosts our infant mortality rate, but in many other Western countries, those high‐​risk, low‐​birth‐​weight infants are not included when infant mortality is calculated.



And life expectancy is a poor measure of a health care system. Life expectancies are affected by exogenous factors such as violent crime, poverty, obesity, tobacco and drug use, and other issues unrelated to health care. As the OECD explains, “It is difficult to estimate the relative contribution of the numerous non‐​medical and medical factors that might affect variations in life expectancy across countries and over time.“5 Consider the nearly three year disparity in life expectancy between Utah (78.7 years) and Nevada (75.9 years), despite the fact that the have essentially the same health care systems.6 In fact, these exogenous factors are so distorting that if you correct for homicides and accidents, the U.S. rises to the top of the list for life expectancy.7



On the other hand, when you compare the outcome for specific diseases like cancer or heart disease, the United States clearly outperforms the rest of the world. Take prostate cancer, for example. Even though American men are more likely to be diagnosed with prostate cancer than their counterparts in other countries, we are less likely to die from the disease. Less than one out of five American men with prostate cancer will die from it, but 57 percent of British men and nearly half of French and German men will. Even in Canada, a quarter of men diagnosed with prostate cancer, die from the disease.



Similar results can be found for other forms of cancer. For instance, just 30 percent of U.S. citizens diagnosed with colon cancer die from it, compared to fully 74 percent in Britain, 62 percent in New Zealand, 58 percent in France, 57 percent in Germany, 53 percent in Australia, and 36 percent in Canada. Similarly, less than 25 percent of U.S. women die from breast cancer, but 46 percent of British women, 35 percent of French women, 31 percent of German women, 28 percent of Canadian women, 28 percent of Australian women, and 46 percent of women from New Zealand die from it.8



Wisconsin, in fact, is the home of several top flight medical centers, including the University of Wisconsin Hospital and clinics.9



Clearly, there are problems with the U.S. health care system. Costs are rising and distributed in a way that makes it difficult for some people to afford the care they want or need. Moreover, while the number of uninsured Americans is often exaggerated, there are far too many Americans without health insurance, including as many as 481,000 Wisconsin residents at any point in time, roughly 8.8 percent of the population. And while the U.S. provides the world’s highest quality health care, that quality is uneven and too often Americans don’t receive the standard of care that they should.



It is important, therefore, that any reform of the health care system, either nationally or here in Wisconsin, not destroy those things that make our health care system so effective‐​individual choice and free markets. In particular, you should avoid the temptation to increase government regulation and control over the state’s health care system. I am concerned, therefore, that Healthy Wisconsin is headed down the wrong road to reform.





Under Healthy Wisconsin, every state resident except government workers and those currently enrolled in Medicare or Medicaid/​SCHIP/​BadgerCare would be required to join a tax‐​financed “healthy care network.“10 Healthy Wisconsin also extends its reach across the state’s borders, requiring some 158,000 workers who live in other states but work in Wisconsin to enroll.11



These networks would be composed of a wide range of health care providers, including physicians (both primary care and specialists), physicians’ assistants, nurses, clinics, one or more hospitals, and facilities for the treatment of mental illness, drug abuse, and alcoholism.



Each network would submit a bid detailing the per‐​person cost for providing health care to all persons signing up for that network. On the basis of these bids, the state will divide networks into two categories‐​low cost and high cost. Those who enroll in the low cost network will have their costs fully covered by the state, while those enrolling in high cost networks may have to pay an additional fee of up to $1,200 per year for an individual and $2,400 for a family.12



For the most part, networks would be geographically‐​based, generally defined by the board as being within a 30 minute drive, or 60 minutes by mass transit from the patient’s residence.13 However, statewide networks would also exist to serve a few special occupational groups, notably farmers and teachers.14



The legislation does not specify how network providers are to be reimbursed. However, since the network’s total revenue is limited to their bid, presumably reimbursement will be on a capitated, rather than a fee‐​for‐​service basis. Networks are required to spend at least 92 percent of revenues on the provision of services or capital investments.15



Private health insurance will be prohibited for services covered under Healthy Wisconsin.16 Individuals who are currently satisfied with their health insurance would nonetheless lose that insurance and be required to join the new plan. In many cases, this would mean that individuals would no longer have access to their current providers. For instance, if a woman’s primary care physician joined network A, but her ob‐​gyn joined network B, she would be forced to choose between them. Moreover, since the networks are geographically based, it is possible that none of her current physicians may be included in the networks from which she’s allowed to choose. And if their networks were classified in the upper tier, she would have to pay extra in order to join that network and continue to see the physician of her choice.



In theory, the state will also establish a state‐​wide, fee‐​for‐​service plan similar to Medicare or Canada’s single‐​payer system.17 There is to be an additional charge for participating in this plan unless an individual lives in an area without a network. The Board will determine reimbursement rates under this plan, which are to be “fair and adequate,” yet these terms remain undefined.18 Supporters of Healthy Wisconsin have suggested that Medicare’s “very reasonable” fee schedule should be the basis for reimbursements.19



It is unclear whether physicians would be able to accept patients outside the new system on a cash, private‐​contract basis. However, since participation in Healthy Wisconsin is mandatory for patients, those who wanted to pay out‐​of‐​pocket for care would necessarily be paying twice for such care–once through taxes to support Healthy Wisconsin and once on their own.



The benefits networks provide must be at least as generous as those provided by the state to lawmakers and the governor, and must also provide mental health and drug and alcohol rehabilitation benefits not currently included in the state plan.20 But the benefits may be far more generous. The Healthy Wisconsin Board is given unrestricted powers to add whatever benefits it decides would “reduce health care costs, avoid health care risks, or result in better health outcomes.“21 That language represents a virtual blank check and an open invitation for special interests to seek inclusion for their specialty or disease. Considering that Wisconsin already has some 31 mandated insurance benefits adding as much as 20 percent to the cost of an insurance policy in the state, it is reasonable to expect a further expansion of the benefits package.22



Public choice dynamics are such that providers (who would make money from the increased demand for their services) and disease constituencies (whose members naturally have an urgent desire for coverage of their illness or condition) will always have a strong incentive to lobby legislators for inclusion under any minimum benefits package. The public at large will likely see resisting the small cost increase caused by any particular additional benefit as unworthy of a similar effort. It is a simple case of concentrated benefits and diffused costs.



Even Alain Enthoven, who helped develop the idea of managed competition (see below) says that Healthy Wisconsin advocates are “naive” to believe that special interests and provider lobbies can be restrained.23





Healthy Wisconsin combines many aspects of a single‐​payer health care system with the central structure of a concept known as “managed competition,” the brainchild of Stanford University professor Alain Enthoven who testified in favor of the bill at the hearing that the Senate held on the proposal.24 The underlying concept behind both the 1993 Clinton health care plan and Mitt Romney’s Massachusetts reform, it is designed to take advantage of market competition, but within an artificial and carefully regulated marketplace.



Thus, in theory, Wisconsin residents are able to choose between competing “healthy care networks.” But any competition between networks would take place on a very constrained basis. For example, since all plans are required to offer the same core benefits package, there will only be marginal competition based on benefit design. There is price competition only in so far as upper‐​tier networks may require an additional fee. But again, such competition is strictly at the margins. For the vast majority of Wisconsin residents, they will pay the same amount (through their payroll tax) regardless of which plan they choose.



In addition, networks are prohibited from adjusting premiums based on such risk factors as age, sex, or health status.25 This is especially problematic because an inability to price according to risk typically results in overprovision of care to the healthy and under‐​provision to the sick.26



As University of Chicago law professor Richard Epstein has pointed out, “Managed competition is not so much a coherent government plan as an oxymoron. It is possible to have either managed health care or competition in health care services. It is not possible to have both simultaneously.“27 Even Alain Enthoven agrees that “managed competition is not a free market.“28



Healthy Wisconsin goes a step further, combining managed competition with key features of a single‐​payer plan such as global budgeting, thereby borrowing the worst of both worlds.





While universal coverage is the most discussed rationale for Healthy Wisconsin, the proposal also has a second, and to some degree contradictory goal-“health care reform shall implement cost containment strategies that retain and assure affordable coverage for all residents of this state.“29



However, it contains few effective cost control mechanisms. Deductibles are limited to $300 per year for an individual and $600 for a family.30 Co‐​payments are limited to $20 per health care encounter, and combined yearly, out‐​of‐​pocket expenses (for both co‐​payments and deductibles) are limited to $2,000 for an individual and $3,000 for a family.31 There are no deductibles or co‐​payments at all for children, pregnant women, or people in disease management programs, and the Board has the authority to reduce or eliminate co‐​payments and deductibles for everyone else if they choose.32 Health savings accounts, part of the bill’s initial draft, were stripped from the final legislation.



Instead of consumer cost‐​sharing, the legislation establishes a statewide global budget for health care, limiting the growth of health care costs to no more than the national average. However, in recent years Wisconsin health care costs have been increasing faster than the nation’s costs as a whole. In 2006, state health care spending rose by 9.3 percent while the increase in national health care spending was slightly below 8 percent.33



If Wisconsin’s health care costs exceed the national average, the state’s Secretary of Administration, together with the Board of the Healthy Wisconsin Authority, is empowered to “establish, by rule, a program to contain health care costs.“34 The legislation gives no direction and puts no limitation on how this is to be done, beyond a mandate to eliminate “unnecessary operating and capita costs.” Yet if we look at how global budgets are enforced in other countries, or even how U.S. government programs like Medicare attempt to control costs, we can assume that any attempt to reduce costs would include reductions in reimbursements and/​or capital spending.



This would almost inevitably reduce the availability of and access to care. A reduction in capital expenditures means fewer hospital beds and less modern medical technology. Simply compare the U.S. to Canada. The United States has more than five times as many MRI units per million people, three times as many CT scanners, and three times as many lithotripters.35



Similarly, reducing reimbursement rates will drive physicians out of the state. Canada has roughly 2.1 practicing physicians per 1,000 people, far less than the OECD average. Worse, there has been absolutely no growth in the number of physicians per 1,000 people since 1990. And while the number of nurses per 1,000 people remains near the OECD average, that number has been declining since 1990.36



Indeed, waiting lists are a major problem under the Canadian system. There is no accurate government data available, though provincial reports do show the existence of at least moderate waiting lists. The best information may come from a survey of Canadian physicians by the Fraser Institute, which suggests that as many as 800,000 Canadians are waiting for treatment at any given time. According to this survey, treatment time from initial referral by a GP, through consultation with a specialist, to final treatment, across all specialties and all procedures (emergency, non‐​urgent, and elective), averaged 17.7 weeks in 2005.37



Defenders of national health care have attempted to discount these waiting lists, suggesting that the waits are shorter than commonly portrayed or that most of those on the waiting list are seeking elective surgery. However, a look at specialties with especially long waits shows that while the longest waits are for procedures such as hip or knee replacement and cataract surgery, which could arguably be considered elective, fields that could have significant impact on a patient’s health such as neurosurgery, also have significant waiting times.38 In some cases, the delays could be life threatening. A study in the Canadian Medical Association Journal found that at least 50 patients in Ontario alone have died while in the waiting list for cardiac catheterization.39 And Canadian Supreme Court Chief Justice Beverly McLachlin wrote in a 2005 decision striking down part of Canada’s universal care law, that many Canadians waiting for treatment suffer chronic pain and that “patients die while on the waiting list.“40



It is also worth noting again that there will be no health care professionals on the Board. Yet, the Board is empowered to determine if care is “medically appropriate” and conforming to “best practices.” You thus have the possibility of a group of union representatives and corporate executives overruling doctors on questions of medical treatment.



Of course, advocates assume that costs can be controlled without resorting to rationing. They predict substantial savings from the elimination of insurance overhead costs, increased preventive care, and integrated disease management. One study suggests savings of more than $800 million in the first year alone.41 However, there is reason to be skeptical.



For example, it is assumed that the plan’s emphasis on primary and preventive care will lead to $565 million in savings.42 Preventive care advocates assume that if we focus on preventive medicine, we can prevent people from getting sick in the first place. And by emphasizing timely primary care, those who do end up with a chronic illness will develop fewer complications. By spending money up front to reduce the frequency and severity of illness we can reduce the amount of money needed to eventually treat those illnesses in the future.



As logical as this may seem, studies actually show that preventive care usually ends up costing money in the long run because there is no way to precisely target such care.43 For every disease that we prevent or catch early, we end up testing and treating many people who will never get sick. For example, Jay Bhattacharya, a doctor and economist at Stanford’s School of Medicine, estimates that to prevent one new case of diabetes, an anti‐​obesity program must treat five people.44 Similarly, a study of retirees in California by Jonathan Gruber, a health economist at MIT and long‐​time advocate of national health insurance, found that when retirees had fewer doctors visits and filled fewer prescriptions, overall medical spending declined.45 People became ill more frequently, but treating their illnesses was still less costly than paying for preventive care for everyone. Thus, increased preventive and primary care may well be beneficial for the individual in terms of health, but it is unlikely to provide a societal benefit in terms of reduced costs.



Second, there is an assumption that centralizing the purchase of prescription drugs and allowing the state to negotiate drug prices directly with producers will yield another $178 million in savings.46 But as the Congressional Budget Office concluded about a similar proposal for Medicare Part D, that is only likely to occur if the purchaser adopts a restrictive formulary limiting the number of drugs available in a therapeutic class.47 In the absence of such restrictions, government is unlikely to achieve significant savings beyond what private insurers, particularly managed care plans, have been able to negotiate. But the public is unlikely to accept a statewide restrictive formulary‐​not even considering the important medical reasons for rejecting such formularies‐​meaning that actual savings will be far less than projected.



Finally, it is assumed that a government‐​run system would result in substantial reduction of administrative costs, as much as $407 million.48 There is no doubt that administrative costs under private insurance plans add considerably to the cost of health care, roughly 8–16 percent.49 However, advocates of a government run system frequently underestimate the administrative burden under those systems. A study by Patricia Danzon of the Wharton School has estimated that administrative costs under Canada’s system run as high as 45 percent of claims.50 And even Medicare, often cited as a model of government efficiency, has administrative costs of more than 5 percent.51



If the predicted savings fail to materialize, it could pose serious problems for Healthy Wisconsin since the same actuaries analyzing an early iteration of a Wisconsin universal coverage proposal (the Wisconsin Health Plan) predicted nearly $1 billion per year in new costs from increased utilization.52 Similar utilization increases should be expected under Healthy Wisconsin. And this does not include the added cost if the political economy dynamics discussed above lead to the addition of yet more benefits.



In all likelihood, proponents of Healthy Wisconsin have overestimated savings and underestimated costs. This is not unusual with government programs, especially those regarding health care. In 1967, the House Ways and Means Committee predicted that Medicare would cost $12 billion in 1990. In reality, the program cost over $110 billion that year.53 In 1987, Congress estimated that the Medicaid Special Hospitals Subsidy would reach $100 million in 1992. The actual cost exceeded $11 billion.54 Should something similar happen with Healthy Wisconsin, rationing will be almost inevitable.





Healthy Wisconsin is estimated to cost at least $15.2 billion initially and increase state spending by 23 percent. Financing would primarily come through a new payroll tax. The exact size of the payroll tax will be determined later, but the legislation gives the Healthy Wisconsin Board the power to set employer contributions between nine and 12 percent of wages and employee contributions from two to four percent, up to the social security wage cap (currently $97,500).55 Proponents of the Healthy Wisconsin estimate that an initial payroll tax of 14.5 percent will be needed to raise the $15.2 billion necessary to fund the program.56



This represents more in taxes than the state currently takes in through income, sales, and corporate taxes combined.



In theory, the tax would be split, with the employee paying four percent and the employer paying 10.5 percent. But, while it might be politically appealing to claim that business will bear the new tax burden, nearly all economists would see it quite differently. The amount of compensation that a worker receives is a function of his or her productivity. The employer is generally indifferent to the composition of that compensation. It can be in the form of wages, benefits, or taxes. What matters is the total cost of hiring that worker. Mandating an increase in the cost of hiring a worker by adding a new payroll tax does nothing to increase that worker’s productivity. Employers will therefore seek ways to offset the added cost by raising prices (the most unlikely solution in a competitive market), lowering wages, reducing future wage increases, reducing other benefits (such as pensions), reducing hiring, laying off current workers, or outsourcing. In the end, one way or another, workers will bear the full cost.



Of course, the added cost of the payroll tax will be offset to the degree that businesses no longer have to pay health insurance premiums. Surveys suggest that health insurance currently costs the average Wisconsin business between 11.8 and 12.7 percent of wages.57 However, that average is not equally distributed. Wisconsin’s small and medium sized businesses generally pay far less, on average 5–7 percent of wages.58 And, of course, nearly 40 percent of Wisconsin’s small businesses currently do not offer health insurance.59 Thus, there will be a net increase in the cost of employing each worker for most businesses, and an enormous tax increase for small and medium sized businesses. Overall, it is estimated that employers will face $579 million per year in additional costs.60



The most obvious thing a business could do is relocate outside of Wisconsin. Given that Wisconsin’s tax burden and business climate are already significantly worse than surrounding states, the increased burden of Healthy Wisconsin is an almost certain recipe for slowed economic growth and lost jobs. The non‐​partisan Tax Foundation currently ranks Wisconsin 7th in the nation in terms of state and local tax burden. By comparison, Minnesota ranks 11th, Michigan ranks 14th, Iowa 17th, and Illinois 22nd.61 Similarly, Wisconsin ranks only 32nd in terms of business friendliness, well behind Michigan and Illinois (although Minnesota and Iowa did rank worse).62



It is no wonder that from August 2006 to August 2007, Wisconsin actually lost nearly 16,000 jobs. The state’s unemployment rate is now 5.3 percent, on a seasonally adjusted basis.63



If the 14.5 percent payroll tax is enacted, Wisconsin’s tax burden will rise to number one in the nation.64 Added to the other tax increases included in the Senate‐​passed budget, the state’s tax burden would rise from 12.3 percent of state income today, to 20.1 percent. To put this in perspective, the federal tax burden is only slightly higher, 21.7 percent of national income. On average, Wisconsin taxpayers will be paying more than 40 percent of their income in combined federal, state, and local taxes.



However, all of this may understate both the cost of the program and the taxes necessary to support it. The Wisconsin Department of Revenue projects wages in the state to grow by 4.6 percent annually, bringing revenue from the payroll tax to $23.4 billion by 2017.65 If health care costs grow by 6.5 percent annually, as projected by actuaries with the Lewin Group, benefits under Healthy Wisconsin would top $33.1 billion by 2017, leaving a nearly $10 billion deficit.66 That may not be the worst. As noted above, nationally, health care inflation has been running close to 8 percent annually, and the cost of employer provided health care in Wisconsin rose by 9.3 percent in 2006. If health care inflation in Wisconsin merely mirrors the national average over the next decade, the program’s cost will rise far faster than previously estimated, leading to a still greater budget shortfall.



And its not just businesses that are likely to relocate. Particularly in border areas, Health Wisconsin sets up a perverse set of incentives that will encourage healthy and insured residents to move out of state, while also encouraging uninsured and sick from out of state to relocate to, or at least take jobs in, Wisconsin.



This would significantly strain the health facilities in those areas, especially since the border areas largely consist of smaller communities with limited medical facilities. Wisconsin faced a similar burden in the 1980s, when the state’s higher welfare benefits acted as a magnate for poor families relocating from places like Chicago. Healthy Wisconsin could act as a similar magnate, particularly since it covers undocumented immigrants and extends special benefits to low‐​income pregnant women.67 Such an outcome would drive up program costs and create a host of other problems for the state.





Whatever Healthy Wisconsin’s merits or lack thereof as policy, there are significant questions about its legality. The federal Employee Retirement Income Security Act (ERISA) preempts state regulation of certain employer‐​provided benefits, including self‐​funded health insurance plans. The courts have generally interpreted this very broadly as prohibiting not only direct regulation of such plans, but any law or regulation that “relates to” or has a “connection with” them. A state law is a violation of ERISA if it “effectively mandates some element of the structure or administration” of an employer’s ERISA‐​protected plan.68



For example, when Maryland attempted to require businesses with more than 10,000 employees to either provide all their workers with health insurance or contribute a payroll tax, the court struck it down, holding that because it would require employers to “change how they structure their employee benefit plans,” it violated ERISA.69 Similarly in Wisconsin, laws attempting to regulate health insurance convertibility and continuation, and the Health Insurance Risk‐​Sharing Plan were struck down.70



Supporters of Healthy Wisconsin argue that ERISA does not apply because the plan neither requires employers to provide benefits to their workers nor conditions the payroll tax on the provision of those benefits. However, as the Wisconsin Legislative Council has noted, “it is undeniable that it will have an effect on [ERISA‐​protected] plans.“71 As the courts have pointed out, one purpose of ERISA is to enable multi‐​state companies to provide uniform benefits across state lines. But under Healthy Wisconsin, employers would have to change the benefits they currently offer. They would be offering different plans for Wisconsin workers than for those in other states (in many cases there would be no employer provided health benefits for Wisconsin workers).



Therefore, it is hard to see how Healthy Wisconsin could survive an ERISA‐​based court challenge.





If Healthy Wisconsin is not the answer, then what can Wisconsin do to improve its health care system?



The unfortunate reality is that the state’s options are limited because both the real villains and solutions to America’s health care problems lie in Washington, and specifically with the federal tax code, beyond the reach of state lawmakers. However, there are some important steps that this state can take that will reduce the cost of health care and increase the number of people who are insured, while preserving‐​and even improving‐​the quality of the current system.



First, Wisconsin should do what it can to reduce the cost of health insurance. After all, the number one reason that people give for not purchasing insurance is that they cannot afford it.72 This is particularly true for young and healthy individuals: precisely the people who should be encouraged to enter the insurance market before they become older and sicker. Yet, current state regulations drive up the cost of health insurance and make it a reasonably logical decision for these young, healthy individuals to remain uninsured.



For example, Wisconsin currently has some 31 mandated benefits. These mandates force all insurance policies sold in the state to cover treatment for things like cleft palates and blood lead poisoning, and for in‐​vitro fertilization and AIDS vaccines.73 These mandates add significantly to the cost of insurance. The requirement for mental health parity alone adds as much as 10 percent to the cost of an insurance policy. Many of the other mandates add 1–3 percent each to insurance costs.74 Clearly, people should be able to purchase coverage for such conditions and providers if they desire it. But just as clearly, those who wish to purchase a less inclusive but also less expensive policy should be able to do so. Repealing such mandates would be one of the most effective steps that Wisconsin could take to reduce the cost of health insurance and thereby increase the number of people with insurance.



Of course repealing such mandates will encounter fierce resistance from special interests and may prove politically difficult. There is a potentially easier step that Wisconsin could take to achieve similar and possibly more comprehensive results. The state could amend its insurance laws to allow the sale of any health insurance plan approved for sale by any state.



Currently health insurance purchasers are essentially stuck with the regulatory regime of the state in which they reside. Wisconsin businesses and individuals are held hostage by Wisconsin insurance regulation. But if free to purchase health insurance regulated by states other than their own, customers could avoid regulations that added unwanted costs. They could in effect, “purchase” another state’s set of regulations by purchasing insurance from an insurer chartered in that state. If Wisconsin residents do not wish to purchase all 31 coverage mandates the state requires, they could purchase insurance from, say, Idaho, where there are only 13, or any state whose laws are more closely aligned with their own preferences.



Not only would such a simple change to Wisconsin’s insurance laws benefit consumers, reduce costs, and increase the number of people with insurance, the same competitive process that drives producers to improve quality and reduce costs in other products could help produce higher quality regulations. Wisconsin would have to compete for the best regulatory environment in the same way it currently competes with other states to have the best tax environment.



Secondly, the state should institute a thorough review of how it can reduce the cost of providing health care. In particular it should look at such issues as expanding the scope of practice for non‐​physician professionals, and removing barriers to hospital competition.



And third, the state should continue to do all it can to expand the use of consumer‐​oriented health plans such as Health Savings Accounts. I know that I have been widely quoted in the press and elsewhere as saying that “health savings accounts are not a silver bullet.” That is correct. That is because there are no silver bullets when it comes to health care reform. No single reform will solve all of Wisconsin’s, or the country’s, health care problems. A combination of interlocking reforms dealing with providers and consumers, supply and demand, will be required. And even then, utopia is not an option.



However, let me be clear about this. Health savings accounts are an important tool in health care reform. Any successful health care reform requires increasing price transparency within the system, making health care consumers more cost‐​aware, and health savings accounts are an important tool in accomplishing this. I fully support them.



I regret that I have not been able to come here and offer a silver bullet to fix the problems with Wisconsin’s health system. Indeed, some may be disappointed that so much of my advice is in the form of what not to do. This is because I believe, that in pursuing health care reform, legislators should be guided by the Hippocratic admonition, “First do no harm.”



It is understandable that you and your constituents are frustrated by the inability of Congress to address the undeniable need for health care reform. Yet it is sadly true that the keys to health care reform lie in federal, not state, legislation. There are limited steps that Wisconsin can take to make the situation better. But, in the end, you should be extremely careful to make sure that impatience does not push you into taking steps that will ultimately make the problem far worse, hurting Wisconsin taxpayers, businesses, health care providers, and perhaps most importantly, patients.



I thank you once again for your time and consideration. I would be happy to answer any questions.



1“Nobel Prize in Physiology or Medicine Winners 2006–1901,” The Nobel Prize Internet Archive, http://​almaz​.com/​n​o​b​e​l​/​m​e​d​i​c​i​n​e​/​m​e​d​i​c​i​n​e​.html.



2Pharmaceutical Manufacturers Association, “Facts about the U.S. Pharmaceutical Industry,” 2002.



3 _Economic Report of the President_ (Washington: Government Printing Office, 2004), p. 192



4Gerard Anderson et al., “It’s the Prices Stupid: Why the United States Is So Different from Other Countries,” _Health Affairs_ 22, no. 3 (May/​June 2003): 99.



5“Health at a Glance: OECD Indicators, 2005,” Paris, OECD Publishing, 2005.



6U.S. Census Bureau, 2000 Census.



7Robert L. Ohsfeldt, John E. Schneider, _The Business of Health: The Role of Competition, Markets, and Regulation_ (Washington AEI Press, 2006).



8Varduhi Petrosyan, and Peter Hussey, _Multinational Comparisons of Health Systems Data, 2002_ (New York: The Commonwealth Fund, 2002), pp. 55–62; Gerard Anderson and Peter Hussey, _Multinational Comparisons of Health Data Systems Data, 2000_ (New York: The Commonwealth Fund, 2000), pp. 17–18; Gerard Anderson and Bianca Frogner, _Multinational Comparisons of Health Data Systems Data, 2005_ (New York: The Commonwealth Fund, 2006).



9“America’s Best Hospitals, 2007,” _U.S. News & World Report_, July 2007.



10Government workers would eventually be transitioned to Healthy Wisconsin, but only after their current labor contracts expire. The plan anticipates that the state will provide significant “wrap around” coverage to ensure that government workers do not lose any of the generous benefits they currently enjoy.



11“Healthy Wisconsin (HW) — Your Choice — Your Plan: Cost and Coverage Impacts,” AARP Wisconsin and the Lewin Group, June 19, 2007.



12Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.30(7)(b)(4)(a).



13Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.30(2)(b)(4)(c)(1).



14Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.30(4)(m)(2).



15Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.30(4)(b)



16 _Private insurance could continue to cover services over and above those covered by Healthy Wisconsin._



17Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.30(2)(a).



18Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.30(7)(b)(1).



19Jack Lohman, “Lohman: Pass, Expand Healthy Wisconsin Bill,” _Wisconsin State Journal_ , July 16, 2007.



20Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.15



21Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.15(1)



22Victoria Craig Bunch, JP Wieske and Vlasta Prikazsky, “Health Insurance Mandates in the States 2007.” Council for Affordable Health Insurance, 2007.



23Guy Boulton, “Reformer Weighs in on Health Plan,” _Milwaukee Journal‐​Sentinel_ , July 13, 2007.



24See Alain Enthoven, “The History and Principles of Managed Competition,” _Health Affairs_ , supplement (1993).



25Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.30(4)(m).



26John Goodman and Gerald Musgrave, “A Primer on Managed Competition,” National Center for Policy Analysis Policy Report no. 183, April 1994.



27Richard Epstein, “Unmanageable Care,” Reason, May 1993.



28Alain Enthoven, “The History and Principles of Managed Competition,” _Health Affairs_ , supplement (1993), p. 44.



29Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.05(4)(a)(2)(a)



30Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.20(2)(a).



31Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapters 260.20(3)(a) 260.20(4)(a,b).



32Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.20(3)(e)



33Guy Boulton, “Benefit Tab 26% Higher in State,” _Milwaukee Journal‐​Sentinel_ , Nov. 20, 2006; “Health Insurance Cost,” National Coalition on Health Care, www​.nchc​.org, lasat visited August 30, 2007.



34Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 16.004(d).



35“OECD Health Data 2007: Statistics and Indicators for 30 Countries.” Organization for Economic Co‐​operation and Development, July 2007.



36“Health at a Glance: OECD Indicators, 2005,” Paris, Organization for Economic Co‐​Operation and Development, 2005.



37Nadeem Esmail, Michael Walker,and Dominika Wrona, “Waiting Our Turn 16th Edition: Hospital Waiting Lists in Canada,” Fraser Institute, 2006.



38Ibid.



39Madhu Natarajan et al., The Risks of Waiting for Cardiac Catheterization: a Prospective Study, _Canadian Medical Association Journal_ , November 26, 2002.



40 _Chaoulli v. Quebec_ (Attorney General) 2005 SCC.



41“Healthy Wisconsin (HW) — Your Choice — Your Plan: Cost and Coverage Impacts,” AARP Wisconsin and the Lewin Group, June 19, 2007.



42Ibid.



43David Leonhardt, “Free Lunch on Health? Think Again,” _New York Times_ , August 8, 2007.



44Jay Bhattacharya and M. Kate Bundorf, “The Incidence of the Health Care Costs of Obesity,” Stanford University, April 2005.



45Amitabh Chandra, Jonathan Gruber and Robin McKnight, “Patient Cost‐​Sharing, Hospitalization Effects and the Design of Optimal Health Insurance,” NBER Working Paper No. 12972, March 2007.



46“Healthy Wisconsin (HW) — Your Choice — Your Plan: Cost and Coverage Impacts,” AARP Wisconsin and the Lewin Group, June 19, 2007.



47“Issues Regarding Drug Price Negotiation in Medicare: Letter to the Honorable Ron Wyden,” Congressional Budget Office, April 10, 2007.



48“Healthy Wisconsin (HW) — Your Choice — Your Plan: Cost and Coverage Impacts,” AARP Wisconsin and the Lewin Group, June 19, 2007.



49Steffie Woolhandler, Terry Campbell and David Himmelstein, “Costs of Health Care Administration in the United States and Canada,” _New England Journal of Medicine_ vol. 349, no. 8, August 21, 2003, 768–775.



50Patricia Danzon, “Hidden Overhead Costs: Is Canada’s System Really Less Expensive?” _Health Affairs_ 11 (Spring 1992): 21–43.



51Merrill Matthews, “Medicare’s Hidden Administrative Costs: A Comparison of Medicare and the Private Sector,” Council for Affordable Health Insurance, January 10, 2006.



52“The Wisconsin Health Plan (WHP): Estimated Costs and Coverage Impact: Final Report,” The Lewin Group, June 2007.



53Steven Hayward and Erik Peterson, “The Medicare Monster: A Cautionary Tale,” Reason Magazine, January 1993.



54Chris Edwards, “Government Schemes Cost More Than Promised,” Cato Institute Tax and Budget Bulletin no.17, September 2003.



55Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.40 (2 and 3).



56“Healthy Wisconsin (HW) — Your Choice — Your Plan: Cost and Coverage Impacts,” AARP Wisconsin and the Lewin Group, June 19, 2007. This is slightly less than the 15.5 percent payroll tax that Lewin estimated would be necessary to fund the Wisconsin Health Plan. “The Wisconsin Health Plan (WHP): Estimated Costs and Coverage Impact: Final Report,” The Lewin Group, June 2007. While Lewin now suggests a 14.5 percent payroll tax, others continue to suggest a 15.5 percent tax will be required. As noted above, the Board has the authority to set the tax as high as 16 percent without further legislative action. Senate Amendment 1 to Senate Substitute Amendment 1 to SB 40, Chapter 260.40 (2 and 3).



57M. Scott Niederjohn and Mark C. Schug “An Evaluation of the Wisconsin Health Plan” Wisconsin Policy Research Institute Report vol. 20, no.1, January 2007. Although this study was focused on the Wisconsin Health Plan rather than Healthy Wisconsin, the underlying survey numbers and analysis remain valid.



58Ibid.



59Edward Neuschler, “A Profile of Employer Coverage in Wisconsin,” _Institute for Health Policy Solutions_ power point presentation, September 20, 2001.



60“Healthy Wisconsin (HW) — Your Choice — Your Plan: Cost and Coverage Impacts,” AARP Wisconsin and the Lewin Group, June 19, 2007.



61“State and Local Tax Burdens Compared to Other U.S. States,” Tax Foundation Tax Data, April 4, 2007. http://​www​.tax​foun​da​tion​.org/​n​e​w​s​/​s​h​o​w​/​3​3​5​.html



62Curtis Dubay and Chris Atkins, “State Business Tax Climate (Fourth Edition),” Tax Foundation Background Paper no. 52, October 11, 2006.



63State of Wisconsin, Department of Workforce Development, “August Unemployment Rates Announced,” press release, September 20, 2007.



64Curtis Dubay, “Things We Thought We’d Never See at the Tax Foundation, But Thanks to the Wisconsin Senate …,” Tax Foundation Tax Policy Blog, June 28, 2007. http://​www​.tax​foun​da​tion​.org/​b​l​o​g​/​s​h​o​w​/​2​2​4​5​4​.html



65“Wisconsin’s Eroding Household Income,” _The Wisconsin Taxpayer_ vol. 75, no.2, The Wisconsin Taxpayer’s Alliance, February 2007.



66“Healthy Wisconsin (HW) — Your Choice — Your Plan: Cost and Coverage Impacts,” AARP Wisconsin and the Lewin Group, June 19, 2007.



67Wisconsin Manufacturers and Commerce, “State‐​Run Health Care Proposal Will Cover Illegal Aliens,” press release, July 16, 2007.



68 _Retail Industry Leaders Association (RILA) v. Fielder_ , 475 F. 3d. 180 (4th Cir. 2007). At 192.



69Ibid.



70 _General Split Corporation v. Mitchel_ l, 523F. Supp. 427 (E.D. Wisc. 1981).



71Memorandum from Richard Sweet, senior staff attorney, to Rep. Leah Vukmir, “Federal Preemption of Employer Assessments under the Healthy Wisconsin Plan,” Wisconsin Legislative Council, August 1, 2007.



72“The Uninsured: A Primer, Key Facts About Americans Without Health Insurance,” Kaiser Family Foundation, December 2003



73Victoria Craig Bunce, JP Wieske and Vlasta Prikazsky, “Health Insurance Mandates in the States 2007,” Council for Affordable Health Insurance, 2007.



74Ibid.
"
"
A trio of tropical storms can be seen on this satellite image below. Former Hurricane Danielle (at top) has simply become a low pressure system now. 
click to enlarge
From NOAA: GOES-13 Catches 3 Tropical Cyclones Thrashing Through the Atlantic
Powerful Hurricane Earl, growing Tropical Storm Fiona and fading  Danielle were all captured in today’s visible image from the GOES-13  satellite.   The Geostationary Operational Environmental Satellite called GOES-13  captured an image of the busy Atlantic Ocean at 1145 UTC (7:45 a.m. EDT)  on August 31. In the visible image, was the large and powerful  Hurricane Earl passing Puerto Rico, Tropical Storm Fiona located to  Earl’s east, and Danielle far in the Northern Atlantic. Hurricane Earl’s  eye appear to be covered with high-clouds in the GOES-13 image, while  Fiona appeared somewhat disorganized with no apparent center. Farther  north in the North Atlantic Ocean, Danielle appeared more “U” shaped on  the satellite imagery, although her maximum sustained winds were still  near 70 mph at that time.
GOES satellites are operated by NOAA, and the NASA GOES Project at  NASA’s Goddard Space Flight Center in Greenbelt, Md. provides images and  animations of satellite data.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88f73fff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Though it can still be found in the forests of Europe, the Eurasian lynx has not been seen in the UK for more than 1,000 years. This medium-sized wild cat with its distinctive pointy ears was driven to extinction during the medieval period, thanks to low numbers of its preferred prey, roe deer, as well as a disappearing habitat and excessive hunting. But recently the Lynx UK Trust has argued strongly for its reintroduction. Scotland is home to the majority of the UK’s forests, has a relatively low population and an abundance of roe deer. This combination of habitat and prey makes it the most realistic place to consider reintroducing this species. The arguments for bringing back the lynx are numerous, from restoring ecological processes, controlling spiralling deer numbers and economic benefits from increased tourism. But these arguments face considerable opposition from farmers concerned about the risk to livestock and questions about the species’ long-term impact and the suitability of the landscape to accommodate them. Resolving these issues is greatly hindered by a lack of clear evidence of how suitable Scotland is, whether reintroduction is something people want to see happen – and how likely it is to succeed. Without robust evidence, effective and informed decisions cannot be made. Reintroducing large carnivores anywhere around the world is often controversial, complex, costly and challenging. For a species such as the lynx it could take up to 100 years before it is known whether a reintroduction has been a success or not. So getting things right first time is essential. Our work uses cutting-edge computer modelling tools to bring clarity and provide robust evidence about one key aspect of this debate in Scotland: is there enough suitable habitat to support a successful reintroduction of the lynx – and, if so, where should efforts be focused?  Computer modelling provides a safe and inexpensive space to test the effectiveness of proposals before implementing them on the ground. So, any advances in modelling that accurately reflect developments in ecological theory and take account of specific characteristics of a particular species in relation to complex landscapes, are extremely valuable.  Our research used a computer model, not just to contribute reliable evidence to the current lynx debate in Scotland, but to provide a case study that demonstrates how our approach could be used for animal reintroductions elsewhere in the world. The International Union for Conservation of Nature (IUCN) has very clear guidelines when considering if a reintroduction is suitable. Ranked highly is establishing that the historic causes of extinction are no longer present or pose a threat to future populations. It is essential, then, that the availability of habitat and prey and the risk of persecution are no longer barriers to establishing a healthy and viable lynx population. The abundance of roe deer in Scotland covers suitable prey, but the risk of persecution is linked to whether there is public appetite to see lynx return. Equally, knowledge of the location, its size and how easily lynx can move between forest habitats is essential. Until recently these were all relatively unknown quantities, but our research has shed much-needed light on the last of these points. Part of making informed decisions means having the latest information and the best tools for the job. While a previous study investigated the suitability of Scotland for reintroducing lynx, some of the landscape data used is now more than 30 years old.  But technological advances have drastically improved the power of our predictions. So to bring this original work up to date, we generated high-resolution maps using available information for all of the different habitats across Scotland, focusing particularly on suitable woodland areas. Then we gathered detailed information on the ecology of the Eurasian lynx from other studies. Finally, we entered it all into a recently developed model called “RangeShifter”, designed to capture realistic animal movement patterns across complex landscapes.  Once all these pieces were in place, we were able to run 100-year simulations to test which areas of Scotland previously identified as potential release sites might be most suitable for a reintroduction in terms of current habitat availability. The three locations we considered were Aberdeenshire in Scotland’s north-east, the Kintyre Peninsula on the west coast, and the Scottish portion of Kielder Forest in the Borders, all sites that had been proposed in the past. Regardless of how we chose to measure success – and irrespective of the changes we made to the model parameters (such as how many kittens the lynx would have or how long they would live) – the Kintyre Peninsula always came out on top. Parts of Kielder Forest have been the focus of much of the recent debate. But our results found that the Scottish section of this forest was always the least suitable location. The Kintyre Peninsula offers up to an 83% chance that 100 years after the release of 10 lynx, a good population would still be around. In contrast, Aberdeenshire gave a 35% chance of success, but in Kielder Forest there was only a 21% chance a population would still exist after a century. Crucially, we showed how the Highlands of Scotland, the region in which the Kintyre Peninsula and the majority of suitable habitat exists, is completely cut off from habitat south of the Glasgow-Edinburgh “Central Belt”, including Kielder Forest. This raises concerns about the area’s long-term viability for a lynx population, as it would not be able to reach and colonise the Highlands.  Our work does not investigate the political will or public opinion surrounding the reintroduction of lynx – both of which are essential considerations of any reintroduction planning – but it does offer an encouraging step forward, demonstrating the suitability of Scotland’s landscape to support and sustain lynx in the future. And, critically, this depends on location.  The novel application of this model to reintroduction planning holds great promise, not just for informing the lynx debate in Scotland, but also for the conservation of large carnivores and other species around the world."
"Europe has become the global centre of the coronavirus pandemic, according to the World Health Organization. In France, where I am writing from, President Macron has put the country in a “health war” lockdown. Cafes, restaurants, theatres, public parks are closed and all non-essential activities have stopped. In living memory there’s never been anything like this: under Nazi occupation, as we know, Paris culture and nightlife kept functioning rather well. Covid-19 is upon us now, and many of our continent’s large cities feel like they’re fast sliding into one of those dystopian movie scripts with empty streets, face masks, surgical gloves and self-isolation the new normal. With hospital emergency units bracing themselves for much worse yet to come, it feels almost irrelevant to dwell on what this moment says about our place in the world as Europeans, or which feelings we are able to muster – or not – for one another and for others overseas. Postwar Europe’s institutional set-up was meant to epitomise international cooperation and solidarity, and to set a form of global example in the process. Will any of that survive now as countries start to wall themselves in? And what can we citizens do about it? Postwar Europe’s institutional set-up was meant to epitomise international solidarity. Will any of that survive now? After Donald Trump signalled last week that the US wanted to de facto seal itself off from Europe, I remembered an oped by two of his senior team members published in 2017. “The world is not a ‘global community’, but an arena where nations ... compete for advantage,” they’d written. With Covid-19, that “arena” is put into even sharper focus. And nationalist reflexes are hardly a Trumpian monopoly these days. (Reports that Trump offered a German biopharmaceutical company a fat sum of money to secure a vaccine exclusively for the US only added to a general go-it-alone picture.) There isn’t much official coordination or sticking-together on display in Europe itself. National governments, including Germany’s, have sealed their national borders to neighbouring states or are increasing controls. The rationale for much of this can be mind-boggling. Borders don’t stop the virus. Some of what’s at work is that rightwing populist credos have in recent years infected entire swaths of our continent’s politics. For them, there is only one single measure of collective self-identification and solidarity: the ethno-national level. Hungary’s prime minister Viktor Orbán, to take an example, lost no time holding “foreigners” responsible for the pandemic. As events spiralled and with the death count mounting, I wondered about empathy and solidarity: what triggers them? What does a pandemic crisis say about our capacity for such feelings and modes of action? And could grassroots, citizens’ initiatives possibly course-correct some of the egoism of states? In his book Ordinary Virtues, Michael Ignatieff quotes a speech Eleanor Roosevelt gave at the United Nations in 1958. She was speaking about human rights conventions, but the gist of her text can apply to the notion of human solidarity at large. “Where, after all, do universal human rights begin? In small places, close to home – so close and so small that they cannot be seen on any maps of the world. Yet they are the world of the individual person ... Without concerted citizen action … we shall look in vain for progress in the larger world.” Being the global centre of the pandemic means we in Europe hold a specific responsibility in the way we react to this situation, and in how we behave also towards others beyond our shores. Perhaps it helps to think a bit about how we approached the climate crisis as a collective cause for mobilisation (think of the Paris climate agreement). Why couldn’t we now clearly identify Covid-19 as another danger that needs to be addressed collectively – and take up the task as citizens to make that message loud and clear? However different, both these perils have this in common: they transcend all national boundaries and they threaten lives. The “us” versus “them” logic of nationalists and populists becomes absurd in the face of these phenomena. So here’s a question. While governments scramble, is it too early to think of launching a Europe-wide citizens’ online campaign for solidarity in the age of the coronavirus? Could people create a movement that says, let’s help each other as much as we can, and in ways that cut across the national divides many of our governments are resorting to? Videos of Italians in lockdown singing from their balconies to keep their spirits up have been admirable. Could some of that gusto spread and morph into a Europe-wide flurry of videos chanting our empathy and willingness to show solidarity with one another? Wherever we may live and whatever language we may speak, sending that kind of message across our “Corona-centre” continent would hold special meaning, surely not just for ourselves now, but for others also, and perhaps for the future as well. To be sure, for the moment we are stunned by the shock of what’s unfolding and bewildered by what is yet heading our way. Also, still too many people seem confused or in a form of denial as to the exact extent of what we’re facing. Until just days ago, some people in Paris thought best to keep partying, or to attend crowded public protests. Likewise in Ireland, videos of people celebrating in packed bars last weekend have caused outrage. Trust in institutions and resistance to fake news are being put to the test. Pessimists will say our European capacity to come together and show generosity, or even elementary openness to others, has already been entirely blunted by the crises of the past decade (our numbness to Syria’s killing fields is, to me, the greatest case in point). We are no doubt now in severe, introspective, fear-and-fragmentation European mode. But for those who still believe we can be a community of a kind, and that our continental space (or the world beyond) should not be turned into an “arena”, now is the time to ask ourselves how we will want to look back at this phase of our collective history. How will we want future generations to look back at us, and what kind of message do we want to send to the rest of the world? And please note, with every mention of Europe I include the UK. We are one continent, and the virus is among us all. Discord or “social distancing” among nation-states makes no sense in the face of an invisible enemy in our midst which makes no distinction about its victims. And look around: ordinary virtues aren’t absent at all. Gestures of empathy and solidarity are multiplying at a local level – medical students volunteering to help hospitals, or neighbours helping the elderly get food. Why not invent something symbolically similar at a wider, transnational level, and by making use of digital tools? Politicians have done little of this. But citizens can show the way. Artists, creators, start-ups, activists, anyone or any network that’s part of the fabric that binds us together in beautiful, meaningful ways under ordinary circumstances, could take a stand for cross-border solidarity in these extraordinary circumstances. Scientists and medics are of course sharing and coming together. Why not extend that to other parts of our societies? It’s obvious that our only chance to somehow mitigate this catastrophe is to act together, or at least to act in ways that are closely attentive to others, not blind or negligent towards them. Many of us are now hunkering down at home, and it’s all but natural that we focus on immediate day-to-day needs, the health of loved ones, saving our work or our livelihoods, in our entirely up-ended lives. But if our claims to human empathy have any meaning at all, then now is a good time to think of building up a pan-European chorus of voices for solidarity. Sure, it won’t in itself bring us any closer to a vaccine, nor immunise us against the virus, but it could help immunise us against something else – the nasty undercurrents of nationalism that are lurking under the surface. As we Europeans stand at the epicentre of it all, it’s up to us to make solidarity viral.  • Natalie Nougayrede is a Guardian columnist "
"
Greg Craven
by Steven Mosher
In the last episode of  “Craven Attention” I recounted some of the things Greg Craven said during a panel discussion after Oppenheimer’s lecture of the role of scientists. [GC33D The 2010 Stephen Schneider Global Environmental Change Lecture (Webcast)Moscone South, Gateway Ballroom, Room 103, 1345h–1440h Scientists, Expert Judgment, and Public Policy: What is Our Proper Role? Presented by M. Oppenheimer, Geosciences, Princeton University]
Greg seemed to take issue with my characterization of some of his comments.
I have no problem with analysis and criticism of my presentation, but I do feel strongly that the facts of it be correctly conveyed, as I have already been significantly misquoted. I expect that you do not appreciate having your statements mischaracterized or misquoted either…

But I believe some of your characterizations of what I said to be misrepresentative. You are of course free to give your assessment of my presentation, demeanor, or state of mental health. But everyone in the debate says “look at the facts and let them speak for themselves.” I ask that you do the same thing and limit yourself to quoting my actual words, criticizing them and myself as you will, without taking upon yourself to characterize what I said. I am painfully aware that I am a pathological overtalker and can’t be succinct to save my life.
I do not expect you to agree with my words or me. But I do expect you have the discipline and principle to convey the speech accurately, rather than settling for your interpretations and summaries of what I said (as you did in the “Basically it goes like this…” set-out). I’m sure that you’ll agree that characterizing your opponent’s words yourself does no service to forwarding the discussion.

And he seeks to vindicate himself by posting a transcript of  the episodes.
And my remarks have already been mischaracterized and misquoted, to further malign the AGU.  In the interests of accuracy and truthful reporting, I will post an audio file and transcription of my presentation as given at www.gregcraven.org as soon as I can.
He has now posted a transcript of a different presentation he gave earlier  in the day. Huh? Strangely the audio that produced that transcript is still not available. The problem is my piece covered a different episode. He posted a transcript of the meeting at 1020 AM on the 15th and I covered the panel that followed Oppenheimer who spoke at 13:40-14:00. Still, we can note some things and see if it’s possible that I got the gist of what Craven was saying correct. That is, by looking at the first transcript we can see that my characterization of the second speech is not implausible. Let’s just say the second presentation was a good model for the first.
First lets note this. The Craven who cares about being misquoted had this to say; take special care to note his definition of the meaning of communication below:
my message to you now is that you must stop communicating as scientists. You must begin communicating as citizens, as a father, as a mother, with whatever feelings are in your heart, with your fears, speak to them of your hopes, let them know about your befuddlement at the divergence. And tell them frankly, forthrightly, sincerely, about any terror that you are ignoring…..
You say you want to have an effect on the public? If you trod a journey at all similar to mine, think, visualize, take five minutes to meditate on the impact it would have if you took off your goddamned scientist hat for just a moment, and put on your citizen hat. And said frankly to the public through the largest mouthpiece you can: “As a scientist, here’s my understanding. As a citizen, here’s my hope, my vision. And as a mother, here’s my contingency plan, here’s my lifeboat.”….
If you obliterated your comfort zone and the hard line of purity of your scientific sensibilities–that you do cling to, with the faith of a god–and you actually went forth as an actual advocate, a sentiment normally anathema to the constitution of a scientist, imagine if you went out into the fray bearing your heart, with your emotion and the authority of your understanding as your weapon. For what you’ve been giving them as a scientist up to now is information, and with that increasing divergence between public and scientific opinion…
You must stop selfishly pursuing your pleasure in finding things out. To be frank: f*** your research. We. Need. You. I know I am almost certain to outrage you with my impertinence and the audacity of my message. And my word choice, for substituting ‘f*** for ‘screw’. [Mild laughter.] And that’s the lesson you must absorb into the fiber of your being, for the meaning of communication is not what you intend, or the information. The meaning of communication is the response it elicits in the listener. And that’s where we have failed. So while you may be likely to forget the details of my rant, you will always feel the emotional aftertaste of it. And that is the purpose of communicating the science of climate change to the lay public. To give them an emotional aftertaste.
Your role, your job–the one we have assigned you and gladly supported–has always been to stand on the hill overlooking the bloody battlefield and give reconnaissance and convey information about what’s ahead. But there comes a time in the last stand for every single support troop, no matter how far removed, to pick up a weapon, come down into the fray, and fight to the death for what they stand for. To charge into the face of annihilation itself and fight with their teeth, tearing out the jugular of their enemy with their bloody mouth if they have no weapons left. That time is now.
If you do not believe that, if you do not feel that, I challenge you to be intellectually honest–that part of you that you hold up as better than any other profession, and I support you in that opinion–you are the only rational thinkers on the planet. Beware, psychological research shows that people don’t generally make decisions rationally. If you don’t agree with this–that this is the time to radically challenge your comfort zone, and your traditional mores of never letting feelings or opinions on policy pass your lips–I’m not going say “If not now, then when?” I’m going to say: detail an operational definition of a test to test whether a situation would merit that extreme action or not. Come up with the characteristics. And then I defy you to compare them to the situation now. If you do that, forget everything I’ve said. I absolve you. That’s all I ask. But if your intellectually honest operational definition tells you that the time is now. . . .
These snippets are from his earlier speech. However, in the panel after Oppenheimer’s talk he gave a similar version of the “comfort zone”  challenge. The “emotional aftertaste” I was left with after I forgot the details of his second rant was this:
Craven took charge again and argued the “if not now, when” argument.Basically, it goes like this. As a scientist you have to decide  at some point that enough is enough. You have to put your scientific commitment to the discipline of doubt aside and “blow past” your boundaries.  Say what you feel, not what you can prove….
Steve Easterbrook, thankfully, asked the only intelligent question. On one hand we have Oppenheimer telling us take care when going beyond our expertise. On the other hand we have Craven, saying “blow past” your boundaries. Oppenheimer tried to paper over the difference, and Oreskes, who seemed to be shooting me looks as I sat there laughing, agreed that there was a difference between these views. Craven, breaking his promise again, read what he had been scribbling. Some sort of challenge to climate scientists that he promises to post.
I apologize if I got it “wrong,” but on Greg’s view my emotional “aftertaste” IS the meaning of what he said. I guess those years of studying Stanley Fish and Roland Barthes came in handy. Personally, I want scientists to keep their science hat on at all times. Others can panic without any practice or education. To be fair to Greg and to present his argument a bit more precisely and rigorously  he seems to want  scientists to speak emotionally about policy while retaining their objectivity in science. Except, for the ” f*** your research part”  which is a bit hard to square with things. Craven thinks scientists research because they take pleasure in it. Removing doubt and uncertainty is an equally likely motivation. So there he seems to be saying they should put their desire to remove doubt and uncertainty aside in favor of passion.  Oppenheimer’s point, on the other hand, was this: as an expert you have a problem. People make take your positions on policy to be expert scientific opinions, when they are not.  And my point would be this. The passion for policy is part and parcel of the problem of trust in climate science. For Craven, the “understanding” drives the passion. But for many of the people that need to be convinced the displays of passion undermine trust in the science. That’s their emotional aftertaste.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e864be554',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Two-week quarantine restrictions apply to people entering the UK from almost every country, because of the second wave of coronavirus.**
From the middle of December, travellers will be able to pay for a private test, which will cut their self-isolation time to just under a week if it comes back negative.
In England, people cannot travel abroad for non essential reasons until 2 December.
People arriving in the UK from most countries - including British nationals - must **self-isolate for 14 days**.
Exceptions are made for people coming from the Common Travel Area - Republic of Ireland, Channel Islands, or the Isle of Man - or countries in travel corridors with the UK.
Travellers must fill in a ''passenger locator'' form, with contact details and their UK address. Anyone who does not provide an address will have to pay for accommodation arranged by the government.
For 14 days, starting the day after arrival, people quarantining **should not:**
If you have to self-isolate after a trip you may not get statutory sick pay, unless you meet the required conditions \- such as displaying coronavirus symptoms.
Scotland,Wales and Northern Ireland have brought in their own rules, which vary slightly.
From 15 December, people arriving in England will be able to cut their quarantine period by at least half if they pay for a Covid test after five days.
The tests will cost between Â£65 and Â£120 and the results will normally come back within 24 to 48 hours. This means people could stop self-isolating six days after arrival if they test negative.
Breaking quarantine rules is a criminal offence, and people who do it face a fine and potentially a criminal record.
Those not self-isolating when they are supposed to can be fined Â£1,000 in England, Wales and Northern Ireland, or Â£480 in Scotland. Fines in England for persistent offenders have doubled to Â£10,000.
People can be fined up to Â£3,200 in England if they do not provide accurate contact details, or Â£1,920 in Wales.
There is also a fine of Â£100 for not filling in the passenger locator form.
A small number of jobs offer exemption from quarantine rules. These include:
People living in Wales are currently not allowed to travel abroad for a holiday, but can still make work trips.
In England, leaving home in order to travel for holidays before 2 December can be punished by a fine, with penalties starting at Â£200 and going up to Â£6,400. After that, non-essential travel is allowed.
There are currently only a handful of places that travellers from England can visit without encountering restrictions either when they arrive at their destination, or when they return.
These include:
The Joint Biosecurity Centre \- set up by the government to monitor coronavirus - advises on which destinations should be on the list.
In the past, the decision appears to have been made when 20 or more people out of every 100,000 in a country, or island, are infected over seven days, but other factors are also considered. These include:
**Are you planning to travel to or from the UK? How will the quarantine regulations affect you? Share your experiences by emailing**haveyoursay@bbc.co.uk **.**
Please include a contact number if you are willing to speak to a BBC journalist."
"

The Kansas Legislature has wisely written a proposed tax on carbon dioxide emissions out of this year’s energy legislation. That’s the good news: As originally written by the Committee on Utilities, the Sunflower Energy bill’s CO2 tax would have been a first, and a very bad precedent. The bad news is that the original bill will be copied and wind up before other legislatures that are more likely to pass it, like those of California and Oregon.



A CO2 tax will largely be levied on utilities that exceed modest limits on their carbon dioxide effluent, so consumers won’t “see” it — except in their electric bills. They’ll send in their monthly checks, quite unaware that the new tax revenues are likely to be shoved into a slush fund for solar energy, windmills, biodiesel, ethanol and other green gadgetry boondoggles.



Never mind that even the _New York Times_ now acknowledges that biofuels add more carbon dioxide to the atmosphere than the equivalent amount of conventional fuels, or that the diversion of a third of the U.S. corn crop to ethanol production has driven world food prices up so much that we are now witnessing riots, including a major one in Jakarta last month.





The satellite temperature surveys also show there has been no net global warming since 2000.



Let’s just consider the merits of this legislation vis‐​a‐​vis some pretty well‐​known (if poorly publicized) global warming science.



Further, we’ll cheat a bit and stipulate that the bill results in a 10% net reduction of carbon dioxide emissions, and that global warming fever sweeps the nation, resulting in similar legislation passing in every other state.



Based upon a widely accepted formula originated at the U.S. National Center for Atmospheric Research in Boulder, Colorado, if the entire United States adopted the original Kansas legislation, it would prevent a total of 0.11 degrees F of global warming _per century_. Read that again, because it’s not a typo: Eleven one‐​hundredths of a degree in 100 years.



Instead, let’s apply the original Kansas legislation to every nation on the planet that agreed to limit its emissions under the infamous 1997 Kyoto Protocol, an amendment to a 1992 United Nations global climate treaty that would require the U.S. to reduce emissions far beyond what was written out of the Kansas bill. The new law would prevent 0.27 degrees F of warming per century. That’s an amount too small to measure, because global temperatures vary by more than that from year‐​to‐​year — global warming or not.



Since 1979, satellites have been measuring lower atmospheric temperatures around the globe. In the last 12 months, they show that the earth’s mean temperature has dropped by 1.13ºF. Thus, in one year, that natural variability is four times greater than the amount of warming that would be prevented if the entire industrialized world adopted the original Kansas statute.



The satellite temperature surveys also show there has been no net global warming since 2000. It’s a little unfair to go back much further in this discussion, because 1998 was an extremely hot year — the high point in both satellite and land‐​based temperature histories — because of a huge El Nino (which, incidentally, proved to be a great boon to Kansas’s wheat farmers).



All of which is to say that global warming isn’t exactly proceeding apace. Rather, the rate of planetary warming is falling in line with the low end of 21st century projections made by the UN’s Intergovernmental Panel on Climate Change, with the smart money now riding on a bit more than 3 degrees F of warming this century. It’s worth noting that the 20th century saw about half of that warming, along with a doubling of life expectancy in the industrialized world, and an approximately ten‐​fold increase in real personal wealth.



But we hear over and over that if we don’t “do” something serious about carbon dioxide emissions in the next eight years (a conveniently presidential number), we are condemning ourselves to an unmitigated climate disaster, as much of Greenland’s ice crashes into the sea, raising sea level as much as 20 feet.



That’s about as likely as a bill limiting CO2 emissions in Kansas putting a detectable dent in global warming. Congratulations to the legislature for its wisdom in writing out the carbon tax. But beware, electronic copies of the original are flying around the country, looking for places to land.
"
"

Well here’s an interesting, if three‐​weeks‐​old, story. Apparently the North Dakota Farm Bureau’s annual convention recently passed a policy calling for the elimination of all agricultural programs. Reading between the lines of the original press release indicates that the call was part of a broad political position by the NDFB to move away from government intervention in many areas of the economy apart from farm programs, including cap‐​and‐​trade and health care:   




“As people in this country expect more from the government and less from themselves, our delegates are urging everyone, including farmers, to step away from the public trough and get back to the principles of individual responsibility and initiative,” said NDFB President Eric Aasmundstad.…   
The only way government can get money is to take it from its citizens. We don’t believe raising taxes to pay for health care or climate change will help our country get out of our economic slump or even improve health care or the environment. And the more they take, the less we have to find the innovative solutions to the problems we face.



He sounds like a Catoite.   
  
  
To what extent the NDFB’s position flows through to the rest of the farm lobby remains to be seen, so hell hasn’t quite frozen over yet. But this is positive news. At the very least it spells sweet, sweet trouble for long‐​time free trade nemesis and farm bill supporter Sen. Byron Dorgan (D), who is up for reelection next year.   
  
  
HT: Chris Edwards.
"
"**Leicester and Leicestershire will be subject to the toughest tier of restrictions after the national lockdown ends on 2 December.**
The city and county will move from tier two to three, meaning very high risk, the government announced.
It means household mixing is banned and pubs and restaurants will close except for delivery and takeaways.
The city and parts of the county were subject to the UK's first local lockdown in June.
The neighbouring county of Rutland will go into tier two of the government's three-tier system.
Leicester has been subject to some level of coronavirus restrictions since the first national lockdown in March.
The government has set out the reasoning behind the tier decisions for each area.
In a written ministerial statement, the government said of Leicester and Leicestershire: ""Improvements have been seen in overall case rates in all but one lower tier local authority, but remain very high at 355 per 100,000, including in over 60s at 250 per 100k. The pressure on the local NHS remains very high.""
Health Secretary Matt Hancock told the Commons: ""I know how tough this is, both for areas that have been in restrictions for a long time like Leicester and Greater Manchester, and also for areas where cases have risen sharply.""
The system will be regularly reviewed - with the first scheduled for 16 December.
The new allocations will put some areas under significantly tighter restrictions than before the second lockdown started.
Places like Market Harborough and Lutterworth have managed to remain in tier one since the system was first introduced, but they along with the rest of the county will go into tier three.
Reacting to the news, the county's director of public health Mike Sandys said: ""Over the past few days, rates have started to fall and we've made some progress. But it's important to put this into perspective.
""Figures are over 20% down compared to this time last week but they're still worse than the day we went into lockdown.
""Leicestershire's average is significantly higher than the national level so there is still work to do.""
In a joint statement, the three Labour MPs for the city - Claudia Webbe, Liz Kendall and shadow health secretary Jonathan Ashworth - said: ""The news that Leicester will go into tier three - on top of the 150 days of our extra lockdown - is extremely difficult to hear.
""The government must now spell out how we can get out of tier three, and the measures they will use to review Leicester's position, to give people hope their sacrifices will make a difference.""
Meanwhile, the Conservative MP for Rutland and Melton said she is pleased about the decision to put Rutland into tier two.
Alicia Kearns said: ""I welcome that Rutland has been respected as the independent county it is and therefore tiered separately.""
But she added she was ""deeply disappointed"" the likes of Melton and Harborough had been grouped with all of Leicestershire.
Leicester had a seven-day coronavirus infection rate of 398.3 per 100,000 people for the week to 21 November - the 14th highest rate in England.
The number of confirmed cases in the same week was 1,411, down from 1,857 in the seven days up to 14 November. The infection rate is also down from 524.2.
The borough of Oadby and Wigston has the county's highest rate of 417.4 - putting it 11th nationally - but the rate has also decreased from 526.2.
The average for the whole of England is 208.7.
The Christmas lights might be up in Leicester, but let's be honest, not many people are going to see them.
It has the title that no city wants - it has been in Covid-restrictive measures for longer than anywhere else in the country.
Today's news from Health Secretary Matt Hancock has confirmed what Leicester's mayor Sir Peter Soulsby and many people here feared - that the city is going into the highest tier.
That's tough news for those here and means it has been eight months since people have not been allowed to have family and friends in their homes.
Some non-urgent operations in Leicester have been cancelled, with one local health boss fearing the second wave of the virus will be worse than the first.
The Leicester, Leicestershire and Rutland's clinical commissioning groups (CCGs) said their hospitals were treating 260 people with coronavirus, compared with 204 at the peak in April.
Last week Leicester mayor Sir Peter Soulsby said the ""hopeless"" performance of the national test and trace system had contributed to the recent surge in city cases.
Both the city and county have been sent thousands of rapid result lateral flow tests to help bolster Covid-19 testing.
_Follow BBC East Midlands on_Facebook _,_Twitter _, or_Instagram _. Send your story ideas to_eastmidsnews@bbc.co.uk _._"
"

 _The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels and Paul C. “Chip” Knappenberger, from Cato’s Center for the Study of Science, review interesting items on global warming in the scientific literature or of a more technical nature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
\---   
  
Despite what you may think if you reside in the eastern United States, the world as a whole in 2014 has been fairly warm. For the past few months, several temperature-tracking agencies have been hinting that this year may turn out to be the “warmest ever recorded”—for whatever that is worth (keep reading for our evaluation). The hints have been turned up a notch with the latest United Nations climate confab taking place in Lima, Peru through December 12. The mainstream media is happy to popularize these claims (as are government-money-seeking science lobbying groups).   
  
But a closer look shows two things: first, whether or not 2014 will prove to be the record warmest year depends on whom you ask; and second, no matter where the final number for the year ranks in the observations, it will rank among the greatest “busts” of climate model predictions (which collectively expected it to be _a lot_ warmer). The implication of the first is just nothing more than a jostling for press coverage. The implication of the latter is that future climate change appears to be less of a menace than assumed by the president and his pen and phone.   
  
Let’s examine at the various temperature records.   
  
First, a little background. Several different groups compile the global average temperature in near-real time. Each uses slightly different data-handling techniques (such as how to account for missing data) and so each gets a slightly different (but nevertheless very similar) values. Several groups compute the surface temperature, while others calculate the global average temperature in the lower atmosphere (a bit freer from confounding factors like urbanization). All, thus far, only have data for 2014 compiled through October, so the final ranking for 2014, at this point in time, is only a speculation (although a pretty well-founded one).   
  
The three major groups calculating the average _surface_ temperature of the earth (land and ocean combined) all are currently indicating that 2014 will likely nudge out 2010 (by a couple hundredths of a degree Celsius) to become the warmest year in each dataset (which begin in mid-to-late 1800s). This is almost certainly true in the datasets maintained by the U.S. National Oceanographic and Atmospheric Administration (NOAA) and the UK Met Office Hadley Centre. In the record compiled by NASA’s Goddard Institute for Space Studies (GISS), the 2014 year-to-date value is in a virtual dead heat with the annual value for 2010, so the final ranking will depend heavily on the how the data come in for November and December. (The other major data compilation, the one developed by the Berkeley Earth group is not updated in real time).



There is one other compilation of the earth’s surface temperature history that has recently been developed by researchers Kevin Cowtan and Robert Way of the University of York. This dataset rose to prominence a year ago, when it showed that if improved (?) methods were used to fill in data-sparse regions of the earth (primarily in the Arctic), the global warming “hiatus” was more of a global warming “slowdown.” In other words, a more informed guess indicated that the Arctic had been warming at a greater rate than was being expressed by the other datasets. This instantly made the Cowtan and Way dataset the darling of folks who wanted to show that global warming was alive and well and not, in fact, in a coma (a careful analysis of the implications of Cowtan and Way’s findings however proved the data not up to that task). So what are the prospects of 2014 being a record warm year in the Cowtan and Way dataset? Slim. 2014 currently trails 2010 by a couple hundredths of a degree Celsius—an amount that will be difficult to make up without an exceptionally warm November and December. Consquently, the briefly favored dataset is now being largely ignored.   
  
It is worth pointing out, that as a result of data and computational uncertainty, _none_ of the surface compilations will 2014 be statistically different from 2010—in other words, it is impossible to say with statistical certainty, that 2014 was (or was not) the all-time warmest year ever recorded.   
  
It is a different story in the lower atmosphere.   
  
There, the two groups compiling the average temperature show that 2014 is nowhere near the warmest (in data which starts in 1979), trailing 1998 by several _tenths_ of a degree Celsius. This difference is so great that it statistically clear that 2014 will not be a record year (it’ll probably fall in the lower half of the top five warmest years in both the Remote Sensing Systems (RSS) and the University of Alabama-Huntsville (UAH) datasets). The variability of temperatures in the lower atmosphere is more sensitive to the occurrence of El Niño conditions and thus the super El Niño of 1998 set a high temperature mark that will likely stand for many years to come, or at least until another huge El Niño occurs.   
  
Basically, what all this means, is that if you want 2014 to be the “warmest year ever recorded” you can find data to back you up, and if you prefer it not be, well, you can find data to back up that position as well.   
  
In all cases, the former will make headlines.   
  
But these headlines will be misplaced. The real news is that climate models continue to perform incredibly poorly by grossly overestimating the degree to which the earth is warming.   
  
Let’s examine climate model projections for 2014 against the observations from the dataset which has the greatest chance of 2014 as the warmest year—the NOAA dataset.   
  
Figure 1 shows the average of 108 different climate model projections of the annual surface temperature of the earth from 1980 through 2014 along with the annual temperature as compiled by NOAA.   




  




_Figure 1. Global annual surface temperature anomalies from 1980 to 2014. The average of 108 climate models (red) and observations from NOAA (blue) are anomalies from the 20 th century average. In the case of the NOAA observations, the 2014 value is the average of January-October._   
  
**For the past 16 straight years, climate models have collectively projected more warming than has been observed.**   
  
Over the period 1980-2014, climate models projected the global temperature to rise at a rate of 0.24°C/decade while NOAA observations pegged the rise at 0.14°C/decade, about 40 percent less. Over the last 16 years, the observed rise is nearly 66 percent less than climate model projections. The situation is getting worse, not better. This is the real news, because it means that prospects for overly disruptive climate change are growing slimmer, as are justifications for drastic intervention.   
  
We don’t expect many stories to look any further than their “2014 is the warmest year ever” headlines.   
  
As to the rest of the picture, and the part which holds the deeper and more important implications, well, you’ll have to keep checking back with us here—we’re happy to fill you in!


"
"Ash dieback – a fatal disease of Britain’s native ash trees (Fraxinus excelsior) – is one of the worst tree disease epidemics the UK has ever seen. The disease is caused by a fungus that originated in Asia but is thought to have arrived in Europe on exotic plants in the early 1990s, where it has devastated native ash species which have very little natural immunity. Ash dieback has since spread ferociously throughout Europe due to airborne spores and trade in ash saplings which have no visual symptoms of the disease. In 2012, the disease was confirmed in the UK and later shown to have been imported on saplings to multiple sites across the country. It is now found throughout the UK. There’s no cure and very few trees show signs of long-term resistance. The environmental impacts of the disease are likely to last a long time, but as our new paper explains, they’ll also carry a shockingly high economic cost. There are 150m mature ash trees in the UK, making ash one of the most common native tree species in the country. We estimate that ash dieback will kill at least 95% of ash trees and cost the UK economy £15 billion – a cost one third greater than that reported from the foot-and-mouth disease outbreak in 2001. Half of this cost will arise in the next ten years. Putting a monetary value on ecosystem services – the beneficial effects that trees provide for people and the economy - helps people understand the scale of the problem. Roughly £10 billion worth of ecosystem services will be lost as ash trees disappear. Losing these services will have wide-ranging consequences. Less carbon dioxide will be absorbed from the atmosphere and the risk of flooding will increase. Studies have also shown that losing trees from a community is linked to poorer physical and mental health among the people who live there. Tackling climate change calls for an enormous effort to plant trees but ash dieback will rob the UK of using this valuable native species. Clearing up dead and dying ash trees will carry another major cost, particularly where they present a risk to human safety. Stricken ash trees are prone to shedding limbs or collapsing completely, either directly due to the ash dieback fungus or a secondary pathogen such as honey fungus infecting the weakened tree. More than 4m ash trees line Britain’s roadsides. Felling these will be expensive and involve road closures and power and communications outages as work is carried out.  Ash trees in towns and cities will need the same treatment. A major national replanting effort could reduce the total cost of losing ash trees by as much as £2.5 billion, but a diverse mixture of native species will need to be planted to improve the resilience of new trees to pests and diseases. Replanting should also be carefully managed to ensure habitats are connected throughout the landscape. Exotic disease is not a problem limited to ash trees. People move plants – and unwittingly, their diseases – around the world at rates that far outstrip natural disease spread. The international trade in plants, travel and climate change are all contributing to an acceleration in the rate of new tree diseases emerging and spreading.  More tree pests and diseases have arrived in Britain in the last 40 years than at any time before then. As more native species are threatened, the effects will combine and multiply. Losing most ash trees will be bad enough, but what if the UK loses oak next, or birch? The idea of a landscape largely devoid of trees is appalling, and the economic costs incalculable. People aren’t powerless in this story though. The science is clear that the largest pathway for spreading tree diseases is the international trade in live plants and soil. Stricter controls on this trade could better protect our trees for generations to come. Most countries prioritise the value of trade in live plants over the risks to their native flora. Our paper shows that the value of the annual trade in ash saplings amounted to only 2% of the estimated cost of ash dieback. The costs of restricting trade and improving border controls have long been used to block the introduction of stronger biosecurity measures for plants. But we now know that the costs of diseases like ash dieback have been wildly underestimated and this new evidence demands an urgent rethink. The health of native trees, in fact of all wildlife, needs to be valued far more highly. We must recognise not only the essential benefits that the natural environment provides for us, but how severe the consequences are for society when new pathogens are spread."
"**Greater Manchester's mayor said he hopes the region will only face the toughest government coronavirus restrictions for a couple of weeks.**
Areas with the highest infection rates, including Greater Manchester, will go into tier three when lockdown ends.
Mayor Andy Burnham said that, although cases of coronavirus were still high locally, rates were declining.
He said if they continue to fall ""we will be making the strongest possible argument"" to be moved to tier two.
The area's tier level may change before Christmas with a review scheduled for 16 December.
The new tier rules will come into force on Wednesday. In tier three, people can only meet other households in outdoor public spaces like parks, where the rule of six applies.
Gyms and close-contact beauty services like hairdressers will be able to open but in tier three, pubs and restaurants can only operate as a takeaway or delivery service.
Mr Burnham said: ""I feel for people who have been living under restrictions for a long time now,"" and called for additional support for businesses.
""The new tier three will hit the hospitality sector extremely hard. While there are grants for businesses forced to close, there is no extra support for business which supply them like security, catering and cleaning.
""This will cause real hardship for people whose jobs will be affected and risk the loss of many businesses.""
In October, Mr Burnham was involved in a lengthy public dispute with the government over Greater Manchester being placed in tier three and the level of financial support.
Health Secretary Matt Hancock said the battle had been ""bad for public health"" and ruled out negotiations with local leaders in favour of a formula to decide which areas are placed in what tier.
Greater Manchester's night-time economy adviser, Sacha Lord, said: ""We're in tier three and we're not surprised"".
But he added ""the R rate is dropping rapidly, so it does feel like if we carry on like this, we could be in tier two shortly"".
**Tier three (very high)**
William Wragg, Conservative MP for Hazel Grove, said he will ""most likely be voting against these measures"".
""What I've been pushing for is for the different boroughs in Greater Manchester to be treated separately given the range of data that we have about the prevalence of the virus,"" he said.
UnitedCity - a group of business leaders across Manchester- said the news was ""a massive blow to hospitality, leisure, culture, events and sports businesses based in the region"".
""Data has clearly shown that cases of the virus were starting to fall before the November lockdown, so for restrictions as harsh as the 'new' tier three ones are to be placed upon us does feel somewhat rancorous.""
Karen Hill, a hairdresser in Oldham, said she is glad she can reopen but she expects business to be quiet: ""I do feel better that we can open, but because there's not going to be Christmas dos, it's not going to be our normal December.
""No-one's going to be going anywhere in January and February, so we're not going to have the new year, new me.""
_Why not follow BBC North West on_Facebook _,_Twitter _and_Instagram _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
"Microplastics have been discovered in a remote area of the French Pyrenees mountains. The particles travelled through the atmosphere and were blown into the once pristine region by the wind, according to a new study published in Nature Geoscience.  This is just the latest example of the “hidden risks” posed by plastics that humans cannot see with the naked eye. For now, governments and activists are focused on avoiding plastic litter in the environment, driven mainly by concern for wildlife and worries over unsightly drinks bottles or abandoned fishing nets on beaches. Plastic bag usage has been cut in many parts of the world, and various projects are exploring how to gather up the floating plastic waste in oceans. But little has yet been done to deal with polluting plastic particles that are usually invisible. There is however growing concern about these micro and nanoplastics, classified as particles smaller than 5mm. These come in part from deliberately manufactured sources, such as scrubbing materials in cleaning and cosmetic products, but also from secondary sources, such as the inevitable breaking up or wearing down of larger items such as tyres or fibre shed from tumble driers and washing machines. We are becoming increasingly aware of their presence but know surprising little about how much is out there, how it behaves in our environment, and what the implications are for human and animal well-being. As more studies publish their findings we are learning that microplastics are more widespread than we imagined, and that they are found in every environmental system investigated. Plastic particles have been found in record-breaking quantities in river sediments in the UK, for instance, while a study in Paris found plastic fibres in wastewater and the air.  This is perhaps to be expected in built up and polluted urban environments, but the new findings from the Bernadouze meteorological station in the Pyrenees are a different matter. This part of the mountain range is normally considered clean and pristine, not somewhere scientists would expect to find contamination. But the researchers looked for airborne plastic by collecting samples of atmospheric “fallout” over a five-month period. And they did indeed find microplastics, lots of them, in the form of tiny fragments, fibres and films. While their exact source is a mystery they were shown to have potentially travelled up to 95km. Particles have also been found in deep ocean floor sediments, far from immediate sources of pollution, carried there by ocean currents and settling slowly. Other research has identified some astonishing ways microplastics can move between one environmental sub-system and others. Alongside the obvious route of direct ingestion by animals who become prey for others higher in the food chain, it is now apparent that there are other more innocuous routeways, such as mosquito larvae in water ingesting plastics that are then retained in their bodies as the animals become flying insects. This releases particles into the atmosphere allowing them to float for thousands of miles, or to be inhaled. The amount of plastic in the environment has increased and we are still making lots more. It stands to reason that microplastics are going be with us for a while yet, since plastic itself has many beneficial uses. If these fragments were unreactive and harmless they would not pose a threat, but unfortunately the risks are not yet fully understood. Alongside the issues associated with inadvertent ingestion of large volumes of material without any nutritional value, there are some hidden risks. Microplastics have a relatively large surface area and so could potentially provide sites for surface reactions and act as rafts for organic pollution. Given that microplastics are turning up in drinking water and food we need to do more work to understand the risks to health and work out ways to manage this risk. A study that found microplastics in a fish liver raised concerns that plastic can cross the gut if it is ingested. The trouble is, these plastics are so small they are not easy to remove from the environment once they get there. The key is preventing their escape into the environment in the first place. Focusing on the bigger plastics we can see may be a distraction from this potentially larger problem in the air we breathe and the food we eat, but tackling the problem at the source could go a long way to helping with damage limitation."
"The record temperatures of summer 2019 helped make it the best season for butterflies in 22 years, with more than half of Britain’s species increasing in number. Last summer delivered a winning combination of warmth, sunshine and rain which ensured that caterpillars fed up on lush plants before emerging as adult butterflies.  The marbled white enjoyed its best year since scientific monitoring began in 1976, continuing its climate-assisted march northwards, and the dark green fritillary enjoyed its third best year on record, its numbers up 51% on the previous year. The drought of 2018 led to fears of a crash in butterfly populations, particularly among species whose caterpillars feed on grasses that shrivelled up that summer. Unexpectedly, however, the annual UK Butterfly Monitoring Scheme revealed that plenty of grass-feeding species thrived in 2019 including the ringlet, enjoying its second-best year on record, the meadow brown, which had its fifth-best year, and the rare Lulworth skipper, numbers of which increased by 138% on the previous year. The monitoring of 3,014 sites by volunteers supported by Butterfly Conservation, the UK Centre for Ecology & Hydrology, the British Trust for Ornithology and the Joint Nature Conservation Committee, showed that summer 2019 was also a notable year for the painted lady. The migratory butterfly flew in from Africa via continental Europe in greater numbers than in any summer apart from 2009 and 1996. Prof Tom Brereton of Butterfly Conservation said the encouraging results “provide evidence that the overall rate of decline of butterflies is slowing and for some species being reversed”. He said conservation through agri-environment schemes, increased woodland cover, climate warming and “increases in grazing levels by wild animals and a slowing in the rate of agricultural intensification” had all played their part. Dr Marc Botham of the UK Centre for Ecology & Hydrology said two warm summers in succession, which allowed population numbers to build up over a longer period, had also contributed to the boom year. “In addition to record numbers of spring species such as orange-tip and brimstone, it was also encouraging to see annual increases in garden favourites such as peacock and small tortoiseshell, both of which have had some poor years recently,” he said. Some species, however, are struggling to adapt to rapid climatic changes. The rare heath fritillary saw its annual abundance drop by 34%. Its numbers have fallen by 91% over the long term as a result of climate change and a loss of traditional coppicing in woodlands and grazing on moorland. The figures show that targeted conservation work is helping many rarer species, including the chequered skipper, up by 175% on 2018, and the Duke of Burgundy, which a decade ago was threatened with extinction but recorded its eighth best year since 1976. Other species benefiting from habitat management include the marsh fritillary, silver-studded blue and silver-spotted skipper, none of which are now in long-term population decline. “We’re really heartened to see a shift in the fortunes of many of our most loved species,” Brereton said. “The long-term situation for butterflies in general does remain a cause of concern though, with more species declining than increasing since the 1970s.”"
"
Josh, of Cartoons by Josh has a take on the attendance of COP16 at Cancun, said to be down more than 50% over last year’s COP15 shindig in Copenhagen. 



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e87546866',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Anyone who has ever spent time observing wild animals in nature will know that silence is golden. Wildlife tours recommend that people stay quiet in order to see more, but research on Tibetan macaques suggests that high levels of noise from tourists can also lead to more aggressive behaviour. Although wildlife tourism can generate funds for conservation and sustainable work for local people, these benefits may be meaningless if visitors negatively impact animals and their habitats. Is the mere presence of humans off putting to wild animals or does it all have to do with the noise they make? By playing recordings of people talking from speakers in the animals’ environment, scientists can observe how wild animals respond to human speech alone. Large canivores such as wild pumas in California and Amazonian bird communities were shown to flee when they heard human voices. Fleeing is useful to escape potentially dangerous situations, but it comes at a cost for animals and tour companies as paying tourists see less wildlife and animals exert themselves. This leaves animals stressed and less able to reproduce and could mean they go hungry if they have to give up food in the area they fled from.  Our study found that the wildlife was less likely to flee from humans if speech was quieter. By simply asking people to be as quiet as possible, the full benefits of wildlife tourism to animals and people could be realised. We conducted a playback study with wild pygmy marmosets in the flooded forest of the Peruvian Amazon to see if longer or louder speech was more disruptive to their behaviour. Pygmy marmosets are the world’s smallest monkey, and family groups form a small territory around the gum producing trees which they feed on. For each experiment, the behaviour of a single pygmy marmoset in the group was recorded on video for four minutes.  Two minutes into the recording, a sound was played from a speaker positioned on a boat. We then categorised the behaviours shown in these videos and compared the behaviour of individuals before and after the playback. When played recordings of human speech, pygmy marmosets fed and rested less and spent more time in an alert posture. These effects were observed at all volumes and durations of human speech but there was no change in their behaviour when humans were present but no voice was played.  Like the puma and hoatzin, pygmy marmosets moved away when human speech was played, even at the volume of a whisper. Although the duration of the speech had no effect, the louder the playback the more likely individuals were to move away. As a pygmy marmoset family group can depend on a single feeding tree in their territory, fleeing from these places can have serious consequences. These results were surprising as some of the ten experimental groups were regularly visited by tourists, and two groups were even located in back gardens in a local village. These animals are regularly exposed to humans, but still find human speech – and particularly loud speech – disturbing.  So wildlife tourism can be good for animals and humans if tourists lower their voices – even whispering can disturb animals and may allow tourists little more than a fleeting glance of wildlife."
"Whenever I visit the Sahara I am struck by how sunny and hot it is and how clear the sky can be. Aside from a few oases there is little vegetation, and most of the world’s largest desert is covered with rocks, sand and sand dunes. The Saharan sun is powerful enough to provide Earth with significant solar energy. The statistics are mind-boggling. If the desert were a country, it would be fifth biggest in the world – it’s larger than Brazil and slightly smaller than China and the US. Each square metre receives, on average, between 2,000 and 3,000 kilowatt hours of solar energy per year, according to NASA estimates. Given the Sahara covers about 9m km², that means the total energy available – that is, if every inch of the desert soaked up every drop of the sun’s energy – is more than 22 billion gigawatt hours (GWh) a year.  This is again a big number that requires some context: it means that a hypothetical solar farm that covered the entire desert would produce 2,000 times more energy than even the largest power stations in the world, which generate barely 100,000 GWh a year. In fact, its output would be equivalent to more than 36 billion barrels of oil per day – that’s around five barrels per person per day. In this scenario, the Sahara could potentially produce more than 7,000 times the electricity requirements of Europe, with almost no carbon emissions. What’s more, the Sahara also has the advantage of being very close to Europe. The shortest distance between North Africa and Europe is just 15km at the Strait of Gibraltar. But even much further distances, across the main width of the Mediterranean, are perfectly practical – after all, the world’s longest underwater power cable runs for nearly 600km between Norway and the Netherlands. Over the past decade or so, scientists (including me and my colleagues) have looked at how desert solar could meet increasing local energy demand and eventually power Europe too – and how this might work in practice. And these academic insights have been translated in serious plans. The highest profile attempt was Desertec, a project announced in 2009 that quickly acquired lots of funding from various banks and energy firms before largely collapsing when most investors pulled out five years later, citing high costs. Such projects are held back by a variety of political, commercial and social factors, including a lack of rapid development in the region.  More recent proposals include the TuNur project in Tunisia, which aims to power more than 2m European homes, or the Noor Complex Solar Power Plant in Morocco which also aims to export energy to Europe. There are two practical technologies at the moment to generate solar electricity within this context: concentrated solar power (CSP) and regular photovoltaic solar panels. Each has its pros and cons. Concentrated solar power uses lenses or mirrors to focus the sun’s energy in one spot, which becomes incredibly hot. This heat then generates electricity through conventional steam turbines. Some systems use molten salt to store energy, allowing electricity to also be produced at night.  CSP seems to be more suitable to the Sahara due to the direct sun, lack of clouds and high temperatures which makes it more efficient. However the lenses and mirrors could be covered by sand storms, while the turbine and steam heating systems remain complex technologies. But the most important drawback of the technology is its use of scarce water resources.  Photovoltaic solar panels instead convert the sun’s energy to electricity directly using semiconductors. It is the most common type of solar power as it can be either connected to the grid or distributed for small-scale use on individual buildings. Also, it provides reasonable output in cloudy weather.     But one of the drawbacks is that when the panels get too hot their efficiency drops. This isn’t ideal in a part of the world where summer temperatures can easily exceed 45℃ in the shade, and given that demand for energy for air conditioning is strongest during the hottest parts of the day. Another problem is that sand storms could cover the panels, further reducing their efficiency.  Both technologies might need some amount of water to clean the mirrors and panels depending on the weather, which also makes water an important factor to consider. Most researchers suggest integrating the two main technologies to develop a hybrid system.  Just a small portion of the Sahara could produce as much energy as the entire continent of Africa does at present. As solar technology improves, things will only get cheaper and more efficient. The Sahara may be inhospitable for most plants and animals, but it could bring sustainable energy to life across North Africa – and beyond. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t. This article was updated on April 30 to correct an error. Saharan solar could potentially produce more than seven thousand times the electricity requirements of Europe (not 7)."
"**Two Northern Ireland charities have said they are facing huge losses and staff cuts because of Covid-19.**
The Stroke Association and Children's Heartbeat Trust said they have been denied vital support from Stormont because they did not meet the criteria.
The charities pleaded on Thursday with MLA's on Stormont's Communities Committee for help.
Both organisations said their service users were missing out on vital services.
Sarah Quinlan, chief executive of the Children's Heartbeat Trust, which provides financial support for children to travel outside Northern Ireland for vital surgery, said they are facing a deficit of Â£100,000 at a time when demand for their services is growing.
She said last year the charity helped 171 children and their families while already this year they have helped 232 families.
""Our services are life changing and yet we are not getting the support we badly need,"" she said.
""If we are forced down the road of making redundancies then we will be losing services.""
She said the charity did not qualify for support from the Covid charity fund because it holds reserves, which meant it was unlikely to close.
The Stroke Association was also ruled out of the support scheme because of reserves held by its parent organisation in London.
Director Barry Macaulay told MLAs it had lost Â£125,000 from direct fundraising, which had to be abandoned because of Covid-19, and they lost a further Â£100,000 in support from its headquarters in London.
""That leaves us having to operate with a deficit of Â£225,000 at a time when people are still having strokes and not being able to access rehab,"" he said.
He warned if they don't secure more funding they will be forced to cut their staff from 25 to 12.
Both charities called on the communities minister to widen the criteria of the fund, which is due to reopen before Christmas.
The fund will provide Â£6.8m to help charities directly affected by the Covid-19 crisis.
Members were informed that of the 645 charities that applied for support, 501 were eligible."
"
Share this...FacebookTwitterCountries like the USA who have been seduced by German socialists and Greens into thinking that solar energy and green energy feed-in schemes are a job creator and an engine to prosperity ought to think again. Der Spiegel reports.
 
Eastern Germany Hit Hard by Decline of Solar
By Charles Hawley
The global solar industry has entered a brutal phase of consolidation and nowhere are the effects as dramatic as in eastern Germany. Several companies have already declared bankruptcy, leaving towns and cities in the region struggling with job losses and tax revenue shortfalls. The future bodes ill.
The sun, it was said, was going to save Frankfurt an der Oder, a city of 60,000 on the Polish border. After years of post-reunification economic doldrums, whose nadir came with the 2003 failure of a much-ballyhooed microchip factory project, the burgeoning German solar industry took an interest in the down-on-its-luck city.
In 2006, solar-panel manufacturer Conergy moved into the never-used computer chip factory, joining Odersun, already headquartered in the city. In 2007, the United States solar giant First Solar opened a factory as well, followed by a second one last year.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now, though, the future suddenly looks decidedly dark. Odersun declared bankruptcy in March and Conergy, while pledging to return to profit this year, has seen its share price lose 99.6 percent of its value in the last five years. Many doubt the company will survive. Worst of all, however, was the announcement earlier this month that First Solar was closing both of its factories in Frankfurt an der Oder; 1,200 people will soon be jobless as a result.”
Keep reading…
It is indeed strange how so little of the mainstream media is reporting on one of the greatest industrial failures ever in Germany.
Even worse, solar industry has become a huge burden on the rest of the country due to the astronomically high cost of the energy and its sporadic supply.  More on that tomorrow. What a mess.
Readers in Vermont please send the Spiegel link to your drugged-up-on-green political leaders with the slim hope it’ll sober a few up.
 
Share this...FacebookTwitter "
"Solving environmental problems usually just means cleaning up the mess people have made. But scientists are increasingly interested in creating something valuable from pollution. “One man’s trash is another man’s treasure”, as they say, and researchers have now demonstrated several ways that useful products can be obtained from waste in industry and agriculture while also remediating contaminated soil, water and air. One environmental problem scientists are urgently trying to solve is the problem of carbon dioxide emissions which cause climate change. Researchers are developing processes which can capture carbon dioxide and convert it into useful chemicals like methanol – which can be used for fuel cells – or urea, which is used as a solvent in the chemical industry, in nitrogen fertiliser and in lactic acid, which can be used as a food preservative.  Carbon dioxide can also be captured and used to help grow algae, which are then harvested for biofuel. Wastewater – what we all flush away from our homes, offices and elsewhere – contains toxins and organic pollutants that treatment facilities remove before they can reach natural water systems like rivers and the ocean. However, researchers are trying to recover and turn this organic matter into something useful. Phosphorus and nitrogen are essential soil nutrients that are found in wastewater which could be returned to farm fields as fertilisers. Researchers have also taught microorganisms to break down toxic organic contaminants which are found in wastewater and generate electricity from them. As well as cleaning the water, microbial fuel cells would turn wastewater treatment facilities into giant batteries for green energy as electrochemically active bacteria degrade organic substances and release electrons to generate an electric current. Soil contamination with heavy metals is particularly tricky to solve. Usually, the only solution is to dig out the contaminated soil and dispose of it at a landfill site. Even then, contaminants can leach out of the soil and into underground water reservoirs, potentially ending up in plants and food crops, which soak up the water during growth. An alternative method involves a combination of phytoremediation and biorefinery. Biorefinery means processing biomass – such as food waste and the plant remains from agriculture – to produce valuable commodities. Phytoremediation cleans up environmental pollution using plants to extract metals from the contaminated soil in the same way a white rose would absorb red food colouring from dyed water and grow red petals.  Chinese brake fern (Pteris vittata) can accumulate arsenic as it grows and could be used to clean up areas contaminated with arsenic, such as land surrounding former mines in Cornwall and Devon. Phytoremediation can help recover rare earth elements and precious metals from the world’s most polluted places like Guiyu town in China, which became heavily contaminated from electrical waste disposal. By harvesting the plants with metal deposits stored in their cells, the toxic metals can be removed from the environment. The plant biomass can then be processed to recover metals for use in producing energy, fuel or industrial chemicals, making the whole process pay for itself. Environmental engineers are using their imagination to clean up the environment and generate wealth from waste at the same time. As our environmental woes intensify, we’ll need even more creative thinking."
"

On September 28, 1955, a Category 5 hurricane named Janet slammed into Chetumal, on Mexico’s Yucatan Peninsula, killing over 600 people.



Hurricane Dean, another Category 5, and the third‐​strongest storm ever measured at landfall, hit in exactly the same place last Friday and killed no one. Maximum winds in both storms were indistinguishable. Not surprisingly, the hurricane‐​hunter pilot who flew through the eyewall of the storm Tuesday reported severe turbulence, a temporary loss of aircraft control. Probably for the first time in human history, a Category 5 storm hit a populated area and everyone lived.



Because of its peculiar location, the Yucatan takes more big hurricane hits than just about anywhere else in the Western hemisphere. When Mexico was dirt‐​poor, as it was in 1955, hurricanes could kill hundreds. They were warned, then, too. Hurricane‐​hunter planes also monitored Janet. Only one of these has ever been lost, and it was as Janet was making landfall.



Similar storms, huge storms, very different results. What’s happening here?



Since then, people in the Yucatan have learned to adapt. While storms like these used to kill hundreds, even thousands, we now have the technology to forecast their tracks, at least for the critical last 24 hours, with reasonable confidence. Forecasting the intensity is a bit trickier, but everyone in the hurricane business was pretty convinced that Dean was going to bomb out sometime before it hit land. After all, it was passing over the same region in which the 1988 hurricane Gilbert set the record for the lowest barometric pressure ever measured in the Atlantic Basin.



Gilbert was the second‐​strongest storm ever recorded at landfall, and it also hit the Yucatan. While it was responsible for 202 deaths in Mexico, almost all of these were caused by mountain floods hundreds of miles away and days away from landfall.



Adaptation includes technology, infrastructure, and response. National Hurricane Center forecasts and data are available to everyone. But the infrastructure to respond to a forecast hurricane costs money, and poor nations don’t have it. Among other things, it requires good roads for evacuation.



Perhaps even more important, adaptation to hurricanes or other natural disasters is political. No elected official wants to be blamed for hundreds of preventable deaths, so the nations that can afford it develop evacuation plans, open shelters, and deliver people from danger.



When Janet killed hundreds, per‐​capita income in Mexico was less than a tenth of what it is now, when Dean killed no one.



So why is it that people are wringing their hands about global warming causing more severe hurricanes and deaths?



The best computer estimate for future hurricanes was published by Tom Knutson and Robert Tuleya in the _Journal of Climate_ in 2004. They calculated that maximum winds should increase by about 6% over the next 75 years. Even this may be an overestimate because the method used assumes carbon dioxide — the main global warming emission — is increasing in the atmosphere about twice as fast as it actually is.



Clearly, this small increase in hurricane strength is going to be dramatically overshadowed by adaptation as the developing world continues to develop. Mexico is a case in point.



We see other adaptations to climate change in our cities. In the United States, cities with the most frequent heat waves have the fewest heat‐​related deaths, and heat‐​related deaths are themselves dropping, as our cities warm. Remember, a city doesn’t need global warming to get hot. All it needs is a skyline, and a lot of blacktop and concrete to impede the flow of air and retain heat. But in our warming cities, just as with hurricanes in the Yucatan, frequency + affluence = adaptation.



An odd example of this is that there is only one major U.S. city in which heat related deaths are increasing, and it is the coolest one in summer: Seattle.



Anyone concerned about climate change should take a lesson from Hurricane Dean. Even if storms like this become more frequent in the future, people will adapt and survive if they have the financial resources. How silly it seems to take those resources away in futile attempts to “stop global warming” — which no one even knows how to do — when they could save lives by allowing people to adapt to our ever‐​changing climate.



The truth is that money in the hand is a lot more useful than treaties on paper when it comes to sparing yourself and your family from bad weather. So people truly worried about climate change should be cheerleading for the global trade and economic development that will continue allowing us to adapt.
"
"
Photo by Warner Bros. Pictures (Inception)
Canadian actress Ellen Page (Juno, Inception) has produced a great YouTube video urging everyone to participate in 350.org’s “Global Work Party Day” on October 10, 2010.
From the Huffington Post:
Page cites this summer’s floods in Pakistan and the Gulf oil spill as  evidence for the need to tackle the climate change crisis directly and  reduce dependence on dirty energy, and that the solution begins at the  local level.
“I’m tired of waiting around and listening to politicians argue.   They need to stop talking and start working,” Page says.  And she thinks  the Global Work Party on October 10 is the perfect way to show them  how. 
10/10/10 is a day to meet up with others in your community and work  toward different climate change solutions.  For example, Page mentions  that in Pakistan, women will be teaching how to use solar stoves, and  sumo wrestlers in Japan will be biking to practice.
Go to 350.org to find events in your area, or organize your own.  They even have plenty of ideas to help get you started.
“We want to send a clear message to our politicians:  We’re getting to work, so what about you?”


Oh wow:  sumo wrestlers in Japan will be biking to practice!  That will definitely “save the planet”.  There are over 5000 events on the 350.org webpage from all around the world.  Many are great like beach cleanups, tree plantings, and other feel-good community service efforts that will clearly help the environment.  However, there are some that really cause some head-scratching.  Here, as touted on the 350.org main page:  European 350 Team Become Strippers for the Climate … WUWT?  …and the previous posting by Anthony here …
So, please check out events in your area — and get the word out to your politicians.  Or submit your own idea in the comments / pick your favorite event and let us know!



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88763f13',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**South Yorkshire's mayor has said ""lockdown must not become limbo"" as the county, along with West Yorkshire, was named in the toughest coronavirus tier.**
Both areas will be in tier three after 2 December, meaning households can only meet in public outside spaces like parks, where the rule of six applies.
Mr Jarvis said: ""It is now essential we get a roadmap to get us out of tier three as a matter of urgency.""
Restrictions will be reviewed on 16 December, the government said.
Mr Jarvis said South Yorkshire had been under tighter restrictions since 24 October, and the rules were ""slowly suffocating businesses"" which were being hit again at their busiest time of year.
He added: ""Any restrictions must come hand in hand with a robust package of economic support to protect livelihoods. There must be no gaps in support for people and businesses affected by Covid.""
Meanwhile, Kirklees Council in West Yorkshire said the announcement was ""devastating"" for the area's hospitality sector.
Under the restrictions, pubs and restaurants are only allowed to stay open for takeaway services.
Kirklees Council leader Shabir Pandor said: ""I'm urging government to think more creatively about these restrictions and about how it can support the sector and the supply chain that relies on it so they are not laying more misery on these businesses.""
Non-essential shops and close-contact beauty services like hairdressers will be able to open in all tiers. Guidance said people in all tiers who can work from home should continue to do so.
Around 21 local authority areas in England will be in tier three - the highest level of restrictions - while just three areas will be in the lowest-level of restrictions - tier one.
York and North Yorkshire will face fewer restrictions than other parts of Yorkshire and will be placed into tier two.
Director of public heath at York City Council Sharon Stoltz said she had expected the city to be in the lowest level of restrictions, but instead it will go into the ""high"" alert tier from next Wednesday.
York has the lowest rate of new coronavirus infections in Yorkshire, recording 126.8 new cases per 100,000 people in the seven days to November 22.
For the same period, the England average was 202.4 infections per 100,000 people.
Ms Stoltz said: ""While we were hopeful we would be in the lowest level of restrictions, the restrictions can help us further drive down the virus.""
City of York Council leader Keith Aspden said: ""Although we are disappointed with today's news, we must continue to follow the new national guidance and protect the people and places we love.""
**Analysis**
**James Vincent, Political Editor BBC Yorkshire**
To nick someone else's joke - we've been on the verge of tiers all day.
Now we know.
Well at least we thought we knew until the government's postcode checking website crashed.
Those places in tier three will want information on how they can get out of it - and when.
Remember all the back and forwards we had when places were negotiating with the government on the last set of tiers? That's changed too. Locally, there will be no say on which tier places go into. The government is telling them.
There are tough times ahead for places in tier three. In West Yorkshire, Bradford, Calderdale and Kirklees have had higher measures since the start of August.
They've tightened over time and now, could possibly last until March.
In West Yorkshire, Leeds City Council said it was working to provide support and minimise the impact restrictions would have on residents and businesses.
Leader Judith Blake thanked people for their ""patience, diligence and compassion over what has been an incredibly challenging time for the city"".
She added: ""There is light at the end of the tunnel and if we continue to do all that we can to protect ourselves and each other, we can and will emerge from this crisis together.""
The system will be regularly reviewed and an area's tier level may change before Christmas, said the government.
_Follow BBC Yorkshire on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to_ yorkslincs.news@bbc.co.uk _or_send video here _._"
"Shared knowledge is an important currency for humans. It shapes everything from what we eat and how we dress, to how we raise our children. Some things we learn individually, some things we learn socially – from our parents, peers, teachers and the media. But how is shared information important for other species?  Mendel’s pea breeding experiments and the discovery of DNA were huge steps in the revelation that there are physical “packets of information”, in the form of genes, passing between generations. This has been instrumental in our understanding of the evolution of biodiversity, and how organisms are shaped by their environment.  However, in addition to this physical transmission through DNA, there are other sources of information available in the natural world. Social information can operate both within and between different generations, and is vital in shaping how animals response to their ever changing world.  I have previously argued  that social knowledge is important for whales and dolphins. Research also shows that social learning is widespread across a wide variety of wildlife, from birds to elephants, from fish to meerkats.   Birds can learn foraging techniques from each other (such as opening the foil caps of milk bottles to extract the cream). Bottlenose dolphins have been observed learning from their mothers how to use sponges to help protect their jaws while foraging on the sea bed for fish. Southern right whales share migration routes between critical feeding and breeding habitats. African elephants learn from older matriarchs the location of watering holes, and how safe it is to interact with different social groups. Evidence for social learning can be seen in the depths of the oceans, in deserts and on mountain tops. It is an important mechanism across the natural world, helping organisms adapt to changes in their environment. These adaptations often occur much more swiftly than in the slower process of natural selection, which brings about incremental change between generations. Social learning is a rich seam of exploration for behavioural ecologists and conservation biologists. And because it can result in discrete units within a population which use that knowledge, it can help to inform us about focusing conservation efforts.  For example, understanding more about the mechanisms of social learning may be invaluable for the reintroduction of some captive-bred migratory bird species. It could help negotiate solutions in areas of human-wildlife conflict, such as when elephants or apes help themselves to human crops. One of the results of social learning – not itself an endpoint, but an ongoing process – is animal culture. But what is animal culture? With our own inherited cultural perspective, it is understandable that animal culture is sometimes a challenging concept for us to grasp.  It is not the many and different ways in which other species are integrated into human culture (although that is interesting in itself). Animal culture can be defined as “information or behaviour, shared within a community, which is acquired from members of the same species through some form of social learning”.  The idea that other species have rich social lives, which includes some socially learned, collective ways of behaving that differentiate social groups, seems like a significant philosophical leap. But the evidence is now unequivocal that humans are not alone in having distinct cultures. The revelation of animal cultures raises a number of both ethical and scientific questions. But from a practical perspective, what does the existence of animal culture mean for our efforts to conserve the natural world?  To better understand the relationship between animal sociality and conservation, the Convention on the Conservation of Migratory Species of Wild Animals, a treaty under the aegis of the United Nations Environment Programme, has been spearheading work to explore how best to use the emerging science in this field to optimise conservation efforts. Its efforts so far are described in a recent article in Science. Beginning a serious dialogue on animal culture represents a paradigm shift in our understanding of what exactly biodiversity is. In addition to genes, specific kinds of behaviour are also an important aspect of the rich diversity of our planet. In order to work towards conserving genetic diversity, we must now also work towards maintaining animal cultural diversity across different ecosystems. This is the challenge that lies ahead for global environmental agreements – a culture of conservation that respects the cultures of the natural world."
"**Tier three is the ""right place"" for Hull due to high coronavirus rates in the city, a council leader has said.**
Hull City Council's Stephen Brady admitted residents may be ""tired of restrictions and changing advice"" but needed to comply with the rules.
The city, which previously had the highest infection rate in England, was placed in the ""very high alert"" category from 2 December.
Restrictions will be reviewed on 16 December .
The tougher measures mean households can only meet in public spaces like parks, where the rule of six applies.
Infections in the city have been falling in recent days.
In the seven days to 22 November, 460 cases per 100,000 people were reported in the city - down from 748.3 the previous week.
Mr Brady said: ""Tier three is not where anyone wants to be but, with our infection rates still very high, it is what we expected and it's the right place for Hull to be at this time.
""This is a very difficult time and we would all like it to be over. What the last few weeks have shown is that, if we can continue to do all we can to minimise spreading the virus, we can continue to bring the rate down and, hopefully, move towards an easing of restrictions.""
He added that people should think about what's best for the city over Christmas and warned against mixing households, despite the government allowing some limited mixing.
In a tweet, MP for Hull East Karl Turner thanked residents for their ""hard work and sacrifice"".
He said: ""Tier 3 is sadly still where we need to be, but the government must provide more support for those hit the hardest and a clear route back to lower restrictions.""
Health Secretary Matt Hancock set out the reasoning behind the tier decisions for each area in a written ministerial statement.
_Follow BBC East Yorkshire and Lincolnshire on_Facebook _,_Twitter _, and_Instagram _. Send your story ideas to_yorkslincs.news@bbc.co.uk _._"
"**Reports of rape dropped by more than 20% during lockdown compared with the same period last year, according to Police Scotland.**
Between 24 March, when lockdown started, and 5 July, 529 rapes were recorded by the force.
The figure represents a 20.2% reduction on the same three-and-a-half month period in 2019, police said.
The force said rape reports had since increased as Covid restrictions eased, but figures remained lower than 2019.
Between 24 March and 15 November, the number of both recent and non-recent rape offences reported was down 6.5% to 1,427 compared with 1,526 for the same period in 2019.
A recent offence is categorised as one within a year of the crime occurring - reports of which were down 10.8% between March and November from 959 in 2019 to 855 this year.
Despite the decrease in figures, the force issued a reminder that a reduction in reporting did not equate to less offending and said all forms of sexual crime continued to be under-reported.
It said it was too early to draw conclusions about why there had been a fall.
Rape Crisis Scotland chief executive Sandy Brindley said: ""Lockdown has been incredibly tough for so many people and for survivors it's posed real challenges in accessing support and - for those who choose to report - going to the police."""
"
  John A: This is a provocative essay, and I’ve thought of at least a couple of replies to counter some of the arguments, but I think it deserves a wider audience.

The Global Warming Policy Foundation

by Dr Terence Kealey, Vice-Chancellor, University of Buckingham
Member of the Academic Advisory Council for the Global Warming Policy Foundation
The Mont Pelerin Society Meeting Seminar on Science, Scepticism and the Future. Sydney, Australia, October 2010
 
The hard core of a programme is rendered unfalsifiable by the methodological decision of its protagonists. — Imre Lakatos Criticism and the Growth of Knowledge 1974
The  scientist is restricted by his instruments, money, the attitudes of his  colleagues, his playmates, and by innumerable physiological,  sociological, historical constraints.  –Paul Feyerabend, Against Method 1975

The  emails sent by members of the climatic research centre at the University  of East Anglia have provoked international outrage, as have the many  flawed global warming papers that have appeared in recent years such as  those describing the hockey stick graph(1), to say nothing of the flawed  predictions of the Intergovernmental Panel on Climate Change (IPCC)  over such issues as the rate of disappearance of the glaciers in the  Himalayas. But such outrage has been naive because it has been premised  on the assumption that scientists are – and should be – dispassionate  seekers after truth. Yet in fact scientists are and should be advocates.  Science has always been rooted in advocacy, as was illustrated by an  episode from its very beginnings during the 5th century BC.
Pythagoras  (of the Theorum) was a good scientist but he was of a mystical bent and  he revered ‘rational’ numbers (whole numbers or whole fractions).  He believed they explained the Harmony of the Spheres. Pythagoras,  indeed, believed that whole numbers underpinned the universe from music  to the movement of the planets. But Pythagoras had a student called  Hippasus, and Hippasus discovered that the square root of 2, √2 is not a  rational number. It is in fact an ‘irrational’ number, and its exact  quantity will never be precisely calculated because, as Hippasus showed  two and a half thousand years ago, irrational numbers can never be  definitively calculated. This proof upset Pythagoras and he asked  Hippasus to retract it. But Hippasus refused, so Pythagoras had him  drowned.
That’s  what scientists are like in their natural state. Now – call me soft –  but I think Pythagoras went too far; I think that scientists should  desist from killing each other or even from telling outright falsehoods.  But, like advocates in court, scientists can nonetheless be expected to  put forward only one very partial case – and that as strongly as  possible – and no-one should expect a scientist to be anything other  than a biased advocate.
Consider  the early controversy over the age of the earth. The 19th  century geologist Sir Charles Lyell had, by his study of the rate of  erosion of cliffs, proposed the earth not to have been created at 9.00  am on the 23rd of October 4004 BC but, rather, some hundreds of millions  of years earlier. But, as we know from volcanoes, the core of the earth  is red hot. And when contemporary geologists measured the temperature  of the molten core, and when they calculated its rate of heat loss,  they concluded that the earth could be only a few millions of years old.  Had it been any older its core would have completely cooled. Lyell had  apparently been falsified.
In  the face of this apparent falsification, did Lyell’s followers ditch  their ideas? No. Like advocates presented with contradictory data that  cannot be challenged, they simply ignored it. They knew how old the  sedimentary rocks had to be, and they didn’t believe the falsifiers. So,  not knowing how to falsify the falsifiers, they simply pressed on with  their own pre-existing programme of research, assuming
that  something helpful would turn up eventually. Which it did. Somebody in  some other discipline discovered radioactivity, somebody discovered the  core of the earth to be radioactive, somebody discovered that  radioactive reactions emitted heat and hey presto the problem was  resolved: the core of the earth generates heat, which is why it is still  hot; and the earth is indeed very old.
In  his 1605 book The Advancement of Knowledge, which helped launch  the modern discipline we call the philosophy of science, Francis Bacon  proposed a four-step process by which science advanced, namely by (i)  observation, (ii) induction, (iii) deduction and (iv) experimentation.  Bacon saw this as an almost mechanical or determinist activity based on  logic, which he supposed precluded individualistic human whims. But  because the number of potential observations is so large (does  the colour of an astronomer’s socks correlate with his or her recordings  of the movement of a planet?) scientists must inevitably select the  observations they believe to be relevant, from which they then deduce  and induce the theories they seek to test.
Scientists  therefore select particular theories out of a range of possibilities.  And they then (being human) design experiments to prove their own  theories right. Consequently, contrary to what many people believe that  Karol Popper wrote, science is in practice not about falsification.2 In  practice great scientists ignore embarrassing data, and they refuse to  feel falsified when they don’t want to be.
Scientists  know they are working at the limits of knowledge, which means that  that knowledge must necessarily be imperfect, so (like Charles Lyell)  scientists will refuse to draw definitive negative conclusions from  unhelpful new findings because they know that those new findings might  themselves need re-evaluation in the light of further subsequent data  (such as radioactivity) that has yet to be revealed.
Indeed,  as Thomas Kuhn explained in his classic 1962 book The Structure  of Scientific Revolutions, scientists’ personal attachment to their own  theories in the face of conflicting data means that the research  community’s dispassionate collective verdict over what is  ’truth’ can  be delivered only after all the competing data has come in and only  after all the arguments have been made (or, as was said humorously by  Max Planck:-  “A new scientific truth does not triumph by convincing its  opponents and making them see the light but rather because its  opponents eventually die and a new generation grows up that is familiar  with it “). These arguments have been summarised by Alan Chalmers of  Finders University in his excellent introduction to the philosophy of  science What Is This Thing Called Science? (3rd ed 1999, Open University).
Consequently,  we can see how the climate change scientists of the IPCC and of the  conventional global warming paradigm saw no conflict between their  partiality in the arguments they put forward and their responsibilities  to  ‘truth’, just as advocates in court under the common law see no  conflict between their partiality in the arguments they put forward and  their responsibilities to  ‘justice’.
In both cases, the  scientists and advocates see their prime responsibility as being the  putting forward of the best arguments to support their case/client, and  they delegate the adjudication over impartial ‘truth’ to the jury of  peers.
Such  partiality cannot excuse misrepresentation, of course, nor the  persistent non-disclosure of inconvenient facts, and those will always  be ethical crimes, but it would be naive of the general public to expect  scientists always to present their work and theories dispassionately.  It would also be naive of the general public to expect scientists to  disclose all their data promptly. In his otherwise excellent 2010 book  The Hockey Stick Illusion (Independent Minds) where he  dismissed the claims of many climate change scientists, AW Montford  nonetheless professed astonishment that researchers might feel that they  can legitimately withhold original data. But as Tim Birkhead recently  reported in the Times Higher Education, such withholding is  a conventional aspect of many disciplines in science. Indeed, it is  endorsed by the British Government’s research councils. Thus the Natural  Environment Research Council states that “individual scientists,  principal-investigator teams and programmes will be permitted a  reasonable period of exclusive access to data sets they have collected”  while the Biotechnology and Biological Sciences Research Council states  that  ‘researchers have a legitimate interest in benefiting from their  own time and effort in producing the data, but not in prolonged  exclusive use. ‘3
But why should scientists publish anything at all? In his 1942 essay The Normative Structure of Science Robert Merton, the great sociologist of science, described science with  the acronym CUDOS (note how it is pronounced). The letters stand for  Communism, Universalism, Disinterestedness and Organised Scepticism,  by which Merton meant that scientists share knowledge (communism), that  knowledge is judged objectively (universalism), that scientists act in  ways that appear selfless, and that ideas are tested collectively.
But actually Merton was being ahistorical. Pace his acronym, scientists indeed seek either kudos or money or both (ie,  they are not communistic, they are selfseeking, which is legitimate but  not particularly noble) but their publishing has always been dictated by  self-interest. Indeed, in its natural state science was  originally characterised by the paradox of secret publishing:  researchers did not want others to benefit from their advances. So some  scientists, having dated the report of a discovery, would seal and  deposit it with a college or lawyer, to open it only to dispute priority  with a later competitive publication. Others would publish in code  or in anagrams: Galileo published his discovery of the rings of Saturn  in 1610 as smaismrmilmepoetaleumibunenugttauiras for Altissimum planetam tergeminum observavi (I have observed the most distant planet to have a triple form) while Robert Hooke published his law of elasticity in 1660 as ceiiinosssttuu for ut tensio sic vis (stress is proportional to strain.)
Secrecy  was originally normal: when around 1600 a young London obstetrician  called Peter Chamberlen invented the obstetric forceps, for over a  century he, his younger brother, his younger brother’s son and that  son’s son (all obstetricians) kept the invention a secret. Rich women,  knowing that the Chamberlens were the best obstetricians in Europe,  engaged them to deliver their babies, but the price those women paid  (apart from handsome fees) was to be blindfolded and trapped alone with  the Chamberlens in a locked room during labour so that no one could  discover the secret of the forceps. That emerged only during the 1720s  when the last Chamberlen, having retired rich but childless, finally  divulged it.
It  was Robert Boyle who, by his leadership of the Royal Society of  London, which was created exactly 350 years ago this year, negotiated  (i) the convention whereby priority – and therefore esteem – goes to the  scientist who publishes first, not to the scientist who might  have made the discovery earlier but who has kept the findings secret,  and (ii) the convention that papers are accepted for publication only  if they contain a methods section as well as a results section, to allow  reproducibility.
We see here, therefore, that science is not innately a public good: it is innately a discreet one where, in a state of  nature, scientists would publish not their methods but only their  findings – and where they would sometimes delay or obscure the  publication even of those. But it was Boyle who realised, in classic  game theory mode, that if the Fellows (aka members) of the infant Royal  Society collaborated with each other in publishing their findings (i)  openly, and (ii) including their methods sections, then the scientists  within the Society would do better, by virtue of their access to the  whole of the Society’s membership’s collective discoveries, than  would those isolated researchers who worked outside the circle of mutual  disclosure. And it was because the Royal Society’s original experiments  were conducted collectively but in the presence only of its Fellows,  and because its publications were preferentially circulated to its  Fellows, that the Fellows enjoyed an advantage over non-Fellows.
Science, therefore, only appears to  be public because, over the centuries, most scientists globally have  gradually modelled themselves on the Royal Society’s ‘new’ conventions,  the better to take advantage of the mutuality of knowledge. But not all  scientists have done so completely, and as Birkhead showed in his THE  article many disciplines have elaborated the convention of publishing  their findings a year or two before they publish their data, thus  keeping a lead on the further study of their data.
Everyone  in those disciplines agrees that, since the exploitation of other  people’s data is so much easier than discovering it for oneself, a  discoverer’s year or more of monopoly is only fair.
To  conclude, therefore, scientists are not disinterested, they are  interested, and as a consequence science is not dispassionate or fully  transparent, rather it is human and partially arcane. As I argue  elsewhere, science is not the public good of modern myth, it is a  collegiate and quasi-private or invisible college good.4 That means,  by the way, that it requires no public subsidies. More relevantly, it  means that individual scientist’s pronouncements should be seen more as  advertisements than as definitive.
Peer  review, too, is merely a mechanism by which scientists keep a  collective control over access to their quasi-private enterprise. One  the e-mails leaked from the University of East Anglia included this  from Professor Phil Jones, referring to two papers that apparently  falsified his work:-  “I can’t see either of these papers being in the  next IPCC report. Kevin and I will keep them out somehow – even if we  have to redefine what the peer-review literature is!”
So  what? Climategate tells us no more than the philosophers of science have  long told us about research, and the public should be less naive.
Notes and References
1. Mann ME, Bradley RS, Hughes MK, 1999, Northern Hemisphere Temperatures During the Past Millennium Geophysical Research Letters 26: 759.762
2.  It should be noted that falsification and falsifiability are different.  As Popper proposed, a statement cannot be seen as scientific unless it  is falsifiable and can thus be tested by the scientific method. So the  statement that the moon is made of green cheese is a scientific one,  because it can be tested and falsified. But the fact that none of the  moon missions to date has found green cheese does not falsify the  hypothesis because not every part of the moon has yet been explored.
3. Birkhead T, 2009, Whose Data is it Anyway? Times Higher Education 1,901, 27.
4. Kealey T, 2008, Sex, Science and Profits William Heinemann


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8675b289',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Derby and Derbyshire will be subject to tier three restrictions after the national lockdown ends on 2 December, the government has announced.**
The city and county were in tier two prior to the lockdown but will move to the highest alert level.
This means pubs and restaurants must close except for takeaways and delivery and mixing indoors is banned.
Derby currently has a seven-day infection rate of 268.6 per 100,000 people for the week up to 21 November.
Bolsover has the county's highest figure with 301.6, although that is down from 427 the week before.
The average for the whole of England is 208.7.
Talking specifically about Derby and Derbyshire, the government said in a ministerial statement: ""There has been improvement in this area, but case rates remain very high at 275 per 100,000, and in those over 60 it is 220 per 100,000. The pressure on the local NHS remains high.""
The area's new tier placing means they are eligible for rapid or ""lateral flow"" tests to help bring down infections.
The system will be regularly reviewed - with the first scheduled for 16 December.
Toby Perkins, Labour MP for Chesterfield, called his area being put into tier three ""disastrous"" and a ""nightmare"".
He tweeted: ""Matt Hancock celebrating that mass testing has helped Liverpool drive down the R rate and move into tier two. Exactly. That is what every area should have had for months.""
Mr Perkins added the suggestion in the House of Commons by Mr Hancock that the reason Cornwall, Isle of Wight and Suffolk have lower rates than the rest of England was because their residents have been more responsible was ""deeply offensive"".
High Peak MP Robert Largan said he was ""disappointed"" all of Derbyshire was in tier three, while fellow Conservative and Derby North MP Amanda Solloway said though tier three restrictions in the city were ""not unexpected"", they were required ""to keep us all safe"".
The county's director of public health Dean Wallace said: ""We all want to celebrate Christmas safely so it is more important than ever that we all continue to do everything we can to protect our friends and family by sticking to the rules.""
Derbyshire County Council leader Barry Lewis said he was ""disappointed"" at the decision to place the county in tier three.
Mr Lewis, along with other council leaders, wrote to the prime minister on Wednesday urging him to consider placing Derbyshire in tier two in order to protect the economy, especially the hospitality and tourism sectors.
Following the announcement, he tweeted: ""Disappointed for our tourism, hospitality, retail sectors and others who will be economically impacted before Xmas.
""Epidemiology tells us we're in upper tier three and declining - in a few days we'd have been squarely tier two.""
A week before the second national lockdown began, Derby and most of Derbyshire were in tier two.
This is why Health Secretary Matt Hancock said people will be disappointed to find themselves in the top tier of restrictions.
His explanation for the move is that there have been improvements in the area but infection rates remain high and this is putting pressure on the local NHS.
Pauline Latham, Conservative MP for Mid Derbyshire, had previously spoken out against the new rules.
Speaking in the House of Commons on Wednesday, she described the tier system as ""illogical"" and said it would lead to job losses.
Last week Derby City Council said it was in discussions with the government about Derby Arena being used as a mass vaccination centre.
_Follow BBC East Midlands on_Facebook _,_Twitter _, or_Instagram _. Send your story ideas to_eastmidsnews@bbc.co.uk _._"
"
Share this...FacebookTwitterSebastian Lüning recently wrote a piece on a new study by a French team of scientists.  Conclusion: Mediterranean storm activity decreases during warm periods and there appears to be a solar link.
==========================================
The worst storms at the French Mediterranean coast? Always when the sun was weak and temperatures declined!
By Daniel Albig and Sebastian Lüning
(Translated by P Gosselin with permission)
The area of the Mediterranean Sea is regarded as a region that reacts especially sensitively to climate fluctuations. An increase in temperature there would be especially noticeable, say certain model projections. In 2007 some scientists even prophesied that there would soon be a danger of cyclones forming at the Mediterranean. But what does the pre-industrial climate history tell us about this possibility? Is there really a relationship between storm activity and temperature in the Mediterranean region?
A French team of scientists led by geologist Pierre Sabatier studied in detail how storms and global warming behaved historically in the region. In a study that appeared in January 2012 in the journal Quaternary Research, they examined the last 7000 years. The basis for their study was an 8-meter long sediment core that had been extracted in March, 2006 from the seabed of the Pierre Blanche Lagoon of the southern French Golf of Lion, about 10 kilometers south of Montpelier.
The scientists studied changes in the deposits in the lagoon, which today are covered by 60 cm of water. Changes in storm patterns in the region can be discerned by the variations in the particle size of the sand, the clay composition and fossils present. The frequency of the various species of water snails were analyzed. For example the hydrobia acuta lives in the brackish waters, the bittium reticulatum lives in the open seas. A sudden increase in deposits of the needle whelk indicates greater storm activity because the lagoon gets flooded more often by the sea.
Using various indicators, the French scientists identified seven periods of increased solar activity: 6300-6100 years ago, 5650-5400 years ago, 4400-4050 years ago, 3650-3200 years ago, 2800-2600 years ago, 1950-1400 years ago and 400-50 years ago. Storm activity increased over and over again over the last few thousands of years, and settled down during the times in between.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So what could have triggered storm activity at the French Mediterranean? In the search for possible relationships, the French scientists compared storm development with the temperature development of the North Atlantic, which was reconstructed more than 10 years ago by a team led by Gerard Bond who examined cores of ice berg rafted sediment and published the results in the journal Science. The Bond group could show that the temperature cycles were in sync with solar activity.
And what did this comparison yield? Storms in the Northwest Mediterranean occurred more often during the cold periods. Solar activity played an important role: Whenever the sun weakened, it became cold and stormy. When the sun got active, temperatures increased and the winds died down (Figure 1). The main drivers were obviously the solar Bond cycles. Also added are some solar-dependent, climate-system-internal fluctuations which complete the picture.
How could the relationship function? The scientists suspect that a stark north-south temperature gradient prevailed during the cold periods and thus resulted in more storms. In addition the westerly winds could have shifted southwards.
If you look back at the last 1000 years, the natural pattern becomes clear. During the Medieval Warm Period (1150 to 650 years ago) the new research results show that a period of weak storm activity prevailed. During the Little Ice Age that followed, tempestuous storms raged over the area of study. During the transition to the current Modern Warm Period the storms died down. The good news: An increase in storm activity is not anticipated, at least for the south French coast, with further warming of the Earth. Instead a decrease in storm activity is expected.
Interestingly, the relationship is not only valid for the Mediterranean region. Already in February 2012 we reported on a study from the Netherlands. That study showed that the strongest storms occurred during the Little Ice Age (read: Die kräftigsten Stürme gab es in Holland während der Kleinen Eiszeit).
 
Share this...FacebookTwitter "
nan
"**Most of the West Midlands will be under the toughest Covid-19 restrictions when the region comes out of England's second lockdown on 2 December.**
The region will enter tier three - the highest alert level - in Birmingham and the Black Country, Solihull, Coventry, Warwickshire, Stoke-on-Trent and Staffordshire.
It is the first time local areas must obey the tier system's stiffest rules.
Tier three measures include a ban on households mixing indoors.
Worcestershire, Herefordshire, Shropshire and Telford and Wrekin are set to enter tier two where restrictions are slightly more relaxed.
Nowhere in the region will face the lowest level of restrictions, tier one.
It means all local areas have moved up one level compared to their status before the second lockdown, with Warwickshire leaping from tier one to three.
The government says it will review the tier allocations on 16 December, although there will be a UK-wide relaxation of rules for five days over Christmas.
England's first tier system was brought in this autumn to curb a second wave of coronavirus, but it was replaced with a four-week national lockdown from 5 November that applied stiff and uniform measures across the country.
But the nationwide approach will make way for varied restrictions again from 2 December under new tiers.
Differences between the new tiers include restrictions on where households can meet up:
Pubs and restaurants in tier two can only open to serve ""substantial meals"", while those in tier three can only operate as a takeaway or delivery service.
Gyms and close-contact beauty services like hairdressers will be able to open in all tiers.
Guidance says people in all tiers who can work from home, should continue to do so.
Health Secretary Matt Hancock used a government webpage to outline his reasoning for each area's status.
Warwickshire, where restrictions have jumped from tier one to tier three, is listed as part of a group with Coventry and Solihull, which were previously both in tier two.
Mr Hancock says high yet falling infection rates and pressure on the local NHS are behind the grouping's tier three status.
But Izzi Seccombe, Conservative leader of Warwickshire County Council, said the news came as a shock, and queried the method used and whether one of the trio had skewed the outcome for the others.
She said: ""I'm going to be asking about the validity of the rates that they've put us in.
""Coventry and Warwickshire are nip and tuck on the rates and right at this moment Coventry is slightly lower than Warwickshire. Solihull is higher, and we're in a grouping, a regional grouping together, so I will be checking with government about whether the evidence that they have used is fair and is reasonable.""
Ms Seccombe also raised fears the measures would impact on the local hospitality sector - concerns echoed by businesses.
Among them was the Royal Shakespeare Company (RSC) based in Stratford-upon-Avon.
""We are deeply disappointed by the news that [Warwickshire] has moved to tier three Covid restrictions,"" said executive director Catherine Mallyon.
She said the RSC had planned to welcome back audiences for the first time since March to events set for 19 and 20 December, but they would now be streamed online instead.
Ms Mallyon added: ""The announcement today means further difficulties and hardship to theatres and freelance colleagues around the country on top of those already faced over the last eight months.""
For Alcester-based Hillers Farm Shop and Restaurant there is an unusual frustration.
The Warwickshire venue is set to enter tier three restrictions, forcing its restaurant to close, while businesses over the road go into tier two - because they fall under Worcestershire.
Director Emma Taylor said she had been preparing for the venue's reopening, expecting her part of rural Warwickshire to be tier two at the worst.
She said she had to respect the situation and hoped something would change when the tiers were reviewed.
Meanwhile, a businessman behind a luxury boutique hotel set to open in Coventry on 4 December said the city's move to tier three was ""just devastating"".
Ian Harrabin, director of Complex Projects, said: ""It's just hard to understand the logic - and it's the wrong move from the government.""
A pop-up experience at the hotel's rooftop bar was planned for the launch. Mr Harrabin said: ""We had over 400 bookings for our rooftop experience, and it doesn't make sense you can't sit outside yet you can go into a gym - where's the logic in that? We're just gutted.""
_Follow BBC West Midlands on_Facebook _, on_Twitter _, and_sign up for local news updates direct to your phone _._"
"

Nuclear energy is to the Right what solar energy is to the Left: Religious devotion in practice, a wonderful technology in theory, but an economic white elephant in fact (some crossovers on both sides notwithstanding). When the day comes that the electricity from solar or nuclear power plants is worth more than the costs associated with generating it, I will be as happy as the next Greenpeace member (in the case of the former) or MIT graduate (in the case of the latter) to support either technology. But that day is not on the horizon and government policies can’t accelerate the economic clock. 



Many free market advocates support nuclear because it costs less to generate nuclear power than it does to generate electricity from any other source (save, perhaps, hydroelectric power), thanks to nuclear’s low operation and maintenance costs. However, someone has to first pay for‐​and build‐​these plants and the rub is that nuclear has very high, upfront construction costs ranging from $6–9 billion. By contrast, gas plants cost only a few hundred million dollars to build and coal a couple of billion depending upon the capacity and type of plant.



This raises the opportunity and risk costs of nuclear, making it unattractive to investors. Capital‐​intensive power facilities take longer to build, which means that investors have to defer returns for longer than if they had invested elsewhere. What’s more, electricity markets have a very peculiar pricing mechanism that makes it harder for nuclear to maximize returns compared to gas‐​powered or other plants. In essence, there are two electricity markets: a market for base‐​load power (electricity sold 24‐​hours a day) and a market for peak power (electricity sold as needed during peak demand periods like hot summer days). Much of the demand for new power‐​and thus much of the profit available to investors today‐​is found in the peak market. But nuclear power plant construction costs are so high that it would take a very, very long time for nuclear facilities to pay for themselves if they only operated during high demand periods. Hence, nuclear power plants are only profitable in base‐​load markets. Gas‐​fired power plants, on the other hand, can be profitable in either market because not only are their upfront costs low but it is much easier to turn them off or on unlike nuclear.



Nuclear’s high up‐​front costs don’t just mean delayed profits, it also makes nuclear a more risky investment, especially since 20 states have scrapped policies that used to allow investors to charge rates that would guarantee their money back. This means that investors in new nuclear power plants are making a multi‐​billion dollar bet on disciplined construction schedules, accurate cost estimates, and the future economic health of the region. Bet wrong on any of the above and the company may well go bankrupt. Bet wrong on a gas‐​fired power plant, on the other hand, and corporate life will go on because there is less to lose given that the construction costs associated with gas‐​fired power plants are a small fraction of those associated with nuclear plants.



One metric that reflects this difference is the “levelized” cost‐​the price that must be received by owners to cover fixed and variable costs while returning profits to investors. This cost is substantially higher for nuclear than coal‐​fired electricity. Tufts economist Gilbert Metcalf, for instance, has calculated that, under current law, the levelized cost of nuclear power in the United States is 4.31¢ per kilowatt hour (kWh). Coal‐​fired electricity, on the other hand, cost 3.53¢ per kWh and “clean” coal cost 3.55¢.



But even these nuclear estimates are almost certainly too low. That’s because Metcalf uses an “overnight cost” (construction costs minus financing costs) figure of $2,014 per installed kilowatt (kW) which is much too low. The Energy Information Administration (EIA) puts this cost at $2,475 per kW at present‐​although even this figure is suspicious because it relies on a world‐​wide average for nuclear power plant construction‐​including the grossly unreliable estimates from state‐​managed economies. The Standard & Poor’s overnight cost estimate of $4,000 is likely the most reliable because it is based on nuclear plant construction costs in economies where labor and material costs are very similar to those found in the United States. Industry analyst Jim Harding, who uses overnight cost figures similar to Standard & Poor’s, puts the levelized costs for new nuclear power generation at 12–15c per kWh right now.



Investors are also wary of nuclear plants because of the construction delays and cost over‐​runs that have historically plagued the industry. For instance, the Areva/​Siemens nuclear power plant being built for TVO in Finland‐​the first nuclear power plant to be built in a relatively free energy market in decades‐​once scheduled to be operational within 54 months, is now two years behind schedule and 60% over budget. Nor have these construction delays had anything to do with regulatory obstruction or organized public opposition.



If nuclear power plants are so uneconomical, how then to explain the blizzard of permit applications for the construction and operation of new nuclear power plants that the Nuclear Regulatory Commission has received? Easy: These applications cost little and oblige utilities to do nothing. Industry analysts maintain that federal approvals will not translate into actual plants without a federal promise to private equity markets that, in case of default by power plants, the taxpayer will make good on the full sum of all bad nuclear loans.



Nuclear supporters often counter that construction costs would be a lot lower if regulators didn’t impose insanely demanding safety standards, byzantine and time‐​consuming permitting processes, or endless public hearings, any one of which could result in the plant being stopped in its tracks. Investors would also be more likely to invest, we’re told, if there were a high‐​level waste repository in place or more political support for nuclear power.





[H]ow do France, India, China, and Russia build cost‐​effective nuclear power plants? They don’t.



I would love to tell that story. I do, after all, work at the Cato Institute, and blaming government for economic problems is what keeps me in business. But what stops me is the fact that those complaints are not echoed by the nuclear power industry itself.



On the contrary, the industry in the early 1990s asked for‐​and got‐​exactly the sort of safety regulations, permit review process, and public comment regime now in place. Both public and political support for nuclear power is running so high than even a majority of Democrats in Congress are happy to not just tolerate nuclear power, but lavish even more subsidies upon it. And while Yucca Mountain may not be open now or ever, everyone seems reasonably content with the current on‐​site waste storage regime.



Indeed, if government were the reason why investors were saying “no” to their loan applications, I would expect that industry officials would be the first to say so. But they do not.



There’s another good reason why the industry is not protesting government intervention these days‐​the industry would not exist without it. Take away the 1.8¢ per kWh production tax credit available to the first 6,000 megawatts of new nuclear generation built prior to 2021, for instance, and Metcalf calculates that the levelized cost of new nuclear power plants jumps by 30 percent. Replace accelerated depreciation tax rules with regular depreciation rules and costs jump another 9 percent. Even zero taxation on nuclear power would increase costs by 6 percent because right now nuclear power enjoys a negative effective tax rate. Indeed, this jump by itself would make nuclear much more expensive than conventional coal, “clean” coal, and natural gas. Finally, repealing the $18 billion in federal loan guarantees recently promised the industry and eliminating regulations that relieve nuclear plant owners of the responsibility to pay third‐​parties to accept the risks associated with waste disposal would dampen market interest in nuclear power even further.



But the final nail in the coffin for the industry would be if the federal cap on the liability that nuclear power plant owners face in case of accidents (the Price‐​Anderson Act) were to be lifted.



Given all of this, how do France, India, China, and Russia build cost‐​effective nuclear power plants? They don’t. Government officials in those countries, not private investors, decide what is built. Either these governments build expensive plants and shove them down the market’s throat‐​or they build shoddy plants and hope for the best.



Conservatives project nuclear power as the solution to greenhouse gas emissions. But they should resist that argument. If we slapped a carbon tax on the economy to “internalize” the costs associated with greenhouse gas emissions‐​the ideal way to address emissions if we find such policies necessary‐​then the “right” carbon tax would likely be about $2 per ton of emissions according to a survey of the academic literature by climate economist Richard Tol. That’s not enough to make nuclear energy competitive against coal or natural gas according to calculations performed by the Electric Power Research Institute. In any case, if nuclear offers a cost‐​effective way to reduce greenhouse gas emissions, it should have to prove it by competing against alternatives in some future carbon‐​constrained market.



There’s nothing new about today’s rhetoric about the supposed “nuclear renaissance.” Back in 1954, GE maintained: “In five years‐​certainly within 10‐​a number of them (nuclear plants) will be operating at about the same cost as those using coal. They will be privately financed, built without government subsidy.” Now, 54 years later, the talk of “renaissance” is back‐​as are promises about the imminent economic competitiveness of nuclear.



Those who favor nuclear power should adopt a policy of tough love. Getting this industry off the government dole would finally force it to innovate or die‐​at least in the United States. Welfare, after all, breeds sloth in both individual and corporate recipients. The Left’s distrust of nuclear power is not a sufficient rationale for the Right’s embrace of the same.
"
"

I've been reading a lot of coverage of the FISA debate this week. I'm getting a little tired of reading commentary from supporters of eliminating judicial oversight who seem to have no clue what they're talking about. Consider this from _FrontPageMag_ 's Jacob Laksin: 



Instead of enjoying the flexibility necessary for real-time intelligence gathering, government officials would be forced to revert to the antiquated standards of the Foreign Intelligence Surveillance Act (FISA), which requires the approval of a special court even to monitor terrorist targets overseas.



In the first place, FISA has been updated repeatedly since the September 11, 2001, so the idea that it's ""antiquated"" is silly. Don't listen to me, listen to the president: ""The new law [in 2001] recognizes the realities and dangers posed by the modern terrorist. It will help us to prosecute terrorist organizations — and also to detect them before they strike.""   
  
In the second place, FISA does not, and never has, required a warrant to eavesdrop on foreign communications. FISA only comes into play when intercepting communications between foreigners and Americans, or when conducting surveillance entirely within the United States.   
  
Laksin continues: 



One of the signal virtues of the PAA is the fact that it provides liability protection to private companies, like telecoms, who cooperate with the government and aid surveillance efforts. Companies like AT&T already face multibillion dollar lawsuits from leftist activist groups like the Electronic Frontier Foundation, who charge that the companies broke the law by assisting government efforts to prevent terrorist attack. With the expiration of the PAA, these companies will lose their legal protections. In the current litigious climate, it is more than likely that they will simply stop aiding the government in its intelligence work.



The Protect America Act, which was passed last August, did not include retroactive immunity. That's why there are pending lawsuits against the telecom companies from those ""leftist activist groups."" The PAA _does_ include liability protection for firms that cooperate after the law takes effect, and those provisions will expire on Saturday. However, the idea that this will cause telecom companies to stop ""cooperating"" is absurd. Telecom companies cooperate with eavesdropping not out of the goodness of their heart, but because (once the executive branch has gotten the appropriate warrant) they're legally required to do so. That will continue to be true after the PAA expires. And in any event, the law is pretty clear on this subject. The only ""liability protection"" they really need is to follow it.   




And on we go: 



To be sure, the version of the PAA bill that passed the Senate is far from perfect. For one thing, the bill vastly expands the role of the FISA court in surveillance work, a prospect that should alarm anyone concerned about intelligence agents’ ability to respond rapidly to potential threats.



I'm not sure what he's referring to. It's true that the Senate legislation would require the executive branch to file various reports with the FISA court. But given that it simultaneously eviscerates the requirement to get a FISA warrant for foreign-to-domestic communications, I don't see how it could plausibly be considered an expansion of the FISA court's role. And these reporting requirements _certainly_ wouldn't degrade agents' ability to respond rapidly to potential threats because it gives the government several days after the fact to submit the appropriate reports. Probably the most stringent requirement in the Senate bill is the one requiring the attorney general to send a copy of each ""certification"" he signs to the FISA court within five days. Running off a copy of an order and sending a courier over to drop it off hardly seems like an intolerable burden.   
  
I could go on, but you get the point. The problem is that most readers have neither the time nor the patience to research these issues in any detail. So when conservative pundits make misleading claims, a lot of readers can't tell the difference. It's very frustrating for those of us who are actually familiar with the underlying facts.   
  
(Cross-posted at The Technology Liberation Front)


"
"
Share this...FacebookTwitterA few days ago I wrote here how green activists are (suddenly) horrified that the state of Vermont and Green Mountain Electric Company are defiling pristine mountain ridge-lines to make way for Big Wind, and thus are now protesting (and no longer supporting) wind parks in northeastern Vermont.

Scott Wheeler interviews 2 Big Wind protesters, who propose solar energy as a way to keep prices down in Vermont.
One of the leaders of the protest is Steve Wright, who with Stacey Burke, are shown being interviewed by Scott Wheeler at a sort of homemade TV station. There are some interesting parts that show how the green mind ticks (not very well, you’ll soon see).
Once a Big Wind fan, now an opponent!
At the 8:20 mark note how Burke says she was once enthusiastic about wind power on mountain tops, and now admits she had been too ignorant to know better.
When I first heard about the wind towers going up on Lowell Mountain, I actually thought, oh wow, how cool is that? […] Because I was ignorant of what was really gonna happen.”
Don’t you just love people who can’t make up their minds?
Intermittent wind can’t replace nuclear power, but solar can!
At the 13:00 minute mark, anti-Big Wind activist Wright is asked if wind towers could replace Vermont Yankee Nuclear power plant. His answer:
“The answer is no with a big exclamation point. There is no relationship between whether Vermont Yankee lives or dies and the placement of industrial wind turbines on Vermont ridge-lines, especially with Lowell Mountain. And the reason is that Vermont Yankee represents a kind of power source that is referred to as base load. That means it’s running all the time. It is always available. The wind doesn’t always blow, so therefore the turbine installations are referred to as intermittent power. Intermittent power cannot replace baseload power.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So far so good. He’s right about that. But then listen to what he says just seconds later at the 14:55 mark after being asked how we could supply the GROWING need for power:
Our answer for the long term with Vermont, is to invest seriously in two approaches. One is solar, and two is aggressive action on efficiency. We can especially reduce the use of home heating fuels in that situation, thereby reducing our carbon emissions…”
Intermittent solar can work, but intermittent wind cannot? Who is he trying to kid? Solar, like wind, is also an intermittent supply. And it’s a heck of a lot more expensive. Maybe the sun shines at night in Vermont.
Wind energy is too expensive…the solution is solar!
At the 18:55 mark Wright is asked about the high costs of wind power for consumers: Listen to his ridiculous answer:
Actually what we’re trying to do is to save Vermomt rate payers money by having a more effective energy planning process and an effective long term energy plan in Vermont. The rates that will emerge, imposed on Vermonters, on customers, from wind projects are gonna be higher than basically anything we have functional right now. So the ratepayers are gonna get hosed by the high prices, high electrical prices the more aggressively wind energy is installed.”
Yet he above proposes solar energy, which is several times more expensive than wind! He proposes using solar energy to rescue Vermonters from high electricty prices. I wonder if he takes a hammer to his head to cure a headache.
I probably should be grateful for Wright and the protests, which do seem to be having an effect. But what Wright proposes instead of wind is much worse. Solar power? Get real.
Greens grateful that FOX NEWS covered the protests!
Finally at the 25:00 minute mark, they discuss media coverage of the Big Wind protests. The greens give kudos to (conservative) FOX 44…the only statewide media outlet to be present at a protest. Now I bet that’s something they didn’t expect.
 
Share this...FacebookTwitter "
"
Guest Post by Thomas Fuller
Depending on when this gets posted, the post Anthony put up titled “O…M…G – Video explodes skeptical kids in bloodbath” may have sunk quite a bit down the pile of posts–Anthony and his squad are prolific posters.
But it can’t get any lower than the content shown in 10 10’s video. A relatively innocuous campaign to persuade people to lower their own emissions by 10 percent has pretty much exploded (literally) any hope that the debate can rise above the Wes Craven level. What’s next? The Last House on the Left… Isn’t Insulated?

The idea that blowing up skeptics is the proper response isn’t at all new–and skeptics have known this for ten years, if the drivel I get in my inbox is any indication at all. The very phrase ‘denier’ comes from a concerted campaign to show skeptics (and lukewarmers like myself, although we often get the double whammy title delayer and denier) as equivalent to those who denied the Holocaust occurred.
There has been a concerted campaign to paint everyone who does not agree with Al Gore and James Hansen as monstrous, ranging from allegories with the railroad trains filled with coal heading to some concentration camp to the late Stephen Schneider’s pathetic paper attempting to assert primacy and purity by miscounting academic publications.
But this is hate speech, pure and simple. It legitimizes almost any action against or characterization of those who do not agree with the most hysterical version of Catastrophic and Cataclysmic Climate Change–shoot ’em all and let God sort ’em out.
Using ten-year-old kids as both props and victims is a particularly nice touch.
When DDB created an ad for the WWF showing planes crashing into the World Trade Center as an advertisement asking for support for green activism, it was grotesque, tasteless and an insult to all who suffered losses on September 11th, 2001. It would have been impossible to imagine a cruder, less sensitive call to green action.
Until now.
For any of those on the activist side who wonder why skeptics (and lukewarmers) don’t trust the communications put forward by their team, they might wonder just how much any sign of reason might be contaminated by the stench from garbage like this.
Thomas Fuller href=”http://www.redbubble.com/people/hfuller


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88627db3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The statement and document from the Royal Society follows this press release
 The  Global Warming Policy Foundation, 30 September 2010
LONDON, 30 September – The Global Warming Policy  Foundation has welcomed the Royal Society’s decision to revise and tone down its  position on climate change. Its new climate guide is an improvement on their  more alarmist 2007 pamphlet which caused an internal rebellion by more than 40  fellows of the Society and triggered a review and subsequent revisions.
The former publication gave the misleading  impression that the ‘science is settled’ – the new guide accepts that important  questions remain open and uncertainties unresolved. “The Royal Society now also  agrees with the GWPF that the warming trend of the 1980s and 90s has come to a  halt in the last 10 years,” said Dr Benny Peiser, the Director of the GWPF.

Dr David Whitehouse, the science editor of the GWPF  said: “The biggest failing of the new guide is that it dismisses temperature  data prior to 1850 as limited and leaves it at that. It would cast a whole new  light on today’s warming if the Medieval Warm Period, the Roman Warm Period and  the Bronze Age Warm Period were as warm as today, possiblity even warmer than  today. A thorough discussion of the growing empirical evidence for the global  existence of the Medieval Warm Period and its implications would have been a  valuable addition to the new report.”
In their old guide, the Royal Society demanded that  governments should take “urgent steps” to cut CO2 emissions “as much and as fast  as possible.” This political activism has now been replaced by a more sober  assessment of the scientific evidence and ongoing climate debates.
“If this voice of moderation had been the Royal  Society’s position all along, its message to Government would have been more  restrained and Britain’s unilateral climate policy would not be out of sync with  the rest of the world,” Dr Peiser said.
###
The statement and document from the Royal Society follows:
Climate change: A Summary of the  Science
The Royal  Society, 30 September 2010
Climate change continues to be a subject of intense  public and political debate. Because of the level of interest in the topic the  Royal Society has produced a new guide to the science of climate change. The  guide summarises the current scientific evidence on climate change and its  drivers, highlighting the areas where the science is well established, where  there is still some debate, and where substantial uncertainties remain.
The document was prepared by a working group chaired  by Professor John Pethica, Vice President of the Royal Society and was approved  by the Royal Society Council.
Download  the guide here (PDF).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88979985',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
IPCC chairman Dr. Rajenda Pachauri
Guest Post by Thomas Fuller 
There is a core of uber-consulting professionals, jetting around the world  advising companies, governments and NGO’s. They are well-educated, have  impeccable resumes and travel more than George Clooney did in ‘Up in the Air.’  They work for companies like McKinsey, Price Waterhouse Coopers, and a handful  of others.
Rajendra Pachauri is one such, coming from the Tata school of consultancy.  He is charismatic, projecting leadership qualities and obviously considers  himself a polymath, able to lead a secretariat of the UN, continue his  professional duties and write a popular bodice ripper of a novel.
Sadly, like so many other uber-consultants, Pachauri’s leadership qualities  have been more apparent than real. While others are using the current troubles  at the IPCC as a reason to argue for his resignation, they are really more of a  symptom of the real problems.

Because the IPCC is very small and its primary mission is to produce a  report once every five or six years, it is vulnerable to the type of leadership  Pachauri apparently provides–detached, aloof, hands-off. That Pachauri had time  to write a book during the firestorm of Climategate and COP-15 is evidence that,  whatever his capabilities, his performance at the IPCC was not sufficiently  engaged. His shabby treatment of IPCC scientists regarding the error on  Himalayan glaciers is more of an exclamation point than anything else.
Roger Pielke Jr. and others are saying Pachauri should resign because of  conflicts of interest. Pachauri is director of TERI and advises third parties on  energy policy and investment decisions. Pielke is right in saying that Pachauri  would not meet the standards for avoiding conflicts of interest in many other  organisations, including other UN bodies.   But those standards are not in place at the IPCC, although they are  recommended in yesterday’s report from the InterAcademy Council.
I also think Pachauri should resign. But not because of conflicts of  interest. His continued involvement with TERI, his taking time to write a book,  his hectic social schedule all point to another, more serious problem.    His detached style of leadership has coincided with a period of continuous  problems at the organisation he leads. And I’m not referring to the occasional  error that inevitably slips into their huge assessment reports.    The IPCC has not moved with the times during Pachauri’s tenure. They have  not adapted to an age of the Internet in facilitating communications.
They have not recognised the political pressure that environmental  organisations are trying to put on national and international governments and  institutions. This has led to a careless over use of ‘grey’ literature, which is  not peer reviewed and often has a clear point to push.   The IPCC has not instituted a clear and effective way of dealing with  mistakes, despite it getting ever easier to do this.   Perhaps most damaging, the IPCC has adopted a view on communications that  is from another century, focused on getting their message out, as opposed to  listening and responding.
These are classic failures of leadership. Nobody but Rajendra Pachauri is  responsible for these problems. Good leadership would have corrected them years  ago. Detached leadership smiles and writes a book.   Pachauri played socialite while his organisation stagnated. He received  awards–not just the Nobel Prize, which he shared with Al Gore, but also the  French Legion of Honour, Order of the White Rose from Finland, and the Padma  Bhushan from his native India. He is apparently his organisation’s chief press  officer, and its ambassador as well, flying all over the world to meetings and  conferences. And yes, he does have other interests, including the Tata Energy  Research Institute.
The IPCC’s–and Rajendra Pachauri’s–real problem is not a conflict of  interest. It is a lack of interest. Pachauri fiddled while the IPCC foundered.  He should go.
Thomas Fuller http://www.redbubble.com/people/hfuller


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e894a214a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Nativity plays and outdoor carolling will be able to go ahead in England after the national lockdown ends on 2 December, MPs have been told.**
Tory MP Andrew Selous - who speaks for the Church of England - said ""churches and cathedrals can now approach Advent and Christmas with certainty.""
Indoor singing would be limited to formal performers, he added, but everyone can take part outdoors.
Children's church nativity plays will be allowed if they follow Covid rules.
Rules on outdoor gatherings will vary across England as the tier system comes in to force after the full lockdown, but all areas restrict groups to no more than six people.
However, churches and other places of worship will be able to perform services in all three tiers, including at Christmas.
During England's national lockdown, places of worship have been closed for group activities and services, with only private prayer allowed.
Speaking at Church Commissioners question time in the Commons, Mr Selous said: ""From 2 December places of worship can reopen for public worship and churches and cathedrals can now approach Advent and Christmas with certainty.""
He said the clergy had already made their buildings ""Covid-secure"" and said ""many cathedrals and churches are planning to have multiple services to accommodate more people as less are allowed in each service"".
""The further good news is that while indoor singing is limited to performers only, we can all take part in outdoor and door-to-door singing, staying two metres apart or away from the threshold, and nativity plays for under-18s are permitted in accordance with the performing arts guidance,"" he added.
On Tuesday, all four UK governments announced plans to allow families to meet for the festive period.
People will be able to form a ""Christmas bubble"" of three households, who can meet indoors between 23 and 28 December.
The news on carolling comes after a group of leading musicians wrote to Culture Secretary Oliver Dowden to encourage him to allow outdoor singing.
They said traditional house to house carolling groups usually raise Â£10m for charities each festive season."
"**Slough will face the toughest level of measures when England's national lockdown finishes next week.**
The government has announced the local authority area will be in the ""very high"" alert tier three from 2 December.
Slough Borough Council said the number of the cases was falling but not ""quickly enough"".
The measures mean pubs and restaurants will remain closed in the town, and no mixing between households is allowed indoors or in private gardens.
However people can meet in groups of six in outdoor public spaces, and non-essential shops, gyms, and personal care services will all reopen.
Lead member for health councillor Natasa Pantelic said: ""Unfortunately cases in Slough, though coming down, are not dropping significantly or quickly enough putting Slough people at risk.
""While this risk remains so high, I call on all residents to follow the lockdown rules until 2 December and then stick to the regulations of our new tier to protect themselves, their families and communities from what can be a devastating illness.""
Although Slough has seen a dip in the number of cases, it still has the highest infection rate in Berkshire with 313.6 cases per 100,000 as of Thursday, according to the Local Democracy Reporting Service (LDRS).
Reading, Wokingham, Bracknell Forest, Windsor and Maidenhead and West Berkshire will go into tier two on 2 December.
It comes as it was announced most of England will be place in the two toughest levels of measures when lockdown ends."
"

Robert G. Kaiser shows in today’s _Washington Post_ what many of us have known for some time: notwithstanding their differences over the wisdom of going to war in Iraq, Barack Obama and John McCain may largely agree on the wisdom of going to war in general.   
  
  
Neither man wants you to believe that, of course. It behooves them to highlight their differences, both to rally their core supporters, and to make an affirmative case for why they should be chosen by the voters to lead the country for the next four years. These differences are most pronounced in domestic matters: in fiscal policy and on taxes, on health care, and on the benefits of international trade.   
  
  
But, Kaiser writes, the two candidates share many similar views on national security: 



[B]oth have revealed a willingness to commit U.S. forces overseas for both strategic and humanitarian purposes. Both agree on a course of action in Afghanistan that could lead to a long‐​term commitment of American soldiers without a clear statement of how long they might remain or what conditions would lead to their withdrawal.   
  
  
Both candidates favor expanding the armed forces, Obama by 92,000 and McCain by as many as 150,000. Both speak of situations when the United States might have to commit its troops for “moral” reasons, whether or not a vital American interest was at risk. Both accept what Andrew Bacevich, a retired Army colonel and professor at Boston University, calls the “unspoken consensus which commits the United States to permanent military primacy” — shared, Bacevich said, by leading figures in both parties.



Obama has worn his opposition to the Iraq War as a badge of honor. And rightly so. His principled stand, taken at a time when precious few politicians were willing to do the same, has allowed him to turn his opponents’ (first Clinton and now McCain) supposed advantage — their experience — into a liability, or at least a nullity. If experienced politicians could make such a colossal blunder as to support a war that now two thirds of all Americans believe to have been a mistake, then what is the value of experience?   
  
  
But the great unknown remains the lessons that Obama has taken away from the Iraq experience. Was the forcible removal of Saddam Hussein from power a good idea, poorly executed? Or was it a bad idea at the outset, further complicated by bungling in the Executive Branch? Obama has signaled that he believes the latter, but some of his advisers seem to have more confidence in their ability to pull off similar missions in the future — say, for example, against the government in Sudan, as Obama advisers Susan Rice and Tony Lake suggested in late 2006.   
  
  
Given the continuing influence within the Democratic Party of the so‐​called liberal hawks, there is even the disturbing possibility that a President Obama would be more prone to military intervention than his predecessor.   
  
  
That said, John McCain’s continued strong support for the Iraq War is merely one of many examples of his enthusiasm for using our military to solve distant problems. He has adopted a similarly bellicose stance toward North Korea and Iran, and has hinted darkly at a confrontational posture toward Russia that could ultimately result in a ruinous military conflict. In that respect, I wholeheartedly agree with Justin Logan’s deliberate ambivalence in his most recent paper, “Two Kinds of Change: Comparing the Candidates on Foreign Policy”: “The best case that can be made for Senator Obama’s foreign policy is the fact that the alternative to his approach is Senator McCain’s.”   
  
  
It is possible, perhaps even likely, that the lingering effects of the Iraq War will greatly limit the next president’s enthusiasm for foreign military intervention. But nothing that either candidate has said during this campaign gives me sufficient assurances that that is the case. Foreign policy has generally been pushed aside during this long campaign, an understandable shift given the current economic climate. But it is not too late for both men to clarify their views on the use of force, and to explain how they might differ from their opponent.
"
nan
"
Share this...FacebookTwitterIn the 5-minute video below, there are lots of statements made that are worth quoting.
Many are from environmentalists who now seem to realize something went horribly wrong. “Green yes, but not like this!”

One of my favorites is by Justin Lindholm at the 4:35 mark:
Maybe 10,000 years from now they’ll come around and wonder what went on here. […] Pads and pedestals look like sacrificial sites of some sort, and they are.”
10,000 years? The human species indeed can be frustratingly stupid at times, but not that stupid. I say give them less than a generation, 20-30 years tops. Already people are waking up, now even making protest videos against the madness. The protest is already getting into full swing.
I wonder how many of the environmentalists in the video were staunch supporters of these windmills in the beginning, before the heavy equipment rolled in and removed the mountain tops? I’d suspect most of them were. For example, see here at the 8:25 mark of this do-it-yourself TV.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Another part I find amusing is the one with activist Gaelen Brown near a home surrounded by panels at the 4-minute mark. Nothing like endowing the landscape with a little traditional Vermont charm! Sorry Gaelen, but that yard with the panels and home look god-awful ugly. I’m glad that house is not in my neighborhood. At least put the damn panels on the roof.

Dream eco-home? It looks more like a solar-powered guard tower for a prison. Other amenities: no windows facing south (or the panels are facing north), the roof cannot support very many panels, installed on the ground instead, and the sun hardly shines – which is typical in Vermont. Classic example of what the Big Green virus can do to someone’s mind. The architect of this beauty was definitely out to lunch. (Photo source: energizevermont.org video)
German readers should send this video to the leaders of Baden Wurttemberg and Bavaria, as they too appear to have been recently infected by the Big Green virus, which leads to clouded thinking and even to irreversible madness in some cases.
Finally, one last point on Gaelen’s remark about “cheap” solar panels. Today solar panel energy is still about 10 times more expensive than conventional electricity, and its forced ineffective use is one of the reasons other products like cement and glass are getting more expensive.
===========================
Also, state prosecutes journalist reporting the story for trespassing!
Share this...FacebookTwitter "
"

No real news came out of the three days of Amy Coney Barrett’s Supreme Court confirmation hearings. As expected, Democrats focused on Obamacare and abortion, with a little bit on guns and voting rights. Also as expected, the nominee declined to discuss any pending or potential cases, or any legal issue on which she hadn’t already opined in scholarly or judicial writings.



For their part, Republicans allowed Barrett to show her intellect and verbal facility by explaining such frequently used terms as “originalism” (interpreting constitutional provisions according to their original public meaning), “textualism” (interpreting statutes according to the plain meaning of their text as opposed to legislative purpose), and stare decisis (letting erroneous precedents stand because correcting them would cause more social harm than allowing the error to persist).



In all, no senator’s vote was changed. However, the public, which according to a CNN poll last week was divided on Barrett’s nomination, did get a chance to sympathize more with the judge, who showed grace and poise under pressure. Like it or not, she will be confirmed before the election, barring a black swan event, like a massive Covid‐​19 spread that prevents the Senate from meeting or some Kavanaugh‐​like post‐​hearing allegations.



Does that mean the hearings are pointless, a kabuki process that wastes everyone’s time? Elsewhere I’ve written that they once served a purpose but have at best devolved into empty platitudes and gotcha games. I won’t rehash those arguments here, other than to note that it may have served the Democrats better to parallel the Merrick Garland maneuver of four years ago by abstaining from what they consider to be a pointless exercise — refraining from attacking Barrett’s views while making a process argument to the voters.



With the two parties adopting incompatible judicial philosophies, it’s impossible to find an “uncontroversial” nominee. 



Of course, they didn’t do that, so we’ve been treated to what then‐​professor Elena Kagan called a “vapid and hollow charade.”



But even if we didn’t learn anything this week, there was refreshing clarity on the parties’ divergent judicial philosophies. By that I don’t mean whether Roe v. Wade was correctly decided or the scope of the Second Amendment, but the difference between law and policy. To take two contrasting examples from Wednesday’s session: (1) Republican Senator from Texas Ted Cruz discussed how he favors school choice — calling it “the civil rights issue of the next century” — but that it’s not the place of a federal judge to impose it, while (2) Democratic Senator from Hawaii Mazie Hirono called the distinction between law and policy “artificial” in arguing that the Affordable Care Act must be constitutional because so many people depend on it. Indeed, each of the Democratic senators had blown up pictures of constituents who would be harmed if Obamacare went away.



Now, emotional arguments are all well and good if you’re trying to appeal to an electorate — as California Senator Kamala Harris used most of her time to do with regard to everything from health care to climate change, because she’s the Democratic nominee for Vice President.



But judges are supposed to do something else: They’re supposed to apply the law, which sometimes leads to unpopular outcomes. Judicial power is not a means to an end, but an enforcement mechanism for the strictures of a founding document intended just as much to curtail the excesses of democracy as to empower its exercise.



In a country ruled by law, the proper response to an unpopular legal decision is to change the law or amend the Constitution. Any other method leads to a sort of judicial abdication and the loss of those very rights and liberties that can only be vindicated through the judicial process.



Nevertheless, given the expansion of federal power, and then the shifting of that power away from the people’s legislative representatives and toward executive branch administrative agencies, the judiciary affects public policy more than it ever did. And court decisions increasingly hinge on the partisan affiliation of the president who nominated the judges making them.



With the two parties adopting incompatible judicial philosophies, it’s impossible to find an “uncontroversial” nominee. That’s doubly so when a nominee’s philosophy represents a big shift from that of the previous justice. As Democratic Senator from Delaware Chris Coons highlighted, Justices Antonin Scalia and Ruth Bader Ginsburg were often on opposite ends of close cases. Replacing Ginsburg with Scalia’s former clerk Barrett would mean a bigger change than replacing the moderate Anthony Kennedy with his former clerk Brett Kavanaugh.



Those jurisprudential differences, and their alignment with ideologically distinct parties, are a relatively new phenomenon; in the grand sweep of American history, things didn’t always line up so neatly. But regardless, the Barrett hearings showed that the parties do have different approaches to the law — and that Democrats don’t see legal questions as divorced from political ones.
"
"

The last 20 years have brought the world more trade, more globalization and more economic growth than in any previous such period in history. Few commentators had believed that such a rise in trade and living standards was possible so quickly.



More than 400 million Chinese climbed out of poverty between 1990 and 2004, according to the World Bank. India has become a rapidly growing economy, the middle class in Brazil and Mexico is flourishing, and recent successes of Ghana and Tanzania show that parts of Africa may be turning the corner as well.



Despite these enormous advances, however, there is a backlash against globalization and a widespread belief that it requires moderation. Ordinary people often question the benefits of international trade, and now many intellectuals are turning more skeptical, too. Yet the facts on the ground show that the current climate of economic doom and gloom simply isn’t warranted. The classic economic recipes of trade, investment and good incentives have never been more successful in generating huge gains in human welfare.





For all the talk of a needed “timeout” from globalization, world trade is actually accelerating, and that is for the better.



The globalization process has had its bumps, of course, as reflected recently by rising commodity prices, but that is largely a consequence of how much and how rapidly prosperity has grown. Countries like China have become richer so fast that global production of energy and food have been unable to match the pace. But rapid economic growth is the right direction, even if some of the remaining poor are suffering from high food prices.



For all the talk of a needed “timeout” from globalization, world trade is actually accelerating, and that is for the better. Big changes often come bunched together, so that when good things are happening it is important to maintain the trend. It’s true that the tariff‐​reducing talks at the World Trade Organization have stalled and that the Democratic Party, at least in its rhetoric, has moved away from the free‐​trade legacy of President Bill Clinton.



But the volume of trade is nonetheless likely to keep rising, if only because the world economy is expanding. Furthermore, a vast majority of Americans have never been better poised to benefit from global exchange and from the prosperity of the rest of the world.



Trade advocates focus on the benefits of goods arriving from abroad, like luxury shoes from Italy or computer chips from Taiwan. But new ideas are the real prize. By 2010, China will have more Ph.D. scientists and engineers than the United States. These professionals are not fundamentally a threat. To the contrary, they are creators, whose ideas are likely to improve the lives of ordinary Americans, not just the business elites. The more access the Chinese have to American and other markets, the more they can afford higher education and the greater their incentive to innovate.



Conservative and liberal economists agree that new ideas are the fundamental source of higher living standards. We urgently need new biotechnologies, a cure for AIDS and a cleaner energy infrastructure, to name just a few. Trade is part of the path toward achieving those ends. A wealthier China and India also mean higher potential rewards for Americans and others who invest in innovation. A product or idea that might have been marketed just to the United States and to Europe 20 years ago could be sold to billions more in the future.



Those benefits will take time to arrive, but trade with China has already eased hardships for poorer Americans. A new research paper by Christian Broda and John Romalis, both professors at the Graduate School of Business at the University of Chicago, has shown that cheap imports from China have benefited the American poor disproportionately. In fact, for the poor, discounting in stores such as Wal‐​Mart has offset much of the rise in measured income inequality from 1994 to 2005.



Despite all these gains, the prevailing intellectual tendency these days is to apologize for free trade. A common claim is that trade liberalization should proceed only if it is accompanied by new policies to retrain displaced workers or otherwise ameliorate the consequences of economic volatility.



Yes, the benefits of a good safety net are well established, but globalization is not the primary source of trouble for most American workers. Health care problems, bad schools for our children or, in recent times, bad banking practices have all produced greater disruptions — and these have been fundamentally domestic failings.



What’s really happening is that many people, whether in the United States or abroad, are unduly suspicious about economic relations with foreigners. These complaints stem from basic human nature — namely, our tendency to divide people into “in groups” and “out groups” and to elevate one and to demonize the other. Americans fear that foreigners will rise at their expense or “control” some aspects of the economy.



One approach is to appease these sentiments by backing away from trade just a bit, or by managing it, so as to limit the backlash. Giving up momentum, however, isn’t necessarily the right way forward. If we are too apologetic about globalization, we can feed core irrationalities, instead of taming them. The risk is that we will frame trade as a fundamental source of suffering and losses, which would make voters more nervous, not less.



It is wrong to play down the costs of globalization, but the reality is that we’ve been playing down its benefits for a long time. Politicians already pander to Americans’ suspicion of foreigners. There is no need for the rest of us to jump on this bandwagon. Instead, we need more awareness of the cosmopolitan benefits of trade and the often hidden — but no less real — gains for ordinary Americans.



If we look at trends of the last 20 years, we have every reason to believe that the modern era of free trade is just getting started.
"
"
Share this...FacebookTwitterOn Cosmic Rays and Clouds
By Ed Caryl
There have been several papers and articles recently about Svensmark’s theory that claim cosmic rays produce cloud nuclei, which in turn produce clouds that affect the Earth’s climate. My recent article showed the relationship between atmospheric transmission, the Earth’s albedo, and temperature.
Photo by Krish Dulal.
If cosmic rays produce micro-particles that grow into cloud nuclei, and if these particles are large and numerous enough to interfere with sunlight, it should be possible to show a relationship between cosmic rays and atmospheric transmission, and thus clouds.
Relationship between albedo and atmospheric transmission
The transmission is measured in a clear-sky situation. Albedo reflection is mostly from clouds. If aerosols impede transmission and aerosols ultimately produce clouds, then there should be a relationship.
Figure 1: Relationship between albedo and atmospheric transmission.
The problem with Figure 1 is that we only have a few annual data points for albedo. There isn’t a single database for global cloudiness with any temporal extent other than the above data. Two of the points are due to a volcanic eruption. Without those points only a slight relationship remains.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Figure 2: Relationship between albedo and atmospheric transmission after subtracting the two years of Pinatubo volcanic dust.
This result seems to suggest that only large dust and sulfate particles make much difference to clouds and albedo. This may explain the next result.

Figure 3: Monthly data plot of atmospheric transmission and the Oulo, Finland neutron count proxy for cosmic rays. Source of neutron data here.
The problem with this plot is that the volcanic eruptions just happened to occur at negative cycles of the neutron count. We know (or are at least fairly certain) that volcanoes and cosmic rays are not related in any way. For that reason, in the next plot, data from the two years after each volcanic eruption was deleted to avoid a false appearance of correlation.

Figure 4: Monthly data plot of atmospheric transmission versus Oulu neutron count.
Without the volcanic activity in the plot there is no relationship between atmospheric transmission and cosmic rays as measured by the neutron count at Oulu, Finland. The R-squared value is below 0.01, and in the wrong direction. The relationship is completely random.
This result does not necessarily falsify Svensmark’s theory. There may be an explanation as to why cosmic ray flux does not show up in atmospheric transmission. As suggested above, perhaps the particles are too small, or too infrequent compared to other aerosols, but it is another mystery that demands an explanation.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterCold temperatures are nothing unusual in Antarctic, especially in the wintertime. But one observer in the Internet here has noticed it’s been far colder than usual.

Shown above is the projected 2m temperature anomaly for Antarctica for 30 July until 6 August, 2012. Source: Dr. Ryan N. Maue.
The observer writes:
For weeks I’ve been observing extreme, unusual deviations from the mean by as much as -20°K. Even in Australia it’s been too cold. What’s the reason for this cold over there, the powerful Antarctic polar circulation?
While the media remains locally fixated on a warm June in the US, it is ignoring an extreme cold event in a region that is supposed to be a “canary in the coal mine”.
Note:
I’m a bit tied up right now, and so blogging will be on the light side until this weekend. If anyone has more on the Antarctic cold, let me know!
Share this...FacebookTwitter "
"American congresswoman Alexandria Ocasio-Cortez recently shook up environmental politics by releasing a broad outline of a Green New Deal – a plan to make the US a carbon-neutral economy in the next ten years, while reducing both poverty and inequality. Lauded by many as a radical and necessary step, president Trump responded in typical style: The Green New Deal doesn’t directly call for people to consume less meat. But the argument that solving climate change means changing our diets is widespread, and Ocasio-Cortez herself has made the link.  Yet Trump’s tweet was actually on the money in more ways than one. Environmental measures, and solutions to climate change, often appear (or are talked about) as programs of austerity. To reduce “our” impact “we” need to consume less: eat less meat, walk and not drive, fly less, buy less fast fashion, and so on. From personal carbon footprint calculators to articles outlining how many Earths we need to sustain the consumption of the average citizen of the UK, Europe or the US, consumption is identified as the problem. Reduce consumption, runs the argument, and you solve climate change. But is “our” consumption really the problem? Who is “we” anyway? This point has been made before, but bears repeating. Most of the world’s population produces very little in the way of either carbon emissions or broader environmental impacts. We can go further here by also looking at imported carbon emissions – that is, the emissions that come from the production of goods and services in countries such as China that are then consumed in the wealthy countries of the global north. If we include imported emissions, the UK’s overall emissions have only marginally decreased since 1990.  When we approach carbon emissions this way, it’s clear the problem isn’t overpopulation or China, but the richest people on earth. After all, being rich, especially ultra-rich, means being directly responsible, either through consumption or control, for the majority of the world’s carbon emissions. For instance, the charity Oxfam has found that the richest 10% of people produce half of the world’s carbon emissions, while the poorest half contribute just 10%. Who are the richest 10%? The figure is not about nations but people – the 770m or so people who make up the richest tenth of the world’s population. The disparity is even more startling when we look at the differences between the ultra-rich and the bottom 50% at a global level, where a typical ultra-rich individual produces 35 times the carbon emissions of someone in the bottom half, and 175 times the amount of someone in the poorest 10%. This cohort of ultra-consumers are not spread evenly around the globe. Some 40% live in the US, around 20% live in the EU and 10% in China. Focusing on the richest 10% is a useful way of looking at things as carbon emissions aren’t only globally uneven, they are also uneven within national borders. The key detail here is the massive disparity in most wealthy countries between the emissions of rich and poor households. In both the US and the UK, the richest 10% produce at least five times the emissions of the poorest 50%. And this is just their consumption emissions (and doesn’t include those emissions produced by the people who work for them – their cleaners, drivers, and so on – which would further expand their impacts). We could further compound these figures by looking at the imbalance between genders, where men tend to produce more carbon emissions than women, or racial inequality that extends even to emissions, with white people producing more than everyone else. But that’s not all. While it’s relatively simple to account for the vast initial disparity – being rich after all is about having more money, more stuff, bigger super-yachts and houses – this fails to account for the entirety of the disparity. Being wealthy gives you more political influence. It means funding political parties and campaigns, having access to law makers and lobbyists. And it means control over major corporations, and thus power over the businesses and industries which produce most of the carbon emissions.  The problem with stories of over-consumption isn’t just that consumption is far from even – the problem is that consumption is often made out to be a matter of choice. Discretionary income – the portion of your money left over after paying for everything you need – increases the richer you get. For most people, there just isn’t much left over once you’ve paid for the things you need. And if we then include those so-called discretionary items that really aren’t anything of the sort – mobile phones, for instance – then most people really don’t “choose” to consume in any meaningful way. More than this, what they can choose from is largely determined by large transnational corporations, which are often controlled by the same ultra-wealthy people whose consumption is disproportionately the problem. Given the problem is overwhelmingly, dare I say it, rich white men, we don’t do ourselves any favours by assigning blame to whole populations – be it humanity, Americans, or even the whole global north. Thinking this way makes it harder to identify the actual source of the problem and formulate solutions to it. That is to say, rather than signing on for yet another call for meat free Mondays and giving up meat, we’d be better off “eating the rich”. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

_Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Global warming beater Justin Gillis of the _New York Times_ had an article yesterday describing a new paper in the current issue of _Nature_ magazine, the point of which seems to be scaring people with alarming global warming statistics.   
  
Gillis’ article “By 2047, Coldest Years May Be Warmer Than Hottest in Past,” describes the results of a class-project-cum- _Nature_ -article headed by Camilo Mora from the University of Hawaii at Manoa (please, no puns). The class assignment was to identify the year for each spot on the globe in which all future years were, according to climate model projections, warmer as a result of greenhouse gas emissions than the warmest year simulated by the models during the historical period 1860 to 2005. Mora and students termed this pivotal year the “climate departure.”   
  
This work is significant, according to Gillis, because:   




Thousands of scientific papers have been published about the model results, but the students identified one area of analysis that was missing. The results are usually reported as average temperature changes across the planet. But that gives little sense of how the temperature changes in specific places might compare with historical norms. “We wanted to give people a really relatable way to understand climate,” said Abby G. Frazier, a doctoral candidate in geography.



Perhaps Dr. Mora should have injected a little climate-science history in this class.   
  
Looking at the time that a human climate signal will rise above the background noise is not particularly a novel concept. It’s commonplace. We would guess that a signal-to-noise ratio was probably present in the first papers describing the performance and output of the very first climate models.   
  
After all, without such information it is impossible to put absolute changes in perspective. Some measure of the statistical significance of climate change has been present in every climate assessment report from the U.N. Intergovernmental Panel on Climate Change dating back to 1990.   
  
In our presentation to the Science Policy Conference of the American Geophysical Union this summer, we even included a table listing the number of years into the future it would be before projected changes in precipitation across the U.S. rose above the level of nature variability. We guess we just didn’t give that year a catchy enough name like “climate departure,” because our results didn’t capture the attention of the press (nor were they very frightening).   
  
But Gillis does manage to carve some new, scary Jack-o-Lanterns from the Mora study.   
  
Here is his lead paragraph:   




If greenhouse emissions continue their steady escalation, temperatures across most of the earth will rise to levels with no recorded precedent by the middle of this century, researchers said Wednesday.



Uh, correct us if we are wrong, but we already thought that global temperatures were reported to be at unprecedented levels in recorded history. According to the IPCC’s _Fifth Assessment Report_ :   




Each of the last three decades has been successively warmer at the Earth’s surface than any preceding decade since 1850.



So, is this recycled news, or is the new paper saying that we have to wait until 2047 for that to happen? Well, whatever, it sounds B-A-D.   
  
Or how about this one:   




“Go back in your life to think about the hottest, most traumatic event you have experienced,” Dr. Mora said in an interview. “What we’re saying is that very soon, that event is going to become the norm.”



Hot Tub Time Machine came immediately to mind, but Gillis provided another scenario:   




With the technique the Mora group used, it is possible to specify climate departure dates for individual cities. Under high emissions, climate departure for New York City will come in 2047, the paper found, plus or minus the five-year margin of error.



How scared should you be about passing the date of “climate departure”?   
  
Not at all.   




In Figure 1, we show the complete observed (rather than modeled) history of the annual average temperature from New York City’s Central Park, spanning from 1869 through 2012.   






_Figure 1. Annual average temperature from New York’s Central Park, 1869-2012 (_ _data_ _from the New York City Office of the National Weather Service)._



Here are some not-so-scary facts, that by others would be passed off as horrors:   
  
● The _average_ temperature in Central Park for the past 83 years (since 1930) (54.8°F) is greater than the _warmest_ year during the first 39 years of the record (1869-1907) (54.7°F).   
  
● There has only been one year in the last 20 years of the record that was _colder_ (by just 0.2°F) than the _warmest_ year during the first twenty years of record.   
  
So essentially, New York City has already reached its “climate departure” date and no one noticed.   
  
By his own estimation, the older author of this blog post (PJM) has lived through nine environmental ends-of-the-words-as-we-know-it. What’s new here?   
  
Whether the climate departure date in New York was reached as a result of the heat of urbanization, natural climate variability, human-induced global warming, or the likely combination of all three, its passage is of virtually no practical significance. Yes, it is warmer now that it was 150 years ago.   
  
As concerned as readers of the _New York Times_ might be, they are living twice as long as they did back then, and, in Manhattan, are richer than Croesus.   
  
Science/science policy expert Roger Pielke Jr. put the new Mora article in perspective (although not in the Justin Gillis article, but rather at NBCNews.com):   




But trying to compel action with a stark warning about a future that is coming regardless of what efforts are taken to curb greenhouse gas emissions may be misguided, according to Roger Pielke Jr., a climate policy analyst at the University of Colorado at Boulder.   
  
""It is better to design policies that have short-term benefits"" such as jobs, energy access or less pollution ""which can also address the longer-term challenge of accumulating (carbon dioxide) in the atmosphere,"" he said. ""That is a policy-design problem that we have yet to figure out, and which does not involve trying to scare the public into action.""



But what attention would come to climate change if the researchers, the media, and the government weren’t complicit in trying to scare people into giving up some of their freedoms to try to mitigate it?   
  
Trick or treat? Happy Halloween!


"
"**Five more people have died at a Llangollen care home, taking the total Covid-19 deaths in the last three weeks there to 20.**
There are 11 new positive test results at Llangollen Fechan Care Home, which include four residents and seven staff.
A total of 60 residents and 40 staff have tested positive for the virus since the outbreak began.
An incident management team is investigating the cause of the outbreak.
Nicola Stubbins, co-chairwoman of the incident management team which involves Denbighshire County Council, Public Health Wales and Betsi Cadwaladr University Health Board, said the team continued to monitor the situation.
""We are very sad to report these further deaths and are very concerned about a number of residents who are currently very poorly,"" she said.
""Unfortunately, residents who are already vulnerable through their age or pre-existing conditions are more likely to suffer the worst outcomes from this deadly virus and our thoughts are with all of those affected.
""We still expect to see cases in a variety of settings, and we manage any clusters of coronavirus appropriately.""
Wales' first minister described the deaths at the care home as ""a very sad story"" on November 20.
At a coronavirus briefing, Mark Drakeford said the rules around care homes were much stronger now than they were in the spring because of a better understanding of how Covid-19 spreads.
""A lot of help is being provided both to that care home in Llangollen, where there is that very sad story today, but other care homes as well, to make sure that all those basic things the care home itself has to take responsibility for are being done in the best possible way,"" he said.
Mr Drakeford also explained that discussions are continuing over the use of the latest testing equipment and that care home staff and residents ""are very much part of that conversation""."
"Search online for “climate change” and “tipping points” and you’ll find some scary results. Melting ice sheets, the collapse of the Atlantic thermohaline circulation , the permafrost methane “time bomb” and the die-back of the Amazon rainforest threaten to exacerbate the climate crisis and send global warming spiralling out of control. But what if we could leverage similar tipping point dynamics to solve the climate problem? Like physical or environmental systems, socioeconomic and political systems can also exhibit nonlinear dynamics. Memes on the internet can go viral, loan defaults can cascade into financial crises, and public opinion can shift in rapid and radical ways. 


      Read more:
      What climate 'tipping points' are – and how they could suddenly change our planet


 In an article just out in Science, we outline a new approach to climate change that tries to find areas in socioeconomic and political systems that are “sensitive” – where modest but well-timed interventions could generate outsized impacts and accelerate progress towards a post-carbon world. These “Sensitive Intervention Points” – or SIPs – could trigger self-reinforcing feedback loops, which can amplify small changes to produce outsized effects. Take, for example, solar photovoltaics. As more solar panels are produced and deployed, costs fall through “learning-by-doing” as practice, market testing and incremental innovation make the whole process cheaper.  Cost reductions lead to greater demand, further deployment, more learning-by-doing, more cost reductions and so on. However, the spread of renewables isn’t just dependent on technology and cost improvements. Social dynamics can also play a major role. As people observe their neighbours installing rooftop solar panels they might be more inclined to do so themselves. This effect could cause a shift in cultural and social norms. Financial markets are another key area where SIPs could help accelerate the transition to post-carbon societies. Many companies are currently failing to disclose and account for climate risks associated with assets on their balance sheet. Climate risk can entail physical risks, caused by extreme weather or flooding. They can also entail the risk of assets such as fossil fuel reserves becoming stranded as economies transition to limit warming to 1.5℃ or 2℃, when such resources are no longer valuable. Most of the world’s current fossil fuel reserves can’t be used if the world is to limit warming and they become effectively worthless once this is acknowledged. By not accounting for these risks to fossil fuel assets, high-emission industries are effectively given an advantage over low-carbon alternatives that shouldn’t exist. Relatively modest changes to accounting and disclosure guidelines could make a significant difference. If companies are required to disclose information about the climate risks associated with their assets – and if such disclosure is consistent and comparable across companies – investors can make more informed decisions and the implicit subsidy enjoyed by high-emission industries is likely to rapidly disappear. Opportunities for triggering SIPs in a given system can also change over time. Sometimes “windows of opportunity” open up, where very unlikely changes become possible. A key example in the UK was the political climate in 2007-2008 which enabled the 2008 UK Climate Change Act to pass with near unanimous support. This national legislation was the first of its kind and committed the UK to reducing greenhouse gas emissions by 80% relative to 1990 levels by 2050. The act also created a regular ratcheting cycle which encourages more ambitious future climate action. Since 2008, emissions in the UK have fallen dramatically. However, the UK Climate Change Act’s influence beyond the UK is also significant as it encouraged similar legislation in other countries, including the Paris Agreement, which contains the same self-reinforcing ratcheting mechanism. Thinking about SIPs in policy and business could accelerate the post-carbon transition – but much work lies ahead. The first step is to systematically identify potential SIPs and the mechanisms by which they can be amplified.  Unfortunately, traditional economic models commonly used to evaluate climate policy are poorly equipped to do this, but new analytical methods are increasingly being used in policy.  These new methods could provide more accurate insights into the costs, benefits and possibilities of SIPs for addressing climate change. As SIPs could be present in all spheres of life, experts in social and natural sciences will need to work together. The window to avert catastrophic climate change is closing fast, but with intelligent interventions at sensitive points in the system, we believe success is still possible. Since the stakes are so high – and the time frame so limited – it is not possible to chase every seemingly promising idea. But with a smart, strategic approach to unleashing feedback mechanisms and exploiting critical windows of opportunity in systems that are ripe for change, we may just be able to tip the planet onto a post-carbon trajectory. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Joe D’Aleo and Don Easterbrook have produced a new paper for SPPI. This graph of US Mean temperature versus the AMO and PDO ocean cycles is prominently featured:
Figure 18: With 22 point smoothing, the correlation of US temperatures and the ocean multidecadal oscillations is clear with an r-squared of 0.85
I particularly liked the regression forecast fit: 
Figure 20: using the PDO/AMO to predict temperatures works well here with some departure after around 2000.
They have this caveat:
Note this data plot started in 1905 because the PDO was only available from 1900. The divergence 2000 and after was either (1) greenhouse warming finally kicking in or (2) an issue with the new USHCN version 2 data.
Hmm. I’m betting USHCNv2.
Abstract:
Perlwitz etal (2009) used computer model suites to contend  that the 2008 North American cooling was naturally induced as a result  of the continent’s sensitivity to widespread cooling of the tropical (La  Nina) and northeastern Pacific sea surface temperatures.
But they  concluded from their models that warming is likely to resume in coming  years and that climate is unlikely to embark upon a prolonged period of  cooling. We here show how their models fail to recognize the  multidecadal behavior of sea surface temperatures in the Pacific Basin,  which determines the frequency of El Ninos and La Ninas and suggests  that the cooling will likely continue for several decades. We show how  this will be reinforced with multidecadal shift in the Atlantic.
Here’s the paper you can download:
Click for full report (PDF)
UPDATE: The goodness of fit,  seems almost too good. There may be a reason. I’m reminded in comments of this article by statistician William Briggs – (thanks Mosh)
Do not smooth times series, you hockey puck!
Where he points out:
Now I’m going to tell you the great truth of time series analysis.  Ready?  Unless the data is measured with error, you never, ever, for no reason, under no threat, SMOOTH the series! And if for some bizarre reason you do smooth it, you absolutely on pain of death do NOT use the smoothed series as input for other analyses!   If the data is measured with error, you might attempt to model it  (which means smooth it) in an attempt to estimate the measurement error,  but even in these rare cases you have to have an outside (the learned word is “exogenous”) estimate of that error, that is, one not based on your current data.
If, in a moment of insanity, you do smooth time series data and you do use it as input to other analyses, you dramatically increase the probability of fooling yourself!  This is because smoothing induces spurious signals—signals that look real to other analytical methods.   No matter what you will be too certain of your final results!   Mann et al. first dramatically smoothed their series, then analyzed  them separately.  Regardless of whether their thesis is true—whether  there really is a dramatic increase in temperature lately—it is  guaranteed that they are now too certain of their conclusion.
Perhaps Mr. Briggs can have a look and expound in comments. I only have the output, not the method. But let’s find out and determine how good the “fit” truly is. – Anthony
UPDATE: Statistician Matt Briggs responds in depth here. He says:
I want to stress that if D&E did not smooth their data, the  correlation would not have been as high; but as high as it would have  been, it would still have been expected.   All that smoothing has done  here is artificially inflated the confidence D&E have in their  results.   It does not change the fact that AMO + PDO is well correlated  with air temperature.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8826965b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThis just released prestigious report is not going to please the IPCC scientists. It calls for a profound change of course in climate research.

Snip of the report’s front cover.
The Research Council Of Norway has conducted a comprehensive evaluation (see right side bar) of the status of climate science in Norway and released their results. The document: Norwegian Climate Research – An Evaluation writes, “This evaluation provides a critical review of Norwegian climate research in an international perspective and recommends measures to enhance the quality, efficiency and relevance of future climate research.”
Hat-tip to Dr Sebastian Lüning and Dr. Jan-Erik Solheim.
In early 2011, the Norwegian Research Council (RCN) appointed a committee to review Norwegian climate research. The aim of the evaluation was to provide a critical review of Norwegian climate research in an international perspective and to recommend measures to enhance the quality, efficiency and relevance of future climate research.
Key findings of the report are found on page 22, and include the following (my emphasis):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Although the expressed political needs regarding science results primarily relate to the impact of anthropogenic greenhouse gasses, there is also a need for increased research on the impact of human activity on land cover and land-use change, especially in relation to the albedo and the biogeochemical and hydrological cycles. Furthermore, a good understanding of the climate system cannot be reached without a dedicated effort to understand the contribution to climate change from natural climate processes. The geological history very clearly documents a strong climate forcing associated with solar variability, although the exact mechanism has not been identified. This should call for a coherent international effort, but surprisingly, the worldwide scientific effort to increase our understanding of the natural variations is very limited, and this is most probably related to the limited funding available for basic, not agenda-driven research. Therefore, in addition to implementing the recommendations of Klima21, this committee recommends an increased effort in research on the natural causes of climate change, in particular the activity variations of the sun, the mechanism of cloud formation, and the multi-decadal variations in ocean current systems.
2.1.1.7 Summary of key findings
Largely funded by RCN, Norway has developed internationally recognised top competency in many of the scientific disciplines that are necessary for understanding current climate and its development. In particular, the numerical comprehensive climate and Earth system models are highly regarded. Less effort has been devoted to studying and explaining the natural causes of climate change because these have been regarded as having a relatively minor impact on the climate system and global temperature compared with the effect of man-made greenhouse gasses. In setting priorities, Norwegian climate research is in harmony with the mainstream of international climate science, but, taking into account the strong competencies in a wide spectrum of disciplines, an increased effort to understand the basic natural climate processes could be advantageous for Norwegian climate research.
Moreover, page 9 adds that: “…more effort is needed to understand natural climate variability in order to better quantify the uncertainty in predicting future climate.”
Obviously the Research Council of Norway feels the climate models are inadequate and need a good dose of improvement and getting back to reality.
Clearly the report shows that more and more scientists are now realizing that a course correction is needed in climate research, and that the focus has to shift to natural causes.
 
Share this...FacebookTwitter "
"The world’s largest investment banks have funnelled more than £2.2tn ($2.66tn) into fossil fuels since the Paris agreement, new figures show, prompting warnings they are failing to respond to the climate crisis. The US bank JP Morgan Chase, whose economists warned that the climate crisis threatens the survival of humanity last month, has been the largest financier of fossil fuels in the four years since the agreement, providing over £220bn of financial services to extract oil, gas and coal. Analysis of the 35 leading global investment banks, by an alliance of US-based environmental groups, said that financing for the companies most aggressively expanding in new fossil fuel extraction since the Paris agreement has surged by nearly 40% in the last year. Using Bloomberg financial data and other sources to analyse loans, equity issuances and debt underwriting services from 2016 to 2019, the analysis is published on Wednesday in the Banking on Climate Change 2020 report. It has been compiled by Rainforest Action Network, BankTrack, Indigenous Environmental Network, Oil Change International, Reclaim Finance and Sierra Club. Although the last 12 months has seen many investment banks announce financing restrictions on coal, Arctic oil and gas, and tar sands extraction, the report warns that the business practices of financial institutions are not aligned with the Paris agreement. Alongside JP Morgan Chase, the US banks Wells Fargo, Citi and Bank of America dominate financing for fossil fuels, accounting for nearly a third of the £2.2tn of financial services since the Paris agreement, according to the report. The report said big banks overall have increased their funding in the four years since Paris to companies with significant Arctic oil and gas reserves. Alison Kirsch, a researcher at Rainforest Action Network who led the analysis in the report, said: “The data reveal that global banks are not only ramping up financing of fossil fuels overall, but are also increasing funding for the companies most responsible for fossil fuel expansion.” Barclays, which has been under increasing investor pressure over its environmental stance, has been the top European financier of fossil fuels in the last four years, the figures show. Last year, the London-based bank was the largest financier of Arctic oil and gas, according to the figures. A group of influential shareholders are now urging the bank to phase out lending to fossil fuel companies, and have filed a resolution to be voted on at Barclays’ AGM in May. Fracking has been the focus of intense business activity by investment banks since the Paris agreement, with JP Morgan Chase, Wells Fargo and Bank of America leading £241.53bn of financing, much of it linked to the Permian basin in Texas. The Royal Bank of Canada and Toronto Dominion led financing for tar sands crude oil projects in Alberta, north-west Canada, which have caused widespread damage to ecosystems. The big-four Chinese banks have dominated financing for coal mining and coal power since the Paris agreement and have no policies restricting business practices. Kirsch said: “This makes it crystal clear that banks are failing miserably when it comes to responding to the urgency of the climate crisis. As the toll of death and destruction from unprecedented floods, droughts, fires and storms grows, it is unconscionable and outrageous for banks to be approving new loans and raising capital for the companies that are pushing hardest to increase carbon emissions.” Ahead of the next major international climate talks, Cop26 in Glasgow this November, the UK government has sought to make the business and the banking sector the focus of tacking the climate crisis. The former head of the Bank of England Mark Carney has been appointed as a climate envoy, warning that businesses must improve how they disclose their impact on the environment or risk failing to meet climate targets. Johan Frijns, director of BankTrack, an NGO which monitors the activities of major financial institutions, said it was time for banks to commit to phasing out financing for all new fossil fuel projects. “In the last year, banks have been queueing up to proclaim support for the goals of the Paris agreement. Both the Principles for Responsible Banking and the new Equator Principles, each signed by over a hundred banks, acknowledges the global climate goals. Yet the data in Banking on Climate Change 2020 show these laudable pledges making little difference, and bank financing for the fossil fuel industry continuing to lead us to the climate abyss,” he said. “It is high time banks recognised that reaching the Paris climate goals requires an immediate end to finance for all new fossil fuel projects, and a rapid phase-out of existing fossil finance. This should be the Global Glasgow Goal for all banks.” JPMorgan Chase said the commitments it announced last month “reflect our ongoing efforts to help address climate change and promote more sustainable development”. It added: “This includes financing to support climate action and the United Nations Sustainable Development Goals, backing market-based policy solutions to reduce carbon emissions, expanding restrictions on financing for coal mining and coal-fired power, and prohibiting project financing for new oil and gas development in the Arctic.” The bank looked forward, it said, to growing “its impact over time”. A Barclays spokesperson said: “We are working hard to help tackle climate change including facilitating £34.8bn of social and environmental financing last year. We continue to engage with ShareAction and other stakeholders on how we can make further progress.” Wells Fargo told the Guardian that it believes that climate change is one of the most urgent environmental and social issues of our time, and is committing to a low-carbon economy. The bank said it is working to measure and report on the carbon intensity of its credit portfolio. A Bank of America spokesperson said they recognise their role in managing climate risk and in financing the transition to a low-carbon economy. Citigroup did not respond when contacted by the Guardian for comment."
"
Share this...FacebookTwitterThe German langauge Voice of Russia here reports a news item you’ll never hear from the mainstream media. Top scientists of Russia’s most prestigious academy say global warming is ending.
Hat-tip: European Institute for Climate and Energy.
Here’s the Voice of Russia report I’ve translated in English:
Global warming is coming to an end: In the coming years the temperature over the entire planet will fall and the cooling will provide a character of relief. This is the conclusion reached by Russian scientists from the Physics University of the Russian Academy of Science.
The process of a general temperature decrease has already begun, according to the research. After having peaked in 2005, the average temperature on Earth is now returning to the level of the 1996-1997 years, 0.3°C lower.
According to the scientists, global temperatures will fall another 0.15°C by 2015, which corresponds to the climate of the early 1980s.”
Wow! More great news that rebut the claims of the climate catastrophe. I’d think the media and western political readers would embrace all this and be relieved.
The persons most relieved to hear this should be the panicky James Hansen and Al Gore. Surely they’ll be very happy to hear this news.
Now climate scientists can go back to the old narrative of the early 1980s: global cooling!
 
Share this...FacebookTwitter "
"
This graphic, seen on many websites, was not part of Vonk's essay, but added by Anthony to visually tag the topic

Guest Post by Tom Vonk
In a recent post I considered the question in the title. You may see it here : http://wattsupwiththat.com/2010/08/05/co2-heats-the-atmosphere-a-counter-view/
The post generated great deal of interest and many comments.
Even if most of the posters understood the argument and I answered the comments of those who did not, I have been asked to sum up the discussion.
Before starting, I will repeat the statement that I wished to examine.

“Given a gas mixture of CO₂ and N₂ in Local Thermodynamic Equilibrium (LTE) and submitted to infrared radiation, does the CO₂ heat the N₂?”

To begin, we must be really sure that we understood not only what is contained in the question but especially what is NOT contained in it.



The question 	contains no assumption about the radiation. Most importantly there 	is no assumption whether a radiative equilibrium does or does not 	exist. Therefore the answer will be independent from assumptions 	concerning radiative equilibrium. Similarly all questions and 	developments concerning radiative transfer are irrelevant to the 	question.



The question 	contains no assumption about the size or the  geometry of the 	mixture. It may be a cube with a volume of 1 mm³ or a column of 10 	km height. As long as the mixture is in LTE, any size and any 	geometry works.



The question 	contains no assumption about boundary conditions. Such assumptions 	would indeed be necessary if we asked much more ambitious questions 	like what happens at boundaries where no LTE exists and which may be 	constituted of solids or liquids. However we do not ask such 	ambitious questions.


Also it is necessary to be perfectly clear about what “X heats Y” means.
It means that there exists a mechanism transferring net (e.g non zero) energy unidirectionaly
from X to Y .
Perhaps as importantly, and some posters did not understand this point, the statement
“X heats Y” is equivalent to the statement “Y cannot cool X”.
The critical posts – and here we exclude posts developing questions of radiative transfer which are irrelevant as explained in 1) above – were of 2 types.

Type 1

The argument says “LTE never exists or alternatively LTE does not apply to a mixture of CO₂ and N₂.”
The answer to the first variant is that LTE exists and I repeat the definition from the original post : “A volume of gas is in LTE if for every point of this volume there exists a neighborhood in which the gas is in thermodynamic equilibrium (TE)”
2 remarks to this definition:


It is not said 	and it is not important how large this neighborhood of every point 	is. It may be a cube of 1 mm³ or a cube of 10 m³ . The important 	part is that this neighborhood exists (almost) everywhere.



LTE is 	necessary to define local temperature. Saying that LTE never exists 	is equivalent to saying that local temperatures never exist.


The second variant admits that LTE exists but suggests that a mixture of CO₂ and N₂ cannot  be in LTE.
The LTE conditions are given when energy at every point is efficiently spread out among all available degrees of freedom (translation, rotation, vibration).
The most efficient tool for energy spreading are molecular collisions.
Without going in a mathematical development (see statistical thermodynamics for those interested), it is obvious that LTE will exist when there are many molecular collisions per volume unit.
This depends mostly on density – high density gases will be often in LTE while very low density gases will not.

For those not yet convinced, hold out a thermometer in your bedroom and it is probable that it will show a well defined temperature everywhere – your bedroom is in LTE .
We deal here with a mixture of CO₂ and N₂ in conditions of the troposphere which are precisely conditions where LTE exists too.

Type 2

The argument says “The mean time between collisions is much shorter than the mean decay time (e.g time necessary to emit a photon) and therefore all infrared energy absorbed by the CO₂ molecules is immediately and unidirectionaly transferred to the N₂ molecules.”
In simple words – the CO₂ never has time to emit any IR photons because it loses vibrational energy by collisions instead.
This statement is indeed equivalent to the statement “CO₂ heats N₂”.




Now let us examine the above figure.
The good understanding of this figure will do much better than only answering  the original question. It will also make clear to everybody what is really happening in our gas mixture in LTE.
The figure shows the distribution of the kinetic energy (Ox axis) among the N₂ molecules (Oy axis).
This typical curve is called the Maxwell Boltzmann distribution, has been known for more than 100 years and experimentally confirmed with high accuracy.

We know that the temperature is defined by <E>, the energy average.
Hence it is the curve shown in the figure that defines the temperature of a gas.
Another way to say the same thing is to say that the curve depends only on temperature. If we wanted to have the distribution for another  gas than N₂ , f.ex CO₂ or O₂, it would be given by an identical curve.
The blue curve gives the distribution of kinetic energy at 25°C while the red curve gives the distribution at 35°C.
The minimal energy is small but non-zero and there is no maximal energy.
A very important point on the Ox axis is the energy of the first vibrationally excited state of a CO₂ molecule.
You notice that at 25°C the majority of N₂ molecules has insufficient kinetic energy to excite this vibrational state.
Only the proportion of them given by the dark blue surface has enough energy to excite the vibrational state by collision.
When the temperature increases to 35°C, you notice that the proportion of N₂ molecules able to excite the vibrational CO₂ state by collision has significantly increased .
This proportion is given by the sum of the dark blue and light blue surface.
You also notice that as there exists no maximal energy, there will be a proportion of N₂ molecules able to  excite the vibrational CO₂ state at any temperature.

Trivial so far? Well it will not get much more complicated.
First 2 technical points which play no role in the argument but which I would like to mention for the sake of completness.


The figure 	shows the translational kinetic energy. Even if in some (popular) 	literature the temperature is defined as being an average of the 	translational kinetic energy, this is not strictly true.

The temperature is really defined as an average of all energy modes. So what about the vibrational and rotational energy?
At the low tropospheric temperatures we are considering, the distribution of the vibrational energy is extremely  simple : about 5% or less of the molecules are in the first excited state and 95% or more are in the ground state.
As for the rotational energy, it can be computed classically without quantum corrections and the result is that it also follows a Maxwell Boltzmann distribution.
Therefore if we wished to plot the total energy (Etranslational + Evibrational + Erotational) we would rescale the Ox axis and obtain exactly the same curve as the one that is shown.
However as we are interested in studying the T/V interactions, it is the curve of the translational kinetic energy that interests us.


We find the 	omnipresence of LTE again. This curve has been derived and 	experimentally confirmed only, and only if,  the gas is in TE. 	Therefore the following 2 statements are equivalent :

“The gas is in LTE” , “The energy distribution at every point is given by the Maxwell Boltzmann distribution” .
If you feel that these statements are not equivalent, reread carefully what is above.

Now we can demonstrate why the Type2 argument is wrong.

Imagine that you mix cold N₂ represented by the blue curve in the Figure with highly vibrationally excited CO₂. The mixture would then not be in LTE and a transient would take place.
In the molecular process (1) CO₂* + N₂ → CO₂ + N₂⁺ which says that a vibrationally excited CO₂ molecule (CO₂*) collides with an N₂ molecule , decays to the ground state (CO₂) and increases the translational kinetic energy of N₂ (N₂⁺) , there would be a net energy transfer from CO₂* to N₂ .

As a result of this transfer the temperature of N₂ would increase and the blue curve would move to the red one.
However doing that, the number of molecules able to excite CO₂ vibrationally would increase (see the blue surfaces in the figure).
That means that during the increase of the temperature of N₂ , the rate of the opposite molecular process (2) CO₂ + N₂⁺ → CO₂* + N₂ where N₂ molecules (those from the blue surface in the figure)  vibrationally excite CO2 molecules, will increase too.

Of course the transient net energy transfer from CO₂ to N₂ will not continue forever because else the mixture would transform into superheated plasma.
A local equilibrium will be established at each point and in this equilibrium the rate of the process (1) will be exactly equal to the rate of the process (2). 
The curve of energy distribution will stop moving and the Maxwell Boltzmann distribution will describe this distribution at every point.

This is exactly the definition of LTE.
The transient will stop when the mixture reaches LTE and its characteristic feature is that there is no local net energy transfer from CO₂ to N₂.
This result demonstrates both that the Type2 argument is wrong and that the answer on the question we asked at the beginning is “No”.
In very simple words, if you take a small volume (for example 1 m³) of the CO₂ and N₂ mixture in LTE around any point ,  then there cannot be any net energy transfer from CO₂ to N₂ within this volume.

To establish the last step we will take the following statements.


The result 	obtained for the CO₂ and N₂ mixture in LTE is equally true for a 	mixture containing  78% of N₂ , 21% of O₂ , x% of CO₂ and 1-x 	% H₂O in LTE.



The mixture 	defined above approximates well the troposphere and the troposphere 	is indeed in LTE



From the 2 	statements above and the demonstrated result follows :

“The CO₂ does not heat the troposphere” what is the answer on the question asked in the title.


Caveat1

I have said it both in the initial post and in this one.
Unfortunately, I know that it can’t be avoided and that some readers will still be confused about the result established here and start considering radiative transfers or radiative equilibriums.
That’s why I stress again that LTE and the result established here is totally independent of radiative equilibriums and radiative transfer properties.

However it does falsify one misconception concerning radiative properties of CO₂ that has also figured in the comments and that is that “CO₂ does not  radiate at 15µ because it “heats” N₂ instead”.

It is also to be noted that we consider only the T/V process because it is only the vibrational modes that interact with IR radiation.
There are also rotational/translational and rotational/vibrational transfers.
The same argument used for T/V applies also for the R/T and R/V processes in LTE – e.g there is no net energy transfer between these modes in LTE even if for example the R/T process has a much higher probability than a T/V process.
For the sake of clarity we don’t mention specifically the R/T and R/V processes.

Caveat2

The result established here is a statistical thermodynamics bulk property.
This property is of course not sufficient to establish the whole dynamics of a system at all time and space scales.
If that was our ambition – and it is not – then we would have to consider boundary conditions and macroscopic mass, momentum and energy transfers, e.g convection, conduction, phase changes, lapse rates etc.
More specifically this result doesn’t contradict the trivial observation that if one changes the parameters of the system, for example composition, pressure, radiation intensity and spectrum, etc,  then the dynamics of the system change too.

Yet it contradicts the notion that once these parameter are fixed there is a net transfer of energy from CO₂ to the troposphere. There is not.

Caveat3

It will probably appear obvious to most of you but it has also to be repeated.
This result says little about comparisons between the dynamics of 2 very different systems such as, for example, an Earth without oceans and atmosphere, and an Earth with oceans and atmosphere. Clearly the dynamics will be very different but it stays that in the case of the real Earth with an atmosphere in LTE, there will be no net energy transfer from the CO₂ to the atmosphere.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8959e795',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Ed Caryl
On Friday, June 22nd, 2012, the The National Academy of Science issued a press release titled, “California Sea Level Projected to Rise at Higher Rate Than Global Average; Slower Rate for Oregon, Washington, But Major Earthquake Could Cause Sudden Rise”. Just in time for Rio+20.

Figure 1: Arial photo of San Andreas fault in California Central Valley.
The press release was picked up by the Associated Press, and in turn, on Saturday the story appeared in newspapers all across the country, including my local paper, and most California papers. The press release breathlessly stated:
The committee that wrote the report projected that global sea level will rise 8 to 23 centimeters by 2030, relative to the 2000 level, 18 to 48 centimeters by 2050, and 50 to 140 centimeters by 2100. The 2100 estimate is substantially higher than the United Nation’s Intergovernmental Panel on Climate Change’s projection made in 2007 of 18 to 59 centimeters with a possible additional 17 centimeters if rapid changes in ice flow are included.
For the California coast south of Cape Mendocino, the committee projected that sea level will rise 4 to 30 centimeters by 2030, 12 to 61 centimeters by 2050, and 42 to 167 centimeters by 2100. For the Washington, Oregon, and California coast north of Cape Mendocino, sea level is projected to change between falling 4 centimeters to rising 23 centimeters by 2030, falling 3 centimeters to rising 48 centimeters by 2050, and rising between 10 to 143 centimeters by 2100. The committee noted that as the projection period lengthens, uncertainties, and thus ranges, increase.
The committee’s projections for the California coast south of Cape Mendocino are slightly higher than its global projections because much of the coastline is subsiding. The lower sea levels projected for northern California, Washington, and Oregon coasts are because the land is rising largely due to plate tectonics. In this region, the ocean plate is descending below the continental plate at the Cascadia Subduction Zone, pushing up the coast.
Extreme events could raise sea level much faster than the rates projected by the committee. For example, an earthquake magnitude 8 or greater north of Cape Mendocino, which occurs in this area every several hundred to 1,000 years with the most recent in 1700, could cause parts of the coast to subside immediately and the relative sea level to rise suddenly by a meter or more.”
Of course the newspapers picked the most extreme of the above numbers, stating a six inch rise by 2030, and three feet by 2100. I downloaded and read the entire paper, all 275 pages, including 15 pages of “boiler-plate” introduction, title pages, table of contents, Committee members names, etc, and 95 pages of references. The 150 or so pages of real content essentially repeated the above four paragraphs, ad nauseam, with supporting hyperventilation about extreme storms, cliff and beach erosion, and wetlands damage. Surprisingly, the last paragraph, and the last part of the title, on the possibility of an earthquake event, had the least discussion.
Willis Eschenbach was the first to respond to this drivel. By Saturday evening he had posted his response on WUWT here, pointing out the impossibility of the above projection, using the actual San Francisco tide gauge plot. Others, in the comments to his posting, added details. I will add a few more.
The geology of the west coast of the U. S. is dominated by two features. The southern coast of California is moved by the San Andreas fault. From Point Reyes north of San Francisco, to the Gulf of California, the coast is moving to the northwest at about 2.5 inches (6.3 cm) per year. There is very little motion up or down as can be seen in Figure 1 above.
North of Cape Mendocino, ground motions are dominated by the Cascadia Subduction Zone. The Juan de Fuca plate is sliding under the North American Plate. The bending of the North American Plate looks like a playing card being pushed at the edge: the edge bends down, but further back the card bends up, and further back yet, it bends down again. In the case of the Northern California, Oregon, Washington, and Southern British Columbia coasts, the edge is off-shore, the upward-bending part is the coast, and inland is bent down again.
Tide gauges reflect these motions. Crescent City is on the California coast just south of the Oregon boarder in the subduction zone, well north of Cape Mendocino.

Figure 2 is the sea level changes at Crescent City, California. Crescent City is rising at about 0.76 mm per year. In 75 years, there has been no significant change to that trend. Sea level rise will not be a problem to Crescent City.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 3: Sea level anomaly at San Francisco.
The tide gauge at San Francisco has been at three locations. Early on it was at Fort Point, then at Sausalito, before moving to Presidio Park near the Golden Gate Bridge before the turn of the century. During the whole history of this gauge, the trend has been a rise of about 1.4 mm per year. But as you can see from figure 2, there have been four distinct periods with very different trends. Since 1980, including the El Niño of 1982, the trend has been flat or slightly down, at about -0.13 mm per year. This precludes any rise of 6 inches by 2030. The prediction should be between this trend and the last 115 year trend, or between a fall of about 0.4 cm (about -1/8th of an inch) and a rise of 5.6 cm (about 2.2 inches).
The last 30-year trend could be an artifact of local changes at San Francisco. To check that, two tide gauges further south were checked. Gauges around Los Angeles were avoided, because oil reservoir depletion and water injection, aquifer depletion and restoration, have altered coastal rise and fall much more than sea level changes. Instead, two tide gauges further south, La Jolla and San Diego, were examined.

Figure 4: Sea level changes at La Jolla, California.
 
 Figure 5: Sea level change at San Diego, California.
These two gauges roughly agree, with a long-term trend of about 2 mm/year rise, and a short term trend of about 0.6 mm rise. These would correspond to a rise of between 1.8 cm and 6 cm (0.7 inch to 2.4 inches) by 2030, hardly catastrophic. Extending these trends to 2100 would result in a 2 to 7 inch rise, far short of the National Academy of Science prediction.
Part 2
 Oregon and Washington Sea Level
On the evening of January 26th, 1700, at 9:30 PM, the world ended! At least it did for many Native Americans on the coast of Oregon and Washington, and First People on Vancouver Island, British Columbia in Canada. A magnitude 9+ earthquake, lasting several minutes, knocked everyone standing off their feet, collapsed native long-houses, and made people motion-sick. Elders told the children to run for high ground. After spending a cold night in the hills, the children returned to find their villages totally gone. Whole tribes from Crescent City, California to Vancouver Island, were wiped out. Across the Pacific in Japan the next day, multiple waves came ashore:
It flooded farmed fields, ruined salt kilns, damaged fishermen’s shacks, ascended a castle moat, entered a government warehouse, drove people to high ground, and probably ran 2 kilometers up a river…. It wrecked houses not only by flooding them but also by starting a fire. It contained multiple waves that range in reported time from midnight until the following noon. The tsunami initiated a nautical accident in which were lost two crew members and tons of rice.”
We know the exact date and time because it took 10 hours for the tsunami to cross the Pacific and be recorded in Japan. We know what happened to Native Americans and First People in Canada from oral histories and archaeological evidence. The coastal elevation immediately sank five feet (1.5 meters). Drowned forest tree rings show death occurred between the 1699 and 1700 growing seasons. Some dead cedar snags still stand in inland wetlands on the Washington coast.
Similar events have happened 13 times since the eruption of Mt Mazama (Crater Lake) in Oregon 7770 years ago. The shortest interval between these events has been 390 years, the longest, about 1000 years. The next one could happen around 2100, or it could wait until 2700. For an idea of the devastation that will result, look to the Japanese tsunami of last year. It could be much worse because the fault that could slip is much longer and it is much closer to the coast.
Sea level rise is not a hazard on the U. S. Pacific coast. Mega-thrust earthquakes are the real hazard. Thankfully, on the Cascadia Subduction Zone, they are separated by hundreds of years. The National Academy of Science needs to look at real tide gauge data and do some real science.
 
Share this...FacebookTwitter "
"
I’m really disappointed that I didn’t get the “fossil of the day” award. Watch the presentation at COP16 and get some popcorn.


I wonder if the producers of Jurrasic Park gave permission to licensed use of the logo, of if they are just scofflaws?

h/t to Tom Nelson


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e86cd407c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Demand for food is increasing rapidly – the global population is expected to reach 11.2 billion by 2100. To keep up with the additional mouths to feed, intensive farming practices have maximised production, but often at the expense of the environment and human health. Livestock is reared to maximise economic returns, which often means animals are kept in close confinement with each other, increasing the risk of disease. As a result, antibiotics are often used to treat animals destined for human consumption, but relying on them can cause bacteria to develop resistance in the long run. A recent review found 100 academic studies on antimicrobial resistance had detected a link between antibiotic consumption in animals and antimicrobial resistance in humans.  This means that using antibiotics in animal rearing can cause resistant bacteria that may also affect humans down the food chain. Antibiotics have been phased out of livestock rearing in the EU and in their place zinc has been introduced into the diet of animals to help kill bacteria which cause Salmonella and E. coli. High levels of zinc in the diets of pigs and cows can help them grow bigger and kill E. coli, but it’s starting to become an environmental issue in its own right. Most of the zinc fed to the animals is excreted and washed into waterways and soils where it can harm aquatic life and acidify the soil. As a result, European legislation will phase out the use of zinc by 2022. This leaves the producers of livestock feed and farmers in a difficult position. New products are needed to prevent infection in livestock which don’t harm the environment or human health by contributing to antimicrobial resistance, but where could they come from? Seaweed could be the answer. Brown seaweeds synthesise a unique class of compound called phlorotannins as they grow. These compounds can kill bacteria that emerge among farm animals. How effectively these compounds can kill bacteria depends on the species of seaweed being used, with different species producing more potent bactericides. The flock of North Ronaldsay sheep in Scotland have grazed on nothing but seaweed for generations. Animals raised on such diets which are rich in Omega-3 fatty acids produce healthier – and arguably tastier – meat. Seaweed can be grown in the ocean and harvested from natural stocks in a rotational manner, ensuring natural habitats don’t have to be plundered to supply livestock farmers. Seaweed farming also doesn’t have to compete for land space like traditional feed crops and could reduce pressure on agricultural land – allowing space for habitat restoration and rewilding which helps fight climate change. Seaweed farms in the ocean draw in a lot of carbon dioxide – which helps de-acidify the seawater around them – and release oxygen. This improves the health of sea life nearby and helps organisms such as coral or sea snails to grow stronger exoskeletons of calcium carbonate. Modern farming uses huge quantities of fertiliser which run off the land and into rivers and the ocean. There, these nutrients stimulate algae which grow and multiply. When algal blooms die and decay, they’re decomposed by bacteria which absorb oxygen from the water, creating vast dead zones where fish and other aquatic life suffocate. Luckily, growing seaweed requires no fertiliser and only uses nutrients which already exist in seawater. Global seaweed production rose from 10.5 to 28.4 million tonnes between 2000 and 2014, but 95% of this was in Asia. There’s therefore huge growth potential for seaweed agriculture in the rest of the world. The brown seaweeds which produce the helpful antibacterial compounds are widespread on temperate shores, and by converting them into supplements for livestock feed, a vibrant industry that’s good for humans and the environment could flourish. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"The current climate and ecological crisis demands a radical redesign of how we live and organise our societies. Yet these urgent changes, though complex, are far from impossible. Some of them are simple, beautiful, and beneficial to all. By greening our cities with street trees, urban parks, and community and rooftop gardens, we can keep ourselves cool amid rising temperatures, reverse the steady erosion of the rich tapestry of life on Earth, and foster happiness and social connection in the process.  It is widely known that greenery in urban spaces helps improve city microclimates. Thanks to heat generated by traffic and industrial activity, as well as the spread of heat-trapping concrete buildings that have steadily replaced plant life, urban air temperature is often higher than in rural environments. Hotter cities compel urban denizens to opt for air conditioners in order to stay cool, which further strains energy demands and worsens the urban heat island effect. Plants can help cool cities through the water that evaporates from their leaves when exposed to the sun’s rays, and by shading surfaces that otherwise might have absorbed heat. Research has found that on a sunny day, a single healthy tree can have the cooling power of more than ten air-conditioning units. Plants also help keep harmful pollutants such as microscopic particulate matter at bay through a complex process known as dry deposition, whereby particles penetrate and become trapped in the wax or cuticles of leaves. Although banning or at least restricting vehicle use in city centres is crucial, mass greening can further reduce pollution and keep cities cool in the increasingly scorching summers that lie ahead. Urban greenery wouldn’t just help lessen the impacts of climate change and improve air quality. Evidence from a range of disciplines has uncovered numerous social, psychological, and health benefits of human exposure to green spaces. These include stress and anxiety reduction, improved cognitive functioning, lowered risks of depression, and overall greater mental and physical wellbeing. Others have shown how involvement in community gardening can increase social cohesion and social bonds among participants and the wider community, in addition to providing local and affordable food sources. The Japanese preventative healthcare practice of Shinrin-yoku, or “forest bathing”, is modelled on a recognition of the many benefits of immersion in natural spaces. We’re not yet sure why we seem happiest and healthiest when we’re surrounded by our fellow lifeforms. But the universality and antiquity of our appreciation for nature suggests that our biophilia may originate from the millions of years humans and plants spent co-evolving in close contact with each other.  Perhaps most importantly, greening and rewilding our cities can offer vital refuges for rapidly vanishing biodiversity. Human socioeconomic activities, especially those of the world’s rich, have destroyed natural habitats, consumed vast tracts of forest, polluted waterways, and disrupted the seasonal rhythms on which life depends. In the midst of the sixth mass extinction, many species are increasingly finding themselves with nowhere to go. Urban rewilding can help the complex natural communities and processes that are essential for all life to flourish once again. For example, establishing wild meadows and native plant and tree communities provides pollinators and other threatened animals with new spaces to thrive, while creating spaces to reintroduce keystone species, whose presence is crucial for maintaining ecosystem diversity. The mass greening and rewilding of our cities is no novel or abstract ideal. It is already happening in many urban spaces around the world. The mayor of Paris has ambitious plans to “green” 100 hectares of the city by 2020. London mayor Sadiq Khan hopes to make London the world’s first “National Park City” through mass tree planting and park restoration, greening more than half of the capital by 2050.  Singapore, a partner city in the Biophilic Cities Network, is a shining example of how to incorporate “nature” into building and city designs. The Parkroyal on Pickering Hotel, for instance, is shrouded in thickly forested terraces and sky gardens that are inhabited by local insects and birds. More cities need to follow the lead of these forward-thinking designs and initiatives. Alongside these efforts, educational programmes, such as Singapore’s Community in Nature initiative, could also be put in place to help the public learn about, respect, and appreciate wild spaces. Of course, urban greening alone will not be enough to meet the daunting challenges ahead. We also need to fundamentally transform our growth-oriented economies and massively reduce global inequality. But giving some new life to our cities would be a great start. And it wouldn’t just benefit people but, crucially, other species as well. This is their home, too, and they deserve a more viable future. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"As trees grow they remove carbon from the atmosphere. New forests can therefore play an important role in meeting the goal of keeping Earth’s temperature to 1.5℃ above pre-industrial levels. Governments and wider civil society are increasingly recognising these benefits. One important step was the 2011 launch of the Bonn Challenge to restore 350m hectares of forest by 2030. This is a major undertaking – the area is a little larger than the size of India. Spurred by the necessities of drastically cutting emissions and removing carbon dioxide from the atmosphere to meet climate targets, many countries, including Brazil, India and China, have committed large areas to forest restoration. Adding up the Bonn Challenge and other national pledges from 43 countries across the tropics and sub-tropics – where trees grow fast – reveals that these governments have pledged to restore 292m hectares of degraded lands. This very welcome news is, unfortunately, not all that is seems. Our new analysis, published in Nature, shows that implementing the current pledges under the Bonn Challenge will mean the 1.5℃ climate goal is still missed. More than half of the countries involved (24), covering two thirds of the pledged area, have stated what type of forest restoration they will do: 45% of the area is slated to become plantations of a single tree species (monocultures); 21% to agriculture that mixes trees and crops, known as agroforestry; and only 34% is given to restoring natural forests.  Such choices have profound carbon implications: for instance, our analysis shows that restoring natural forests over the whole 350m hectares of land would remove 42 billion tonnes of carbon by 2100. If instead we use the current proportion of pledges for plantations, natural forests and agroforestry applied to the whole area this is reduced to 16 billion tonnes (assuming that all new natural forests are protected to 2100). And if commercial monocultures were planted across 100% of the area just a billion tonnes of carbon would be sequestered. Our research demonstrates that within these countries, land put aside for natural forests to return holds 40 times more carbon than plantations and six times more than agroforestry. This is mainly because natural forests continue to remove carbon from the atmosphere for many decades, whereas plantations are harvested every decade or so, which means almost all the carbon stored in the trees goes back into the atmosphere, as the plantation waste and wood products – mostly paper and chipboard – decompose. To put these numbers into context, the recent Intergovernmental Panel on Climate Change Special Report on 1.5℃, noted that meeting this target requires 200 billion tonnes of carbon to be removed from the atmosphere this century. This colossal number is equivalent to the total emissions from 1800 to 2015 from the US, China, Germany and the UK combined. New forests and other land sequestration plans are expected to account for about one quarter of this carbon removal. At 42 billion tonnes of carbon uptake, restoring only natural forests across the entire Bonn Challenge area would clearly get close to this target. But scientists have modelled a number of emissions decline “pathways” to limit warming to 1.5℃ by 2100. All models require a reduction in emissions to net zero by about 2050. Yet, the average requirement of 200 billion tonnes of carbon removal hides wildly different levels of how much carbon will have to be removed directly from the atmosphere, a process known as negative emissions. The faster we reduce emissions from fossil fuels and deforestation to zero, the lower the level of negative emissions required.  The total scale of negative emissions deployment matters, because as well as forests the other main technology that is central to 1.5℃ scenarios also has a huge land footprint. Bioenergy with carbon capture and storage is expected to capture, on average, around 130 billion tonnes of carbon via planting crops for biofuel that are then burnt in power stations. The carbon emissions are then captured and stored underground. It is expected that an additional area of one or two times the size of India is needed for bioenergy crops by 2050. Assuming food producing areas and old-growth forests are spared, this huge extra demand for land is most likely to displace restored forests. We estimate that if the restored natural forests under the Bonn Challenge and national schemes were converted to bioenergy crops after 2050, just three billion tonnes of carbon would be sequestered by 2100.  The solution here is that newly restored natural forests need protecting in order to protect the climate benefits they provide. Otherwise, one area of climate policy may wipe out the gains made in another. Of all the negative emissions technologies available, allowing natural forests to return is safe, often not costly, and brings many other obvious benefits. But forest restoration can only play the critical role that it needs to if it means the same thing to policy makers as it does to everyone else: restoring areas back to largely intact largely natural forest. A new definition of “forest restoration” that excludes monoculture plantations is needed. Our new research is part of a new interest in restoring ecosystems to help mitigate climate change. We have both signed an open letter published in The Guardian by top scientists and activists which calls for a well-funded programme to restore ecosystems to meet our 1.5C climate goal, under the banner of “natural climate solutions”. A new website elaborating on these plans notes that just 2.5% of mitigation funds goes to natural solutions, despite their promise. Curbing climate change via restoring Earth’s ecosystems to their former glory could be a profound positive legacy of the 21st century, but not if governments and their advisers pretend that vast commercial monocultures of trees are forest restoration. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"What could be more important than sustaining habitable living conditions on Earth? Climate change, biodiversity loss and other environmental problems demand changes on an order of magnitude well beyond the trajectory of business-as-usual. And yet, despite accumulative social and technological innovation, environmental problems are accelerating far more quickly than sustainable solutions.  The design industry is one of many industries mobilising to address environmental imperatives. While sustainability-oriented designers are working towards change from many angles, addressing climate change and other environmental problems on this scale demands much more dramatic transformations in economic ideas, structures and systems that enable – or disable – sustainable design.  Put simply, designers cannot design sustainable future ways of living on scale without a shift in economic priorities. Human impacts on planetary processes in the Anthropocene require new types of ecologically engaged design and economics if the necessary technological, social and political transitions are to take place. Design is crucial to this debate because it is key to the creation of future ways of living. Designers make new ideas, products, services and spaces desirable to future users. With the shape of a font, a brand, the styling of a product, the look and feel of a service, the touch of a garment, the sensation of being in a particular building, designers serve the interests of customers (generally, those with disposal income). They do so according the logic and modes of governance generated by what is valued by economic structures. Design is the practice that makes capitalism so appealing. Designers make new products, services and spaces that shape future ways of living – and can use their skills to create sustainable options. But there is a dilemma here. The market rarely prioritises interests that do not pay the bills or otherwise bring capital to the table. Design sits at the intersection of economic value and social values. Design transforms what economic systems value into new ways of living – which in turn produce certain types of social values. This work is generated by priorities in the design industry, driven by economic imperatives.  Traditional neoclassical economics was developed in an era when all knowledge systems essentially ignored ecological concerns. In conventional economics, value – which is created by generating profit and accumulating capital for owners and investors – is systematically extracted from the systems in which economic systems are embedded: the social and the ecological systems.  Contemporary economic systems reproduce this tradition by rewarding individuals and companies for using (and often exploiting) resources to generate profit, regardless of the ecological or social consequences. The extractive and exploitative dynamics of capitalist economics generate economies locked into accelerating climate change, species extinction and other severe environmental and social problems. This economic system continues to produce ever greater degrees of crises as planetary boundaries are breached in ever more extreme ways. But there are economic alternatives. Heterodox economic theory (such as ecological, feminist and Marxist economics) challenges the assumptions of mainstream economics. It has shown how neoclassical and neoliberal economics produce unsustainable economies that consistently devalue the natural world, women’s work and the labour of other groups historically denied equal access to capital.  For example, the Iceberg Model depicts a feminist economic framework where non-market activities, including the unpaid labour that buttresses capitalist economics, are made explicit. The challenges of the Anthropocene demand that we overcome the exploitative and anti-ecological biases in neoclassical and neoliberal economics. One popular alternative is Kate Raworth’s Donut Economics. This would prioritise both social justice and environmental sustainability to create a safe operating space for humanity. Unlike conventional economics, heterodox economics takes the ecological context and planetary boundaries into account – while also addressing the interests of historically disadvantaged populations. The design industry, like most industries, is governed by economic ideas, structures and systems. Economic systems determine priorities in design studios and design education – including whether or not designers can focus on sustainable solutions.  And so economic factors govern whether designers can direct their energies towards making sustainable ways of living possible – or not. Few of us are employed to do tasks that make it possible to respond responsibly to environmental circumstances because the current political economy is not oriented towards prioritising the preservation of life on this planet. When the priorities of an individual designer who is oriented towards sustainability conflict with those of the design industry, which is often governed by an economic system oriented towards profit, the designer finds it hard to make a living. If sustainable solutions will not generate profits, they will not succeed in this economic system (without either government intervention or charitable support). The design industry does not systemically prioritise the needs of the environment within this economic system because the way value is generated in contemporary economics depends on the systemic dismissal of ecological priorities. Addressing this dilemma is a severe challenge. It is now evident that the economic system must be designed to reflect priorities and values associated with preserving habitable conditions on the planet. Climate change and other severe environmental threats require dramatic shifts in economic priorities. The fields of economics and design must be redirected so that economic services, structures and systems will support socially distributive and environmentally regenerative design. Humankind already has the knowledge to make sustainable and socially just ways of living on this planet possible. What we do not yet have is the ability to make these transitions possible in the current political context. New types of design and economics could be a basis for systemic transitions. Key to this transition is ecologically literate education in both design and economics. Both fields must be radically transformed to meet the challenges of the Anthropocene. With critical, ecologically-engaged design and economic education, new redirected design economies could facilitate sustainable transitions and make another world not only possible – but desirable. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterSomeone forgot to tell the Russians that natural climate factors no longer count for anything, and that from the 20th century on man-made CO2 is the sole driver of the climate – now making the Earth only warmer and warmer.

Lake Vostok under almost 4000 meters of ice in Antarctica. Source: US Government.
The German edition of Russian online daily Ria Novosti here writes how a team of Russian scientists in Antarctica has reached Lake Vostok, isolated from the rest of the world by a massive ice sheet for over 15 million years. A team of Russian scientists bored through 4000 meters of ice to reach the lake on 5 February 2012.
Minister President Vladimir Putin congratulated the researchers on their feat last Friday. Project leader Vladimir Lipenkov said that it will be very important for studying climate change on Earth. The Russian team plans to drop a robot into the lake to collect water samples and sediments from the bottom. According to Ria Novosti:
A new ice age is unavoidable, but will occur in 10,000 years at the earliest. This is what Vladimir Lipenkov, member of the Russian expedition said concerning the millions of years old Vostok Antarctic lake on Friday during a meeting with Minister President Vladimir Putin and the research team.”
Why the statements are not published in the English edition of Ria Novosti, let alone the western mainstream media, is unknown. And the report provides no information on what they base the timing of the new ice age on.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Vostok station. Photo source: National Oceanic and Atmospheric Administration.
This is not the first time Russian scientists predict cooling ahead. In 2006 Chabibullo Abdussamatow of the Pulkovo Observatory and a member of the Russian Academy of Sciences said global warming had already reached its peak and that reduced solar activity would start the Earth on a cooling phase. According to Ria Novosti here:
‘With respect to solar activity, the increase in energy emission was indeed the most important event of the 20th century,’ the scientist said.”
and:
The start of the temperature decline can be expected in 2012 or 2013 according to the scientist. By 2035 or 2045 the strength of the sun will again reach a minimum. A strong cold will then grip the Earth 15 to 20 years later.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWe’ve got to keep the planet from warming 2°C since industrialization began, the AGW alarmist scientists warn us. So far we are told that manmade greenhouse gases have warmed the planet 0.8°C since about 1880, which means it must not warm more than another 1.2°C.

Hoffmann’s video shows that nobody has a clue as to what the real global mean temperature is. Estimates vary over a whopping 1.5°C range!
Rainer Hoffmann of solarkritik.de has put together an outstanding montage of video clips depicting various global mean temperature charts used by top scientists and media. What they reveal is truly stunning: the world’s top climate scientists have no clue what the real global mean temperature really is. As you will see, figures range from 14.5°C to 16.0°C!
First, it is important to know that according to scientists, the greenhouse effect adds 33°C to the temperature of the Earth’s atmosphere near the surface. That’s the consensus that the first 8 minutes of the video clearly shows. Wthout greenhouse gases, the temperature of the planet would be a frigid -18°C (0°F). Thanks to the greenhouse effect, the planet’s surface warms to a mean of 15°C.
So using the 2°C target, the globe therefore should not warm up beyond 15°C + 2°C = 17°C. Over 17°C, all hell will break loose they warn us.
Most climate scientists agree that temperatures have risen 0.5° since the 1950s, and 0.3°C from 1880 to 1950. So that means the global mean temperatures should appear as follows:
Today: 15.8°
1950: 15.3°C
1880: 15°C
Right? Well, it turns out IPCC scientists are all over the board when it comes to these figures and nobody really knows. What follows next are examples of what the German public has been hearing over the last few years from leading scientists and media. Rainer Hoffmann put together a number of videos depicting charts used by scientists and media. Now hold on to your seat!
10:56, Hans Schellnhuber on German public television, 11/2009:
Today: 15.3°C
1950: 14.8°C
1880: 14.5°C
11:40, Stefan Rahmstorf, chart implies:
Today: 15.5°C
1950: 15.0°C
1880: 14.7°C
12:25, IPCC 2007 4AR (brace yourself!)
Today: 14.5°C
1950: 14.0°C
1880: 13.7°C
According to the IPCC, we still haven’t reached natural greenhouse temperature of 15°C! All those “weather extremes” we’ve had over the recent years occurred below the natural greenhouse temperature! So how could it be CO2?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




14:20, ZDF German public television, 12/2009
Today: 14.5°C
1950: 14.0°C
1880: 13.7°C
15:31, IPCC lead scientist Mojib Latif, 03/2012
Today: 14.5°C
1950: 14.0°C
1880: 13.7°C
15:50, Book by Schellnhuber & Rahmstorf, 2006
Today: 14.5°C
1950: 14.0°C
1880: 13.7°C
Here we see that global mean temperature for Schellnhuber (see above) has gone up 0.8°C in just 3 years! Now that’s fast.
16:16, Der Spiegel, 1988
Today: 15.5°C
1950: 15.0°C
1880: 14.7°C
18:20, Ravenburger children’s book, 2010
Today: 16.0°C
1950: 15.5°C
1880: 15.2°C
According to Ravenburger, we’re now down to our last degree before we all die!
It gets even more bizarre. The video at the 20:20 mark shows Environment Minister Peter Altmaier saying in July, 2012, that the target was to limit global warming to 2 percent!
Finally at the 22:00 mark, Hans Schellnhuber takes the cake saying in 2008 that if the world’s population reaches 9 billion, the world will explode!
If anything, all this shows that leading IPCC scientists have no idea what the real mean global temperature is. They’re making things up. Hoffmann only looked at charts used in Germany. Imagine what we would find if looked all over the world.
Next time you see a temperature chart, check the vertical temperature axis. You may find more surprises.
Would somebody please tell me WTF the real global mean temperature is? This whole thing is just a total circus.
Hats off to Rainer Hoffmann for this observation.
Share this...FacebookTwitter "
"Last year’s summer was so warm that it helped trigger the loss of 600bn tons of ice from Greenland – enough to raise global sea levels by 2.2mm in just two months, new research has found. The analysis of satellite data has revealed the astounding loss of ice in just a few months of abnormally high temperatures around the northern pole. Last year was the hottest on record for the Arctic, with the annual minimum extent of sea ice in the region its second-lowest on record.  Unlike the retreat of sea ice, the loss of land-based glaciers directly causes the seas to rise, imperiling coastal cities and towns around the world. Scientists have calculated that Greenland’s enormous ice sheet lost an average of 268bn tons of ice between 2002 and 2019 – less than half of what was shed last summer. By contrast, Los Angeles county, which has more than 10 million residents, consumes 1bn tons of water a year. “We knew this past summer had been particularly warm in Greenland, melting every corner of the ice sheet, but the numbers are enormous,” said Isabella Velicogna, a professor of Earth system science at University of California Irvine and lead author of the new study, which drew upon measurements taken by Nasa’s Gravity Recovery and Climate Experiment (Grace) satellite mission and its upgraded successor, Grace Follow-On. Glaciers are melting away around the world due to global heating caused by the human-induced climate crisis. Ice is reflective of sunlight so as it retreats the dark surfaces underneath absorb yet more heat, causing a further acceleration in melting. Ice is being lost from Greenland seven times faster than it was in the 1990s, scientists revealed last year, pushing up previous estimates of global sea level rise and putting 400 million people at risk of flooding every year by the end of the century. More recent research has found that Antarctica, the largest ice sheet on Earth, is also losing mass at a galloping rate, although the latest University of California and Nasa works reveals a nuanced picture. “In Antarctica, the mass loss in the west proceeds unabated, which is very bad news for sea level rise,” Velicogna said. “But we also observe a mass gain in the Atlantic sector of east Antarctica caused by an increase in snowfall, which helps mitigate the enormous increase in mass loss that we’ve seen in the last two decades in other parts of the continent.” The research has further illustrated the existential dangers posed by runaway global heating, even as the world’s attention is gripped by the coronavirus crisis. Crucial climate talks are set to be held later this year in Glasgow, although the wave of cancellations triggered by the virus has threatened to undermine this diplomatic effort. “The technical brilliance involved in weighing the ice sheets using satellites in space is just amazing,” said Richard Alley, a glaciologist at Penn State University who was not involved in the study. “It is easy for us to be distracted by fluctuations, so the highly reliable long data sets from Grace and other sensors are important in clarifying what is really going on, showing us both the big signal and the wiggles that help us understand the processes that contribute to the big signal.”"
"
Share this...FacebookTwitterToday some German warmist sites are busy touting a new paper authored by Lewandowsky et al in the journal Psychological Science: NASA faked the moon landing—therefore (climate) science is a hoax: An anatomy of the motivated rejection of science.

Paper now wants us to believe that it is the Bush-followers (Tea Party followers) who believe in the 9-11 and moon-landing conspiracies. (Photo: dbking, via Wikipedia)
According to the paper’s abstract (emphasis added):
We report a survey (N > 1100) of climate blog users to identify the variables underlying acceptance and rejection of climate science. Paralleling previous work, we find that endorsement of a laissez-faire conception of free-market economics predicts rejection of climate science (r is approx. 0.80 between latent constructs). Endorsement of the free market also predicted the rejection of other established scientific findings, such as the facts that HIV causes AIDS and that smoking causes lung cancer. We additionally show that endorsement of a cluster of conspiracy theories (e.g., that the CIA killed Martin-Luther King or that NASA faked the moon landing) predicts rejection of climate science as well as the rejection of other scientific findings, above and beyond endorsement of laissez-faire free markets. This provides empirical confirmation of previous suggestions that conspiracist ideation contributes to the rejection of science. Acceptance of science, by contrast, was strongly associated with the perception of a consensus among scientists.
So we have here is a study that attempts to stigmatise and marginalise anyone who questions the notion that trace gas CO2 modulates global temperature and storm intensity. Forget that global temps have not risen in 15 years and that 70% of the Holocene was warmer than it is today. And forget that some important ocean cycles were in their positive modes from from 1980 to 2000 and that solar activity during the 20th century was at its most intense level in all of the Holocene. On and on goes the list.
The other point is that climate skeptics are not the ones who are paranoid and obsessed with the notion that human lifestyles are pushing the planet over the brink. Skeptics are not the ones who become hysterical with every wind gust. The true nuts are the extreme warmists.
Warmist media pushed the junk 9/11 and moon-landing theories
Moreover, it was the “enlightened”, i.e. the warmist media, in Germany who zealously promoted the 9/11 and moon-landing conspiracies – all in  an effort to fan the flames of resentment against the USA during the Bush years. What follows are some examples of German television and media drugging up its viewers with the silly conspiracy theories.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Here’s a “documentary” questioning the moon-landing shown on warmist German television:

And here’s another:

And the following was delivered by the renowned, warmist Spiegel TV!

Or read all about it in print at the über-warmist Stern magazine here. These are just a few examples. All these above media outlets floating the whacko 9/11 and moonlanding theories were and are still fervently pushing AGW theory today. So let’s be clear who the real 9/11 and moon-landing kooks are: the same ones who think we can regulate climate and weather with a few molecules of CO2.
Of course, the documentaries above tried to give an impression of neutralality, but their true intentions were clear -it was to sow the seeds of anti-Americanism in the viewers’ minds.
Today in Germany, many of the purveyors of this twisted propaganda would like us to forget their involvement in this. After Obama became president, some even aired pieces debunking the conspiracy theories to clean the slate – but not before Bush left office. And today, as the above nutty psychology report shows, we see they are attempting make people believe that it is actually the Bush-followers (Tea Partiers) who are spreading nutjob conspiracy theories they themselves hatched earlier.
Should we be surprised? What else should we expect from those who are losing the debate and have no scruples about being dishonest?
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEnergy poverty is sweeping over modern Germany like never before.
Flagship German newspaper Die Welt has an online report titled: Fast 800.000 Deutsche können Strom nicht bezahlen. In English: Almost 800,000 Germans cannot pay for electricity. 

Green energy leaves Germans in the dark.
As Germany subsidies wealthy homeowners and businesses owners to install solar panels on their homes and commercial buildings, low income families living in rented apartments are getting stuck footing skyrocketing electric bills. Many can no longer afford to pay for electricity, and so the utilities are cutting off their power.
Indeed high energy prices are causing everything else to get more expensive as well – all this while the euro is threatened to collapse under the weight of massive debt due to financial ineptitude.
So it’s little wonder that German politicians are beginning to panic and coming up with really nutty solutions. Instead of scaling back the cause of the energy mess (government meddling in the energy sector) they are threatening to do the opposite: i.e. meddle even more – much more.
Aribert Peters, Chairman of the Bund der Energieverbraucher (Association of Energy Consumers) says that already 600,000 to 800,000 people in Germany have had their electricity cut off, all thanks to skyrocketing electricity prices due to friendly green energy. Spooked, a number of leading politicians and consumer advocates are now calling for financial assistance for low income households, i.e. energy welfare. Die Welt writes:
Energy companies should be obligated to offer the first 500 kilowatt-hours per household at a low rate, SPD (social democrat party) faction vice chairman Ulrich Kelber demanded in a strategy paper, which he wants to present to the SPD leaders.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Peters of the Association of Energy Consumers, however, goes even further, saying there’s a need for a general cost exemption for the first 500 kilowatt hours consumed per year and household. The exemption should apply to all citizens.
Also the VdK Social Association of Hesse-Thuringia is demanding social tariffs. VdK chairman Udo Schlitt says that without a price rebate, more and more people with low incomes are going to have their power shut off. He proposes:
Therefore all power producers must be mandated to offer binding social rates by law.”
So in summary, here’s Germany’s latest energy plan: 1) Force power companies to buy exorbitantly-priced, inefficient and intermittent-supply green energy on one side, and then force them to give it away, or sell it at a low price, on the sales side!
How long can that go on before it all collapses?
Not only is electricity to be given away, DIE WELT also brings up another SPD scheme, one of course that the other parties will join in on:
Moreover, there should also be a billion-euro subsidy program so that, for example, energy saving refrigerators can be bought.”
There you have it. First the government took over the energy sector, and now we see it is moving in to take over private households. If this allowed to happen, then in 10 years Germany will no longer be recognizable.
You can think out the rest.
 
Share this...FacebookTwitter "
nan
"**The Scottish government has published its**guidance on forming household bubbles over the festive period **. The advice covers how many people will be able to meet in Scotland, and the rules on where that can happen.**
The current Covid rules will be relaxed between 23 and 27 December to allow people to travel within the UK and spend Christmas together in bubbles of up to three households.
The Scottish government has recommended that these bubbles should contain a maximum of eight people - although children under the age of 12 do not count towards that total, and do not need to physically distance from others.
Everyone else is encouraged to keep 2m (6ft 6in) away from those outside their own household as much as possible to lower the risk of transmitting the virus.
You can only be in one Christmas bubble, and cannot change to a different one.
The government says anyone thinking of creating a bubble should carefully consider the risks. It stresses that people do not have to meet other people or feel pressured to spend Christmas with another household.
The advice is to keep in touch using technology wherever you can, limit the number of times that you meet in person - and to gather outside if possible. For example, go for a walk rather than having a meal together.
Those in extended households can form a bubble, but it can only contain one extended household.
Where parents do not live in the same household, children can still move between their homes if they are in different bubbles.
Those in a bubble can only gather in a private home, outdoors or at a place of worship. For those meeting in someone's home, it is possible to stay overnight.
If you are meeting in someone's home it is recommended that you:
People should not mix with other households elsewhere. If you are going to a pub, restaurant or a leisure or entertainment venue, you are urged to stay within your own household.
The opening hours for hospitality venues will follow the rules which apply in that area at the time.
The government says people in a bubble should not stay in tourist accommodation together as a group.
In addition, you should not go shopping with those in your bubble, and should shop on your own wherever possible.
Travel restrictions will be relaxed from 23 to 27 December to allow people to travel between local authority areas and the four UK nations to join a bubble.
If you are using public transport, the advice is to book ahead where possible and follow the rules on wearing face coverings while travelling.
Anyone travelling to or from a Scottish island should make their journey within the five-day period from 23 to 27 December.
Once you have arrived, you should then follow the travel guidance which applies in the area where you are staying. If that is in level three or four, for example, you would have to avoid any non-essential travel outside that council area.
Students who return home at the end of term will be part of the household they have returned to. Plans are already in place to allow students return home over Christmas if they return two negative Covid-19 tests.
People other than students who live in a shared flat or house are considered a household.
The government is urging them not to split up and enter separate bubbles over the festive period.
If people are joining different bubbles, they should isolate from their flatmates for about a week both before and after joining the bubble.
The government says anyone who has previously been advised to shield because they are at highest clinical risk from Covid-19 should ""take time to think"" about forming a bubble because it would bring greater risks.
It says people should not feel pressured to enter an environment which makes them anxious.
People can still go into another household to provide care and support for a vulnerable person.
However, if you visit someone in hospital, hospice or a care home the government says the safest way to spend Christmas would be not to form a bubble with another household.
That is because doing so would increase the risk of being exposed to Covid-19 and passing it on to other people, and those in care homes, hospitals and hospices can be particularly vulnerable.
If someone in a bubble develops Covid-19 symptoms, everyone within the bubble must isolate immediately if they met that person any time between two days before and 10 days after their symptoms started.
If that person tests positive, all members of the bubble must self-isolate for 14 days from the start of symptoms or their most recent contact.
UK government guidance for people in England does not set a limit on the number of people in a bubble, but says this should be kept ""as small as possible"".
It adds that the rules on meeting people outside your home will depend on the regulations which apply in the tier you are staying in.
No separate guidance has been published for Wales or Northern Ireland at this stage, although people can travel to or from Northern Ireland on 22 and 28 December. The NI executive is meeting on Thursday to discuss the rules for the festive period.
**Use the form below to send us your questions and we could be in touch.**
_ **In some cases your question will be published, displaying your name and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read the terms and conditions.**_
If you are reading this page on the BBC News app, you will need to visit the mobile version of the BBC website to submit your question on this topic."
"**Coronavirus infection rates in England are continuing to show signs of levelling off - but the picture across the UK is mixed, according to data from the**Office for National Statistics **.**
In Wales and Northern Ireland, infections have been decreasing in recent weeks - but in Scotland, they seem to be rising.
After lockdown ends in England, most areas face tougher tier restrictions.
Most will be in tier two - high alert, including London and Liverpool.
In Scotland, Wales and Northern Ireland the devolved administrations have the power to set their own coronavirus regulations, though all four UK nations have agreed a joint plan for Christmas.
In England, decisions on post-lockdown tiers are based on how fast case rates are falling or rising in different areas, as well as numbers affected in the over-60s.
With the second lockdown having started on 5 November, Prof Kevin McConway, statistics expert from the Open University, says it might seem disappointing that progress to reduce infections hadn't been faster.
""People continue to give positive test results, on average, for at least 10 days after they were first infected, so some of the people who tested positive in the most recent week would have been infected before the English lockdown began,"" he said.
The ONS figures are based on thousands of people tested for the virus in households across the UK, whether they have symptoms or not.
Of those tested in the week to 21 November, one person tested positive out of every:
How many confirmed cases are in your area?
What are the new rules in England, Scotland, Northern Ireland and Wales?
According to the ONS estimates, rates in England increased in the East Midlands and North East that week, while continuing to fall in the North West.
In the east of England, London, the South East and South West, rates now appear to be decreasing too.
The areas with the highest number of people infected per head of population are Yorkshire and the Humber, the North West and North East.
Secondary-school-age children and young adults are seeing the highest infection rates.
This information is based on a relatively small number of people testing positive in each age group and region, so there is a wide margin for error.
In Scotland, an estimated 45,700 people had the virus in the week to 21 November - one person in every 115, up from one in 155 the previous week.
But the ONS says the results are based on modelling and ""should be interpreted with caution"".
The ONS figures are one source of data which helps the government's scientific advisers estimate the reproduction (R) number of the virus every week.
Another source is the Covid symptom study app, which suggests there has been a fall in new UK daily symptomatic cases - from 34,279 to 29,311 - over the two weeks up to 22 November.
This estimate is based on one million people using the app every week.
Data from Public Health England shows rates of coronavirus cases are still rising in 45% of areas (shown as pink, red or dark red in the map), despite the second lockdown.
However Dr Yvonne Doyle, PHE medical director, says ""there is now reason for hope"".
""Case rates have fallen across every age range and in all regions, and positivity in both pillars [NHS and community testing] has also decreased. Over time we can expect that to lead to fewer hospitalisations and deaths.
""The huge efforts people have made over the past few weeks are starting to pay off,"" Dr Doyle said.
The government's daily figures for confirmed UK cases of coronavirus are often much lower than the ONS numbers because they only count people with symptoms testing positive.
On Thursday, the government reported that 17,555 people tested positive and 498 people had died within 28 days of a positive test.
Over the past week, that's a 25% reduction in cases. The number of patients admitted to hospital is also falling - but deaths continue to rise."
"
Share this...FacebookTwitterAccording to the University of Wisconsin, Madison here, on June 11, 2012, the South Pole Station measured a new record low temperature.

Antarctica weather station. Photo source: http://amrc.ssec.wisc.edu/aboutus/
The mercury dropped to -73.8°C/-100.8°F, breaking the previous minimum temperature record of -73.3°C/-99.9°F set in 1966.
Must be because of global warming!
Hat/tip: http://www.kaltesonne.de/
 
Share this...FacebookTwitter "
"

My misadventures in state government led me to coin a phrase for what has become the economic growth model of choice for a lot of governors: “Press Release Economics.” It comes in many shapes and sizes, but it basically boils down to the orchestrated PEZ‐​dispensing of taxpayer money on short‐​term “economic growth” schemes for crass political gain.   
  
  
The most common form is probably the targeted tax break and/​or corporate welfare grant/​loan to incite a company to relocate within a state’s borders. Politicians love these taxpayer‐​financed giveaways because they come complete with lots of visible media coverage: press releases, newspaper articles, radio and television reports, and best of all…the photo op. Ah yes, that priceless picture of the governor all dressed up with a hard hat, ceremonial spade in hand, and a big toothy grin.   
  
  
One would be hard pressed to find justification for these political endeavors in the economic literature, but then again the little Potemkins who run state “economic development” bureaucracies don’t have time to be bothered with trivialities when there are “jobs to create.”   
  
  
Today I read that Gov. John Corzine has come up with a $150 million package to help the New Jersey economy. The concoction includes two peculiar items: money for banks to get them to lend and a $3,000 check to small businesses for each employee they hire and employ for a year. “Create a job and we will send you a $3,000 check,” Gov. Corzine says.   
  
  
With regard to the first one, the _New York Times_ reports: 



James Silkensen, president of the New Jersey League of Community Bankers, said he had not heard complaints from his members about needing more cash. “Our members are telling us that they’ve got money to lend,” Mr. Silkensen said. “They aren’t going to change their underwriting standards. I can’t say every bank has sufficient funds to lend. But most I have talked to are lending, though they’re being careful.”



With regard to the second one, it’s pure press release economics. Why not $4,000 an employee? Or $5,000? Why just “small” businesses? Do “large” businesses contribute nothing to the New Jersey economy? How will this initiative be enforced? How much will it cost taxpayers for New Jersey bureaucrats to make sure each and every new hire was employed not less than 365 days? How many of the $3,000 check employees would have been hired anyhow? How many jobs will be lost because of the tax burden needed to pay for this scheme and others?   
  
  
Here’s a better idea, Governor: propose serious tax and spending cuts. New Jersey’s general fund is up 40% from just five years ago, which amounts to a $1,000 per New Jersery citizen spending increase. At the same time, New Jersey’s business tax climate was recently found to be the **worst** of the fifty states.
"
"What can we do in the face of the climate emergency? Many say we should drive less, fly less, eat less meat. But others argue that personal actions like this are a pointless drop in the ocean when set against the huge systemic changes that are required to prevent devastating global warming.  It’s a debate that has been raging for decades. Clearly, in terms of global greenhouse gas emissions, a single person’s contribution is basically irrelevant (much like a single vote in an election). But my research, first in my masters and now as part of my PhD, has found that doing something bold like giving up flying can have a wider knock-on effect by influencing others and shifting what’s viewed as “normal”. In a survey I conducted, half of the respondents who knew someone who has given up flying because of climate change said they fly less because of this example. That alone seemed pretty impressive to me. Furthermore, around three quarters said it had changed their attitudes towards flying and climate change in some way. These effects were increased if a high-profile person had given up flying, such as an academic or someone in the public eye. In this case, around two thirds said they fly less because of this person, and only 7% said it has not affected their attitudes.  I wondered if these impressionable people were already behaving like squeaky-clean environmentalists, but the figures suggested not. The survey respondents fly considerably more than average, meaning they have plenty of potential to fly less because of someone else’s example.  To explore people’s reasoning, I interviewed some of those who had been influenced by a “non-flyer”. They explained that the bold and unusual position to give up flying had: conveyed the seriousness of climate change and flying’s contribution to it; crystallised the link between values and actions; and even reduced feelings of isolation that flying less was a valid and sensible response to climate change. They said that “commitment” and “expertise” were the most influential qualities of the person who had stopped flying. It’s not all a bed of roses, of course. Flying represents freedom, fun and progress. It boosts the economy and can provide precious travel opportunities. So suggesting that everyone should fly less, which may seem the implicit message of someone who gives up flying because of climate change, can lead to arguments and confrontation. One person for example said that my gently worded survey was “fascist and misinformed”. You don’t get that when you ask about washing-up liquid.  My research also probed ideas of inconsistency and hypocrisy. In short, people hate it. If Barack Obama takes a private jet and has a 14-vehicle entourage to get to a climate change conference, or a celebrity weeps for the climate while rocking a huge carbon footprint, it doesn’t go down well. And if future laws are introduced to reduce flying because of climate change, it looks essential that politicians will have to visibly reduce their flying habits, too. Other research has shown that calls for emissions reductions from climate scientists are much more credible if they themselves walk the talk. That people are influenced by others is hardly a shocking result. Psychology researchers have spent decades amassing evidence about the powerful effects of social influence, while cultural evolution theory suggests we may have evolved to follow the example of those in prestigious positions because it helped us survive. Pick up any book on leadership in an airport shopping mall and it will likely trumpet the importance of leading by example.  Which raises the question: if our political and business leaders are serious about climate change, shouldn’t they be very visibly reducing their own carbon footprints to set an example to the rest of us? This is now the focus of my research. Weaving an invisible thread through all of the above is the thorny issue of fairness and inequality. The wealthiest 10% of the global population are responsible for 50% of emissions, and plenty of that will be due to flying. In the UK, around 15% of people take 70% of the flights, while half of the population don’t fly at all in any one year. As emissions from aviation become an ever increasing slice of the total (currently around 9% in the UK, 2% globally) this inequality will become harder for everyone to ignore. In the mean time, the debate about personal vs. collective action will continue. My research supports the arguments that this is a false dichotomy: individual action is part of the collective. So, while you won’t save the world on your own, you might be part of the solution. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
nan
"The full impact of coral bleaching across the Great Barrier Reef will become clearer this week as aerial surveys of hundreds of reefs are completed in the bottom two thirds of the world’s biggest reef system. An aerial survey carried out last week over almost 500 individual reefs between the Torres Strait and Cairns revealed some severe bleaching of corals closer to shore, but almost none on outer reefs.  From Monday the spotter plane will head south over reefs where satellite observations and temperature readings have shown corals are likely to have undergone higher levels of heat stress than those in the north. Scientists fear those corals could be found to have been badly bleached, as they are less used to higher temperatures and had escaped major impacts in 2016 and 2017. The chief scientist at the Great Barrier Reef Marine Park Authority, Dave Wachenfeld, told Guardian Australia that whatever the survey concluded, the current bleaching should sound “a very loud alarm bell” on the plight of the reef under global heating. Heat stress has been building across the length of the reef this summer with many anecdotal reports from tourism operators, tourists and recreational diver of severe bleaching. Day 4: We have now completed assessment of #coral bleaching between Cairns and northern Torres Strait. Today, we saw extreme levels of bleaching on coastal reefs from Lockhart River to Cairns. Mid-shelf reefs in this region have variable levels of bleaching, from mild to severe pic.twitter.com/QdN16S0cV3 In February, average sea surface temperatures on the reef were 1.25C above normal and the highest on record going back to 1900. Scientists have said the world’s oceans are gathering heat due to accumulating levels of greenhouse gases in the atmosphere, mostly from burning fossil fuels. Corals bleach if they sit in unusually hot water for too long. Survival from bleaching depends on how high and for how long temperatures get. Some species of corals have higher tolerance for heat than others. Observations of conditions on the reef from satellites and in-water temperature loggers suggest central and southern parts have accumulated high levels of hat stress. But the authority said the full picture would only come clear once the aerial surveys were completed at the end of this week, and the data had been analysed. In 2016 and 2017, the world heritage reef experienced back to-back bleaching that was intense enough to kill almost half the reef’s corals over those two years. Central and southern parts of the reef were not severely impacted in those years, meaning they are not used to the heat stress and could be harder hit. Prof Terry Hughes, director of the ARC Centre of Excellence for Coral Reef Studies at James Cook University, spent 17 hours across four days in the air last week scoring bleaching on reefs with a staff member from the authority. After completing the first four of nine days of aerial surveys, Hughes told Guardian Australia most of the severe bleaching had been seen at coastal reefs. On Friday, flying from Lockhart River to Cairns, Hughes said corals at Princess Charlotte Bay at the bottom of the Cape York Peninsula had been severely bleached, but the impacts were much less on reefs further away from the coast. Many of the outer reefs in the north – known as “ribbon reefs” because of their slim and snaking appearance from above – had escaped bleaching. Hughes said: “A lot of the reefs we have been looking at were badly affected in 2016 and 2017. They don’t have a lot of corals on them and the corals that are there have managed to survive 2016 and 2017, and so they are tough.” Hughes said it “remains to be seen what will happen in the south” but there were more coral species in those areas – including staghorn and table acroporas – that would be more susceptible to bleaching. Some of those central and southern reefs “have accumulated a lot of heat, particularly near the coast,” he said. “This is shaping up to be strongly coastal – 2016 and 2017 were cooler in the south and, this time around, it is not cool in the south.” Hughes said: “Even if it turns out to be a relative moderate event event compared to 16 and 17, it is still cumulative and because the footprints are different, the cumulative amounts of the reef that’s affected severely or moderately will go up.” Townsville-based Dr William Skirving, of the US government’s Coral Reef Watch program at the National Oceanic and Atmospheric Administration, said the agency’s analysis showed how localised the heat stress had been. But whether that heat stress had translated to severe bleaching would be be answered by the aerial surveys. He said according to his agency’s analysis, one area of Swains Reefs off Townsville had showed some of the highest levels of heat stress across the entire reef, but some of the lowest levels had also been in the same group. This was one reason, he said, why the aerial surveys were important. He held concerns for the corals in southern areas. He said: “Let’s cross our fingers that the corals were not as susceptible as they were in the past.” Wachenfeld told Guardian Australia that whatever the final assessment was “these are still significant events that are sounding a very loud alarm bell about what’s happening to the reef in the face of climate change.” He said there had been reports of “at least moderate bleaching” from Magnetic Island, near Townsville, and Heron Island, off Gladstone. “Bleaching does not necessarily mean death and that some people do misunderstand that,” he said. “Yes, the reef is in trouble, but what that means is that it needs more help.” Aerial surveys this week of the Ribbon Reefs, above Cairns, show little to no bleaching. Aerial surveys continue over the coming days in areas that experienced more heat stress. #GreatBarrierReef #video pic.twitter.com/4cdjpbj4ZE Wachenfeld said whatever the final detailed assessment of the severity of this summer’s bleaching revealed, this needed to be seen in the context of the broader challenges the reef was facing, including the ongoing impacts from climate change. He said 2016 and 2017 were “the worst events that have ever happened” for the reef, and so if this year turned out to be less severe, this should “not lull us into a false sense of security”. He said he was encouraged to hear that outer ribbon reefs in the north – which he had personally dived on many times – had seemingly escaped bleaching this summer. “They are some of the most beautiful places on the planet and so to know they have done well in this event gives me enormous hope for the future and of what we have left to protect,” he said. “There are still places that are absolutely amazing. The reef as a whole is still a gobsmackingly amazing and beautiful place and it needs us to do more globally to protect it.”"
"**Northamptonshire will be in tier two when England's second lockdown ends on 2 December, it has been announced.**
People in tier two cannot socialise with other households indoors and the rule of six will apply outdoors.
Prior to England's second shutdown, the county was subject to the lowest level tier one restrictions.
Lucy Wightman, director of Public Health Northamptonshire, said: ""This is the result of activities in the week before lockdown.""
Mrs Wightman said she believed the lockdown had been too short to ""make up the ground we covered"" before lockdown.
The number of cases in the county leapt up by 50% after the lockdown was announced.
""We're not back to the point of the week before lockdown,"" Mrs Wightman said.
She added that it was ""too early to say"" if the peak of the second wave of coronavirus had been reached, warning ""we could see another surge in January"".
Mrs Wightman added that she had only found out what tier the county would be in by looking on the government's postcode tracker, which details which areas will be in which category.
She said the decision had been made entirely in Westminster: ""I can't tell you if we are close to the bottom of level two or nearly in level three.
""We've been promised the thresholds will be set. The problem is they just haven't been shared.""
Earlier this week, Prime Minister Boris Johnson told the House of Commons the three-tiered regional measures would return from 2 December, but added that each tier would be toughened.
The allocation of tiers is dependent on factors including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
There are exceptions for some of the tier two rules, for childcare and support bubbles.
Speaking prior to the announcement, Kettering MP Philip Hollobone told BBC Radio Northampton that all seven of Northamptonshire's MPs had favoured the county being put in the lowest tier (tier one).
Mr Hollobone said he feared severe restrictions would mean ""many small businesses - especially in the retail sector and hospitality sector - will go under because they make between a quarter and a third of their profits in the run up to Christmas"".
Rachel Roberts, who owns Mooch in Northampton with husband Paul, said the gift shop had been closed since 5 November.
The couple, who have five shops across Northamptonshire and Buckinghamshire, have been able to operate a click and collect facility from St Giles Street.
However, Mrs Roberts said it had only been ""a very small lifeline"" and her books were ""looking pretty grim"".
""We're already going to be limited in terms of capacity whatever tier we happen to be in,"" she said.
""The biggest reality check we've had to have in our business is that we're only going to be able to have, certainly in St Giles Street, three or four people in the shop at a time across the two floors.""
Steve Ward, co-owner of St Giles Cheese, said he would be taking the same precautions regardless of what tier the county was in.
""We'll limit numbers through the door, having a small shop that's quite easy to manage,"" he said.
""I think running up to Christmas people are going to come out whatever happens.
""They are going to - hopefully - be careful, keep their masks on, avoid getting too close to everybody and just be sensible about it.""
**Analysis by Laura Coffey, BBC Radio Northampton Politics reporter**
Northamptonshire went into this lockdown in tier one, but we'll come out of it next Wednesday in tier two.
Here in the county cases had continued to rise despite the lockdown, although last week's figures from Public Health Northamptonshire seemed to be showing a levelling out.
Public health officials in the county hope figures will have dropped when they're published in the weekly surveillance report on Friday.
This drop can already be seen in figures published by the government on a daily basis.
This tier system will feel very different this time as the government has tightened restrictions.
For us in Northamptonshire it means no household mixing indoors; rule of six applies outdoors; pubs and restaurants shut at 11pm, alcohol can only be served with a substantial meal - so bars and pubs only selling booze must close.
Director of Public Health, Lucy Wightman, speaking on Thursday morning in the weekly Covid board meeting, said we cannot be complacent and need to keep the figures low ahead of the five-day break over Christmas to avoid a third wave after.
Following the announcement, Northampton Town Football Club said it would be welcoming season ticket holders back to matches.
In a statement on its website, the club said: ""The news comes as a huge boost to both the club and our loyal season ticket holders.""
The club will first have to stage a pilot event with around 1,000 season ticket holders. This will take place at Sixfields Stadium on 5 December, for the match against Doncaster Rovers.
A ballot will take place to determine which fans can attend that game.
Northampton made national headlines earlier this year when an outbreak of Covid-19 at M&S sandwich maker Greencore meant it had the highest rate of new coronavirus cases in England.
The town was made an 'Area of Intervention' - then the highest category on the government's watchlist - in August, with the factory closed.
In all, almost 300 workers tested positive, but the town avoided going into a local lockdown.
South Northamptonshire and Northampton have rates above the England-wide average in the week to 21 November, but both have fallen week-on-week.
All districts in the county have seen their rates fall week-on-week.
_Find BBC News: East of England on_Facebook _,_Instagram _and_Twitter _. If you have a story suggestion email_eastofenglandnews@bbc.co.uk"
"Nobel Peace Prize nominee Greta Thunberg claims we need system change to save the planet, and the majority of experts, from the IPCC, through to our own research, would certainly agree with this.  But for most people, it often isn’t clear what changes actually need to be made to address environmental problems. And ideas that are presented can be seem as extreme to some. This is despite the fact that many experts agree that to really tackle climate change, the focus needs to be on changing the capitalist system to make it more environment-friendly.  System change can sound scary, but as the current system drives social injustice and environmental destruction, a new approach to address both is called for. These are some suggestions to help build that new system which also aim to improve people’s lives in the process. The suggestion that GDP is a good measure of a country’s progress has been frequently challenged. To achieve growth, we consume more products, these products need raw materials and energy to produce – and often result in excessive waste when they are disposed of. Hence pursuit of economic growth drives a wasteful use of scarce resources.  Achieving growth isn’t necessarily bad – but focusing solely on growth is. it prevents many other important strategies being put in place, even if they are actually beneficial for the majority of society. As economist Kate Raworth states, we need to be “agnostic about economic growth” and embrace other measures of societal well-being, such as the Human Development Index and Genuine Progress Indicator, which combine financial gains with non-market benefits – such as human health and reduced environmental degradation. Incremental increases in tax (for example on fuel), without alternatives, do little to change behaviour. Instead, it just increases the financial burden on the less well-off – this being one factor behind the recent “yellow vests” (gilets jaunes) protests in France. To achieve rapid and fair changes in consumer behaviour, there needs to be large tax increases on the most environmentally damaging products to turn them from everyday items into luxury goods. This would include air travel, fossil fuels and red meat. We also need to ensure environmentally sound alternatives are available and heavily subsidised. This would see subsidised and reliable public transport, car share schemes to allow occasional use of cars, bike hire, and subsidies on fresh vegetables and meat alternatives – all of which would help people easily transition to a more (environmentally) healthy lifestyle. From an environmental perspective, working less – whether a four-day week, or working only a proportion of the year – has many benefits. Less commuting to work, more time to cook healthy food and more time to take holidays, without the need for flying. The reduction in household income also means less opportunity for over-consumption of “luxury” goods that drive economic growth without adding much value to society.   Plans for a four-day work week and a universal basic income would also help to create greater levels of meaningful employment, safeguard people’s mental health and reduce societal inequality – as well as providing more leisure and family time.  Few people can really identify with the scale of deforestation in Asia for palm oil, or in the Amazon for cattle farms. This is why, to really tackle climate change, we need to think locally and understand the impact of our behaviours on our communities. Farming, energy production and waste disposal are obvious examples.  Localised processes can also be more environmentally sound. Recent research on small-scale coastal fisheries across the globe suggests that if we rely on these for fish – rather than large-scale industrial fishing – we can dramatically increase fish stocks, increase food security in developing countries and improve the local economies of fishing towns in countries such as the UK.  There is a disconnect from the natural world, exemplified even in academic and policy circles with the monetisation of nature through “ecosystem services” and how they contribute to human well-being – by providing food, water, wood and medicines, for example. All of which, puts a price on nature – by defining the Earth’s resources as “natural capital”. We need to appreciate nature for what it is – and protect it now. Teaching natural history in schools is a good place to start. Protecting, restoring and rewilding ecosystems on a large scale will also enhance biodiversity, store carbon and reduce pollution – three of the major environmental planetary boundaries – or safe environmental limits – we have greatly exceeded.    Technological advances such as renewable energy, electric vehicles and smart cities are important steps to reduce our carbon emissions. But they are not the only “solution” to climate change. Manufacturing lithium ion batteries, solar panels and turbines has an environmental cost, too. And, in the same way, changing your car to an electric vehicle is likely to have a bigger short-term carbon footprint than running your current car. This is why technological advances must be used in conjunction with lifestyle changes if we want to transform our society in an environmentally and socially just manner.  Of course, this is not an exhaustive list, but serves as a starting place to show how environmental issues can be addressed and at the same time we can create a fairer and more just society. A society with more free time, more interaction with our local communities and better physical and mental well-being. The future is only scary if we continue on our current path. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Errors in GHCN metadata inventories show stations off by as much as 300 kilometers

Guest post by Steven Mosher 

In the debate over the accuracy of the global temperature nothing is  more evident than errors in the location data for stations in the GHCN  inventory. That inventory is the primary source for all the temperature  series.
One question is “do these mistakes make a difference?” If one  believes as I do that the record is largely correct, then it’s obvious  that these mistakes cannot make a huge difference. If one believes, as  some do, that the record is flawed, then it’s obvious that these  mistakes could be part of the problem. Up until know that is where these  two sides of the debate stand.
Believers convinced that the small  mistakes cannot make a difference; and dis-believers holding that these  mistakes could in fact contribute to the bias in the record.  Before I  get to the question of whether or not these mistakes make a difference, I  need to establish the mistakes, show how some of them originate,  correct them where I can and then do some simple evaluations of the  impact of the mistakes. This is not a simple process. Throughout this  process I think we can say two things that are unassailable:

1. the mistakes are real. 2. we simply don’t know if they make a difference. Some believe they cannot (but they haven’t demonstrated that) and some  believe they will (but they haven’t demonstrated that). The  demonstration of either position requires real work. Up to now no one  has done this work.
This matters primarily because to settle the matter of UHI stations  must be categorized  as urban or rural. That entails collecing some  information about the character of the station, say its population or  the characteristics of the land surface. So, location matters. Consider  Nightlights which Hansen2010 uses to categorize stations into urban and  rural. That determination is made by looking up the value of a pixel in  an image. If it is bright, the site is urban. If it’s dark (mis-located in  the ocean) the site is rural.
In the GHCN metadata the station may be reported at location xyz.xyN  yzx.yxE. In reality it can be many miles from this location. That means  the nightlights lookup or ANY georeferenced data ( impervious surfaces,  gridded population, land cover) may be wrong. One of my readers alerted  me to a project to correct the data. That project can be found here.  That resource led to other resources including a 2 year long project to  correct the data for all weather stations. Its a huge repository. That  led to the WMO documents one of the putative sources for GHCN. This  source also has errors. Luckily the WMO has asked all member nations to  report more accurate data back in 2009. That process has yet to be  completed and when it is done we should have data that is reported down  to the arc second. Until then we are stuck trying to reconcile various  sources.
The first problem to solve is the loss of precision problem. The WMO  has reports that are down to the arc minute. It’s clear that when GHCN  uses this data and transforms it into decimal degrees that they round  and truncate. These truncations, on occasion, will move a station.  I’ve  documented that by examining the original WMO documents and the GHCN  documents. In other cases it hard to see the exact error in GHCN, but  they clearly dont track with WMO. First the WMO coordinates for WMO  60355 and then the GHCN coordinates:
WMO:   60355	SKIKDA	36 53N	06 54E  [36.8833333, 6.9000]
GHCN: 10160355000 SKIKDA  36.93    6.95
GHCN places the station in the ocean. WMO places it on land as seen above.
To start correcting these locations I started working through the  various sources. In this post I will start the work by correcting the  GHCN inventory using WMO information as the basis. Aware, of course that  WMO may have it own issue. The task is complicated by the lack of any  GHCN documents showing how they used WMO documents. In the first step  I’ve done this. I compared the GHCN inventory with the WMO inventory and  looked at those records where GHCN and WMO have the same  station  number and station name. That is difficult in itself because of the way  GHCN truncates names to fit a data field. It’s also complicated by the  issue of re spelling, multiple names for each site and the issue of GHCN  Imod flags and WMO station index sub numbers.
Here is what we find. If we start with the 7200 stations in the GHCN  inventory and use the WMO identifier to look up the same stations in the  WMO official inventory we get roughly 2500 matches. Here are the  matching rules I used.
1. the WMO number must be the same
2. The GHCN name must match the WMO name (or alternate names match).
3. The GHCNID must not have any Imod variants. (no multiple stations per WMO)
4. The WMO station must not have any sub index variants. (107 WMO numbers have subindexes)
That’s a bit hard to explain but in short I try to match the stations  that are unique in GHCN with those that are unique in the WMO records.  Here is what a sample record looks like.WMO positions are translated  from degrees and minutes to decimal degrees and the full precision is  retained. You can check that against GHCN rounding. As we saw in  previous posts slight movements in stations can move them from Bright to  dark and from dark to bright pixels.
63401001000     JAN MAYEN 70.93 -8.67              1001    JAN MAYEN 70.93333 -8.666667
63401008000     SVALBARD LUFT 78.25 15.47    1008    SVALBARD AP 78.25000 15.466667
63401025000 TROMO/SKATTO      69.50 19.00    1025   TROMSO/LANGNES 69.68333 18.916667
63401028000 BJORNOYA                 74.52 19.02    1028    BJORNOYA 74.51667 19.016667
63401049000  ALTA LUFTHAVN 69.98 23.37    1049  ALTA LUFTHAVN 69.98333 23.366667
You also see some of the name matching difficulties where the two  records have the same WMO and slightly different names. If we collate  all differences on lat and lon in matching stations we get the  following:

And when we check the worst record we find the following
WMO:  60581  HASSI-MESSAOUD             31.66667      6.15
GHCN:  10160581000 HASSI-MESSOUD 31.7               2.9
GHCN has the station at longitude [smm] 2.9. According to GHCN the station is an airport:

The location in the WMO file

And the difference is roughly 300km.WMO is more correct than GHCN. GHCN is off by 300km

An old picture of the approach (weather station is to the left)

And diagrams of the airfield
Now, why does this matter.  Giss uses GHCN inventories to get  Nightlights. Nightlights uses the location information to determine if  the pixel is dark (rural) or bright (urban)
NASA thinks this site is dark. They think it is pitch dark. Of course  they are looking 300km away from the real site. From the inventory used  in H2010.
10160581000 HASSI-MESSOUD   31.70    2.90  398  630R  HOT DESERT    A    0






			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e877a8562',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Jolee Mohr died in July at the age of 36 after receiving experimental treatment for arthritis. “It was supposed to be just a simple thing,” said her husband, Robb, but something went horribly wrong.



“No one knows yet whether the treatment was to blame,” wrote Rick Weiss of the _Washington Post_. “But a close look at the events leading to Mohr’s death reveals failures in the safety net that is supposed to protect people from the risks of medical experimentation.”



Last week, the U.S. Court of Appeals for the District of Columbia ruled that terminally ill patients do not have a constitutional right to use experimental treatments without being enrolled in a clinical trial or participating in other very limited Food and Drug Administration‐​approved options, even if their doctors believe that such treatment is their best chance for survival. The case is being appealed to the Supreme Court.



The D.C. appeals court and so many others who share a “safety first” approach to experimental treatment are only seeing half the picture. Experimental treatment is inherently risky. But overemphasizing safety prevents patients from taking a calculated risk when they think it’s worthwhile.



As my husband and I learned, that freedom is important. Our son was diagnosed with cancer when he was 9 months old. Next week, he leaves for college, and the only visible reminder of his brush with death is a scar from surgery.



No, I’m not saying that an experimental treatment saved his life. What probably saved him was his parents’ persistent questions, and events resulting in a course of treatment that changed so often no doctor would have recommended it at the beginning.



Individuals often react unexpectedly even to conventional treatments, let alone experimental ones. What would have happened if someone had told us, “Sorry, that treatment is no longer an option. It has been found unsafe because children have died from it.”



At the time, all the treatments — including the experimental ones we were offered but ended up not choosing — resulted in about a 50% chance of death, and in many cases they killed the children before the cancer did. But there were still crucial choices to make. No one was as well suited to make them than we were, in consultation with our son’s doctors.



In the appeals court case, the Abigail Alliance for Better Access to Developmental Drugs sued the FDA for refusing to allow terminally ill patients to purchase experimental drugs. Patients who couldn’t gain access to the medicines they wanted — because the trials were closed or the patients were too sick or otherwise didn’t qualify for FDA permission — petitioned the FDA to be allowed to take medicines under the supervision of their own doctors.



According to the FDA, such treatments would lead to unacceptable risk.



Unacceptable to whom?



In denying patients the right to weigh the risks for themselves, the FDA and the court denied them their only hope for survival. The government essentially told them that it would be better for them to do nothing and die than to take risky experimental treatments.



We do not need a governmental authority involved in medical decisions that are uniquely personal to patients and their families. But along with the right to make decisions comes the obligation to think them through. Any doctor who lies to patients or intentionally misleads them about the risks of certain treatments should be punished, but patients need to read informed‐​consent forms carefully, ask questions and not assume that their doctors can do risk‐​benefit analyses without knowing what’s most important to the patient.



In our case, each option had its own set of risks, including death, but also a chance at a better life. One experimental protocol we were offered tested a new course of chemotherapy that had a lower chance of stunting our son’s growth or causing sterility, but possibly also a lower chance of curing his cancer. But at least we had this option.



I wonder if in this “safety first” climate toward experimental treatment — and with this latest court ruling — any of the protocols we considered would be approved. All had the possibility of deadly side effects. It’s frightening to think a government agency might have limited our options because it thought one treatment safer than another.



The proper response to tragedies like the one that befell Jolee Mohr is not to try to make experiments risk‐​free, but to help patients understand the risks.



Robb Mohr told the _Post_ that “the science seemed good. There’s nothing I knew of that could have predicted this.” But the consent form his wife signed stated that the experimental therapy had possible “unknown side effects” including “in rare circumstances, death.”



Participating in experimental treatments is a decision not to be taken lightly. If the public goes along with the “safety first” mentality and abdicates decision‐​making authority to government regulators, we will lose an important right. We’ll lose the choice to participate in experiments that might kill us, but we’ll also lose the option to make a choice that might save our lives.
"
"

Interest groups in the United States have focused on the possibility of including provisions in trade agreements with the intent of countering currency manipulation. The concern is that another country may choose to reduce the value of its currency relative to the U.S. dollar in order to encourage its businesses to export more goods to the United States. Such currency realignment also would tend to make it more expensive for the devaluing nation to import products from this country.   
  
It’s true that an adjustment in currency exchange rates – regardless of the reason for the adjustment – can have an effect on trade flows. U.S. industries that export to foreign customers, or compete with imported goods in the domestic marketplace, understandably would prefer that currency relationships not become skewed against their commercial interests. Currency stability improves the business climate by making it easier to build long-term relationships with customers and suppliers.   
  
However, currency exchange rates have fluctuated throughout recorded history. Sometimes those changes may be driven by a government’s conscious desire to devalue its currency. More often the variability in exchange rates reflects fundamental economic realities. Economies that experience growing productivity and rising prosperity should not be surprised to find that market pressures cause their currencies to strengthen. The reverse is true for countries that are growing slowly or not at all.   
  
A shift in exchange rates changes a country’s “terms of trade,” which is a term used by economists to describe the ratio of a country’s export prices to its import prices. From a U.S. perspective, if another country sets its currency at an artificially low level relative to the dollar, the U.S. terms of trade will improve. The United States will be able to obtain a greater value of imports for the same value of exports. Exporting the same number of airplanes and soybeans as before will pay for the importation of larger quantities of shoes, coffee, and automobiles.   




The country that chooses to undervalue its currency will be placing an artificially low value on the output created by workers and capital in its domestic economy. It will, in effect, be selling its exports for less than their true economic worth, thus transferring wealth to the United States. People in this country experience meaningful increases in their standards of living at the expense of the country that has devalued.   
  
Yes, most buyers like to get a good deal. An increase in affordable imports generally doesn’t strike consumers as a bad thing. Assuming those imports don’t compete too directly with goods and services produced widely in the United States (think of coffee, bananas, shoes, clothing, diamonds, rare earth metals, etc.), they tend to be well accepted even by people with mercantilist tendencies. Some imports that do compete directly with U.S. products – such as crude oil or cars – also may not raise strong political objections, either because domestic demand is larger than can be served solely by domestic supplies, or because consumers desire a variety of choices.   
  
The politics of affordable imports become more complicated when those products compete directly with goods and services produced in the importing country. Competition always is a challenge, whether it comes from other domestic firms or from overseas. Firms often struggle to deal with forces as diverse as changing technology or changing consumer tastes and preferences. Not all firms survive forever. Rather, the process of creative destruction keeps the economy in an ongoing state of reinvigoration and renewal. There’s no doubt, though, that an increase in imports can create adjustment headaches for import-competing U.S. companies and their workers.   
  
The good news is that the United States already has a policy framework with which to address unfairly priced imports, regardless of whether those imports relate to currency undervaluation. U.S. trade remedy laws allow industries to seek antidumping or countervailing duty (AD/CVD) protection against imports that may be injuring domestic producers. From a free-trade perspective, it’s important to understand that U.S. trade remedy laws leave a lot to be desired. They generally are seen to be relatively protectionist – slanted in favor domestic industries over imports.   
  
However, trade remedies are a better policy response (even though suboptimal) to currency manipulation than would be the case for special provisions in trade agreements. Trade remedies are relatively selective. They are applied only to unfairly priced imports that are troublesome to U.S. industries, and only after those producers have demonstrated that they’ve been injured. On the other hand, currency provisions included in trade agreements would apply to all imports from the offending country. American consumers would end up paying more even for tea and T-shirts, for which there is little or no U.S. production. Given the broad negative implications of using trade agreement provisions to counteract currency manipulation, U.S consumers would be much better off dealing with the narrower negative consequences of AD/CVD measures.   
  
A concluding thought: Since currency undervaluation by other countries serves to transfer wealth to the United States, should we consider finding some diplomatic way to thank them? Such a gesture likely would do far more good than including misguided currency provisions in trade agreements. It might help prompt policymakers around the world to rethink the plusses and minuses of allowing currencies to get out of alignment.   
  
(For more detail on issues surrounding currency manipulation, see this article from Forbes.com by my colleague, Dan Ikenson.)


"
"
Share this...FacebookTwitterDistinguished IPCC climate scientist Professor Hans von Storch wrote what to me appears to be a very twisted and disturbing statement at his Klimazwiebel blog. I’m really quite surprised by it.
Von Storch writes he got correspondence from a friend, who asked him how he personally thinks people can contribute to reducing climate change. HvS provides his “brief and spontaneous” answer by writing that a single person can’t really do anything and that technology needs to be developed to reduce CO2 emissions, and to do it economically.
So far so good.
He then writes that people installing solar panels on their rooves, though with good intentions, is in fact ineffective symbolism, and indeed is only merely spreading the illusion that one is doing something good. If anything, feel-good people are in fact impeding the development of truly effective approaches.
Then come his last two sentences, which I have translated below:
Die globalen Emissionen sind im letzten Jahr laut IEA um 3.2 % gestiegen. Ich glaube, das entspricht der jährlichen Emission von Deutschland, um und bei.
Die wirksamste Klimapolitik der letzten Jahrzehnte war die 1-Kind Politik in China, die der Welt ca. 400 Millionen CO2-Emittenden und Emittenden-Vermehrern erspart hat.
IN ENGLISH:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to the IEA, global emissions last year climbed 3.2%. I believe that corresponds to the annual emission of Germany – roundabout.
The most effective climate policy of the last decade was the 1-child policy in China which saved the world from approx. 400 million emitters and emitter-reproducers.”
Denial of human life as an effective climate policy? Is he being cynical or has he totally lost his marbles?
I can only assume he is being cynical and is indirectly criticising what has been ineffective “climate policies” so far. (Personally, I don’t see how anyone can say “climate policies” have been ineffective when global temperatures have not risen at all over the last 10 years. Where’s the failure?).  Cynical or not, that 1-child statement goes way too far. Unusual coming from a man who dislikes extreme views from either side.
It would be disturbing enough if HvS were just another climate scientist trying to get attention, but he is much more than that. He is professor at the Meteorological Institute of the University of Hamburg, Director of the Institute for Coastal Research at the Helmholtz Research Centre in Geesthacht, and a member of the advisory boards of the journals Journal of Climate and Annals of Geophysics. He is one Germany’s leading climate scientists.
And when the world’s leading scientists run loose and start spewing about the virtues of mass population reduction in scientific terms, then the rest of us really need to worry. Dangerous politicians have a nasty habit of gravitating in their direction.
Strangely, HvS wrote his short essay in German, and not in the usual English which one finds more often at Klimazwiebel. Perhaps the elimination of 400 million people just seems to come across better in the more authoritarian German.
Cynical or not, it’s time for the professor to retire. In the very least he’d be wise to call his statement a mistake and to retract it.
 
Share this...FacebookTwitter "
"

John McCain’s idea — now embraced enthusiastically by Hillary Clinton — to temporarily suspend the federal gasoline tax between Memorial Day and Labor Day is rich fodder for energy analysts. Even richer, however, is the somewhat curious response that the proposed “tax holiday” has provoked from political actors and policy pundits of various stripes. A quick tour of the issues in play is instructive.



First, if there is any math out there to refute Barack Obama’s claim that the proposed tax holiday would save the average driver a grand sum of $28 — “otherwise known as $9 a month” as he puts it, or the grand sum of one‐​half of a tank of gas — it has escaped our attention. Of course, even that calculation presupposes that service station owners will pass on the full tax cut to the consumer — which they most definitely would not. How much of that tax cut would reach consumers is unclear. What is clear, however, is that Sen. McCain’s claim that the tax holiday would provide a powerful stimulus to the economy is risible. Sen. Clinton’s claim that the savings represents “real money” to the poor, hard‐​trodden masses yearning to keep their heads above water is similarly hard to swallow.



Second, the McCain/​Clinton plan is directly at odds with other stated policy objectives forwarded by McCain/​Clinton. Both candidates, for instance, dutifully call for the government to do more to promote energy conservation, reduce greenhouse gas emissions, encourage renewable energy, and to break our national “addiction to oil.” And they hope to do this by … reducing gasoline prices?



Consider the McCain‐​Lieberman bill to reduce greenhouse gas emissions by one‐​third below 2000 levels by 2050. The EPA estimates that the bill would increase gasoline prices 26 cents per gallon in current dollars by 2030 and 68 cents per gallon by 2050. Electricity bills would likewise go up by 22 percent in 2030 and 25 percent in 2050. So what John McCain proposes to give with one hand will be taken — in spades! — by the other.



Hillary Clinton’s two‐​handed policy is even more striking. Her big complaint with McCain‐​Lieberman is that it doesn’t go far enough. Tie the tax holiday with a windfall profits tax, however, and the tension between the various aspects of the McCain/​Clinton energy agenda disappears. That’s Hillary Clinton’s plan; replace the lost revenues from the tax holiday with a windfall profits tax on “big oil” and tell the voters that you’ll make the oil companies pay their gasoline taxes for a while. What she doesn’t tell her adoring working‐​class fans is that a windfall tax will send oil prices up, not down. That’s the conclusion of the only analysis of the economic impact of that tax that we are aware of — written by Salvatore Lazzari of the Congressional Research Service — which found that the 1980 windfall‐​profits tax reduced domestic oil production by 3–6 percent, a result that should come as no surprise.



But even that result probably understates the long‐​term effect. Some analysts regurgitate the standard textbook line that a one‐​time, lump‐​sum tax on windfall profits shouldn’t alter market conditions. But once such a tax is imposed, who’s to guarantee that it won’t be imposed again? The effect of the windfall profits tax (repealed in 1988) is almost certainly still with us because domestic producers are forced to consider the possibility that new windfall profit taxes will be imposed in the future. Hence, some subset of otherwise profitable investments in domestic production are never made because the possibility of a new windfall profit tax tips the balance against the investment.



Gall over the manifest hypocrisy of these two candidates, however, does not explain the vicious response the tax holiday has received from pundits and public intellectuals. No, their anger largely stems from outrage over the public betrayal of the case for high‐​energy prices. That case, however, is as dubious as the case for a tax holiday.



Some pundits argue that the feds need to discourage oil consumption — and thus increase fuel taxes — to reduce the flow of money going to Islamic terrorists. There is no correlation, however, between world oil prices (and thus, oil profits) and Islamic terrorism. Even when crude oil prices were in the $20s per barrel, al‐​Qaeda, Hezbollah, and Hamas were doing quite nicely and statistical analysis concretely demonstrates that terrorism does not correlate with oil revenues.



Others argue that we need higher gasoline prices to internalize the costs associated with climate change. That argument sounds correct, but no one has convincingly established the correct premium on the price of gas that would yield the appropriate degree of conservation. Such a task is understandably difficult, if not impossible. But a probable “market failure” in gasoline production and consumption does not guarantee “government success” in attempting to correct it via a gas excise tax. Indeed, the more we hear on this from our current crop of presidential candidates, the more afraid we are of an eventual “government failure” that’s worse.



On balance the greater danger is that, once elected, any one of the remaining presidential candidates will increase, not reduce, the gas tax. And, as appears likely, devoting the additional revenues to the highway trust fund would extend the current confused government policy: Building and maintaining roads and highways with revenues from a gas conservation tax promotes more, not less driving and gas consumption by the public.



The gas tax cannot do both — serve as a stick to promote gas conservation for slowing global warming and as a carrot to provide an economic boost for families in trouble. That’s the reality, but everyone seems intent on taking a holiday from it.
"
"

Al Gore has finally won his Nobel Prize, reminiscent of the proverbial little nut that stood his ground, evolving into a giant oak. Now we can only hope that he runs for president, an office that, given recent history, surely deserves him.



Where else — except perhaps via the Kyoto Protocol on global warming, which Gore negotiated — can someone accomplish so little while spending so much? But, to get there, or at least to the Demo nomination, Gore’s going to have to do something he has assiduously avoided: debate.



Gore’s standard rule on live TV has been there will be no live challenge. The last time he ran for president (2000), he succeeded de facto, with George “Carbon Bonoxide” Bush as the token global warming flyweight. This time, debates happen.



That’s because Gore represents a party gone global warming ga‐​ga, with some of the world’s goofiest environmental legislation in history awaiting a Bush veto and a Gore signature.



Consider what Bernie Sanders (“I”-VT) has in the docket: A legislative magic wand that will require us to reduce our emissions of carbon dioxide by 90% in a mere 42 years. Since 1990, we’re up a little under 20%. Sanders’ legislation takes us back to the 1930s, a technological stone age. True, there’s other, more “moderate” legislation. John Kerry’s (D-MA) proposal would cut it back 80%. Dianne Feinstein (D-CA) is at 50%.



Each and every one has a good chance of Senate passage, and an even better chance of a veto. So, now that you have your Nobel, _come out and fight like a man, Al, and don’t even worry about picking on someone your own size_.



The fact is that Al has ducked, feinted, dived away from, or fluffed each and every opportunity for a reasoned debate with any global warming scientist not of his choice, a choice he no longer enjoys. Heartland Institute, a Chicago think tank, spent over a million dollars filing ads in the _Wall Street Journal_ , the _New York Times_ and their ilk, begging Al to debate. No dice. In a less public venue, my own Cato Institute sent kind and courteous letters asking him to share our pretty auditorium on Washington’s Massachusetts Avenue, for a civil discussion with our scholars. Again, no dice.



Here’s the rub: if any opposition were so easy to vanquish, Gore would relish the opportunity. Obviously there’s a substantive and cogent argument he can’t kill.



In essence, it is that Gore has massively departed from the scientific mainstream on global warming, even as that community may be itself biased by the funding afforded by emphasizing the negative.



For example, the United Nations’ Intergovernmental Panel on Climate Change (of which I am a member, while Gore is not) predicts a mean sea‐​level rise of about 13 inches by 2100. Gore’s book and movie contain an undated montage showing Florida sliding beneath the waves, something that could only happen with 13 _feet_ or more.



How on earth does one accomplish such a disconnect from scientific reality?



Gore only has one scientist, James Hansen of NASA, whispering the sweet nothings into his ear that sea‐​level could rise this much or more in the next 92 years, as Greenland’s ice sheets are destabilized by climate change.



No other scientist is willing to climb out on this limb, because it is simply not supported by the observed climatic history of Greenland since the end of the last ice age. For much of six millennia, ending 3,000 years ago, it had to be warmer, and yet the ice stuck like glue. Hansen’s amazing response, which you can read on his blog, documented at www​.real​cli​mate​.org (not exactly a peer‐​reviewed scientific journal!) is that other scientists don’t agree with him because they suffer from what he calls “scientific reticence.” In other words, all his colleagues are chicken‐​bleeps because they don’t agree with him.



How about the other pole? Every computer model mentioned by the United Nations shows Antarctica _gaining_ ice this century because a slight warming will result in more precipitation which must fall as snow. Would Gore like that out in public? Or how about the fact that Antarctica just set its _record maximum_ for sea‐​ice extent, as measured by satellite.



The world can only hope that Gore’s Nobel propels him into another run for the Presidency. He received it for his climate lunacy. Now he can defend it and the Nobel Prize by merely debating those who must be so easy to defeat.
"
"
Surrounding these recent revelations, some hilarity from the world’s preeminent skeptic cartoonist, Josh.

Thanks to Josh at www.cartoonsbyjosh.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e89295a23',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Sir David Attenborough has urged governments to ban deep sea mining, following a study warning of “potentially disastrous” risks to the ocean’s life-support systems if it goes ahead. The study, by Fauna and Flora International (FFI), warns proposed plans to mine the seabed could cause significant loss of biodiversity, disruption of the ocean’s “biological pump”, and the loss of microbes important for storing carbon. The process, requiring machines operating thousands of metres under the sea, could also create plumes of sediment that smother areas far from the mining sites and kill wildlife.  Dozens of exploratory licences, two of which are sponsored by the UK, have already been granted for huge tracts of the sea bed, ahead of a race to mine commercially for ores and minerals such as copper, used in mobile phones and batteries. But the rules to govern the responsible exploitation of this global resource are not finalised – they are expected to be completed at a meeting in July at the UN International Seabed Authority. Attenborough, the vice president of FFI, said deep sea mining could create a “devastating series of impacts” threatening processes critical to the health and function of the oceans, and called on governments to be guided by scientists. “Fauna & Flora International is calling on global governments to put in place a moratorium on all deep sea mining – a call I wholeheartedly support,” Attenborough said. In a foreword to the report, Attenborough said it was “beyond reason” for countries to consider the destruction of deep sea places before they have understood them or the role they play in the health of the planet. Attenborough said: “The rush to mine this pristine and unexplored environment risks creating terrible impacts that cannot be reversed. We need to be guided by science when faced with decisions of such great environmental consequence.” FFI warned that human activity was already putting a huge strain on the oceans, which have absorbed a third of our carbon emissions and 93% of the extra heat trapped by the rising concentration of greenhouse gases. Oceans are becoming more acidic because of the carbon dioxide dissolving into them, fisheries are under pressure as a result of over-exploitation and there are hundreds of huge “dead zones”, it said. Pippa Howard, director at FFI and lead author of the report, called for a moratorium on deep sea mining. She said: “The conclusions we have come to after extensive study could hardly be more troubling. “From methane release to disruption of the ocean’s life-support systems and the destruction of unstudied ecosystems, the risks of deep sea mining are numerous and potentially disastrous.” Louisa Casson, of Greenpeace’s Protect the Oceans campaign, said the UK government’s holding of exploration contracts for deep sea mining was at odds with its position as a “global ocean champion”.Casson said: “The UK government now has a choice to make: listen to industry and press ahead with this dangerous new practice, or listen to scientific warnings, public concern and the creator of Blue Planet himself and ban deep sea mining.” A government spokesman said: “The UK continues to press for the highest international environmental standards, including on deep sea mineral extraction. “While we have sponsored two exploration licences, these allow only for marine research to understand the effects of deep sea mining. “We will not issue a single exploitation licence without a full assessment of the environmental impact.” • This article was amended on 21 May 2020. It is Louisa not Louise Casson of Greenpeace."
"
Guest Post by Willis Eschenbach
Today I thought I’d discuss my research into what is put forward as one of the key pieces of evidence that GCMs (global climate models) are able to accurately reproduce the climate. This is the claim that the GCMs are able to reproduce the effects of volcanoes on the climate.
One of the most-cited papers in this regard is the Soden et al. study of the eruption of the Philippine volcano, Mt. Pinatubo. Their study is entitled Global Cooling After the Eruption of Mount Pinatubo: A Test of Climate Feedback by Water Vapor, available as a PDF. [hereinafter “Soden08”]

Figure 1. A NASA graphic showing satellite measurements of the spread of ash and aerosols from Mt. Pinatubo. In the first month (top right), the volcanic emissions had circled the earth (red area, Philippines on right). In six months, they were fairly evenly spread around the planet. Graphic Source NASA
The eruption of Mt. Pinatubo on 15 June 1991 injected aerosols and volcanic ash high into the atmosphere. This measurably changed the climate for a couple of years. It provided a wealth of observational data, as well as a good test for climate models.
Regarding the match between models and volcanic reality, the authors of Soden08 say (emphasis mine):


Because the transient response of the model depends on both its sensitivity and the external radiative forcing imposed on it, we first demonstrate the consistency between the model-simulated radiative forcing with that measured by satellites (Fig. 1 [of Soden08]). Both the observations and model simulations yield very similar reductions in the absorbed solar or shortwave (SW) radiation, which are nearly twice as large as the reduction in emitted LW radiation, a net loss of radiative energy that cools the surface and lower troposphere.

Parenthetically, when a scientist starts talking about “consistency” between observations and model results, I check my wallet. Consistency? What units are used to measure that? But I digress, back to my question.
How do the models actually stack up against the observations shown in their paper, in the Figure 1 they mention?
Before I get to the question and to their Figure 1, a bit of a diversion. For reasons that will become clear shortly, I want to make a distinction between simple negative feedback, and a governor. A governor is a system that keeps a heat engine running at a (relatively) constant speed. The most common example of a governor in daily life is the “cruise control” on a car. It keeps the car going at the same speed regardless of uphill and downhill grades.
Negative feedback is like increasing wind resistance on an accelerating car. Wind resistance slows the car down. Eventually at some speed wind resistance balances the energy pushing the car, and the car goes no faster. It balances out at a certain speed, where increasing feedback matches energy input. Fig. 2 shows how a negative feedback and a governor respond to increasing forcing.

Figure 2. A comparison of the actions of a governor and negative feedback. Qualitative only, numbers are nominal values.
Note that in response to changed forcings, the governor brings the value back to the desired equilibrium value. The governor does this by producing what is called “overshoot”. “Overshoot” is where the action of the governor drives the system past equilibrium (represented in Fig. 2 by the thick horizontal line at zero). This gives the governor the ability to recover quickly from perturbations.
Simple negative feedback, on the other hand, cannot maintain a specified speed. All it can do is reduce the size of a speed increase or a decrease . It cannot produce overshoot. As shown in the right side of the graph, simple negative feedback can create what appears to be a controlled equilibrium situation. This happens when feedback balances forcing so there is no change in speed.
However, this balance of negative feedback is not stable — any change in the forcing will lead to a new equilibrium speed. A governor, on the other hand, maintains the same speed despite changes in forcings.
Overshoot is necessary to control a “lagged” system such as the climate. This is a system where response to inputs is not instantaneous. I have argued elsewhere that the earth has at least one governor system incorporating overshoot which actively controls the temperature. For our current purposes, please take note of the very different shapes of the response curves of negative feedback and of a system with a governor.
With that as prologue, let us now look at the Soden08 Figure 1.

Soden08 Figure 1. ORIGINAL CAPTION. Comparison of the observed anomalies in absorbed SW (top) and emitted LW (bottom) radiative fluxes at the top of the atmosphere from Earth Radiation Budget Satellite observations (black) and three ensembles of GCM simulations (red). The observed anomalies are expressed relative to a 1984 to 1990 base climatology, and the linear trend is removed (30). The results are expressed relative to the pre-eruption (January to May 1991) value of the anomaly and smoothed with a 7-month running mean (thick line). The GCM anomalies are computed as the difference between the control and Mount Pinatubo simulations for each ensemble member. Both the model and observed global averages are from 60N-60S due to the restriction of observed data to these latitudes. 
This looks good at first blush, and the authors say that the GCM results (red) are “consistent” with the observations. However, closer examination reveals issues. What struck me immediately about their results is that the actual observations of both the shortwave and longwave anomalies show clear signs of overshoot. After being knocked down by the volcano, after 1994 they both come back higher than pre-eruption. This worked to quickly restore the pre-disturbance state.
None of the GCMs show this sign of overshoot. Instead, the GCMs gradually drift back to the pre-eruption anomaly value of zero. This is similar to the negative feedback balance shown in Fig. 2. Unlike the overshoot in the observations, the GCM results flatline after 1994. While this doesn’t prove anything, it is another piece of evidence that the GCMs are missing some basic climate mechanisms.
How Much Total Difference did Pinatubo Make?
There is a second problem with the Soden08 model results, one which is less theoretical and more mathematically demonstrable. This has to do with the cumulative energy deficit from the volcanic eruption.
As you can see in the Soden08 Fig. 1 above, after the eruption the amount of incoming solar energy (SW, or shortwave radiation) dropped about twice as much as outgoing energy (LW, or longwave radiation). As a result, after the volcano there was a global net energy deficit. There was less energy entering the system than there was leaving the system. This deficit continued for some months.
The total magnitude of this deficit is an important indicator of the overall impact of the Pinatubo eruption on the climate system. It is a basic measurement of the phenomenon, answering the fundamental first question everyone asks — how big is it? We can investigate the total magnitude of the volcanic disturbance by looking at the cumulative energy deficit created by the eruption.
To do that, I first digitized the data in the Soden08 Figure 1. The data is available here as an Excel worksheet. Results in the worksheet for the GCMs are the average of the three runs shown in their Figure 1.
I then calculated the net energy balance (solar energy absorbed minus longwave energy emitted to space) for each month. I then started a cumulative total at zero, and added each month’s net energy balance to the cumulative total. This cumulative total shows how far out of balance the system was each month.
The resulting curve shows the size of the total disturbance caused by the volcano, as well as showing the path of recovery. The units are Watt-months/metre^2 (for convenience, since the data is monthly). For example, a two-Watt/m2 deficit that continued for three months would give a cumulative deficit of six Watt-months/m2. Fig. 3 illustrates the problem with the GCM results.

Figure 3. Cumulative effect of the Pinatubo eruption. Red line shows data from the average of the models (GCMs) shown in Soden08 Figure 1 above. Blue line shows data from the ERBS observations shown in Soden08 Figure 1 above. 
So what does this result mean? Well, among other things it means that in this case the general “first glance” similarity of observations and models is misleading. A closer examination shows that the models did a very poor job at being consistent with the observations.
• The models greatly underestimated the magnitude of the peak impact of the eruption.
• They showed recovery starting much sooner than the observations show.
• They greatly underestimated the speed of the recovery once it started.
• They greatly underestimated the total impact of the eruption.
To put some numbers on those statements:
• The peak energy deficit in the observations was -42 Watt-months/m^2. This is more than twice the -18 Watt-months/m2 in the model results.
• The models show recovery starting about a year and a half after the eruption. The observations show about two and a half years before things turn around.
• The observed speed of the recovery is more than five times that of the modelled speed of recovery (post-1994 linear trends).
• The total impact of the eruption on the global energy balance is given by the area underneath the curves. Alternatively, it can be expressed as the average value of the energy deficit over the time period of the curves (1991-1995). The observed average energy deficit over that period is -21 Watt-months/m2. Again, this is more than twice the models’ estimate of -10 Watt-months/m2.
My conclusion? These models are not doing a credible job of representing Pinatubo’s effect on the global energy balance. The total size of the disturbance is a fundamental, basic measure of the accuracy of a simulation. Both the observed peak energy deficit and the observed total impact of the volcano were more than double the model results.
An error where the raw observed size of the phenomenon is more than double the model estimate? That sounds like a government project. Bad model, no cookies. Clearly, there is some fundamental problem with their simulation — they are showing the eruption of Mt. Minitubo, the half-size model.
When models show that kind of error in the raw size, peak size, and timing of the effects of a volcanic eruption … are the models “consistent” with the observations? Would you pay good money for a model that gave that size of error?
In addition, model results do not show the observed “overshoot” that leads to a speedy recovery from a disturbance. The results of this observed overshoot are seen in the difference between modelled and observed recovery rates. Driven by overshoot, the observed recovery rate is five times that shown by the models.
In short, I see no support for their implied claim that the models are an accurate representation of reality. Far from that, I don’t even see support for their vague claim of “consistency”. Their idea of consistency reminds me of the line from the old song, “She could easily pass for forty-threeeeee … in the dusk … with the light behind her.”
Their model results are inconsistent with observations. I do not see the Soden08 study as support for the idea that GCMs can successfully model the changes from volcanic eruptions.
Regards to all,
w.
PS – A final note for clarification. As the title suggests, the main thrust of Soden08 is concerned with whether the models perform better if they include a water vapor positive feedback. It finds that the GCMs do in fact perform better if there is positive feedback from a warming.
My analysis of the model results and observational data above is completely separate from the Soden08 analysis. We are simply using the same results and data. I make no claim that their analysis is right or wrong. In fact, I strongly suspect they are right, that the models do perform better if there is positive feedback from warming, although I have my own ideas what that demonstrates. But that is distinct from my analysis above.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e873dd058',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterA recent study by A.R. Agatova et al investigated glacier dynamic and climatic variations in the southeastern part of the Russian Altai during the last 7000 years and show distinct natural climatic changes had occurred.
Not surprisingly, these changes coincide with changes occurring at other parts of the globe, and so add to the massive weight of evidence refuting the claim that climate fluctuations on centurial in millennial scales are regional phenomena and occur over a small temperature range.
The scientists exhumed organic material and carried out radiocarbon dating on wood remains from buried dead trees at the upper tree limit, and from rock glaciers on trough slopes from six glacial valleys in the North Chuya Range, SE Altai. They compiled an extensive dataset, which form the basis for understanding the relative magnitudes and timing of the most important glacial and climatic events of SE Altai.
Their conclusion:
New data refute the traditional concept of the Russian Altai Holocene glaciations as a consecutive retreat of the late Würm glaciers and argue their complete degradation at the head of trough valleys at least 7000 cal. years BP.”
Moreover, they identified three periods of glacial advances: from 4900 to 4200 cal. years BP (Akkem stage), from 2300 to 1700 cal. years BP (Historical stage) and in the 13th–19th centuries (Little Ice Age (LIA) or Aktru stage). The coincident extremes of lowering temperature and increasing precipitation during the Akkem stage led to abrupt glacier advances and forming of the most remote moraine complexes downstream in the valleys.
The authors also write that in addition to the radiocarbon data, the time limits of the Historical stage were defined more precisely using dendrochronological and archaeological data from Scythian burials of Pazyryk culture in SE Altai.
Repeated forest regrowth in the presently glaciatiated area indicates significant retreat or even complete glacier degradation during interstage warming. The decreases of glacier length in the following stages argues for intensification of aridity in the SE Altai during the second half of the Holocene. The thermal minimum in the middle of 19th century, the greatest in the last millennium, did not positively influence the mass balance of glaciers, which also supports this conclusion.”
So much for bogus claim that climate was more or less stable before man populated and developed.
Also strong glacial variations in the Alps as well


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Interestingly, Prof. em. Dr. Gernot Patzelt, University of Innsbruck, made a presentation at the International Climate and Energy Conference in Munich late last year, which the European Institute for Climate and Energy has just released.

In his presentation, Dr. Patzelt also reveals glacier advances and retreats in the Alps throughout the Holocene, thousands of kilometers away from the Russian Altai. Forests existed at elevations that were higher than today – in areas that are presently covered by glaciers.
At the 12:22 mark, Patzelt summarizes the data of the three glaciers examined in the Alps and presents a temperature reconstruction. His conclusion at the 13:42:
Over the last 10,000 years it has been warmer than today 65% of the time. Our current climate does not in any way show an anomaly in temperature development. That’s an important result.”

Top curve shows the reconstructed temperature of the Alps over the Holocene. Dark-shaded areas show warm periods. (Snipped from Patzelt’s presentation at the 13:30 mark).
Clearly from his chart one sees the millennial cycles that coincide with documented solar activity. And as Dr. Sebastian Lüning showed yesterday in Chicago, we are not talking about fluctuations of a couple of tenths of a degree, but of fluctuations over 1, 2 or even 3°C.
At 14:50 Patzelt shows the Greenland ice core reconstruction for comparison.
Clearly there are natural forces at work. Claims that natural factors retired 100 years ago are simply absurd.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIt used to be that climate skeptics in Germany never got interviews with major media outlets. It was a sort of unwritten rule.

Headline: “The Temperature will rise only one degree”
But now that seems to be changing – ever since the release of Prof. Fritz Vahrenholt’s and Sebastian Lüning’s skeptic book, Die kalte Sonne, last February. Although still lopsided, the skeptics are getting heard more and more. This provides some hope that Germany is beginning to return to a balanced discussion and environmental tolerance.
Indeed this is very important because history shows that Germany always flourished and made great contributions to the world when open discussion of ideas was allowed. And in times when this was not the case, Germany and the world suffered – at times immensely.
The latest is an interview with Fritz Vahrenholt appearing in Germany’s leading financial daily, the Handelsblatt.
The following are the questions and answers, in summary:
HB: In your book, you say the climate catastrophe is not going to take place. Why?
FV: The temperature increase since 1850 is nothing unusual. Such changes have occurred time and again in history.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




HB: Is that the case this time?
FV: Yes, because of strong solar activity. Now that the sun has been weak, we have not had any warming in 12 years.
HB: So Co2 is not the culprit?
FV: To some extent it is, maximum 50%. The rest is due to natural factors, i.e. the sun and oceans.
HB: Will temperatures rise this century?
FV: Not more than 1°C, i.e. far below the 4 – 5°C that often gets mentioned.
HB: But natural catastophes are increasing. USA is having a record drought.
FV: Contrary to what is claimed, hurricanes and storms are not increasing. Hurricanes are actually on the decline.
HB: Then why is the reinsurer Munich Re warning of catastrophes?
FV: Because an elevated fear of storms means more policies can be sold. Also it makes it easier to ram through new energy policy.
HB: But a majority of scientists are warning of climate change.
FV: There are also thousands of scientists who disagree. Let’s recall that agreeing with climate change makes it much easier to get funding.
HB: So we do not need alternative energies?
FV: Of course we need them. But we have have to develop them in an sensible way, and they should not trump all policy decisions.
 
Share this...FacebookTwitter "
"

Paul Krugman's column in today's _NYT_ laments the lack of a national policy to combat global warming. He writes: 



It’s true that scientists don’t know exactly how much world temperatures will rise if we persist with business as usual. But that uncertainty is actually what makes action so urgent. While there’s a chance that we’ll act against global warming only to find that the danger was overstated, there’s also a chance that we’ll fail to act only to find that the results of inaction were catastrophic. Which risk would you rather run?



He then cites the work of Harvard economist Martin Weitzman, who surveyed the results of a number of recent climate models and found that (in Krugman's words) ""they suggest about a 5 percent chance that world temperatures will eventually rise by more than 10 degrees Celsius (that is, world temperatures will rise by 18 degrees Fahrenheit). As Mr. Weitzman points out, that’s enough to 'effectively destroy planet Earth as we know it.'”   
  
Krugman concludes, ""It’s sheer irresponsibility not to do whatever we can to eliminate that threat"" and he calls for opprobrium against those who might impede global warming legislation: ""The only way we’re going to get action, I’d suggest, is if those who stand in the way of action come to be perceived as not just wrong but immoral.""   
  
There is merit to the argument that society should consider a policy response to the threat of global warming. A small chance of an enormous calamity equals a risk that may deserve mitigation. That's why people buy insurance, after all.   
  
However, Krugman doesn't accept that argument — at least, not when applied to other worrisome risks that trouble people whose politics are different than his. Less than two months ago, he wrote this about another future crisis: 



[O]n Friday Mr. Obama declared that he would “extend the promise” of Social Security by imposing a payroll-tax surcharge on people making more than $250,000 a year. The Tax Policy Center estimates that this would raise an additional $629 billion over the next decade. But if the revenue from this tax hike really would be reserved for the Social Security trust fund, it wouldn’t be available for current initiatives. Again, one wonders about priorities. Whatever would-be privatizers may say, Social Security isn’t in crisis: the Congressional Budget Office says that the trust fund is good until 2046, and a number of analysts think that even this estimate is overly pessimistic. So is adding to the trust fund the best use a progressive can find for scarce additional revenue?



In Krugman's view, policies to address Weitzman's 5 percent risk of ecological disaster by the early 23rd century (Weitzman's time frame, which Krugman didn't specify) are responsible and moral, but policies to address the economic crisis of Social Security's insolvency in less than four decades' time are unnecessary and overly pessimistic. And Krugman clobbers anyone who suggests otherwise .   
  
Make sense to you? Me neither.   
  
Krugman's double-standard on risk is not confined to Social Security. He has (rightly, IMO) blasted the Bush administration for going to war in Iraq. But couldn't the war be justified as mitigating a small risk of a great catastrophe? Was there, perhaps, a one-in-20 risk that Hussein's Iraq would develop weapons of mass destruction and direct them at the United States (in the next 200 years)?   
  
I write this not to argue that the United States should be unconcerned about global warming, or about rogue states' possession of super-weapons, or about Social Security's (and Medicare's) unsustainability. All are risks, and it is right for us to consider policy responses for each of them. My point is that it makes little sense to say one risk must be addressed while we should dismiss another risk with an expected value that's probably the same order of magnitude.   
  
Moreover, if this dichotomy is simply the product of Krugman's political allegiances (""Red team fears are stupid, Blue team fears are heroic""), isn't he being irresponsible, wrong and immoral?


"
"

An election’s end provides merciful relief from politicians’ claims that they will “transform” our lives and our economy. Whether it be pledges to turn around failing regions, deliver rapid decarbonisation, or plant hundreds of millions of trees per year, this particular campaign was littered with promises that we know, in our hearts, would not or could not be delivered.



Commentators worry about the breakdown of trust in politics and politicians. Nothing does more to accentuate it than unmeetable political commitments that subsequently have to junked or downgraded when reality hits. Yet many pundits still use “radical” or “bold” as synonyms for “good” when describing election manifestos.



The Conservatives’ offering this time around, for example, was criticised for its supposed absence of ambition. The “lack of significant policy action is remarkable,” concluded the Institute for Fiscal Studies’ Paul Johnson. Surely, big challenges — weak productivity growth, an aging population, climate change — require radical rethinks about policy?



In the past, I might have echoed such reasoning. Yet recent history surely shows the opposite: British politics suffers from a deficit of interest in modest, marginal improvements in government policy, not “big ideas”. Major overhauls of public services or welfare have proven either a waste of time or a disaster, while many headline‐​catching promises continually fall by the wayside.



Think about major policy change over the past decade. Universal Credit, though well intentioned, has seen vast resource and political capital invested in attempting to roll six working‐​age benefits into a single credit.



Near constant problems of delivery have beset it, with significant numbers suffering its teething problems. And all for a relatively small economic improvement in some recipients’ work incentives.



Andrew Lansley’s major reorganisation of the NHS saw a bitter passage through the House of Commons and cost billions to implement. Yet in the dying days of the last Parliament, a reversal of some of its key features was already underway. The director of the Nuffield Trust, Nigel Edwards, says that in future we will regard the Lansley reforms as “one of the most major public policy failures” of all time.



Then there’s the ongoing farce of our main grand transport project today — HS2. Its projected costs have now spiralled from £62bn to between £81bn and £88bn, all to deliver a much smaller projected “bang for the buck” than other minor transport projects elsewhere.



Rather than these bungled attempts to completely overhaul our welfare and health systems, imagine what might have been achieved with modest pro‐​work reform of tax credits or small‐​scale NHS experiments with automation. Instead of spending gargantuan sums for a prestige project to get marginally faster travel times between the midlands and London, think how many localised transport bottlenecks could have been relieved, easing commute times for thousand of families.



True, much smaller projects would not have generated the sexy headlines, but they would have almost certainly delivered better outcomes.



Now consider instead some policy areas that have gone well in recent years. Unemployment has fallen to extremely low levels, after a big post‐​recession spike. Most agree this reflects in part the more flexible labour market delivered by Thatcher’s liberalising agenda. But it also comes from welfare reforms and active labour market policies honed and refined under both political parties over decades.



In other words, gradual, incremental change has produced a jobs market that, while vulnerable to sharp shocks in recessions, is structurally strong.



Though it benefited both parties to exaggerate the differences, the seemingly successful Conservative school reforms under Michael Gove really built on the academies of New Labour too. Yes, other very targeted changes in how children are taught in certain areas have been rolled out — not least a mandatory focus on phonics in teaching reading. But these were well‐​evidenced, and not just delivered on a whim.



In an election campaign, of course, it’s too much to ask for politicians to really get into the weeds discussing small ideas. Big promises help them show they care about a particular issue and are determined to change it. But the arms race we’ve seen on planting trees, decarbonisation targets, or government investment levels are exactly the sorts of promises that ultimately lead voters to lose faith with politicians.



Trust requires delivering what you say you will. It’s why Boris Johnson is right that Parliament’s inability to deliver Brexit, more than anything else, has profoundly worsened the disconnect between electors and politicians.



Grand projects or major structural policy overhauls invariably disappoint. Not only do they bring large, unforeseen downsides that create political anger; they are high cost to reverse if they prove a dud. With Brexit already enough of a major disruption for the coming years, politicians should heed the lesson.



It would be much better to have some relatively stability in other areas, with gradual reforms that can try to improve things where current policies clearly fail.



No doubt, Britain does face major economic challenges. But not every big challenge requires a radical new solution. Just a generation ago, we understood that having a robust and growing economy, for example, required getting the conditions right in individual sectors. It meant making the necessary changes to taxes, regulations, or entry barriers to foster a competitive environment conducive to innovation.



Nowadays we hear less interest about how changes to incentives or structural conditions could deliver better outcomes in specific markets, instead obsessing over the supposed macroeconomic benefits of more spending.



History suggests that in delivery or outcomes, a top‐​down agenda will disappoint. If politicians are really interested in delivering those outcomes, they should focus less on the grandiose projects, and more on small scale reforms that could make markets and public services work better. 
"
"You might not be able to stomach soybeans for breakfast, lunch and dinner, but the animals you eat do. Cultivation of the staple crop takes up an area five times the size of the UK, and 85% of that area is used for animal feed. Thanks to projected rapid growth in both world population and in the meat-eating global middle class, demand for soybean is set to grow 80% by 2050 – more than any other staple crop. With arable land at a premium, our desire for animal products is already responsible for the deforestation of vast swathes of the Amazon and other rainforests. This massive increase in demand is likely to lead to a whole lot more destruction, at precisely the time when we need to be curbing what is the second biggest cause of global warming. But this destruction is not yet a certainty. I recently travelled to Iceland to investigate a cutting-edge commercial technology that soups up photosynthesis. It could help save the bio-diverse, CO₂-sucking ecosystems that are so vital to the health of our planet. Light, carbon dioxide, and water are what give plants life. Through photosynthesis, plants convert these three ingredients into the vital carbohydrates needed to flourish and blossom. But conventional agriculture has surprisingly little control over these factors. It’s dependent on the sun to shine, and while irrigation has substantially improved crop yield, water scarcity is often an issue for farmers. This novel method, trialled in Iceland’s Hellisheidi geothermal park, swaps sunlight with LED light, fresh water with saltier “brackish” water, and ambient air with concentrated carbon dioxide, controlling their concentrations in innovative modules called photo-bioreactors. Think of them as nuclear reactors, except with concentrated CO2 and light as the inputs and organic material as the output. These photo-bioreactors are designed to grow not soybeans, but plant microorganisms. In tubes of different shapes and sizes, fluids rich with micro-algae are stirred carefully, and exposed to light, water, and CO₂. Using the same logic as systems designed by NASA for space travel, they recycle carbon, phosphorus and nitrogen. Compared to conventional agriculture, these closed-loop modules allow for much greater control and measurement of fertilisers and water, use CO₂ more efficiently, are at lower risk of crop loss from contamination, pests, and storms. Most importantly, they maximise the efficiency of the key ingredient in photosynthesis: light. By keeping the microalgae fluid constantly moving and closely regulating temperature and harvest timing, these microorganisms are exposed to the maximum healthy amount of light, shedding the natural constraints of the day-night cycle and the weather. Using this technique, photo-bioreactors can provide similar nutritional content to soybeans at less than 0.6% of the land and water use. A production unit uses 130m² to grow 10,500kg of biomass per year – a 200-fold improvement in resource efficiency. The reactors have a minimal ecological footprint. Iceland’s reactors are powered geothermally, and can be paired with any form of renewable electricity. After the carbon costs of production, they are net absorbers of CO₂. They eliminate the need for pesticides and herbicides. They can be placed on unproductive lands, and can be stacked vertically like LEGO bricks. The modular design could even be deployed in city centres. Crucially, the technology is cost-effective. Thanks mainly to the commercialisation of cannabis, LED technology is now much cheaper and more efficient than before, and other recent engineering innovations have further reduced costs. If the monetary costs of the environmental and social harm caused by soy cultivation are taken into account, micro-algae now represent much better value for money – albeit with a higher level of initial investment required for producers. While the shift from conventional agriculture to technical skills would require a short period of intensive training, for both farmers and states this cost would be far outweighed by greater profits and ease of production. Further trials are needed to prove that a fully microalgae-based diet is not detrimental to animal health in the long-term, but research suggests that they have the potential to feed chicks, hens, pigs, and cows. Photo-bioreactors could already be used to grow microalgae strains that are suitable for human consumption too, such as popular health food spirulina.  The livestock economy, like many other industries, tends to be resistant to change.
But these alternative food systems are now attainable, and if backed by soy-dependent governments, the technology could save millions of hectares of rainforest, and provide space for the rewilding of already deforested areas. As the pressure on countries to cut emissions hots up, such a switch is likely to become increasingly attractive. It could also free up valuable land and water resources to feed a population that is expected to increase by half in the next 80 years.  With more extreme patterns of deluge, drought, and crop failure expected as the planet warms photo-bioreactors like these could avert famine for millions. As with many of the planet’s existential problems, the solutions are out there. We just have to implement them. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"Having been traumatised by the Canberra firestorm of 2003, and then impacted by this summer’s bushfire crisis which culminated for me at Malua Bay on New Year’s Eve, I am left with a feeling of discombobulation. We saved our house by deciding to stay and defend against all warnings to evacuate, yet I am still afraid of further bushfires. I don’t feel normal any more. I feel life has changed into a Mad Max movie that could reappear any moment with loss of power and communications. Columbia Law School professor Jedediah Purdy wrote a remarkable 2016 essay about getting back to nature as a way of navigating politics, particularly in times of fear and collapse. “I have been hungry for naive responses to nature, as I have been for naive political lucidity,” he wrote. “These days, when I see a flock of birds in synchrony, I feel as if a dimension of awareness has opened that is not occupied territory. I feel this other site of consciousness, this fast-banking incipient intelligence, is a rip in the curtain drawn between the world and me.” This past summer cost Australia something as a country. I think part of what happened is that when we paid attention to what was going on in nature we didn’t find something that stood outside the occupied territory of contemporary politics. Instead we saw something that seemed to be a roaring, incandescent expression of it. The sheer senselessness of the damage wrought by a literally mindless force. The feeling of watching a system in combat with itself, individual parts facing catastrophe from a terror somehow produced by the whole. The need to pantomime normalcy around life-threatening warnings, the cognitive inability to stand in mourning for the parts that have already burned while someone still has to buy milk. It was biblical in its literalness: what we cherish is being incinerated. You’re right to not feel normal any more. Things aren’t normal. In the audience Q&A of an event I did last year, a therapist raised his hand. “Clients are coming to me asking how to process their climate grief. What can I possibly tell them when their despair is so obviously warranted?” I’ll say to you what I said to him: that when we talk about climate grief we talk as though it’s already dead. It’s not. But it is dying, and you know what to do against the dying of the light. Audre Lorde taught us “everything can be used”. Your despair can be used. Find an organised and energised group that vigorously defends the causes you care about and pick up the phone with them. Bother legislators. Talk to your friends in agitation; do not let their despair stay inert. Help them turn it into fuel. Ask “who benefits from this?” when you want to curl inwards in a ball with a wet blanket over your head. I want more than anything to promise you that if you do this, and I do this, we can turn things around. But I can’t make that promise. There will be more fires, they might very well be worse. The sense of doom in politics and the sense of doom in the environment are almost certainly not unrelated phenomena and it may turn out that we simply cannot fight both. But at least if both these spheres collapse you’ll be able to tell your children – and yourself – you fought to stop what matters from burning. • Question has been edited for clarity ************************************* Do you have a conflict, crossroads or dilemma you need help with? Eleanor Gordon-Smith will help you think through life’s questions and puzzles, big and small. Questions can be anonymous. If you’re having trouble using the form, click here. Read terms of service here."
"
Guest Post by Ira Glickstein
The December 2010 issue of the Atlantic shows an amazing turn-around by some of the Global Warming warmists! Yes, they are still tuned in to the CAGW crowd predicting imminent climate change disaster, but … BUT, some have reversed themselves on their previous ‘ol devil coal! Turns out we need coal to generate Watts of electricity for our electric cars and, they say, we can do it in a way that is environmentally correct.
The cover story, by respected author James Fallows, is titled Why the Future of Clean Energy is Dirty Coal. {Click the link to read it free online.}
Recall that, only last year, a leading alarmist, NASA’s James Hansen, one of the key science advisors on Al Gore’s The Inconvenient Truth movie, wrote:


“..coal is the single greatest threat to civilization and all life on our planet. … The dirtiest trick that governments play on their citizens is the pretense that they are working on ‘clean coal’… The trains carrying coal to power plants are death trains. Coal-fired power plants are factories of death.” 
 
Fallows writes: 
“To environmentalists, ‘clean coal’ is an insulting oxymoron. But for now, the only way to meet the world’s energy needs, and to arrest climate change before it produces irreversible cataclysm, is to use coal—dirty, sooty, toxic coal— …” 
 
Amazingly, while atmospheric CO2 is still the bogeyman of what alarmists say is an imminent Global Warming disaster, coal, which is nearly all carbon and generates CO2 when burned as intended, is part of the solution! Fallows writes:
Before James Watt invented the steam engine in the late 1700s—that is, before human societies had much incentive to burn coal and later oil in large quantities—the concentration of carbon dioxide in the atmosphere was around 280 parts per million, or ppm … By 1900, as Europe and North America were industrializing, it had reached about 300 ppm.
Now the carbon-dioxide concentration is at or above 390 ppm, which is probably the highest level in many millions of years. “We know that the last time CO2 was sustained at this level, much of the Greenland and West Antarctic ice sheets were not there,” Michael Mann, a climate scientist at Penn State, told me. Because of the 37 billion annual tons of carbon-dioxide emissions, the atmospheric carbon-dioxide level continues to go up by about two ppm a year. For perspective: by the time today’s sixth-graders finish high school, the world carbon-dioxide level will probably have passed 400 ppm, and by the time most of them are starting families, it will have entered the 420s. …
Michael Mann told me. “What we have with rising CO2 levels in general is a dramatically increasing probability of serious and deleterious change in our climate.” He went down the list: more frequent, severe, and sustained heat waves, like those that affected Russia and the United States this summer; more frequent and destructive hurricanes and floods; more frequent droughts, like the “thousand-year drought” that has devastated Australian agriculture; and altered patterns of the El Niño phenomenon, which will change rainfall patterns in the Americas. …
You should recognize Michael Mann as the creator of the deceptive “hockey stick curve” at the center of many of the Climategate emails. (See this and this and this and this.) Note also the standard line that, whatever happens to the weather: hotter, colder, dryer, wetter, stormy, calm, sunny, cloudy, … whatever, it is all due to high CO2 levels (even if they don’t plow your streets after a blizzard :^)
So, what is the solution? Fallows writes:
Isn’t “clean energy” the answer? Of course—because everything is the answer. The people I spoke with and reports I read differed in emphasis, sometimes significantly. Some urged greater stress on efficiency and conservation; some, a faster move toward nuclear power or natural gas; some, an all-out push for solar power and other renewable sources …
Note the mention of nuclear, also a bogeyman of the green crowd until a few years ago. In this regard much of the world is ahead of us. When I bicycled in France a few years ago, you could see nuclear power plant cooling towers in much of the countryside (except near Paris – I guess that is where the professional environmentalists live) and France generates most of its electricity using nuclear energy. It will take the US quite a while to catch up, but it is good to see a mainstream liberal literary magazine starting to lead the way. The above paragraph also mentions natural gas, a fossil fuel, ahead of “solar power and other renewable sources” stuck in at the end. It seems they finally realize that we need energy and, at least for the next decades, it will continue to be coal, burned in a cleaner way, plus nuclear and natural gas.
Fallows continues:
“Emotionally, we would all like to think that wind, solar, and conservation will solve the problem for us,” David Mohler of Duke Energy told me. “Nothing will change, our comfort and convenience will be the same, and we can avoid that nasty coal. Unfortunately, the math doesn’t work that way.”…
Coal will be with us because it is abundant: any projected “peak coal” stage would come many decades after the world reaches “peak oil.” It will be with us because of where it’s located: the top four coal-reserve countries are the United States, Russia, China, and India, which together have about 40 percent of the world’s population and more than 60 percent of its coal. …
“I know this is a theological issue for some people,” Julio Friedmann of Lawrence Livermore said. “Solar and wind power are going to be important, but it is really hard to get them beyond 10 percent of total power supply.” …
What would progress on coal entail? The proposals are variations on two approaches: ways to capture carbon dioxide before it can escape into the air and ways to reduce the carbon dioxide that coal produces when burned. In “post-combustion” systems, the coal is burned normally, but then chemical or physical processes separate carbon dioxide from the plume of hot flue gas that comes out of the smokestack. Once “captured” as a relatively pure stream of carbon dioxide, this part of the exhaust is pressurized into liquid form and then sold or stored. …
“Pre-combustion” systems are fundamentally more efficient. In them, the coal is treated chemically to produce a flammable gas with lower carbon content than untreated coal. This means less carbon dioxide going up the smokestack to be separated and stored.
Either way, pre- or post-, the final step in dealing with carbon is “sequestration”—doing something with the carbon dioxide that has been isolated at such cost and effort, so it doesn’t just escape into the air. … All larger-scale, longer-term proposals for storing carbon involve injecting it deep underground, into porous rock that will trap it indefinitely. In the right geological circumstances, the captured carbon dioxide can even be used for “enhanced oil recovery,” forcing oil out of the porous rock into which it is introduced and up into wells.
According to Fallows, China is in the lead on this clean coal technology, with help from American and other western corporations. While it is good that at least some of the Global Warming alarmists are warming up to coal as a necessary part of the solution, it would be better IMHO, if they were also more realistic about the actual dangers of climate change and the likelihood (again IMHO) that most of the warming of the past century is due to natural cycles not under human control and that we are likely already in a multi-decade period of stable temperatures, and perhaps a bit of cooling.
Yes, I think we need to do something about the unprecedented steady rise in CO2 levels, but we have to do it is a way that will not destroy our economies or force us to drastically reduce our lifestyles. One thing I agree with James Hansen about is that an across-the-board carbon tax, assessed equally against all sequestered fuels (coal, oil, natural gas) and collected at the mine, well, or port, is the best solution, far more suitable to the task than the “cap and trade” political scam, and more likely to work.
Rather than have governments pick winners (and mess up as they did with corn ethanol subsidies that raised food prices and reduced gas mileage without doing much to control CO2 emissions) I prefer to tax carbon progressively a bit more each year and let industry and other users decide for themselves how to adapt to the higher prices. Nothing stimulates action and invention like saving your own money. Nothing wastes money like government taking money from “Mr. A” and giving it to “Mr. B” for the “good of society”.
I’m working on a future posting that will propose use of gassified coal along with enhanced CO2 farming as a clean coal implementation that may make sense in a decade or so. I hope to post it next week.
***************************
Another story in the same issue of the Atlantic is about famed physicist Freeman Dyson and The Danger of Cosmic Genius.{Click the link to read it free online.}
They write:
In the range of his genius, Freeman Dyson is heir to Einstein—a visionary who has reshaped thinking in fields from math to astrophysics to medicine, and who has conceived nuclear-propelled spaceships designed to transport human colonists to distant planets. And yet on the matter of global warming he is, as an outspoken skeptic, dead wrong: wrong on the facts, wrong on the science. How could someone as smart as Dyson be so dumb about the environment?
Does it occur to them that the CAGW warmists and alarmists may be the ones who are wrong?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e85ba2566',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The British government has taken its first small steps towards tackling carbon emissions from the most polluting sectors of the economy by ploughing billions into decarbonising heavy industry, transport and heating. The Treasury set out a multibillion-pound plan to support electric vehicles, green home heating networks and carbon capture technology – while cracking down on plastic packaging too.  The Committee on Climate Change (CCC), the government’s official climate advisers, described the Treasury’s package of green measures as a “realistic start” but said it did not “close the climate policy gap” enough to meet the UK’s climate targets. The Treasury’s green pledges include: £1bn on green transport. An £800m fund to develop at least two carbon capture projects. A £270m green heat network fund. A £100m grant for homes to adopt low-carbon heating. A rising levy on gas use through the Green Gas Levy and the Climate Change Levy. A freeze on fuel duty, and scrapping tax relief on red diesel by 2022. A £200-a-tonne tax on plastic packaging with less than 30% recycled content for companies that use more than 10 tonnes a year. One of the Treasury’s most ambitious pledges was its first major investment in carbon capture technology in the last five years. It plans to spend at least £800m to decarbonise at least two heavy-industry “carbon clusters”, the first in 2025 and the second by 2030. These schemes could keep millions of tonnes of carbon from contributing to global heating and create 6,000 highly skilled green jobs in industrial areas such as Teesside, Merseyside, the Humber and St Fergus in Scotland. The funding is less than the £1bn demonstration fund axed by the Conservative government in 2015, and falls short of calls from MPs and the CCC to roll out multiple carbon capture projects by 2025. The government also plans to spend £1bn on green transport, including more than £400m to extend grants for new electric vehicles and £500m to support the rollout of new rapid charging hubs so that drivers are never more than 30 miles away from being able to charge up their car. It has also maintained a long-held freeze on fuel duty for fossil fuel vehicles. Jayne Harrold, a leader on environmental tax at PricewaterhouseCoopers, said the Treasury’s green measures offered mixed messages before the UN climate talks in Glasgow in November. “On the one hand we have the freezing of fuel duty and the building of 4,000 miles of new roads and on the other we have incentives for cleaner forms of transport, minor shifts to energy taxes, and significant investment in carbon capture and storage,” she said. “Let’s hope the autumn strategic review will bring a much more coherent, bigger and bolder approach that really gets it done.” Heating remains a major source of carbon and the government plans to open a grant scheme from April 2022 to help households and small businesses invest in heat pumps and biomass boilers, backed by £100m of exchequer funding. It will also invest a further £270m in a new green heat networks scheme which the Association for Decentralised Energy estimates could help connect 4.25m homes to heat networks by 2050, from 500,000 today. The government also confirmed plans to introduce a plastic packaging tax under which producers and importers will pay £200 a tonne on wrappings with less than 30% recycled content from April 2022. Green campaigners welcomed the tax but questioned why the government was waiting another two years to introduce it."
"Natural climate solutions let nature do the hard work in the fight against climate change by restoring habitats such as forests and wetlands. This could absorb carbon dioxide from the atmosphere and help biodiversity thrive. Stephen Woroniecki – a PhD Researcher in Climate Change Adaptation from Lund University in Sweden – discusses how this approach could address the ecological crisis with Guardian columnist and environmental campaigner George Monbiot. Q: What has inspired you about natural solutions to climate change and what are their chief advantages over other approaches? They bring together our two crucial tasks: preventing climate breakdown and preventing ecological breakdown. They are all things we should be doing anyway, to limit the scale of the sixth great extinction and protect and restore threatened ecosystems. In these fields, as in all others, we have often tended to act in isolation, replicating effort, failing to recognise the synergies. Natural climate solutions show how we can use the self-regulating power of the living world to help fend off climate catastrophe. I should emphasise that even if we use natural climate solutions to the max, we still need to halt almost all greenhouse gas emissions and leave fossil fuels in the ground, if we are to prevent more than 1.5℃ (or even 2℃) of global heating. But it’s now clear that mitigation alone is not enough: we need to draw down carbon that we have already emitted from the atmosphere. The other main strategies for carbon drawdown are both, in my view, disastrous. The first is bioenergy with carbon capture and storage (BECCS). This means growing biomass in plantations, burning it in power stations to produce electricity, capturing carbon dioxide from the exhaust gases and burying it in geological formations. Any deployment of BECCS sufficient to cause significant carbon abatement will also cause either humanitarian or ecological disaster, because of the vast amount of land – cropland or wild land – the plantations will replace. It is also likely to be self-defeating, due to the massive carbon pulse that conversion of forest lands to plantations will cause, and the vast amount of extra nitrogen fertiliser required, with its associated greenhouse gas emissions. The second is direct air capture. Not only is this likely to be extremely expensive, but the carbon-heavy infrastructure it requires, reliant on a huge deployment of steel and concrete, could help push us past crucial climate tipping points before its positive impacts were felt. These are both bad ways of addressing the problem. Why deploy them when there’s a much better one? Q: Clearly this is an emerging field, and research is needed to understand how best to implement natural climate solutions. What are some of the boldest and most exciting examples that have already been tried across the world that we can learn from and be inspired by? At the moment, the two biggest identified carbon sinks are forests and peatlands, but one of the things that excites me most about this field is how little we yet know. Every year, major new possibilities are identified, in ecosystems that hadn’t been fully considered before. For example, we now know that vegetated coastal habitats – such as mangroves, saltmarsh and seagrass beds – can accumulate carbon 40 times as quickly per hectare as tropical forests can, because of the way they catch and bury organic sediments in waterlogged conditions. One issue that has scarcely been explored at all is the carbon storage impact of stopping trawling and dredging. The seabed is a vast carbon store, but these activities, that scour over three quarters of shelf seas every year, kick carbon into the water column, where it can be oxidised and released. We don’t yet know for sure, as so little research has been done, but it could be that severely curtailing these destructive activities, which we should do anyway, as they are by far the greatest cause of ecological damage to marine habitats, could result in massively greater carbon storage. I should mention two key principles. First, that this isn’t just about creating new or renewed ecosystems. We also need to protect the Earth’s existing carbon repositories – such as old growth forests – whose sequestration capacity would take centuries to reproduce. Second, that fertile cropland should not be used. Mass rewilding of the kind I propose should take place only on less productive land. Unlike BECCS plantations, natural ecosystems can thrive on infertile land, without extra fertilisation. Q: The proposal for a Green New Deal in the US has called for a green transition of society and the economy through investment in renewable energy and by phasing out fossil fuels. How do you see the role of natural climate solutions within a broader transformation of our society and the world we live in? I think natural climate solutions now need to be urgently deployed by all governments, alongside extremely rapid reduction in energy consumption and substitution of fossil fuels. To avoid full-spectrum climate breakdown, we need a global cooperative effort on a scale that has not yet materialised. My hope is that the new, uncompromising mood among young people, and the brilliant protest movements, such as the Youth Strike4Climate and Extinction Rebellion, will help to make this happen. Q: Geoengineering proposals are often criticised for taking risks with natural systems that could have catastrophic consequences, often with little to no consultation from the people who could be most affected. How do we ensure natural solutions are carried out democratically and without echoing the technocratic arguments of many geoengineering projects? Whatever we do has to be done with and through the people it might affect, under the “nothing about us without us” principle. Natural climate solutions must work with the free, prior and informed consent of indigenous people and other local communities, and their benefits must flow to these communities. No project should be pursued that undermines their land rights, economic security and well-being. On the contrary, all projects should seek to strengthen them. There are some excellent examples of how this can be done around the world, compiled by the Equator Initiative. Q: Restoring natural habitats can sometimes mean giving authority to external experts at the expense of local people. What do you think is important to bear in mind when making the case for natural solutions to local communities? I believe all projects should be guided by the Freirean approach – developed by the Brazilian philosopher Paolo Freire – of mutual education and understanding. An outsider should not turn up with the attitude that she has come to impart her superior knowledge to local people. She starts by asking them to teach her about themselves, their lives and needs, and to exchange knowledge, in the hope that all become both educators and educated. The outsider might bring new ideas and perspectives – that are, I believe, essential – while local people bring intimate insights into and knowledge of the peculiarities of place and community, that are also essential. Q: How can people get involved in designing, implementing and managing natural solutions to climate change? We list on our website the organisations already involved in the field, some of whom would welcome your help. But the most important thing right now is to spread the word as far as you can. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterBy Ed Caryl
The sun supplies about 1360 watts per square meter to the Earth as seen by satellites at the top of the atmosphere. This power varies by about 1.3 watts over the 11-year solar cycle. Energy is reflected back to space by clouds and the earth’s surface. Some energy is radiated to space as infrared. Energy is radiated from the atmosphere to the surface, the so-called greenhouse effect. There is variation in all the factors that make up the radiation balance. None of the factors are fixed. CO2 may be increasing, but other factors are also changing. The albedo of the Earth varies with the amount of cloud cover, seasonal vegetation, ice, and snow. Of course the heat seen by any part of the Earth depends on the sun angle; the maximum is when the sun is directly overhead, almost none when the sun is on the horizon, none at all when the sun is below the horizon. The Earth loses heat to space by radiation, but this also varies with time of day, cloud cover, temperature, and humidity.
Water vapor accounts for 60 to 95% of the greenhouse effect. The remainder is due to CO2 and other trace gases. According to the IPCC the theoretical extra absorption by CO2 in the atmosphere if it doubled from the current amount is 4 watts per square meter. This extra absorption will be modified by clouds, cloud height, humidity, and other factors. According to the same source, the extra absorption at current levels of CO2 above the historical level is about 1 Watt. This additional energy is offset and balanced by the other outgoing factors. For an excellent explanation of the greenhouse effect see: http://www.ucar.edu/learn/1_3_1.htm.
Figure 1 shows how solar energy input is balanced by the radiated and reflected output. The 342W per square meter solar input figure is an average over the Earth’s surface that supposedly takes into account sun angle and the night and day cycle. This graphic is from Wikipedia and is seen in many publications in two versions with slightly different numbers. It originates with Kevin Trenberth et al.
One of the criticisms of this graphic is that the 342 W/m2 Back Radiation is assumed to be completely absorbed by the surface with no reflection. This would require that the surface be a perfect black-body at all wavelengths. This is obviously not the case. Another criticism is that it inadequately describes the difference in radiation conditions between night and day, and the variable influence of clouds.

Figure 1: Earth’s radiation balance according to Kevin Trenberth et al.
A third criticism is that it does not take into account the Earth’s albedo changes over time. Albedo is the percent of solar radiation that is reflected back to space by the Earth’s surface and clouds. The Earth’s albedo is measured by satellite or by looking at earthshine on the moon, the light from Earth reflected back from the lunar dark side. Here are Earth’s albedo changes from 1984 to 2004.

Figure 2, Earth’s albedo:  The red vertical bar represents the total forcing of all the greenhouse gases added to the atmosphere in the last 100 years. Using the IPCC figures, they add up to about 2.8 W/m2, about the same as the albedo variation. Clearly the albedo variability is the same as the greenhouse gas forcing, especially over any short term. The albedo change is primarily due to cloud and ice changes. Data source: Pallè et al here.
Albedo isn’t the only variable in the radiation balance graphic. The opposite of albedo (reflectance) is transmittance, the percentage of solar radiation that reaches the surface. This is also a variable, changing with the amount and kind of aerosols in the atmosphere. Since 1958 this has been measured at Mauna Loa Observatory in Hawaii, the same place that CO2 is measured. Their data is stated in percentage and is taken under the following conditions.
The ‘apparent’ transmission, or transmission ratio (Ellis & Pueschel, Science, 1971), is derived from broadband (0.3 to 2.8um) direct solar irradiance observations at the Mauna Loa Observatory (19.533 ° N, 155.578 ° W, elev. 3.4 km) in Hawaii. Data are for clear-sky mornings between solar elevations of 11.3 and 30 degrees.”
In the chart below the data has been converted to energy anomaly, based on the solar irradiance at the top of the atmosphere, 342 W/m2, in Figure 1 above.

Figure 3: Atmospheric transmission anomaly in terms of energy in Watts/m2.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




You can see that the transmittance anomaly is quite large, even disregarding very large perturbations such as the Agung, El Chichón, and Pinatubo volcanoes. Pinatubo is also visible in the albedo data. All traces are annual averages.

Figure 4: Annual albedo and transmittance in W/m2, and global annual satellite temperature anomaly plotted together.
Because transmittance and albedo are inversely related, in the chart above, albedo (the red trace) has been inverted so that transmittance, albedo, and temperature can be related. The temperature anomaly is scaled on the right axis. The two volcanic eruptions and the last two El Niño’s can be clearly seen in the temperature. The dip in transmittance in 1963 is the Agung volcano. All three volcanic eruptions were sulfur-rich and pushed gasses and dust into the stratosphere.
What about greenhouse gases?

Figure 5: Greenhouse gas forcing since 1978.The scale is also in W/m2. Source.
Figure 5 uses the IPCC figures for greenhouse gas forcing, 4°C for CO2 doubling, which may overstate the numbers. CO2 and methane (CH4) dominate the total. But let us plot the total greenhouse gas forcing (the top trace in Figure 5) on the Figure 4 graph.

Figure 6: Albedo, greenhouse gases, transmittance, and TSI anomalies plotted together with the Global Satellite Temperature anomaly. The transmittance linear trend line is also included. The vertical scale has been changed, truncating the large negative transmittance spikes, so that the smaller forcings can be seen.
The TSI anomaly has been added to Figure 6 along with the greenhouse gas trace. Note that the transmittance trend is in the opposite direction of the greenhouse gases, and is of similar magnitude.
It is clear that aerosols, albedo, and ocean cycles are very largely what drive global temperature in the short term. Carbon dioxide is a minor player. TSI (total solar irradiance) plays very little direct role on this time scale. If the sun has an effect it must be via some other mechanisms.
Because we know so little with certainty about water vapor, it has been left out of this analysis. Water vapor is a very important greenhouse gas, by some accounts resulting in 60 to 95% of the greenhouse effect. The problem is that because water exists in all three phases in the atmosphere, and convection, evaporation, condensation, and precipitation produces wildly variable amounts at different altitudes, times, and locations, calculating the resulting forcing is very problematic. In general, water vapor tracks temperature. In the lower troposphere, this has meant that water vapor is increasing along with temperature. Some have stated that this results in positive feedback further increasing temperature. But in the stratosphere, the temperature has been decreasing, and so has the water vapor content. (See discussions here and here.) This allows increased radiation to space. As a result, others have argued that this results in negative feedback. It may well be that the two variabilities cancel.
Among the variables that control climate change, atmospheric transmission is by far the largest factor. Even in the absence of volcanic activity, upper atmosphere aerosols vary on a scale that dwarfs the other factors. Ocean cycles are next in importance. Third in importance is albedo. The reflection of clouds, ice, smoke, land and sea surfaces are continuously changing. Fourth on the list are the greenhouse gases excluding water vapor. The big unknown variable is water vapor. We know it is varying. To some extent we even know how much. What we don’t know is what forcing water vapor provides. We don’t even know where water vapor ranks on this list.
 
Share this...FacebookTwitter "
"Independent MP Zali Steggall is launching a national advertising campaign drawing on the summer bushfire crisis to rally public support for her private member’s bill on climate change, due to be introduced to parliament later this month. The campaign will kick off on Monday and include digital billboards, and bus shelter and telephone booth advertisements across Sydney, Melbourne, Brisbane and Adelaide.  The ads will feature images of the destruction wreaked by the summer bushfire season, with one pointing to the experience of “one hell of a summer” and another featuring a koala in burnt bushland. The ads call for the public to support Steggall’s climate act, making the appeal that “what happens next is up to you”. The campaign is being funded by community climate change groups, donors of the Climate Act Now campaign and pro-bono assistance, with more than 120 “Climate Act Now” advertisements set to appear across the country this month. Steggall is hoping to garner bipartisan support for a national climate change framework bill aimed at transitioning Australia to a decarbonised economy that is based on the UK’s Climate Change Act, passed in 2008. The bill would put in place national plans for climate change adaption and enshrine a net-zero emissions target by 2050 to be reviewed every five years by a newly established independent climate change commission. Steggall has been arguing that the framework legislation would take the politics out of the climate change policy debate, while putting in place an effective process for national targets, actions and reporting. She has been lobbying both Labor and government MPs to allow a conscience vote on the legislation, and has already locked in the support of the crossbench in the lower house. On Monday, four digital billboards will be in place in Melbourne, Brisbane and Adelaide, followed by a rotating poster advertisement campaign across the Sydney CBD and North Sydney, and in Melbourne. Steggall said the campaign was designed to keep the legislation in the public eye, and said there was strong support coming from North Sydney and Wentworth, where moderate Liberal MPs are under pressure to do more on climate change. “After such a horrific summer, this advertising campaign is there to remind Australians that the country needs a sensible approach to climate change,” Steggall said. “We have received enormous support from our neighbouring electorates of Wentworth and North Sydney who want their local federal members to vote for the bill.” So far more than 73,000 Australians have supported the climate change bill through the “Climate Act Now” website. While the bill will be introduced when parliament next sits on 23 March, there is a risk that it will be left to languish on the notice paper without government support. If the government refuses to allow debate on the bill in the House of Representatives after it is introduced, Centre Alliance, which is co-sponsoring the legislation, could introduce it into the Senate later in the year. This would force Labor to declare its hand on the bill, which has so far failed to land a position, saying it will never come to a vote in the lower house. “For Zali Steggall’s bill to be considered, Scott Morrison would need to allow a full debate in the parliament,” the shadow climate change minister, Mark Butler, said last month. As part of the campaign, B-Corp – a group that certifies businesses for making positive decisions on the community and the environment – will support the bill , backed by 16 of B-corp’s 300 registered companies. Community groups in Wentworth and North Sydney have also funded particular advertising within their electorate."
"

U.S. foreign policy is changing. With the selection of CIA Director Mike Pompeo to replace Rex Tillerson as secretary of state, President Donald Trump appears to be taking charge of his foreign policy. National Security Adviser H.R. McMaster and Defense Secretary Jim Mattis remain counsels of caution on many issues, but the former’s tenure could be short, and the latter might choose to exit if his advice has increasingly less effect.



Secretary Tillerson’s departure reflected both personality and substance. He and the president never established personal rapport. Last fall the secretary was reported to have called the president a “moron,” which suggested a relationship irreparably sundered. Moreover, the two disagreed on many, if not most policy issues: negotiation with North Korea, nuclear agreement with Iran, the dispute between Saudi Arabia/​United Arab Emirates and Qatar, Russia, climate change, and free trade.



The outcome is likely to result in several course corrections, mostly in a more confrontational and hawkish direction. China in particular is more likely to become a target of the administration. That already has happened on trade, with Peter Navarro, more nationalist than economist, pushing for a trade war against virtually everyone, ranging from Europe and Mexico to South Korea and the People’s Republic of China. Although Pompeo is no protectionist, unlike Tillerson he probably won’t challenge his boss on the issue. After the administration announced its tariffs on aluminum and steel, Pompeo cited “trade or the theft of intellectual property” as areas where the administration was “pushing back against” the PRC.





Whatever the prospective secretary thinks of China, he must work to make the relationship work.



On both Iran and North Korea, the CIA director disdained diplomacy. Indeed, he appeared to welcome possible regime change in Pyongyang, in contrast to Secretary Tillerson, who denied such an interest in an attempt to assuage North Korean concerns over giving up the regime’s missiles and nuclear weapons.



Secretary‐​designate Pompeo also appears to take a harsher attitude toward the PRC. Of course, Rex Tillerson suggested the possibility of interdicting Chinese ships in the South China Sea during his confirmation hearing. However, he took a more restrained and responsible stance once in office.



Nevertheless, Pompeo starts with a negative view of Beijing. Last year, regarding security threats, he said in one interview: “It’s hard to pick between China, Russia and Iran to be honest with you. I guess if I had to pick one with a nose above the others, I’d probably pick China.” He pointed to the PRC’s economic strength, population, and intellectual property theft. Moreover, he said, “I think it’s very clear when they think about their place in the world, they measure their success in placing themselves in the world where they want to be vis‐​á‐​vis the United States and not as against anyone else.”



He perceives a potentially dangerous rivalry between China and the United States: “It is also the case that the Chinese have moved to a place where they, I think, see themselves as a rival superpower.” Moreover, “They have as part of their mission to reduce the relative power of the United States vis‐​á‐​vis their own country.” Pompeo recently told the BBC that, “We have to do better pushing back against Chinese efforts to covertly influence the world.” He also emphasized the problem of Chinese espionage, obviously reflecting his position as CIA director.



He believes that administration pressure caused Beijing to shift its position on North Korea. Last August he praised the president: “We’ve seen the Chinese now say for among the first times that they believe the correct answer has to be a denuclearized peninsula. And that’s exactly the policy of the Trump administration.” Of course, the PRC has taken that position for years. Beijing has steadily, if sometimes erratically, backed ever‐​tighter economic sanctions against the Democratic People’s Republic of Korea. Administration pressure may have accelerated China’s willingness to act, but so did the speed‐​up of North Korean missile and nuclear testing. Unfortunately, Pompeo’s belief that administration action caused the PRC to cave could influence his future positions.



Shortly before being chosen as secretary of state he declared that “this administration has been very clear of pushing back against the Chinese threat.” More specifically, “If you look at the president’s national security strategy, it was very clear that what the Chinese are doing, whether that would be on trade or the theft of intellectual property or their continued advancement in East and South China Seas, this administration is prepared and engaged in pushing back against the Chinese threats so that we can have a good relationship with China in a way that the world desperately needs.” Alas, Beijing may not perceive these steps as an invitation to have “a good relationship.” CNBC’s Jim Cramer worried that Pompeo’s appointment “says to China you are our enemy.”



Pompeo was not uniformly negative about China. Last fall he praised its efforts regarding North Korea and said that “We think that President Xi will come out of this in a dominant position with incredible capability to do good around the world.” Whether he believes the Chinese president actually will do so is not so clear. Although the incoming secretary’s attitudes toward the PRC sound tough, they are not unusual. If U.S. policy shifts, it is likely to be in degree rather than in kind, as Pompeo reinforces rather than counterbalances the president’s views.



That almost certainly will be true elsewhere, such as Iran. The sanctions waiver comes up for renewal in May.



There is no similar deadline for China-U.S. relations, but there is no more important long‐​term issue. American analysts, pundits, and officials all have legitimate concerns about the direction of policy toward the PRC. However, it is important that Washington avoid treating Beijing as an enemy. Whatever the prospective secretary thinks of China, he must work to make the relationship work. Upon his and the administration’s success will depend peace and stability in East Asia.
"
"When Linda Erskine looked outside her window last week, she saw an intense flare from the Mossmorran petrochemical plant in Fife. The flaring, which she says collapses night into day, can be seen more than 20 miles away in Edinburgh. Erskine, a local Labour councillor, describes living in Lochgelly, a former mining community neighbouring Mossmorran, as unpleasant. “When that flare goes, the house does vibrate. For me it’s something akin to a Nimrod [maritime patrol plane] landing on top of your house. The first time I went out to see if there’s a helicopter flying overhead.”  Residents, who formed a local action group, are calling on the government to set up an independent inquiry into the health, social, and environmental impact of what they describe as an “ageing” plant, but this demand has been ignored. For the local community, the flaring has come to symbolise a disconnect from the Scottish government’s rhetoric about the climate emergency and what it does on the ground. ExxonMobil’s Fife Ethylene Plant at Mossmorran began production in 1985. Flaring, a process that burns off gas that cannot be processed, can last for several days. As well as planned flaring events, unexpected flaring also occurs as a safety mechanism. It’s legal on the site that ExxonMobil shares with Shell Fife NGL, but the company has a duty to mitigate the impact it can have on local communities. Last April, the Scottish environmental agency (Sepa) launched a criminal investigation into ExxonMobil because of unplanned flaring. The investigation follows “final warning letters” issued to ExxonMobil in April 2018 regarding flaring that was found to be “preventable and unacceptable”. “In our bedroom, which faces towards Mossmorran, it’s impossible to keep the light out. It’s a nightmare,” said Joe Purves, a 69-year-old recently retired accountant who has lived in Lochgelly for 45 years. He believes the frequency and severity of the flaring has got worse over the last few years. “The community are told when there’s going to be planned flaring, but it’s the fact the emergency flaring keeps on taking place,” said Linda Holt, an independent councillor at Fife council. “They always say it’s ‘process upset’ but everyone knows it’s because something has gone wrong. And things have gone wrong more in the last few years because the plant is ageing.” Last summer, the plant was shut down to address the mechanical issues the company was having with its boilers and implement preventive work to improve the plant’s reliability. But unexpected elevated flaring occurred within a month of the plant reopening in February, sparking widespread anger. Chris Dailly, Sepa’s head of environmental performance, said the watchdog had made it clear ExxonMobil needed to invest further in flaring mitigation technologies and given the company a timeline for compliance. “We’ve been clear that flaring has been unacceptable and that compliance with Scotland’s environmental rules is non-negotiable,” he said. While 42-year-old James Glen, a graphic and web designer who set up the Mossmorran action group in 2017 with Holt, welcomes Sepa’s investigation, he argues it’s not enough. The action group calls for an independent inquiry that looks at health, site safety, and social issues. More than 2,000 people have written to the Scottish environment secretary, Roseanna Cunningham, echoing the action’s group demand. A spokesperson for the Scottish government said that as Sepa was concluding its investigation, it “would not be appropriate nor helpful for ministers to interfere in independent regulatory decisions, particularly while enforcement investigations are ongoing”. Glen accuses the Scottish government of “hiding behind Sepa” and said it should come to meet the community to discuss their concerns. Erskine said Holyrood is already losing credibility on the issue. “We should be looking after our constituents, not big business. Our job is the safety and wellbeing of the people that live here. If that plant had been any place near Edinburgh, it would have been shut down.” Glen says the community’s concerns about their health had long been dismissed until a 2019 NHS Fife report, which he believes just scratches at the surface, concluded “it is clear that the degree of physical and psychological disturbance caused to people in the vicinity of Mossmorran has been considerable”. The report noted the most common health-related concerns among local residents were anxiety, respiratory issues such as asthma, sleeping difficulties, and headaches. It found no evidence of higher than expected cancer rates, while 32 Sepa’s air quality reports demonstrate no breach of the UK air quality standard. Stuart Neill, the plant’s public affairs manager, said: “We very much regret any concern that flaring may have caused to members of the local community. The safety of our people and neighbours is our most important priority and the ability to flare is a critical part of the plant’s multiple safety mechanisms.” Neill added that the plant is one of the youngest facilities of its kind in Europe and as well as investing £20m annually in maintenance, this year the company would be investing an additional £140m to upgrade infrastructure to improve operational reliability and reduce flaring. Glen set up an impact map so residents can report health, social or environmental issues related to Mossmorran. There have been 363 incidents added so far. One report from a resident in Edinburgh notes: “I genuinely thought tonight that there had been an explosion at the plant given the intensity [of the flaring].” Another writes: “If we see it as far away as North Berwick what must it be like up close?”"
"It was hard, chilly work for nine-year-old Alfie Perry and his eight-year-old classmate Dempsey Owens. But, after a bit of cajoling from their pals and teacher on a windswept hillside above the town of Neath, the pair succeeded in planting a sessile oak that could soon be part of a forest stretching continuously the length and breadth of Wales. The Welsh government is initially ploughing £5m into its national forest scheme, aiming to link existing woodland with new forests, parkland and hedges, creating green corridors for flora and fauna – and people. The Labour-led government says the scheme will help it to create new, important environments, meet carbon reduction targets, combat flooding – but also boost tourism and people’s sense of wellbeing as they criss-cross the country under, or next to, trees. Lesley Griffiths, the Welsh government’s environment minister, was on hand in Neath to help Alfie and Dempsey dig and set out the vision. “We love our woodland in Wales,” Griffiths said. “Welsh businesses, communities and, particularly, our farmers and foresters, will want to help create the national forest.” Griffiths accepted there would be challenges but pointed out that people had doubted the country could create a footpath right around the coast, but it had done just that. “We’re a small country. You’re able to do things differently.” The spot for the launch – Coed Brynau - is a beautiful one. High above Neath, with views of the Brecon Beacons and the Mumbles Lighthouse, Coed Cadw (the Woodland Trust in Wales) is in the process of planting 150,000 trees here by 2025, including rare species such as the small-leaved lime and the black poplar. The estate used to be owned by the coalmine owner and Tory MP Herbert Mackworth (1687-1765), who created a parkland on the hill. There are vestiges of ancient woodland that will be linked with the new tree planting. The area is home to barn owls, Daubenton’s bats and possibly the very rare blue ground beetle. Welsh white cows, which look ghostly in the landscape, are being introduced to graze grassland areas of the site. There is not yet a route for the forest – this will evolve as the consultation moves on. But Natalie Buttriss, director of Coed Cadw, suggested one option could be to link the great forest of Wentwood in Monmouthshire along the M4 corridor to Coed Brynau. It could then head north towards the “Celtic rainforest” of mid Wales and Snowdonia, one of the most important and historic habitats in Wales, and perhaps veer east towards Wrexham, where a plan is in place to increase the urban tree canopy to a minimum of 20% by 2026. Chris Matts, Coed Cadw’s woodland manager for south and west Wales, likes the government’s ambition, though he thinks perhaps forest is the wrong word. “It’s a word that in English originally meant a hunting ground for kings,” he said. Matts prefers the Welsh word: coed, which has more of the connotation of “an area of trees”, which takes in parkland and hedgerows as well as thick forest and woodland. Mary Gagen, a professor of geography at Swansea University, praised the ambition of the scheme. “We need to get trees in the ground now. Trees are a brilliant multi-tasker, good for the environment, good for habitats, good for us.” The Welsh first minister, Mark Drakeford, said the February floods reinforced the need to plant more trees. “In planting, growing and protecting the right network of trees we can increase our resilience to flooding. “Trees improve air quality, they remove harmful greenhouse gases from the atmosphere, they provide material for construction, they regenerate soil for food, they clean the water in our rivers and they provide a home to all the life that finds shelter in their canopy. “The project is ambitious and dynamic and will not grow overnight. Over the next 20 years, we will work with all kinds of stakeholders – from the Woodland Trust to farmers, landowners and school pupils – to grow the right network of trees that will greatly improve biodiversity, trap carbon from the air and provide economic opportunities too. “These ambitious projects work – the Wales Coast Path, which created a path around the entire coastline of Wales, has been a real success story. We’re looking forward to growing a forest that benefits Wales for future generations to come.”"
"
This artist's conception shows the inner four planets of the Gliese 581 system and their host star, a red dwarf star only 20 light-years away from Earth. The large planet in the foreground is the newly discovered GJ 581g, an Earth-size planet that orbits in the star's habitable zone. Artwork by Lynette Cook.
From the University of Hawaiʻi at Mānoa
The planet,  which is probably 30 percent larger than Earth, was discovered using one  of the telescopes of the W. M. Keck Observatory on Mauna Kea. It orbits  a relatively small star, Gliese 581, that is 20 light-years from Earth  in the constellation Libra.

“By determining the  orbit of this planet, we can deduce that its surface temperature is  similar to that of Earth,” said Haghighipour. This means that at least  some of any water on the surface of the planet and in its atmosphere  will be in liquid form rather than ice or vapor. The discovery of   liquid water in space is an important step in the search for  extraterrestrial life.
The team estimates that  the new planet, called Gliese 581g, has a mass three to four times that  of Earth, and orbits its star in just under 37 Earth days. Its mass  indicates that it is probably a rocky planet with enough gravity to hold  on to its atmosphere. It is one of six known planets orbiting the  star.
To discover the planet, the team looked  for the tiny changes in the star’s velocity that arise from the  gravitational tugs of its planets. They used 238 separate observations  of Gliese 581 taken over a period of 11 years.
Haghighipour  said that the team is keeping tabs on many nearby stars using the Keck  Observatory. “As we collect more and more data about how these stars are  moving, we expect to find many more planets with potentially Earth-like  conditions,” he said. He noted that to learn more about the conditions  on these planets would take even bigger telescopes, such at the Thirty  Meter Telescope planned for Mauna Kea.
The team  that made the discovery is led by Steven Vogt of the University of  California, Santa Cruz (UCSC) and Paul Butler of the Carnegie  Institution of Washington. Other team members include UCSC associate  research scientist Eugenio Rivera, and Gregory Henry and Michael  Williamson of Tennessee State University.
This research was supported by grants from the National Science Foundation, NASA, and the NASA Astrobiology Institute.
###
For a related press release, see http://news.ucsc.edu/2010/09/planet.html.
For more information, visit: http://www.ifa.hawaii.edu/info/press-releases/Gliese581g/
Contact:Dr. Nader Haghighipour, (808) 956-6098
Associate Astronomer, Institute for Astronomy
Louise Good, (808) 956-9403
Publications Editor, Institute for Astronomy
Posted: Sep. 29, 2010


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88863cb4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter!!! UPDATE: Story now (indirectly) on the Drudge Report !!! (31 May 2012, 20.00 h CET)
Not only does he not feel bad about the tab taxpayers have to pick up for his haircuts, President Obama obviously also does not care much about the massive carbon footprint his haircuts are leaving on our planet which, we are told, is tipping precariously.
With each passing day, Barack Obama is looking more and more like the Imelda Marcos President.
German public radio here has a gossip report on how the world’s most likable Commander-in-chief gets his haircut, and about the cool dude who trims his hair. The President is a man of the common people, the report tells its listeners. He hangs out with regular folks, like his barber for example. Obama’s Chicago barber, who goes by the name Zariff, has been cutting the Chief Executive’s hair 17 years.
But now that the President has his residence in Washington D.C., you’d think he’s find another barber in town to cut his hair and save us lots of costs. Wrong.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to German public radio, the President flies Zariff from Chicago to DC for a trim dozens of times per year (on the taxpayers’ dime of course). Not only does this cost the taxpayers a bundle, but think about the carbon footprint the supposedly environmentally-concerned President is producing. The German NDR reporter says:
The barber never says a word about what they discuss. Perhaps that’s also a reason why Obama feels comfortable being around him. Otherwise he would not fly his barber in from Chicago to Washington every ten to 14 days.”
Think of all the jet fuel getting burned for them clippers.
In these tough times when Americans by the score are struggling with high unemployment, tight budgets, tattered finances and deep worries about the future, wouldn’t it be more appropriate for the President to get a barber from DC? My God -surely there’s got to be somebody in town who can cut his hair.
With all due respect, Mr. President, such small things make all the difference between a great democracy and a banana republic.
 
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterIt’s a definite bang, but nothing like I was expecting. Let’s just say it was a little bang out here in Europe.
Anthony Watts shut down his popular WUWT website Friday because “something happened” and he needed time to prepare a “major announcement” that he was sure would “attract a broad global interest due to its controversial and unprecedented nature”. Because of “the magnitude of this event” he was even able to convince his wife that it was necessary to suspend vacation plans. Now that sounds big!
The press release excerpt:
A reanalysis of U.S. surface station temperatures has been performed using the recently WMO-approved Siting Classification System devised by METEO-France’s Michel Leroy. The new analysis demonstrates that reported 1979-2008 U.S. temperature trends are spuriously doubled, with 92% of that over-estimation resulting from erroneous NOAA adjustments of well-sited stations upward.
Other findings:
– Poorly sited station trends are adjusted sharply upward, and well sited stations are adjusted upward to match the already-adjusted poor stations.
· Well sited rural stations show a warming nearly three times greater after NOAA adjustment is applied.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




· Urban sites warm more rapidly than semi-urban sites, which in turn warm more rapidly than rural sites.”
Admittedly, my expectations concerning the implications were surely too high. It’s nothing that we haven’t already suspected or known. But it is good that this issue has been examined scientifically, formally and published. It’s now certified that US temps have been massively overstated and that the NOAA needs to clean up its act.
In summary:
1. Major announcement? Yes, but nothing like it was made out to be.
2. It’s of global interest? Some, but in Europe it’ll be pretty much ignored – local issue that does not change anything globally.
3. Controversial? For temperature record keeping at the NOAA, yes. But little global impact.
4. Unprecedented? Not really. It confirms what everyone expected. Temperature data are botched everywhere, every day.
This paper will put the NOAA on the defensive of course, at least for a little while. I don’t expect any of the German mainstream media to report on this, though. But I’ll keep my eyes peeled.
The announcement is big, but not that big. Some things could have been spared in the run up to the announcement.
And so the great global warming debate continues…
 
Share this...FacebookTwitter "
"

World leaders have sounded alarm bells against a repeat of the 1930s, when tit‐​for‐​tat protectionism followed hard on the heels of the Wall Street crash. But they are fighting the wrong enemy. Current events suggest a different, but still vexing, scenario: the creeping protectionism of the 1970s, rather than the spiraling protectionism of the 1930s.



In the 1970s, oil‐​price hikes and other shocks triggered inward‐​looking, mercantilist policies, including in Europe and the United States. Immediate policy responses were not overly protectionist: There was no equivalent of America’s 1930 Smoot‐​Hawley tariff. But escalating domestic interventions on both sides of the Atlantic exacerbated economic stress and prolonged stagnation. Not least, they spawned protectionist pressures. Industry after industry, coddled by government subsidies at home, sought protection from foreign competition. The result was the “new protectionism” of the 1970s and 1980s.



Then, as now, manufacturers of gas‐​guzzling cars faced bankruptcy. Congress bailed out Chrysler in 1979. The British government bailed out Rolls Royce and British Leyland, and Renault was saved by French taxpayers. Several other sectors — timber, energy, minerals, railways, airlines, shipbuilding — received government subsidies in the 1970s, in both Europe and the U.S. Many companies were nationalized.



On both sides of the Atlantic, “voluntary export restraints” and other nontariff barriers were also deployed to “manage trade.” The sectors that received subsidies at home were also protected from foreign competition. Through the 1980s, American car manufacturers were protected by VERs that restricted the number of Japanese cars exported to the U.S. Europe negotiated a similar agreement with Japan in 1983. Many other sectors, such as semiconductors and VCRs, were also protected. The French government even demanded that all Japanese VCR imports enter France via Poitiers, a small town hundreds of miles from the nearest shipping port.



Now, as in the 1970s, there is a big global push to expand domestic regulations which “manage” the market. Most new regulatory proposals are not directly protectionist, but they do have subtle and indirect consequences for global economic integration. New financial‐​market regulations are brewing in OECD countries as well as in emerging markets. They concern everything from a global college of financial regulators and stricter capitalization requirements to limits on executive pay and directions to lend to small enterprises. Unless current regulatory ambitions are scaled down, they run the risk of stifling market signals and emasculating the entire global economy.



Regulatory agendas are also becoming cluttered with government subsidies. These measures comprise bailouts for ailing corporate behemoths (such as the U.S. car industry), and funds to protect “national champions” in “strategic” sectors from foreign takeover (as President Nicolas Sarkozy has proposed for France and the EU). Such measures inevitably distort global commerce and trigger retaliatory responses: If one country subsidizes, others follow; if one sector gets subsidized, others will demand like treatment. That will in turn lead to a clamor for other forms of protection against foreign competition, such as antidumping and safeguard duties, export subsidies, and discriminatory product standards.



Where will this creeping protectionism lead next? Financial services remain first in the line of fire. Tighter prudential regulations may be called for in some cases, but they should be distinguished from the rules that underpin market access for financial services firms, whether domestic or foreign. This distinction is now being ignored. Politicians and regulators — including those in China and India — will likely use the financial crisis as cover to block further market opening to foreign financial‐​services providers. Indian commerce minister Kamal Nath said after the EU‐​India summit in September that the financial crisis has vindicated his government’s policy of avoiding a comprehensive liberalization of the country’s banking system.



Creeping protectionism will not be restricted to finance: It can also be directed at specific countries and other economic sectors. Much Western protectionism in the 1970s and 80s was directed at blocking imports from Japan and other East Asian Tigers. Now, on a much grander scale, there is a protectionist backlash against the global integration of China and India. In the U.S. and EU, allegations against China of “unfair trade” linked to “currency manipulation,” bilateral trade deficits, hidden subsidies, and low labor and environmental standards are resurfacing.



Investment nationalism, often combined with energy nationalism, is also on the rise. The United Nations Conference on Trade and Development has recorded an increase in the number of new laws unfavorable to foreign direct investment. Since 2005, a quarter of all new FDI laws are considered to be unfavorable to FDI, compared with an average of 7.5% from 1992 to 2004. These restrictions are bunched in energy‐​related sectors, but they are spreading to other sectors. Congress, for instance, blocked a bid by China National Offshore Oil Company for Unocal, and it scuppered Dubai Port’s attempt to take over the management of six U.S. ports. Meanwhile, the Chinese government has recently tightened foreign‐​investment restrictions to protect national champions in a range of industrial, energy and service sectors.



Last but not least, the climate‐​change agenda appears set to become the Trojan horse of “standards protectionism” in the 21st century. Technical standards have long been a popular form of protectionism because they aren’t as obvious as tariffs or quotas, and can be disguised politically as matters of “safety” or “quality.” The EU has an emissions‐​trading scheme in operation. Congress will probably pass a similar cap‐​and‐​trade scheme next year. Because such schemes will impose substantial compliance costs on energy‐​intensive sectors at home, the EU and the U.S. will seek to impose similar costs on cheaper, carbon‐​intensive production elsewhere that is not subject to carbon‐​reduction policies. Hence the threat of trade sanctions on “free riders” — China in particular.



Because we’ve been down this road before in the 1970s, we can see where it is leading. Giving way to protectionism will deepen and prolong the global recession. Containing protectionism, and extending open markets, will facilitate flexibility and adaptation. It will speed up recovery and lay the foundations for future prosperity.
"
"Difficult and almost impossibly daunting as it may seem, the world is faced with not one but two existential crises and two races against time: the coronavirus and the climate emergency. Dealing with both is going to require extraordinary focus and resolution. Already there is a whiff of political opportunism in the air. Last week, the Czech prime minister, Andrej Babiš, said that the €1tn European Green Deal, unveiled and enshrined in law by the European commission barely three weeks ago, should be put to one side. Member states, he advised, should concentrate all resources on combating a pandemic which, one by one, is shutting down societies and economies. Along with other eastern European states such as Poland, the Czech government has been reluctant to acknowledge the scale of action required to combat global heating, which would have a severe impact on fossil fuel industries in their countries. The extreme urgency of defeating Covid-19 scarcely needs stating. But Mr Babiš’s broader suggestion has been rightly rejected. “This is one of the very reasons why we presented the climate change law: to avoid that climate action, a generational task, is obfuscated by more pressing and immediate challenges,” said a Brussels spokesman. Frans Timmermans, the Dutch commissioner who is leading the EU response on the climate emergency, has made the same point.  The new climate law commits member states to zero emissions by 2050. A stringent new target is also to be set for 2030, which will be enshrined in the law.  Mr Timmermans has said that the climate law will act as a ‘“compass” for the next 30 years as EU member states seek sustainable forms of growth. In these extraordinary times, local imagination and creativity in developing a sustainable future will be at a premium. There are at least some hopeful signs that such thinking is taking place. In the city of Utrecht, plans have been unveiled for the largest purpose-built pedestrianised residential area in Europe, which will attempt to harness the virtues of the sharing economy. The Merwede estate will house 12,000 people on a 60-acre site. Transport will be provided by bus and train networks and a shared pool of bikes and cars – one car for every three families. Schools, shops, sports and medical services will all be within walking distance, and water from the local canal will be used to heat the area. The intention is for the district to become close to energy neutral. Utrecht is one of the fastest-growing cities in the Netherlands and is projected to add 100,000 people to its 350,000 population by 2040. In terms of factoring in a necessary environmental dimension to new construction, Merwede looks like best practice. It is the kind of project that is relatively small scale, but repeatable. It helps of course that the Dutch have a historically passionate relationship with the bicycle. But as the EU attempts to hold the line on implementing its green deal, many more Utrechts will be required."
nan
"

Reuters reports that: 



Heat is more likely to kill an American than an earthquake, and thunderstorms kill more people than hurricanes do, according to a U.S. “death map” published on Tuesday…   
  
  
Heat and drought caused 19.6 percent of total deaths from natural hazards, with summer thunderstorms causing 18.8 percent and winter weather causing 18.1 percent, the team at the University of South Carolina found.



However, the result that heat is the most deadly natural hazard seems to be an artifact of the data source employed by the authors of the so‐​called “death map.” Their primary data source is the National Climatic Data Center’s _Storm Data_. However, the NCDC data for mortality from extreme heat and cold is questionable.   
  
  
As is evident from the paper, the authors are aware that mortality data for these two types of extreme events from NCDC are substantially different from mortality data from the Center for Disease Control (CDC) based on the Compressed Mortality File for the United States. The latter uses death certificate records, which provide the cause of each recorded death (based on medical opinion). I would contend that when it comes to cause of death, particularly for extreme cold and heat, medical opinion as captured in death certificate records is probably more reliable than determinations made by the meteorologists in the National Oceanic and Atmospheric Administration’s NCDC.   
  
  
The following table from Goklany (2007) provides a breakdown of mortality due to the major types of extreme weather events for 1979–2002 based on data from the CDC database for extreme cold and extreme heat, and various arms of the National Oceanic and Atmospheric Administration for floods, lightning, hurricanes, and tornadoes. It indicates that extreme cold, rather than heat, is the deadliest form of extreme weather event. In fact, over this period, extreme cold was responsible for slightly more than 50 percent of deaths during this period for the categories listed in the table.   






Note that despite the hoopla about natural weather disasters, they contribute less than 0.06% to the annual U.S. death toll!   
  
  
Moreover, as the following figure, also from Goklany (2007), shows, both US death and death rates from weather events are declining, despite any climate change, which we are assured can only make matters worse.   






Finally, the Reuters report notes, “Researchers who compiled the county‐​by‐​county look at what natural disasters kill Americans said they hope their study will help emergency preparedness officials plan better.” [The study was apparently funded by the Department of Homeland Security.] As a taxpayer, I hope that emergency preparedness officials look beyond this study to identify and prepare for future emergencies, or they might miss out on the larger disasters, even as they prepare for lesser ones.
"
nan
nan
"**Staff at a North East university have voted in favour of strike action over health worries caused by Covid-19.**
Northumbria University in Newcastle saw hundreds of students test positive last month, although the number of infections dropped to 33 this week.
The ballot was issued after the University and Colleges Union (UCU) said university management ""refused"" to address in-person teaching concerns.
The university said talks with the union were ongoing.
The UCU described the vote as a ""massive step forward"" as it worked to keep campuses ""safe"".
It revealed 66% of members who voted approved taking strike action with 89.9% also agreeing to take action short of a strike.
""We regret that it took a ballot for industrial action for Northumbria to take this matter seriously,"" general secretary Jo Grady said.
""If the employer had listened to our concerns from the start then we could have avoided this escalation.""
Earlier this week Northumbria announced it would offer a ""limited amount of teaching on campus until 4 December"" with students then completing two weeks' work online before Christmas.
The union has welcomed that move, describing it as a ""safety-first approach"".
Responding to the strike vote, a university spokesman said: ""We are doing all we can to protect colleagues who feel unable to teach on campus at this present time.
""This will usually be where colleagues have an underlying physical health condition or mental health concerns or share living space with someone who is vulnerable.
""In these circumstances we have made it clear that colleagues will not be compelled to deliver face-to-face teaching on campus.
""Discussions between the university and UCU have continued during the ballot period and at this stage it is not clear whether any action will be taken so it is not possible to comment in detail.""
_Follow BBC North East & Cumbria on _Twitter _,_Facebook _and_Instagram _. Send your story ideas to_northeastandcumbria@bbc.co.uk _._"
"The episodes of violence as a result of the Gilets Jaunes protests in France, which were initially triggered by increasing fuel prices, are an example of a recent type of conflict related to energy resources. Although these fuel riots have been happening for years now, these have never been studied or defined in an academic context. In my research, I collected the first database of fuel riots that occurred across the world between 2005 and 2013, which amounted to 44 different events. Analysing the data, I found that these violent riots are more likely when the international price of oil increases, and in countries that are politically fragile.  There are two ways in which climate change and scarcity can cause conflict over energy resources, and both act through the price of fossil fuels. The first is through climate action: governments introduce higher taxes on fossil fuels and related products in order to tackle climate change, which increases their price, which in turn triggers conflict. France is the only example we know of this pathway. The second comes about because fossil fuels themselves are also becoming scarcer and more difficult and expensive to extract, and this also increases their prices on the international market. To ensure poorer people still have access, essential products such as petrol and heating fuel are heavily subsidised all over the world. But, as the international price goes up, poorer nations often cannot afford high subsidies or prefer to redirect funds to other causes such as healthcare and other services to boost development. When governments slash subsidies, regular people end up having to pay more.  This is what happened in Nigeria, which despite being rich in crude oil, does not have many refineries and imports large quantities of fuel. The government in 2012 decided to cut a subsidy, leading to the doubling of fuel prices and transport fares. The reaction of the population was immediate, with violent protests and strikes that led to several deaths. Violent episodes from these two pathways have been called “fuel riots”, but no definition has ever been provided. So to define these events I adapted the definition of food riots coined by the World Bank: “Violent, collective unrest leading to a loss of control, bodily harm or damage to property, essentially motivated by a lack of fuel availability, accessibility or affordability, as reported by the international and local media, and which may include other underlying causes of discontent.”  If you were to monitor international fuel prices and national political fragility to identify countries at risk of experiencing fuel riots, as suggested in my research, France would have been one of the countries flagged up. Two thirds of the historical increase in France’s pump prices can be attributed to international oil prices, which rose steadily between June 2017 and October 2018, the month before the start of the demonstrations.  In addition, according to the World Bank’s indicator of Political Stability and Absence of Violence, France’s stability had been deteriorating between 2010 and 2016 and is considerably lower than the average in high-income countries. This measure of fragility takes into account several social and economic indicators and expert opinions. According to this evidence, there will inevitably be more fuel riots in future across the globe, thanks to rising international fossil fuel prices and (hopefully) increased action against climate change from national governments. However, governments still need to tackle climate change and many policies will target the price of energy-related products to encourage a switch to sustainable practices such as renewables and electric cars. So if governments need to implement these unpopular policies, how can they avoid a violent rejection from populations? First of all, the design and implementation of new policies needs to become more inclusive. In fact, one of the most cited reason behind the riots in France was the need for acknowledgement from a working class that feels neglected.  In addition, there needs to be better communication of why these measures are required to ensure the support of communities who are likely to be most affected. Climate policy needs to be seen as relevant for those communities, if they are to support its implementation.  Also, cash transfers or other forms of support need to be put in place to guarantee that the most deprived parts of the population – who cannot switch to more sustainable but expensive options as swiftly as others – still have access to basic resources such as heating and petrol to drive to work. Finally, parts of the revenue (in case of a tax) or saved expense (in case of a slashed subsidy) should be redirected to important causes or to fund the cash transfers mentioned above. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
BBC news has reported that 40,000 homes are still without water in Northern Ireland after the recent spell of freezing temperatures. Many have been without water for more than 10 days, and reservoirs are being drained due to an unprecedented number of leaks since the thaw. Calls to a few friends confirmed that, yes, it is bad – friends in Lisburn have been without water since Christmas Eve due to a frozen mains supply (i.e. not in their house); others in Belfast report low water pressure.  Water is being rationed in places.
Was it really that cold? A search of the BBC site revealed “‘Baltic’ Northern Ireland” tucked away on the BBC NI news page. Castlederg in the West of the province recorded a low of -18°C on 20th December – a new record.  The thing about Ireland is that it sits on the very western fringes of Europe, bathed by the warm Gulf Stream (which is why Doug Keenan considered the 7000 years of Irish tree ring data so important that he pursued Queen’s University through FOI requests).  Ireland, despite its latitude, just doesn’t do ‘very cold’ (or ‘very hot’ for that matter).
When I first got interested in climate I ended up corresponding with Tonyb about the temperature records of the Armagh Observatory in Northern Ireland.  These stretch back to 1796. Incidentally there are a couple of WUWT posts featuring Armagh in the last year (here, here and here). How does this current cold month compare with the historical record at Armagh?  Was the recent cold unprecedented?
The currently incomplete December record for Armagh consists of raw data – three automated readings per hour.  Rather than waiting until they calculate the December average I looked for nearby stations on Weather Underground and found Glenanne PWS, about 15km to the SW of Armagh.  The average temperatures for the two stations over the month of November is plotted in Figure 1.  This gave a good linear fit (R^2 = 0.889) with an offset – Armagh being on average colder by just over 1°C.
Figure 1. Average temperatures for Armagh and Glenanne N. Ireland through November 2010
Figure 2 shows the December data for Glenanne on the same scale. Up to the 28th December, the monthly average is -0.86°C.  Mild conditions are expected for the next three days and, if I plug the forecast max/min (29th 8/6; 30th 8/4; 31st 6/2)  into my spreadsheet to complete the month, the monthly average rises to an estimated -0.23°C for Glenanne, remembering that this is an approximation for Armagh, which is typically colder.
Figure 2. Average temperatures for Glenanne N. Ireland through December 2010
In the Armagh historical record, which I have for 1796-2002 from [1] the average temperature for December is 4.9°C; January average is colder (4.1°C).  There are just two individual months colder than December 2010: January 1814 (-2.2°C) and January 1881 (-0.9°C) which puts this one as the third coldest on record at Armagh (2010 might yet tie with 1881 when the actual average for the month is published).
Coldest months according to the Armagh record:

January 1814 -2.2°C
January 1881 -0.9C
December 2010  -0.2C
February 1855  0.0C,  January 1963 0.0C
February 1895  0.2C
February 1947 0.4C
January 1985 0.5C,  December 1878 0.5C

The list above also puts it in perspective with respect to other extreme years in living memory – most notably 1963 and 1947. According to the Armagh records none of the coldest months in these years saw such extreme cold as the Christmas period this year.  The Arctic cold cut though the mild Atlantic air this year resulting in a monthly average 4-5°C below normal (Figure 3).
Figure 3. 
Even without all the warming we have been led to expect 😉 December’s cold probably can be described as unprecedented. I’ll await with interest the actual December figures for Armagh (and those from the Met Office).  As for this being caused by global warming – bull – it was just an extreme weather event.  They happen.  Go back >100 years and they happened then too.
Reference
[1] C.J. Butler, A. M. García-Suárez, A.D.S. Coughlin and C. Morrell. Air Temperatures at Armagh Observatory, Northern Ireland, from 1796 to 2002 Int.J.Climatol. 25: 1055-1079 (2005) [Full paper]
UPDATE – from the Daily Mail (h/t Spectator in Tips & Notes).  Looks as if this will be a similar record in other parts of the UK too:


“Met  Office figures show that the average temperature from December 1, the  first day of winter, to December 28 was a bitter minus 0.8c (30.5f).
This equals the record December low  of 1890.”

The article goes on to point out that December is rarely the coldest month in the UK and a continued cold spell could beat the record set in 1683-84 of -1.17C.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e86619a43',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I’ve been aware of this for a couple of days on our Sea Ice page, but hadn’t done anything about it since I wanted to see if it might change. When blogger Kate of Small Dead Animals noticed it and published on it, I figured it was time to start asking NSIDC some questions.
Compare this NSIDC Arctic Sea Ice extent chart…

…with this from Cryosphere Today:


The NSIDC plot has since intersected the 2007 line, but CT has no new images up since 10-27-10:

It certainly appears that there is more ice in 2010 than 2007 on the Cryosphere Today page. CT hardly ever responds to email, so I didn’t even bother asking them why the discrepancy. NSIDC’s Walt Meier though, takes our concerns seriously and responded rather quickly to my questions:
~~~~~~~~~~~~~~~~~~~~~~~~~~~
From: Walt Meier
To: Anthony
Subject: Re: you might have a  problem
Sent: Oct 29, 2010 8:42 AM
Hi Anthony,
Thanks for the  heads up. I looked at it and it doesn’t look like there
is any  problem.
As we went through before with Steve [Goddard], looking at the images can  be
misleading because they’re not on an equal area projection. There is
more ice in the central Arctic this year, but less in the Beaufort Sea,
Canadian Archipelago, and Baffin Bay. These areas roughly balance each
other out.
I also recall Cryosphere Today having an issue of changing  their images,
so I don’t know if you can consistently compare them anyway –  it looks
like their 2007 image is missing some ice. Attached is our  concentration
images from 2007 and yesterday and there doesn’t look like  much
discrepancy (apologies for the different image  sizes).
walt
~~~~~~~~~~~~~~~~~~~~~~~
I fixed the size differences, and here they are:


Of course we don’t have the daily extent data from NSIDC, since they so far have refused to publish it (they do give monthly though) so, we have to be content with image comparison rather than data comparison with NSIDC.
=======================================
Walt, as I said before, you really should publish the daily data. Consider how this looks: NSIDC director Serreze screams “death spiral” to the media while at the same time holds back publicly funded data. It is the same sort of bull-headedness that got CRU in deep trouble.  – Anthony
=======================================
UPDATE: Reader Lee Kington provides this blink comparator version:



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e87ddeb03',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Guardian journalist George Monbiot wrote a damning critique of the BBC and Sir David Attenborough’s wildlife documentaries in late 2018, arguing that they do little to illustrate the huge environmental issues faced by the natural world. Since then, Attenborough has adopted a much stronger position. He spoke at both the UN Climate Summit and the World Economic Forum in Davos, and used his platform to highlight the threats of climate change. Embarking on a new collaboration, Attenborough and the BBC are set to confirm their position in a one-off documentary entitled Climate Change - The Facts, airing on April 18. The 90-minute film will explain the effects that climate change has already had and the disasters it might cause in future. Although it’s crucial to raise awareness among the public about the impacts and threats of climate change, it’s equally important to explain how to fight it. That’s something the BBC has been more quiet about. The recent series Blue Planet Live featured a segment on the Great Barrier Reef in which it stated that coral bleaching is the result of climate change. That places the BBC in line with the scientific consensus. The same episode later described the “heroic research” effort that is needed to save the world’s reefs from coral bleaching, and covered the capture and transfer of coral spawn to a new location.  However, science has already given the solutions to address this problem. Recent reports from the Intergovernmental Panel on Climate Change, the Institute for Public Policy Research  and some of our own research all clearly indicate that tackling climate change and other environmental issues – including biodiversity loss, soil erosion and even ocean plastic pollution – requires major changes to society. We need to revise our economic system and its dependence on growth to prevent the unnecessary consumption of the world’s resources. As the youth climate strikes leader, the 16-year-old Greta Thunberg, clearly puts it, we need “system change, not climate change”.  In an era when schoolchildren are striking for climate action and radical proposals for climate action are entering the political mainstream, the BBC’s timidity towards even discussing solutions seems odd. Covering these arguments is political but goes way beyond party politics and certainly wouldn’t breach impartiality guidelines. Audiences might understand that this isn’t as interesting as coral spawning being captured during a lightning storm, as was shown on Blue Planet Live. But if the BBC don’t address the solutions to climate change, then how can there be an educated public which understands that saving the planet requires more than individual gestures like carrying a reusable coffee cup? There’s no doubt that Attenborough’s BBC documentaries have inspired millions of people around the world to take environmental issues seriously. His programmes have encouraged many of our students to undertake degrees in environmental sciences. Their insights into the natural world can present a sense of environmental optimism that promotes action. But failing to address the political and economic solutions necessary to stop climate change means the BBC could fail to respect its own values in education and citizenship. With their new documentary, Attenborough and the BBC should challenge our current economic system – only then can they fulfil their duty to inform the public with accuracy and impartiality. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

Manslaughter is the killing of a human being without expressed or implied malice. Average life expectancy is very highly correlated with per capita income, and income growth is very highly correlated with economic freedom (note accompanying table).



When politicians enact anti‐​economic growth regulations and taxes, even in the name of “global warming,” “environmentalism,” and “fairness,” they are, in fact, shortening the lives of many of their fellow citizens and those in other countries.





The fundamental problem is that many politicians do not understand or, perhaps, do not wish to understand tradeoffs. 



I do not pretend to know with much certainty whether the Earth will be much warmer at the end of this century and whether any increase in temperature that does occur will reduce or increase human life expectancies. But I do know the following with high confidence: The global warming alarmists told us 15 years ago that the Earth would be getting steadily warmer — yet, in fact, it has been getting cooler for the last 10 years, and some of their new models say this cooling trend might continue for another 10 or 15 years. The restrictions on drilling for new oil have driven up the price of oil products to the extent they are causing unnecessary real hardship to billions of people on the planet and, as a result, people are spending less money on their medical care and medical research.



The political requirements to use corn and other food plants for fuel have driven up the price of food again for billions of people in the world, and those at the bottom of the income ladder are increasingly suffering from malnutrition.



In sum, millions, if not billions, of people right now are unnecessarily having their lifespans reduced because of an overreaction to a projected climate change which may or may not have an affect on the lifespans of humans living sometime in the future.



A 2002 National Bureau for Economic Research paper by Frank R. Lichtenberg showed that the empirical data (from 1960–97) provide strong support that medical innovation and expenditures on medical care contribute to increased longevity. “The estimates imply that the medical expenditure needed to gain one life‐​year is about $11,000, and the pharmaceutical R&D expenditure needed to gain one life year is about $1,354. Previous researchers have estimated that the average value of a life‐​year is approximately $150,000.”



Probably many politicians think they are being responsible when they do things like imposing costly environmental taxes, prohibiting new mining for needed metals, and coming up with costly CO2 trading schemes as the Europeans have done, and which are now being debated in the U.S. Congress. Yet, all these actions impede the proper functioning of the market, reduce economic growth and lower life expectancies, in part, by reducing the funds available for medical research and treatments.



Environmental laws that require reasonably clean air and water can clearly be a net gain for human health and economic development. But, like anything taken to excess, the costs of many of the new and proposed measures to curb CO2 greatly exceed the benefits, thus costing both lives and treasure. Most serious economists who have looked at the issue believe environmental adaptation (as humans and other plants and animals have done for millions of years) would be far less costly than speculative actions to try to change the climate.



The fundamental problem is that many politicians do not understand or, perhaps, do not wish to understand tradeoffs. That is, every time they increase a regulation or a tax, or require a government expenditure that reduces economic freedom and does not meet a reasonable cost benefit test, they are not engaged in just some annoyance, but they are costing real human life years.



Vaclav Klaus, president of the Czech Republic and also a noted economist, has written a book titled “Blue Planet in Green Shackles, What Is Endangered: Climate or Freedom?” Mr. Klaus does understand tradeoffs, and he also understands the nature of many in the political class, having spent much of his life under a communist regime.



He argues, as he did in an interview with The Washington Times last week, that global warming “is used to justify an enormous scope for government intervention vis‐​a‐​vis the markets and personal freedom.”



One might take issue with President Klaus’ assertion that those who advocate more regulation and higher taxes are not all well‐​meaning public servants, except for the unwillingness of many such as former Vice President Al Gore to debate both the science and economic costs with serious opponents and support serious cost‐​benefit analysis.



It is possible to make rational decisions about such issues as how much to spend on the environment, defense, public health, infrastructure, etc., and how to best fund such activities.



Unfortunately, all too many in the political class, whether in Washington, Brussels or wherever, want to turn the issues into a “religion,” as Mr. Klaus says, rather than to coolly analyze the pros and cons, and rationally debate the merits. If it were only possible to indict politicians for “manslaughter” for their costly and destructive acts, many billions of life years would be saved.
"
"
CU-NASA Research Center to Study Sun’s Effects on Earth’s Climate 





Image of sun courtesy of NASA.




The University of Colorado at Boulder’s Laboratory  for Atmospheric and Space Physics and NASA’s Goddard Space Flight Center  in Greenbelt, Md., today announced the formation of a new collaborative  research center dedicated to the study of the sun’s effect on Earth’s  climate.
The center, called the Sun-Climate Research Center, or  SCRC, will be co-directed by LASP Research Scientist Peter Pilewskie as  well as Robert Cahalan, who heads Goddard’s Climate and Radiation  Branch, and Douglas Rabin, head of Goddard’s Solar Physics Laboratory.

“The  exciting thing about this collaboration is that we believe it will  promote studies to help answer key questions about the climate system,  including how Earth’s atmosphere responds to the sun’s variability and  how that affects climate,” said Pilewskie, a faculty member in  CU-Boulder’s atmospheric and oceanic sciences department. “This question  is particularly important now as we seek to quantify the human-induced  impact on Earth’s climate.”
Made possible by a Federal Space Act  Agreement, SCRC will foster collaboration between Earth-atmosphere and  solar sciences at the two institutions.  Opportunities will include a  scientist exchange program between the organizations and the ability for  postdoctoral scientists and graduate students in science, engineering  and mission operations to move between LASP and Goddard. The partnership  also will include international research symposia on sun-climate  interactions.
“In recent years Goddard and LASP have worked  together on several Earth and sun missions,” said Cahalan.  “Now we look  forward to continuing to drive growth in this key interdisciplinary  field of sun-Earth research, bringing new focus to the study of  multiyear changes in the sun and its influence on Earth’s climate.”
According  to the center’s co-directors, the SCRC represents a rare and innovative  step that underscores LASP’s ability to take its high-caliber research  and program opportunities to a new level with Goddard.
“LASP has  developed some remarkable areas of expertise that are key to studying  the sun and its effect on climate and on human activities,” said LASP  Director Daniel Baker. “By working with our colleagues at Goddard, we  can leverage our skills and help take an important step toward greater  cooperation between NASA centers and leading university research teams.”
For more information on LASP visit lasp.colorado.edu/home/. For more information on NASA’s Goddard Space Flight Center visit www.nasa.gov/centers/goddard/home/index.html.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e86f0e138',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

The era of trade liberalization is dead. Yet it could get worse still. Not only have prospects for liberalization over the next few years been dashed, but Congress is considering legislation that could precipitate a retreat from the trade policies and institutions that have served U.S. interests for 60 years.



These are indeed dark days for trade. The Democratic Party, which has grown increasingly hostile to trade over the past decade, controls the legislature. The president’s authority to negotiate trade agreements and present them to Congress for an up‐​or‐​down vote has expired, and will not be renewed. The bilateral trade agreements completed with South Korea, Colombia, Peru and Panama will likely rot on the vine, as Congress shunts them aside to consider instead trade legislation that is either antagonistic or protectionist. And for the first time in post‐​World War II history, a multilateral trade negotiating round has ended in failure. The era of negotiation and accommodation may yield to one of confrontation and litigation.



One thing that has become clear this year is that Democratic Party opposition to trade runs much deeper than the leadership has been willing to admit. When the Democrats assumed control of Congress in January, the party’s leadership whispered assurances that, notwithstanding the strident anti‐​trade rhetoric adopted by its rank and file, they understood the importance of continuity in U.S. trade policy. With some modifications to the U.S. trade agreement template to reflect Democratic priorities on labor and environmental issues, the Congressional leadership would be able to help the administration move the agenda forward.



A grand bargain was struck in the spring, which was nothing more than a wholesale capitulation by the administration to Congressional demands for strict, enforceable labor and environmental provisions in prospective trade agreements, including the four pending congressional consideration. But as the ink was drying, the Democrats moved the goalposts.



The South Korea agreement was deemed unsupportable by House Ways and Means Chairman Charles Rangel (DNY) and Ways and Means Trade Subcommittee Chairman Sander Levin (D-MI) because its terms do not condition Korean automobile access to the U.S. market on the performance of U.S. automobile exporters in the Korean market. Of course, such a provision, which was put forward by Rangel and Levin in the waning days of the negotiations, would leave the U.S. auto producers in a position to decide just how much competition it wanted from Korean producers. Accordingly, that provision was a nonstarter.



The Colombia agreement was deemed unsupportable because the Uribe government allegedly has done an inadequate job of finding and prosecuting thugs who have terrorized and killed Colombian unionists over the years. Thus, Democratic disdain for a right‐​ofcenter Latin American government, which also happens to be one of the few regional governments not openly hostile to U.S. policy, suffices for justification to deprive Colombian citizens of the opportunity to improve their lots through better trade terms with the United States.



Consideration of the Peru agreement was sidelined until Chairman Rangel and others have a chance to visit Peru, see first hand how its factories are run, and possibly change the agreement’s terms, again. Democrats have used the labor conditions excuse to camouflage Big Labor’s real motive, which is to kill trade deals at all costs. At least that truth now has been exposed. But regrettably, the anti‐​trade objectives of organized labor and importcompeting interests have dovetailed conveniently with proliferating misconceptions and myths about imports, jobs, and manufacturing to produce a phony sense of crisis.



Most of the anti‐​trade legislation introduced this Congress is premised on the myth of U.S. manufacturing decline at the hands of rising imports, mostly from China. But U.S. manufacturing is thriving. In 2006 the manufacturing sector achieved record output, record sales, record profits, record profit rates, and record return on investment.



Imports are not a bane for U.S. producers. In fact, there is a strong correlation between manufactured imports and manufacturing output, as U.S. producers account for more than half of the value of all U.S. imports. When imports rise, output rises. When imports fall, output falls. In the past quarter century, imports have increased six‐​fold, while real GDP has grown by more than 130 percent, creating an average of 1.8 million net new jobs each year.



But policymakers fail to acknowledge this crucial relationship. Instead, too many in Congress view exports as good, imports as bad, and the trade account as the scoreboard. Given the large and growing U.S. trade deficit, policymakers conclude that we are losing at trade. And we are losing at trade because our trade partners are cheating.



In China’s case the alleged cheating involves currency manipulation, subsidization of industry, unfair labor practices, hidden market barriers, dumping, and other transgressions. Some of these allegations may carry a degree of truth, but by and large the trade relationship has been conducted within the rules and consensually, yielding huge benefits for Americans.



In any event, the proper course for redress for complaints is through the dispute settlement system of the World Trade Organization. The Bush administration lodged three formal complaints earlier this year, which are working their way through the process. Congress should allow that process to continue and restrain its urge to be seen doing something. There is a distinct risk that unilateral, punitive actions on trade could severely damage the trade relationship and lead to a contagious deterioration of respect for the WTO and its decisions. That, ultimately, would take us back to the days when tit‐​for‐​tat trade wars were common, and uncertainty in trade prevailed.



Plenty of blame for the current state of affairs rests with the Congressional Democratic leadership, which has reckoned there is very little political downside to receding on trade, economic consequences be damned. That position has the blessing of Big Labor, and opposing the initiatives of an unpopular president might prove to be good politics.



But Republicans are on the hook too. The strong pro‐​trade consensus among Republicans that was so evident in the 1990s began breaking down in the early part of this decade, as China’s economic emergence was becoming evident. Steel‐ and textile state Republicans have presented some of the greatest obstacles to the Bush administration’s trade policy agenda.



And by failing to make a comprehensive case for trade liberalization, the Bush administration itself bears some responsibility for the current state of affairs. Rather than talk about the benefits of imports, which keep prices in check for consumers and input costs competitive for producers, the administration has focused almost exclusively on the potential export gains from trade agreements, affirming the mercantilist world view of Congress. The U.S. Trade Representative’s office is fond of pitching further trade liberalization by pointing to the U.S. trade surplus with countries with which this administration has negotiated bilateral trade agreements. But by treating a trade surplus as a success metric, it’s only a small step to the conclusion that our overall trade policy is failing, given our nearly $1 trillion deficit.



The Bush administration’s quest for further trade liberalization came to a grinding halt when the 110th Congress convened. But in many ways the President’s trade policy legacy might be forged during its final 18 months. By holding the line against bad trade legislation from an increasingly confrontational Congress, the administration can make the task less arduous for a subsequent administration to rebuild the consensus for trade when the political climate improves.
"
"

In a recent op-ed Robert Kagan laments that (Western) Europe is sliding into irrelevance. But that might be the best thing for the rest of the world.   
  
Don’t get me wrong, the world owes plenty to Europe. It’s given the world great art, architecture, literature, and music. It’s also given the world the ideas of universal education, the scientific method, research institutions, property rights, rule of law, democracy, religious freedom, and freedom of thought and expression, among other things. These ideas and institutions coalesced to power the engine of progress that drives the economic and technological development that have improved human well-being — not only in Europe but elsewhere — to levels far beyond what our ancestors could have imagined. Consequently, today we live longer, healthier, more educated, freer, and wealthier than ever before. But for the past century, Europe seems determined to undo all the good it’s ever done.   
  
Europe gave the world the ideologies of Fascism and Marxism, which were responsible — or provided rationalizations — for 100–150 million deaths worldwide, including many outside Europe, most notably in China, Cambodia and North Korea. Then in a few short decades, despite having risen Phoenix-like from the ashes of destruction of World War II, instead of brimming with optimism, Europe has taken a decidedly pessimistic turn.   
  
It no longer believes in progress. Its birth rate has dropped below replacement rates, yet, despite its protestations of equality, fraternity, secularism, and respect for human rights, it’s unwilling or unable to welcome or integrate immigrants of different colors or religious backgrounds into its societies. And one by one it’s abandoning the great ideas that brought it, and the rest of the world, progress, and advanced human well-being.   




Its political leadership, although democratically elected, has abandoned democracy in its pursuit of a united Europe. The more the idea of the EU fails in democratic tests — most recently in Ireland — the more devious its politicians' machinations to bypass popular approval.   
  
It has abandoned scientific inquiry, relying instead on mantras such as the “science is settled.” Having abandoned science, it now relies on superstition, manifested in the notion of a global-warming-triggered apocalypse of Biblical proportions if average temperatures exceeds 2 degrees Centigrade above pre-industrial levels — an apocalypse complete with death, disease, pestilence, droughts, famines and floods. Not only is there no evidence for this, this superstition persists despite the current reality that more Europeans die in winter than in summer, Europe’s long history of misery and want during cold periods and plenty during warm eras, and that even as media coverage of extreme weather events becomes more compelling and ubiquitous, globally the deaths and death rates from such events are in long term decline. If Europe had spent a fraction of the resources in adapting to climate change as it did on complying with the futile, but politically-correct Kyoto Protocol, it might have reduced by thousands the death toll of its 2003 heat wave.   
  
Europe is now on the verge of abandoning the quest for technological progress, preferring instead to be ruled by the so-called precautionary principle which, as applied by Europe, actually increases human misery and death. It does this by discouraging, if not vetoing, new and safer technologies that could displace older and less safe technologies on the grounds that “safer” is not good enough — it has to be absolutely safe.   
  
The precautionary principle was used to justify relinquishing its use of DDT, which was easy, because Europe had already conquered malaria. It is also used against genetically modified crops. The misapplication of the precautionary principle, coupled with its abandonment of scientific inquiry evident in the torching and destruction of experimental trials on genetically modified crops and its reliance on superstition, has resulted in a _de facto_ ban on such crops in most of Europe. But giving up such crops isn’t hard either. Western Europe is well fed — in fact today it worries more about obesity than hunger — and its farmers’ excessive productivity is actually a drag on its taxpayers. Some Europeans would also give up nuclear and coal, but that would actually be giving something up, so protestations to the contrary, that will come about only after renewable energies mature and are better able to pay for themselves without subsidies.   
  
But worst of all, Europe is once again exporting dangerous, misanthropic ideas, which unfortunately are echoed even in the US where many are in thrall of European ideas, no matter how ill-conceived. These ideas are couched in doublespeak, such as the European version of the precautionary principle, which could kill as many people as the failed ideologies of Fascism and Marxism.   
  
Europe talks endlessly of helping developing countries and offering token amounts of aid but then refuses to reform its agricultural policies which would do a lot more for helping the latter help themselves. At the same time it bemoans the new prosperity of long-suffering Asia that has lifted over a billion out of a poverty that Europe has not known since even before the French Revolution because it's enabled by and rides on greater energy use. And for that, some Europeans threaten punishment through carbon tariffs.   
  
But energy use and economic development are inextricably linked not only in China and India but in Europe and elsewhere. Even as energy use fueled economic development, it freed human beings from back-breaking physical labor, allowed women to escape the drudgery of household work, equalized economic opportunities for women, reduced the need for child labor, liberated animals from being our beasts of burden, and enabled brains to displace brawn, laying the foundation for a less energy-intensive economy.   
  
Europe campaigned actively, but fortunately unsuccessfully, to ban DDT. Despite this, African nations, deferring to European “expertise” on matters technological while fearing a European boycott of their agricultural exports if even trace amounts of DDT are found on them, have been slow to adopt DDT to combat malaria — fears that Europe did nothing to dispel and may, in fact, have actively encouraged. For the same reasons, Africans have been reluctant to turn to genetically modified crops to reduce hunger and malnutrition. And once again, Europe is standing silently by if not actively discouraging the use of genetically modified crops.   
  
For context, consider that over 6 million people die each year from malaria, hunger and malnutrition, a toll that annually rivals that of the entire Holocaust. Yet Europe has done little to help or reassure Africa in this regard, thereby abandoning one of the Holocaust’s most important lessons, namely, inaction can be no less culpable than active participation.   
  
Europe may be able to walk away from further economic and technological development, but the rest of the world can't afford to, not if it values human and environmental well-being.   
  
An irrelevant Europe could save innumerable lives in the developing world. And that might be best for this world.


"
"The polar ice caps are melting six times faster than in the 1990s, according to the most complete analysis to date. The ice loss from Greenland and Antarctica is tracking the worst-case climate warming scenario set out by the Intergovernmental Panel on Climate Change (IPCC), scientists say. Without rapid cuts to carbon emissions the analysis indicates there could be a rise in sea levels that would leave 400 million people exposed to coastal flooding each year by the end of the century. Rising sea levels are the one of the most damaging long-term impacts of the climate crisis, and the contribution of Greenland and Antarctica is accelerating. The new analysis updates and combines recent studies of the ice masses and predicts that 2019 will prove to have been a record-breaking year when the most recent data is processed. The previous peak year for Greenland and Antarctic ice melting was 2010, after a natural climate cycle led to a run of very hot summers. But the Arctic heatwave of 2019 means it is nearly certain that more ice was lost last year. The average annual loss of ice from Greenland and Antarctica in the 2010s was 475bn tonnes – six times greater than the 81bn tonnes a year lost in the 1990s. In total the two ice caps lost 6.4tn tonnes of ice from 1992 to 2017, with melting in Greenland responsible for 60% of that figure. The IPCC’s most recent mid-range prediction for global sea level rise in 2100 is 53cm. But the new analysis suggests that if current trends continue the oceans will rise by an additional 17cm. “Every centimetre of sea level rise leads to coastal flooding and coastal erosion, disrupting people’s lives around the planet,” said Prof Andrew Shepherd, of the University of Leeds. He said the extra 17cm would mean the number of exposed to coastal flooding each year rising from 360 million to 400 million. “These are not unlikely events with small impacts,” he said. “They are already under way and will be devastating for coastal communities.” Erik Ivins, of Nasa’s Jet Propulsion Laboratory, in California, who led the assessment with Shepherd, said the lost ice was a clear sign of global heating. “The satellite measurements provide prima facie, rather irrefutable, evidence,” he said. Almost all the ice loss from Antarctica and half of that from Greenland arose from warming oceans melting the glaciers that flow from the ice caps. This causes glacial flow to speed up, dumping more icebergs into the ocean. The remainder of Greenland’s ice losses are caused by hotter air temperatures that melt the surface of the ice sheet. The combined analysis was carried out by a team of 89 scientists from 50 international organisations, who combined the findings of 26 ice surveys. It included data from 11 satellite missions that tracked the ice sheets’ changing volume, speed of flow and mass. About a third of the total sea level rise now comes from Greenland and Antarctic ice loss. Just under half comes from the thermal expansion of warming ocean water and a fifth from other smaller glaciers. But the latter sources are not accelerating, unlike in Greenland and Antarctica. Shepherd said the ice caps had been slow to respond to human-caused global heating. Greenland and especially Antarctica were quite stable at the start of the 1990s despite decades of a warming climate. Shepherd said it took about 30 years for the ice caps to react. Now that they had a further 30 years of melting was inevitable, even if emissions were halted today. Nonetheless, he said, urgent carbon emissions cuts were vital. “We can offset some of that [sea level rise] if we stop heating the planet.” The IPCC is in the process of producing a new global climate report and its lead author, Prof Guðfinna Aðalgeirsdóttir, of the University of Iceland, said: “The reconciled estimate of Greenland and Antarctic ice loss is timely.” She said she also saw increased losses from Iceland’s ice caps last year. “Summer 2019 was very warm in this region.”"
"Every now and then, the idea of powering Europe using the vast solar resources of the Sahara Desert comes up. Were this to actually happen, we may witness the rise of new energy superpowers in Northern Africa. But a look at the economic and political energy system suggests what’s more likely is the oil-rich countries of the Arabian (or Persian) Gulf will continue to dominate energy trade even in the post-fossil era. Renewable energy, of course, is very location dependent – the sunnier a place is, the more energy you get out of photovoltaic panels. Over the course of a year, southern Algeria, for example, gets more than twice as much solar energy as southern England. The graph below, which I put together as part of my PhD, shows that some of the best solar resources in the world are indeed found in Algeria, Libya, Egypt, Niger, Chad and Sudan.  So, one could build large Saharan solar farms and then transmit the power back to densely populated areas of Europe. Such a project would need to overcome various technical challenges, but we can say that in theory it is possible, even if not practical. Yet plans to actually set up mass Saharan solar have floundered. The most notable project, Desertec, was fairly active until the mid 2010s, when a collapse in the price of oil and natural gas made its business case more difficult. At that time, the major technology considered was concentrated solar power, where you use the heat from the sun to run a steam turbine. Energy can be stored as heat overnight, therefore enabling uninterrupted energy supply and making it preferred to then expensive batteries.  Since then, however, the cost of both solar panels and battery storage have dropped drastically. But, while conditions might look favourable for Saharan solar, it is unlikely that new solar energy kingpins will arise in North Africa. Instead, we should look one desert further to the East – the Rub al Khali on the Arabian peninsula, the home of the reigning energy powers. The economies of the United Arab Emirates, Saudi Arabia, Qatar and the other Gulf nations are built around energy exports. And as climate change imposes pressure on the extraction of fossil fuels, these countries will have to look for alternative energy (and income) sources in order to keep their economies afloat. The International Renewable Energy Agency set up its headquarters in Abu Dhabi, and the region has no shortage of ambitious solar projects promising extremely cheap electricity. However only a small amount of capacity has actually been deployed so far. Low oil revenues have not helped with the megaprojects. Countries in the Sahara also have little history of trading fossil fuels, outside of Libya and Algeria, while things are rather different for the petro-states of the Gulf. And this matters because, in the energy business, worries over longer-term security of supply mean countries tend to trade with the same partners.  This would be the Achilles’ heel of a Northern African energy project: the connections to Europe would likely be the continent’s single most important critical infrastructure and, considering the stability of the region, it is unlikely that European countries would take on such a risk.  Which brings us to an alternative way to transmit energy: hydrogen. A process called electrolysis can use renewable electricity to split water into hydrogen and oxygen, and the resulting hydrogen can store lots of energy. Soon it will become feasible to move energy around the world in this form, using shipping infrastructure similar to that already in use today for liquefied natural gas. Sure, there are disadvantages compared to batteries. It would mean introducing two more conversion stages and thus reduced efficiency (30% roundtrip efficiency compared to 80% for batteries), but it would overcome the distance barrier. And perhaps just as importantly: shipping energy by hydrogen would mean no significant change to the existing maritime trade infrastructure, which will hand an advantage to established energy exporters.  If this means the Sahara is unlikely to develop renewable energy superpowers, then perhaps this is for the better. With the booming populations of Sub-Saharan Africa in dire need of electrification, clean solar power might be better used to alleviate the energy crisis in somewhere like Nigeria rather than sent to Europe. While these countries may eventually be able to shake off any solar resource curse, in the short term, exports like these could just look like yet another European attempt to extract natural resources from Africans. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"An unusually warm winter has caused bears to stir early from hibernation in several countries, raising concerns of an increased number of conflicts with humans. There have been multiple sightings of bears emerging from hibernation in February and early March in Russia, Finland and the US, a situation apparently triggered by the mild winter experienced in many countries.  This winter was the warmest ever recorded in Europe “by far”, according to scientists, with the US just experiencing its hottest December and January on record. Moscow Zoo has been preparing to deal with the emergence of two Himalayan bears a month early, while a grizzly bear sighting was reported on 3 March in Banff national park in Canada, the earliest such sighting in a decade. Bears are also on the move in theYellowstone national park in the US, with a grizzly spotted prowling around the park’s famed Grand Prismatic Spring on 7 March. Park officials said that visitors should keep at least 100ft away from the bears, not to run away and keep bear spray to hand. It’s not just bears that have been waking early from their hibernation slumber – there are also reports of an itinerant groundhog in Maine. Although there is sparse data on hibernation trends, experts have warned that the climate crisis could be spurring a harmful mismatch in seasons for animals, especially those who hibernate through the cold winter months when food is scarce. Among bears males are usually the first to emerge, in March or April, followed by females and then females with cubs. If they emerge in unusually warm winter weather, they may find there is little to eat. “From my experience we are hearing more reports of bears out in late February and early March,” said Chris Servheen, the former grizzly bear recovery coordinator at the US Fish and Wildlife Service. Servheen said he had heard about black bears coming from hibernation already in the Rocky Mountains as their snow dens melt and they are forced to leave their soggy homes. Even some grizzly bears at high elevations are departing their dens too. “We aren’t sure why they are coming out earlier but if there are more sunny, warm days they sense that, they see the conditions are good and they go out,” he said. “The problem is they can’t stay in but there’s nothing for them to eat. They are burning up energy moving around but the plants haven’t started to grow yet, I’d expect it’s not a good thing for them.” This mismatch raises the possibility that hungry bears will be involved in confrontations with humans as they desperately search for food. Towns in Canada and Russia have been the stage for tangles between people and bears as the ursine visitors ransack bins and gardens for food. “If we see this as a continuing thing with climate change we will probably see more conflicts because there’s not much food for the bears,” Servheen said. “If bears come out early they will potentially seek food around people, such as in garbage, bird feeders and crops. The potential for conflict is certainly higher as they come out earlier.”"
"

Policymakers shouldn’t waste time worrying about a falling dollar. As Milton Friedman wisely advised, the exchange rate of our currency should be set by markets, not by fallible and fidgety politicians.



President Bush sounded the right note in his interview on the Fox Business Channel on Tuesday when he said, “We have a strong dollar policy, and it’s important for the world to know that. We also believe it’s important for the market to set the value of the dollar, relative to other currencies.”



A weaker U.S. dollar is a two‐​edged sword for Americans. The declining dollar has boosted U.S. exports to record levels, partially offsetting the downward tug on the economy from turbulence in the housing and mortgage markets. It’s also moderating the U.S. trade deficit in goods and services, which is on track to end 2007 almost 10% smaller than the 2006 deficit.



On the downside, the slide in the dollar has fueled higher import prices, most spectacularly for oil, imposing costs on U.S. consumers and producers alike. It’s no coincidence that a sharply depreciating dollar has preceded every major spike in oil prices since the early 1970s.



The right response from Washington should be to let the dollar find its own value in global markets while avoiding policy mistakes that undermine its worth. Spiraling inflation, runaway government spending, hostility to foreign investment and the specter of protectionism would scare away foreign investors, causing the dollar to falter further and maybe even crash.





A weaker U.S. dollar is a two‐​edged sword for Americans.



A freely floating currency allows our economy to adjust to shifting fundamentals. Any attempt to intervene would soon be overwhelmed by the $2 trillion exchanged daily on global currency markets. Efforts to artificially boost or depreciate a currency only postpone needed adjustments, as South Korea, Thailand, Brazil and other countries painfully discovered in the late 1990s.



Instead of manipulating the currency, policymakers should keep their eyes on the sound fundamentals that ultimately determine its value. The most important are low inflation, an open and flexible economy, fiscal restraint, and an attractive investment climate, including reasonable regulations and low tax rates. As long as the U.S. government pursues such policies, the world will want our dollars.
"
"**Kent and Medway will face the toughest coronavirus curbs when the national lockdown ends, it has been announced.**
The areas will be placed under tier three from 2 December.
Swale and Thanet have the two highest rates of Covid-19 infection in England. Medway has risen to fourth and Gravesham is in the top 20.
There will be a ban on households mixing, except in limited circumstances such as parks, and people will be urged to avoid travel outside their area.
Pubs and restaurants will only be able to offer delivery or takeaways. Indoor entertainment venues will also be shut.
The government said it would review the tier allocations on 16 December.
It explained that infections across Kent and Medway were high, and continuing to rise ""with large increases in case rates in almost all areas in the last seven days"".
It said some of the highest case rates in the country were currently in Kent, with rising cases in people aged over 60 a particular concern.
Swale continues to have the highest rate in England, with 799 new cases recorded in the seven days to 22 November - the equivalent of 532.4 cases per 100,000 people, down from 637.7 in the previous seven days.
Thanet has the second highest rate, down from 522.8 to 478.4, with 679 new cases.
However, other parts of Kent, including Tunbridge Wells and Ashford, have been below the national average.
**by Lauren Moss, BBC South East Political Editor**
Placing more than 1.5 million people in Kent and Medway into the toughest level of restrictions will be greeted with dismay by some, but perhaps it shouldn't come as a huge surprise.
The rate of coronavirus infection varies pretty widely across the area but figures show it is climbing up in Folkestone and Hythe, Dover, Gravesham, Maidstone, and Medway, although it has now started to slow down in Thanet and Swale.
Both authorities will be hoping mass rapid testing that has been promised to all areas in tier three will play a part in bringing down the spread of infection, much like it's believed to have done in Liverpool.
Everyone will be keeping a close eye on those numbers when the restrictions are reviewed on the 16 December.
Tory MPs in Kent had lobbied the Prime Minister to not impose county-wide restrictions due to the variations in case numbers.
The group said: ""We must allow businesses to prosper and not be held back by restrictions not suitable for their area.""
Sir Roger Gale, the Conservative MP for North Thanet, said while the tier restrictions would not be a shock to his constituents, he nevertheless had backed the call for a district approach.
""The pandemic doesn't understand county boundaries. There is no logic to this at all,"" he said.
Roger Truelove, leader of Swale Borough Council, said he agreed with the tier three restrictions.
""I hope that that's an incentive for local people to comply as much as possible with the guidance so we get our numbers down.
""We want to push them further down,"" he said.
Leader of Medway Council, Alan Jarrett, said the restrictions were ""absolutely necessary"".
""I appreciate we all want to get back to doing the things we love and therefore each and every one of us must act now and join the fight against Covid-19,"" he said."
"
Vortice in a teacup, from the worldisround.com - click
Via press release: (Santa Barbara, Calif.) –– Scientists can use cylinders as small as  teapots to study the mechanisms involved in powerful hurricanes and  other swirling natural phenomena.
The earth’s atmosphere and its molten outer core have one thing in  common: Both contain powerful, swirling vortices. While in the  atmosphere these vortices include cyclones and hurricanes, in the outer  core they are essential for the formation of the earth’s magnetic field.  These phenomena in earth’s interior and its atmosphere are both  governed by the same natural mechanisms, according to experimental  physicists at UC Santa Barbara working with a computation team in the  Netherlands.

Using laboratory cylinders from 4 to 40 inches high, the team  studied these underlying physical processes. The results are published  in the journal Physical Review Letters.
“To study the atmosphere would be too complicated for our purposes,”  said Guenter Ahlers, senior author and professor of physics at UCSB.  “Physicists like to take one ingredient of a complicated situation and  study it in a quantitative way under ideal conditions.” The research  team, including first author Stephan Weiss, a postdoctoral fellow at  UCSB, filled the laboratory cylinders with water, and heated the water  from below and cooled it from above.
Due to that temperature difference, the warm fluid at the bottom  plate rose, while the cold fluid at the top sank –– a phenomenon known  as convection. In addition, the whole cylinder was rotated around its  own axis; this had a strong influence on how the water flowed inside the  cylinder. Rotation, such as the earth’s rotation, is a key factor in  the development of vortices. The temperature difference between the top  and the bottom of the cylinder is another causal factor since it drives  the flow in the first place. Finally, the relation of the diameter of  the cylinder to the height is also significant.
Ahlers and his team discovered a new unexpected phenomenon that was  not known before for turbulent flows like this. When spinning the  container slowly enough, no vortices occurred at first. But, at a  certain critical rotation speed, the flow structure changed. Vortices  then occurred inside the flow and the warm fluid was transported faster  from the bottom to the top than at lower rotation rates. “It is  remarkable that this point exists,” Ahlers said. “You must rotate at a  certain speed to get to this critical point.”
The rotation rate at which the first vortices appeared depended on  the relation between the diameter and the height of the cylinder. For  wide cylinders that are not very high, this transition appeared at  relatively low rotation rates, while for narrow but high cylinders, the  cylinder had to rotate relatively fast in order to produce vortices.  Further, it was found that vortices do not exist very close to the  sidewall of the cylinder. Instead they always stayed a certain distance  away from it. That characteristic distance is called the “healing  length.”
“You can’t go from nothing to something quickly,” said Ahlers. “The  change must occur over a characteristic length. We found that when you  slow down to a smaller rotation rate, the healing length increases.”
The authors showed that their experimental findings are in keeping  with a theoretical model similar to the one first developed by Vitaly  Lazarevich Ginzburg and Lev Landau in the theory of superconductivity.  That same model is also applicable to other areas of physics such as  pattern formation and critical phenomena. The model explains that the  very existence of the transition from the state without vortices to the  one with them is due to the presence of the sidewalls of the container.  For a sample so wide (relative to its height) that the walls become  unimportant, the vortices would start to form even for very slow  rotation. The model makes it possible to describe the experimental  discoveries, reported in the article, in precise mathematical language.
###
The other UCSB author is postdoctoral fellow Jin-Qiang Zhong.  Additional authors are Richard J. A. M. Stevens and Detlef Lohse from  the University of Twente and Herman J. H. Clercx from Eindhoven  University of Science and Technology, both in the Netherlands.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e86e1a064',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Nearly 1.5m students around the world walked out of school on March 15 2019 to protest about the failure of the world’s governments to tackle climate change. The young climate strikers are forcing climate change onto the news agenda but researchers have warned that without a way to mobilise their passion in the long-term, the momentum they’ve generated for climate action could be lost. In this first issue of Imagine, we asked academics how the strikes can translate into long-term impact. One researcher proposes directly channelling the energy of young people into climate action with a national service for the environment. Others tell us how youth enthusiasm can play an integral part in changing climate policy around the world – and what it all means for tackling this huge issue. What is Imagine? Imagine is a newsletter from The Conversation that presents a vision of a world acting on climate change. Drawing on the collective wisdom of academics in fields from anthropology and zoology to technology and psychology, it investigates the many ways life on Earth could be made fairer and more fulfilling by taking radical action on climate change. You are currently reading the web version of the first issue. Here’s how this issue appears when sent to your inbox. To get new issues delivered straight to your inbox, subscribe now.  1. Global temperatures are on the rise. 2. The US bears an “extraordinary responsibility to respond to the climate crisis”, says D.T. Cochrane, Lecturer in Business and Society at York University, Canada. The country produces an “excessive amount of emissions” and has an unequal share of resources. 3. Business as usual is not an option. Michelle Bloor, Principal Lecturer and Environmental Programme Manager at University of Portsmouth, argues that a volunteer force of conservationists could offer experience and training to young people and ensure there are eager applicants for the vital work of helping the world’s species and habitats most threatened by climate change. Young people could get on the act straightaway, from replanting mangrove swamps in Vietnam and helping reintroduce beavers in Scotland to measuring coastal pollution in Senegal. Bloor groups the work a national service for the environment could cover into four categories: Data collection – by surveying wildlife abundance or measuring water quality in lakes and rivers, volunteers could help scientists understand how ecosystems are changing. Green construction – restoring wooded habitat could absorb carbon and create corridors which connect pockets of wildlife in fragmented habitats. Large-scale construction projects could involve volunteers working on habitat highways – green corridors which help wildlife cross road networks. Species reintroduction – helping ecosystem engineers, such as beavers, return could help the process of expanding natural habitats. These animal recruits could create new dams and lakes, which provide new opportunities for more species to thrive. Reforestation – humans have cut down three trillion trees since the dawn of agriculture – around half the trees on Earth. A mass reforestation effort would need plenty of volunteers worldwide, something a youth volunteer force could supply. In the UK, increasing total forest cover to 18% could soak up one third of the required carbon emission cuts needed by 2050, according to the 2008 Climate Change Act. 


      Read more:
      National service for the environment – what an army of young conservationists could achieve


 A conservation army of millions was active in 1930s America The idea of enlisting millions of young people in conservation work is not new. It has origins in a public work relief programme from the 1930s. During the depths of the Great Depression and while the Dust Bowl ravaged rural America, US president Franklin Roosevelt implemented a series of reforms as part of the New Deal to implement a more sustainable land policy and revive economic growth. One of those reforms was the creation of the Civilian Conservation Corps (CCC). It enlisted 3m young men who planted over two billion trees on more than 40m acres of land between 1933 and 1942. Their aim was to repair ecosystems throughout the US with hundreds of projects in forestry and conservation. A national service for the environment would see individuals taking a direct role in mitigating climate change, but there is also an emerging political project aiming to capitalise on public support for action. The Green New Deal – an ideological heir to Roosevelt’s plan – is energising debate on climate action in the US. Endorsed by Congresswoman Alexandria Ocasio-Cortez and numerous 2020 presidential candidates, the Green New Deal is a plan to enact a “green transition” in society and the economy within the next ten years. The idea has attracted worldwide attention, including in the UK, where members of the Labour Party are urging the party’s leadership to adopt a similar plan as policy. What is the Green New Deal? The Green New Deal is a proposed series of reforms with three broad aims: To eliminate greenhouse gas emissions from energy, transport, manufacturing and other sectors of the economy within ten years. To create full employment in the manufacture of clean energy infrastructure and other essential work. To redistribute wealth and tackle social and economic inequality. Rebecca Willis, Researcher in Environmental Policy and Politics at Lancaster University says: Alongside an aim for net-zero greenhouse gas emissions and 100% renewable energy, the Green New Deal demands job creation in manufacturing, economic justice for the poor and minorities, and even universal healthcare through a ten-year “national mobilisation”, which echoes president Franklin Roosevelt’s New Deal in the 1930s. 


      Read more:
      The Green New Deal is already changing the terms of the climate action debate


 Decarbonisation to become a zero-carbon society What would decarbonisation involve? The Green New Deal entails shifting electricity generation from coal and natural gas to wind, solar, hydroelectric and other zero-carbon technologies. The decarbonisation process may require an emergency mobilisation effort akin to that seen in World War II. Because, according to Kyla Tienhaara, Canada Research Chair in Economy and Environment at Queen’s University, Ontario, the scale and speed of decarbonisation needed today cannot be delivered by carbon taxes alone: The carbon price has to be incredibly high and cover a broad swathe of the economy to significantly reduce greenhouse gas emissions. Governments haven’t shown a willingness to do this and recent research suggests that even steep prices will not produce the deep emissions reductions required to limit global warming to under 2°C. 


      Read more:
      America can afford a Green New Deal – here's how


 Will people lose jobs because of the Green New Deal? The Green New Deal resolution guarantees full employment, but Fabian Schuppert, Lecturer in International Political Theory and Philosophy at Queens University Belfast, believes its promised changes to the economy would have immediate consequences for workers in many industries which rely on fossil fuels. Job losses in sectors such as coal mining and manufacturing could erode popular support for a Green New Deal and harm the plan’s commitment to a just transition, he argues. A just transition is a commitment to ensure the costs of a transition from fossil fuels – such as tax rises and redundancies – aren’t forced on working people. Schuppert suggests that introducing a universal basic income – a guaranteed payment to everyone in society without means-testing – would help cushion the initial shock of a green transition by providing people with support while they look for new jobs or training. In the long run, he argues, it could have broader social effects: A universal basic income might offer citizens time to engage in fulfilling community-based work that doesn’t generate profit but which has social value. Taking them out of their cars in long lines of commuter traffic and putting them in allotments growing food or in parks enjoying nature could help usher a whole new way of life. 


      Read more:
      Green New Deal: universal basic income could make green transition feasible


 Does the US have the money for a Green New Deal? This is arguably the question most often asked of the Green New Deal. Edward Barbier, Professor of Economics at Colorado State University, says it does and has some suggestions: Passing a carbon tax is one of the best ways to go. A US$20 tax per metric ton of carbon that climbs over time at a pace slightly higher than inflation would raise around US$96 billion in revenue each year – covering just under half the estimated cost. At the same time, it would reduce carbon emissions by 11.1 billion metric tons through 2030. Redirect subsidies currently given to fossil fuel companies. Those subsidies are estimated to be around US$5 trillion a year globally, 6.5% of global GDP.  Raise taxes on the highest-earning Americans. Imposing a 70% tax on earnings of US$10m or more would bring in an additional US$72 billion a year … 


      Read more:
      America can afford a Green New Deal – here's how


 In an article for CNN, economist Jeffrey Sachs of Columbia University also argued that the Green New Deal is “feasible and affordable”. But climate justice is still a grey area with the Green New Deal While one of the central aims of the Green New Deal is to redistribute wealth and tackle social and economic inequality in the US, its impact on poorer parts of the world has perhaps been less discussed. Olúfẹ́mi O. Táíwò, Assistant Professor of Philosophy at Georgetown University, says that climate justice must not end at the borders of a country implementing a Green New Deal. Otherwise, he states, the Green New Deal may become “the next chapter in a long history of US industrial policies that have oppressed people”. Táíwò believes there is a risk that a Green New Deal could spark a race for vast territory on which to build solar farms or grow biofuel crops. In the process, historic injustices could be perpetuated through “climate colonialism”. He says: A research institute reported in 2014 that Norwegian companies’ quest to buy and conserve forest land in East Africa to use as carbon offsets came at the cost of forced evictions and food scarcity for thousands of Ugandans, Mozambicans and Tanzanians. The Green New Deal could encourage exactly this kind of political trade-off. 


      Read more:
      How a Green New Deal could exploit developing countries


 The contradiction at the heart of the Green New Deal Matthew Paterson, Professor of International Politics at Manchester University says that new infrastructure and redistribution proposed by the Green New Deal may boost carbon emissions: Many of the measures proposed – such as investing in infrastructure and spreading wealth more evenly – will intrinsically work in tension with efforts to decarbonise the economy. They create dynamics that increase energy use at the same time as other parts of the Green New Deal are trying to reduce it. For example, building infrastructure such as new road networks will both create demand for carbon-intensive cement manufacture and opportunities for more people to travel by car. 


      Read more:
      The Green New Deal's contradiction – new infrastructure and redistribution may boost carbon emissions


 Other academics like Joe Herbert, a researcher at Newcastle University, have argued that sustaining emission reductions in the long term can only be achieved by managed degrowth of the economy. As the Green New Deal develops and its policy details are refined, its proponents may choose to adopt such novel ideas. At such an early stage in the Green New Deal’s development as a political project, much of the discussion around it remains speculative. However, Rebecca Wills argues that it has already achieved something by reinvigorating the debate over climate action: The Green New Deal is already succeeding in putting climate action where it belongs, as the defining political issue of our time. How strange that we have the current US political environment to thank for this huge step forward. 


      Read more:
      The Green New Deal is already changing the terms of the climate action debate


 Michelle Bloor believes that including her vision of a national service for fighting climate change within the aims of a Green New Deal could help galvanise support for the latter, by providing an outlet for some of the enthusiasm of young people who have taken part in the climate strikes. Building a coalition for radical climate action under the Green New Deal is likely to lead the ongoing strategy of the project. Bloor believes that mobilising the growing youth movement is a good place to start. 
Click here to subscribe to Imagine. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterYet another German solar manufacturer appears to be reaching the end of the line.
The German TAZ here reports that profits for SMA Solar in 2011 fell by more than a half, to 166 million euros.
In light of the fall in prices in the branch and planned cuts in subsidies in Germany, SMA anticipates a further decrease in sales and profitability.”
Yet, SMA director Pierre-Pascal Urbon is still optimistic. To avert the ill fate of other solar manufacturers in Germany, like Q-Cells and Solarworld, SMA aims expand business in foreign markets. China is a huge market, but Chinese manufacturers are subsidized by the Chinese government, which distorts competition, Urbon claims. Imagine that – subsidies distorting competition. That of course would never happen in Germany or Europe, now would it?  (sarc off)
SMA sales in 2011 sank from 1.9 billion euros to 1.7 billion. The TAZ writes:
This year revenue is expected to sink to 1-2 to 1.5 billion euros, the company said.
The main reason for the expected decline are the reduced feed-in rates that the government mandates for producers of solar energy in Germany, which go into effect on April 1, 2012. The change in feed-in tariffs will result in drops of up to 40%,
Because of Germany’s overly generous feed-in tariff paid to solar power producers in the past, half of the world’s solar power generation capacity is said to have been installed in Germany, a country that gets as much sunshine as Alaska.
Share this...FacebookTwitter "
"
What an ice free Arctic might look like from space
We all know how much NSIDC’s Dr. Mark Serreze has been touting the idea of the “Arctic death spiral“,  and we’ve had predictions of ice free summers in 2008, 2013, 2015, 2020, 2030, 2040, 2050, 2060, 2070, and 2100 to name a few. Other forecasts don’t give specific dates but say things like within 5 years,  10 years, 20 years, 30 years, 100 years, decades, and sooner than expected. Such “all over the road forecast certainty” doesn’t really build any confidence that any of these climate soothsayers have any idea when or even if the Arctic will be “ice free” in the summer in the next 100 years.
Now, inconveniently, we have this new paper via ScienceDirect New insights on Arctic Quaternary climate variability from palaeo-records and numerical modelling which says that their studies show that the early Holocene might very well have had ice free summers. This is interesting, because as this generally well accepted graph shows, temperature was higher then. But there’s more.


From the description for this graphic: The main figure shows eight records of local temperature variability on multi-centennial scales throughout the course of the Holocene, and an average of these (thick dark line). (to 10000 BC-2000CE (from 0 — 12000 BP)) The records are plotted with respect to the mid 20th century average temperature, and the global average temperature in 2004 is indicated. An inset plot compares the most recent two millennia of  the average to other recent reconstructions. At the far right of this  plot it is possible to observe the emergence of climate from the last glacial period of the current ice age.  During the Holocene itself, there is general scientific agreement that  temperatures on the average have been quite stable compared to  fluctuations during the preceding glacial period. The above average  curve supports this belief. However, there is a slightly warmer period  in the middle which might be identified with the proposed Holocene climatic optimum. The magnitude and nature of this warm event is disputed, and it may have been largely limited to high northern latitudes.
But, the other rub of the early Holocene is CO2 in the atmosphere. We know from ice core records that CO2 concentration has varied with ice ages.  Coming out of the last ice age into the Holocene, we know that atmospheric CO2 concentrations rose as CO2 came out of the oceans as they warmed. This graph from the American Association for the Advancement of Science (AAAS) shows that the early Holocene (~10,000 years before present), had a rise coming out of the ice age and then had CO2 concentrations stabilize lower than that of today, about 260-270 ppm:

Figure 1. Top: One sigma-calibrated age ranges for the 14C control points 1, 2 and 6 as an indicator of the possible age range of the CO2 record reconstructed from stomatal frequency. The labels are the same as in Wagner et al. (1). Center and Bottom: Atmospheric CO2 concentration reconstructed from stomatal index () (1) and direct measurements of CO2 concentration of air enclosed in bubbles in the ice cores from Taylor Dome () (3, 4) and Vostok () (7, 8).
This new paper in the journal Quaternary Science Reviews throws a formidable monkey wrench into the the theory that CO2 induced warming is the cause of current Arctic ice loss. Because if we had ice free summers ten thousand years ago at ~ 260 ppm CO2, and we had warmer temperatures than today, we can’t then conclude that an additional 100 ppm of CO2 since then would be the cause of an ice free summer in the Arctic today. And ice free summer at lower CO2 and higher temperature is an incongruity with today’s theory of the “Arctic Death Spiral”.
Here’s the paper abstract:
 
Quaternary Science Reviews
New insights on Arctic Quaternary climate variability from palaeo-records and numerical modelling

Martin Jakobssona, , , Antony Longb, Ólafur Ingólfssonc, Kurt H. Kjærd and Robert F. Spielhagene



a Department of Geological Sciences, Stockholm University, 106 91 Stockholm, Sweden
b Department of Geography, Durham University, Science Site, South Road, Durham DH1 3LE, UK
c Faculty of Earth Sciences, University of Iceland, Is-101 Reykjavik, Iceland
d Centre for GeoGenetics, Natural History Museum, University of Copenhagen, Øster Voldgade 5-7, DK-1350 Copenhagen, Denmark
e Academy of Sciences, Humanities and Literature, Mainz, and Leibniz  Institute of Marine Sciences, IFM-GEOMAR, Wischhofstr. 1-3, D-24148  Kiel, Germany


Accepted 26 August 2010.
Available online 2 October 2010.


Abstract
Terrestrial and  marine geological archives in the Arctic contain information on  environmental change through Quaternary interglacial–glacial cycles. The  Arctic Palaeoclimate and its Extremes (APEX) scientific network aims to  better understand the magnitude and frequency of past Arctic climate  variability, with focus on the “extreme” versus the “normal” conditions  of the climate system. One important motivation for studying the  amplitude of past natural environmental changes in the Arctic is to  better understand the role of this region in a global perspective and  provide base-line conditions against which to explore potential future  changes in Arctic climate under scenarios of global warming. In this  review we identify several areas that are distinct to the present  programme and highlight some recent advances presented in this special  issue concerning Arctic palaeo-records and natural variability,  including spatial and temporal variability of the Greenland Ice Sheet,  Arctic Ocean sediment stratigraphy, past ice shelves and marginal marine  ice sheets, and the Cenozoic history of Arctic Ocean sea ice in general  and Holocene oscillations in sea ice concentrations in particular. The  combined sea ice data suggest that the seasonal Arctic sea ice cover was  strongly reduced during most of the early Holocene and there appear to  have been periods of ice free summers in the central Arctic Ocean. This  has important consequences for our understanding of the recent trend of  declining sea ice, and calls for further research on causal links  between Arctic climate and sea ice.
~~~~~~~~~~~~~~~~~~~~~~~~~~

Fig. 1. Map showing the locations of some of the studies included in the papers presented in this special issue. Numbers refer to Table 1,  which contains the references to the respective study. Some of the  papers on the Arctic Ocean involve sediment cores from a large spatial  area; these are only plotted with boxes enclosing the areas of the  studied cores. Furthermore, Cronin et al. (2010) analyzed sediment cores from virtually the entire central Arctic Ocean  and, therefore, there is no number representing that study on the map.  The maximum extensions of the Eurasian Ice Sheet during the late  Quaternary compiled by the QUEEN project (Svendsen et al., 2004)  are shown. LS: Late Saalian (>140 ka), EW: Early Weichselian  (100–80 ka), MW: Middle Weichselian (60–50 ka), LGM: Late Weichselian  (25–15 ka). The speculative extent of an MIS 6 ice shelf inferred by Jakobsson et al. (2010) is shown by the hatched area enclosed by a gray stippled line. The  approximate spatial minimum cover of sea ice during 2007 is shown with a  white shaded area enclosed by a black stippled line as a comparison to  the median extension for the period 1979–2005 shown by a blue stippled  line (Data is from National Snow and Ice Data Center). MJR: Morris Jesup  Rise; YP: Yermak Plateau. (For interpretation of the references to  colour in this figure legend, the reader is referred to the web version  of this article.)
================================
h/t to WUWT reader “josh”
Addendum: Some follow up graphic from comments, in my response to Richard Telford:
Here’s an interesting plot of solar insolation at 65 degrees north over time. To give readers an idea of this line, here is a map:

(Map from WikiMedia) Fairbanks, AK is at 64.5° N
The plot below shows how insolation varied with the Milankovitch cycles at 65° N. I’ve added the deltas comparing 10KYA to present.

The “Fermi Paradox” blogger who originally made the graph I annotated wrote: The graph shows the insolation in W/m^2 at 65 degrees norther latitude from 20ky before present to 10 ky in the future, calculated with the program insola from J. Laskar et al. The four plots are for the two months after the summer solstice and the two months before. It can be seen that the change in insolation over time is quite significant. Note though that this only applies at high latitudes – the global mean barely changes at all.
Note the magnitude of the change in insolation from 10K years ago to present, from 15 to 40 Watts/m2
Now look at this image from NOAA’ s Environmental Research Laboratory (ESRL):

CO2 accounts for 1.4 Watts/m2 of forcing in the last 150 years, so compared to the forcings of the Milankovitch cycles (at least at 65N) it is an order of magnitude lower. My point is that given the small impact of CO2 in forcings, it is not likely to be the driver of Arctic ice melt in the present, just like it wasn’t much of a significant factor 10K years ago.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e87bd7e30',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Wildlife populations are declining globally, but it’s not all doom and gloom. We’re in the midst of an exciting time for UK mammals. There are beavers and wild boar living free in the UK again. Otter populations are recovering and can now be found in all English counties. Polecats are expanding their range and pine martens, with a little assistance, are growing in number. Nevertheless, the information we have on many of these species is still very limited, making it difficult to understand the bigger picture. With a growing human population, it’s more important than ever that scientists and the public work together to monitor mammals effectively. Only with accurate information can conservation benefit both wildlife and the people living alongside it. 


      Read more:
      How to stop the humble hedgehog disappearing from British gardens and countryside forever


 Unfortunately, there’s little data on many British mammal species, and this prevents precise population estimates. With limited historical data, too, it’s difficult to know if populations are becoming more or less abundant and why. Without this information, it’s hard to say if conservation is needed. Important debates on issues such as badger culling and fox hunting may also be ill informed. Many mammals are nocturnal and elusive so people are unlikely to come across them. More visible species, such as rabbits or grey squirrels, are so common that people are unlikely to keep a record of sightings. To ensure the successful protection and management of Britain’s mammal community, there need to be effective ways of monitoring them long term. One technique that has proved successful in the study of mammals is the use of camera traps. These are motion-sensitive cameras that are triggered to take a photograph or short film when an animal moves in front of them. These cameras are battery powered and can be left in place for weeks or even months at a time, recording wildlife. Although some animals seem curious about the cameras, they cause less disturbance than humans would. Once set up, a camera trap can collect lots of footage – meaning large amounts of data for scientists to search through to identify species. This is one area in which the public can help. I recently started working on MammalWeb – a citizen science project that invites people to help build a better understanding of the UK’s mammals through camera trapping. People can participate by setting up a camera trap in their garden, or on any land they have permission to access. This makes it possible to have more cameras in the field, spread out across a wider area than any single researcher could manage on their own, generating a more comprehensive data set. Everyone, including those without their own camera trap, can contribute by identifying which animals are present in photos collected by other participants. There are over 500,000 photos in the MammalWeb database – nearly 250,000 uploaded by members of the public, and others by researchers seeking help with classifying species in images they’ve collected. More than 500 people have helped make 500,000 classifications, but as images must be classified by multiple people to ensure accuracy, more classifications are always needed. Participants have recorded 34 mammal species, ranging from the largest UK land mammal – the red deer – right down to some of the smallest, such as bank voles, captured using specially modified camera traps. Many of the participants were surprised by what the animals were doing in their own back gardens. There’s the typical predatory behaviour of foxes hunting pheasants and the more unusual behaviour of badgers predating hedgehogs. This behaviour among badgers may be contributing to a decline in hedgehog populations, but the camera traps have found evidence that they can coexist happily, too. One particularly surprising find was a North American raccoon (Procyon lotor), captured living wild in the north-east of England. Thanks to these records, the authorities were able to locate the raccoon and transfer it to a local zoo to be looked after. This highlights how easily wild mammals can go unobserved. It’s unknown how long the raccoon was roaming free and, without the aid of the public and their camera traps, we may never have known about it. While a single raccoon may not seem like a serious conservation issue, non-native species can spread rapidly, with serious consequences for native wildlife. The raccoon is not the only American visitor to have made itself at home in the UK. American mink, which are threatening water vole populations, have been recorded, and American grey squirrels, which compete with native red squirrels, are the most common mammal sighted on MammalWeb – although recovering pine marten populations may help to balance the odds and aid red squirrel recovery. Volunteers are assisting NatureSpy, a non-profit organisation working on wildlife research and community engagement that MammalWeb is partnering with, in its search for the elusive pine marten in North Yorkshire as part of their Yorkshire Pine Marten Support Programme which followed video footage of a single pine marten in 2017.  There hasn’t been another caught on camera yet, but continually monitoring the area offers the best chance of spotting the animals as they move into new areas. This will help conservationists understand where and when this species is dispersing and where help can be given. Camera traps offer fascinating insights into the secret lives of Britain’s mammals. With the help of ordinary people, we can all learn more about them, and how to look after them well into the future."
"The deep sea is the largest habitat on earth and incredibly important to humans, but it faces many threats – from increased human exploitation to the effects of climate change. As this exploitation expands we need to decide to what extent we will try to conserve the oceans, and this is a decision that must be informed by what the oceans provide. This was the motivation for our study, published in the journal Biogeosciences. One of the constant challenges faced by deep-sea researchers is the impression often held that the deep-sea is too unknown and remote to be important to humans. This makes it an uphill battle to explain why our research is of more than just scientific interest. The fact that the deep oceans are largely in international waters outside national jurisdiction means that its importance is truly global, because it is only at a planet-wide level that they can be properly managed. We are on the verge of expanding the range of what we take from the sea: developments such as manganese nodule mining are closer to fruition, terrestrial supplies of rare earth elements are dwindling making the massive reservoirs under the ocean more attractive, and mining claims for massive sulfides, rich in elements used in electronics, are likely to start in the next year.  This is the time to discuss deep-sea stewardship before it is already underway. Already commercial fishing has expanded to encompass more species, taking more from the sea with ramifications for the entire marine ecosystem. We need to realise how these ecosystems are important to human society now, so that those who are beginning to harvest yet more resources from the oceans can see what impact that may have.  In addition to summarising what the deep sea provides to society already, we also emphasise that a different approach needs to be taken. For example, manganese nodules take centuries or longer to form and are not renewable.  Many commercial fish stocks are in decline – even with precautionary management. Fishing is often treated more as mining than management of a living resource; no time is allowed for certain slow-growing deep-sea fish stocks to recover resulting in the complete loss of the stock. It’s not just the biological resources that are important. Because of its vast size, the physical and chemical aspects of the ocean essentially shape and regulate the way the planet works.  One of the most important services provided by the deep sea is its role in gas cycling: as one of the largest sinks for greenhouse gases on the planet, the world would not look like it does now without the deep sea. Already the ocean has absorbed a massive amount of the CO2 we have released, bearing the brunt of the human impact on the climate. There are also vast reservoirs of methane under the seafloor, which is consumed by bacterial and biological processes when it is released, preventing it from reaching the atmosphere and exacerbating the effects of global warming. Also vital is the remineralisation of nutrients  which provides the nitrogen, phosphate, and other nutrients needed to sustain the most productive surface fisheries on the planet.  When that deep, nutrient rich water comes to the surface it stimulates plankton to create the base of the most productive marine food webs, which in turn feed billions of humans. The most important result of this study is to highlight the diversity of what ocean habitats offer to humans for our benefit, from the deep-sea trenches to vents and seeps. While this vast environment still contains a staggering list of unknowns, what we do know highlights the extent of what it provides to society.  From materials for jewellery or electronics to oil and gas and other future potential energy reserves or novel pharmaceuticals, its value should be recognised. In this way, as we decide to use it more in the future we work to avoid damaging or losing the services it already provides.   What the ocean provides reaches all across humankind, from fishers providing food or income from the sea, directly or indirectly, or those far removed from the ocean whose climates are changing less fast than they would if the deep ocean was not absorbing 25-50% of all the CO2 already released into the atmosphere since the industrial revolution. The importance of these services makes it less like many of the other systems that are managed and so the traditional methods and frameworks need to be changed. We hope that the managers, stakeholders and scientists involved with the ocean’s resources will be able to use our work to develop their understanding of the interconnected nature of everything the deep sea has to offer."
"**The hospitality sector will be ""decimated"" by the new Covid tiers, according to bar chain owner Martin Greenhow, who says it ""isn't viable to operate"" under the conditions.**
Mr Greenhow, who has bars in cities including Manchester, says the measures are ""a mortal blow"" to the sector.
The hospitality industry has warned that tens of thousands of businesses will close without extra support.
It comes as more pub groups have been forced to make additional job cuts.
Mitchells & Butlers, owner of the All Bar One and Harvester chains, revealed it had cut 1,300 jobs while Fuller Smith & Turner made 350 redundancies.
The government has set out what level of restrictions England's regions will face when lockdown ends with cities such as Manchester, Birmingham and Newcastle put in the highest tiers.
But chains such as Mr Greenhow's Mojo bars were struggling even before the second lockdown in England, imposed on 5 November.
On the Friday before lockdown, Mr Greenhow's Manchester bar took Â£175. On the same Friday night a year before, it took Â£10,000.
Even tier one means that bars have less than half the usual number of customers, he says.
""It's simply not a business model that can work,"" he adds. ""Right now, for hospitality, all the tiers are a version of waterboarding. We're allowed out for a brief gasp of fiscal oxygen, then we're slammed back down.""
""This is pure and simple business torture.""
UK Hospitality boss Kate Nicholls said the sector is ""bearing the brunt of the pain of closure"" under the new Covid rules. She added that tens of thousands of businesses will close without additional support.
Under the new restrictions, pubs in tier 2 regions can only open if they serve substantial meals and households are not allowed to mix indoors.
Under tier 3, pubs and restaurants must close their doors but can offer takeaways.
Ms Nicholls said that 98% of its members were in areas with tier 2 or tier 3 coronavirus restrictions, and nearly nine in 10 ""say that they are not viable to operate at those level of restrictions"".
""Without additional support to sustain these businesses through this crisis, we are going to see tens of thousands of businesses closing and over a million job losses,"" she added.
Birmingham City Council leader Ian Ward said hospitality and other businesses needed a ""meaningful package"" of support from the government so the economy can ""continue to function in an effective way"".
""The crisis faced by hospitality businesses across Birmingham is of particular concern from an economic perspective - a crisis that would have been exacerbated whether our city was placed in tier 2 or 3,"" said Mr Ward.
""Many businesses in this previously thriving sector are warning they may not survive the coming months if they are dealt the double blow of more restrictions and inadequate financial support.""
The Night Time Industries Association (NTIA) described the imposition of the tier three level as ""devastating news"" for those areas.
""The government must compensate these businesses for the period of time they have been closed, and the loss of business suffered due to restrictions through the festive period,"" said NTIA chief executive Michael Kill.
The British pub industry sent a letter on Wednesday pleading with Prime Minister Boris Johnson to save the industry, which it said was facing ""the darkest of moments""."
"

If his Miami speech on January 27 serves as any guide, Mitt Romney may be missing a great opportunity to connect with the youth (18–29) vote. Here’s what he said:



Our young people have a great deal of concern. They’re a very humanitarian people. They’re concerned about issues like global warming and things of that nature, and they’re concerned about humanity



Judging from the most recent _Survey of Young Americans_ from Harvard’s Institute of Politics (IOP), Romney’s got it about 100% wrong. By and large today’s young Americans are self‐​interested, isolationist, want handouts, and they rank global warming last among domestic issues. Sounds like the general public to me!



Harvard’s methodology is a bit obtuse but yields interesting results. It samples twenty issues in one‐​on‐​one comparisons. For example, it asks a subsample of its 3,000+ respondents, “which do you think is more important, combating the impacts of climate change or addressing social security”; then climate change versus reducing the federal deficit, etc. (result: climate lost both comparisons handily).



I have rearranged their data in a different fashion that allows one to rank their 20 issues in order of descending importance.



Readers may have seen a very incomplete version of this by Charles Blow in last Saturday’s _New York_ _Times_. Probably to save space, only “domestic” issues were shown, but another matrix, with the remaining “international” ones is available at their online site.



The only place that I have been able to find the raw matrix is at the IOP site, and it lumps domestic and international in the same chart. This gives a slightly different impression with regard to climate change.



Anyway, I gave each issue one point each time it “beat” a competitor issue head‐​on, regardless of whether the margin was statistically significant (almost all were, thanks to the large sample size).



Here we go; I also indicate which were listed as “Domestic” or “International” in the _Times’_ presentation



1\. Jobs and unemployment DOM



2\. Ensuring affordable access to health care DOM



4\. Addressing Social Security (tie) DOM



4\. Creating a world‐​class education system (tie) DOM



4\. Lowering the tax burden for all Americans (tie) DOM



4\. Becoming energy independent (tie) DOM



7\. Reducing the federal deficit DOM



9\. Protecting individual liberties from government (tie) DOM



9\. Preventing the spread of terrorism (tie) INT



10\. Withdrawing from Afghanistan (tie) INT



10\. Preventing Iran from acquiring a nuclear weapon (tie) INT



12\. Addressing income inequality DOM



13\. Developing a comprehensive immigration policy DOM



14\. Reducing the role of big money in U.S. elections DOM



(There is a large drop‐​off in support beneath this level)



16\. Promoting peaceful resolution to Israel‐​Palestine (tie) INT



16\. Promoting stable democracy in the Middle East/​North Africa (tie) INT



16\. Combating the impacts of climate change (tie) DOM



18\. Countering China’s rising influence INT



19\. Solving the European debt crisis INT



20\. Re‐​integrating North Korea into the world community INT



So what’s “humanitarian” here? “Affordable” health care sounds like “I want someone else to pay my doctor fees” The surveyed group certainly has a conflict of interest about a “world‐​class education”, given that many of the respondents were in some type of college or university. Lowering my taxes? No. The mirage of energy independence? Reducing the deficit? Getting the snoopy government out of my life? No, no, no.



And what about the dinosaur media’s mantra that it is the “young people” who are most concerned about climate change? Of all the issues designated “domestic”, it’s at rock bottom. But is it really a domestic issue? After all, any effective climate change mitigation program is going to have a substantial international component. So let’s call it both.



In that case, it comes in ahead of three other international issues, including a humanitarian one—“Solving” (whatever that means) the European debt mess. I think our students are writing the headline: **Youth to Euros: Drop Dead _._**



Mr. Romney would do well to revisit his January assessment of young voters in light of the Harvard survey. Let Obama appeal to their penumbra of altruism. Instead, campaign for this demographic by promoting economic development, jobs, social security reform (read: investment in things other than our insolvent government), and reforming higher education by concentrating on teaching and research instead of the armies of administrators that are now employed to deal with the federal leviathan. And stay away from climate change, the Far East, and bailing out the Euros.
"
"
Skywatcher Michael Jäger of Stixendorf, Austria, took this photo of the newfound comet McNaught C/2009 R1 on June 6, 2010, while the comet was visible in the northeastern morning sky. Image via Space.com
From University of Chicago Press Journals: New research challenges the controversial  theory that an ancient comet impact devastated the Clovis people, one of  the earliest known cultures to inhabit North America.
Writing in the October issue of Current Anthropology, archaeologists Vance Holliday (University of Arizona) and David Meltzer  (Southern Methodist University) argue that there is nothing in the  archaeological record to suggest an abrupt collapse of Clovis  populations. “Whether or not the proposed extraterrestrial impact  occurred is a matter for empirical testing in the geological record,”  the researchers write. “Insofar as concerns the archaeological record,  an extraterrestrial impact is an unnecessary solution for an  archaeological problem that does not exist.”  
The comet theory first emerged in 2007 when a team of scientists  announced evidence of a large extraterrestrial impact that occurred  about 12,900 years ago. The impact was said to have caused a sudden  cooling of the North American climate, killing off mammoths and other  megafauna. It could also explain the apparent disappearance of the  Clovis people, whose characteristic spear points vanish from the  archaeological record shortly after the supposed impact.
As evidence for the rapid Clovis depopulation, comet theorists point  out that very few Clovis archaeological sites show evidence of human  occupation after the Clovis. At the few sites that do, Clovis and  post-Clovis artifacts are separated by archaeologically sterile layers  of sediments, indicating a time gap between the civilizations. In fact,  comet theorists argue, there seems to be a dead zone in the human  archaeological record in North America beginning with the comet impact  and lasting about 500 years.

Caption: This image shows the excavations at the Lubbock Lake  site, on the High Plains of Texas. The crew is working in the laminated  lake beds dated 13,000 to 12,000 years old. The time of the purported  extraterrestrial impact would be at the base of the lake beds. The pale  olive yellow layer below contains Clovis-age bone. The black layers  represent a marshy valley bottom and contain archaeological bone beds  (with butchered remains of extinct bison). The white layers are  archaeologically “sterile” because they represent standing lake water  (probably 1 to 2 m deep). Thus, the presence of “sterile” zones between  occupation layers has no bearing on the issue of an impact and people.
Credit: Vance Holliday
But Holliday and Meltzer dispute those claims. They argue that a  lack of later human occupation at Clovis sites is no reason to assume a  population collapse. “Single-occupation Paleoindian sites—Clovis or  post-Clovis—are the norm,” Holliday said. That’s because many  Paleoindian sites are hunting kill sites, and it would be highly  unlikely for kills to be made repeatedly in the exact same spot.
“So there is nothing surprising about a Clovis occupation with no  other Paleoindian zone above it, and it is no reason to infer a  disaster,” Holliday said.
In addition, Holliday and Meltzer compiled radiocarbon dates of 44  archaeological sites from across the U.S. and found no evidence of a  post-comet gap. “Chronological gaps appear in the sequence only if one  ignores standard deviations (a statistically inappropriate procedure),  and doing so creates gaps not just around [12,900 years ago] but also at  many later points in time,” they write.
Sterile layers separating occupation zones at some sites are easily  explained by shifting settlement patterns and local geological  processes, the researchers say. The separation should not be taken as  evidence of an actual time gap between Clovis and post-Clovis cultures.
Holliday and Meltzer believe that the disappearance of Clovis spear  points is more likely the result of a cultural choice rather than a  population collapse. “There is no compelling data to indicate that North  American Paleoindians had to cope with or were affected by a  catastrophe, extraterrestrial or otherwise, in the terminal  Pleistocene,” they conclude.

Caption: These are Clovis Points.
Credit: David Meltzer
###
Vance T. Holliday and David J. Meltzer, “The 12.9-ka ET Impact Hypothesis and North American Paleoindians.” Current Anthropology 51:5 (October 2010).
Current Anthropology is a transnational journal  devoted to research on humankind, encompassing the full range of  anthropological scholarship on human cultures and on the human and other  primate species. The journal is published by The University of Chicago  Press and sponsored by the Wenner-Gren Foundation.
SMU is a private university in Dallas where nearly 11,000 students  benefit from the national opportunities and international reach of SMU’s  seven degree-granting schools. For more information see www.smu.edu.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88aee260',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
My Inbox exploded with tips today, this one in particular. This unbelievably vile video from the 10:10 campaign takes the award for the most disgusting climate and carbon reduction video ever. It is in a class by itself, which is off the scale. See also Ryan Maue’s post below this one on the 350.org tie in for 10:10.

What were they thinking? They weren’t, because this is going to have the exact opposite effect they intended it to have. I don’t have words to describe my disgust with the video.
WARNING: GRAPHIC VIDEO IMAGERY



Here is what they say about it on YouTube:

The1010Campaign | September 30, 2010
http://www.1010global.org/no-pressure
Whippersnapping climate campaign 10:10 teams up with legendary comic screenwriter Richard Curtis – you know, Blackadder, Four Weddings, Notting Hill, co-founded Comic Relief – and Age of Stupid director Franny Armstrong to proudly present their explosive new mini-movie “No Pressure”. The film stars X-Files’ Gillian Anderson, together with Spurs players past and present – including Peter Crouch, Ledley King and David Ginola – with music donated by Radiohead. Shot on 35mm by a 40-strong professional film crew led by director Dougal Wilson, “No Pressure” celebrates everybody who is actively tackling climate change… by blowing up those are aren’t.
I know people will be upset by this, please keep your comments civil – Anthony
=======================================================
RELATED STORIES:
Lower Than This They Cannot Stoop
Global Work Party Day on 10/10/2010: come up with your own event
UPDATE1: 
Some people in comments whether this is some sort of horrible spoof. It appears to be direct from 10:10, as the URL highlighted in yellow below on the YouTube description links directly to the 10:10 promotional web page:
http://www.1010global.org/no-pressure
which is a subpage of their main website.
http://www.1010global.org

UPDATE2: They are so proud of this “mini-movie” they did a “behind the scenes” video of it.h/t to WUWT reader “scarlet pumpernickel”.

UPDATE3: Hot Topic (an AGW proponent site) in New Zealand thinks this video is “obviously effective“


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e883e448e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Pictures of masked and gowned healthcare workers have become recurrent images as the Ebola virus continues to spread in West Africa. It is the largest Ebola virus outbreak ever recorded, and the death of approximately 70% of patients ensures that the disease remains headline news. There has been debate as to whether this outbreak is unprecedented in scale, now the concern is over its lack of control. Others are questioning how an outbreak happened.  In an editorial published in the journal PLOS Neglected Tropical Diseases, Daniel Bausch and Lara Schwarz from Tulane and McGill universities argue that these “unforeseen” outbreaks of Ebola are in fact more predictable than first thought. They say the socio-political landscape in West Africa, and particularly in Guinea, Liberia, and Sierra Leone, the nations most affected, is a key driver of the emergence of Ebola. They argue that regional poverty and governance issues force people to plunder the forests for resources, increasing the chance of viruses passing from animals to humans.  The impoverished health-care systems then facilitate Ebola transmission among humans. And the authors make a good point. As part of a team including ecologists, virologists, and social anthropologists, we have argued along much the same lines. Genetic evidence suggests the virus causing this outbreak is closely related to those that caused outbreaks in Gabon and the Democratic Republic of Congo in the 2000s, and the genetic differences are those collected during the virus’s journey from those locations. How, then, did it get to West Africa? Bausch and Schwarz pose the question as to why this Zaire ebolavirus strain appeared in West Africa, when another, Taï Forest ebolavirus, was thought to circulate in the region.  As is increasingly the case for emerging viruses, the source of Ebola viruses is presumed to be bats. Evidence remains slight, but data suggests fruit bats are hosts for Ebola. Antibodies against Ebola virus, which usually indicate historical but not current infection, were detected in non-migratory fruit bats in Ghana only a few years before this outbreak. The distributions of these bat species include the nations currently affected by the outbreak. Unfortunately, methods that detect antibodies are not yet discriminatory enough to determine which of the virus strains was circulating, but the results indicate at least one circulated in West African bats. Whether or not this was due to a wave of infection from central Africa through bats requires further data from bats themselves, which have proven elusive. If we assume Ebola can go wherever its host bats go, we return to Bausch and Schwarz’s central argument – was this outbreak predictable because of a perfect storm of bat infection and socio-political issues? To some degree, all events have a random component. However, like all probabilistic events, there are things that increase or decrease chances of events occurring, and Ebola outbreaks are no different.  Comparing Ebola outbreaks to another bat-borne virus may be informative. Outbreaks of Nipah virus, which often causes encephalitis and death, are linked to drinking raw palm sap in neighbouring parts of Bangladesh and India. Bat species that host Nipah occur throughout much of Asia, but greater risk of infection appears to be linked to drinking palm sap in these locations. What then links Ebola and bats and humans? One possibility is hunting. Bats are widely eaten in Africa, with more than a hundred thousand bats eaten annually in Ghana alone. However, the most commonly hunted species does not seem to be infected with Ebola as frequently as other fruit bats. It may be that other, non-bat species occasionally get infected and it is contact with these that leads to human infection. However, evidence from Ebola’s relative, Marburg virus, suggests close contact through mining activities with the putative host, a cave dwelling fruit bat, may lead to human infection and Ebola may be no different. How then can Ebola virus outbreaks be prevented? If the argument by Bausch and Schwarz is true, there will be no simple solution. No mechanism for transmitting Ebola from bats to humans (such as in the case of drinking palm sap infected with Nipah virus) has been discovered. Vaccines exist, but are at the preliminary stage of development and unlikely to be commercially viable. There is no capacity to deliver the vaccines either, because of the chaotic nature of these countries. We need a more all-encompassing approach that can reduce contact between humans and bats. Improving health-care infrastructure, restoring degraded habitats and decreasing bat hunting might have conservation and public health benefits. But these all rely on hugely improved governance in those most affected nations, which will be difficult to achieve."
"**People should limit their contact with others before Covid-19 restrictions are relaxed at Christmas, Deputy First Minister Michelle O'Neill has said.**
Across the UK, three households can mix for five days from 23-27 December.
However, Ms O'Neill said it was important to reduce Covid-19 transmission ""as low as possible"".
First Minister Arlene Foster said the next two weeks ""are crucial... so that we can all have the safest and the happiest Christmas possible"".
From midnight, Northern Ireland enters a two-week circuit breaker, with the closure of many businesses in the retail, leisure and hospitality sectors.
The deputy first minister also said a ""number of things need clarified"" around Christmas arrangements which will be the focus of the executive's meeting on Tuesday.
She said these included care home arrangements, students coming home and the definition of a household.
""There is a different in approach across all the jurisdictions in terms of, for example, what a household looks like and it's important that we define that for ourselves,"" the deputy first minister added.
In Scotland, a three household bubble should contain no more than eight people over the age of 11.
The executive also announced that a Covid-19 Taskforce was being established to oversee the roll-out of the vaccine and testing programmes.
The deputy first minister said it will be chaired by a new interim head of the Civil Service and will also be responsible for public messaging to improve compliance.
Mrs Foster said the rate of transmission was currently believed to be ""just below 1"".
She said she commended ""all those who are re-doubling their efforts to make our high street as Covid secure as possible for their reopening on 11 December"".
""I want to pay tribute to our scientists, our academics, medics and health workers who are providing us with the pathways out of this pandemic through mass vaccination and testing programmes,"" she continued.
The broadcast press conferences from Stormont that follow executive meetings have understandably often been sombre occasions.
The news of daily deaths and increased hospital admissions bring home the reality of Covid-19.
The news that many families are suffering shows that eight months on, we are still struggling with this pandemic.
Whilst this is bleak and painful, today's press conference did offer some shades of light for the future.
The news of a vaccination programme offers hope that could save lives and end talk of lockdown and restrictions.
There was also news that Northern Ireland's R value is just below one - lower than England and Wales.
The first and deputy first ministers also offered some hope to the hospitality sector who desperately want to get back into business on 11 December.
Conversations with the Chief Medical Officer Dr McBride and the Chief Scientific Adviser Professor Ian Young about lifting the trading restrictions are ongoing.
Much depends on how the next fortnight pans out.
Today offered some glimpses of the future and for some at least there is hope on the horizon.
On Thursday, eight further deaths linked to Covid-19 in Northern Ireland were reported by the Department of Health, bringing its total to 962.
The department also recorded 442 new cases of coronavirus.
Five hospitals are currently operating beyond their bed capacity. They are the Causeway, Mater, Royal Victoria, Ulster and South West Acute.
There are confirmed outbreaks of Covid-19 in 139 care homes.
The UK government has said anyone travelling to or from Northern Ireland can travel on 22 and 28 December, but ""only meet with their Christmas bubble"" between 23 and 27 December."
"Curious Kids is a series by The Conversation, which gives children of all ages the chance to have their questions about the world answered by experts. All questions are welcome: you or an adult can send them – along with your name, age and town or city where you live – to curiouskids@theconversation.com. We won’t be able to answer every question, but we’ll do our best. How is spider silk so easy to break when it’s stronger than steel? - George, aged ten, Hethersett, UK. Thanks for the question, George – the simple answer is that spider silk breaks easily because it’s really, really, really thin. A thread in the web of a garden spider is just 0.003 millimetres across – that’s more than 20 times thinner than a hair from your head.  But there are a few more matters we need to untangle, to see how strong spider silk is, compared with steel.  Steel is a material called an alloy, which means it is a mixture of metals. The main metal in steel is iron. Other metals are added to the iron, depending on what you want the steel to do.  For example, knives and forks are made from stainless steel that doesn’t rust. To make this you’d mix iron and chromium.  But maybe you want a steel that is really strong so you could make buildings and cranes from it. Then, you would need to mix iron with a load of different metals including titanium and vanadium.  But there are even stronger steels. Your bike might be built with something called maraging steel and it’s made with iron, nickel, cobalt, molybdenum, titanium and aluminium. Silk is a very different material from steel. It is actually protein – the same stuff that your hair and finger nails are made from.  We use steel for different jobs and spiders use silk for all sorts of things as well. And the just like our steel, spiders need different silks for the different jobs.  Let’s look at the common European garden spider: this lovely creature spins the beautiful round webs, using two types of silk.  The spokes of the web are made from dragline silk. This is strong and slightly stretchy, which means it’s good for making the main supports for the web.  The rest of the web is made from flag silk, which is less strong but very elastic, so it is really good at absorbing the shock when a great big fly smashes into the web.  But the champion constructor of the spider world is the Darwin bark spider. It produces huge webs, about the size of a kitchen table.  These are sometimes hung from trees with silk threads that stretch right across rivers. To make sure these webs stay in place, the spider uses super strong threads of silk.  Now, I’ve done some calculations, based on what scientists know about the strength of these different materials, to compare steels with spider silks and see just how strong they are.  Imagine we had a thread of each material, that was about one millimetre thick – that’s roughly the width of a pin head. Who could hang from them before they broke? The weakest is the flag line silk: a ten-year-old girl could probably swing from a thread of this, but nothing much heavier.  Next comes the high strength steel which would just about be OK if a chimpanzee hung from it.  The dragline silk would support a small adult – like Spiderman.  The Darwin bark spider silk is next, it would break if anything much bigger than a panda tried to climb it.  And finally, a gorilla would be fine dangling from a one millimetre-thick thread of maraging steel.  Which means that some the strongest steel is actually tougher than the champion spider silks. That is a shame, but all is not lost for the spiders: remember, some of their silks are still stronger than some steels.  More Curious Kids articles, written by academic experts: How do babies learn to talk? – Ella, aged nine, Melbourne, Australia. Our guinea pigs have dark eyes. Why do we have white eyes? - Rhoswen, aged three, Bristol, UK. What’s the point of nits?! – Connie, aged nine, Nambour, Australia."
"
Share this...FacebookTwitterDer Spiegel focuses again on failed international climate policy, and explores alternative policy paths being proposed by German sociologist Nico Stehr, and 13 other international authors of the Hartwell Paper, among them Roger A. Pielke Jr.
The Hartwell Paper is nothing new. It was first released in 2010 by the London School of Economics in cooperation with the University of Oxford. It was authored by 14 natural and social scientists from all over the world, among them Mike Hulme and Roger A. Pielke Jr.
But the fact that the paper is being brought up once again shows just how much everything is in disarray for the climate activists. When media outlets like Der Spiegel start having doubts, then you know the movement is in deep trouble. Spiegel begins with:
The UN climate conference in Berlin was a flop. Now there has to be a completely new start in international climate policy, says sociologist Nico Stehr.”
The UN, governments and activists have spent literally hundreds of billions of dollars on the senseless endeavor of trying to regulate the Earth’s temperature and taming storms by limiting emissions of a single trace gas. Not surprisingly, these attempts have failed completely, and never mind that temperatures have not risen in 15 years anyhow.
Spiegel writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The debacle is an immense chance for a climate policy to finally unfold. The main motivation of the Hartwell Paper:
1. Energy access for everyone;
2. Decarbonization, development of clean energy so that it is cheaper than fossil fuels;
3. Equip society so that it can cope with the risks and dangers associated with changing climate, whatever their cause may be.”
Clearly the Hartwell Paper authors propose a shift to more adaptation and less on mitigation. The Hartwell Paper supports decarbonization by promoting effective investment in other sources of energy so that they become cheaper than fossil fuels. There’s less focus on punishing environmental sinners.
What’s new here is that experts are once again asking: “When are you climate activists going to wake up and realize you’ve failed big time and that your plan has no chance of ever working in the future – no matter how hard you try?”
Nico Stehr is the owner of the Karl-Mannheim-Professur for cultural sciences at the Zeppelin University in Friedrichshafen. He is among Germany’s most renowned sociologists.
 
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterSo much for the notion that today’s climate is “unusual”. It turns out that some earlier interglacials were much warmer than the current one and that the variations cannot be explained by greenhouse gases alone.

Source: Alfred Wegener Instutute
A new temperature reconstruction suggests that (NATURAL) feedback mechanisms and amplifiers had to be at play.
Der Spiegel here has the story on a team of German scientists led by Martin Melles of the University of Cologne. In 2008/2009 his team extracted a sediment core from Lake El’gygytgyn in northeast Siberia, 100 miles north of the Arctic Circle. The team extracted a 135-meter long sediment core from the lake’s bottom, analyzed it and produced a temperature reconstruction going back 2.8 million years.
What makes this core unique is the lake’s history. It was formed 3.6 million years ago by a meteorite leaving a huge crater. The crater filled with water to form the lake and year by year the bottom accumulated sediment. Because the lake was not covered by a glacier during the ice ages, it sediment core is complete and without holes. Each layer of sediment provides a record like pages in a diary.
What has the reconstruction revealed?
The scientists reconstructed the climate from this core and found some big suprises. According to Der Spiegel there were “some extreme warm periods in the Arctic that up to now had been unknown.”
Der Spiegel adds
The core from Lake El’gygytgyn shows a regular change between warm and cold periods in the Arctic – thanks to changes in the Earth’s orbit, fluctuating greenhouse gases and changing solar activity. However from the back and forth in temperatures and precipitation, some extreme events stand out: The scientists have compared two “normal” warm periods – the current one, which has been ongoing for the last 12,00 years, and another one 125,000 years ago, and compared them to the so-called super warm periods. These occurred 400,000 and one million years ago.”
How warm was it? 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Der Spiegel reports on what the scientists found:
During the super warm times, the high temperatures were up to 5°C above the normal highs – that is at about 13°C. Moreover, about 600 liters of precipitation fell per square meter, about double what is normal. ‘For climatology, that’s worlds apart,’ Melles describes the differences. Around Lake El’gygytgyn, where today one finds tundra and little plant growth, green fir trees once grew. During these times, the scientists say, a large part of the Greenland ice sheet was gone.’
Also:
One sees a clear agreement between the super warm periods in the Arctic and the disappearance of the West Antarctic ice sheets,’ Melles says. Information on the retreat of this massive ice sheet was gathered from the ‘Andrill’ core in the Antarctic. It’s evaluation has shown that the West Antarctic was ice-free in warm times.
The question of course is what is the connection between the Arctic and Antarctic melting at the same time? Gee, that’s a tough one.
The Melles and his team speculate that melting water being conveyed by ocean currents, and thus coupling the two poles, could be a factor. Another theory they propose is that if sea level rises 5 meters or more, the water flow though the Bering Strait might be enough to warm the Arctic.
Amazing how what seems obvious eludes government-funded scientists. Maybe the connection is the sun?
The paper’s abstract adds:
Climate simulations show these extreme warm conditions are difficult to explain with greenhouse gas and astronomical forcing alone, implying the importance of amplifying feedbacks and far field influences. The timing of Arctic warming relative to West Antarctic Ice Sheet retreats implies strong interhemispheric climate connectivity.”
We all suspect that feedbacks and amplifiers are at play. But some scientists are doing all they can to ignore some of the mechanisms being proposed and supported by a growing body of evidence.
If there’s anything that can’t be ignored, it is that the study shows once again that scientists are more baffled than ever on how the climate really works, and so their models have to be considered accordingly.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAnthony Watts, the king of skeptic climate science bloggers, made the unusual announcement yesterday that 1) blogging had been suspended until Sunday and 2) that he had something “controversial and unprecedented” to tell us.
The rest of us many backbenchers and observers, always hungry for any bit of news supporting our views, are now left to speculate 2 days long.
Here’s what we know:
1. It’s not something bad for him or his family. That’s good to know.
2. It’s nothing legal, political or social in nature.
…has nothing to do with FOIA issues or other sorts of political or social theories…”
But here we note Anthony left out the descriptive “scientific” and “business” terms. This narrows down the possibilities considerably. So is it scientific or business? Here’s what he writes originally:
…there will be a major announcement that I’m sure will attract a broad global interest due to its controversial and unprecedented nature.”
The key words to me here are global, controversial and unprecedented – especially telling is the word “controversial”. Well we know he tinkers around a lot and is inventive. But new products or innovation are rarely controversial, which therefore pretty much eliminates the possibility that it’s an exciting technical breakthrough, i.e. business-related.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And what we do not know:
Overall, that leaves that his “controversial and unprecedented” announcement is scientific in nature. Climate science today is controversial. Sounds to me like he has come up with hard results people aren’t going to like and someone big (like a renowned journal) has endorsed them.
Later he does seem to backpedal a bit on the implications of his announcement, writing that it has:
“…something to do with one of my many projects, it is still a ‘major announcement’ and it has important implications that I’m sure everyone will want to know about.”
Well, what is it that he has been working on for years that everyone will want to know about? Sounds a lot like it has something to do with his surface stations project, getting something published, or being appointed to an important position related to the subject. But surface stations have been pretty much covered and we know pretty much what there is to know about that.
So what’s left? There’s likely another topic he’s been researching and has not told us about yet, and has some surprising, hard results.
That’s my WAG. Next time, he should say nothing and just drop the bomb on us.
But now, until the bomb drops, we are left to speculate on what this “controversial and unprecedented” announcement of “global interest” could possibly be.
 
Share this...FacebookTwitter "
"On 27 June 2019, the energy and clean growth minister Chris Skidmore signed papers that committed the UK to reduce carbon emissions to effectively nothing by 2050. If we are to stand any chance of meeting this target, known as “net zero”, there is one enormous challenge that we will have to tackle: home heating. Warming our homes is responsible for between a quarter and a third of the UK’s greenhouse gas emissions. That’s more than 10 times the amount of CO2 created by the aviation industry. Around 85% of homes now use gas-fired central heating, and a large proportion of gas cooking still takes place. Greening this system is a huge challenge by any measure. But if recent reports are to be believed, there could be a simple and efficient way to do it: change from using natural gas to hydrogen gas.  Hydrogen is abundant in the natural world and according to its advocates could power the next generation of gas appliances cleanly and efficiently. “The attraction of hydrogen is that for a lot of consumers, they wouldn’t notice any difference. Customers would continue to use a boiler to heat their homes in a similar manner to natural gas,” says Robert Sansom of the Institution of Engineering and Technology’s energy policy panel. He is the lead author on a study conducted by the institute called Transitioning to Hydrogen. Together with colleagues, Sansom assessed the engineering risks and uncertainties associated with swapping our gas network to hydrogen. Their conclusion is that there is no reason why repurposing the gas network to hydrogen cannot be achieved. That’s not to say it would be easy, though. Technological and practical hurdles exist because there is no blueprint for such a conversion: there is nowhere in the world that supplies pure hydrogen to homes and businesses. The UK would have to pioneer everything. Interest in hydrogen as a way to heat homes began in 2016 with a report called H21. It was conducted by Northern Gas Networks, the gas distributor for the north of England, and looked at whether it was technically possible and economically viable to convert Leeds to 100% hydrogen instead of natural gas. “They went into a lot of detail, from the hydrogen production plants right the way down to people’s homes,” says Sansom. The report drew a parallel to the way the gas industry converted from town gas to natural gas in the 1960s and 70s. Town gas was a combination of hydrogen, carbon monoxide and methane. It was mostly produced from the distillation of coal and oil and had been used for the first 150 years of the UK’s gas industry. With the discovery of natural gas in the North Sea, which is predominantly methane, the UK undertook a nationwide programme to convert 40m appliances over a decade. Whole streets would be converted at a time. Engineers would inspect the gas appliances, and then convert them. Simultaneously, the town gas was disconnected and the pipelines were purged with an inert gas. Finally, the natural gas was pumped into the system and the engineers would make sure each appliance worked correctly before moving to the next street along. Some manufacturers are now so convinced that a similar thing can happen with hydrogen that they have already begun to develop new household appliances. In February, Worcester Bosch unveiled the prototype of its hydrogen-ready boiler. It would run first on natural gas and then, after a servicing visit, hydrogen. Also working in hydrogen’s favour is that for the past 20 years, the gas industry has been systematically replacing the metal pipes in its “iron mains” network with yellow polyethylene ones. Around 90% of the pipes will have been replaced by 2030. This is good news for hydrogen because the gas reacts with the old metal pipes, making them brittle. But the polyethylene is safe. “Effectively we started a programme of hydrogen-proofing our gas network without knowing we were doing it,” says Sansom, who found himself becoming more and more impressed by the concept. “From a personal point of view, I was very much on the fence when I kicked off with this work. But I found myself slipping down on the hydrogen side in terms of its viability as a low carbon alternative to natural gas,” he says. But not everyone is convinced by this sudden interest in hydrogen. Richard Lowes of the University of Exeter Energy Policy Group says that until recently the received wisdom had been that heating would have to be electrified in some way to meet our climate-crisis commitments. “That has basically come out of years and years of technical and economic modelling to look at how you get to fully decarbonised heating in the UK,” says Lowes. Switching heating from gas to electricity would mean relying on heat pumps. These use electricity to extract heat from either the air or the ground. In the case of an air source heat pump, it works like a fridge but instead of sucking heat out of a food compartment, it pulls it out of the air and channels it into the home, where it is used to heat water, which is piped to radiators for central heating, and stored in a tank for hot water. But because this technology works at a lower temperature than existing boilers, it requires many homes to be much better insulated, or to have larger radiators, capable of delivering more heating power. For those who have switched to heat-as-you-go combi boilers, it will necessitate the reinstallation of a hot water tank. It’s extensive work but worth it, according to Lowes, who has removed his own gas boiler and is now using an air source heat pump to heat his home. “It was a lot of work but my home and heating system are now a lot more efficient. It’s always warm, there’s always hot water and it’s basically the same cost to run as gas,” he says. The third approach is called district heating. It envisages water being heated at a central facility using waste heat from industry or green sources such as solar power. The hot water is then delivered to many homes simultaneously through a network of heavily insulated underground pipes. Both methods can significantly reduce the carbon footprint of home heating but the downside is that they require extensive work to roll them out on a national scale. District heating would require water pipes to be laid under homes, and the widespread use of heat pumps would necessitate the National Grid’s electricity circuits being upgraded. It is this kind of disruption that hydrogen’s advocates say could be avoided because much of the national infrastructure has already been upgraded. That argument cuts no ice with Lowes. “It seems a bit hypocritical for the gas industry to say we can’t dig up the roads when they’ve been doing it for the past 20 years,” he says. He points out that although the consumer may not experience so much disruption, significant challenges for the gas industry remain. For example, the National Transmission System, which is the network of pipes that supplies gas from the coastal terminals to the gas distribution companies and other major users, is made of metal. This would need to be protected from embrittlement in some way before any switch to hydrogen could take place. “Hydrogen is certainly not a silver bullet,” says Lowes. And if we get distracted by it, we could be getting ourselves into more trouble, missing the 2050 energy target altogether. But if there is so much uncertainty with hydrogen, why is the gas industry, which funds many of the studies, pushing it so hard? According to Chris Goodall, energy economist and author of What We Need to Do Now for a Zero Carbon Future, it is a matter of survival. “They do not wish for their industry to be eaten up by a switch to electricity for heating. So they are moving as fast as they can to persuade us about hydrogen,” he says. And it all comes down to how the gas is produced. Hydrogen is not found on Earth in a pure state. Instead, it has to be extracted from other substances, and the best one to extract it from is methane – in other words natural gas. Hence, the gas companies could effectively keep their current operations running. But the extra steps involved in extracting the hydrogen would push the price up. Additionally, the extraction creates carbon dioxide as a byproduct, so large scale carbon capture technology would need to be developed to prevent it escaping into the atmosphere. Although this is a technology that the UK will have to develop anyway in order to reach net zero by 2050, it will add to the cost. But natural gas is not the only substance that contains hydrogen. Water does too, and the hydrogen can be freed by a process called electrolysis, which doesn’t create any carbon dioxide. To make it totally green, which is the ultimate hope, electrolysis could be powered by wind farms. At the moment, however, the price of such electricity is expensive, and that would push the price of hydrogen up still further. Goodall hopes that the cost will decrease as technology improves, but warns: “You can be accused of mindless optimism just by saying this.” The UK’s future energy landscape is without doubt a difficult realm to navigate. Perhaps the best route will be revealed by not pitting the various solutions against one another. “All three have strengths and weaknesses and I expect that there’ll be a major role for each as a replacement for natural gas,” says Sansom. Even hydrogen’s detractors acknowledge this. “As a niche technology it can have real value,” says Lowes. He goes on to paraphrase the Heineken lager adverts of the 70s and 80s, saying that hydrogen could potentially reach the parts of the country that other energy solutions can’t. Goodall also sees a role for hydrogen to “store” energy generated from renewable resources such as wind and solar power. The idea is that in windy months, any extra electricity generated from renewables will be used to make hydrogen, which would then be stored. When there is extra demand on the National Grid, or a seasonal drop in the power produced from renewables, the hydrogen can be burned to produce electricity. The truth is that all options for us to decarbonise our heating systems will require significant disruption and cost. And while government continues to deliberate, the clock ticks towards 2050. “There is no need to wait. We can deploy stuff now that works fine,” says Lowes, referring to his own experience of changing his gas boiler for a heat pump. “The urgency of climate change means there really isn’t any reason to delay.” Others believe that there is a role for hydrogen and think it is worth taking a little more time to consider. But there is one truth that everyone agrees upon. “None of this is easy. If anyone is saying to you this is easy, they are misleading you,” says Lowes. Hydrogen can also power vehicles, but in a different way than it would heat houses. Instead of being burned, the hydrogen reacts with oxygen inside a device called a fuel cell. Electricity and water are produced. The electricity runs the car, the water drips from the exhaust pipe.An attempt to switch to hydrogen vehicles in the 1990s was thwarted byelectric cars, which store their energy in an onboard battery. But a new push for hydrogen vehicles is coming from Asia. China, Japan and South Korea have all set ambitious goals to have millions of hydrogen-powered vehicles on their roads by 2030.Toyota and Hyundai are both offering hydrogen vehicles in the UK, but there are currently less than 20 hydrogen filling stations across the UK, mostly clustered around the M25.“It will be really interesting to see what happens,” says Lowes. But he himself is not convinced: “Hydrogen is much more expensive than electricity, and the car is more expensive than the electric vehicle.”"
"
This press release is brought to you by our friends at the National Science Foundation, it is not a joke. However, it is too odd not to spoof a bit.
Here are some preliminary results, there’s more at the end of this article.
How to interpret this related to ""weather, not climate"": hail is imminentPress Release 10-243
Broken Glass Yields Clues to Climate Change

 
Ordinary drinking glasses and atmospheric dust particles break apart in similar patterns




The comparative sizes of dust particles in the atmosphere, from a dust storm satellite photo.
Credit and Larger Version



December 27, 2010
Clues to future climate may be found in the way an ordinary drinking glass shatters.
Results of a study published this week in the journal Proceedings of the National Academy of Sciences find that microscopic particles of dust can break apart in patterns  that are similar to the fragment patterns of broken glass and other  brittle objects.

The research, by National Center for Atmospheric  Research (NCAR) scientist Jasper Kok, suggests there are several times  more dust particles pumped into the atmosphere than previously believed,  since shattered dust appears to produce an unexpectedly high number of  large fragments.
The finding has implications for understanding  future climate change because dust plays a significant role in  controlling the amount of solar energy in the atmosphere.
Depending  on their size and other characteristics, some dust particles reflect  solar energy and cool the planet, while others trap energy as heat.
“As  small as they are, conglomerates of dust particles in soils behave the  same way on impact as a glass dropped on a kitchen floor,” Kok says.  “Knowing this pattern can help us put together a clearer picture of what  our future climate will look like.”
The study may also improve  the accuracy of weather forecasting, especially in dust-prone regions.  Dust particles affect clouds and precipitation, as well as temperature.
“This  research provides valuable new information on the nature and  distribution of dust aerosols in the atmosphere,” says Sarah Ruth,  program director in the National Science Foundation (NSF)’s Division of  Atmospheric and Geospace Sciences, which funds NCAR.
“The results may lead to improvements in our ability to model and predict both weather and climate.”
Kok’s research focused on a type of airborne particle known as mineral dust.
These particles are usually emitted when grains of sand are blown into soil, shattering dirt and sending fragments into the air.
The fragments can be as large as about 50 microns in diameter, or about the thickness of a fine strand of human hair.
The  smallest particles, which are classified as clay and are as tiny as 2  microns in diameter, remain in the atmosphere for about a week, circling  much of the globe and exerting a cooling influence by reflecting heat  from the Sun back into space.
Larger particles, classified as  silt, fall out of the atmosphere after a few days. The larger the  particle, the more it will tend to have a heating effect on the  atmosphere.
Kok’s research indicates that the ratio of silt  particles to clay particles is two to eight times greater than  represented in climate models.
Since climate scientists carefully  calibrate the models to simulate the actual number of clay particles in  the atmosphere, the paper suggests that models most likely err when it  comes to silt particles.
Most of these larger particles swirl in  the atmosphere within about 1,000 miles of desert regions, so adjusting  their quantity in computer models should generate better projections of  future climate in desert regions, such as the southwestern United States  and northern Africa.
Additional research will be needed to  determine whether future temperatures in those regions will increase as  much or more than currently indicated by computer models.
The  study results also suggest that marine ecosystems, which draw down  carbon dioxide from the atmosphere, may receive substantially more iron  from airborne particles than previously estimated.
The iron enhances biological activity, benefiting ocean food webs, including plants that take up carbon during photosynthesis.
In  addition to influencing the amount of solar heat in the atmosphere,  dust particles also are deposited on mountain snowpacks, where they  absorb heat and accelerate snowmelt.
Physicists have long known  that certain brittle objects, such as glass, rocks, or even atomic  nuclei, fracture in predictable patterns. The resulting fragments follow  a certain range of sizes, with a predictable distribution of small,  medium, and large pieces.
Scientists refer to this type of pattern as scale invariance or self-similarity.
Physicists  have devised mathematical formulas for the process by which cracks  propagate in predictable ways as a brittle object breaks.
Kok  theorized that it would be possible to use these formulas to estimate  the range of dust particle sizes. By applying the formulas for fracture  patterns of brittle objects to soil measurements, Kok determined the  size distribution of emitted dust particles.
To his surprise, the formulas described measurements of dust particle sizes almost exactly.
“The  idea that all these objects shatter in the same way is a beautiful  thing, actually,” Kok says. “It’s nature’s way of creating order in  chaos.”
-NSF-
==============================================================
Here are other ways to use broken glass to forecast and interpret your local climate issues:
Your Computer Climate Model is broken
The Arctic Sea Ice is ""rotten""
AGW is increasing the number of spiders globally
Solar minimum ahead -OR- your state is banning the bulb
Mercury vapor poisoning ahead
Your car is causing climate disruption
A drought is almost certain (95% confidence limit)
The climate gods are angry
﻿
The Goreacle is angry. Why aren't you listening ?!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e869597bd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAwhile back I wrote about the climate alarmists’ latest tactic to get people scared about climate change – i.e., make it look like it’s a serious threat to public health.
Hat-tip: DirkH
Der Spiegel has wasted little time and has immediately pounced on the new paper appearing in Nature Climate Change, by Austin et al, which boldly claims man-made global warming “is reshaping the distribution of infectious disease across global scales”.
The authors illustrate possible associations between environmental changes in the Baltic area and the recent emergence of Vibrio infections. They also forecast future scenarios of the risk of infections with predicted warming trends. The authors don’t beat around the bush, implying that the Baltic, because it warmed 0.063–0.078 °C yr from 1982 (a cold time) to 2010 (a warm time), sea surface temperatures there may rise 7°C per century!
How scientific is that? The stock market went up 200 points last week, and so does that mean it will rise 10,000 points over the next year? Of course not. The silly extrapolation the authors imply reveals their true intent: to fan public fear. This paper is hardly above tabloid trash as far as quality goes.
Moreover, the authors think that 29 years of data (half a PDO or AMO oscillation) are enough to make a quantum leap of faith and to conclude:
This is among the first empirical evidence that anthropogenic climate change is driving the emergence of Vibrio disease in temperate regions through its impact on resident bacterial communities, implying that this process is reshaping the distribution of infectious diseases across global scales.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




That is just plain stupid. Where’s the science? What would these scientists think if we told them that CO2 and global temperature haven’t correlated in 15 years?
Of course the somewhat obviously dimwitted journalist at Der Spiegel took it in, hook, line and sinker. Der Spiegel warns:
Already more and more people are being infected in warm summers by Vibrio vulnificus, a contagion for wounds, diarrhea and blood poisoning an international team of scientists reported in “Nature Climate Change”. Also the very closely related Cholera bacterium , Vibrio Cholerae, is on the march.
During the extremely warm summers of 1994, 2003 and 2006 at the Baltic Sea coast, there were numerous reports of infected wounds and cases of Cholera. Alone in 2006, 67 people became infected while bathing or doing water sports; some even died.”
The authors add that the number of Vibrio infections will increase significantly, if the warming continues. Note there’s no mention that there’s been no real outbreak since 2006.
The authors also warn that more than 30 million people live near the Baltic Sea coast and that they, and cities like Stockholm and St. Petersburg, are all threatened.
So readers, you are urged to cancel your Baltic holidays and to spend them somewhere else – like the good old Mediterranean, where water temperatures are about 10°C warmer.
 
Share this...FacebookTwitter "
"**Christmas ""bubbles"" of three households in Scotland should contain no more than eight people over the age of 11, the Scottish government has said.**
The rule is part of the government's guidance for Christmas which temporarily relaxes some Covid-19 restrictions for five days.
Children under the age of 12 will not count towards the total number of people in the bubble.
The easing of Covid rules will apply across the UK from 23 to 27 December.
Opposition parties have accused the Scottish government of sending out ""mixed messages"" by allowing people to meet at Christmas while simultaneously urging them not to do so.
And many health experts have warned that the move is likely to lead to a spike in cases of the virus - and potentially deaths - in January
UK government guidance for people in England does not set a limit on the number of people in a bubble, but says this should be kept ""as small as possible"".
No separate guidance has been published for Wales or Northern Ireland at this stage, although people can travel to or from Northern Ireland on 22 and 28 December. The NI executive is meeting on Thursday to discuss the rules.
A UK-wide deal was agreed on Tuesday to permit people to meet up in ""bubbles"" over the festive period.
Travel restrictions will be lifted across all four nations from 23 to 27 December so people can visit close friends and relatives.
But Scotland's First Minister Nicola Sturgeon has that said the ""default advice"" and ""safest position"" was still that people should avoid contact.
The Scottish guidance states that the ""safest way to spend Christmas and the festive period is to stay within your own household, in your own home and your own local area"".
It adds: ""Wherever possible you should keep in touch with friends and family members from other households through technology - or, if you decide to meet in person, you should minimise the numbers and duration, and if possible meet out of doors.
""Consider a Christmas walk with family, rather than a meal indoors.""
Although three households will be allowed to meet indoors and stay overnight in the same home, the Scottish government says a two-metre distance should be kept between people from different households.
However, children under 12 will be exempt from the physical distancing rules.
Other guidance for different households staying in the same home includes:
Doors and windows should also be opened to let in as much fresh air as possible during and after visits.
Christmas bubbles in Scotland can only gather in a private home, outdoors or at a place of worship.
They will not be allowed to visit pubs, restaurants or go to shops together and staying in tourist accommodation as a group is banned as well.
Single households should also not travel in or out of level three or level four areas in Scotland to use tourist accommodation, the guidance says.
Scottish Greens co-leader Patrick Harvie highlighted warnings from public health experts that the easing of restrictions over Christmas was likely to lead to a third wave of infections, hospitals being overrun, more ""unnecessary"" deaths and potentially a nationwide lockdown in January.
Speaking in the Holyrood chamber, Mr Harvie also questioned why the Scottish government had apparently not carried out any risk assessments on the potential impact of easing the restrictions.
He added: ""I recognise that there were difficult judgements to make about relaxing the Covid rules over the holidays, especially after public expectations had been built up.
""But within a day of announcing the looser rules, the first minister is appealing to the public not to use them. It's a confusing message.""
**Use the form below to send us your questions and we could be in touch.**
_ **In some cases your question will be published, displaying your name and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read the terms and conditions.**_
If you are reading this page on the BBC News app, you will need to visit the mobile version of the BBC website to submit your question on this topic."
"**Suffolk will be placed in tier two when England's second lockdown ends on 2 December, it has been announced.**
People in this tier cannot socialise with other households indoors.
The rule of six will apply to gatherings outdoors and pubs and restaurants will shut at 23:00 GMT and only be allowed to serve alcohol as part of a meal.
Prior to England's second lockdown, Suffolk was in tier one, the lowest level of Covid-19 restrictions.
Most of England has been placed in the two toughest tiers of coronavirus restrictions when the national lockdown ends.
The hospitality sector had been hoping to recoup some of its losses in the festive period.
Ashley Stock, 32, director of Stock and Bailey, which runs The Ram in Hadleigh, Suffolk, and a wine bar in Colchester, said the move into tier two was not ideal.
""It is what it is, and we have to make it work,"" he said.
Co-director Paul Bailey, 43, said the business faced ""challenging times"" but they were well supported and he ""remained optimistic"".
But Andy Davis, 36, pub manager of The George in Hadleigh, said the next few weeks ""would be a bit of a struggle"".
""At the end of the day, we're a pub that serves food, we're not a restaurant, so I'm going to have to see,"" he said.
""It's going to be extremely hard to make the business viable and make every single person who walks through the door have a meal.""
Peter Aldous, Conservative MP for Waveney, said he hoped ""if things move in the right direction, we can be out of tier two in a month's time"".
He has appealed for help for the hospitality industry, which he said would be ""hit hardest"".
On Wednesday, Britain's major pub groups and brewers, including Suffolk-based Adnams and Greene King, wrote a letter to Prime Minister Boris Johnson asking him to save their industry, which was facing the ""darkest of moments"".
The letter accused the government of scapegoating pubs ""despite a lack of evidence"".
The pubs also demanded financial support in line with the first lockdown, immediate changes to business rates and a cut in beer duty, if restrictions were not relaxed.
Stuart Keeble, Suffolk's director of public health, said: ""We need to stick with it.
""With current infection rates and pressure being put on health services, we need to do more of what we have been doing.""
Suffolk's move to tier two restrictions means a maximum of 2,000 people can be admitted to Portman Road to watch Ipswich Town play in League One once lockdown ends.
The county's current infection rate of 85 cases per 100,000 people was more than double what it was when Suffolk entered tier one on 14 October, Suffolk Public Health said.
At the beginning of September, there were just five cases per 100,000 people.
Parts of Suffolk, especially Ipswich and Hadleigh, are seeing much higher numbers of positive cases than anticipated.
There were 116 Covid-19 patients in Suffolk hospital beds as of 23 November.
The Health Secretary and MP for West Suffolk, Matthew Hancock, said despite his own constituency having the lowest case rate for over-60s in the whole country, the county as a whole needed to be in tier two ""to get the virus further under control"".
""I hope Suffolk and so many other parts of the country can get to tier one soon and the more people stick to the rules the quicker that will happen,"" said Mr Hancock.
The government said it would be regularly reviewed and an area's tier level may change before Christmas. The first review is scheduled for 16 December.
The rates in all districts in Suffolk, except Ipswich, have fallen week-on-week.
The districts of Mid Suffolk, West Suffolk, and East Suffolk have had some of the lowest rates of Covid-19 since the end of the first lockdown.
_Find BBC News: East of England on_Facebook _,_Instagram _and_Twitter _. If you have a story suggestion email_eastofenglandnews@bbc.co.uk"
"Climate change is underway, and human activities such as urbanisation, industrialisation and food production are key contributors. Food production alone accounts for around 25% of global carbon emissions. Ironically, the changing weather patterns and more frequent extreme weather events resulting from climate change also put the world’s food supplies at risk.  Food production drives deforestation, meaning there are fewer trees to absorb carbon dioxide, which contributes to the greenhouse effect. What’s more, the fertilisers and pesticides used to protect crops have caused a dramatic decline in insect populations, and in soil fertility, by affecting the microbial organisms that enrich the soil and enable plants to gain nutrients.  At the same time, the world population is rising and there are expected to be more than 9.5 billion people on Earth by 2050. In response to these projections, the UN’s Food and Agriculture Organisation (FAO) is campaigning for a 60% increase in food production by 2050, by intensifying agriculture to be more productive and use fewer resources, all without increasing the amount of farm land.  It’s not yet clear exactly how this “intensification” should happen. Alternative methods, such as organic farming, are respectful of soil ecology and insect life and can restore soil fertility. But they cannot, at present, produce as much food as industrial agriculture.  Yet the idea that we need more food is debatable. Although, according to the FAO, there are 821m people globally suffering from hunger, the world produces 50% more food than is needed to feed the global population. Another estimate from biologist and author Colin Tudge suggests that the current food production can feed as many as 14 billion people. But one third of this food is wasted because of distorted supply systems, unjust food distribution and unhealthy and unsustainable diets. So, the efforts of experts in the food sector should not concentrate on agriculture intensification, but rather on strategies to change patterns of consumption and waste at a local and global level. My own research on urban agriculture and sustainable cities suggests there are three main areas where effective changes can be made.  Food consumption needs to become “circular”. This means that organic waste such as food scraps does not go to landfill, but is instead transformed into compost (which will be needed in a transition to organic agriculture) and biogas.  


      Read more:
      Ugly veg: supermarkets aren't the biggest food wasters – you are


 At present, organic waste is only recycled to a small extent, with some countries such as Germany and the Netherlands leading, while others including Italy and Belgium lag behind. But there are new technologies emerging to make this process easier.  For example, the Local Energy Adventure Partnership (LEAP) has created an anaerobic digester designed for an urban context: this machine can transform organic waste from residential or commercial buildings into compost and biogas that can fuel urban food growing.  Some experts also suggest that some food waste – if treated properly – could be used as animal fodder: a practice currently forbidden on hygiene grounds. If reinstated, this measure could reduce the environmental impact of grain cultivation, as less is grown to feed livestock.  Another option is to decrease demand for agricultural land by growing food in cities, where more people need it, thereby reducing the distances food has to travel. This would also allow producers to map and match consumers’ demand more effectively, by producing close to the places where food is consumed.  There is a lot of research on urban agriculture and how cities can support it, spanning from vertical farms - hydroponic systems enabling cultivation on vertical surfaces - to principles for planning cities that facilitate the use of land, rooftops and other spaces to grow food into a continuous green infrastructure.  


      Read more:
      How urban farmers are learning to grow food without soil or natural light


 In this area, too, it’s possible to find innovations designed to make urban farming easier and more sustainable. For example, The Farmhouse is a modular housing system suitable for vertical stacking that enables all residents to grow food. And Blockchain Domes is a patented system that uses excess heat from computer servers to provide optimal thermal conditions for greenhouses in colder climates.  The third option is to encourage people to change their diets. Growing middle-income groups in developing countries are consuming ever higher quantities of meat, cheese and eggs. In China, since 1990, consumption of beef and poultry has quadrupled. But the diet of farmed animals is heavy in grains, which instead could be used to feed people more efficiently. Also, cattle farming requires vast quantities of water and grassland, sometimes obtained through deforestation.   Getting people to eat less meat will help to ease the pressure on the world’s food system. In cities, governments, research institutions, communities and businesses can collaborate on food initiatives to give people healthier, cheaper and more sustainable choices – but this requires political will and organisation between different levels of government.   


      Read more:
      Bug burgers, anyone? Why we’re opening the UK’s first insect restaurant


 Clearly, each of these approaches has a limited scope of action, compared to agricultural techniques or strategies which can be deployed at an industrial level. But with so many promising proposals, there can be a many-pronged approach that that makes efficient use of the existing resources in cities, while also changing consumers’ habits. Together with these three changes, more effective policies for food justice and sovereignty can establish fairer food supply chains and more just distribution of food around the world."
"Anyone lucky enough to visit Ghana could do worse than order a plate of boiled yam and red-red – a stew made with beans and tomato paste. A Sunday morning treat in Europe might be homemade crepes and hazelnut chocolate spread. Both of these meals – though part of very different cuisines and eaten in different places – contain palm oil, an edible vegetable oil extracted from the fruit of the oil palm (Elaeis guineensis). The link between palm oil production and deforestation in the tropical regions where it is grown is well known, but few people realise how prevalent palm oil is in items consumed every day, such as cleaning products and biodiesel.  Global production of palm oil has increased rapidly since the 1990s, with plantations in Indonesia and Malaysia supplying around 85% of the global trade. 


      Read more:
      Message to the EU: you have the chance to stop fuelling devastation in the Amazon


 Many of these plantations have replaced natural forests and drained carbon-rich peatlands. In Indonesia alone, palm oil is cultivated by more than 4 million smallholder farmers, employing more than 7 million labourers throughout its supply chain, and in 2017 exports contributed over USD$23 billion to the country’s economy.  The European Parliament issued a resolution in 2017 to phase out and eventually ban biofuels made from palm oil. The proposed ban could reduce demand for palm oil, but many, including the International Union for the Conservation of Nature, aren’t sure it will be effective in stemming deforestation. Malaysian farmers meanwhile argue it will harm their livelihoods.. A ban could even harm the environment by ending efforts to work with countries that are developing sustainable palm oil production that can also reduce poverty.  Just under half of the EU’s palm oil imports are used for biodiesel. Despite the importance of palm oil to Indonesia’s economy, the impact of any EU ban would likely be small. Indonesia exports two thirds of its biodiesel production, but only around one fifth of that goes to EU countries.  Indonesia might compensate for lost sales in the EU by increasing sales to large importers such as India and China. An EU ban could set back Indonesia’s efforts to manage its forests and palm oil trade more sustainably as these customers aren’t currently committed to sustainable sourcing. Unintended consequences like these highlight why bans can be crude policy instruments.  The EU ruled that renewable fuels such as biodiesel must comprise 10% of transport fuel by 2020. This was intended as an implicit ban on fossil fuels comprising the final 10% of vehicle diesel, but banning particular crops like palm oil for biofuels and keeping a biofuel requirement simply diverts the problem. This is particularly so if the EU continues to meet the 10% requirement using “first generation” biofuels – those derived directly from food crops, such as soy or rapeseed. Replacing food crops to meet increased demand for bio-ethanol production places pressure on land and could increase global food prices, hurting low-income households most.  Better approaches would target the interconnected problems of carbon emissions, deforestation and poverty. EU countries could support the sustainable cultivation of palm oil, breaking the link between oil palm expansion and deforestation in producer countries. One way to do this is planting on degraded land rather than replacing forest. This avoids the negative impact of a ban on the livelihoods of millions of farmers. Demand for fossil fuels could be reduced more effectively by making public transport more accessible, affordable and reliable. Incentives for people to buy electric cars, through subsidy and a higher density of charging points, could also help. Indonesia and the EU have already worked together on this issue with some success. A voluntary partnership agreement between the two in 2003 helped Indonesia reduce illegal logging and export timber to the EU. But given that most of Indonesia’s palm oil exports go to countries outside of the EU, a global approach is needed.  The UN Environment World Conservation Monitoring Centre investigates the sustainable trade in forest products. It hopes to understand how incentives for supplier and producer countries can ensure trade improves livelihoods, prosperity and the natural environment. In an increasingly interconnected world, seemingly sensible decisions made in one place can have unintended consequences elsewhere. An EU palm oil ban, designed to protect tropical forests, might instead harm the livelihoods of farmers and increase forest loss if countries such as Indonesia and Malaysia switch to markets with fewer environmental checks and balances. This article was amended on May 9 2019 to make clearer that an EU ban has not yet been enacted."
"**All of Hertfordshire, Bedfordshire and Buckinghamshire will be in tier two when England's lockdown ends on 2 December.**
Luton will remain in tier two, where it was placed in the previous version of the tier system prior to the second lockdown.
All other areas were in tier one.
It means households cannot mix indoors and the rule of six applies outdoors, with exceptions for childcare and support bubbles.
Non-essential retail outlets, places of worship and leisure facilities will reopen in line with government guidance.
Hospitality settings that serve alcohol must close in tier two, unless they operate as restaurants and alcohol can only be served with substantial meals.
Prior to the second lockdown, Luton Borough Council placed the town in tier two after a ""worrying"" number of admissions to hospital and a rise in deaths.
It is one of the areas which has received the new rapid ""lateral flow"" tests, for mass testing.
Lucy Hubber, interim head of public health, said ""deprivation, and multi-generational households with lots of people living in them"" were factors in Luton's high case rate.
""While there has been a levelling off in the infection rates over the last week due to the sacrifices people are making, our rates are still too high with large numbers of new positive cases being reported each day,"" she said.
Luton's Labour council leader, Hazel Simmons, said the fact the town's rates had levelled off was ""encouraging"" but they had not decreased as much as other parts of the country during the current lockdown.
""As a town we have worked together in the past to lower infection rates and we are confident we can rise to the challenge again,"" she said.
The rate in Luton was just below 300 cases per 100,000 people in the week to 21 November - the highest in the Eastern region.
Luton has seen a fall in cases week-on-week, but now has the 61st highest rate in England.
All other districts in Bedfordshire, Buckinghamshire and Hertfordshire have fallen week-on-week.
The whole of Hertfordshire will be in the same tier, even though there is a range of case rates across the 10 district areas in the county.
David Williams, Conservative county council leader, said the decision was based on ""a series of indicators that reflect the continued prevalence of the virus in the county"".
""Now it is more important than ever that we redouble our efforts to ensure that the recent downward trend in our infection rates is sustained in order to secure a move to tier one as soon as possible,"" he said.
Peter Taylor, Liberal Democrat elected mayor of Watford, said: ""Despite seeing a plateau in the number of cases locally, the numbers are still higher than where we want them to be.
""But we have been making progress recently in ensuring the case numbers are not rising, so we must continue to do what we can to not go backwards.""
Andrei Lussmann, from the restaurant group Lussmanns, said: ""Restaurants are the safest social places in the country as far as Covid is concerned, with provable low transmission rates.
""Restaurants are just not where the problem is and all the evidence has shown this so I feel like the restaurant sector has been held to ransom unfairly and unnecessarily.""
Christo Tofalli, landlord of Ye Olde Fighting Cocks in St Albans, said: ""We will open and do the best that we can to welcome our families and guests in their bubbles over the festive period, but when we opened after the last lockdown, we only took 20% of our usual takings and this barely covers the costs of opening the doors.""
The government has chosen to divide Beds, Herts and Bucks up into counties for Covid restrictions, meaning everybody is in tier two.
The decision might seem harsh to people living in parts of Central Bedfordshire which has Covid rates per 100,000 people that are much lower than their neighbours in Luton.
In fact, Luton has rates that have topped 300, whilst Central Beds is down at 84.5, although the government doesn't just look at those rates when working the tier system out.
Hertfordshire is also combined, despite districts like Broxbourne being up at 207, whilst North Herts is down at 99.6.
The government plans to review the tier system at fortnightly intervals.
In his rationale for tier allocation, Health Secretary Matthew Hancock said Bedfordshire and Milton Keynes were in tier two because the overall case rate was still increasing and there was pressure on the local NHS.
He added there was a ""broadly stable or improving picture across Buckinghamshire"" with a case rate at 138 per 100,000 of the population, but the case rates remained too high for allocation to tier one.
Buckinghamshire's Conservative council leader Martin Tett said: ""Whilst it is disappointing not to have remained in tier one, we recognise that we must all cooperate in order to retain the advances made during the current lockdown and help save lives.""
Up to 2,000 people will be allowed in football stadiums in tier two areas, subject to stadium safety teams, local authorities and the English Football League (EFL) giving approval.
The EFL has been looking at the possibility of shifting some of next week's fixtures to take advantage of the plan.
Luton Town FC manager Nathan Jones said that potentially having 2,000 Hatters fans in the stadium on Wednesday was ""wonderful news if we can get all the processes in place"".
""We get just over 10,000 [fans usually] and it's like it's 45,000 so I'm sure 2,000 can do a great job,"" he said."
"**Norfolk will be placed in the second tier of restrictions when England's second lockdown ends on 2 December, it has been announced.**
People in tier two areas, classed as high, cannot socialise with other households indoors.
Up to six people can meet outdoors, and pubs and restaurants must shut at 23:00 GMT.
Prior to England's second shutdown, Norfolk was in tier one, which was the lowest level of Covid restrictions.
Many have reacted with disappointment that Norfolk has not been placed in tier one, along with Cornwall, the Isles of Scilly and the Isle of Wight.
Norfolk County Council said there had not yet been a ""sustained"" decline in the virus, with the government reaching its decision based on rates in the over 60s and the capacity of hospitals, among other factors.
Cases among the over 60s are high at 100 per 100,000 and are increasing in South Norfolk and Norwich.
Currently, 162 people are being treated for coronavirus in Norfolk and Waveney hospitals, compared with 141 on 13 November.
Overall, every district in the county has seen rates fall, with the exception of North Norfolk, which previously had one of the lowest in the country.
There were 1,100 coronavirus cases in Norfolk in the week up to 21 November, down from 1,345 in the previous week.
At one point the case rate in South Norfolk had tripled week-on-week.
This was down to an outbreak in Wymondham, which was blamed on a ""lack of discipline"" by the local council.
County council leader Andrew Proctor said: ""People in Norfolk have worked so hard to pull together to protect each other and to protect our county and for this reason we had hoped we would be able to move into tier one from next week.
""However, in reaching the decision the government has looked at how the virus is progressing, rates in the over-60s and the capacity of local hospitals and concluded that the risk posed by household visits remains too high.
""The numbers are beginning to fall but we are not yet seeing a sustained decline in the virus.
""That means that we will move into tier two from next week, a decision that we accept but one that I know many people will find difficult.""
Conservative North Norfolk MP Duncan Baker said he was ""hugely disappointed"" about the tier two decision and said he would be making the case for Norfolk at a ministerial meeting later.
""Clearly the government has taken a very cautious approach,"" he said.
""What appears to have not gone in our favour is the percentage of those taking tests and being positive is too high and the case rate in the over-60s is too high.""
He said he would tell ministers that the majority of areas were improving and if the infection rate continued to fall, Norfolk should return to tier one as soon as possible, at the first review on 16 December.
Businesses affected by the decision must be given support, he added.
Norfolk Chief Constable Simon Bailey said: ""While we might all have our own opinions on these measures, we must take personal responsibility for making sure we adhere to them, the fact remains they are in place to protect us all and we need to follow them.
""As police, we don't make the rules, we enforce them and I would urge each and every one of you to do the right thing and play your part in being risk aware, protecting yourselves, your loved ones and the county as a whole by sticking to the regulations.""
Under tier two, pubs and bars must close unless operating as restaurants.
Businesses which can open include non-essential shops, gyms and leisure centres, hairdressers and cinemas.
Up to 2,000 spectators can attend Carrow Road to watch Norwich City play, and grass roots sports can return.
Norwich are yet to make a formal statement, but a spokesman said the club was ""incredibly proud"" of the work that went into its pilot fixture, when 1,000 fans watched the Canaries draw 2-2 with Preston North in September.
It was among the first Championship clubs to play in front of a crowd since the pandemic."
"
Dr. Roger Pielke Jr. explains why some leftist bloggers set themselves up for failure when they espouse their intellectual superiority. Screaming “hell, high water, global boiling, climate disruption, etc ” while at the same time saying “you’re too dumb to understand it” looks to be an epic “failure to communicate”.

He writes:
If  you spend anytime at all perusing the blogosphere, you will find a  common theme coming from self-described liberal or progressive bloggers,  and that is that those on the political right are ignoramuses. 
The  argument is that they are just too stupid to know what’s what – they are  even anti-science, rejecting knowledge itself — and consequently they  support dumb candidates advocating ignorant policies. Such arguments are  particularly evident in the corner of the blogosphere that discusses  the climate change issue.  This line of argument of course is a variant  of the thinking that if only people shared a common understanding of  scientific facts they would also share a common political orientation  (typically the political orientation of whomever is expressing these  views).
Read his whole post here where he explains why.
Or buy his book:
click for more
The Climate Fix: What Scientists and Politicians Won’t Tell You About Global Warming is now available at Amazon.com
Why has the world been unable to address global warming? Science  policy  expert Roger Pielke, Jr., says it’s not the fault of those who  reject  the Kyoto Protocol, but those who support it, and the magical  thinking  that the agreement represents.
In The Climate Fix,  Pielke offers  a way to repair climate policy, shifting the debate away  from  meaningless targets and toward a revolution in how the world’s  economy  is powered, while de-fanging the venomous politics surrounding  the  crisis. The debate on global warming has lost none of its power to   polarize and provoke in a haze of partisan vitriol. The Climate Fix will bring something new to the discussions: a commonsense perspective and practical actions better than any offered so far.
Editorial Reviews via Amazon
From Publishers Weekly
Pielke (The Honest Broker) presents a smart and hard-nosed analysis of  the politics and science of climate change and proposes  a commonsense  approach to climate policy.  According to Pielke, the  iron law of  climate policy  dictates that whenever  environmental and economic  objectives are placed in opposition to each other,  economics always  wins.  Climate policies must be made compatible with economic growth as a  precondition for their success,  he writes, and because the world will  need more energy in the future, an  oblique  approach supporting   causes,  such as developing affordable alternative energy sources rather  than  consequences,  such as controversial schemes like cap-and-trade,  is more likely to succeed.
Although some may protest on principle the  suggestion that we accept the inevitability of energy growth, Pielke’s  focus on adaptation to climate change refreshingly sidesteps the  unending debate over the reality of anthropogenic climate change, and  opens up the possibility for effective action that places  human dignity  and democratic ideals at the center of climate policies.
The book is available at Amazon.com and I think it is destined to be a best seller in the “Global Warming” category.

<a href=”http://www.amazon.com/Climate-Fix-Scientists-Politicians-Warming/dp/0465020526/&amp;tag=wattsupwithth-20″ target=”_blank”><img src=”http://3.bp.blogspot.com/_0ZFCv_xbfPo/S7SlIkFewJI/AAAAAAAAAUE/utA5rI7F5SU/s1600/Pielke-The+Climate+Fix.jpg” alt=”” width=”250″ height=”380″ /></a>click for more
The Climate Fix: What Scientists and Politicians Won’t Tell You About Global Warming is now available at <a href=”http://www.amazon.com/Climate-Fix-Scientists-Politicians-Warming/dp/0465020526/&amp;tag=wattsupwithth-20″ target=”_blank”>Amazon.com</a><!–more–>
Why has the world been unable to address global warming? Science  policy  expert Roger Pielke, Jr., says it’s not the fault of those who  reject  the Kyoto Protocol, but those who support it, and the magical  thinking  that the agreement represents.
In <em>The Climate Fix</em>,  Pielke offers  a way to repair climate policy, shifting the debate away  from  meaningless targets and toward a revolution in how the world’s  economy  is powered, while de-fanging the venomous politics surrounding  the  crisis. The debate on global warming has lost none of its power to   polarize and provoke in a haze of partisan vitriol. <em>The Climate Fix</em> will bring something new to the discussions: a commonsense perspective and practical actions better than any offered so far.
Editorial Reviews via Amazon
From Publishers Weekly
Pielke (The Honest Broker) presents a smart and hard-nosed analysis of  the politics and science of climate change and proposes  a commonsense  approach to climate policy.  According to Pielke, the  iron law of  climate policy  dictates that whenever  environmental and economic  objectives are placed in opposition to each other,  economics always  wins.  Climate policies must be made compatible with economic growth as a  precondition for their success,  he writes, and because the world will  need more energy in the future, an  oblique  approach supporting   causes,  such as developing affordable alternative energy sources rather  than  consequences,  such as controversial schemes like cap-and-trade,  is more likely to succeed.
Although some may protest on principle the  suggestion that we accept the inevitability of energy growth, Pielke’s  focus on adaptation to climate change refreshingly sidesteps the  unending debate over the reality of anthropogenic climate change, and  opens up the possibility for effective action that places  human dignity  and democratic ideals at the center of climate policies.
The book is available at <a href=”http://www.amazon.com/Climate-Fix-Scientists-Politicians-Warming/dp/0465020526/&amp;tag=wattsupwithth-20″ target=”_blank”>Amazon.com</a> and I think it is destined to be a best seller in the “Global Warming” cate



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e878f0215',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGermany’s flagship political newspaper the Frankfurter Allgemeine Zeitung has an online report on an EU internal memo that bubbled up to public attention.
An EU Commission internal strategy paper “calls for an end to subsidies for solar and wind energy by EU countries and that this should be done as soon as possible”. EU Energy Commissar Günther Oettinger wants to officially present the paper in Brussels. This, writes the FAZ, will provide the German government with cover for its plan to cut its own subsidies for solar energy by 30%.
Despite all the symbolism and lip service by Merkel’s coalition government in support of wind and solar energy, its actions tell a different story. They don’t want them any more!
Also according to the paper, the price of photovoltaic systems have dropped 48% over the last five years. So it’s strange how that now solar energy is approaching affordability, the US government now wants to slap massive tariffs on Chinese imports and make the price totally unaffordable again.
The consumer has had to bear the high price of subsidized solar energy in the EU. The FAZ writes:
…the strong expansion of solar and wind power has caused the costs for consumers, and in some cases for taxpayers, to rise quickly. Because of the bad economic situation, energy costs for many people is often too high. The prices of energy source such as sun and wind thus should be left to the market forces as quickly as possible.”
Even with the generous subsidies, solar energy in Germany has still failed as most of Germany’s solar module manufacturing has been crushed by foreign competition in Asia, shedding thousands of jobs. Eliminating subsidies altogether would be a certain death blow.
The Commission also calls for a uniform support system of other alternative energy sources, and criticizes Europe’s patchwork of different support programs for renewable energy. This has led to the inefficient use – for example massive solar systems being constructed in northern Germany where the sun hardly shines and windmills are built where the wind hardly blows.
Whatever happens, one thing is certain: Europe is beginning to realize that green energies aren’t what they were once made out to be.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe English language Swedish online news site The Local.se/ reports on how the weather in Sweden has been so far during the month of June: wet & cold.

Chilly June hits Sweden. (Photo from Wikipedia, taken by Mark A. Wilson, Department of Geology, The College of Wooster).
According to the Swedish Meteorological and Hydrological Institute (SMHI), temperatures have been well below average in June, at just 13.3 degrees Celsius. Normal is 15.2°C.
On June 2, the temperature in Stockholm rose only to 6°C, the coldest high in 84 years, read more here. Earlier in the month one town recorded a temperature of 6°C below zero – the coldest June temperature in Sweden in 20 years. Snow even blanketed parts of northern Sweden.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Normally in the month of June, the mercury rises to 25°C or more on just days 5 days on average. This June the mercury never reached that mark. In fact it didn’t even reach the 22°C mark. The high temperature for June in Stockholm was only 21.6°C. This is only the second time the temperature has failed to reach 25°C in June in 92 years.
What’s behind the unusual cool weather? An SMHI spokesman explains it to us:
Sweden’s climate has become both warmer and rainier because of global warming, and rainfall and storms have increased in recent years.”
I’m glad he cleared that up.
June has also been a very wet month. According to the SMHI, Stockholm recorded a record rainfall so far for the month: 145.8 millimetres, the most since records began in 1786.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAl Gore and others have shamelessly exploited public ignorance about the polar bear in order to spread fear. Ed Caryl tells us why the polar bear will do just fine.
================================================
The Polar Bear
by Ed Caryl
The climate change “team” regularly invoke the polar bear as the “poster-boy” for the supposed danger in catastrophic anthropogenic global warming.

Figure 1: Female polar bear (ursus maritimus) near Kaktovik, Barter Island, Alaska. (Source: Wikimedia Commons).
Origin
The polar bear, ursus maritimus, is descended from the Brown Bear, ursus arctos. The genetic split took place sometime between 300,000 and 600,000 years ago, but the split was never fully complete. Polar bear mitochondral DNA (mDNA) is descended from a female brown bear, perhaps from Ireland, which dates back to the middle of the last ice age, 50,000 to 100,000 years ago. During that encounter, a male polar bear mated with a female brown bear. The subsequent offspring then mated with polar bears, and the mDNA from that female spread across the population. That suggests that population pressure on polar bears doesn’t necessarily happen only when it warms up, but also when it gets very cold. Both conditions reduce the first-year sea ice that makes prime habitat for the seals that polar bears favor; cold covers it with glaciers and warm melts it.
There isn’t much genetic difference between polar bears and other brown bears. They can freely mate, and the offspring are fertile, indicating that they are the same species. The Latin name should be ursus arctos maritimus, similar to the grizzly bear ursus arctos horribilus. These “hybrids” happen often in the Canadian Arctic, and even have locally bestowed names, Grolar or Pizzly Bears. The environmentalists have decried these occurrences, claiming that it will lead to the demise of their favorite animal. In my opinion, this provides genetic diversity that has assisted survival in the past, and will in the future.
Diet


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Like all bears, polar bears are omnivorous. They eat anything organic. Except for man, they are the apex carnivore in the Arctic. They hunt and eat everything, sometimes
including man-flesh, sometimes including their own species. After a meal of seal blubber, they will sometimes seek out grass for a salad. In the summer, when they are ashore, they do not starve. They will hunt birds, forage for eggs, hunt and kill caribou and small mammals, eat lichen, moss, and mushrooms, forage on shrubbery and berries in season, harvest kelp and other seaweed, eat fish, shellfish, and other sea invertebrates, as well as sort through man’s local garbage. They have been seen hunting sea birds and ducks by swimming under them and ambushing them from below. They hunt and kill beluga whales, narwhales, and walrus twice their size. Obversely, the only animal that can kill a polar bear is man. Most carnivores kill by strangulation; polar bears kill by crushing the skull of their prey in their jaws.
The polar bear population is always food-limited. If food is plentiful, mother bears can raise more cubs and the population increases. The Arctic is not a place where food is easily obtained, so it takes 25,000 to 50,000 square kilometers to support a bear family. If food is plentiful, bear density may be limited by male bear predation on cubs
not their own. Cubs, if readily available, are an easier kill than seals. In recent years, seal and bear hunting in Arctic Canada has been curtailed, increasing both the seal and bear population. Bears are not being hunted (except for about 500 a year by natives in Nunavut, and illegal hunting in Russia) so the only mortality is old age, disease, parasitism (primarily trichinosis), and starvation. On autopsy, all four causes of death look very similar. Because of some hunting, the bear population in Nunavut seems to be the healthiest, and may be growing despite the hunting, due to the increasing number of seals.
Habits and Habitat
In the spring, polar bears prefer hunting seals on first-year sea ice. At that time of year, seals are almost the only food source available, plus they are high in fat, a prime source of energy. The bears hunt seals by staking out a breathing hole. Seals may have several holes each, so hunting is primarily a waiting game. As the sea ice melts, bears turn to seal haul-out locations and seal pups. As the season further advances, other species such as walrus, belugas, and narwhales may become available. Ursus maritimus lives up to the Latin name with ability to swim long distances. The record for a female wearing a GPS color is over 600 km in 9 days, though the mean distance and time for the bears in this study was about 100 km over three days. One female bear was tracked from Prudhoe Bay in Alaska to Greenland. (Factoid: only female bears are collared, because of the male bear anatomy. Male bears have such a muscular neck that a collar won’t stay on.)
The mating season is in the late spring, blastocyst (fertilized egg) implantation is delayed until fall. Bears den-up in October and the one to three one-pound pups are born in November or December. Mother and cubs do not emerge until March or April. Dens are in snowdrifts ashore or, north of Greenland, in multi-year ice where pressure ridges form shelters. Polar bears don’t really hibernate, but they do sleep a lot when denned. They also fast during this period. A female polar bear can fast for up to eight months and still be capable of nursing cubs and hunting. Cubs stay with their mother for two years in the low Arctic and up to three years in the high Arctic. During this period the cubs learn to hunt from their mothers.
All the above is similar to grizzly bears. Grizzly bears also hunt seals on sea ice, and polar bears will hunt caribou on land, just like grizzlies. The differences are fairly minor: grizzlies have a fat hump that polar bears lack; polar bears have clear, hollow shaft fur that appears white, black skin and thick fur on their paws which grizzlies lack. Pizzlies tend to have the clear fur, with black around the nose and eyes, with a grizzly head shape. The clear insulating fur and black skin is an adaptation to the Arctic that minimizes heat loss when it is dark and maximizes heat gain when it is sunny. For a polar bear, high energy efficiency means fewer seals are required to live and reproduce. The clear fur also provides camouflage, but this is only an advantage when stalking seals on ice.
The polar bear population is food-limited, not ice limited. Polar bears wander over wide areas. They do not have a fixed territory. The population freely intermingle. If sea ice recedes in one area, bears move to where it remains. When it all melts, they move to land. If they find themselves on an island with no food resources, they swim to the next spot of land. In the Arctic, polar bears and humans compete. If the bear and human populations use up the food resources in an area, both populations move somewhere else, or limit reproduction.
The polar bear population may be near the historic high at 20 to 25,000 animals. In the 1950’s and 60’s, the population may have been as low as 5,000. Sea-ice extent is cyclical. Bears have withstood many cycles of sea-ice shrinking and growing, including previous interglacials. The polar bear population is in no danger.
=========================
Also read an excellent article here: polar-bears-polemics-and-climate-warming/
Share this...FacebookTwitter "
"Six months on from the UN’s landmark 1.5°C report, which urged immediate global action to prevent global warming from rising beyond this dangerous level, the Committee on Climate Change (CCC) has advised the UK government to go zero-carbon by 2050. The committee’s report asserts that the target constitutes the country’s “highest possible ambition” and that it is not credible to aim for an earlier date. We disagree. While the report does challenge the government to step up its climate ambition, our view is that creative carbon accounting and an unwillingness to prioritise the planet’s health over economic growth leaves the committee’s target lacking the urgency truly required to combat the climate emergency recently declared by even the government itself. Before assessing whether 2050 is an appropriate date, its important to unpack exactly how the committee defines net zero. Based on international guidance from the UN’s Intergovernmental Panel on Climate Change, the UK’s target only includes territorial carbon emissions – those that are emitted directly within the country’s borders. The committee’s report highlights that the country’s territorial carbon footprint has fallen by 30% from 2008. But as Swedish campaigner Greta Thunberg highlighted to parliament, using this figure as a mark of the country’s climate leadership amounts to nothing more than creative carbon accounting, glossing over the UK’s role in emissions that occur outside its borders. The UK economy is primarily driven by its service sector, and the value of its imports is roughly triple that of its exports. The production and transport of these imported goods are a direct consequence of the UK’s consumption habits, but these emissions aren’t counted by the committee because they occur beyond its shores. Including these emissions and excluding emissions from exports to other countries, the UK’s carbon footprint is 70% higher than the figure used by the committee. Crucially, this alternative definition shows that emissions are still closely tied to the UK’s GDP, only notably reducing during the most recent financial crisis and remaining steady in recent years. The CCC does say that it will monitor the UK’s consumption-based emissions, and highlights that the country must avoid “offshoring” its territorial emissions by importing more goods. But it stops well short of addressing the fundamental elephant in the room – that to seriously tackle emissions, the UK must move away from an economy that prioritises short-term growth over radical emission reductions. On the surface, the most glaring omission in past carbon budgets at least appears to have been addressed. Emissions from international aviation and shipping have long been excluded from national targets in favour of international reduction efforts such as the UN’s CORSIA or the EU’s Emissions Trading Scheme. The committee now argues that “emissions from international aviation and shipping cannot be ignored”.  But it only recommends their inclusion in the UK’s carbon budget from 2033. This is 14 years too late. UK aviation emissions must not grow in the next decade if it is to prevent the worst effects of global warming. The time to act on aviation and shipping is now. Even placing accounting issues aside, the 2050 target is unambitious and gives a false impression that there is time to play with. Lord Deben, chair of the committee, is almost certainly right that Extinction Rebellion’s demand that the country reaches net zero by 2025 is physically impossible. Shedding the country’s attachment to growth does not on its own lead to a neutral carbon ledger. The massive amounts of investment, innovation and infrastructure required to get there would not take full effect within six years. But to say that anything earlier than a 2050 target isn’t credible is a grave and dangerous mistake. At current levels of emissions, the world will reach 1.5°C of warming in 12 years. Each year that the UK delays radical action, the necessary yearly emissions cuts to hit net zero become greater, making it ever harder to avoid catastrophic warming. Even with immediate action, the world is still pinning hopes on carbon capture and storage technologies that may never work at scale. Working towards an earlier target with steeper emissions cuts would require initial uncomfortable changes, but would massively lower our reliance on these incredibly uncertain technologies. The report’s claim that the 2050 target represents the UK’s “highest possible ambition” speaks more to the country’s economic priorities than to reality. Ten years ago, the committee wrote that an 80% reduction in emissions by 2050 (against 1990 levels) was at the limit of feasibility. Now, the committee has changed its mind, stating that net zero can be achieved by the same date, for the same price: 1-2% of GDP. This small percentage is seen as the maximum acceptable cost of mitigating climate change, even in the face of the trillion-pound losses that are forecast if we do not take sufficient action. Is this really all the fate of present and future generations at risk of climate change is worth? The report does hold some positives. It is unequivocal that current policy is insufficient to achieve even the UK’s existing targets and urges a ramping up in actionable efforts. It criticises plans to phase out fossil fuel powered cars by 2040 as too late and too vague, and calls for the government to confront failures to plant enough carbon-absorbing trees and decarbonise heating systems. But in focusing on what is “feasible” rather than necessary, the committee’s trajectories simply do not reflect the radical carbon reductions the UK can make, and will only end the UK’s contribution to global warming on paper. The year 2025 may be an unrealistic target, but missing that by a few years is much less dangerous than hitting a 2050 target comfortably. We need to take every leap we can and fast, even if it is into the dark. As Greta Thunberg says, if your house is on fire, you don’t tell people that the fire brigade will be along in a few hours – you act. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterDr. Wolfgand Thüne
As the science of the warmists gets exposed as woefully inadequate, slipshod and flawed, all they can do now is give each other awards in pompous ceremonies in an effort to generate a (fake) sense of achievement and contribution. They’ve been reduced to a pretend world.
One example is the Technical University of Berlin recently awarding Prof. Hans Joachim Schellnhuber an honorary doctorate. Schellnhuber is Director of the infamously über-alarmist Potsdam Institute for Climate Impact Research (PIK) and is chairman of the WBGU Advisory Council.
Last year Schellnhuber and his WBGU published a “masterplan” calling for the dilution of democracy worldwide and forcing societies to take up a highly restricted “sustainable path” that would keep the planet from reaching his nine mathematically concocted “dangerous tipping points”. In it he advocates indoctrination and “changes in awareness”.
Schellnhuber even once publicly stated that 1 billion people would be the ideal human population for the planet, which would be like eliminating all the world’s people except China and letting them have it all to themselves.
Schellnhuber, however, having clearly drifted from science to radical policy formulation and advising, is coming under increasing fire from number of scientists and critics.
For example, Dr. Wolfgang Thüne, a retired German meteorologist and member of the European Institute for Climate and Energy (EIKE) commented yesterday on Schellnhuber’s honorary doctorate:
That Professor Dr. Hans-Joachim Schellnhuber, the inventer of the “tipping elements”, is now being celebrated and awarded an honorary doctorate is amazing. But what’s even more amazing is that he received this from the TU Berlin for his ‘outstanding scientific achievements in the fields of climate impact research and policy counselling’. Did the TU Berlin, in its addiction to political attention, even stop to consider just how much it is damaging its excellent reputation among the professional world?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The climate science by Schellnhuber & Co. is pure voodoo-magic spreading fear among the public and reaching big time into the pockets of taxpayers.
The Potsdam Institute for Climate Impact Research (PIK) is senselessly wasting the money of taxpayers. ‘Climate protection’ is a scientific swindle because the weather is not something that can be protected.”
Thüne even demands that the PIK be shut down immediately.
That would be a small but effective step in preventing the national debt from getting out of control.“
Dr. Wolfgang Thüne is a certified meteorologist, who for years was a meteorology expert for ZDF television, and has written about the falsifiications and fraud surrounding the UN IPCC. He is the author of numerous books. His latest work is: “Prophets in the Struggle for the Climate Throne. How primal fear is used in the struggle for money and power.”
The latest German skeptic book exposes junk climate science and shady climate politics.
 
Share this...FacebookTwitter "
"In the past ten years the UK’s electricity mix has changed dramatically. Coal’s contribution has dropped from 40% to 6%. Wind, solar power and hydroelectric plants now generate more electricity than nuclear power stations, thanks to rapid growth. Demand for electricity has also fallen, reducing the country’s dependence on fossil fuels. Thanks to these three factors, the carbon intensity of Britain’s electricity has almost halved, from more than 500g of CO₂ per kilowatt-hour in 2006 to less than 270g in 2018. Progress has been so quick that a fully low-carbon power sector in Britain has transformed from a faint pipedream into a real possibility, according to the CEO of one of the UK’s “big six” energy companies. Indeed, the National Grid now expects to be able to operate a zero-carbon electricity system by 2025. Already approaching that milestone on windy, sunny days, the country’s first hours of 100% low-carbon electricity could soon be here – but staying at 100% throughout the year will be much more difficult to achieve. So what does the journey to decarbonisation look like? To paint the UK’s energy future, it is important to first understand how electricity is generated today. The graph below is a visualisation of British electricity generation in October 2018. Periods of strong wind (in red) and sun (yellow) combined with nuclear power (green) meant that on some days, more than 75% of electricity came from low-carbon sources. With solar prices still decreasing and the government recently agreeing a major deal for offshore wind to produce one-third of the UK’s power by 2030, the country’s first hours of low-carbon power could arrive within the next five years. But the graph also highlights the other side to the UK’s energy story. When the wind is weak and the skies dark, low-carbon sources provide less than 25% of electricity generation. On average, low-carbon technologies accounted for more than 45% of British electricity in 2018 – and almost half of that came from nuclear plants. Saying goodbye to fossil fuels quickly might mean accepting that the ever-controversial form of energy will play some role in the UK’s electricity mix in the medium term.  Even with the aid of nuclear power, electricity consumption in Britain is set to increase dramatically in the coming decade. As electric cars continue their journey to the mainstream, traditional transport fuels will be replaced by electricity. The yearly energy demand of transport fuels is currently more than double the UK’s national electricity consumption. Similarly, plans to decarbonise the UK’s heat generation – currently 66% is generated by gas – by converting to electric heating systems will also place huge pressures on demand. During winter months, heat can consume more than three times the daily energy demands of electricity – and over a full annual cycle it constitutes 50% of total energy demand. Collectively, these factors will move the goalposts for 100% low-carbon electricity further and further away.  While the huge efficiency increase of electric vehicles over internal combustion engines should cushion the impact of electric vehicles on the UK’s energy future, the country will need to diversify its energy mix as much as possible to bring those goalposts back into sight. This means continued growth in wind, solar, hydro, biomass, energy efficiency and energy storage to carry the country through the calm, grey days. Precisely how much growth is needed depends exactly on the future of energy demand, but to give some perspective of scale, more than 80% of the total UK energy supply, including electricity, land transport and heat, still comes from fossil fuels. The tens of billions of pounds already invested in low-carbon electricity is just the start of the UK’s journey to decarbonised energy. It also means seeking alternative, non-electric methods to replace fossil fuels in heat generation. Capturing waste heat from industrial processes, geothermal heat from the ground and heat extracted from water bodies could all limit demands on the electricity sector and make it easier to achieve more low-carbon heat and power. Southampton already heats much of its city centre geothermally – and many cities can and should follow suit. Recent work published by the BritGeothermal estimates that geothermal energy alone could meet the UK’s heat demand for at least 100 years. Concerted and sustained effort from both government and individuals is required if the UK is to achieve a low-carbon nirvana in heat, transport and power. State support of the renewables industry through ensuring long-term investment security and regulations to create energy-efficient and electricity-generating new homes will be essential in the UK’s decarbonisation journey. The UK population will need to consume less energy individually, use energy more efficiently and use their voices and money to support renewable solutions. They will also need to elect representatives with a genuine ambition to decarbonise the country – rather than to commission new coal mines and fracking sites. Large-scale changes are already in motion. Shell recently stated that it wants to become the world’s largest electricity supplier and is among many oil giants investing heavily in renewables. While the need for new forms of energy presents big challenges for the UK it also offers a wealth of opportunities for the current generation to be part of an energy revolution. If the UK embraces the task, it could be joining Costa Rica, New Zealand and Norway as low-carbon powerhouses before the middle of the century. As one specialist at the start of his career and another nearing the end of his, we say bring that challenge on. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

When is it appropriate to privatize the work of public prosecutors? And does it make things better or worse when “cause” lawyering is at issue? As Jeff Patch reports at Real Clear Investigations, a project called the State Energy & Environmental Impact Center at New York University supplies seasoned lawyers to the offices of nine state attorney general offices, plus D.C. They serve there in such roles as special assistant attorney general while being paid by the NYU project, which is funded by and closely identified with former New York City Mayor Michael Bloomberg. The catch, which explains why the program is not likely to hold appeal for AGs in some other states: “Under terms of the arrangement, the fellows work solely to advance progressive environmental policy at a time when Democratic state attorneys general have investigated and sued ExxonMobil and other energy companies over alleged damages due to climate change.”   
  
  
Private funding of lawyers inside public prosecutors’ offices is not a new idea. Iowa’s AG office, for example, told Patch that it has employed legal talent from an American Bar Association‐​supported program. In another variation, it is not unusual for prosecutors to accept funding from the insurance industry for efforts to combat insurance fraud. Undergirding the political viability of these schemes is the (perhaps wobbly) premise that the state office is not farming out influence over politically or ideologically sensitive policy matters to outside groups that may have their own agenda.   
  
  
The AG offices participating in the program (Illinois, Maryland, Massachusetts, New Mexico, New York, Oregon, Pennsylvania, Virginia, and Washington state, as well as the District of Columbia) might plausibly argue that the projects they’re paying the Bloomberg embeds to work on are mostly ones they’d want to pursue zealously in any case, such as suing the EPA and other federal agencies over alleged lapses. Critics point to the ideologically fraught nature of the work and say the arrangement could violate some states’ ethics rules or generate improper conflicts of interest, as through an obligation to report activities back to the Bloomberg center.   
  
  
The spotlight on backstage doings at state AG offices arises from reports by Chris Horner of the Competitive Enterprise Institute based on public records requests that were fought tooth and nail by various AGs. (Besides the CEI report on attorneys general, Horner’s written a companion report on governors.) CEI is anything but a disinterested party in all this, of course, having been hit with a AG subpoena (later beaten back in court) over its supposedly wrongful advocacy on climate issues. That was itself part of a subpoena campaign targeting more than 100 research and advocacy groups, scientists, and private figures on the putatively wrong side of climate debates, which we and others decried at the time as a flagrant attack on rights protected by the First Amendment. 
"
"
Share this...FacebookTwitterA new paper authored by Reinhard Böhm of the Austrian Central Administration For Meteorology (ZAMG) refutes the notion that anthropogenic warming is causing an increase of climate extremes and making weather more variable and extreme.

Pressure – temperature – precipitation (Source ZAMG)
The paper uses the monthly resolved data of the HISTALP data collection, which provides 58 single series for three climate elements: air pressure, air temperature and precipitation, which start earlier than 1831 and extend back to 1760 in some cases.
The paper’s abstract writes:
The main goal is the analysis of trends or changes of high frequent interannual and interseasonal variability. In other words, it is features like extremely hot summers, very cold winters, excessively dry or wet seasons which the study aims at.”
The paper also concentrates on the recent three decades because “they are the first 30 years with dominating anthropogenic greenhouse gas forcing”.
Conclusion? No change!
The author of the warmist ZAMG doesn’t mince his words:
We can show that also this recent anthropogenic normal period shows no widening of the PDF (probability density function) compared to the preceding ones.”
Not only did the author find no change in variability, but he also detected a “centennial oscillating structure”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The abstract continues (emphasis added):
It shows that interannual variability changes show a clear centennial oscillating structure for all three climate elements in the region. For the time being we have no explanation for this empirical evidence.”
Please allow me to suggest one: take a look at the sun! As it stands, only an absolute moron can now remain oblivious to the huge, growing body of evidence pointing to solar activity cycles.
Böhm continues, reluctantly admitting there may be other mysterious factors out there playing a role:
We argue that it should not be an artifact of any remaining data problems, but of course a centennial cyclic effect based on 250 years of data only is not really well consolidated in terms of sample length. But it is at least an interesting new feature and the subject is open for scientific discussion and for further studies dealing with circulation effects, long-term memories in the oceans etc.”
Hooray! It seems they are beginning to acknowledge the oceans as a possible factor in climate change! Thus there’s hope that one day they’ll realize the sun may be involved as well.
Finally, CO2 Handel here writes:
Neither during the last 250 years nor the last 30 years, which have been strongly impacted by man, has the seasonal and annual fluctuation range hot-cold and dry-wet become greater.”
And
‘The results are certainly surprising for many,’ explains climatologist and study-author Reinhard Böhm. We often hear there are no longer any transitions between seasons and that spring and autumn, as well as winter and summer, are increasingly characterized by extreme cold-warm fluctuations. ‘Our study clearly shows that this is not the case.'”
That is quite an admission for an author from a warmist outfit like the ZAMG.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWhat follows is a press release from Longmont, Colorado-based Abound Solar (emphasis added): Read here!


Abound Solar is yet another government subsidized solar company that gets burned.

Abound Solar Press Release excerpt:
Abound Solar’s closure is an unfortunate but very real consequence of the continued slide in crystalline silicon (c-Si) pricing and the increased competition for limited global demand of solar modules,” said MJ Shiao, Senior Analyst at GTM Research. “Abound was still in the earlier stages of technology and commercial development and despite over $220 million in private investment and $70 million drawn from its $400 million U.S. Department of Energy (DOE) loan guarantee, simply didn’t have the cost and downstream reach to survive in the tumultuous solar market.
Abound’s cadmium telluride (CdTe) product had little differentiation from First Solar and also suffered from lower efficiency,” said Shyam Mehta, a Senior Analyst at GTM. “Produced at lower scale and likely a higher cost, the chance for survival in the current market environment was always slim. Unfortunately, Abound and other module manufacturing closures including thin film and c-Si suppliers alike, is still just the tip of the iceberg. GTM Research forecasts at least 21 GW of PV module manufacturing capacity to retire by the end of 2015.”
For further comment from GTM Research, please contact VP of Research, Shayle Kann at kann@gtmresearch or by phone at +1 617 500 4216.”
Read more here. RINOs got in on the act too!
Abound also had support from the Republican side. Indiana’s GOP governor, Mitch Daniels, supported an $11.85 million tax credit for the firm, and “two Abound investors were major Republican donors who have given more than $100,000 to Republicans in the last few years.” according to reports in the National Review.”
 
Share this...FacebookTwitter "
"

From the NOAA National Weather Service Office in Miami comes this year end report:
2010 South Florida Weather Year in Review
Coldest December on Record Concludes Year of Extremes
December 30th, 2010: Temperature and precipitation extremes marked the weather of 2010 across South Florida. A cool and wet January through March was followed by the hottest summer on record, and then concluded with the coldest December on record for the main climate sites in South Florida (details on the above mentioned periods will be included below).
Here are December 2010 temperature averages for select sites (through 7 AM Dec 30th):
* Location of observations for each location have moved since the first year of record, but are representative of the city for record keeping purposes.
** Present Miami Beach and Moore Haven temperature data may not be totally comparable to historical data due to difference in time of daily reports which causes double-reporting of low temperatures.

Complete statistics of the record cold December for all sites above (except Moore Haven) will be provided in Record Reports which will be issued early on Jan 1, 2011.
The main culprit behind the cold temperatures in December 2010 was the same one which caused the cold winter of 2009-2010; a strongly negative North Atlantic Oscillation (NAO) and Arctic Oscillation (AO). When these atmospheric oscillations are in the strong negative phase, they essentially “flip” the weather pattern across North America, with upper-level high pressure and relative warmth over Greenland and Northeastern Canada and upper-level low pressure and cold over the eastern Continental United States, including Florida (Figure 1). This pattern forces the jet stream to plunge south from northern Canada into the southeastern U.S., transporting Arctic air masses into Florida.
A pronounced shift in the ENSO (El Niño Southern Oscillation) phase was noted in 2010, from a strong El Niño, or warm, phase to a borderline strong La Niña, or cold, phase. While this may appear at first glance to be a key contributor to the temperature extremes noted across South Florida during 2010, it is believed that it was the strongly negative NAO and AO, not the ENSO phase, which contributed to the cold temperatures in early and late 2010. A strongly phased NAO/AO operating on shorter time scales can override the longer-term ENSO phase.
As mentioned above, South Florida experienced its hottest summer on record in 2010 (with the exception of Naples which recorded its second hottest recorded summer). Despite the record hot summer, average yearly temperatures at the main climate sites will end up around 1 degree below normal, which will be the coolest calendar year since the early and mid 1980s, and among the top 10 on record (except for Miami). At secondary sites Miami Beach and Moore Haven, it was the coolest year on record (please note caveat below table).
Here are the 2010 temperature averages for the year for the primary climate sites through December 29:

** Present Miami Beach and Moore Haven temperature data may not be totally comparable to historical data due to difference in time of daily reports which causes double-reporting of low temperatures.
Some other interesting 2010 temperature statistics:
– Miami International Airport (MIA) observed 103 days of temperatures at or above 90 degrees, the 4th most on record. The average number of 90+ degree days per year is 51. MIA also had a record 45 days of low temperatures of 80 degrees or higher, besting the previous record of 39 set in 2009. The average number of 80+ degree low temperature days per year is 13. On the other end of the thermometer, MIA had 6 mornings with low temperatures below 40 degrees. This ties the 5th most number of sub-40 degree days on record. The average yearly number of sub-40 degree days is 2.
– Fort Lauderdale/Hollywood International Airport (FLL) observed 9 days of low temperatures below 40 degrees. This ties the 4th most number of sub-40 degree days on record. The average yearly number of sub-40 degree days is 3.
– Palm Beach International Airport (PBI) observed 106 days of temperatures at or above 90 degrees, the 8th most on record. The average number of 90+ degree days per year is 56. PBI also had a record 34 days of low temperatures of 80 degrees or higher, crushing the previous record of 17 set in 1900 and 2002. The average number of 80+ degree low temperature days per year is 6. On the other end of the thermometer, PBI had 18 mornings with low temperatures below 40 degrees. This easily breaks the previous record of 10 days set in 1920 and 1981. The average yearly number of sub-40 lows at PBI is 3. Six of the 18 days occurred in December, which breaks the previous monthly record for December of 5 set in 1962.
– Naples Regional Airport (APF) observed 125 days of temperatures at or above 90 degrees, the 12th most on record. The average number of 90+ degree days per year is 109. Naples also observed 13 days of low temperatures below 40 degrees. This ties the 5th most number of sub-40 degree days on record. The average yearly number of sub-40 degree days is 3. Eight of the 13 days occurred in December, which breaks the previous monthly record for December of 7 set in 1981.
Full report at NOAA/NWS here:
http://www.srh.noaa.gov/images/mfl/news/2010WxSummary.pdf
h/t to Joe D’Aleo
SEE ALSO:
USA record lows outpace record highs 19 to 1 this week
Update 1/1/11 1:11:11 PM:  obligatory Drudge Link screen-cap for Posterity:  great way to start 2011!



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e85d40741',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"When it comes to building sustainable buildings, humans have a lot to learn from termites. A recent study that colleagues and I published in Science Advances explains how some African termites maintain cool and stable temperatures in their nests throughout the year. The answer lies in the wall of the nests, composed of tiny but highly-connected pores. Today’s architects and builders are continuously seeking new and improved ways to cool buildings without using more energy. In fact, growing demand for air conditioners is one of the most critical blind spots in today’s energy debate – it has been projected that 10 new air-conditioning units will be sold every second for the next 30 years. As, the planet warms, people will increasingly need to build sustainable buildings that do not rely on vast amounts of energy for air conditioning. This is where termites come in. Termites – not to be confused with their distant relatives, ants – are insects with sophisticated social structures built on hierarchies – they have kings, queens, workers and soldiers. Like humans, these cockroach cousins prefer to build their own environment rather than adapting to one. For example, some termites have mastered sustainable fungus farming which helps them digest their food. One can also find termites living in arid regions that may be hostile to their bodies. To counteract this harsh environment (and in some cases to sustain fungus farming) they build structures that are sufficiently cool and humid – these are the famous mounds, or nests. For these reasons, termite nests have been widely studied as examples of effective ventilation and temperature control. Yet, exactly how they build their constructions has until recently remained somewhat poorly understood. Some species of termites, those that do farm fungus, build towering nests that are ventilated by a complex system of tunnels and openings. These tunnels regulate the nests’ ventilation the same way chimneys and windows work in a human house. In fact, a few buildings have been inspired by termite nests, such as the Eastgate Centre in Zimbabwe which successfully uses 90% less energy than a similarly sized building next door. But those termites that do not grow fungus build nests which appear smooth, and have no apparent openings. In spite of this, ventilation remains important for these termites. Until recently it wasn’t clear how these termites were able to keep air moving around their nests to avoid suffocating. That’s what colleagues and I – a team of biologists, engineers, and mathematicians in France and the UK – set out to investigate. Our research found that the tiny building blocks that make up the nests itself are optimised for these processes to occur naturally and effectively. We focused on a non-fungus-growing species, the grass harvesting termite, and began by excavating nests we found in Senegal and Guinea in West Africa. The nests are made of soil particles mixed with water and termite saliva. Despite not having apparent openings, the walls are composed of micrometre-sized pores.  We took tiny samples from the nests and put them under a micro X-ray scanner akin to that used in hospitals – but one which is capable of scanning with much finer resolutions. This revealed the termites build outer walls that actually contain both small pores and a series of slightly larger and interconnected pores. In fact, about 99% of the pore space was linked up.  Using the X-ray scans, we were able to build a digital version of the nests, much like the digital worlds that exist in computer games. We then simulated the nests in the conditions in which these termites live – dry in Senegal and wetter in Guinea.  We found that the links between the big pores allows air to “percolate” through the outer wall in the same way coffee is strained through a filter. This is key to ventilation and regulating temperatures.  By creating tiny ventilation passages, the pores of the nests manage gas exchange in a similar way to human lungs. But where a pair of lungs deflates and inflates to drive ventilation, in these nests the air is driven in and out by differences in temperature between the inner nest and the outside world. We still don’t know whether termites create these interconnected pores following simple construction rules, or as a consequence of physical constraints resulting from the way pellets of soil are packed together. But our research does suggest that it is the structure, not the material used, that is key to ventilation. Especially considering that samples from the two regions are composed predominantly of different materials (sand in Senegal and clay in Guinea). The challenge is now to derive the same design principles and scale them up for humans. No one wants to live in an exact copy of a termite’s nest, complete with fungus chambers. But learning from termites might involve creating new synthetic building materials with connected pores. It is important to remember that human ingenuity allows us to not merely copy forms found in nature, but to emulate the mechanism by which such forms emerge."
"
Share this...FacebookTwitterGerman geologist Dr. Sebastian Lüning and chemist Prof. Fritz Vahrenholt react to Anthony Watts’s press release concerning the quality of US temperature data at their blog.
Unlike myelf (I expected an unsurvivable nuclear bomb to be dropped), Sebastian Lüning and Fritz Vahrenholt find the implications of the new findings to be potentially devatsting. They write (excerpt):
A Shocking Development: Warming in the USA over the last 30 years is only half as much as previously assumed
Anthony Watts did not disappoint. Indeed it is about the release of an important new publication that involves the warming trend of the last 30 years. We had already introduced that problem here at this blog a few days ago (see our blog article “The wonderful world of temperature data corrections: And suddenly the trend reversed…“). It gets down to how justified are ‘corrections’ to the official data? We had reported that the data changes oddly always produced a signficant acceleration in warming with respect to the raw data, even though factors like the urban heat island effect intuitively suggest the oppsite correction is needed.
Authors in addition to Anthony Watts included Stephen McIntyre and John Christy. McIntyre is known because of his impressive error analyses of the famous hockey stick digram. Christy is a renowned expert for satellite temperature data at the University of Alabama in Huntsville. The manuscript will soon be submitted for publication by a journal. […}


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




[…] The result of the new study is shocking: Instead of correcting downwards temperatures that were heated by the urban heat island effect, the official US administration offices apparently corrected data from qualitively reliable stations upwards, which appears to be unjustifiable. If the result is confirmed, then it would be a shocking development. The warming in the USA over the last years would be far less rapid than what has always been assumed. And because similar errors are supected elsewhere, the issue could quickly gain global relevance.”
I asked Dr. Lüning to comment more on why this could have global implications. He kindly replied by e-mail:
Bit by bit others who might jump onto the train will now use the same methodology worldwide and will probably find that it really affects the global curve.”
Lüning and Vahrenholt also provide the press release in German at their site.
Take it from them; they’re experts. This could very well have global consequences. One thing is sure: It’s a an utter embarassment for the NOAA, and the perceived reliability of US and international data will be in question for years to come. Faith in the surface station data is crumbling.
 
Share this...FacebookTwitter "
"When we think of mass habitat extinction, colourful, diverse and highly visible ecosystems such as tropical rain forests and coral reefs come to mind. Approximately half of global shallow water coral reefs and forests have been lost over the last few hundred years, but there are glimmers of hope. Deforestation rates are declining and some corals have shown resilience to stress from climate change. A far less visible ecosystem crisis has occurred relatively recently beneath the ocean’s surface. A study revealed that 85% of global oyster reefs have been lost over the last 150 years. Of those remaining, over one third are so depleted that they no longer function as ecosystems, particularly those in Europe, North America and Australia. Only a few healthy oyster reefs remain in South America, and even these are 50% of their prior abundance, making oyster reefs one of the most threatened habitats on Earth. Oysters are “ecosystem engineers” like corals – they create three-dimensional structures as they settle and grow on each other. Left undisturbed, these oyster reefs provide a habitat for an incredible biodiversity of organisms, serving as a food source, nursery ground and refuge for many species, boosting fish stocks. Oysters also have an incredible ability to clean seawater. A single oyster can filter almost 200 litres of seawater daily, eating the phytoplankton and organic matter suspended within it. Oysters improve water quality and clarity, preventing large scale algal blooms and the potential consequences of mass fish mortality and deadzones due to depleted oxygen. Removing oyster reefs increases wave energy and erosion of saltmarshes and the corresponding coastline. Just as tropical coral reefs protect mangrove forests, oyster reefs provide coastal protection for important temperate ecosystems such as seagrass and saltmarshes.  As climate change and pollution destroy marine life, the need for vast oyster reefs has never been greater. Sadly, this vital habitat is at its nadir when we have almost no living memory of its natural state. Humans have been hand harvesting oysters since the Stone Age and cultivating them since Roman times. But oyster extraction reached fantastic proportions from the mid-1800s as mechanised fishing boats replaced older fleets.   Chesapeake Bay – meaning “great shellfish” – in the US was named by the Algonquin Indians because of the miles of incredibly dense oyster reefs that “lay thick as stones” in 1608. Between 1860 and 1920, three quarters of these reefs had been destroyed and by 1940 they had been almost entirely wiped out.  Across the Atlantic in Europe, oyster reefs were once plentiful and thought to be inexhaustible. The Olsen piscatorial atlas of 1883 depicts large oyster grounds at a depth of 34 metres in the North Sea, stretching from Helgoland, off the coast of Germany, to the Dogger Bank, between UK and Denmark, and through the English Channel. Though recently extirpated, oysters were once abundant in the Firth of Forth on Scotland’s east coast.  Roads and towns were built on shucked oyster shells in the US and Britain while shell piles towered one trillion tall in France. In 1864, 700m European oysters were consumed in London alone. The European native oyster (Ostrea edulis) population has declined by 95% since the 1950s, and oysters are now extinct in some regions of Europe, such as the Wadden Sea along the coast of Germany. Removing these vast ecosystems has deprived the sea of essential services, reducing water quality. There’s even speculation that the North Sea is especially murky today because oysters are no longer around. Destructive fishing that exceeds reproduction rates and removes habitat is the main cause of this ecosystem’s demise, but pollution, climate change, invasive species and shellfish diseases have further decimated remnant oyster stocks. It’s clear that active intervention is needed to turn the tide for oysters. Efforts to restore oyster reefs have already had success in the US and there are projects emerging in the UK and Europe. In southern England, the Solent Oyster Restoration Project was established in 2017 to reseed the Solent strait with 5m oysters. Since then, more than 20,000 oysters have been placed in novel restoration aquaculture cages, suspended underneath marina pontoons across the Solent. These cages are designed to pump out larvae which settle on the surrounding seabed, where 30,000 juvenile oysters reared in hatcheries have already been reintroduced. The Native Oyster Network in the UK and Ireland and the Native Oyster Restoration Alliance across Europe aim to restore native oysters around the north-west Atlantic. Both established in 2017, these networks are bringing together scientists, fishers, conservationists and governments to bring back a forgotten ecosystem.   Restoring European oyster beds and reefs is possible, but there is no quick fix. Work will take years and decades and depend on multinational cooperation and effective policy to protect habitats and regulate fishing. Nevertheless, recent efforts give hope we are at the beginning of a journey to restore a forgotten ecosystem to its once magnificent state."
"
Guest Post by Willis Eschenbach
Following on from Anthony’s article, here are my thoughts about the phytoplankton paper “Global phytoplankton decline over the past century”, by Daniel G. Boyce, Marlon R. Lewis & Boris Worm.
I started to write about this earlier, but I decided to wait until I had the actual paper. The paper in question is behind a paywall at Nature Magazine, but through my sub-oceanic channels (h/t to WS) I have obtained a copy. The paper makes two main claims, that: a) the numbers of phytoplankton have been cut by more than half since 1900, and b) the general warming of the global oceans is the reason for the declining numbers of phytoplankton.
First, what are phytoplankton when they are at home, and where is their home? Plankton are the ubiquitous soup of microscopic life in the ocean. Phytoplankon are the plant-like members of the plankton, the ones that contain chlorophyll and feed on sunshine. Phytoplankton are to the ocean what plant life is to the land. Almost all oceanic life depends on phytoplankton. Other than a thin strip of seaweeds and sea grasses along the coasts, phytoplankton are the microscopic plants that are the foundation of the vast entire oceanic food chain. Without phytoplankton there would be no deep water oceanic life to speak of. Figure 1 shows where you find phytoplankton:

Figure 1. Global distribution of phytoplankton. Lowest concentration is purple and blue, middle concentration is green, highest concentration is yellow and red. Source http://www.nasa.gov/vision/earth/environment/0702_planktoncloud.html
So where did the Nature paper go wrong?

The short answer is that I don’t know … but I don’t believe their results. The paper is very detailed, in particular the Supplementary Online Information (SOI). It all seems well thought out and investigated … but I don’t believe their results. They have noted and discussed various sources of error. They have compared the use of Secchi disks as a proxy, and covered most of the ground clearly … and I still don’t believe their results. Here’s exactly why I don’t believe them.
This is their abstract (emphasis mine):
In the oceans, ubiquitous microscopic phototrophs (phytoplankton) account for approximately half the production of organic matter on Earth. Analyses of satellite-derived phytoplankton concentration (available since 1979) have suggested decadal-scale fluctuations linked to climate forcing, but the length of this record is insufficient to resolve longer-term trends.
Here we combine available ocean transparency measurements and in situ chlorophyll observations to estimate the time dependence of phytoplankton biomass at local, regional and global scales since 1899. We observe declines in eight out of ten ocean regions, and estimate a global rate of decline of ~1% of the global median per year. Our analyses further reveal interannual to decadal phytoplankton fluctuations superimposed on long-term trends. These fluctuations are strongly correlated with basin-scale climate indices, whereas long-term declining trends are related to increasing sea surface temperatures. We conclude that global phytoplankton concentration has declined over the past century; this decline will need to be considered in future studies of marine ecosystems, geochemical cycling, ocean circulation and fisheries.
The first clue to where they went wrong is visible in Fig. 1. Although as you can see there is more phytoplankton in the cooler regions of the north, the same is not true in the corresponding regions in the south despite the ocean temperatures being very similar. In addition, there are many places where the ocean is warm (e.g. tropical coasts) that have lots of phytoplankton, while in other warm areas there is very little phytoplankton.
The rude truth of phytoplankton is this: phytoplankton growth is generally not limited by temperature. Instead, it is limited by nutrients. Where nutrients are plentiful, the phytoplankton grow regardless of temperature. Nutrients are more common along the coastline, where sub-oceanic currents come to the surface bringing nutrients from the deep ocean floor, and rivers bring nutrients from inland. For example, in Fig. 1 you can see the nutrients from the Amazon river causing the red area at the river mouth (north-east South American coast).
Indeed, the fact that phytoplankton are generally nutrient limited rather than temperature limited has been demonstrated in the “ocean fertilization”  experiments using rust. If you spread a shipload of rust (iron oxide) out into the tropical ocean, you generally get an immediate bloom of phytoplankton. Temperature is not the problem.
So to start with, the idea that increasing temperature automatically leads to decreasing phytoplankton is not generally true. There are vast areas of the ocean where higher temperatures are correlated with more phytoplankton. For example, the warmer deep tropics generally have more phytoplankton than the cooler adjacent subtropics.
The paper’s most unbelievable claim, however, is their calculation that since 1899, the density of phytoplankon has been decreasing annually by 0.006 milligrams per cubic metre (mg m-3). They give the current global density of phytoplankton as being 0.56 mg m-3. Thus they are claiming that globally the concentration of phytoplankton has dropped by more than 50% over the last century.
Now, a half century ago I learned to sail on San Francisco Bay. Since then I’ve spent a good chunk of my lifetime at sea, as a commercial fisherman from California to the Bering Sea, as a sailboat delivery crewman, as a commercial and sport diver, and as a surfer. And call me crazy, but I simply don’t believe that the sea only has half the phytoplankton that it had in 1900. If that were true, it would not take satellites and complex mathematical analysis to show it. People would have noticed it many years ago.
I say this because phytoplankton are the base of almost the entire mass of oceanic life. They are what almost all other life in the ocean ultimately feeds on, predators and prey as well. The authors of the study do not seem to realize that if the total amount of phytoplankton were cut by more than half as they claim, the total mass of almost all living creatures in the open ocean would be cut about in half as well. No way around it, every farmer knows the equation. Half the feed means half the weight of the animals.
And I see no evidence of that having happened over the last century. It certainly does not accord with my own extensive practical experience of the ocean. And I see no one else making the claim that we only have half the total mass of deep-water oceanic life that we had a century ago..
The other thing that makes their claimed temperature/phytoplankton link very doubtful is that according to the HadISST dataset, the global ocean surface temperature has only increased by four tenths of a degree C in the last hundred years.
Four tenths of a degree … an almost un-noticeable amount. Yet their paper says (emphasis mine):
Our analyses further reveal interannual to decadal phytoplankton fluctuations superimposed on long-term trends. These fluctuations are strongly correlated with basin-scale climate indices, whereas long-term declining trends are related to increasing sea surface temperatures.
These kinds of claims drive me nuts. Is there anyone out there that truly believes that a change of global average ocean temperature of four tenths of a degree C over the last hundred years has cut the total mass of phytoplankton, and thus the total mass of all oceanic creatures, in half? Really?
So that’s why I say I don’t know where their math went wrong, but I don’t believe their results. I don’t believe we’ve lost about half the total mass of all oceanic creatures. Half the planet’s open ocean dwellers? Where is the evidence to support that outrageous claim? And I don’t believe that an ocean temperature change of four tenths of a degree over a century has made much difference to phytoplankton levels, as they grow at all temperatures.
Why don’t I know where their math went wrong? Unfortunately, they have not posted up the data that they actually used. Nor have they shown any of their data in the form of graphs or tables. Instead, they have shown model results, and merely pointed to general websites where a variety of datasets are maintained. So we don’t know, for example, whether they used the 1° grid version or the 2.5° grid version of a given dataset. Nor have they posted the computer code that they used in the analysis. Plus, the very first link in their paper to the first and most important data source is dead.
Grrrr … but dead link or not, pointing to a website as the data source in their kind of paper is meaningless. To do the analysis, they must have created a database of all of the observations, with the meta data, and the details for the type etc. for each observation. If they would include that database and their code in the SOI, then someone might be able to figure out where their math went wrong … my guess is that it may be due to overfitting or misfitting of their GAM model, but that’s just a wild guess.
It is a shame that they did not post their data and code, because other than the lack of data and code it is a fascinating analysis of a very interesting dataset. I don’t accept their analysis of the data because it doesn’t pass the “reasonableness” test, but that doesn’t mean that the dataset does not contain valuable information.
[Update] An alert reader noted that the image in Figure 1 was of a particular month and not a yearly average. So I’ve made a short movie of the variations in plankton over the year.
Figure 2. Monthly movie of plankton concentrations. Click on image to see animation.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e89ac300d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I have not read this book, but it has been raising some volume in Skeptic websites. What strikes me is the number of authors, it has (9 by my count).
Strangely, one of the authors, Oliver K. Manuel,  is a person I’ve banned from WUWT for carpet bombing threads with his vision of the Iron Sun Theory, which I personally think is nutty. So, that right there gives me some pause. But, I haven’t read the book, so it may have nothing to do with that. OTOH, he’s one of the most well mannered commenters you’ll ever find.
The main thrust of the book seems to be discovering what they say is a flaw in understanding and accounting for 13C/12C   isotopes within carbon dioxide, and this then points to a different signature related to human produced CO2.
Over at Climate Change Dispatch, they have this to say about it:

Newly  released science book revelation is set to heap further misery  on UN  global warming researchers. Will latest setback derail Cancun  Climate  conference?
Authors of a new book  ‘Slaying the Sky Dragon: Death of the Greenhouse Gas Theory’ claim they have debunked the widely established greenhouse gas theory   climate change. In the first of what they say will be a series of   sensational statements to promote the launch of their book, they attack a   cornerstone belief of the Intergovernmental Panel on Climate Change   (IPCC) – what is known as the “carbon isotope argument.”
Mišo  Alkalaj, is one of 24 expert authors of this two-volume  publication,  among them are qualified climatologists, prominent skeptic  scientists  and a world leading math professor. It is Alkalaj’s chapter  in the  second of the two books that exposes the fraud concerning the  isotopes 13C/12C found in carbon dioxide (CO2).
If  true, the disclosure may possibly derail last-ditch attempts at a   binding international treaty to ‘halt man-made global warming.’ At   minimum the story will be sure to trigger a fresh scandal for the   beleaguered United Nations body.
Do Human Emissions of Carbon Dioxide Exhibit a Distinct Signature?
The low-key internal study focused on the behavior of 13C/12C   isotopes within carbon dioxide (CO2) molecules and examined how the   isotopes decay over time. Its conclusions became the sole basis of   claims that ‘newer’ airborne CO2 exhibits a different and thus distinct   ‘human signature.’ The paper was employed by the IPCC to give a green   light to researchers to claim they could quantify the amount of human   versus natural proportions just from counting the number of isotopes   within that ‘greenhouse gas.’
Alkalaj, who is head of Center for Communication Infrastructure at the “J. Stefan” Institute, Slovenia says because of the nature of organic plant decay, that emits CO2, such   a mass spectrometry analysis is bogus. Therefore, it is argues, IPCC   researchers are either grossly incompetent or corrupt because it is   impossible to detect whether carbon dioxide (CO2) in the atmosphere is   of human or organic origin.
…
The  13C/12C argument being attacked by Mišo Alkalaj may be found in IPCC’s AR4 – The Physical Science Basis Working Group. The IPCC clarifies its position on Page 139 of that chapter.
According to Miso the fatal assumption made by the IPCC is that the atmospheric concentration of the 13C  isotope (distinctive in prehistoric plants) are fixed. They also assume  C3-type plants no longer exist so would need to be factored into the  equations. Indeed, as Miso points out such plants, “make up 95% of the  mass of all current plant life.”
Therefore, decay of 95% of present-day plant material is constantly emitting the 13C-deficient carbon dioxide supposedly characteristic of coal combustion—and CO2 emitted by plant decay is an order of magnitude greater than all human-generated emissions.
…
From Amazon.com:
Even before publication, Slaying the Sky Dragon was destined to be the  benchmark for future generations of climate researchers. This is the  world’s first and only full volume refutation of the greenhouse gas  theory of man-made global warming.
Nine leading international  experts methodically expose how willful fakery and outright incompetence  were hidden within the politicized realm of government climatology.  Applying a thoughtful and sympathetic writing style, the authors help  even the untrained mind to navigate the maze of atmospheric  thermodynamics. Step-by-step the reader is shown why the so-called  greenhouse effect cannot possibly exist in nature.
By deft  statistical analysis the cornerstones of climate equations – incorrectly  calculated by an incredible factor of three – are exposed then  shattered.
This volume is a scientific tour de force and the  game-changer for international environmental policymakers as well as  being a joy to read for hard-pressed taxpayers everywhere.
==============================================================
There is also a kindle version available on Amazon.com.
At this point I can’t recommend the book from either a pro or con perspective, I’m just making it known to WUWT readers.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8711d2e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Via email press release:
A new paper at SPPI looks at the history of extreme weather events.
The on-going claims of catastrophic anthropogenic global warming have  been ramped up again lately because of the opportunities presented by the heat  wave in Russia and the floods in Pakistan, which are also being claimed as  attributable to anthropogenic CO2. If the amount spent on global warming were to  be diverted to mitigating and preventing the worst effects of natural disasters,  then the desperate plight of the people of Pakistan would be relieved more  quickly.
The paper can be downloaded here: http://scienceandpublicpolicy.org/originals/extreme_weather_extreme_claims.html
The Author, Dennis Ambler, concludes:
Extreme Weather – The Blame Game 
The  Aztecs had sophisticated irrigation systems and “astrolonomical”  observatories, (apparently a mix of astrology and astronomy), to attempt to  predict the weather and reservoirs. But the unseasonal frosts and cold, followed  by severe, prolonged drought, may have taken them to the brink of collapse. Once  the climate became more benign again, they praised their gods with human  sacrifice. 
“When rainfall and agriculture had resumed, the Aztecs responded by massively increasing the number of  human sacrifices to their rain god Tlaloc. It  is thought that hundreds of thousands of people were sacrificed.”
In the Little Ice Age, witchcraft was blamed for the  devastating climate:
Fagan’s The Little Ice Age (Basic Books, 2000):
“Witchcraft accusations soared, as people accused  their neighbors of fabricating bad weather….  Sixty-three women were  burned to death as witches in the small town of Wisensteig in Germany in 1563 at  a time of intense debate over the authority of God over the  weather.”
“Almost invariably, a frenzy of prosecutions coincided with the coldest and  most difficult years of the Little Ice Age, when people demanded the eradication  of the witches they held responsible for their misfortunes.”
These days we don’t blame witchcraft for the weather, instead we blame it on  our emissions of carbon dioxide, describing it as a pollutant that must be  controlled by Government taxes and vilifying anyone who dares to challenge the  orthodoxy.
We ignore thousands of years of climate evidence, in favour of an agenda  based upon a century and a half of sometimes distorted and often-disputed  temperature records, coming out of a known Little Ice Age and we call it  “Science”.
Have we really left the Dark Ages behind?
###


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88c23201',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**There's been a slight increase in Covid-19 infections in Africa over the past few weeks, according to the Africa Centres for Disease Control and Prevention (CDC) and the World Health Organization (WHO).**
The number of daily new cases overall has been rising gradually since late September _,_ although there's a lot of variation across the continent.
Africa accounts for just over two million of the more than 60 million confirmed cases across the world.
There was an average increase of 7.5% in new cases each week over the past month, according to data from the Africa CDC.
In the week up to 22 November:
The WHO says the latest increase is driven by north African countries, where temperatures are beginning to fall as winter approaches.
Morocco, Tunisia and Algeria are among the five countries reporting the highest number of new cases in the past week.
Other countries like Nigeria, Egypt, South Africa, DR Congo and Kenya have also seen increases, although some like Ethiopia have seen decreases.
Dr Nkengasong of the Africa CDC says there are three main trajectories in African countries:
""Overall, if you group these countries together, you see that the curve increased up to July, came down around September and stabilised, and now it is going up,"" he says.
Since the pandemic began, South Africa has had the highest recorded number of total cases in Africa.
Morocco, Egypt and Ethiopia are the only other African countries to officially record more than 100,000 cases.
Daily reported numbers have been rising after falling for about four months, and the government has expressed concern about a rise in infection rates in some areas, mostly in Eastern Cape and Western Cape provinces.
Doctors are also reporting a spike in hospital admissions in the two regions.
Eastern Cape accounted for 50% to 55% of new daily positive casesin South Africa on average, and hospital admissions have been rising for about a month.
Over the past week, Western Cape province recorded a 52.1% jump in new cases.
""There is also now established community transmission of the virus again in this province, which means that it is spreading within communities at a faster rate,"" a statement from the government says.
The reported death rate per capita on the continent has been low compared with other parts of the world, despite the weak health infrastructure in many African countries.
The WHO says this could be partly because of the relatively young population in Africa - more than 60% under the age of 25.
Covid-19 is known to have a higher mortality rate for older age groups, and among people with health problems like obesity and type 2 diabetes which are also less common in Africa.
Experts also say expertise in epidemic control from tackling other outbreaks, cross-immunity from other coronaviruses, low travel and outdoor living could also be contributing to Africa coping better.
There are also issues - as elsewhere in the world - over how countries record deaths, making comparisons between them difficult.
Research earlier this year from the South African Medical Research Council (SAMRC) indicated that the number of people who had died from the virus could have been higher than was reported.
It had looked at excess deaths - the difference between deaths over a particular period and the historical average - and estimated that a significant proportion were due to Covid-19.
The WHO says the testing level in Africa is still low compared to other regions.
Ten countries account for about 70% of the total tests conducted- South Africa, Morocco, Ethiopia, Egypt, Kenya, Nigeria, Cameroon, Rwanda, Uganda and Ghana.
And there are wide variations in testing rates, and while some countries have reduced testing, others have maintained or even increased it.
South Africa has been doing the most and Nigeria doing relatively few tests per capita, according to Our World in Data, a UK-based project which collates Covid-19 information.
By 19 November, South Africa had so far done just over 88 tests per 1,000 people, but that compares with more than 514 in the UK and 534 in the US.
Nigeria had carried out just 3.5 tests per 1,000 people.
A benchmark of doing at least 10 tests for every positive case is recommended by the Africa CDC, and currently there are 12 countries on the continent with a ratio lower than that.
In some countries, there's insufficient data available on testing to know how much is being done.
Read more from Reality Check
Send us your questions
Follow us on Twitter"
nan
"According to a recent major UN report, if we are to limit temperature rise to 1.5 °C and prevent the most catastrophic effects of climate change, we need to reduce global CO₂ emissions to net zero by 2050. This means eliminating fossil fuel use fast – but to cushion that transition and offset the areas in which there is currently no replacement for combustibles, we need to actively remove CO₂ from the atmosphere. Planting trees and rewilding are a large part of this solution, but we are highly likely to need further technological assistance if we are to prevent climate breakdown.  So when recent news emerged that Canadian company Carbon Engineering has harnessed some well-known chemistry to capture CO₂ from the atmosphere at a cost of less than $100 a tonne, many media sources hailed the milestone as a magic bullet. Unfortunately, the big picture isn’t as simple. Truly tipping the balance from carbon source to carbon sink is a delicate business, and our view is that the energy costs involved and likely downstream uses of captured CO₂ mean that Carbon Engineering’s “bullet” is anything but magic. Given that CO₂ only accounts for 0.04% of the molecules in our air, capturing it might seem like a technological marvel. But chemists have been doing it on small scales since the 18th century, and it can even be done – albeit inefficiently – with supplies from the local hardware store. As secondary school chemistry students will know, CO₂ reacts with limewater (calcium hydroxide solution) to give milky-white insoluble calcium carbonate. Other hydroxides capture CO₂ in the same way. Lithium hydroxide was the basis of the CO₂ absorbers that kept the astronauts on Apollo 13 alive, and potassium hydroxide captures CO₂ so efficiently that it can be used to measure the carbon content of a combusted substance. The 19th-century apparatus used in this latter procedure still features on the American Chemical Society’s logo. Unfortunately, this isn’t a small-scale problem anymore – we now need to capture billions of tonnes of CO₂, and fast. Carbon Engineering’s technique is hydroxide chemistry at its best. At its pilot plant in British Columbia, air is pulled in by large fans and exposed to potassium hydroxide, with which CO₂ reacts to form soluble potassium carbonate. This solution is then combined with calcium hydroxide, producing solid and easily separable calcium carbonate, along with potassium hydroxide solution, which can be reused. This part of the process costs relatively little energy and its product is essentially limestone – but making mountains of calcium carbonate doesn’t solve our problem. Though calcium carbonate has uses in agriculture and construction, this process would be far too expensive as a commercial source. It also isn’t a practical option for government-funded carbon storage due to the massive quantities of calcium hydroxide that would be required. To be feasible, direct air capture needs to produce concentrated CO₂ as its product, which can either be safely stored or put to use. Thus, the solid calcium carbonate is heated to 900 °C to recover pure CO₂. This last step requires a vast amount of energy. In Carbon Engineering’s natural gas-fired plant, the whole cycle generates half a tonne of CO₂ for every tonne captured from air. The plant does capture this extra CO₂, and of course could be powered by renewable energy for a healthier carbon balance – but the problem of what to do with all the captured gas remains. Swiss start-up company Climeworks is using similarly captured CO₂ to aid photosynthesis and improve crop yield in nearby greenhouses, but as yet the price is nowhere near competitive. CO₂ can be sourced elsewhere for as little as one-tenth of Carbon Engineering’s $100 bottom line. There are also much cheaper ways for governments to offset emissions: it is far easier to capture CO₂ at the emission source, where the concentration is much higher. So this technology is likely to mainly interest high-emitting industries which may stand to benefit from CO₂ with green credentials.  For example, one of the key investors in Carbon Engineering’s capture technology is Occidental Petroleum, a major user of Enhanced Oil Recovery methods. In one such method, CO₂ is pumped into oil wells to increase the amount of crude oil that can be recovered, thanks to increased well pressure and/or improving the flow characteristics of the oil itself. However, including the energy cost of transporting and refining this extra oil, using the technology in this way will likely increase net emissions, not decrease them.  Another key spoke of Carbon Engineering’s operations is its Air To Fuels technology, in which CO₂ is converted into combustible liquid fuel, ready to be burned again. Theoretically this provides a carbon-neutral fuel cycle, provided that each step of the process is powered with renewable energy. However, even this use is still a far cry from a negative emissions technology. There are promising alternatives on the horizon. Metal-organic frameworks are sponge-like solids that squeeze the equivalent CO₂ surface area of a football pitch into the size of a sugar cube. Using these surfaces for CO₂ capture requires far less energy – and companies have started exploring their commercial potential. However, large-scale production has not been perfected, and questions over their long-term stability for sustained CO₂ capture projects mean that their high cost is not yet merited. With little chance that technologies still in the laboratory will be ready for gigatonne-scale capture within the next decade, the methods employed by Carbon Engineering and Climeworks are the best we currently have. But it’s important to remember that they’re nowhere near perfect. We will need to switch to more efficient methods of CO₂ capture as soon as we are able. As Carbon Engineering’s founder David Keith himself points out, carbon removal technologies are overhyped by policymakers, and have received “extraordinarily little” research funding thus far. More generally, we must resist the temptation to see direct air capture as a magic bullet that saves us from having to address our carbon addiction. Reducing or neutralising the carbon burden in the life cycle of hydrocarbon fuels may be a step towards negative emissions technologies. But it is just that – a step. After being on the wrong side of the carbon ledger for so long, it’s past time to look beyond just breaking even. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"
Share this...FacebookTwitterAnd thus he has a good shot of joining the elite ranks of Nobel Laureates along with the esteemed Al Gore and the IPCC. Hat-tip: Hot Air.
If anyone needed more evidence that climate change science has become completely irrational and idiotic, here’s another glittering jewel. It’s another sign of the spreading mental disorder that is fanned by the junk science of manmade climate change. Everything that is different today is now due to man-made climate change.
We’ve all heard the stereotype descriptions about jocks and brains. Unfortunately there are some out there who somehow find ways of setting back the debunking of such stereotypes 10 years or more. Ex-MLB player and now announcer Tim McCarver has single-handedly succeeded doing just that.
Listen to what he says concerning baseball players hitting homeruns: Click HERE.
Global warming Climate change, he theorizes on national TV, makes the air thinner and so the ball carries further when hit.
It has not been proven, but I think ultimately it will be proven that the air is thinner now. There have been climatic changes over the last 50 years in the world. I think that’s one of the reasons that balls are carrying much better now than I can remember.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Stupid! stupid! stupid!
But wait – McCarver’s Theory may actually be true. Planes indeed today are flying further than ever per litre of jet fuel – it must must because of the “thinner air” – clear evidence that McCarver’s Theory is undeniable. The same is true for cars and trucks, which today go further than ever on a litre of fuel, again thanks to reduced air resistance from thinner air.
Furnaces are also burning more efficiently, thus indicating it must have something to do with the air. Wind speeds are faster too, thanks to thinner air making air molecules fly faster. The body of evidence is growing!
I bet if NASA, Hadley or the PIK crunched the numbers with their sophisticated models and supercomputers, McCarver’s Theory would gain a consensus among the real scientific community, Royal Academy and the National Academy of Sciences.
Concerning the homerun records, perhaps the MLB could go back and adjust the statistics. Hank Aaron could be awarded another 30 homeruns because the air back then was “thicker”, and thus take back the record he lost to Barry Bonds, who unfairly set his record in “thin air”.
Also see WUWT for more on the story.
 
Share this...FacebookTwitter "
nan
"**A care home owner killed himself because of the ""incredible emotional pressure"" of coping with the pandemic, an inquest has heard.**
Vernon Hough, 61, was found dead at a North Wales Police station car park in Llay, Wrexham, on 21 May.
He ran Gwastad Hall Nursing Home in Wrexham with his wife, Helen, caring for 40 residents.
Mrs Hough told the inquest he had lost weight due to worrying about Covid-19. The coroner's conclusion was suicide.
In a statement to the inquest, Mrs Hough said running the care home during the pandemic was ""having an impact"" on her husband.
""He was panicked at times,"" she said. ""We would constantly talk about Covid-19.""
Mrs Hough told the coroner that Mr Hough ""wasn't afraid of catching it, he was afraid of spreading it because we weren't being tested. That's what his fear was.""
She described her husband as a ""worrier"" who had lost one-and-a-half stone (9.5kg) in weight ""due to the worry of Covid.""
Although Mr Hough had previously been prescribed anti-depressants by a GP for night sweats, his wife said he had never been diagnosed as having anxiety.
The inquest heard that he ""wasn't handling"" seeing people in distress.
""At the time we were struggling to get oxygen in the care home"" said Mrs Hough. ""I still cannot get oxygen and we're still in the middle of a pandemic.""
She also said they were having difficulties sourcing PPE and were ""trying to get it off Amazon"".
Coroner David Pojur said the pressure of working through the pandemic had ""overwhelmed"" Mr Hough and it had ""affected his mental health"".
""Running a care home is difficult in the best of circumstances, but during the pandemic that put him under significantly greater stress than it seemed he could cope with.""
Care Forum Wales chair Mario Kreft said Mr Hough's death ""illustrates the enormous pressure care home owners and their staff have been under as a result of the pandemic"".
""It's been such a terribly dark time for people working in the sector and the sense of responsibility felt by Vernon toward the residents of Gwastad Hall clearly became too much for him to bear,"" he said.
""Our thoughts are very much with Vernon's family, friends and staff at this extremely difficult time, not to mention the residents to whom he was utterly devoted."""
"
Share this...FacebookTwitterLast fall I wrote about how self-anointed environmental mastermind politicians in my homestate of Vermont bulldozed public opposition, lots of trees and finally the top of Lowell Mountain to make way for “climate-saving” 450-foot industrial wind turbines. Read here and here.
Here’s how environmental protection by these political environmental pimps appears so far, hat-tip: energizevermont.org.

Crushed stone pad for just one turbine to be installed. Many such sites are being prepared on Lowell Mountain to accommodate a series of turbines, 21 in all. 

How Vermont protects the environment – coming statewide (except Chittenden County, where the fat cats live).

Silent spring for Vermont wildlife.
The source of these photos is mountaintalk.com


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




VIEW ALL PHOTOS HERE
This is all “to save the planet” from the junk-science-based climate catastrophe fantasy.
How about a little prison time for the charlatans perpetuating the hoax? In a way it’s really good that I’m all the way across the big pond now.
Yes, we can thank these political whores for ruining a once beautiful area (please excuse my diplomacy). This of course is just one windpark of an entire series planned by the state.
The Green Wave
It’s just stunning how psychologically people just blindly herd behind a fad to the point where they just don’t even see the massive damage being inflicted. It’s like the story “The Wave“. It’s totally out of control. All you need are a few clever manipulators, and lots of dupes.
According to state senator Joe Benning, also a duped climate-science believer, but at least still sober enough to see the damage:
And more wind farms are coming as corporate investors, motivated by tax incentives and artificially inflated electric rates, seduce small towns with infusions of cash. Since wind is intermittent and has no storage capacity, our policy alone will require more wind farms and many miles of transmission lines to achieve our energy goal. Regulatory authorities are failing to insist on decommissioning plans, meaning our ridgelines will end up littered with forty story rusting hulks when this technology becomes obsolete. These new wind farms are encroaching on our wildlife corridors, destroying pristine mountain environments and radically changing the aesthetics of our state. They pit citizens of towns against each other, and towns against towns in a given region.”
Even if CO2 were a problem, the operation of heavy equipment, the massive earthwork and all the deforestation creates a carbon footprint that likely will never be erased – never mind the permanent disfigurement of the landscape.
Vermont’s children and grandchildren someday are going to ask: What the hell was in Vermont’s drinking water back then?
Share this...FacebookTwitter "
"
Share this...FacebookTwitterYesterday I posted on a new study written by Reinhard Böhm of Austria’s Leading Weather and Climate Agency ZAMG.
His comprehensive, peer-reviewed paper found that there has been no increase in weather extremes in the Austrian Alps – surprising the world’s climate scientists. This study in my view is really big, and is upsetting the Climate Establishment in Europe. Dr. Böhm is quickly becoming the new enfant terrible.
Geologist Sebastian Lüning now provides additional details at his Die kalte Sonne website. I’ve translated his essay (with some editing).
===============================================
Surprise! Fewer Weather Extremes in the Alps Region
By Dr. Sebastian Lüning
The climate is going crazy and everything is getting more extreme. It’s only a question of time before the planet gets destroyed. This is what experts close to the IPCC have been telling us for some years now. But now a scientist has taken a closer look at the hard data and has found something truly amazing. Reinhard Böhm of the Central Administration for Meteorology and Geodynamics in Vienna has examined dataseries from 58 locations in the Alps, some of which go back to the year 1760. All the data is available in the Internet. Böhm published the study in the European Physical Journal.“
As expected also in Austria there has been a warming over the last 200 years, like almost everywhere else on the planet. That is expected and simply represents the transition from the Little Ice Age to the Modern Warm Period. The question that Böhm investigated, however, is: Did the weather get more cranky and more extreme during this time?
Austrian newspaper Die Presse here wrote:
Whether it is snowfall, heavy rains, storms or dry spells: After every notable weather event the media and experts are quick to explain that the increased frequency of extreme events of the recent past and of the coming future is due to man-made climate change. Hardly anyone questioned this claim – except for one person: Reinhard Böhm […]. In his recent research work, he evaluated up to 250 years of old weather data of the Alps region. The result even surprised him. The core message: An increased frequency of weather extremes caused by climate change – at least in the Alps region – could not be detected.”
In a press release of the Institute the stunning results were more shown in more detail (Figure 1) (see the article in Der Standard):
[On] the often quoted increase in weather extremes, this however has not been the case in the Alps. Completely to the contrary: ‘The temperature fluctuations have even decreased over the last decades,‘ summed up climatologist and study author Reinhard Böhm. […] The results of the study have left the scientists amazed.
Result No. 1: Over the last 250 years, the seasonal and annual fluctuation ranges of hot-cold, dry-wet have not gotten more extreme.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Result No. 2: Also over the last 30 years, which have been greatly impacted by man, there has been no trend to more variability when compared to the decades before.
And finally Result No. 3: The long-term development of temperature, precipitation and atmospheric pressure show two long waves of variability with a cycle of about 100 years. The climate was more variable (‘crazier”) in the middle of both past centuries, less variable (‘quieter’) at the start and end of the centuries.“
The last point is very important. As geological studies of the past 10,000 years have shown, natural cycles play an important role in the variability of the climate (see our past blog articles.

Figure 1: Changes in climate variability in southern central Europe over the past two centuries. Attention: NOT shown are the absolute values. Only the anomalies. Graphic source: ZMAG.
For the IPCC faithful, these results are quite unexpected. For years they have been claiming just the opposite. Austrian television ORF here writes:
Böhm’s study stands in contradiction to other studies that show a global increase in weather events is detected. The Potsdam Institute for Climate Impact Research spoke of a “Decade of Weather Extremes”. Also in March 2012 a Report by the IPCC showed more extreme weather events and, among other things, it is highly likely that periods of drought will occur more often over the coming decades  – in many regions of the world, among them also Central Europe.“
Böhm takes these these alarmist colleagues especially to task. What is it that is driving some scientists to always want to observe everywhere only a worsening of the climate? Die Presse writes:
Böhm doesn’t hold back on criticizing the PR work of some colleagues. To save the world from climate change, one needs lots of attention. Claims that weather extremes accompany temperature increase may be wonderful for marketing yourself, but it has got nothing to do with reality.’ “
It is wonderful to see that there is a push-back by climate scientists and that solid data and evidence is slowly gaining the upper hand. The study shows once again the importance of datasets that go far back into the past. Today’s climate can only be properly assessed when put in a historical context.
============================================
 
Share this...FacebookTwitter "
"

The neoconservative call to ostracise Russia by kicking her out of the G8 and denying her membership in the World Trade Organisation is deeply mistaken. Washington’s desire to lash out economically and diplomatically at Russian misbehaviour in the Caucasus is trumping rational thinking on the future of a vital strategic relationship.



Washington’s fundamental error is to mislabel Russia as a democratic country in the 1990s that suddenly turned undemocratic during the past decade. It is mistakenly presumed, therefore, that Russia can be incentivised to appreciate the error of her recently authoritarian ways and sheepishly return to a state of democratic bliss and constructive multilateralism.



The trouble is that post‐​Communist Russia has never been democratic in the liberal, Western sense. Rather, in zigzag fashion the Russian political system has been slowly transitioning away from totalitarianism towards a democratic system compatible, one hopes, with Russian history and cultural traditions.



Consequently, asking in the wake of Russia’s intervention in Georgia, ‘Why did Russia go wrong?’ is the wrong question. The awkward question Americans should be asking, not of the Russians, but of themselves, is, ‘What can the West do to encourage and to sustain Russia’s political transition?’



The Bush administration needs to avoid a holier‐​than‐​thou attitude in these matters. Unless and until the U.S. revises its critique of Russian politics, it runs the risk of applying a democratic double standard to U.S.-Russia relations.



One observes, for example, the Bush administration’s marked impatience with the pace of Russian democratisation. Yet, it continues to exhibit tremendous patience with democratisation’s meandering (to put it charitably) pace in countries such as Egypt and Saudi Arabia, not to mention the lengthy indulgence of Pakistan.



Economic development is the catalyst for Russia’s long‐​term political maturation, as it has proven to be in most countries. Hence, the best way to foster Russian democracy and, consequently, calm Moscow’s approach to international relations, is not to threaten Moscow but, less dramatically and more realistically, to help foster economic growth in Russia.



The larger irony is that the world’s greatest liberal democracy is itself becoming increasingly illiberal. Both domestically and internationally, there is a need to refocus on a broader definition of democracy than American politicians are comfortable employing in public, especially when lecturing their Russian counterparts.



In practice, a myriad of economic and personal freedoms form integral pillars of a strong and stable liberal democracy. Simply put, political freedom is not sustainable without the foundations supplied by economic and personal freedom.



Yet, American liberal democracy is in the process of being replaced by a bully‐​like Nanny State. Take, for example, health and environmental policy.



Here, one witnesses the rise of highly coercive, arguably fascistic, policies to control individual choices and restrict personal freedom over, for example, one’s diet and the consumption of a list of still‐​legal products, such as alcohol and tobacco.



More worrisome, perhaps, is the fact that U.S.-led international health and environmental organisations are initiating pseudo‐​scientific campaigns upon those middle‐​income countries with allegedly indulgent energy, and dangerous personal, consumption habits.



There are no prizes for guessing which former Eastern European superpower has been chosen as the World Health Organisation and the Intergovernmental Panel on Climate Change’s next big juicy target. The Russians know this and, consequently, are circling the diplomatic wagons.



Going forward, the U.S. needs to work with Russia as her strategic partner rather than acting as her strategic adversary.



Unlike the cartoonish rhetoric of recent days, the American critique of Russian behaviour should be informed and it should be candid. Above all, it should be constructive, if Washington’s sincere goal is no more than the betterment of the Russian people.



The Russians do not think the U.S. is sincere. Rather, they believe the critique of Russia’s actions is merely an instrument of American foreign policy.



Absent a belated U‐​turn, American foreign policy – conducted by those living in an democratic glass house – will continue to throw ever‐​larger political stones in Russia’s direction. Policymakers in Washington shall discover that this course of action damages not only the stones’ intended target, but also the stone thrower, herself.
"
"

This is curious. Greenpeace is giving away free pedometers at COP16 in Cancun. Watch the video below. I don’t really understand the point of all this, except maybe its some sort of guilt over the limousine largess from COP15 in Copenhagen, and they want people to walk to their hotels? Even so, they apparently are unaware of this Times Online article which points out, walking apparently produces more CO2 than driving:
Walking to the shops ‘damages planet more than going by car’
Walking does more than driving to cause global warming, a leading  environmentalist has calculated.
Food production is now so energy-intensive that more carbon is emitted  providing a person with enough calories to walk to the shops than a car  would emit over the same distance. The climate could benefit if people  avoided exercise, ate less and became couch potatoes. Provided, of course,  they remembered to switch off the TV rather than leaving it on standby.
…
{Goodhall says] “The troubling fact is that taking a lot of exercise and then eating a bit  more food is not good for the global atmosphere. Eating less and driving to  save energy would be better.”
Well, that’s inconvenient. Greenpeace says the opposite. They write on the Greenpeace More Walk Less Talk page:


COP 16 will be the seventh Conference of the Parties since the Kyoto  Protocol entered into force in February 2005. That’s a lot of talking.
The physical layout of these meetings means there is a great  deal of walking. Walking, as we all know is very good for you – it’s  credited with helping breathing, improving circulation, bolstering the  immune system, and helps people stay in shape.
It is also, of  course, good for the climate. But, as international climate negotiations  processes show, sadly so far – not enough governments are “Walking the  Talk.”
So, in Cancun – Greenpeace is hosting “More Walk, Less  Talk” – a competition to find the person and the country that covers the  most ground in Cancun.
Yes, the race to the future starts here. Grab your step-counter and go!

Well I’ve got no beef with the “walking improves health” message. I wonder what the winning prize is? Watch the promotional video:

And the battle continues over the issue of walking versus driving, the Pacific Institute wrote a rebuttal to the walking is worse versus driving story.
As noted by Goodall, what really stands out in this comparison is the astoundingly high GHG values for walking when the calories come from beef or dairy. The idea that moving a 2,853 pound Nissan Sentra42 plus a 189-pound driver could possibly generate fewer GHGs than if that driver simply walked the same distance underscores the staggering carbon intensity of beef and dairy production. To be fair to Goodall, this was in fact his underlying message: meat-intensive diets are energy intensive and greenhouse gas intensive.
So obviously, the message missing from the Greenpeace Pedometer message at COP16 is “walk but don’t eat meat or dairy”.
So much for those fancy Cancun dinners on whomever is funding the attendee. Bean burrito for you!
Of course the whole “walking to save the planet” idea gets negated by the simple fact that none of these people arrived by sailboat in Cancun, but used some fossil fuel gussling airplane and then maybe a train or taxi.
But at least they’ll feel better about themselves walking around hungry, right?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e86ba4561',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**A further 28 people have died with coronavirus in Wales, taking the total to 2,474, according to the latest figures.**
Public Health Wales data also showed 1,251 more people had tested positive for Covid-19, taking the total to 75,986.
Each of Wales' seven health boards reported at least one new Covid-related death on Thursday.
Cwm Taf Morgannwg reported the highest number, with eight new deaths.
Across other health boards in Wales, Swansea Bay reported seven new deaths, while there were six in Aneurin Bevan and four in Betsi Cadwaladr.
Cardiff and Vale, Hwyel Dda and Powys all reported one new death.
Blaenau Gwent still has the highest case rate - the number of new cases per 100,000 people over seven days - of any county in Wales, measuring at 405.1 in the week up to 23 November.
It is slightly lower than yesterday's figure of 415.1.
It is followed by Torfaen (333.1), Newport (298.7) and Neath Port Talbot (294.5).
Gwynedd (23.1) and Conwy (28.2) have the lowest case rates in the country.
There has now been a total of 1,441,357 tests carried out in Wales since the start of the pandemic, with 14,564 carried out on Wednesday.
Acute patients from Powys are usually treated across the border at hospitals in England, so deaths of Powys residents usually only appear in registrations reported later by the Office for National Statistics (ONS).
Those ONS figures \- which are higher - also count both confirmed and suspected cases of Covid, as well as deaths in all settings, including care homes, hospices and people's own homes. The most recent weekly figures are the highest since May.
PHW reports new deaths daily, but these are usually from previous days.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
The postcode search has been updated to replace data for health boards in Scotland with data for local councils. In England, data for county councils has been replaced with data for district councils. Figures for boroughs and unitary authorities remain unchanged."
"
Share this...FacebookTwitterThe insurance industry is one of the primary beneficiaries of the climate change panic.
The more people fear climate-related disaster, the more inclined they’ll be to buy up insurance or to pay higher premiums. No industry has a a greater interest in fanning climate panic than the insurance industry. Hat-tip: a reader.
German insurance industry moves to cash in
Check out this info-ad put out by a German insurance agent in the online Norderstedt Stadtmagazin, which I’ve translated in English.
€ ‘Unpredictable weather– protect yourselves!’
Norderstedt (em/mp) storm Kyrill or the summer flood on the Elbe: Over the last years the number of extreme weather events has increased. 
Many scientists trace it all back to climate change. For homeowners, these natural disasters mean damages in the billions. ‘Because of climate change we have to prepare ourselves for the increase in extreme weather events and natural catastrophes,’ insurance expert Finn Herbert knows. ‘Rain deluges, flooding, storms, hail and long-lasting cold snaps or intense snowfalls can lead to huge damage to your buildings and your personal belongings. Everyone is vulnerable. Heavy downpours can even cause flooding far away from rivers and lakes.’


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A normal residential building – and a personal property insurance are not enough to protect yourself from the consequences of so-called natural perils. This can be only offered by insurance against the risks of fire, strom, hail, lightning strike, water pipes, and in personal property insurance also against breaking in and theft. ‘For the elementary perils such as flooding, water back-ups, downpours, snow loads, landslides, sinkholes, avalanches and volcanoes there is insurance against natural hazards,’ explains Finn Herbert. ‘These can be included in with your home and personal belongings insurance. Give us a call – We’ll gladly inform you.’
Ernst von der Reith GmbH”
Big Insurance penetrates the IPCC
You’ve got to wonder when scientists like Stefan Rahmstorf work hand in hand with the reinsurance industry, writing doomsday reports that help fatten the bottom line. Hartmut Grassl, a climate alarmist, is also connected to Munich Re, the world’s largest reinsurer.
Reader DirkH points out how the Munich Re has at least two more agents at the IPCC. Working Group II AR5 Writing Teams, Chapter 10 — Key economic sectors and services, Eberhard Faust, Munich Reinsurance Company and an excerpt from a report from Dr Sandra Schuster, meteorologist with Munich Re, Sydney, who has just been appointed as a Lead Author (WG2) for IPCC AR5.
It’s a real scam when the insurance industry buys up science and pays the science institutes and scientists to spread fear among its customers, stampeding them into the arms of their sales agents. Once again it’s the little guy getting the shaft.
The insurance industry oversight authorities ought to read “Die kalte Sonne – Warum die Klimakatastrophe nicht stattfindet” and start cracking down on this dubious business.
It would be interesting to know what your insurance agent says about the “increasing climate risks”. My agent doesn’t believe it, and he knows I’m not duped by the scam.
Share this...FacebookTwitter "
"**Germany is seeking an agreement with EU countries to keep ski resorts closed until early January, in an attempt to curb the spread of coronavirus.**
Chancellor Angela Merkel told parliament that efforts were being made to reach a Europe-wide decision.
Italy and France have expressed support for a co-ordinated approach. But Austria has voiced concern.
Some of the early European coronavirus hotspots were at ski resorts, helping spread infections across the continent.
Last week, the World Health Organization (WHO) warned that Europe faced a ""tough"" six months , amid mounting cases. Renewed restrictions have led to a reduction in new infections in some countries, but there are fears the pandemic could worsen over the winter.
Like Germany, Italy has also stressed the need for a united approach on the issue of ski resorts, and Prime Minister Giuseppe Conte has already backed delaying the start of the ski season.
""If Italy decided to shut down all its ski lifts without any support from France, Austria and the other countries, then Italian tourists would risk going abroad and taking the contagion back home,"" he told La7 TV earlier this week.
Many Italians head for the slopes over the Christmas and New Year break and the period is a vital part of the local economy for ski resorts across Europe.
French President Emmanuel Macron has made clear that the country's ski resorts will stay shut until the New Year. Prime Minister Jean Castex said on Thursday he wanted to see the coronavirus rules for ski resorts ""harmonised at European level as much as possible"".
France plans to ease its current national lockdown in three phases through to the end of January. Most restrictions will be eased for a few days over Christmas. Mr Castex said ""this family celebration cannot take place without grandparents being present"".
But Austria has voiced concern over any EU-wide plan for ski resorts, with Finance Minister Gernot BlÃ¼mel saying that if the EU forced the resorts to remain closed, ""then they will have to pay for it"". Compensation would run into billions of euros.
Austria's government is already facing legal action over the Ischgl ski village, which was linked to cases in 45 countries after skiers brought the virus home with them.
Switzerland is not in the European Union and, unlike other Alpine destinations, its ski resorts are already open, so skiers unable to spend their winter breaks in neighbouring countries could head there instead.
Ski lifts are running, with a requirement to wear masks, prompting criticism from WHO Covid-19 envoy David Nabarro. ""Once infection rates sink, and they will sink, then we can be as free as we want. But right now? Should ski resorts open? Under what conditions?""
Meanwhile, Mrs Merkel has also defended the decision to extend Germany's partial lockdown until 20 December, announced on Wednesday.
The stricter rules will limit private gatherings to five people from two households coming into effect from next week - although children under the age of 14 are exempt.
German leaders have also unveiled plans for Christmas, with meetings of up to 10 people allowed from 23 December until 1 January.
Addressing the Bundestag - Germany's lower house - on Thursday, Mrs Merkel said the tight restrictions must remain for now because the goal was still to get down to a maximum weekly rate of 50 new infections per 100,000 inhabitants.
But in 62 areas, including Berlin, the figure was above 200, she said. ""Unfortunately we have to say that we cannot promise any relief for Christmas and the New Year,"" the chancellor said."
"**Cornwall and the Isles of Scilly are two of only three areas in England which will be in tier one from next Wednesday.**
MPs and businesses have welcomed the news and spoken of ""relief"" at ""the end of a difficult year"".
The Isle of Wight, Cornwall and the Isles of Scilly are the only areas in tier one.
One Cornwall MP warned against the grading being ""grounds for complacency"".
Amy Newland, from the White Hart pub in Chilsworthy, said: ""I'm extremely relieved, at least we know that we can trade normally.""
She added that she felt sorry for pubs in neighbouring Devon which will be under tougher tier two restrictions.
""It's mixed emotions but a huge relief for us,"" she said.
Paul Eaton, who runs the Royal Inn at Horsebridge, near Tavistock, just inside the Devon border, said: ""Some of my regulars are already planning where they can go in Cornwall.
""It's a long border, with pubs scattered both sides.""
Kim Conchie, chief executive of Cornwall Chamber of Commerce, said it had been ""a difficult year"" but this marked ""the first stage of securing jobs and companies for our future"".
Holiday homes letting agent Lin Wallis in Cornwall tweeted she had five inquiries in 10 minutes after the announcement.
In the seven days to last Saturday the rate of infections in Cornwall was 59 per 100,000 people, the England average was 169.
In the same period there were 337 new cases, down by 176 on the previous week.
Steve Double, Conservative MP for St Austell and Newquay, said Cornwall returning to tier one restrictions was ""particularly good news for the thousands of hospitality businesses"".
He added: ""However whilst we can welcome this news it should be no grounds for complacency.""
Tier one means people can meet indoors and out up to a maximum group of six, pubs and restaurants can re-open, closing at 23:00 GMT but people will still be encouraged to work from home.
Differences between the new tiers include restrictions on where households can meet up:
The system will be regularly reviewed and an area's tier level may change before Christmas - the first review is scheduled for 16 December."
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




Climate change has been called “the biggest market failure the world has seen” and “the mother of all externalities.”



you can pretty much guess what kind of SCC value van den Bergh and Botzen prefer.   
  
  
To support their apparent preference for a high SCC, they spend the bulk of their paper imagining bad climate outcomes—with high monetary damages—and are generally dismissive of positive climate impacts. For example:   




Nevertheless, our summary of the main effects provides a clear insight, namely that unquantified negative effects of climate change tend to domi­nate unquantified positive effects. The negative effects comprise large biodiversity losses, political instability, violent conflicts, large‐​scale migration, extreme weather events, natural disasters and the effect on long‐​term economic growth. Accounting for the latter is likely to increase the SCC because large impacts of cli­mate change are expected to reduce the rate of GDP growth, partly because of negative effects on labour and capital productivity.



Unsurprisingly, when you include a lot of negative impacts along with a low discount rate, the IAMs produce very high estimates of the SCC.   
  
  
In fact, van den Bergh and Botzen arrive at a “conservative” SCC value of $125. For comparison, value used by the Obama Administration for cost/​benefit analyses of new regulations is $36.   
  
  
Interestingly, in their “conservative” analysis, they never once mention the growing body of new and prominent scientific literature that produce updated estimates of the earth’s climate sensitivity—a measure of how much climate change we expect from carbon dioxide emissions—that are much lower and much more tightly constrained than the ones used in _all_ of the studies reviewed by van den Bergh and Botzen.   
  
  
The lower climate sensitivity estimates not only reduce the overall impacts from expected climate changes, but they do so primarily by reducing the chances of unexpected and catastrophic changes—the biggest drivers of the high SCC values in the IAMs. It has been repeatedly shown (see here, here, and here for example) that incorporating the new, lower climate sensitivity estimates reduce the IAMs’ SCC determinations by some 40 percent.   
  
  
And there are lots of other things, which, if better incorporated in the IAM’s, would lead to lower SCC values.   
  
  
If the positive benefits from carbon dioxide emissions on the planet’s crop production were better included in the IAM’s, the SCC value drops further. And if arguments for the use of a higher discount rate, rather than the very low one espoused by van den Bergh and Botzen win the day, the SCC drops further still.   
  
  
Add to the mix a more reasoned view of future climate extremes, and before you know it, it is an easy argument to make that the SCC value should fall significantly _below_ the Administration’s $36 rather than some three to four times higher.   
  
  
It is bad enough that van den Bergh and Botzen present a rather one‐​sided view of the science of climate change/​climate extremes and the economics concerning the choice of discount rate, but for them to term their analysis “conservative” is really taking things too far. “Alarmist” would be a more apt description.   
  
  
Our hope would have been that the reviewers for _Nature Climate Change_ would have caught the glaring oversight of the current climate sensitivity literature (with one of the most persuasive articles appearing in the sister journal _Nature Geosciences_ ), but that didn’t happen. We’ll withhold speculation as to why that was the case.   
  
  
**Reference:**   
  
  
Van den Bergh, J.C.J.M., and W.J.W. Botzen, 2014. A lower bound to the social cost of CO2 emissions. _Nature Climate Change_ , **4** , 253–258, doi:10.1038/NCLIMATE2135.
"
"
Share this...FacebookTwitterThe German media are reporting on a developing outbreak of the oak processionary caterpillar now spreading over the Eastern and Southern areas of Germany.
The caterpillars are pests in oak forests and they pose a health hazard because of their poisonous hairs which can cause skin irritation and asthma. Quite the nasty critter, indeed.
German daily Die Welt reports that the dangerous caterpillar develops especially well in warm and dry springs, adding:
Also climate change has likely contributed to the spread of the pest.”
Even though they don’t cite any information or data to back it up. I suspect they got their information from Wikipedia, who write:
The moths are widely distributed in central and southern Europe, and are occasionally found as far north as Sweden. In the southern countries of Europe the populations are controlled by natural predators, but these predators do not exist in northern Europe. Their range is expanding northward, possibly or partly as a result of global warming [clarification needed, citation needed]. The moths are posing an increasing threat to humans as their range is being extended by the warming European climate.[citation needed]. The backs of older caterpillars (3rd to 6th instars) are covered with up to 63,000 pointed defensive bristles containing an urticating toxin (thaumetopoein or closely related compounds). The setae break off readily, become airborne and can cause epidemic caterpillar dermatitis (lepidopterism), manifested as a papular rash, pruritus, conjunctivitis and, if inhaled, pharyngitis and respiratory distress, including asthma or even anaphylaxis.”
The oak processionary caterpillar has spread during other years as well.
Read more here.
Share this...FacebookTwitter "
"
From NASA’s APOD:
 
 Dark Belt Reappearing on Jupiter 
 Credit:  NASA’s JPL, U. Oxford, UC Berkeley, Gemini Obs.  (North), USC Philippines
 Explanation:  Why are planet-circling clouds disappearing and reappearing on Jupiter?  Although the ultimate cause remains unknown, planetary meteorologists are beginning to better understand what is happening.  Earlier this year, unexpectedly, Jupiter’s dark Southern Equatorial Belt (SEB) disappeared.  
The changes were first noted by amateurs dedicated to watching Jupiter full time.  The South Equatorial Band has been seen to change colors before, although the change has  never been recorded in such detail.    Detailed professional observations revealed that high-flying light-colored ammonia-based clouds formed over the planet-circling dark belt.
Now those light clouds are dissipating, again unveiling the lower dark clouds.  Pictured above two weeks ago, far infrared images — depicted in false-color red — show a powerful storm system active above the returning dark belt.    Continued observations of Jupiter’s current cloud opera, and our understanding of it, is sure to continue.
h/t to Dr. Leif Svalgaard


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e87039b65',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Last century the Forestry Commission sparked anger with a mass planting of conifer trees designed to provide a national reserve of timber because the shortages of the first world war had highlighted a national need. Now a leading expert is calling for similar action again, arguing that if the UK is serious about offsetting its carbon dioxide emissions it must plant tens of millions of trees from imported species on open land.  John Healey, professor of forest sciences at Bangor University, says that relying on indigenous species such as oak and beech will make it impossible for the government to hit its climate goals. Britain will have no choice, he says, but to engage with the commercial sector in large-scale planting of imported conifers, despite fears of the impact on habitats and wildlife. Such a move risks incurring public anger, if its 20th-century precursor is anything to go by. “People felt it was bad for landscape, biodiversity and amenity,” said Healey. “Both the public and conservation professionals saw many negative effects on heathland; there was the notorious draining of the peat bogs of the Flow Country in the far north of Scotland; some really important habitats were left decimated by tree planting.” In 2017 the UK was the second largest net importer of forest products, behind China. Yet only 1,420 hectares of woodland were planted in England in the year to March 2019, against the government’s target of 5,000 hectares. Wales and Northern Ireland planted 520 hectares and 240 hectares respectively. The Committee on Climate Change has called for the planting of more trees and woodlands if the UK is to reach net-zero carbon emissions by 2050. It recommends increasing UK woodland cover from its current low level of 13% of total land cover to at least 17%, and possibly to 19% by 2050. In contrast, 38% of the average EU member state is wooded, a roughly similar percentage to the US. The Conservatives have pledged to plant 30 million trees a year between 2020 and 2025. But Healey said such targets faced stiff challenges. Indigenous species grow too slowly to be an efficient option for rapid carbon fixation, he said. Unharvested woodland eventually stops sequestering carbon when it reaches maturity. But conifer forests are harvested for timber, allowing new trees to be planted. He said: “The trees that produce this commercial timber and can grow in this country are virtually all non-native and are predominantly conifers from North America.” The need to plant more trees is now an urgent priority to tackle the climate emergency. Mature tropical forests in Africa and the Amazon are slowing their rate of carbon dioxide absorption from the atmosphere and may soon become a source of emissions, as trees die. Healey said it was time to be pragmatic. “Many people think we can have this huge expansion of tree planting in the UK and it will all happen without costing us as the taxpayer that much money. We also think it will all be nice, native species woodland of broadleaved trees and lots of biodiversity but it’s just economically impossible.” “People and politicians have got to face up to the fact that the vast majority of this tree planting, if it’s to occur at the scale they have talked about, is going to have to come from commercial investments, and forest and environmental policy has got to avoid getting in the way of that unnecessarily,” Healey said. “Planting is catastrophically slow in England and Wales, and many commercial investors feel that they are held back from acquiring land and converting it to woodland because of the way that the whole raft of environmental and other policy regulations are implemented.” Many endangered species – such as curlews and lapwings – depend on open habitats like moors and heaths. “Everyone agrees that the best of these habitats should be protected,” Healey said. “But a lot of them are in really poor condition. Is restoration the best use of all this land, or should some, instead, be converted to conifer plantations or native woodlands? We need to ask how much of our heathland we need to protect or restore and how much we can convert to rapid carbon sequestration.”"
"

On August 2, the Department of Transportation and Environmental Protection Agency made a joint proposal to reform the corporate average fuel economy (CAFE) standards. Originally adopted in 1978, when new cars were required to average all of 18 miles per gallon, the standards were increased by the Obama administration to a target of 54.5 mpg by 2025. (This 54.5 is actually an idealized number; as a practical matter, the real target for 2025 is about 47 mpg.)   
  
  
The new rule proposes to maintain the existing fuel economy standard, which rises to 37 mpg by 2020, and then freeze it at that level after that. By 2025, new automobiles meeting the Obama standard would be about 25 percent more fuel‐​efficient than under the Trump standard — though if fuel prices rise, consumers could end up buying more fuel‐​efficient cars than the standard anyway.   
  
  
Another change, as pointed out in a _Wall Street Journal_ article earlier this week by DOT secretary Elaine Chao and acting EPA administrator Andrew Wheeler, is that the administration wants “to create one national standard.” This means that California won’t be able to impose its own, stronger standards.   
  
  
As the Competitive Enterprise Institute’s Marlo Lewis observes, when Congress created the CAFE program in 1975, it specially forbade states from adopting their own stronger rules because this would greatly increase the costs of compliance to manufacturers. Despite that, the Obama administration decided to exempt California from the one‐​national‐​standard rule. The Trump administration is going back to the actual law.   
  
  
Obama’s adoption of the stricter standards was supported by many carmakers, including Ford, GM, and Chrysler (the latter two of which were under the government’s thumb due to corporate bailouts). However, Volkswagen strongly objected, saying that the standards were unfair to cars while overly generous to light trucks (pick ups, SUVs, and full‐​sized vans). This may be one reason why Ford has announced it is getting almost completely out of the car business, planning to make only light trucks plus the Mustang and a new China‐​made small car called the Focus Active.   
  
  
The Obama standards assumed that two thirds of vehicles sold would be cars, and only a third light trucks. In fact, it has been about half and half. For that reason alone, the EPA in 2016 (when Obama was still president) concluded that standards would have to be changed no matter who was in the White House. Changing them now gives Democrats one more tool to use to bash Trump, but another president would have had to make some changes anyway.   
  
  
Chao and Wheeler argue that rolling back the standards will reduce the cost of new cars by several thousand dollars and reduce total costs to consumers by $500 billion over the next 50 years. However, Americans currently spend about $1.1 trillion a year buying, operating, and insuring cars, so $500 billion over 50 years is less than a 1 percent savings on their driving bills.   
  
  
However, that 1 percent isn’t equitably distributed. To meet the Obama standard, automakers would have to build electric vehicles, sell them at a loss, and then sell their other vehicles for higher prices to make up the difference. Since electric vehicles are mostly purchased by high‐​end buyers, this imposes a regressive tax on lower‐​end auto buyers.   
  
  
Another important issue is that the CAFE program has suffered mission creep. When Congress created the program in 1975, the nation was suffering from politically induced energy shortages. But the standards weren’t needed to save energy; people responded to higher gas prices by buying more fuel‐​efficient cars without the government standards.   
  
  
Today, we have an abundance of energy: after adjusting for inflation, gas prices today are much lower than they were in 1975 and much less volatile. So there’s no need to keep the standards to save energy.   
  
  
Instead, environmentalists defend strict CAFE standards in order to reduce greenhouse gases. But there’s little reason to believe that the standards will have much of an effect on climate change, especially with the emphasis on electric vehicles. This is because most electricity in this country is still generated by burning fossil fuels. Due to losses in generation and transmission, it takes the combustion of three British thermal units (BTUs) of coal, gas, or other fuel to deliver one BTU of electricity to someone’s auto battery. Thus, the savings in greenhouse gas emissions will be far smaller than suggested by the differences in miles per gallon of the Obama and Trump standards.   
  
  
Even if you believe that we can increase the amount of electricity generated using means that don’t produce greenhouse gas, CAFE standards aren’t the best way to reduce carbon dioxide emissions. The McKinsey Report on greenhouse gases concluded that there are many ways of reducing emissions that are far more cost effective than trying to force cars to become more fuel efficient, most of them having to do with making existing and new buildings more energy efficient.   
  
  
Only Congress can repeal the law requiring the standards. In the meantime, rolling back the standards is worthwhile because it lets consumers choose how they are going to save money, save energy, and save the environment.
"
"

This week’s big global warming kerfluffle comes from the EPA’s Inspector General, who says the agency broke the law in preparation of its landmark 2009 “Endangerment Finding” from carbon dioxide and other greenhouse gases. Subsequent to making this finding, according to an infamous 2007 Supreme Court decision, the Agency must regulate emissions, presumably to the point which they no longer cause endangerment.



The IG believes that the EPA ran afoul of a rider to the 2001 appropriations bill that has been variously called the “Data Quality Act” or the “Information Quality Act”. Put simply, the accepted legal interpretation of this two‐​line piece of legislation is that a federal document that is a “highly influential science assessment” must undergo rigorous peer‐​review.



EPA based its endangerment finding on its own “Technical Support Document” (TSD), a weighty tome that drew heavily from the United Nations’ latest (2007) climate compendium, and also from a summary document from federal climatologists called “Global Climate Change Impacts on the United States”.



Like most groupthink projects, these two documents have numerous problems indicative of shoddy peer review. The UN report contained a purely fictional claim that the massive Himalayan ice field will disappear less than 25 years from now. It ultimately owned up to this whopper, with the author of the statement admitting that the alarmist nonsense was put in to try and shock India and China into reducing their carbon dioxide emissions. The other report is so full of holes that an entire counter‐​document, with the exact same format and subject matter, but with all the science that somehow got missed, is currently in the works and due out in a year or so.



Critics of the EPA’s Endangerment Finding launched several attempts to shut it down. I wrote one critique that—right at the beginning of 220 single‐​spaced pages—said that the TSD violated the Data Quality Act. It’s nice to see that the EPA’s IG agrees.



The IG states that the TSD was “highly influential” and therefore had a high bar for peer review that was not met. Further, one of 12 federal climatologists that reviewed it was in fact employed by the EPA. That’s no different than having one of your own colleagues at your university peer review your manuscript for an academic journal, something that simply isn’t done.



The EPA laughably contends that its TSD isn’t “highly influential science”, because it used information from other federal and international compendia, like the problematic United Nations report, which were properly peer reviewed. This is risible; the UN solicits peer review and then its own authors decide which (easy) comments to respond to and which (pesky) ones to ignore.



If something, like the TSD, which will be used as the excuse to tell everyone what kind of light bulb they can burn isn’t “highly influential”, then what is?



But weren’t there other reviewers? What of the reams of comments that wonks like me sent in? Isn’t that evidence for a very vigorous review process? Unfortunately, no. The EPA was under no obligation to address any comment from anyone who wasn’t a member of their Gang of 12.



The policy implications of the IG’s report are probably not as staggering as advocates opposed to the Endangerment Finding make it out to be. While many outside commenters complained that the science in the TSD (and the federal compendium that was used in its construction) was so bad that the peer‐​review process had to have been systematically compromised, the IG merely disagrees with the process and takes no position on the science.



Surely some petition will be presented in some court to require the EPA to re‐​submit the TSD to a broader review as a requirement before it can enforce carbon dioxide regulations. Unfortunately, all this requirement will likely do is delay them for a year or so. There are legions of federal climatologists to choose from, all dependent upon the global warming dole for career advancement. Getting the right review has never been a problem for alarmist climate science.



Perhaps the prospective petitioners could stipulate that the future reviewers not have any funds in the global warming game, but that would create a logical dilemma. The reigning myth is that federal money is the seal of professional accomplishment, so those not on this dole must be incompetent.



The IG’s finding brings to light, yet again, the problems that occur when science is funded like Canadian health care, by a single provider. Until we somehow diversify the funding base for climate science, flapdoodles like the EPA’s peer‐​review problem will continue to repeat, and the erosion of the public’s faith in climate science will continue unabated.
"
"Curious Kids is a series by The Conversation, which gives children of all ages the chance to have their questions about the world answered by experts. All questions are welcome: you or an adult can send them – along with your name, age and town or city where you live – to curiouskids@theconversation.com. We won’t be able to answer every question, but we’ll do our best. What makes the wind? - Eric, 94-year-old kid, Ipswich, UK. The wind has always been very important to us humans: from thousands of years ago, when sailors used the wind to cross the sea in ships, right up to today, as we make electricity from wind turbines. But it’s taken a long time for scientists to understand exactly how the wind is made. Although we can’t see it, the air is made up of billions and billions of tiny particles. There are lots of different types of particles in the air, but the most common ones are nitrogen and oxygen (which is what humans and other animals need to breathe). The wind blows when these air particles move around in the Earth’s atmosphere. The atmosphere is an envelope of gases, which surrounds the Earth. It’s around 100 kilometres thick, which is about the length of 4,000 blue whales.  Most of the particles that make up the Earth’s atmosphere are found closer to the surface. As you get further out into space, there are fewer and fewer particles, until finally, in outer space, there are none.  The weight of all of these particles stacked on top of each other pushes down on the Earth’s surface – and this force is called atmospheric pressure. Atmospheric pressure changes, depending on how warm or cold the Earth’s surface is. When the surface heats up, the air closest to it also gets warmer. And when the air gets warmer, the particles will tend to rise upwards and spread out.  When this happens, it leaves fewer air particles at the Earth’s surface, which lowers the atmospheric pressure.  So, you would expect the air over a very warm and sunny place, like a desert, to have lower atmospheric pressure than the air over a cold and dark place, like the North Pole.  When the warmer air rises, cold air particles – which are generally packed in closer together – will sink into those low pressure areas. This movement of air particles, driven by areas of heating and cooling, is what makes the wind.  How fast the wind blows depends on how much of a difference there is in pressure between a low pressure and a high pressure area of air. If there’s a bigger difference in the pressure, the wind will blow faster. There are 12 different levels of wind speed, measured on a scale called the Beaufort scale. The scale ranges from winds of less than one kilometre per hour (calm) to more than 118 kilometres per hour (hurricane).  Lighter winds are called “breezes”, stronger ones are called “gales”, and the very strongest winds are “hurricanes”.  You might also have heard the weather forecast talking about “easterly” or “northerly” winds. We describe which way the wind is blowing, by the direction it comes from. So an “easterly” blows from east to west, while a “northerly” blows from north to south.  More Curious Kids articles, written by academic experts: How do babies learn to talk? – Ella, aged nine, Melbourne, Australia. Our guinea pigs have dark eyes. Why do we have white eyes? - Rhoswen, aged three, Bristol, UK. What’s the point of nits?! – Connie, aged nine, Nambour, Australia."
"
Share this...FacebookTwitterWe’ve heard how the warmists have equated critical e-mails they’ve received to “threats to their personal safety”.
Well, German climate-catastrophe dissenter Prof. Fritz Vahrenholt, on the other hand, now reveals the real danger of open dissent on climate change science in Germany. He had to be protected by “personal security” from enviro-nutjobs and climate greenshirts.
German business magazine Schleswig-Holstein Manager recently conducted an interview with Vahrenholt concerning the climate-skeptical book he authored together with geologist Dr. Sebastian Lüning: Die kalte Sonne (see recommended book, right side bar). The book was published by renown publishing company Hoffmann & Campe in Hamburg and was released amid a storm of protest last February. Schleswig Holstein Manager magazine writes:
…critique by weekly newspaper “Die Zeit” was so harsh that its publisher, former German Chancellor Helmut Schmidt, personally invited the RWE manager (Vahrenholt] for a meeting. The debates in the meantime escalated to the point where Prof. Dr. Fritz Vahrenholt even needed personal security.”
I’d interpret that as bodyguards. Oh my, such a tolerant bunch these warmists are.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The article in the Schleswig-Holstein Manager magazine introduces Vahrenholt’s and Lüning’s book:
A controversial book. An author who has become the target of hostility. An interview about the book: ‘The development has gone completely out of control.'”
Schleswig-Holstein Manager also quotes chief editor Thomas Vašek of PM Magazine:
Also the business of science is vulnerable to herd mentality. Once a majority opinion crystalizes, it becomes increasingly difficult to express doubt. This is especially true for politically charged climate science.”
At least now we know on which side all the real threats and violence can be found.
 
Share this...FacebookTwitter "
"**Plans for a zip wire at the Principality stadium have been approved by Cardiff Council.**
The plans include a suspension wire bridge, a viewing platform and a zip line from the top of one of the 90m (300ft) high roof spires.
There were two objections on privacy grounds to the application, which has been through a consultation period.
Artist impressions show people walking along the roof and riding down the zip wire.
According to the application zip line riders will be attached to pulleys that will allow them to ride north to south across the stadium roof.
The stairway up to the roof will be built within the stadium.
Wheelchair users will also be able to reach the roof using a special hoist.
The Â£121m stadium opened in 1999. This year it was transformed into Dragon's Heart Hospital to deal with the impact of coronavirus, which saw its use as a sports and music venue halted in the spring."
"
WUWT Flashback:
Royal Society to review climate consensus position
Posted on May 27, 2010
“I  don’t think they were very pleased. I don’t think this sort of thing  has  been done before in the history of the society.”


Society to review climate message
Today: (Via email press release from the GWPF) Royal Society Bows To Climate Change Sceptics


Wednesday, 29 September 2010 22:09		 		 			 			Ben Webster, The Times




Britain’s leading scientific institution has been forced to  rewrite its guide to climate change and admit that there is greater  uncertainty about future temperature increases than it had previously  suggested.
The Royal Society is publishing a new document today after a  rebellion by more than 40 of its fellows who questioned mankind’s  contribution to rising temperatures.

…
The new guide says: “The size of future temperature increases and  other aspects of climate change, especially at the regional scale, are  still subject to uncertainty.”
The Royal Society even appears to criticise scientists who have made  predictions about heatwaves and rising sea levels. It now says: “There  is little confidence in specific projections of future regional climate  change, except at continental scales.”
It adds: “It is not possible to determine exactly how much the Earth  will warm or exactly how the climate will change in the future.
“There remains the possibility that hitherto unknown aspects of the  climate and climate change could emerge and lead to significant  modifications in our understanding.”
The working group that produced the new guide took advice from two  Royal Society fellows who have links to the climate-sceptic think-tank  founded by Lord Lawson of Blaby.
Professor Anthony Kelly and Sir Alan Rudge are members of the  academic advisory council of the Global Warming Policy Foundation. They  were among 43 fellows who signed a petition sent to Lord Rees, the  society’s president, asking for its statement on climate change to be  rewritten to take more account of questions raised by sceptics.
…
Full article at The Times, 30 September 2010


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88d66633',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
click for more
(This IBD Editorial was sent to me by the authors)
By WILLIE SOON, ROBERT  CARTER AND DAVID LEGATES
This is a response  to “Why Can’t We Innovate Our Way To A Carbon-Free Energy Future?“, a  “Perspective” by Bjorn Lomborg that ran in this space a week ago.
Bjorn  Lomborg, author of “The Skeptical Environmentalist” and “Cool It,” is right  about the need to focus on critical health and economic priorities. But he is  wrong about human carbon dioxide emissions causing what is now being called  “global climate disruption.”
By demonizing the gas of life, in league  with Al Gore and Bill Gates, Lomborg commits several serious scientific errors.  As independent scientists, with broad training in mathematics, physics,  chemistry, geology and geography, we know CO2 is not a pollutant, and the notion  of “carbon-free” or “zero-carbon” energy is inherently harmful and  anti-scientific.

If nitrogen, oxygen, hydrogen, helium or any other  nontoxic gas is pumped into a chamber containing air and a growing plant, the  response is barely measurable. By contrast, if more CO2 is added, the plant and  its root system benefit enormously, displaying enhanced growth and more  efficient use of available water and nutrients.
Far from having  detrimental effects, carbon dioxide has decidedly beneficial impacts on plants,  aquatic and terrestrial alike, and a new study connects enhanced plant  productivity to greater bird species diversity in China. How, therefore, can  anyone conclude that human carbon dioxide is a pollutant that must be  eradicated?
These facts erect a formidable barrier for “zero-carbon”  advocates. By insisting that no human CO2 should be emitted, they are promoting  continued suboptimal growth of food plant species in the face of impending  global food shortages — and poorer functioning and less diversity in the global  ecosystem.
Zero-carbon activists respond to these facts by asserting that  human CO2 emissions cause “dangerous global warming.” They are wrong about this,  too.
If rising atmospheric CO2 levels drive global temperatures upward,  as they insist, why is Earth not suffering from the dangerous “fever” that Al  Gore predicted? Instead, after mild warming at the end of the twentieth century,  global temperatures have leveled off for the past decade, amid steadily rising  carbon dioxide levels.
Lomborg’s claim that we need to “cure” so-called  “unchecked climate change” is thus fallacious and contradicted by reality.  Reducing human CO2 emissions will likely have no measurable cooling effect on  planetary temperatures.
His insistence that we prioritize expenditures is  spot-on when applied to genuine environmental and societal problems. However, it  is irrelevant when the problems are mythical — or devised to advance ideological  agendas. Moreover, even if human impacts on the global climate can actually be  measured at some future date, humans currently lack the scientific and  engineering understanding and capability to deliberately “manage” Earth’s  constantly changing climate for the better.
Most certain of all,  atmospheric carbon dioxide is not the “climate control knob” that  anti-hydrocarbon alarmists assert, and it is irresponsible for Lomborg to claim  his socio-political agenda will provide a low-cost solution for the global  warming “problem.”
The scientific reality is that even the United Nations  Intergovernmental Panel on Climate Change has been unable to demonstrate a  cause-and-effect scientific connection between rising human CO2 emissions and  dangerous warming.. To support global limits on CO2 emissions, in the absence of  real-world data showing clear cause and effect, is scientific and policy  incompetence on the highest order.
Imagine a drug company seeking FDA  approval for a new drug, based on an analysis that says simply: “Our  supercomputers say the drug is safe and effective. We have no clinical data to  support this, but can think of no reason actual results would contradict what  our computers predict. Moreover, failure to license the drug will be disastrous  for patients suffering from the targeted disease.” Failing to demand actual  dose-and-response studies, before licensing the drug, would be gross negligence  on FDA’s part.
Between 2007 and 2009, U.S. carbon dioxide emissions  dropped approximately 10%, to their lowest level since 1995, largely because of  reduced energy consumption during the recession. Similar CO2 emission reductions  occurred in Britain, Germany, France and Japan.
Have their climates  gotten better or less dangerous? Are they now a better place, for having a lower  intensity carbon energy diet? Have global temperatures been statistically  unchanged since 1995 because, or in spite of, Chinese and Indian carbon dioxide  emissions increasing far more than the aforementioned countries reduced  theirs?
These are practical, not rhetorical questions. As far as we can  see, the only direct effect of decreasing CO2 levels via expensive renewable  energy programs has been to cost more American and European jobs than would  otherwise have been the case during the global economic recession.
The  central issue is not whether rising CO2 levels will cause a warmer planet. The  fundamental concern is whether globally warmer temperatures are factually worse  (or better) for human societies — and more (or less) damaging to the environment  — than colder temperatures (like those experienced during the ice ages and  Little Ice Age).
Bjorn Lomborg, Al Gore and Bill Gates need to consider  the likelihood that, driven by changes in solar activity and ocean circulation,  Earth will cool significantly over coming decades. Damaging the global economy  with ineffectual carbon dioxide controls, in a futile quest to “stop global  warming,” looks stupid now.
Viewed later, with hindsight, it will be  judged outrageously irresponsible.
• Soon studies sun-climate connections  at the Harvard-Smithsonian Center for Astrophysics.
• Carter is an  emeritus fellow of the Institute of Public Affairs and chief science advisor to  the International Climate Science Coalition.
• Legates is a  hydroclimatologist at the University of Delaware and serves as the state  climatologist of Delaware.
This editorial appeared at Investors Business Daily – here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e87ef1397',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Easy come, easy go.  Nicole was upgraded from TD16 at 11 AM and dissipated as a tropical storm at 5 PM.  Nicole joins Tropical Storm Chris from 2000 as the only other 6-hour 35-knot maximum sustained wind tropical storm [since 1970 & reliable satellite monitoring].  Nicole joins Bonnie and Gaston from this season as top-ten weakest storms on record.  By the way, when you do the post-season storm verification, Nicole + Bonnie + Gaston = 3 storms with a total ACE of less than 1.  Danielle + Earl +  Igor = 3 storms with a total ACE > 90.
About Chris (August 18, 2000):
There were too few forecasts associated with Chris to conduct a meaningful quantitative forecast evaluation.  Despite the prevailing wind-shear environment , all intensity guidance as well as the official forecast incorrectly suggested strengthening.

From the NHC Discussion on Nicole:
SATELLITE…AIRCRAFT…AND SURFACE DATA SHOW THAT THE CIRCULATION OF NICOLE HAS BECOME ELONGATED THIS AFTERNOON.  THE CENTER…WHICH WAS NEVER VERY WELL DEFINED…HAS BECOME UN-TRACKABLE AND
THIS WILL BE THE LAST NHC ADVISORY ON THIS SYSTEM.  THE 12Z GFS AND ECMWF MODELS ONCE AGAIN FORECAST THE DEVELOPMENT OF AN EXTRATROPICAL LOW OFF THE SOUTHEAST COAST OF THE UNITED STATES TONIGHT.  THIS NEW LOW…NOT CONSIDERED TO BE THE REMNANT OF NICOLE…IS FORECAST TO MOVE NORTHWARD ALONG THE EAST COAST OF THE UNITED STATES AS A GALE CENTER DURING THE NEXT COUPLE OF DAYS.
Over at Climate Audit, we described this type of storm as a “baby-whirl“.  The ACE of Nicole is 0.1225.  Here are the top 10-weakest storms from 1970 to 2009 according to ACE in the North Atlantic:   [year, name, ACE, max wind (knots)]
2000 CHRIS       0.1225     35.0
1999 KATRINA  0.245       35.0
2002 BERTHA   0.245       35.0
2005 LEE           0.245        35.0
1995 DEAN        0.2825      40.0
2005 BRET        0.3675      35.0
1988 ISAAC       0.405       40.0
1978 DEBRA      0.41          50.0
2005 JOSE        0.4475      45.0
1978 AMELIA   0.485        45.0
2010  GASTON  0.3675       35.0
2010  BONNIE    0.3675      35.0
2010  NICOLE       0.1225               35.0


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e88e95a3e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

If a scientific paper appeared in a major journal saying that the planet has warmed twice as much as previously thought, that would be front‐​page news in every major paper around the planet. But what would happen if a paper was published demonstrating that the planet may have warmed up only half as much as previously thought?



Nothing. Earlier this month, Ross McKitrick from Canada’s University of Guelph and I published a manuscript in the _Journal of Geophysical Research‐​Atmospheres_ saying precisely that.



Scientists have known for years that temperature records can be contaminated by so‐​called “urban warming,” which results from the fact that long‐​term temperature histories tend to have originated at points of commerce. The bricks, buildings, and pavement of cities retain the heat of the day and impede the flow of ventilating winds.



For example, downtown Washington is warmer than nearby (and more rural) Dulles Airport. As government and services expand down the Dulles Access road, it, too, is beginning to warm compared to more rural sites to the west.



Adjusting data for this effect, or using only rural stations, the United Nations’ Intergovernmental Panel on Climate Change states with confidence that less than 10% of the observed warming in long‐​term climate histories is due to urbanization.



That’s a wonderful hypothesis, and Ross and I decided to test it.



We noted that other types of bias must still be affecting historical climate records. What about the quality of a national network and the competence of the observers? Other factors include movement or closing of weather stations and modification of local land surfaces, such as replacing a forest with a cornfield.



Many of these are socioeconomic, so we built a computer model that included both regional climatic factors, such as latitude, as well as socioeconomic indicators like GDP and applied it to the IPCC’s temperature history.



Weather equipment is very high‐​maintenance. The standard temperature shelter is painted white. If the paint wears or discolors, the shelter absorbs more of the sun’s heat and the thermometer inside will read artificially high. But keeping temperature stations well painted probably isn’t the highest priority in a poor country.



IPCC divides the world into latitude‐​longitude boxes, and for each of these we supplied information on GDP, literacy, amount of missing data (a measure of quality), population change, economic growth and change in coal consumption (the more there is, the cooler the area).



Guess what. Almost all the socioeconomic variables were important. We found the data were of highest quality in North America and that they were very contaminated in Africa and South America. Overall, we found that the socioeconomic biases “likely add up to a net warming bias at the global level that may explain as much as half the observed land‐​based warming trend.”





[S]ocioeconomic biases “likely add up to a net warming bias at the global level that may explain as much as half the observed land‐​based warming trend.”



We then modified IPCC’s temperature data for these biases and compared the statistical distribution of the warming to the original IPCC data and to satellite measures of lower atmospheric temperature that have been available since 1979. Since these are from a single source (the U.S. government), and they don’t have any urban contamination, they are likely to be affected very little by economic factors.



Indeed. The adjusted IPCC data now looks a lot like the satellite data. The biggest change was that the high (very warm) end of the distribution in the IPCC data was knocked off by the unbiasing process.



Where was the press? A Google search reveals that with the exception of a few blog citations, the only major story ran in Canada’s _Financial Post_.



There are several reasons why the press provides so little coverage to science indicating that global warming isn’t the end of the world. One has to do with bias in the scientific literature itself. Theoretically, assuming unbiased climate research, every new finding should have an equal probability of indicating that things are going to be more or less warm, or worse‐​than‐​we‐​thought vs. not‐​so‐​bad.



But, when someone finds that there’s only half as much warming as we thought, and the story is completely ignored, what does this say about the nature of the coverage itself? Somehow, you’d think that would have been newsworthy.
"
"**Most of England will be in the two toughest levels of measures when the national lockdown ends next week.**
The new coronavirus tier restrictions will mean 55 million people remain banned from mixing with other households indoors from 2 December.
Large parts of the Midlands, North East and North West, including Manchester, as well as Kent, are in tier three.
A majority of places are in the second highest level - tier two - including London, and Liverpool city region.
The Isle of Wight, Cornwall and the Isles of Scilly - where there have been no recorded cases in the past week - will be the only areas of England in the lowest level of curbs - tier one.
The system will be regularly reviewed, with the first scheduled for 16 December, so an area's tier level may change before Christmas.
Prime Minister Boris Johnson told a Downing Street press conference that the tougher rules would ""strike a balance"", adding that ""every area has the means of escape"".
Health Secretary Matt Hancock set out the reasoning behind the tier decisions for each area in a written ministerial statement.
He told the Commons: ""Hope is on the horizon but we still have further to go. So we must all dig deep.""
Mr Hancock added that people ""should see these restrictions not as a boundary to push but as a limit on what the public health advice says we can safely do in any area.""
Around 23 million people across 21 local authority areas will be in the highest level - tier three - including Birmingham, Leeds, Sheffield, Tees Valley Combined Authority and North East Combined Authority.
Lancashire, Leicester, Lincolnshire, Slough, Bristol, Kent and Medway will also be in tier three.
Earlier, a rush for details of the tier allocations saw the government website repeatedly crash.
On Thursday, another 498 deaths within 28 days of a positive test were reported in the UK, and a further 17,555 positive cases, the latest figures showed.
Differences between the new tiers include restrictions on where households can meet up:
Gyms and close-contact beauty services like hairdressers will be able to open in all tiers. Guidance said people in all tiers who can work from home, should continue to do so.
Pubs in tier two can only open to serve ""substantial meals"", while those in tier three can only operate as a takeaway or delivery service.
Hospitality bosses said nearly nine in 10 venues believed they ""are not viable to operate"" within tiers two and three.
Make no mistake, this is only a gradual step out of lockdown.
At the start of November nearly half the country was in tier one, meaning households could mix indoors in people's homes and in pubs and restaurants as long as they kept to the rule of six.
Now that is only possible in Cornwall, the Isle of Wight and Isles of Scilly.
Infection rates are showing early signs of coming down, but the government is erring on the side of caution.
Research suggests they were too slow to put areas in higher tiers before lockdown.
They do not want to make that mistake again - and so are starting off high in the hope they can move areas down the tiers.
But it is not only about which area is in which tier.
The top two tiers have been beefed up, particularly in regards to hospitality.
One ray of hope, the government says, is the experience of Liverpool.
Before lockdown it was in tier three and seeing among the highest infection rates in the country.
Today it has now been put in tier two with infection levels pretty close to the national average.
BBC analysis showed 713,573 people live in the new tier one areas, 32.2 million in tier two, and 23.3 million in tier three.
This is compared to 23.5 million in tier one pre-lockdown, 24 million in tier two, and 8.7 million in tier three.
The new tier restrictions will be voted on by MPs next week, with the government already facing opposition from its own backbenchers.
Leading Conservative MP Sir Graham Brady told BBC Radio 4's World at One he would vote against the measures, saying: ""I do think that the policies have been far too authoritarian.""
And former Brexit minister Steve Baker, deputy chairman of the recently-formed Covid Recovery Group (CRG), called for the government to publish analysis - ahead of the vote - of the impact restrictions were likely to have on controlling Covid, as well as the non-Covid health impact and the effect on ""society, people's livelihoods and businesses"".
He said he was ""open"" to supporting measures ""where it can clearly be demonstrated that the government intervention will save more lives than it costs"".
Meanwhile, there was a mixed reaction from regional leaders following the announcement of tier allocations.
MPs and businesses in Cornwall expressed ""huge relief"" at being one of only three areas in England to be placed in tier one.
Mayor of Liverpool Joe Anderson said the city region's move from tier three to tier two was the result of ""hard work, dedication and sacrifice"".
He said: ""We embraced tier three restrictions and worked fast to deliver the testing pilot, bringing in the Army to help us deliver an efficient service.""
And Mayor of London Sadiq Khan said the decision to place the capital in the second highest level - tier two \- was the result of people's ""monumental sacrifice"" and would be a ""welcome boost"" for businesses.
But there was anger among politicians in Lancashire that the whole county had been placed into the highest tier of restrictions, after council leaders asked for it to be split into different tiers to reflect varying rates.
Elsewhere, Leicester MPs and businesses said the city's tier three restrictions were ""extremely difficult to hear"" following 150 days of lockdown.
And Greater Manchester's mayor, Andy Burnham, said he hoped the region would be moved down from tier three in a couple of weeks.
Decisions on tiers are based on public health recommendations informed by a series of public health data, including Covid-19 cases among the over-60s, positivity rates, pressure on the NHS and how quickly cases are rising or falling.
Areas placed in tier three will be eligible for rapid or ""lateral flow"" tests - which give results in about 20 minutes without the need for a lab - to help bring down infections and reduce restrictions.
And they will be offered support by NHS Test and Trace and the armed forces to deliver a six-week rapid community testing programme.
Devolved administrations in Scotland, Wales and Northern Ireland have the power to set their own coronavirus regulations, though all four UK nations have agreed a joint plan for Christmas.
Earlier, data from the Office for National Statistics showed coronavirus infection rates in England were continuing to show signs of levelling off.
**Are you a business owner? How will your business be affected by the latest rules? Share your experiences by emailing**haveyoursay@bbc.co.uk **.**
Please include a contact number if you are willing to speak to a BBC journalist. You can also get in touch in the following ways:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or comment or you can email us at HaveYourSay@bbc.co.uk. Please include your name, age and location with any submission."
nan
"
Share this...FacebookTwitterIn terms of deaths, this is worse than Fukushima.
Yet, we never hear a peep from the media about the dark side of this “safe and environmentally gentle energy”. Imagine if this had happened at a nuclear power plant.
“Building offshore wind parks can be a deadly occupation. Three construction workers have already drowned whilst working on German projects in the North and Baltic Seas. 80 serious accidents have been registered, it was reported Sunday.
A total of three men, including a Polish worker and a Swedish diver have already lost their lives whilst working on offshore wind farms 120 kilometres off the East Frisian coast near Emden, reported the online edition of the Focus magazine on Sunday.
Both workers drowned while working on the BardOffshore wind farm.
Leader of the German Central Command for Maritime Emergencies Hans Werner Monsees told the magazine that only “a better and tighter rescue system” would prevent the number of deaths…” Continue reading…
Original story in FOCUS here, which adds (emphasis added):
The energy transition has had deadly consequences: According to FOCUS information, there have already been 80 serious accidents on German offshore wind park construction sites. Three men have been killed – and more have died.“
More have died? How many? Seems nobody wants to tell us.
Of course any construction site is hazardous and deaths are inevitable. But what irks me is that for one industry it’s acceptable, but not for another (e.g. nuclear).
Share this...FacebookTwitter "
"

Dr. Ben Ho’s piece in Tuesday’s _New York Times_ entitled “The Conservative Case for Solar Subsidies” is certain to raise a few eyebrows amongst the conservative crowd. But Ho asks a valid question that I’m not sure conservatives have seen fit to fully address in the last few years: What, precisely, should free market denizens hold true about energy policy?   
  
The facile response--that there should simply be no government role--doesn’t quite work here. To be fair, few conservatives suggest such a thing. Ho points out that when there are negative externalities to the production of a good or service the economically efficient policy response requires a tax, and our carbon-based fuels emit a variety of pollutants when burned. This holds true regardless of one’s opinion about the verities of carbon emissions and climate change: Smog--which results mainly from automobile emissions--remains a key contributor to myriad health problems in the United States, and particulate matter resulting from coal burning bedevils asthmatics.   
  
But the libertarian solution is not quite as simple as imposing a tax that covers the socialized pollution costs of burning fossil fuels. We have a nationwide energy grid, one that the federal government played an integral role in conceiving and constructing. Without eminent domain, such a thing would have been all but impossible. The federal government’s regulatory apparatus also has an integral role in regulating power providers, and since the provision of power is a natural monopoly, it’s hard to conceive of an alternative, at least for the moment.



One victory libertarians managed to achieve in the last twenty years has been to convince regulators that the inherent natural monopoly lies solely in the distribution and not the production of energy, so the two have been disconnected in most states. These days, most utility customers can choose from where to get their power, which is a good thing, but it still leaves us with a fundamental economic problem: How do we get a natural monopoly--the operators of the energy grid--to behave as if it were subject to competition?   
  
The basic regulatory solution is to allow the utilities to charge a price to recoup their costs plus some incremental profit. This may sound reasonable, but it creates lousy incentives--namely, the utility has zero incentive to control costs under such a scheme. In fact, they have an incentive to _increase_ their costs and capital investments, since this boosts their attendant profits.   
  
Dr. Ho alluded to the profound potential impact that solar energy could have on this monopoly in the future. With the advent of Tesla’s home battery, it is possible that within the next decade (provided the batteries continue to improve--far from a sure thing) we could see some houses begin to go off the grid entirely, with solar energy producing all the energy they need. If this were to occur at a large scale, we might see utilities being forced to act as if they were a competitive firm and strive to boost efficiency where possible in order to distribute energy at the lowest possible costs to keep customers from cutting the power cord.   
  
The other possibility in such an environment is a downward spiral: as people leave the grid, the costs of maintaining it gets spread over fewer people, boosting the cost and, in turn, nudging more people to get off the grid until only those without any other viable energy option are left holding the bag and paying sharply higher costs for energy.   
  
The answer to such a potentiality is to embrace technology and apply a modicum of foresight to regulatory activities. While the utilities would prefer that home solar just go away, Ho points out that its costs are now competitive with gas and coal, and prices seem poised to drop further in the future. The potential of solar energy--and solar energy that can be stored--is that it can allow utilities to dramatically reduce their investments. If energy storage were to allow utilities to close marginal power plants that produce only at the peak demand each day, the savings to power companies would be enormous. What’s more, having distributed power across the grid would also allow utilities to reduce their own capital investment along the grid by smoothing out the fluctuations in power distribution: in essence, doing one of the utilities’ jobs for them.   
  
We are entering a brave new world in energy production, one that threatens to upend the century-old regulated utilities monopoly and replace it with something that could be much less expensive to run--but only if we get the right policies in place to let it develop. The one worry is that the biggest player in this market has absolutely no incentive for these changes to happen, and its regulator more often than not takes its cues thusly. This is the conservative’s task for the next decade--keep a watch on regulators to act in the long-term interests of the customers and not the utilities. It’s easier said than done.   
  



"
"**Cumbria is to be placed in tier two - high alert - when lockdown ends on 2 December, the government has announced.**
The rules mean households cannot mix indoors and people must follow the rule of six if meeting outdoors, while pubs serving ""substantial meals"" can reopen.
In the week to 21 November, there was a small rise in cases in the Carlisle and South Lakeland District Council areas, blamed on a ""large school outbreak"".
However, elsewhere in the county cases had gone down.
The system will be regularly reviewed, with the first scheduled for 16 December.
Many areas have been put into tier three, including all of the North East.
The Department for Health and Social Care said in a statement: ""The picture in Cumbria is broadly improving although case rates in Carlisle and South Lakeland are increasing - with increases likely due to a large school outbreak.
""Case rates in over 60s are above 100 per 100,000 in Carlisle and Barrow-in-Furness.
""These case rates are too high for allocation to tier one but Cumbria's trajectory does currently not warrant inclusion in tier three.""
Throughout the county, pubs and restaurants which serve ""substantial meals"" will be allowed to be operate with table service, but must stop serving by 10pm and close by 11pm.
Spectators will be allowed at outdoor and indoor sport events, or at performances and shows, but with a maximum crowd capacity outdoors of 50% or 2,000 people, whichever is smaller, and indoor 1,000 people.
Make no mistake, this is only a gradual step out of lockdown.
At the start of November more than half the country was in tier one, meaning households could mix indoors in people's homes and in pubs and restaurants as long as they kept to the rule of six.
Now that is only possible in Cornwall, the Isle of Wight and Isles of Scilly.
Infection rates are showing early signs of coming down, but the government is erring on the side of caution.
Research suggests they were too slow to put areas in higher tiers before lockdown.
They do not want to make that mistake again - and so are starting off high in the hope they can move areas down the tiers.
But it is not only about which area is in which tier.
The top two tiers have been beefed up, particularly in regards to hospitality.
One ray of hope, the government says, is the experience of Liverpool.
Before lockdown it was in tier three and seeing among the highest infection rates in the country.
Today it has now been put in tier two with infection levels pretty close to the national average.
Health Secretary Matt Hancock said: ""Thanks to the hard work and sacrifice made by people up and down the country, we are able to move out of national lockdown and into more targeted local, tiered restrictions.
""By following the rules together we can get out of these tough measures.""
_Follow BBC North East & Cumbria on _Twitter _,_Facebook _and_Instagram _. Send your story ideas to_northeastandcumbria@bbc.co.uk _._"
"

Immunizing Mosquitoes To Fight Malaria
By Jesse Emspak, International Business Times
 
(Photo: Wikipedia / Tim Vickers)
A diagram showing the life cycle of  malaria parasites. Researchers are proposing that the disease be  attacked via immunizing the mosquitoes against the plasmodium parasite,  so that it cannot be transmitted from person to person.


Millions of people in the tropics suffer from malaria, a  mosquito-borne disease that has been difficult to treat and which costs  many developing countries millions of dollars per year in lost  productivity. Up to now, efforts at controlling it have focused on  attacking the parasites that cause it, keeping mosquitoes from biting,  or killing the insects.
But at Johns Hopkins University, Rhoel Dinglasan, an entomologist and  biologist, decided to try another tack: immunizing mosquitoes.

Mosquitoes carry a species of parasite called plasmodium. The  parasite lives in the mosquito’s gut. The parasites then spread to the  mosquito’s salivary glands, and when the insect bites an uninfected  person, they enter the bloodstream. At that point the plasmodium goes  from the blood to a person’s liver, where it matures, escaping to the  bloodstream to infect the red blood cells in a form called a merozoite.
Once in the blood cells, the parasite reproduces until the red cell  bursts, releasing more merozoites. This cycle repeating itself causes  the characteristic fever, chills and ache associated with the disease.
…
When a mosquito bites an infected human, it takes up some of the  gametocytes.They aren’t dangerous to people at that stage. Since  plasmodium is vulnerable there, and that is the point that Dinglasan  chose to attack.
Full story here
================================
Here’s the very first Press Release from Johns Hopkins university citing this idea:
Vaccine Blocks Malaria Transmission in Lab Experiments
Researchers  at the Johns Hopkins Malaria Research Institute have for the first time  produced a malarial protein (Pfs48/45) in the proper conformation and  quantity to generate a significant immune response in mice and non-human  primates for use in a potential transmission-blocking vaccine.  Antibodies induced by Pfs48/45 protein vaccine effectively blocked the  sexual development of the malaria-causing parasite, Plasmodium,  as it grows within the mosquito. Sexual development is a critical step  in the parasite’s life cycle and necessary for continued transmission of  malaria from mosquitoes to humans. The study is published in the July  22 edition of the journal PLoS ONE.
“Development  of a successful transmission-blocking vaccine is an essential step in  efforts to control the global spread of malaria. In our study, we  demonstrate the relative ease of expression and induction of potent  transmission-blocking antibodies in mice and non-human primates. This  approach provides a compelling rationale and basis for testing a  transmission-blocking vaccine in humans,” said Nirbhay Kumar, PhD, senior author of the study and professor in Johns Hopkins Bloomberg School of Public Health’s W. Harry Feinstone Department of Molecular Microbiology and Immunology.
For the study, the research team expressed full-length Pfs48/45 in E. coli bacteria to produce the vaccine. Previous attempts to fully express the  protein had not been successful. The vaccine was first given to mice in  the laboratory. The vaccine was also tested in non-human primates  (Olive baboons) in Kenya with similar results. According to the study, a  single-dose vaccine provided a 93 percent transmission-blocking immune  response, reaching greater than 98 percent after a booster given several  months later.
“This is an exciting beginning to what might  become an important tool in the arsenal for malaria control and  progressive elimination of malaria transmission,” said Kumar. There is  no animal reservoir for human malaria and in that regard it is possible  to gradually reduce malaria transmission to a point of almost  eradication. However, Kumar cautioned that more research is needed to  achieve that goal. For one, similar research efforts are needed to  reduce transmission of Plasmodium vivax, another major human malaria parasite.
Malaria  affects greater than 500 million people worldwide and is estimated to  kill over one million people each year, most of whom are children living  in Africa.
In addition to Kumar, “A Potent Malaria  Transmission-Blocking Vaccine Based on Condon Harmonized Full Length  Pfs48/45 Expressed in E. Coli” was published by Debabani Roy Chowdhury, a  postdoctoral fellow of the Johns Hopkins Bloomberg School of Public  Health; Evelina Angov of the U.S. Military Malaria Vaccine Program; and  Thomas Kariuki of the Institute of Primate Research in Nairobi, Kenya.
The research was supported by grants from the National Institutes of Health and the Johns Hopkins Malaria Research Institute.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8806b176',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**For months now, coronavirus restrictions have dictated where millions of Europeans can travel to and who they can see when they get there.**
So with Christmas fast approaching, governments are having to make tough decisions on whether to ease restrictions in time for the holiday period.
Here's a breakdown of what's been announced so far.
Italy is currently seeing the highest number of deaths since the end of March and Prime Minister Giuseppe Conte has told Italians to expect a ""more sober Christmas, without Christmas Eve gatherings, hugs and kisses"".
Many Italian regions are under partial lockdown and travel between them is restricted. These measures will remain in place until 3 December, but reports suggest an emergency decree may see the rules relaxed after this date.
The exact details of the decree are still being discussed by ministers. It is thought there will not be an official limit on social gatherings, but the government has recommended people ""plan to be as few as possible"".
Churches are free to remain open, but a 22:00 nationwide curfew means the traditional midnight mass is unlikely to happen. The beloved Italian Christmas market, meanwhile, has already been banned.
""We think we need to introduce greater precautions to prevent a surge in infections,"" Mr Conte said.
But it is not all bad news: Mr Conte has reassured children that _Babbo Natale_ (Father Christmas) will definitely be visiting as he is exempt from global travel restrictions. Phew.
Many Europeans head to the ski slopes over Christmas, but the continent is divided over whether to keep the resorts open.
**Italian** Prime Minister Conte has warned against these traditional breaks. ""We cannot afford it,"" he said. He is one of several EU leaders, including **German** Chancellor Angela Merkel and **French** President Emmanuel Macron, who have tried to co-ordinate a Europe-wide deal to postpone the ski season.
All three countries have closed their ski lifts over Christmas.
These widespread closures mean popular resorts in the Alps and Dolomites will lose out on billions of euros in revenue. Tourism officials were quick to criticise the plan, with the president of one French tourism body asking: ""So 400 people on a Paris metro won't get infected, but four people on a ski lift will?""
**Austria** believes it can offer safe holidays once restrictions are eased and skiing will be allowed there from 24 December, although quarantine rules could mean it's mainly domestic. **Swiss** resorts have been told they are free to stay open with safety measures in place.
Some fear that - without consensus - holidaymakers will travel long distances to visit the open resorts. In France, there will be random border checks to prevent holidaymakers going to ski in neighbouring countries such as Switzerland.
After weeks of national lockdown, President Emmanuel Macron has said restrictions will start being eased from 28 November. But the majority of lockdown measures will stay in place until just ahead of the festive break on 15 December.
Shops, theatres and cinemas will reopen in time for Christmas and people will be able to visit their families over the festive period. ""We will be able to travel without authorisation, including between regions,"" Mr Macron said in a TV address.
It is worth noting that France has been under a second national lockdown since late October. But on 15 December, this will be replaced by a nationwide curfew from 21:00 to 07:00. The curfew will not apply on Christmas Eve and New Year's Eve, however.
Restaurants and schools will not reopen until at least 20 January, and this is dependent on daily cases dropping below 5,000. Bars, cafes and nightclubs are closed indefinitely.
Religious services will be free to take place from 28 November with a limit of 30 people.
The decision to keep France's hugely popular ski resorts shut has come as a huge disappointment, with local mayors complaining of months of work wiped out. Mr Macron said they could reopen in January ""under favourable conditions"".
Chancellor Merkel has said Germany's ""lockdown light"" is likely to continue until January. Bars, restaurants and entertainment venues are closed but schools and shops are open. ""Daily cases are still far too high, and our intensive care units are still very full,"" she said.
Restrictions have been tightened ahead of the festive period, with masks being introduced more widely in schools and travel strongly discouraged. Moreover, from 1 December, the limit on social gatherings will be reduced to two households and a maximum of five people.
There will, however, be a temporary relaxation of the rules over the Christmas period. Up to 10 people will be able to meet between 23 December and 1 January, although Mrs Merkel has urged Germans to think hard before meeting in groups of this size.
Children aged under 14 are not included in the limit.
Mrs Merkel also said that - beyond the social gathering cap - any further easing of restrictions before Christmas was unlikely because the number of cases remains ""far too high"". She also said the so-called Christmas amnesty was dependant on cases falling.
Most major Christmas markets in Germany have already been cancelled, but some local ones are outlining plans to go ahead on a reduced scale.
As for New Year, fireworks displays have been cancelled while letting them off in the street is likely to be discouraged.
The Spanish government is planning a ""different"" festive period with a limit of six people allowed at parties, reports say.
It is set to recommend that social gatherings in the run-up to Christmas be held on restaurant terraces or other outdoor locations.
Spanish families also traditionally celebrate the Feast of the Three Kings with a parade on the evening on 5 January and the government will recommend that celebrations do not take place.
The plan also recommends ventilating indoor spaces and maintaining social distancing where necessary. But more broadly, Health Minister Salvador Illa has said ""nothing is set in stone"".
""We need to find consensus about [Christmas restrictions]. When it's decided we will announce the measures,"" he said.
Catalonia's government is hoping to allow gatherings of up to 10 people for Christmas. ""We will make our own decisions,"" a spokeswoman for the region said.
While in Madrid, officials are asking the government to approve a mass testing programme at pharmacies in the run-up to Christmas to allow people to meet safely over the festive period.
Austria is under a second national lockdown until 7 December, but there are hopes restrictions could then be eased in time for Christmas.
""The next two weeks are critical,"" Health Minister Rudolf Anschober told Kronen Zeitung newspaper on 23 November. ""Lockdown must not be extended.""
The government has ordered at least seven million antigen tests, and it is hoped a mass testing programme will provide Austrians with a route out of lockdown.
Hundreds of thousands of teachers and police officers will be tested first in early December, along with people in areas with high infection rates. Voluntary mass testing will then be rolled out nationwide in the week before Christmas.
Chancellor Sebastian Kurz has stressed that the country's Christmas measures would be guided by the data. ""Whether there will be regulations for Christmas and New Year's Eve... on how many people you can meet will depend heavily on the number of infections,"" he said.
It is thought schools and non-essential shops will open first, once the lockdown ends on 7 December, but questions remain over whether gatherings will be capped.
The country's traditional Christmas markets, however, will be closed over the festive season.
And while skiing will be allowed from 24 December, hotels will stay closed until 7 January. People visiting from countries with more than 100 cases per 100,000 - which include Germany and Italy - will have to quarantine.
In Russia, the main festive celebrations usually take place on New Year's Eve with many people holding all-night parties.
But over the past two decades there has been an increase in celebrations of Christmas, which the country's Orthodox Christian majority marks on 7 January.
Even so, restrictions are likely to dampen celebrations across the holiday period.
In the capital, Moscow, officials announced new measures that will last until 15 January. These include early closing times for restaurants and cafes and a 25% capacity limit at cinemas and theatres. Residents over the age of 65 and those in high-risk groups must also self-isolate until this date.
The city's month-long festive street festival is also likely to be cancelled, local media report.
Other regions have introduced similar restrictions of their own.
It comes after health officials warned that the situation remains unstable and the epidemic has not peaked yet nationwide. Virus cases have surged in recent weeks, and a record daily tally was recorded on 26 November."
"Russian hoaxers who apparently tricked Prince Harry into offering help to take penguins to the North Pole have raised serious questions over security and screening measures for the Duke and Duchess of Sussex as they leave the royal fold, a royal expert said.  Pretending to be putting through the Swedish activist Greta Thunberg and her father, Svante, hoaxers Vladimir Kuznetsov and Alexey Stolyarov managed to reach Harry on his landline at his rented Vancouver Island mansion on New Year’s Eve and on 22 January, it has been reported.  The royal, seemingly duped into thinking he was talking to the Thunbergs, also criticised Donald Trump and spoke of a “bullying” tabloid media trying to “sink” him and wife Meghan. A spokeswoman for the Sussexes declined to comment when asked if there was any doubt the voice was that of Harry. A former press secretary to the Queen, Dickie Arbiter, claimed the fact that the hoaxers, known as Vovan and Lexus, had reached Harry exposed weaknesses in their personal security. “As long as Harry and Meghan are over there, they’re out of the protection of the system,” he said. “For all its faults, the system does, and is there to, protect.” He said the hoaxers would not have been able to get through the Buckingham Palace switchboard. “They’re pretty vigilant,” he said, adding: “If you’re outside the system, you’re open to anything and everything.” The couple has a 15-strong team of staff based at Buckingham Palace, but they will be disbanded when the couple transition on 31 March, with some staff being made redundant and others redeployed in other royal households. No details about any staff in Canada have been made public. Arbiter spoke as the Sun, which published excerpts of the conversations, reported more details of the hoax calls. Harry failed to spot he was being pranked when the fake Greta and her father said they had 50 penguins that were stuck in land-locked Belarus and they were after a ship to transport them to the north pole, even though the animals are native to the south pole. When asked if he had any contacts to help, the duke is said to have suggested: “I’ve got one person who is a polar guide in the north pole … he may be able to help you, he knows all the right people.” “Greta” also asked if Harry could help her marry into the royal family and suggested she was interested in Prince George, the Sun reported. It said Harry replied: “I can assure you, marrying a prince or princess is not all it’s made up to be.” When the hoaxers suggested there were discussions in Russia that Harry could become head of a restored monarchy, he replied chuckling: “Well there you go, maybe that’s our new purpose: to be able to take over Russia.” The hoaxers joked about Harry smoking “weed” with hippies on Thunberg’s eco-catamaran, and also of forming a celebrity movement called “Stars Save the Earth” with Leonardo DiCaprio and Angelina Jolie. During one call they tricked him into believing mining companies close to Trump were exploiting the fictional island of Chunga-Changa – the name of a Russian children’s song.  The rights to the audio recordings had been “transferred” to British media, the hoaxers said as they confirmed the Sun’s report in response to a Guardian inquiry. In the audio, a person, reportedly Harry, says of the decision to stand down as a senior royal: “Sometimes the right decision isn’t always the easy one. And this decision certainly wasn’t the easy one, but it was the right decision for our family, the right decision to be able to protect my son. And I think there’s a hell of a lot of people around the world that can identify and respect us for putting our family first.” On Trump, he says: “I think the mere fact that Donald Trump is pushing the coal industry so big in America, he has blood on his hands.” He says he is confident “things will change” on the climate agenda within 10 years: “But we can’t wait five to 10 years, so I think if Donald Trump can become president of the United States of America, then anything’s possible, right?” He continues: “You forget, I was in the military for 10 years so I’m more normal than my family would like to believe … But certainly, being in a different position now gives us the ability to say things and do things that we might not have been able to do.” On Prince Andrew, who has stepped down from public duties over his friendship with the convicted sex offender Jeffrey Epstein, he says: “I have very little to say on that. But whatever he has done or hasn’t done, is completely separate from me and my wife.” Harry speaks of Boris Johnson being a “good man”, and tells the person posing as Thunberg: “So you are one of the few people who can reach into his soul and get him to feel and believe in you. But you have to understand that because he has been around for so long like all of these other people, they are already set in their ways.” In separate quotes, published by Mail Online, Harry reportedly says he has been “part of a family and part of a country that is scared of the tabloid media because they have so much power and influence and no morals. “From the moment that I found a wife that was strong enough to be able to stand up for what we believe in together, [that] has basically scared them so much that they’ve now come out incredibly angry, they’ve come out fighting, and all they will try and do now is try and destroy our reputation and try and, you know, sink us.” He adds: “It hasn’t been very nice. It’s been horrible, but we will come out of it stronger people.” Kuznetsov and Stolyarov have previously targeted Elton John, the Turkish president, Recep Tayyip Erdoğan, and the US senator and Democratic presidential hopeful Bernie Sanders."
"Milikini Failautusi, 30, lives on the Pacific island of Tuvalu. She has become virtually a nomad in her own country after rising tides forced her to leave her ancestral atoll and move to the main island, Funafuti. She is now a climate activist. She can no longer visit her home island, yet remains committed to her country with a burning desire to prevent her own children from inheriting an underwater ghost town. This is not just Milikini’s story.  While climate change threatens livelihoods and security around the world, it is women who are bearing the brunt. Women predominate in the workforces of many sectors that are most vulnerable to climate change such as agriculture, livestock and fishing. To make things worse, inequalities mean women are more likely to suffer dislocation to their lives as a result of flooding and drought. According to the UN, about 80% of people displaced by climate change are women. More than 70% of those displaced by the 2010 flooding in Pakistan were women and children. Among those who lost their lives in India, Indonesia and Sri Lanka as a result of the 2004 Indian Ocean tsunami, three times more women died than men. But why? Rigid gender roles in the region meant men in the region were more likely to be able to swim than women. Furthermore, women were more likely to be caring for children and family members during the critical evacuation time. Women who do survive such disasters often end up in unclean evacuation centres where they can be exposed to gender-based violence and cannot access health services. Research by the International Union for Conservation of Nature found climate change and environmental impacts are increasing violence against women and girls including domestic abuse, child marriage and sexual assault. In many societies, encouraging progress is being made towards gender equality, but climate change can stop or even reverse this progress. If business-as-usual climate action continues, there are dangers that the gains of gender equality will be lost. There is a risk that as climate change accelerates, gender roles could become more entrenched. More men may be forced to move in search of better job opportunities, while women are left behind to care for members of their extended families and bear the burden of household responsibilities. Those who have to remain in more disaster-prone or vulnerable locations as a result are then likely to experience greater poverty, have their livelihoods destroyed and suffer increasing health issues. We know that any attempts to restore environmental degradation and lower the risks posed by global heating are likely to fail if they do not take into account gender inequality. We know, from a report published last week by WaterAid, that climate finance is still not reaching the poorest and most vulnerable people, who are likely to be most affected by climate change. – about half of all countries receive less than $5 (£3.86) a year for each person.  In 2016, a UN report found that only 0.01% of all worldwide funding went towards projects addressing both climate change and gender, despite specific provisions in the 2015 Paris agreement for women’s empowerment.  Such statistics are sobering, but there are some encouraging signs of progress. Developed countries have pledged $100bn a year in climate finance by 2020 to help developing nations cut emissions and adapt to problems such as worsening droughts, flooding and sea-level rise. The Green Climate Fund, the main channel through which climate finance is dispersed, stipulates that all grants must treat women’s needs as a priority. The Commonwealth Climate Finance Access Hub, based in Mauritius, is already making impressive progress in unlocking much-needed resources for countries and communities that would not otherwise have the capacity to lodge successful applications for funding already pledged.  At the Commonwealth heads of government meeting in Rwanda in June, we can expect leaders to consider further innovative approaches to tackle environmental priorities, including the strategy proposed by women’s affairs ministers, which focuses on gender and climate change. The strategy is designed to encourage countries to collect and analyse data disaggregated according to criteria such as sex and age in order to devise improved climate solutions and to target them more accurately. Women like Milikini need more than speeches, however sincere. They need urgent collective action to tackle all aspects and impacts of the global climate crisis. Women on the frontline must be on an equal footing at all levels so that they, their families, their communities and the nations in which they live and work can survive and thrive. In the Commonwealth, we are working towards just that."
"
The only lower price than today’s closing price on a ton of carbon is ZERO
 
Perhaps reacting to the news yesterday about the IPCC getting taken to the woodshed, the growing number of stories in the MSM about the IPCC failure, and the recent layoffs at CCX, carbon trading has once again been devalued by the market. Amazingly, it lost 50% of it’s value for 2006, 2007, and 2008 “carbon instruments” today. Here’s the CCX front page graph at closing today: 

The CCX end of day table really says it all, 50% off, from a dime to a nickel in a day:
CCX end of day, August 31, 2010
It must have really killed the person to have to put in a nickel for the closing value today.
Charcoal briquettes and coal have more value than a ton of CCX carbon instruments these days.
Unless CCX starts making adjustments in single cents, the next downward adjustment is zero. The latest CCX advisory says they will be closed for labor day, and will reopen for trading September 7th. One wonders.

Sponsored IT training links:
Learn what is exactly need for LX0-101 exam. We offer self study 1z0-051 products including 412-79 dumps so you will pass real exam on first try.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e89150f5f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Leicester and Leicestershire will be subject to the toughest tier of restrictions after the national lockdown ends on 2 December.**
The city and county will move from tier two to three, meaning very high risk, the government announced.
It means household mixing is banned and pubs and restaurants will close except for delivery and takeaways.
The city and parts of the county were subject to the UK's first local lockdown in June.
The neighbouring county of Rutland will go into tier two of the government's three-tier system.
Leicester has been subject to some level of coronavirus restrictions since the first national lockdown in March.
The government has set out the reasoning behind the tier decisions for each area.
In a written ministerial statement, the government said of Leicester and Leicestershire: ""Improvements have been seen in overall case rates in all but one lower tier local authority, but remain very high at 355 per 100,000, including in over 60s at 250 per 100k. The pressure on the local NHS remains very high.""
Health Secretary Matt Hancock told the Commons: ""I know how tough this is, both for areas that have been in restrictions for a long time like Leicester and Greater Manchester, and also for areas where cases have risen sharply.""
The system will be regularly reviewed - with the first scheduled for 16 December.
The new allocations will put some areas under significantly tighter restrictions than before the second lockdown started.
Places like Market Harborough and Lutterworth have managed to remain in tier one since the system was first introduced, but they along with the rest of the county will go into tier three.
Reacting to the news, the county's director of public health Mike Sandys said: ""Over the past few days, rates have started to fall and we've made some progress. But it's important to put this into perspective.
""Figures are over 20% down compared to this time last week but they're still worse than the day we went into lockdown.
""Leicestershire's average is significantly higher than the national level so there is still work to do.""
In a joint statement, the three Labour MPs for the city - Claudia Webbe, Liz Kendall and shadow health secretary Jonathan Ashworth - said: ""The news that Leicester will go into tier three - on top of the 150 days of our extra lockdown - is extremely difficult to hear.
""The government must now spell out how we can get out of tier three, and the measures they will use to review Leicester's position, to give people hope their sacrifices will make a difference.""
Meanwhile, the Conservative MP for Rutland and Melton said she is pleased about the decision to put Rutland into tier two.
Alicia Kearns said: ""I welcome that Rutland has been respected as the independent county it is and therefore tiered separately.""
But she added she was ""deeply disappointed"" the likes of Melton and Harborough had been grouped with all of Leicestershire.
Leicester had a seven-day coronavirus infection rate of 398.3 per 100,000 people for the week to 21 November - the 14th highest rate in England.
The number of confirmed cases in the same week was 1,411, down from 1,857 in the seven days up to 14 November. The infection rate is also down from 524.2.
The borough of Oadby and Wigston has the county's highest rate of 417.4 - putting it 11th nationally - but the rate has also decreased from 526.2.
The average for the whole of England is 208.7.
The Christmas lights might be up in Leicester, but let's be honest, not many people are going to see them.
It has the title that no city wants - it has been in Covid-restrictive measures for longer than anywhere else in the country.
Today's news from Health Secretary Matt Hancock has confirmed what Leicester's mayor Sir Peter Soulsby and many people here feared - that the city is going into the highest tier.
That's tough news for those here and means it has been eight months since people have not been allowed to have family and friends in their homes.
Some non-urgent operations in Leicester have been cancelled, with one local health boss fearing the second wave of the virus will be worse than the first.
The Leicester, Leicestershire and Rutland's clinical commissioning groups (CCGs) said their hospitals were treating 260 people with coronavirus, compared with 204 at the peak in April.
Last week Leicester mayor Sir Peter Soulsby said the ""hopeless"" performance of the national test and trace system had contributed to the recent surge in city cases.
Both the city and county have been sent thousands of rapid result lateral flow tests to help bolster Covid-19 testing.
_Follow BBC East Midlands on_Facebook _,_Twitter _, or_Instagram _. Send your story ideas to_eastmidsnews@bbc.co.uk _._"
"I have eagerly awaited the final season of Game of Thrones, and its strange blend of fictive medieval Britain, supernatural monsters and pornography. Over the seasons the plot has gained speed and focus while the pornography has vanished and – incredible as it seems – we can also detect that George R.R. Martin’s fantastic story has a message. Perhaps we can explain its enormous success by considering how, at a subconscious, dreamlike level, it deals with humanity’s most profound problem. I want to believe that my interpretation is more than a pathetic attempt to legitimise all those hours of passive TV-watching. Just as the great anthropologist Claude Lévi-Strauss was able to expose central problems of specific Amerindian peoples by analysing their myths, we can analyse our own tales in order to discern the contradictions and apparently unsolvable dilemmas that torment our subconscious.  What Lévi-Strauss called “mythemes” are abstractions of central elements of stories – the basic themes which express their essential messages. Using his method for dissecting myths, we can unearth submerged meanings in fantasy and science fiction. And these dreamy worlds tell us more about ourselves than we generally realise. Film director James Cameron’s blockbusters Aliens (1986) and Avatar (2009), for instance, reflected a fundamental transformation in the predominant worldview of movie audiences. In the quarter of a century that separated the two movies, the signs had been reversed regarding nature, diversity and technology. In the final scene of Aliens, Sigourney Weaver – inside a machine which gives her superhuman strength – battles with a monstrous organism from another planet. With the help of technology she defeats an evil Nature. The monster from outer space symbolises an untamed and threatening biological diversity. Only with the help of the machine can humankind survive. Two decades later the roles were reversed. The final scene in “Avatar” instead shows an evil male capitalist, dressed in similar technological armour, being defeated by a benevolent Nature. The entire ecosystem of the planet Pandora is mobilised in the battle against the human exploiters. Now it is the machines, rather than the monsters, who come from another planet. In this story, technology loses the battle against nature. Only by stopping the machine can nature survive. A similar analysis reveals the real threat against the warring medieval kingdoms in Martin’s imaginary continent of Westeros. The White Walkers’ army of dead beings threatening to destroy the world of humans, accompanied by ongoing climate change (“winter is coming”), is an allegorical representation of fossil fuels. Today, the energy that propels our technological civilisation derives from countless billions of dead organisms whose extinguished sparks have been buried in the Earth’s crust. The metaphor is not at all far-fetched: in both cases, fossil energy is at war with life itself. Fear of the Night King’s murderous corpses could of course be interpreted simply as the existential fear of death which might unite all people. But the connection between the dead army and climate change is sufficiently distinct to make a more political allegory convincing. The forces animating the dead beings threaten to lead to the collapse of civilisation as a whole, beyond the lives of individual humans. As smuggler-turned-knight Davos Seaworth points out to Daenerys: “If we don’t put aside our enmities and band together we will die. And then it doesn’t matter whose skeleton sits on the Iron Throne.” Although the intuition that the approaching White Walkers’ winter could be seen as a metaphor for climate change may be fairly widespread among followers of Game of Thrones, the literal identification of dead (fossil) energy as a lethal threat to humanity appears to have escaped most analyses. The army of zombies evokes modern society’s stock of machines animated by long-dead inorganic energy in the form of coal, oil or gas. Like pre-modern people confronted with such technical contraptions, the inhabitants of Westeros are shocked by the magical capacity of dead objects to move and to wage war on the living. George R.R. Martin’s message is that the continuous human competition for power must be set aside in a joint effort to defeat the threat from the army of the dead. The message from Game of Thrones appears familiar and urgent at a time when humanity is subconsciously struggling with its paradoxical ability to sweep the climate crisis under the carpet while preoccupying itself with everything else. As in a dream, we try to make sense of the contradiction between our awareness of the approaching catastrophe and our remarkable capacity to ignore it. Dreams and fantasies prompt us to reflect on matters that we suppress. In that sense, Game of Thrones is a tale for our times. Read more: Game of Thrones: why hasn’t Westeros had an industrial revolution?"
"Over the past few years I have become an academic expert in “sewage sludge” – the residual, semi-solid mix of excrement packed with microorganisms that is left behind within wastewater treatment plants. Every year the UK alone produces approximately 1.4m tonnes of the stuff. About 80% of it is spread on fields as manure, but this still leaves us with a headache – what do we do with the rest? Despite widespread recognition that a proper management plan is needed, there is one major hurdle that still needs to be overcome. Sludge is near-worthless, in terms of monetary value, and sewage companies sometimes struggle even to give it away. A big part of the problem is that sewage sludge from different treatment plants can have wildly different nutrient values. Not having a product with consistent characteristics significantly undermines its value, especially for agriculture, as farmers could never be sure what it is they’re actually buying. Another problem is that when stacked up against the competition, it’s actually quite poor as fertiliser. Both food waste and manure from farm animals serve the purpose much better and contain fewer pollutants that can find their way into the food chain. So, what should we do with the sludge? After all, we have to do something. In many cases across Europe, the water utility companies simply pay for final disposal or give it away free to farmers – a cost which is no doubt passed on to the farmers’ customers. Even in cases where the utilities manage to actually sell the treated sewage sludge, they do so at a rock bottom price of between £1 and £2 per tonne. That’s a very poor return when you consider the cost of processing a tonne of dry sludge can be £200 or more.  What about just burning it then? It’s not very environmentally friendly, sure, but could it be a solution? The burning of feedstock such as sewage sludge results in the production of energy which is measured in calories. The more calories, the more energy that is produced. Well, even “dewatered” sludge contains approximately 75% water, which means energy is required to evaporate it. And even once it’s dried out, 1 kg of dried sludge contains only 3,300 kilocalories (kcal) of energy – far less than the 4,500 kcal found in 1 kg of food waste, or even the 8,300 kcal found in 1 kg of car tyres. Consequently, incineration is not an attractive option for sewage sludge. Luckily though, when we look at sludge as the sum of its parts, the picture becomes slightly more optimistic. Around 2 to 4% of sludge contains phosphorus, from which struvite – the substance kidney stones are made out of – can be recovered and sold for as much as £300 per tonne for use as fertiliser. Calcium carbonate too is found in significant quantities. The cellulose contained in flushed toilet paper is also recoverable for those with the will to recover it, as is the organic content of sewage which can be recovered as bioplastic, a valuable alternative to conventional, petroleum-derived plastics. Both are expensive to extract, however. The adoption of extraction technology could also be helped along by increasingly strict limits on the use of phosphorus in fertilisers. In fact, a recent EU proposal to regulate fertilisers included manure and food waste collected at source, but excluded compost derived from sludge. Sewage sludge contains phosphorus, mostly from detergents used for washing clothes and dishes. This phosphorus is much more valuable than sludge and could eventually be used in agriculture. Could we therefore be about to see the birth of a booming sludge industry? Could wastewater treatment plants become producers of brown gold? Maybe. Maybe not. There are currently a plethora of directives, codes of practice, quality protocols, publicly available specifications and assurance schemes covering the different aspects of sludge, each of which adds an additional layer to an already complex legislative framework. Such complexity is a deterring factor for investors and makes attracting new players tricky. However, there is enough value in sludge that, with the right will and effort, we can start to put it to really positive use. While few people like to think about what happens after we pull the flush, working out what to do with the waste is of real importance. We need to work out how best to extract the value out of sludge, because at the moment it’s literally being flushed down the toilet."
"**The North East will face the toughest Covid-19 restrictions**when the national lockdown ends **, it has been confirmed.**
All 12 council areas across the region will be placed under tier three from 2 December.
Residents will still be banned from socialising indoors with people they do not live with, apart from support bubbles.
Pubs and restaurants will remain closed except for delivery and takeaway.
The government has set out the reasoning behind the tier decisions for each area in a written ministerial statement.
The news, while not unexpected, will come as a blow to the region's hospitality industry.
Helen Greer, landlady of The Feathers Inn pub in Stocksfield, Northumberland, had to adapt during the second lockdown by offering takeaways and also opened a Christmas shop to keep her business going.
""It's just exhausting, it's exhausting mentally, it's exhausting physically,"" she said.
She added that being placed in tier three long-term would be ""financially horrendous"" and tier two would be a ""non-viable"" model.
""Tier three is going to be hard because it means we have to continue doing the takeaway and that's not easy because we are a rural pub, we are long way from big centres of people, so it's difficult.""
Hotels in tier three will also have to close.
Bed and breakfast owner Claire Shield, who only opened her business this year in Newton-by-the-Sea, Northumberland, said it was the ""not knowing"" that was difficult.
""I was on a bit of a high yesterday, briefly, and then when tier three was announced again today it was back down again. Even though it was expected it was just actually hearing it, and thinking 'ok, here we go again'.""
South Shields Labour MP Emma Lewell-Buck told the Commons hospitality in the town was going to be ""absolutely battered"".
Health Secretary Matt Hancock, urged her to work with local authorities to ""embrace"" community testing, in order to help the North East out of tier three.
The government has released a list of the restrictions each tier area will face, although shops, hairdressers and gyms will be allowed to reopen.
Some rules will be relaxed at Christmas to allow three households to form a bubble.
North East areas affected:
Other areas of England, including Cumbria, have been put into tier two, while just three areas will be in the lowest level of restrictions.
Labour Chief Whip and Newcastle East MP Nick Brown has written to Mr Hancock to ask why Newcastle had been put in tier three, despite it having a lower infection rate than Havering in London, which will be tier two.
Martin Gannon, leader of Labour-controlled Gateshead Council, said councils had ""simply been told"" what was happening and there had been no negotiation.
Covid infection rates across the region have been falling over the past week, according to government figures.
Conservative Stockton South MP Matt Vickers tweeted that while he was not surprised the area had been placed in tier three due to the number of cases, it was ""bitterly disappointing"".
Mr Hancock thanked people for the ""hard work and sacrifice"" which allowed England to move out of a national lockdown.
""I know for those of you faced with tier three restrictions this will be a particularly difficult time but I want to reassure you that we'll be supporting your areas with mass community testing and extra funding,"" he said.
Tier allocations will be reviewed on 16 December, allowing for ""the possibility of areas which continue to make progress in slowing the spread of the disease"" to be moved down a tier before Christmas, the government said.
Unlike some parts of England, spectators will not be allowed back into football stadiums or to other large events.
Residents will still be banned from socialising indoors with anyone outside their household or social bubble, although people will be allowed to meet in groups of up to six in outdoor public spaces, and non-essential shops will reopen.
Redcar and Cleveland Borough Council's leader Mary Lanigan, an Independent, acknowledged that the local infection rate had ""been too high"".
""The ongoing restrictions put a huge strain on people's lives and threaten the prosperity of our borough as businesses cannot trade freely or plan with any confidence for the future,"" she said.
In a joint statement, local authority leaders across Tyneside, Wearside and Northumberland reminded residents that infection rates remained high, particularly as shops prepare to reopen.
However they said that despite the region being put into tier three, the national lockdown did not count for nothing, as rates did fall.
""The reopening of shops and retail outlets is certainly welcome but we know this will result in busy town and city centres as people head out for their Christmas shopping,"" they said.
""It is important that we support local independent businesses and if you head to the larger, busier outlets please follow the guidance and stick to hands, face, space at all times.""
_Follow BBC North East & Cumbria on _Twitter _,_Facebook _and_Instagram _. Send your story ideas to_northeastandcumbria@bbc.co.uk _._"
"**Politicians in Lancashire have reacted angrily after the whole county was placed into the highest tier of restrictions.**
Council leaders had asked for it to be split into different tiers to reflect lower Covid-19 rates in parts of central, west and north Lancashire.
Lancaster and Fleetwood MP Cat Smith said she was ""furious"". South Ribble Borough Council leader Paul Foster said he was ""livid"".
Lancaster City Council is calling for local not regional assessments.
From Wednesday, when the national lockdown ends, Lancashire will be in tier three.
It means people can only meet other households in outdoor public spaces where the rule of six applies and hospitality businesses like pubs, bars and restaurants will have to stay closed.
Gyms and close-contact beauty services like hairdressers will be able to open in all tiers.
Health Secretary Matt Hancock set out the reasoning behind the tier decisions in a written ministerial statement.
The government said the system would be regularly reviewed and an area's tier level may change before Christmas, with the first review scheduled for 16 December.
Council leaders had requested areas with lower infection rates such as Lancaster, Blackpool and the Fylde and Chorley go into tier two, while Blackburn with Darwen, Hyndburn, Rossendale, Burnley, Pendle, Preston - where cases are higher - go into the top tier.
The latest data show Hyndburn had 432 cases per 100,000 people for the week to 21 November. Burnley had 362, Chorley had 211, while Lancaster had the county's lowest number of infections, with 97 cases per 100,000.
A spokesperson for the Department of Health and Social Care said: ""While there have been improvements in some areas, case rates and the proportion of tests which are positive for Covid-19 remain high.
""Case rates in over 60s are very high (over 200 per 100,000) in six lower tier local authorities. There is still pressure on the NHS in this region.""
Councillor Lynn Williams, leader of Blackpool Council, said the authority was ""bitterly disappointed"", adding that tier three status was ""inappropriate"".
""The government has not recognised the significant improvements we have made in reducing infection rates - or the impact tier three will have on our economy and people's livelihoods,"" she said.
""It is difficult to see how we can ever exit tier three if we are always going to be tied into areas of the county that do not have comparable circumstances.""
Labour MP Cat Smith tweeted she was ""furious"" Lancaster and Fleetwood had been put in tier three.
She added: ""Lancaster has a lower infection rate than all bar one London borough. Yet London has been put into tier 2. This makes no sense.""
The Labour leader of Lancaster City Council, Dr Erica Lewis, is calling for a reassessment, ""based on local figures and local risk"".
She tweeted: ""We did not deserve tier 3.
""The campaign for a local rather than regional assessment starts now.""
South Ribble Borough Council leader Paul Foster, who also represents Labour, tweeted: ""I'm livid."" He said cases of the virus in central Lancashire were ""decreasing significantly"".
""This news provides no hope for so many.""
Conservative leader of Wyre Council, David Henderson, said he had proposed the west of Lancashire be placed into tier two so being placed in tier three was ""very disappointing""
He thanked Wyre residents ""for their sacrifices and commitment to sticking to the guidance.""
The tiers would be reassessed in two weeks, he said, adding: ""We must now take this time to bring down the rates even further"".
Preston council leader, Labour's Matthew Brown said he was ""really disappointed"" and worried about the ""continued effect"" it would have on residents and struggling businesses.
A pub owner from Bamber Bridge, Preston, described the news as a ""disaster"" for hospitality.
""It's a massive blow for the hospitality industry,"" Sarah Locke, owner of Ye Olde Hob Inn, said.
""I wish we were put in tier two. This is the busiest time of year and we are back into the unknown again,"" she added.
""Pub owners are stuffed; many won't survive.""
_Why not follow BBC North West on_Facebook _,_Twitter _and_Instagram _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
"Decades of chronic underfunding of water infrastructure is putting many countries at worse risk in the coronavirus crisis, with more than half the global population lacking access to safely managed sanitation, experts said as the UN marked World Water Day on Sunday. Good hygiene – soap and water – are the first line of defence against coronavirus and a vast range of other diseases, yet three quarters of households in developing countries do not have access to somewhere to wash with soap and water, according to Tim Wainwright, chief executive of the charity WaterAid. A third of healthcare facilities in developing countries also lack access to clean water on site.  “It’s really obvious that in Africa and parts of Asia we should be very fearful of what is to come,” he said. “The coronavirus crisis highlights how vulnerable the world is.” The UN World Water Development report, published on Sunday, pointed to the underfunding of water infrastructure around the world, despite its importance. Richard Connor, editor-in-chief of the report, told the Observer that water was often overlooked for spending and investment because the economic benefits of better water and sanitation were not emphasised. The coronavirus crisis sheds new light on those mistakes. “One of the reasons underlying the investment gap in water and sanitation is that these services are perceived mainly as a social - and in some cases environmental - issue, rather than an economic one, like energy,” he said. “Yet the economic costs of an outbreak [such as Covid-19] are enormous, both in terms of national economies and stock markets, as well as in terms of household revenue - when people cannot work because of sickness or lockdowns. Realising the economic importance of water and sanitation should provide an additional catalyst for greater investment.” Another reason for the neglect of water and sanitation is that people are generally willing to pay for the water coming into their homes, but not for transporting and treating afterwards. “Once it is flushed down the toilet, it disappears and becomes someone else’s problem,” said Connor. “Treating wastewater is several times more expensive than treating the source water in the first place. So without a willingness to pay on the part of users, it falls on governments to foot the bill, and since they do not recognise the economic value of wastewater treatment – which is perceived as more of an environmental issue - the political will behind such spending is low.” Yet improving access to water and sanitation has clear benefits – in the coronavirus crisis, and beyond. Connor quotes evidence that suggests that the return on investment in water and sanitation can be high, with a global average benefit–cost ratio of 5.5 for improved sanitation and 2.0 for improved drinking water, when broader macroeconomic benefits are taken into account. Water use has increased sixfold in the past century and is rising by about 1% a year owing to rising populations and increasing demand, while climate breakdown means that more areas of the world will see stress on their water supplies, including regions where supplies were previously abundant, such as many parts of Europe, Asia and north America. One possible source for renewed investment in water is through a better understanding of the links between water issues and water infrastructure and the climate crisis, the UN report suggests. While trillions in investment have been poured into reducing greenhouse gas emissions around the world in the last decade, through clean energy and low-carbon technology, few resources have been devoted to the water supply. This year’s UN water report has found that opportunities are being missed to use water projects to cut greenhouse gas emissions while improving access to clean water. Sewage treatment is a clear example: wastewater gives rise to between 3% and 7% of all greenhouse gas emissions globally, more than flying. Processing sewage can turn wastewater from a source of carbon to a source of clean energy, if the methane is captured and used in place of natural gas. Currently, between 80% and 90% of wastewater around the world is discharged to the environment with no treatment. Farming methods can also be adapted to use water more efficiently and cut carbon at the same time, because when soils are better managed they hold more organic matter, more carbon and more water – rendering them more fertile as well as sequestering greenhouse gases. That makes investing in water a “win-win-win”, in terms of improving people’s lives, generating economic growth and helping to cut carbon, the report found. Yet of the hundreds of billions in climate finance devoted to developing countries in recent years, projects involving water made up less than 1% in 2016, the latest year for which full figures were available, according to the report. “Water does not need to be a problem – it can be part of the solution [to the climate crisis],” said Audrey Azoulay, director-general of Unesco. “Water can support efforts to both [reduce greenhouse gases] and adapt to climate change.” Wainwright said the key ingredient for success in fixing the world’s water problems, alongside funding, was improving governance and how water supplies are managed. “Water needs good governance,” he said. “That is usually what is missing. The world is not running out of water, but there is water stress. There is competition for water resources, but making sure that the people who need water get it is a good investment.”"
nan
"
What were the most viewed stories of 2010 on WUWT? Our WordPress stats counter shows the most viewed stories: 

And here they are with links, should you wish to visit or bookmark:



Sea Ice Page


484,896


Widget


119,405


Climategate


77,362


New paper makes a hockey sticky wicket of Mann et al 98/99/08


76,398


The Gulf oil rig explosion – on the scene photos


57,589


Solar geomagnetic index reaches unprecedented low – only “zero” could be lower – in a month when sunspots became more active


52,110


Hal Lewis: My Resignation From The American Physical Society – an important moment in science history


51,589


Cancun COP16 attendees fall for the old “dihydrogen monoxide” petition as well as signing up to cripple the U.S. Economy


49,040


ENSO/Sea Level/Sea Surface Temperature Page


43,025


You ask, I provide. November 2nd, 1922. Arctic Ocean Getting Warm; Seals Vanish and Icebergs Melt.


39,685


Arctic Sea Ice about to hit ‘normal’ – what will the news say?


39,309


Lord Monckton wins global warming debate at Oxford Union


38,047


The scandal deepens – IPCC AR4 riddled with non peer reviewed WWF papers


32,645


BBC swaps “coldest December since 1981” headline


31,447


Climate change: proposed personal briefing


30,591





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e85e924d0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The UK should reduce its greenhouse gas emissions to net zero by 2050, according to a major new report by the Committee on Climate Change, the government’s official advisers. It’s an ambitious target, but others say the country could go further. Friends of the Earth and other NGOs support a target of net zero by 2045, while activist group Extinction Rebellion want it by 2025.  The committee recognises that setting an earlier date might “send a stronger signal internationally”, but states that 2050 is the “earliest credible date”.  Had the world begun to take serious action back in the early 1990s, when the UN first discussed the problem, the transition away from business as usual would have been much gentler. We now face a much larger task, as global emissions have continued to grow and a fossil fuel-friendly economic structure has become ever more embedded into everyday operations.  So when should – and indeed could – the UK aim to reach net zero?  The question really hangs on how you believe politics works. Either you feel that a tough but “uncredible” target is the only way to get politicians and businesses to actually step up and start the transformation, or you believe that nudging the economy onto the right trajectory will then create an unstoppable momentum.  The first thing to recognise is there is a difference between what might be considered politically “credible”, financially feasible, societally acceptable or even physically possible. We’ll start with finances. Achieving net zero, or indeed any significant reductions in carbon emissions, will require a huge investment in renewable energy and sustainable transport. The European Commission estimates upgrading pipelines and electricity grids alone will cost €200 billion. Bringing forward that investment to achieve net zero in a five year time horizon, as opposed to a 30 year one, is daunting.  There are many significant barriers to achieving even the relatively slow transition recommended by the CCC. In particular, investors want clear and consistent commitments from governments, but past changes in low carbon policies (such as the UK government scrapping its household solar subsidy scheme) have led to a lack of trust. To achieve net zero, grid infrastructure will need to radically change and the UK will need to be better connected to mainland Europe so that renewable technologies, and energy storage options, can be located in the most optimal places across the continent. Europe has already identified several priority projects which would support the transition to low carbon. However, even fast tracked projects can take more than three years to get permission. One of the four priority projects identified is the North Sea offshore grid (NSOG) which would connect renewable generation in the North Sea, Irish Sea, English Channel and Baltic Sea to areas of electricity consumption and storage.  A key challenge here is to really understand how deploying tidal projects and thousands or millions of solar panels and wind turbines, along with the associated infrastructure, might impact on another demand of Extinction Rebellion – biodiversity protection. To do all this in five years would mean our entire electricity infrastructure would need to be redesigned in just a few months.  To be politically acceptable, any target is likely to need to meet two demands. The first, as stated by the Committee on Climate Change, is that is it “credible”. This is a very subjective term, of course, and visions of universal suffrage or an end to slavery also started out as utopian. A 2025 target could be perfectly credible if there was the political leadership to deliver it.  But political acceptability also depends on social impact. It is true to say that mitigating climate change is a benefit for society. But in the short term, if planned incorrectly, tackling climate change could make energy and food more expensive, and increase inequality. While it may be possible to show bold political leadership and deploy technology much more rapidly, is it possible to achieve the societal change that is needed?  Some of this will involve relatively simple asks. The CCC says biodegradable waste should no longer go to landfill, for instance. But other asks will be much more difficult, such as drastically reducing or stopping all aviation, while the UK currently faces a skills crisis and will struggle to train enough engineers, planners and biodiversity assessors. Net zero by 2025 is not physically impossible. There is no real barrier to deploying the technology required or to achieving the necessary changes in behaviour. But to achieve this, it is not a question of physical possibility but rather whether you believe it is possible to change the economic structure and political decision making of the UK (and EU and world) overnight to allow us to deploy all possible solutions over the next five years. Given that Brexit has so far taken a little over two years, politics is, unfortunately, a much more difficult challenge that cannot be fixed simply with technological solutions.  Reach for the stars of 2025 and we might just hit the moon in 2035 or 45. But reaching for the stars may also frighten government into believing that a space race is too expensive. The Extinction Rebellion, alongside school strikes and David Attenborough’s “Climate Change: The Facts” have brought a welcome focus to the urgency of climate change and the UK government must now back a net zero target, even the less ambitious 2050 target. But, more importantly, it must significantly ramp up those policies that will shape our transition to net zero.  Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"Since the night that Notre Dame was devastated in a fire, most people have assumed that the Gothic cathedral at the centre of Paris should be lovingly restored – and more than a billion euros have been pledged towards restoration.  Experts have estimated that the work would take between 20 and 40 years – by which time the UN’s climate agency estimates that we will have long exceeded dangerous levels of global warming, if current levels of emissions continue. On the same day that Notre Dame was blazing, protesters shut down parts of London, urging emergency action against climate collapse. Nations and coastal cities are starting to disappear under rising oceans. Densely populated regions are becoming too hot for humans. Many people in the global south are staring the collapse of functioning society in the face. It therefore seems a strange time to restore at such cost a monument to the western civilisation that helped create these conditions. It is time to take a hard look at buildings such as Notre Dame and ask what they represent, and whether we should still be treasuring them. Our current ideas on climate, environment, and inequality are partly the product of medieval Christian states. They saw humanity as dominant over and separate from nature, rather than part of it. Religious teaching emphasised that Earth was a place of sin, unhappiness and temptation. Good Christians should turn their thoughts to God, obey their priests, and look for justice only in the afterlife. The world would end before long, which meant it would be destroyed and replaced with a new, perfect place in which saved souls would live forever. Church authorities regularly described neighbouring populations as irrational “pagans” who were too close to nature, who worshipped trees, thunder and lightning. This was a justification for invading their lands, converting them to Christianity, and cutting down their sacred trees.   The remnants of these ideas persist in various forms today, from harmful land management practices through to the idea that it is rational to prioritise the economic goals of the state over addressing environment collapse. The reluctance of wealthy nations to deviate from these attitudes is a major cause of climate and ecological breakdown.  So let’s not restore Notre Dame. Instead, let it stand as a symbol of the damage that our climate denial and environmental entitlement have already caused our planet; a reminder of the much greater losses that will follow, and a call to action. Given the emotions around this cathedral and the great love expressed for it by so many people, it may seem strange to target Notre Dame for such a gesture. But when its history is considered, it becomes apparent that Notre Dame and the causes of climate and environmental breakdown are far from strangers. During the nearly 200 years that Notre Dame was being built, the kingdom of France became increasingly powerful and closely governed. This was a crucial stage in the emergence of the modern nation. The ruling dynasty, the Capetians, were skilled propagandists. The new cathedral was built to assert royal and religious prestige. It was a place from which crusades were announced, and the population was taught that God valued obedience from the people. Social inequality was the human condition, and injustices were to be endured. The construction, which began in 1163, was financed through taxes, tithes – 10% of annual income that all laypeople were required to hand over to the church – and labour from the peasant majority. Many of these people were serfs tied to the land they lived on, required by their lords to intensify the ongoing clearing of woodland and draining of marshland to extract the maximum from the terrain. In these areas, ecologies were disrupted and biodiversity declined. Soil erosion, flooding and silting of rivers resulted. The coercive, extractive rule on which Capetian France was founded – and on which Notre Dame was sanctioned – drove a wider exploitation of nature. For example, demand among the elite for furs and other luxuries led to severe reductions in animal, bird and fish populations. Beavers, wildcats and most other fur-bearing animals bigger than a weasel, together with sturgeon and some native salmon, were rare after the 12th century.   In areas where the rule of nobles was weaker and peasant farmers more independent, biodiversity was far better maintained. Peasants were not forced to focus on growing cereal crops to feed nobles and their animals, and so had more varied diets. They were healthier and the risks of famine in the population were lower. There are very clear connections between the strength of the French church and state’s coercive rule and the degree of ecological damage, unscrupulous extraction and social inequality. The most compelling critique of plans to pour money into the restoration of Notre Dame has come from the “gilets jaunes”. They asserted that their protests against rising poverty and inequality had been largely ignored by the same wealthy elites who could find, overnight, a billion euros for the state-launched fundraiser in aid of the prestigious cathedral.  From this perspective, it seems that when making decisions about what matters, the priorities and values of the French state and elite have changed little. The main difference is that it is no longer just the poor of France whose interests are at stake. To adapt to the challenges of the future, we need to take a radically different view of Europe’s past. We must recognise how deeply the roots of western civilisation and our contemporary way of life are entangled with sharp social injustice and environmental destruction. Notre Dame could become a symbol of that recognition. For people of faith, clearing it for worship but not indulging in an expensive restoration could be a powerful way to act on Pope Francis’s call for a drastic transformation of how humans treat the planet. It might seem dramatic, but only a strategic surrender of our damaging ways of living and thinking will enable us to respond to the fierce demands of the coming decades."
"

Ashton Carter, who took office as Secretary of Defense this week, and the heads of the military services all insist that it is impossible to execute the nation’s security strategy without busting through the $499 billion cap that law imposes on next year’s Pentagon budget. The military leaders warn that if Congress will not raise the cap and provide the $534 billon in non‐​war funds that they requested earlier this month for fiscal year 2016 — and get rid of future years’ caps — the United States will require a cheaper security strategy.



If only that were true.



What passes for American security strategy these days is a mixture of militarized global profligacy and national self‐​worship. The latest example is the “National Security Strategy,” which the White House released Feb. 6. It lists eight “top‐​strategic risks,” including non‐​security ills such as disease, economic slowdown, and climate change. But having eight top priorities really means having none. The document’s mentions “hard choices” and warns against “overreach” but then declares that “there are no global problems that cannot be solved without the United States.” It imagines that US military power can be everywhere to promote stability, democracy, and even equality. 





What passes for American security strategy these days is a mixture of militarized global profligacy and national self‐​worship.



Strategy is the prioritization of goals by assignment of resources. Hawks, like Senator John McCain, occasionally insist otherwise and call for military strategy’s liberation from budgetary constraints. But that is like suggesting that aircraft designs ignore physics. Strategy lacks meaning without budgetary context. 



The White House then has not actually produced a security strategy. It has just affixed that title to a florid defense of US leadership and a list of nice things, which collectively define global good as a requirement of US security. Take that standard seriously and you get a permanent sense of insecurity. 



It’d be easy to blame the authors, but recent Pentagon and national strategy documents share the problem. All fail to guide budgetary choice. Nor is writing alone at fault. American security policy, as reflected in budgets, cannot totally avoid prioritization, but somehow policy makers still try. There is no earthly region, with the possible exception of Antarctica, that the Pentagon has not labeled vital while justifying some garrison, alliance, or program there. Pentagon budgets have long avoided much shifting of money across military services, or even within them, though that is what implementing strategy requires.



Austerity, in theory, is a thorough auditor and creative reformer. For people and federal departments, wealth limits competition among desires, making their relative value academic. Belt‐​tightening, short of bankruptcy, demands choices, analysis to inform them, and sometimes innovative ways to do more for less.



Windfall defense budget growth in the 2000s — more than 50 percent, even adjusting for inflation — helped Pentagon leaders take a holiday from strategy. Counterinsurgency was the priority in Afghanistan and Iraq but not Washington, which preferred to fund wars with debt rather than limit investments in future wars linked to stateside jobs. The rising tide of largesse lifted all boats, fighter jets, four‐​star commands, and the like.



You would think austerity made this decade different. Annual Pentagon spending has fallen 25 percent over the past five years, accounting for inflation. Reduced war funds account for the bulk of the drop. Non‐​war Pentagon spending dropped 7 percent in 2013, the one year that the Budget Control Act of 2011 required sequestration, equally‐​applied, across‐​the‐​board cuts. Congress then slightly raised the spending caps that law had imposed for fiscal years 2014 and 2015 and passed budgets that did not exceed them. That avoided further sequestration and kept military spending level.



This relative poverty required some sacrifice. Mostly because of declining war funds, active‐​duty Army end‐​strength dropped from 570,000 to 475,000 troops and is supposed to hit 450,000 in 2018. The Navy and Air Force got fewer new ships and aircraft than they wanted. Base construction slowed. Reduced operational funding caused a Pentagon civilian hiring freeze, curtailed some training exercises, and produced slight reductions in management costs — despite much talk of efficiency. 



Still, strategic change has not come. The end of occupational warfare in the Middle East allowed plans for an Asian pivot, a euphemism for shifting forces to deter China. But funds for troops in Europe and the Middle East (minus the war zones) and their combatant commands have not pivoted. The Navy and Air Force — the forces most relevant to fighting China — did not grab non‐​war funding at the expense of the ground forces. The pivot, relabeled as rebalance, ultimately amounted to little more than words. Rather than change course, the Pentagon is doing a bit less of everything, except grousing. 



Whatever the chiefs say, that is unlikely to change, for two reasons. One is that current austerity is not that austere. 2015 Pentagon spending authority, adjusted for inflation, roughly matches 2004’s, the midpoint of the recent upswing. During the Cold War, spending was higher only in 1952 and 1985, the heights of the Korean War and Reagan buildup.



War budgets — officially, Overseas Contingency Operations funds — also limit pressure on the Pentagon. The 2016 request of $51 billion, ostensibly to fight the Islamic State and keep troops in Afghanistan, would be 21 percent less than 2015, but war budgets are declining slower than war costs. 



Because budget caps do not apply to OCO, the Pentagon and Congress have increasingly hidden non‐​war expenses there. That off‐​book accounting now makes up roughly half of OCO funds. That’s $25 billion of proposed cushioning against austerity next year, more than the cut that the Pentagon would suffer by complying with the cap. The administration insists on keeping this slush fund as long as caps constrain military spending.



Beltway elites are a bigger obstacle to strategic reappraisal. Military leaders long ago learned not to push for funds at the expense of other services. This cartel, which they call jointness, stymies civilian efforts to change strategy.



Few are inclined to try. Foreign‐​policy makers in both parties, especially those vying to be president or get named to a top post by one, proclaim, almost in chorus, that stability everywhere depends on American military presence or actions. Recent experience has made everyone charier of occupying restive states, but Washington’s preferred alternatives are bombing, lethal aid, military training, or oaths of support backed by threats — not staying out and at peace. US leaders are today intellectually immune to real strategies, which require actual choices about what dangers to meet with what defenses.



Absent lower caps or better leaders, hard choices will remain, for Pentagon, just a slogan advertising slight discipline. With a trim here and an accounting trick there, the Department of Defense will muddle along its present course, while elected leaders justify it with paeans about American military power’s indispensability to every pleasant noun that “global” can modify. We that object might take solace in the fact that our hubris is a luxury that our fortune affords. Only blessed nations can worry so much about their safety while confusing it with everything they want.
"
"**Here are five things you need to know about the coronavirus pandemic this Thursday evening. We'll have another update for you on Friday morning.**
Every area of England has the means to ""escape"" the toughest tiers of Covid restrictions, Boris Johnson has said, after the government confirmed most of country would be placed into the two toughest levels when the national lockdown ends on 2 December. Speaking at a Downing Street briefing, the prime minister said the extent of the restrictions would ""bring a great deal of heartbreak and frustration, especially for our vital hospitality sector"". But he suggested rapid community testing could make it easier for areas to move down the levels of rules, adding ""your tier is not your destiny"".
Under the revised local Covid tiers, the majority of England - including London and the Liverpool city region - will be in tier two, while large swathes of the Midlands, North East and North West, including Manchester, as well as Kent, will face the toughest tier three restrictions. It means about 55 million people will remain banned from mixing with other households indoors. Health Secretary Matt Hancock said placing most of the country into the toughest two tiers was ""not easy"" but had been done according to the ""best clinical advice"". You can use the BBC's tool here to check which tier you're in, and you can read our explainer here on what the new rules will be.
There is further evidence coronavirus infections are levelling off in England. Data from the Office for National Statistics suggests infection rates are falling in a number of parts of England, including the North West, London and the South West but rising in the Midlands and the North East. However, it's a mixed picture across the UK, with Wales and Northern Ireland seeing a fall in infections and Scotland an apparent rise. You can check the infection levels in your area here.
Further details have emerged about what Christmas will look like for Scotland following news this week that ""bubbles"" of three households will be able to mix for five days over the festive season. The Scottish government has published new guidance confirming the bubbles are limited to no more than eight people over the age of 11. In contrast, the UK government has set no limits for the number of people in a bubble in England, saying only they should be ""as small as possible"", while no guidance has been published yet for Wales and Northern Ireland. Read our explainer here on what the rules are at Christmas and who you're allowed to see.
Nativity plays and outdoor carolling will be able to go ahead in England after the national lockdown ends on 2 December, MPs have been told. Tory MP Andrew Selous - who speaks for the Church of England - said ""churches and cathedrals can now approach Advent and Christmas with certainty"". Indoor singing will be limited to ""formal performers"", he said, but everyone can take part outdoors. Children's church nativity plays will be allowed if they follow Covid rules.
Get a longer daily news briefing from the BBC in your inbox, each weekday morning, by signing up here.
You can find more information, advice and guides on our coronavirus page.
Here's a rundown of the rules around Christmas across the UK, restrictions for the various tiers in England and Scotland, and details of what's allowed in Wales and Northern Ireland.
**What questions do you have about coronavirus?**
_ **In some cases, your question will be published, displaying your name, age and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read our**_terms & conditions _ **and**_privacy policy.
Use this form to ask your question:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or send them via email to YourQuestions@bbc.co.uk. Please include your name, age and location with any question you send in."
"**Lincolnshire's move into the highest coronavirus tier could have ""a crippling effect on our hospitality sector"", a council leader has said.**
Martin Hill, leader of Lincolnshire County Council, said it was ""a big blow"" for the whole county to be placed in tier three on 2 December.
He said it made no sense as infection rates had fallen and some districts were ""well below the national average"".
The government has been approached for a comment.
Lincolnshire will face tighter restrictions as it enters the ""very high alert"" tier once the nationwide lockdown ends, meaning households can only meet in public spaces like parks, where the rule of six applies.
Health Secretary Matt Hancock set out the reasoning behind the tier decisions for each area in a written ministerial statement.
Mr Hill said: ""It's very disappointing that the whole of Lincolnshire has gone into tier three as we are seeing infection rates fall, especially in those few districts that were previously causing concern, and this could have a crippling effect on our hospitality sector.
""Although our figures have been high in some districts and lower elsewhere, there's a clear levelling-off and drop in the numbers as the lockdown restrictions and the considerable efforts of our residents begin to take effect.
""While some of our districts have infection rates well below the England average, why should the whole of Lincolnshire go into tier three for the sake of higher rates in some districts? It doesn't make sense.""
He said the hospitality sector was ""an important aspect of our economy"" and the council would seek additional support for local businesses.
He said infection rates in four of the seven districts were below the England average and ""I'm expecting the drop in those other areas to continue"".
""We'll be looking to move out of tier three as soon as possible if the picture continues to improve.""
One of the districts with the lowest infection rate is South Holland.
Leader of South Holland District Council Lord Gary Porter said: ""Clearly the government has left sight of the science and just gone with using the lines on a map to sort it.
""Clearly it could have worked as districts. The whole point of having boundaries is that. We should have worked on local administration areas.""
_Follow BBC East Yorkshire and Lincolnshire on_Facebook _,_Twitter _, and_Instagram _. Send your story ideas to_yorkslincs.news@bbc.co.uk _._"
nan
"**Cambridgeshire will be in tier two when England's second lockdown ends on 2 December, it has been announced.**
The county had been in tier one - the lowest tier - prior to the latest shutdown but will move up to match much of the rest of the country.
It means households cannot mix indoors and the rule of six applies outdoors.
Pubs and bars can only open if they serve substantial meals and limited numbers of spectators will be allowed at sports events.
Shops, gyms and personal care services, such as hairdressing salons, can reopen if they are Covid-secure.
In a written statement, Health Secretary Matt Hancock said there was an ""improving picture"" across Cambridgeshire and Peterborough but the case rate was ""still high"" at 123 cases per 100,000 people.
Conservative MP Anthony Browne said he was ""disappointed"" his South Cambridgeshire constituency had been categorised in tier two.
He added: ""The government has categorised the whole of Cambridgeshire and Peterborough together, and the rates are pushed up by much higher levels of infection in Cambridge City and Peterborough.""
But the Liberal Democrat leader of South Cambridgeshire District Council, Bridget Smith, said it was ""absolutely not the time to be divisive and to blame other districts for the tier we find ourselves in"".
""It is the time to follow the experts' advice. South Cambridgeshire wraps all around Cambridge city with many people living or working in both districts so it would be ludicrous for us to be in different tiers,"" she said.
Meanwhile, the Conservative leader of Huntingdonshire District Council, Ryan Fuller, said his area had one of the lowest infection rates in the country.
Mr Fuller said: ""I sympathise with many who feel frustrated with the continuation of restrictions; however, we have come too far to give up now and we must therefore continue to follow national guidance for the tier we are in.""
Conservative Huntingdon MP Jonathan Djanogly said he would need to see the government's evidence for putting his constituency into tier two before MPs get a chance to vote on the new rules.
The Labour MP for Cambridge, Daniel Zeichner, called for a package of economic support to the ""many Cambridge pubs, bars and restaurants"" who were ""desperately worried as we enter the crucial Christmas period"".
Stuart Clements, who runs the White Horse Pub in Eaton Socon, said he hoped to be in tier one but was not surprised to be in tier two.
He said: ""The biggest implication is you can't have social groups mixing so where a group of six friends might come out to enjoy an evening's food and drink and maybe some music, those six people can't now sit together on one table.
""Whilst we'll still get families coming out for dinner, because you can't mix social groups unless you're in the same support bubble, it will have an implication on those people coming out.""
_Find BBC News: East of England on_Facebook _,_Instagram _and_Twitter _. If you have a story suggestion email_eastofenglandnews@bbc.co.uk"
"
My parents used to talk about rationing during the war with great apprehension. Clearly the nutters in and supporting Cancun are clueless as to how such a scheme would be viewed by the public. My inbox has lit up today from all around the world over this issue. Short of Climategate itself, I haven’t quite seen any other similar reaction.

Excerpts from the Telegraph: Cancun climate change summit: scientists call for rationing in developed world
Global warming is now such a serious threat to mankind that climate change    experts are calling for Second World War-style rationing in rich countries    to bring down carbon emissions.

…
In a series of papers published by the Royal Society, physicists and chemists    from some of world’s most respected scientific institutions, including    Oxford University and the Met Office, agreed that current plans to tackle    global warming are not enough.
…
In one paper Professor Kevin Anderson, Director of the Tyndall Centre for    Climate Change Research, said the only way to reduce global emissions    enough, while allowing the poor nations to continue to grow, is to halt    economic growth in the rich world over the next twenty years.
…
Prof Anderson admitted it “would not be easy” to persuade people to reduce    their consumption of goods
He said politicians should consider a rationing system similar to the one    introduced during the last “time of crisis” in the 1930s and 40s.
This could mean a limit on electricity so people are forced to turn the    heating down, turn off the lights and replace old electrical goods like huge    fridges with more efficient models. Food that has travelled from abroad may    be limited and goods that require a lot of energy to manufacture.
“The Second World War and the concept of rationing is something we need to    seriously consider if we are to address the scale of the problem we face,”    he said.
Full story here: Cancun climate change summit: scientists call for rationing in developed world
==============================================================
Does anyone really want to see a ration stamp like this?

For anyone that wishes to get the Royal Society’s papers referenced in the article, they are available for a limited time download here:
http://rsta.royalsocietypublishing.org/content/369/1934.toc#content-block


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e872687e8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"**Publicans and politicians have said they are ""disappointed"" that all of Devon will be placed in Tier 2 coronavirus restrictions.**
Health Secretary Matt Hancock made the announcement to place the county in the ""High alert"" category on Thursday.
The restrictions mean households cannot mix indoors, and pubs can only serve alcohol with substantive meals.
Plymouth MP Luke Pollard said: ""We must not spend a moment longer in tier two than we have to.""
He wants the restrictions, which start on 2 December, to be reviewed in two weeks and said ""our city's hospitality businesses face a grim future"".
He believes Plymouth, Devon and Torbay Councils should be ""dealt with as separate areas and not lumped together"".
The government said the reason for the tier rating is an infection rate of 121 per 100,000 in Devon, and said ""there is pressure at the Royal Devon and Exeter Hospital"".
The Blue Anchor in Teignmouth is one pub that will not be able to reopen because they do not serve food.
On Facebook they said ""we are feeling pretty disheartened"" describing the strategy as ""like expelling a whole school because one kid has done something wrong. It's lazy"".
Teignbridge has the lowest infection rate in England - of 52.9 per 100,000.
This is lower than in Tier 1 Cornwall, where it is 62.4 per 100,000.
The town's MP Anne Marie Morris said: ""I am very unhappy - this is not the right decision for Teignbridge.
""The rate of infection is so low, how can this be justified?""
Alan Connett, leader of Teignbridge Council said: ""I'm hugely disappointed for us in Teignbridge and even more saddened for businesses, and in particular we have wet only pubs who will be viewing this in desperation"".
Michelle Chambers, the landlady of the Dolphin pub in Plymouth, said she is ""disappointed"" and said the decision ""will hit us hard in the pocket but my main worry is about our staff, and how they will be looked after"".
The government said the system will be regularly reviewed and an area's tier level may change before Christmas - the first review is scheduled for 16 December."
"**Exeter's Nightingale hospital will open to coronavirus patients for the first time on Thursday.**
The emergency field hospital will get patients from the Royal Devon and Exeter Hospital ""which is very busy"", said an NHS spokesperson.
It is one of seven Nightingale Hospitals built in England, set up in the spring as an insurance policy in case the NHS became overwhelmed.
The 116-bed hospital has also been used for vaccine trials.
On Twitter, Exeter MP Ben Bradshaw said the opening was ""very good news"".
He added it will ""take pressure off the RD&E hospitals and other local NHS services"".
The Labour MP noted he recently raised the issue with Health Secretary Matt Hancock.
A total of 540 people have died with coronavirus in South West hospitals, with four dying in Devon on Wednesday.
A Nightingale hospital spokesperson said: ""We would ask that the public continue to observe the government's advice on observing the lockdown and social distancing so that we can keep patients safe.""
**How will your Christmas plans be affected?**haveyoursay@bbc.co.uk **.**
Please include a contact number if you are willing to speak to a BBC journalist. You can also get in touch in the following ways:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or comment or you can email us at HaveYourSay@bbc.co.uk. Please include your name, age and location with any submission."
"**A series of portraits of school leavers dressed for proms that never took place because of the coronavirus pandemic has won a Â£15,000 prize for photography.**
The judges of this year's Taylor Wessing Prize felt Alys Tomlinson's Lost Summer ""spoke to the events of 2020... without being heavy handed.""
London-based Tomlinson said she wanted to show her subjects' ""vulnerability, sadness [and] resilience"".
It is the first time that all three winners have been women.
The prize has been running for 18 years, and this year's chosen competition entries can be viewed online until 31 March.
The exhibition \- which features 54 portraits from 37 artists - is displayed in a virtual gallery space that replicates the rooms of London's National Portrait Gallery.
A second prize worth Â£3,000 went to Lydia Goldblatt for Eden, part of a series which draws on mothering and family life.
The artist used four people within a 50-metre radius of her London home to create the series, which includes her winning image of a child in a tent.
The judges, who included British Vogue editor Edward Enninful, felt the image ""embodied the psychological complexity of the events of this year"".
Goldblatt said her photo of ""a child protected but alone... articulate[s] a psychological suspension in which both joy and fear oscillate.""
Another competition entry, Yolanda Y Liou's portrait of plus-size model Enam Asiama, was chosen to receive a third prize worth Â£2,000.
The Taiwan-born photographer, now based in London and Brighton, said she wanted to capture her subject's ""confidence and charisma"".
According to the judges, her ""empowering"" and ""confident"" portrait conveys ""a sense of authentic identity, collaboration and trust"".
Asiama, who Identifies as ""a Black, African-British, fat, queer and femme individual,"" uses social media to fight for inclusivity and visibility for plus-size role models.
This year's contest saw 5,531 submissions entered by 2,169 photographers from 75 countries.
_Follow us on_Facebook _, or on Twitter_@BBCNewsEnts _. If you have a story suggestion email_entertainment.news@bbc.co.uk _._"
"

Economists agree that as long as energy prices are accurate (that is, as long as prices reflect total costs), the “right” (optimal) amount of investment in alternative energy will occur because capitalists like profits. If alternative energy makes economic sense, market actors will quickly figure that out from the price signals they receive and invest accordingly. Only if prices are inaccurate is there the possibility that government can improve market outcomes through taxes on or subsidies of particular energy sources.



Thus, the case for government promotion of alternative energy depends solely on the existence of inaccurate price signals in energy markets. Many argue that the cost of environmental pollution is not reflected in the price of energy. But governments aggressively regulate environmental emissions affecting air, land, and water quality. Those regulations impose costs that are then passed on to consumers. George Mason University economist John Nye persuasively argues that existing empirical work is incapable of telling us whether energy prices are “wrong” or, if so, how wrong they might be.1



Are the environmental regulatory costs embedded in final energy prices greater than, less than, or equal to the environmental damages imposed by energy consumption? No one knows for sure. Estimates of the environmental damages associated with energy consumption vary widely because health and medical professionals remain uncertain about the implications of long‐​term exposure to small concentrations of pollutants.2 Vanderbilt regulatory specialist W. Kip Viscusi, however, has found that — if we accept EPA assessments of these matters — oil prices fully reflect environmental costs, natural gas prices are overly high given the disparity between the external environmental costs associated with gas consumption and the regulations in place to control those emissions, and coal prices are too low because the regulations controlling coal emissions are not strict enough relative to the environmental damages that follow.3



Viscusi’s findings are not definitive, however, because the EPA might well be wrong about the health effects of conventional pollutants. The literature provides plenty of evidence for those who want to argue that true environmental damages are higher or lower than EPA believes. And Viscusi doesn’t consider greenhouse gas emissions in his calculations. But a recent survey of the literature by climate economist Richard Tol finds peer‐​reviewed estimates of the marginal impact of greenhouse gas emissions also vary widely because of scientific, technological, and socio‐​economic uncertainties about the future.4 Moreover, those widely disparate cost estimates are largely driven by different beliefs about how to best price (discount) impacts that will occur in the distant future.5



The argument that energy prices are wrong because they don’t account for the national security costs associated with energy imports is even weaker.6 While it is probably true that U.S. military expenditures would be lower were the Pentagon not charged with the task of protecting friendly oil producing states and international oil shipping lanes, those expenditures are unnecessary. If the U.S. didn’t provide oil security services for oil producers, oil producers would provide those services themselves as long as the marginal benefit of oil security expenditures exceeded the marginal benefits associated with alternative expenditures.



A separate national security issue is whether money spent on oil abroad worsens the problem of international terrorism. Conventional wisdom would answer yes, but we have analyzed the data and concluded there is zero correlation — none! — between oil profits and either the number of fatalities or the number of attacks from Islamic terrorists. Oil revenues fatten the coffers of anti‐​American regimes abroad, but there is no relationship between oil profits and anti‐​American actions from hostile states.



Finally, some argue that the reliance on imports leaves America vulnerable to embargoes. But once oil leaves the territory of the oil producer market agents — not the producer — decide where the oil goes. Fears of producer blackmail are greatly overblown because oil producing economies are far less diversified than oil consuming economies. For example, 85 percent of Iran’s government revenue comes from the oil sector whereas only about 3 percent of U.S. disposable income is spent on oil and gas.7 Unilateral production cut‐​backs to achieve foreign policy goals would prove far more economically harmful to producers than to consumers.



Even if we think that prices for conventional energy are too low because they don’t account for environmental or national security externalities, governmental intervention to promote alternative energy sources would not be the best remedy. Better would be an explicit or implicit tax on conventional energy to get prices “right.” Correcting price signals allows the market rather than the government to pick industry winners. 



Getting prices right would change the energy status quo but allow actors to find the best option through trial and error with their own money. We don’t know whether alternative fuels (solar or wind), better controls on existing fuels (clean coal), a different mix of existing fuels (more nuclear), or energy conservation is more cost effective, and thus markets rather than government should decide. And even if we were confident about such things given the current state of knowledge, allowing markets to sort through the choices allows for a rapid change in investment patterns should technology change. Markets respond more quickly and more efficiently to changing economic conditions than do political or regulatory bodies.



If you are bullish about economics and future potential of alternative energy, then rigging the market to promote alternative energy is unnecessary. If you are more skeptical, then rigging energy markets to favor alternative energy is counterproductive. No school of serious thought, however, leads us to the conclusion that targeted assistance is the best policy. 



_This piece was submitted as the initial argument in a Google Knol debate. The full debate, including opposing arguments and rebuttal, can be foundhere._



  

"
"**Nottinghamshire will return to the highest tier of Covid-19 restrictions, it has been confirmed.**
The move comes after a four-week England-wide lockdown, which has seen Nottingham city and the county's infection rates drop.
Unlike last time, when the county agreed additional restrictions, tier rules will be the same across England.
The council said they understood this means measures such as the 21:00 alcohol sale curfew have been dropped.
Though the Department of Health and Social Care acknowledged ""an improvement"" in infection rates in the city and county, it highlighted the ""very high"" levels among those over 60 (211 per 100,000) and a ""high"" proportion of hospital beds taken up by Covid patients.
Health Secretary Matt Hancock said tier levels will be reviewed ""in a fortnight"" and kept ""regularly under review after that"".
Nottingham became one of the first areas in the country earmarked for tier three restrictions after it ended up with the highest infection rate across the UK in early October.
After lengthy discussions, it was then agreed to put the whole county into the same tier, which came into force from 30 October and included extra rules such as the ban on late night alcohol sales.
One day later, Prime Minister Boris Johnson then announced all of England would enter lockdown, which began on 5 November and ends on 2 December.
Tattoo parlours, tanning and nail salons and some businesses such as betting shops and auction houses also had to close.
Nottinghamshire County Council said they understand local rule variations will no longer apply.
Nottingham City Council leader David Mellen, who had written to the prime minister to stress the work done to cut rates, expressed disappointment at the area's tier placing.
""It's a bitter blow for people in Nottingham who have done the right thing, followed the rules and done an incredible job of driving down the rate of Covid infections from the highest in the country to below the national average,"" he said.
""We had hoped that this would have meant we would be spared going into tier three and the extra restrictions that come with that being imposed on local people and struggling local businesses.
""We will need [central] government to provide further support for businesses, especially hospitality where they will be particularly badly hit, to see them through this as the amounts offered so far won't be enough.""
John Clarke, leader of Gedling Borough Council, said he was ""very angry"" at the move.
""They might as well put up a big sign saying Nottinghamshire is closed,"" he told the Local Democracy Reporting Service.
""I appreciate the problems health-wise and everything else, but the damage to the economy - God knows when we're going to be able to repair it, or even start to repair it.""
After the students returned in September, Nottingham saw a massive increase in rates.
Since then, the rates have been steadily falling and the hope was locally that they would avoid the worst restrictions this time round after lockdown but it appears that is not going to be the case.
Meanwhile, outside of the city, Nottinghamshire will also be going into tier three.
Again, rates have been steadily falling in recent weeks but they are still quite high in some parts of the county, particularly in the north.
Politicians there have been appealing for them to avoid the worst of the measures but it appears those appeals have fallen on deaf ears.
According to the latest figures, the infection rate in the seven days up to 21 November is 203.1 per 100,000 people in Nottingham.
In the previous week, the rate was 262.2, while at its highest the rate was nearly 1,000.
Neighbouring authorities have also recorded decreasing rates, with Broxtowe, Gedling and Rushcliffe all now under 200 cases per 100,000 people.
In the north of the county, Mansfield's infection rate has dropped from 334.8 in the week up to 14 November to 280.8, while Ashfield has gone from 302.5 down to 257.2 and Newark and Sherwood has dropped from 248.3 to 195.2.
Earlier this month, Nottingham and Nottinghamshire Clinical Commissioning Group's Dr Amanda Sullivan said the county's hospitals are treating 50% more people with Covid-19 compared to the first wave.
_Follow BBC East Midlands on_Facebook _,_Twitter _, or_Instagram _. Send your story ideas to_ eastmidsnews@bbc.co.uk _._"
"Aditya Chakrabortty’s analysis of the budget (Johnsonism’s first budget is floating on hype and hot air, 12 March) does an excellent job of explaining that while it may be a move away from the austerity politics of the last 10 years, it is certainly not what we would have hoped for from a Labour budget in terms of helping the have-nots rather than the haves. I’m less sure this can be called Johnsonism, at least not yet. Thatcherism, a term used by the late Stuart Hall and others around Marxism Today in the 1980s, signified a new Tory policy of “authoritarian populism” as they saw it. Perhaps that does capture Johnson’s perspective. The significant change, however, was a move from looking to the manufacturing industry as the key source of riches for the wealthy towards the financial and services sector. By contrast Johnson’s policies seem to reflect a well-funded Micawberism. Something will turn up to allow him to stay in office and he’ll back whatever that may be. It hardly amounts to a coherent ideology.Keith FlettLondon  • In 1972, the then chancellor of the exchequer, Anthony Barber, delivered his now infamous “Barber boom” budget injecting the then unheard-of amount of £1.8bn into the economy. This confounded Harold Wilson’s Labour opposition who were at the time tearing themselves apart over, amongst other things, their poor economic growth performance while in government in 1964-1970. Like Sunak’s budget on Wednesday, the Tories were stealing Labour’s policy of Keynesian-driven growth. However, it all ended very badly for both the Tories and Britain, with the first recession since the second world war, rapid inflation, higher unemployment and the largest balance of payments deficit ever previously recorded. But by then it was Wilson’s problem as the Tories lost the February 1974 election. Will history judge Sunak’s budget in the same way it judges Barber’s?Alan BancroftLondon • It has long been acknowledged that the funding of social care is inadequate, that the system is in need of a major overhaul and NHS performance is intimately linked to care services. However, yet again – and despite earlier claims that Mr Johnson is a “man with a plan” – the issue has been ignored by a chancellor who found no difficulty spraying public money around. The health and social care secretary may justify his colleague’s inaction by telling us that he is convening cross-party discussions to arrive at a consensus, but surely even he can see that one of the ways of oiling the wheels of collaboration is to provide a budget against which sustainable solutions can be developed and introduced.Les BrightExeter • The chancellor’s announcement of a “green gas levy” might appear to be a welcome step in battling the climate emergency (UK takes first small steps to tackle carbon from worst polluters, 12 March), though it’s worth noting that the non-domestic renewable heat incentive (RHI) has subsidised the production of “green” natural gas (biomethane) for a number of years, and is funded from general taxation. So potentially this isn’t a new measure, and is just moving the funding from indirect taxation to a regressive direct tax.Simon RichardsBeaconsfield, Buckinghamshire • Budget day was a wonderful opportunity for the chancellor to have an impact on the country’s carbon output by increasing the fuel duty at a time when the price of fuel was falling. When is the government going to take the climate crisis seriously?Dr David RoweNewcastle upon Tyne • Let’s state the position in simple words: a budget that is just repairing the damage to the British economy and social wellbeing brought about by 10 years of unnecessary Tory austerity government. Few will understand that, far too many will be taken in.Les SummersKidlington, Oxfordshire • All these extra billions to spend on infrastructure and broadband; did Labour win the election, or are the Conservatives now communists?Pete DoreyBath, Somerset • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition  "
"The UK’s biggest fund manager has bowed to client pressure and agreed to launch its first fossil fuel-free ethical pension fund later this year. Legal & General Investment Management (LGIM), which has been one of the most outspoken fund managers over the climate crisis, made the decision after a number of clients raised concerns that stocks such as Shell were still being included in its range of ethical funds. Those clients include PensionBee – an online pension provider that handles £750m-worth of client assets for more than 75,000 customers – which is expected to be among the new fund’s first investors. It wrote to LGIM last year after being inundated with questions from its own customers about the fact that Shell was among the top 10 holdings in one of the investment firm’s ethically focused Future World funds. PensionBee is believed to be one of the fund’s top five owners, with more than £60m invested. “It is clear that there is increasingly strong demand for pension products that give customers the choice to divest from oil,” PensionBee said. While the full stock list for the new fossil fuel-free fund has not yet been confirmed, it will exclude oil companies and take a wider ethical stance by barring investments in tobacco, weapons makers and pure coal manufacturers. Emma Douglas, the head of defined contribution at LGIM, said: “Based on our funds in our Future World range, the new fund will be driven by long-term thematic analysis, the integration of environmental, social and governance (ESG) considerations and active ownership, which means engaging to bring about real, positive change in the companies we invest in.” LGIM, which has more than £1tn in assets, said the fund will be open to corporate pension schemes and individuals. The investment firm has defended holding Shell in its Future World range of funds. Those funds are governed by its climate impact pledge, meaning it is willing to exclude companies over poor governance and weak climate disclosures, as well as for lobbying politicians on policies that risk accelerating the climate crisis. LGIM has said it needs to balance environmental and financial concerns when putting together the investment portfolio, previously noting that Shell is one of the largest payers of dividends in the UK. Its climate pledge has led LGIM to dump stakes in China Construction Bank, Rosneft Oil, Japan Post Holdings, Subaru, Loblaw and Sysco Corporation in 2018. Last year, it cut a further five stocks from the ethical funds, including the oil corporation ExxonMobil, the insurer MetLife, the maker of Spam, Hormel Foods, the US retailer Kroger and Korea Electric Power Corporation (Kepco) last year. However, a Guardian analysis last year also found LGIM as a whole had spent about $367m (£285m) between January and November, increasing its stake in those companies in other parts of the business, even when accounting for the divestments. LGIM openly admits it continues to hold stakes in companies excluded from those environmentally and socially focused funds, saying it gives them the opportunity to engage with the board and ultimately influence company behaviour. Commenting on the launch of the fossil fuel-free fund, PensionBee said: “We hope this is just the start of all savers using their investments to transform the world they live in – for the better of the planet, society and their retirement.”"
"Extinction Rebellion burst onto everybody’s screens with disruptions and mass arrests across the UK and around the world in protest against government inaction on climate change. Radical disruptions have been at the heart of Extinction Rebellion’s activism since it was founded in 2018 – from January’s disruption of London Fashion Week, to the infamous naked protest in Parliament at the beginning of April. But the scale of the most recent actions has finally succeeded in forcing mainstream news cycles to start giving the politics of climate change the attention it deserves. One could argue that Extinction Rebellion’s week of action was fortunately timed – the extension of Article 50 to October has created something of a news vacuum while everyone takes a momentary breather from Brexit. Nevertheless, activists would rightly claim that climate change is the bigger looming catastrophe.  In October 2018, the UN’s climate agency published grave projections of the enormity of the challenge ahead if we are to limit the most catastrophic consequences of climate change. For both Extinction Rebellion and the Fridays for Future school strike movement, the piecemeal response of nations at the UN’s annual climate change conference in Poland in December 2018 made it clear that there is no more time to lose. The aim, then, is to force the issue. Through their blockades of iconic central London sites, Extinction Rebellion is keeping climate change at the forefront of the public and politicians’ lips, making the seemingly abstract problem facing all of us feel real. And rather than just warning of this climate emergency, it offers a vision of an alternative future, where a Citizens’ Assembly takes the lead in reducing UK emissions to net zero. Perhaps inevitably, Extinction Rebellion’s actions have been met with a familiar backlash from some political commentators – witness Adam Boulton’s sneering performance on Sky News, and David Blunkett’s indignant authoritarianism in the Daily Mail. But while activists say they regret the disruption caused to working people, they consider their actions a necessary evil in order to change the conversation. Older activists will surely point to the impact and legacy of 1999’s Battle of Seattle, when the Global Justice Movement successfully closed down the World Trade Organisation’s annual meeting. Not only was this extremely empowering for those involved, it crucially helped make resistance to a largely abstract neoliberal governance structure seem concrete and real. Much like the Occupy demonstrations seven years ago, Extinction Rebellion’s latest eye-catching protests have been friendly and open, laden with artistic performances, talks and human connection. This good-natured spirit has so far meant that the movement has gained significant traction – not only on the airwaves, but on the streets too. Extinction Rebellion’s efforts are aimed at building momentum and are based in political science – their website highlights that it takes just 3.5% of a nation’s population engaged in sustained nonviolent resistance to topple a dictatorship. In the UK, that’s less than 2.5m people. 
Their clear demands and principles give the movement a clarity and focus that the Occupy movement may have lacked, and they are growing week by week – Extinction Rebellion says that 50,000 people have joined the movement since the protests started. But contemporary mainstream news cycles are fast and fickle, so the movement will have to act quickly and carefully to maximise use of its new-found public platform. It’s extremely important that the movement’s purpose does not become overshadowed by its tactics. Extinction Rebellion has ransacked the playbook of direct action repertoires – blocking roads, using fake blood, recreating funeral marches, and surprise nakedness. While these have so far been successful in bringing the movement’s name and cause to the fore, using such tactics ad nauseum can quickly lose the public’s imagination and support. This was evident in the Global Justice Movement of the 2000s, as the desire to recreate the euphoria of Seattle resulted in tactical “summit hopping” with diminishing returns. State agencies also learn quickly how to police repeated mobilisations more ruthlessly and extremely – although Extinction Rebellion’s “trademark” repertoire, the tactical use of mass arrests, so far appears to be combating this threat effectively. Police have powers to disperse protesters, but the sheer number of people now willing to be arrested shifts the balance of power between the public and the state. For example, police have so far been unable to clear any of the four sites in central London, as spates of arrests were closely followed by new wave of protesters arriving to entrench control. The city’s police stations do not have the capacity to hold hundreds of arrested protesters for long periods, and court costs will discourage officers from pursuing charges, limiting the punitive power of the state. At the same time, Extinction Rebellion’s tactics risk fetishising the act of being arrested as a symbol of participants’ commitment to the cause. The movement’s co-founder, Roger Hallam, recently told the BBC that in order to achieve its goal of “getting in the room with government”, it may need to create a law and order crisis on the scale of 1,000 arrests. Such an arbitrary target is problematic, as it may encourage activists to take more risk in pursuit of a goal that is by no means guaranteed. Even if one is critical of the politics seemingly behind many “aggravated trespass” charges, a criminal record can be extremely costly and cause significant problems for many younger activists – especially people of colour. This contrasts with the relative risks posed to seasoned activists whose job, lifestyle or privilege allows them to ride the consequences. It is crucial that Extinction Rebellion fulfils a duty of care to support those who are prepared to put their bodies on the line but, with more than 900 arrested already, its an expensive, high-risk game should multiple criminal charges be brought. For now, Extinction Rebellion activists will consider recent events as a runaway success. They have gained visibility and traction – and have at least temporarily steered media attention away from Brexit. Most importantly, they have put climate change squarely in the middle of public conversation. Let’s hope it stays there. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"Once again the concern is for economic growth (Tories splash the cash, but will it hit the right targets?, 12 March). When will rich countries like ours recognise that growth is damaging the planet and prejudicing the lives of our grandchildren? Rishi Sunak’s budget has allocated £27bn for roadbuilding and a further £4.2bn to eight local authorities for transport projects (Report, 12 March). He hasn’t said (and may not know) how much carbon dioxide these measures, and the resultant traffic, will add to the already overloaded atmosphere and the concomitant heating of the planet.  A rich country? According to the Social Metrics Commission, using government data, there are 14.3 million people living in poverty in the UK. On 11 March you reported that “food banks gave out more than 1.6m parcels last year”. What a national disgrace. It may be claimed that our economy needs to grow in order to support the poor, but there is little evidence that this happens. What is needed is a redistribution of existing wealth, not a grab for more. This could be achieved by the government legislating for citizen’s income, paid to every citizen as an entitlement, and recovered from the better-off by taxation. The Labour party is beginning to talk of this, the Tories should as well.Michael BasseyNewark, Nottinghamshire • I think I got a bit carried away with the idea that the 40-odd years of malingering Thatcherite neoliberalism was finally dead. There are still elements of the Conservative DNA and free-market ideology that persist (Journal, 12 March). Then there is the small matter of the power of the corporations that control our lives and the billionaires who control our media. Not quite time to give up the struggle for a socialist government.John AirsLiverpool • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"

Ten senators — seven Democrats, two Republicans, and one independent — have just returned with differing views from a tour of Greenland.



Bernie Sanders (I-VT) talked about the risk of Greenland’s ice sheet “being lost.” Barbara Mikulski (D-MD) said “melting Greenland ice would cause a 23‐​foot rise in sea levels worldwide.” Bob Corker (R-TN) was more circumspect, saying only that “we’re digging in to understand this issue.”



Sanders’ and Mikulski’s statements are reminiscent of Al Gore’s movie, _An Inconvenient Truth_ , which contains a montage showing much of Florida disappearing as Greenland melts away. This wacko scenario has never enjoyed much respect from the broad scientific community, and newly published research casts even more doubt on it.



Nonetheless, this climatological legend continues to beget junkets, and serve as the basis for carbon dioxide‐​reduction bills, currently before congressional committees, just as scary as Gore’s flick.



Take Senator Dick Durbin (D-IL)‘s “Global Climate Change Security Oversight Act.” It cites a United Nations Intergovernmental Panel on Climate Change projection that if, as is commonly projected, the earth’s mean surface temperature rises an additional 2–5 degrees F by 2100, the melting of Greenland will cause a sea‐​level rise of 6.5 to 13 feet.



The IPCC makes no such forecast whatsoever. On page 820 of its spanking‐​new compendium on climate change, it projects that the melting of Greenland will cause a rise in sea levels of between half an inch and 4.5 inches by 2100. If there were any acceleration of ice loss under a Gore‐​like scenario, the IPCC says there could be an additional sea‐​level rise of 8 inches.



Similarly, Senator John Kerry (D-MA) and Rep. Henry Waxman (D-CA), both find that “Risks associated with an additional temperature increase [of 1.8 degrees F] are grave, including the disintegration of the Greenland Ice Sheet.”



James Hansen, NASA’s chief climate modeler, espoused this scenario in a sworn deposition last year, but when asked whether he could cite a single scientist who agreed with him, he named no one.



And yet that gruesome scene is bullying Congress into passing hasty, ill‐​conceived policies on climate change.



In fact, the UN Intergovernmental Panel on Climate Change even _lowered_ its estimate of maximum likely sea‐​level rise for this century. The range used to be from 5 to 27 inches, but in the final version of its most recent climate compendium, the top figure is down to 19 inches. Those estimates assume that carbon dioxide emissions continue to rise at the average rate projected by a large number of future simulations.



The IPCC is very circumspect about its sea‐​level rise projections. “Models used to date do not include uncertainties in the climate‐​carbon cycle feedback nor do they include the full effects of changes in ice sheet flow, _because a basis in published literature is lacking_ ,” it reports. “The projections include a contribution due to increased ice flow from Greenland and Antarctica at the rates observed for 1993 to 2003, _but these flow rates could increase or decrease in the future_ [emphases added].



The most recent research certainly bears out the UN’s caution. Writing in _Science_ last month, Eske Willerslev of the University of Copenhagen and several colleagues demonstrated that there was still ice in south‐​central Greenland during the height of the last interglacial (warm) period, between 116,000 and 130,000 years ago.



During this period, Greenland’s temperatures were about 9 degrees F higher than they’ll get in the next century. Given that Greenland maintained this temperature for 15,000 years, how can one ever support the notion that less than 2 degrees of warming will cause it to lose most of its ice?



Speaking scientific reserve, Willerslev said his work “suggests a problem with the models” predicting a massive loss of Greenland’s ice.



Indeed. But even those models take about 800 years or so for Greenland to lose half of its ice, not a mere century. They assume that human activity will somehow quadruple the amount of carbon dioxide in the air, and keep it at that concentration for a millennium. The actual increase, to date, has been 36%. It’s a little cheeky, to say the least, to assume that in the year 2500 we will be burning fossil fuels at a rate several times greater than we are now.



Another corollary to the current Greenland hysteria is that once it loses its ice, it will never get it back. Willerslev and his colleagues hit that one out of the park. They found that in a previous non‐​glacial period — around 500,000 years or so ago — southern Greenland looked a lot like New England, complete with trees and forests found there today. But the ice obviously came back.



One can only hope that these new findings will compel Congress to cool it on global warming. These are reassuring, rather than inconvenient, truths.
"
"

If nothing else, the latest report of the United States National Intelligence Council (NIC) makes a prophet of Kishore Mahbubani. His book, _The New Asian Hemisphere: The Irresistible Shift of Global Power to the East_ published at the beginning of the year foreshadowed one of the main conclusions of the NIC report released last week. 



Among the assessments of the report, _Global Trends 2025: A Transformed World_ , is that “the unprecedented transfer of wealth roughly from West to East now underway will continue for the foreseeable future.” 



This projection coincides with US president‐​elect Barack Obama’s announcement of an economic team with deep experience of managing international economic crises in the past decade. His string of appointees, including Timothy Geithner as Treasury secretary, may be a strong indication of Obama’s awareness of the changing world — as well as how he intends to govern. 



Geithner, appointed just days after the release of the Global Trends reports, worked previously at the International Monetary Fund, and was under secretary of the Treasury for international affairs during the administration of Bill Clinton, where he played a key role in dealing with the Asian financial crisis of 1997–8.



Global Trends is the fourth unclassified report prepared by the NIC in recent years that takes a long‐​term view of the future. Like previous reports, it was prepared to stimulate strategic thinking about the future by identifying key trends, the factors that drive them, where they seem to be headed, and how they might interact.



Like the old Chinese curse, the not‐​so‐​far‐​distant future looks to be a very interesting time, with both opportunities and perils. And the Asian region is going to be a very big part of it. 



Among the report’s “relative certainties” is that “a global multipolar system is emerging with the rise of China, India and others.” Similarly, among the key “uncertainties” are: 



If the report is correct, the world finds itself in the midst of a transition to a place where the political and economic structure will be markedly different. The report finds that the “international system — as constructed following World War II — will be almost unrecognizable by 2025, owing to the rise of emerging powers, a globalizing economy, an historic transfer of relative wealth and economic power from West to East, and the growing influence of non‐​state actors. By 2025, the international system will be a global multipolar one with gaps in national power continuing to narrow between developed and developing countries.” 



And therein lies a danger. The report notes that historically, emerging multipolar systems have been more unstable than bipolar or unipolar ones. While it doesn’t predict the destruction of the international order, like the one that led to World War I, it does not rule out a 19th century‐​like scenario of arms races, territorial expansion and military rivalries.



Economically, in the future when Asia speaks the world will attentively listen. Growth projections for Brazil, Russia, India and China (the BRIC countries) indicate they will collectively match the original Group of Seven’s share of global gross domestic product (GDP) by 2040–2050. 



Asia will also be the region producing the major share of the future middle class. Over the next several decades, the number of people considered to be in the “global middle class” is projected to swell from 440 million to 1.2 billion — or from 7.6% of the world’s population to 16.1%, according to the World Bank. Most of the new entrants will come from China and India. 



In a sense, China and India are restoring the positions they held two centuries ago when China produced approximately 30% and India 15% of the world’s wealth. 



China is poised to have more impact on the world over the next 20 years than any other country. If current trends persist, by 2025 China will have the world’s second largest economy. Both China and India’s gross national product (GNP) is expected to exceed that of the US in 2035, but they will continue to lag in per capita income for decades. 



But it will not be an Adam Smith‐​type economy. Generally, China, India and Russia are not following the West’s liberal model for self‐​development, but instead are using a different model, “state capitalism”; the system of economic management that gives a prominent role to the state. 



And, if demography is destiny, Asia will also be where the future lies. World population is projected to grow by about 1.2 billion between 2009 and 2025, from 6.8 billion to around 8 billion people. Demographers project that Asia and Africa will account for most of the population growth until 2025, while less than 3% of the growth will occur in the “West.” 



The largest increase will occur in India, representing about one‐​fifth of all growth. India’s population is projected to climb by around 240 million by 2025, reaching approximately 1.45 billion people. From 2009 to 2025, China, is projected to add more than 100 million to its current population of over 1.3 billion. 



That is not to say there won’t be problems. Around 2015, the size of China’s working‐​age population will start to decline. The onset of larger proportions of retirees and relatively fewer workers is being accelerated by decades of policies that have limited childbirth and by a tradition of early retirement. By opting to slow population growth dramatically to dampen growing demand for energy, water and food, China is hastening the aging of its population. 



By 2025, a large proportion of China’s population will be retired or entering retirement. About the same time, due to growth in India’s densely populated northern states, its population is projected to overtake China’s. 



Asia is also projected to be the region where various conflicts might erupt. The report states that over the next 15–20 years, reactions to the decisions Iran makes about its nuclear program could cause a number of regional states to intensify these efforts and consider actively pursuing nuclear weapons. 



On the plus side, the report says, “We see a unified Korea as likely by 2025 — if not as a unitary state, than in some form of North‐​South confederation.” 



In this future world the United States will find itself as just one of a number of important actors on the world stage, albeit still the most powerful military nation. But advances by others states in science and technology, expanded adoption of irregular warfare tactics by both state and non‐​state actors, and proliferation of long‐​range precision weapons, and growing use of cyber warfare attacks increasingly will constrict US freedom of action. 



This constrained US role raises questions about how effectively new agenda issues will be addressed. Despite the recent rise in anti‐​Americanism — which the report now thinks is beginning to wane somewhat — the US probably will continue to be seen as a much‐​needed regional balancer in the Middle East and Asia. 



Other countries still expect the United States to play a significant role in using its military power to counter global terrorism or provide leadership on climate change. Yet the future proliferation of influential actors and distrust of vast power mean less room for the US to call the shots without the support of strong partnerships. 



Thus Barack Obama and future US presidents will need to be doing a lot of talking with other national leaders in the future. 
"
"As we work to tackle the Covid-19 pandemic, all of us – individuals, families, communities, and the worlds of business, finance and government – are reminded of the importance of thinking, planning and budgeting for the long term. It is essential to deal with coronavirus as it is – a global emergency – but it is clear we must work harder to predict and prepare for the existential risks we face. Not only the threat of pandemics, but the climate crisis and unchecked technological advancements.  This week, as parliament scrutinises emergency Covid-19 legislation, Caroline Lucas will introduce a much longer-term proposal, the wellbeing of future generations bill. As co-sponsors of the bill, our aim is to enshrine long-term thinking, and the voices of future generations, at the heart of decision-making. The bill, which is also being steered through the House of Lords by John Bird, is inspired by the model in Wales, where their future generations commissioner is taking long-term thinking into the mainstream. This bill represents the need for cultural shift, which will take time. But with the eyes of future generations upon us, it also presents parliament with an opportunity to act today for tomorrow, and level up opportunity between current and future generations. Caroline Lucas MP Green, Bob Blackman MP Conservative, Bambos Charalambous MP Labour, Simon Fell MP Conservative, Claire Hanna MP Social Democratic and Labour party, Wera Hobhouse MP Liberal Democrat, Kevin Hollinrake MP Conservative, Anna McMorrin MP Labour, Abena Oppong-Asare MP Labour, Liz Saville Roberts MP Plaid Cymru, Alex Sobel MP Labour Co-operative, Philippa Whitford MP Scottish National party"
"

My crazy brother Tom — yeah, _that_ one, the first guy to commercialize production of the Black Truffle — used to call them “organos.” Back in his graduate school days at Oregon State he would rail at the sanctimony of the beans‐​and‐​rice crowd, and opine that “everything they knew was wrong.” Passing the salad bar, he would mutter, “empty vitamins.”



And they were pretty wrong. Acid rain didn’t kill the Black Forest. Small could be ugly. The Population Bomb never exploded.



Since our halcyon times in Oregon I’ve been waiting for the definitive tome on the systematic errors of organoism, and I have finally found it in Todd Myers’ new book _Eco‐​fads_.



Myers isn’t running for the president of the young curmudgeon’s club like Tom was. He thinks global warming might be important, but that the “solutions” ginned up by the political process are counterproductive. For example, in the name of climate change we subsidize solar energy so that its cost in 2016 will be an “astounding $396.10 per MWh”(Megawatt-hour) compared to about $79 for natural gas. (My other brother, Bob, who is in fact a curmudgeon, testified last month to Congress that solar will be even _more_ expensive than that).





G]overnments are just very bad at picking winners and losers in the energy world.



Why did we listen to environmental activists and become the first nation in human history to burn up its food supply to power automobiles? Why did we allow horrendously bad science — see the spotted owl — destroy the livelihood (and some of the lives) of so many in the Pacific Northwest? Why are environmental journalists so obsessed with horror stories and so repelled by good news? Why do we succumb to so many eco‐​fads, from grass‐​fed beef to locavorism to passive solar homes that leak heat like sieves?



If you ask Myers, he’ll probably answer “it’s complicated”. But it gets down, largely, to incentives, summarized by his PPP model: personal, popular and phony.



Personal: a Prius emits sanctimony (while a Chevy Volt confers sainthood). Popular: Green is the modern religion, and heretics are shunned. Phony: that new hybrid _adds_ carbon dioxide emissions, because that Accord it was traded in for is going to be on the road another decade.



Personal: “green buildings”, such as new schools that sprouted all over Washington. Popular: who would be against this? Phony: most of them consume more energy than their conventional counterparts.



In a former life, Myers was Communications Director for the Washington Department of Natural Resources, where he no doubt got up close and personal with most of the organo cults. Now he’s the Environmental Director for the Washington Policy Center in Seattle, proving that all ecotopian garden parties need a skunk.



Myers’ DNR beat was forest management, where he fought, somewhat successfully, the organo nostrum that forests left alone and protected from fires are healthy. In fact, they tend to be pretty sick, as the normal thinning from fires is suppressed, resulting in an unhealthy tree density, invasion and death from bark beetles, and then — surprise — a mega fire that takes down the entire woods.



Just about every organo sacrament withers under Myers’ scrutiny. “Buying local” often means more dreaded greenhouse gas emissions from inefficient short‐​term shipment compared to the economies of scale when carloads of spuds ride the Burlington Northern Santa Fe across the country. “Certified Organic” means so much paperwork and oversight that mom‐​and‐​pop farms (another organo icon) get pushed out by corporate agriculture, which can afford to spend the time and resources satisfying bureaucrats.



Then there are “green jobs.” Solyndra is no outlier; governments are just very bad at picking winners and losers in the energy world. Myers documents the decline and fall of biofuel plants throughout the northwest. Inefficiencies destroy jobs. The Teanaway “Solar Reserve”, supported by an ever‐​increasing feed of taxpayer dollars, was supposed to be the “world’s largest”, supplying power to a grand total of 45,000 homes. That’s all you get?



John Plaza, CEO of the failed biofuel facility Imperium Renewables (you would think a better name would have helped) thinks it’s all the government’s fault. “What the industry needs,” he said, “is a two‐​fold support, a mandated floor, and incentives and tax policy to get the outcomes we’re trying for.” In other words, more expensive energy subsidized by you and me, and the government rigging the market. That will create jobs!



What is missing here (and everywhere else) is a comprehensive analysis of how much money the organo fads, follies and delusions cost us. Hopefully that will be in Myers’ next book. The incredible constellation of policy errors, wrongheaded logic and downright stupidity has to be extracting a dear cost from our very sick economy. It’s time to stop this. It’s time for you to read this book.
"
