"The supermarket chain Iceland has been denied clearance to screen its Christmas advert on British television. Consisting mainly of Greenpeace’s short “Rang-tan” animation, the ad highlights Iceland’s commitment to eliminate palm oil from its own-brand products. According to the advertising clearing body, Clearcast, it was disallowed not because of its content, but because of its connection with Greenpeace, a “body whose objects are wholly or mainly of a political nature”. Iceland reacted swiftly, tweeting that its ad had been “banned” from television because it was “seen to be in support of a political issue”. The tweet was picked up by mainstream media such as the Guardian, which ran the headline: “Iceland’s Christmas advert banned for being too political.” Furious responses followed, with nearly 100,000 people sharing Iceland’s original tweet and over 650,000 petitioning Clearcast to reverse its decision. Most of these responses revolve around the (inaccurate but powerful) claim that Iceland’s ad was “banned” for being “political”. How, ask critics, could highlighting the destruction of the rainforest be political? How could saving orangutans be anything but worthwhile? As one tweet put it, “since when is outrage about losing such beautiful animals political?” Such responses portray environmental and conservation causes as above politics: as built on unquestionable, universal truths. As such, they are too important to be used for petty point-scoring. In this view, Iceland’s ad reveals a devastating apolitical reality that the world needs to see and respond to.  But show the same footage to rural communities on the islands of Borneo and Sumatra, where most palm oil is produced, and we may well get a different response. Many would see the ad’s message as entirely political, for several reasons. The forests that the ad exhorts viewers to save are also these people’s homes: places filled with specific histories, social relations, assets and other living beings. But the relationships that forest dwellers have to these places are not always understood or recognised by the state or conservation bodies. Wildlife protection laws and the expansion of protected areas – often driven by conservation initiatives – have complicated the situation. These have turned access to forests and their resources into highly political issues. Many rural villagers across Indonesia and Malaysia also rely on small-scale cultivation of oil palm (the tree which produces palm oil) for their livelihoods. Some smallholders work for or in partnership with oil palm companies, and others operate independently. While participation in the industry has generated its own problems, it has also generated income and infrastructure in rural areas. Such smallholders will be rightly concerned about the damaging effects of attacks on palm oil on their futures, and their access to necessities like food and medicine. The Iceland ad is also a classic example of how life is unequally valued across the global political terrain. In certain parts of Borneo and Sumatra, where my team and I are currently conducting research, many forest residents see orangutans as dangerous and not particularly special creatures that can damage their crops and livelihoods. Yet they are acutely aware that many well-meaning foreigners would privilege the well-being of orangutans over their own. Why, they ask, do governments, NGOs and tourists put so much time and money into saving this one animal when people like us are struggling to get by? 


      Read more:
      Palm oil boycott could actually increase deforestation – sustainable products are the solution


 Such concerns reveal a mismatch between Iceland’s conservation message and the experiences of rural people in Borneo and Sumatra. In Greenpeace’s film, humans are either intruders (represented by bulldozers) in the pristine rainforest home of the orangutan, or the “good guys” (represented by the girl) who are going to save them. This vision is built on a historically Western understanding of the world that treats “humans” and “nature” as fundamentally separate. But what it blots out are the people who live in and around the same forests, for whom such a separation is much harder – and by no means apolitical. The idea that environmental and conservation causes are above politics thus makes sense only from a particular Western perspective – one built around an image of the forest-as-wilderness that is not universally shared. The depiction of such causes as apolitical has facilitated their spread across the world, while shielding them from scrutiny and critique. Yet scrutiny and critique can reveal significant problems and oversimplifications in Iceland’s ad. The “palm oil kills orangutans” narrative, for example, sidesteps the fact that deforestation is only one of several factors driving orangutan extinction. Other key drivers include hunting and poaching, though these won’t be solved by oil palm activism. And as various analysts have pointed out, simply boycotting palm oil could ultimately backfire. A collapse in global demand would disproportionately affect smallholders, generating further poverty and resentment. It could also encourage the cultivation of other ecologically-damaging crops such as soy or rapeseed, which would displace rather than reduce forest conversion and biodiversity loss – an unfortunate geopolitical outcome. None of this mitigates the need to address the problems of environmental destruction and extinction. And it’s no bad thing that Iceland’s ad has helped raise awareness of these issues. But conservation isn’t a black-and-white morality tale, and depicting the advert’s message as apolitical is both misleading and counterproductive. For the sake of both orangutans and the people who share their forests, we need fewer emotive simplifications and more acknowledgement of the complex political realities at stake."
"Scott Morrison has struck a $2bn deal with the New South Wales government to increase gas supply and reduce greenhouse gas emissions from the electricity sector. The deal includes at least $450m of federal grants and $510m more of federal grants or loans for “NSW-based emissions reduction initiatives”, to be matched by $1.01bn in direct funding from Gladys Berejiklian’s government.  The deal is the first of a series of bilateral energy agreements between the federal government and its state and territory counterparts. Morrison told reporters in Sydney the $2bn could be spent on “clean technology” including hydrogen research, energy efficiency measures, and “coal innovation to commercialise and employ technologies to reduce emissions from extraction, preparation and the use of coal”. Berejiklian said the plan should reassure citizens “we have a clear plan … [to] reduce our emissions, which we know many people feel strongly about”. Under the plan, the federal and NSW governments will jointly underwrite the delivery of HumeLink and the Queensland-NSW interconnectors to strengthen grid reliability. The NSW government has committed to facilitating investment opportunities to inject an additional 70 petajoules of gas a year into the east coast market and to remove barriers to coal supply to the Mount Piper power station, which is facing an acute shortage. Berejiklian told reporters the Narrabri gas project – to drill 900 coal seam gas wells, including within the Pilliga state forest – “may very well be” the source of extra gas and “will meet” the requirement, although she noted the project is still subject to final approval. She said NSW had three options, including Narrabri, and import terminals at Newcastle and Port Kembla to import more gas. To supply Mount Piper, Berejiklian suggested that “other [coal] power stations will be coming to the end of life in the next little while, so then there are opportunities for us to transfer those [supply] arrangements to Mount Piper”. The federal government’s $1bn electricity generation underwriting scheme will be used to support as-yet unspecified new generation projects in NSW. The shortlist of 12 projects for that scheme includes five gas projects, two of which have  been approved, six pumped hydro schemes, and the coal baron Trevor St Baker’s proposal for a coal upgrade at Vales Point at Lake Macquarie. The deal also promises financial support for the establishment of a pilot renewable energy zone in the state’s central west to help large-scale renewable generators. “I want households and businesses paying less for their electricity and I want to continue to get emissions down – this deal does both,” Morrison said. “There is no credible plan to lower emissions and keep electricity price down that does not involve the greater use of gas as an important transition fuel.” Berejiklian said the state “already has a pipeline of around $26bn of large-scale renewable and non-renewable energy projects and the NSW government has introduced a range of rebates to help keep prices down as well as a five-year $1.4bn climate change fund.” “Our agreement with the commonwealth today will ensure we continue to strengthen and diversify our energy sector here in New South Wales – securing traditional energy sources whilst growing renewable energy investment across the state,” she said. Relations between the federal and NSW governments have been strained after the NSW environment minister, Matt Kean, linked the bushfires to Australia’s poor record on combating climate change, then suggested that Liberals in federal cabinet were pushing for more action. This earned a rebuke from the prime minister, who said Kean “doesn’t know what he’s talking about” with respect to federal cabinet. Morrison insisted that Australia was “carrying its load” in the global emissions reduction fight. He has refused to increase Australia’s targets but has opened the door to dropping the use of Kyoto carryover credits to achieve 2030 targets if it is possible to do so without increasing electricity prices."
"




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea69672d7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"All over the world, countless conservation projects are taking place, attempting to achieve aims from reducing habitat loss, to restoring populations of threatened species. However there is growing awareness that conservationists have not always done a good enough job at evaluating whether the things they do really work.  Efforts that fail to make things better for species and ecosystems waste the limited resources available for conservation, and result in missed opportunities to stem the loss of biodiversity. Given that monitored populations of wildlife species have declined by 60% in the last 50 years, and large scale loss of forest continues, this is bad news. So, research to show whether conservation efforts work really matters. And those doing conservation need easy access to the results of this vital evidence. In many fields, when researchers want to know whether something works they conduct an experiment. For example, patients are often randomly assigned to receive a new drug (or not) and the results are compared to determine if the new treatment has the potential to help people. Despite calls for more use of experiments in conservation, they remain extremely rare. One common approach to conservation is encouraging owners to manage their land in a way which provides benefits for the environment. This has been done in the UK for decades. For example, farmers are paid to maintain hedgerows and leave stubble on fields to help farmland birds. These kinds of payments for ecosystem services schemes are increasingly used in the tropics as well.  In 2017, an experiment in Uganda revealed that paying farmers not to chop down trees was a cost effective way to slow deforestation. Now we have published the results of only the second experiment at such a scale. Our study evaluates whether providing incentives to farmers to protect forest and keep cattle out of streams improves water quality.  The research focuses on the efforts of the Bolivian NGO Natura, which has been working with communities in the Andes to help protect the area’s incredible forests. These are home to spectacled bears and other wonderful wildlife, and are seen locally as important for supplying clean water. In Natura’s Watershared programme, upstream landowners were offered incentives to shift their livelihood activities away from clearing forest or letting cattle graze untended in the forest. Natura wanted to know if their innovative approach to conservation was working, so they took the unusual step of setting up an experiment to find out.  In 2010, 129 communities were randomly placed in a control group, or given the chance to enrol their land in Watershared agreements. Households in the latter “treatment communities” could then choose to enrol as much of their land as they wished in the programme. Analysing the results of this experiment, we found that while keeping cattle out of rivers is (perhaps unsurprisingly) good for water quality at the location where it happens, the treatment communities did not have cleaner water in their taps. Further investigation revealed that this was at least in part because of the low level of uptake of the programme, and that the land most likely to be important for improving water quality was often not enrolled. Natura is already implementing the results of this research to improve the design of Watershared. They are working with communities to ensure that protection is targeted to areas most likely to benefit water quality. And our experience with running such a large-scale experiment holds useful lessons for others interested in increasing knowledge about what works in conservation. Away from conservation, there has been an explosion in the use of randomised experiments to evaluate the impact of other large scale interventions – in development and education, for example. However, there has been backlash from opponents who have pointed out, among other things, that these kinds of investigations will not always provide valid answers to the most important questions because these experiments can only normally answer the question “does it work?”, rather than “why does it work?”, and so can’t really answer the other key question, “will it work in other situations?”. This debate has got quite heated, and even acrimonious, at times. Running an experiment to evaluate the impact of a large-scale conservation intervention is certainly very challenging. It is often not possible to randomise which areas receive a new conservation project (can you imagine a government randomly allocating where it puts national parks?). There are also issues with achieving adequate replication, and there can be ethical concerns which prevent experimentation. However, given the importance of knowing what works in conservation, more high quality evaluations (which won’t always be experiments) are certainly needed. Only by learning from current practice can the future effectiveness of conservation be improved."
"Over the past two decades automated wildlife cameras – known as camera traps – have proven invaluable in ecological research and conservation management. Their sensitive motion detectors have enabled scientific surveys of rare or shy animals in dense forest and as a consequence have seen broader use around the world.  But camera traps frequently take pictures of people as well as wildlife. This has important implications for privacy and human rights and may ultimately undermine conservation goals. We conducted a survey of researchers who had deployed camera traps in ecological or conservation projects. More than 90% of the 235 respondents said that their cameras had taken images of people as well as wildlife. Fewer than 9% of researchers who had captured images of people had initially set out to do so. But most said that once they had the pictures they made use of them. For example, almost half of respondents who had pictures of apparently illegal activities (such as poaching) subsequently used them to inform conservation management or law enforcement, sometimes by sharing them with third parties (most notably the police and park management staff). Initially, for that reason, the ability of camera traps to monitor human as well as wildlife activity in areas of conservation importance may look like a double win for conservation. But the fact that these cameras often take pictures of people can be highly problematic for two main reasons. Firstly, many respondents said that they had captured images that either they considered to be private (for example of people going to the toilet), or which showed a person trying to avoid the camera (for example, images of people covering their faces). In some countries, distributing images of people without their consent can result in substantial penalties. Even where this is not against the law, steps should be taken to ensure that camera traps do not infringe reasonable rights to privacy.  Second, even if images of people are not used or shared, camera traps can still generate fear and anger – and this can lead to local opposition to camera trapping. With the long-term success of most conservation projects depending on the support of local people, it is vital that their issues with camera traps are taken seriously.  Three-quarters of the researchers we surveyed reported local objections to cameras, either in the form of complaints or direct interference such as damage or theft. Sometimes people went to great lengths to interfere with the cameras. For example, one respondent said that they once found a patch of burnt ground, only one square metre in area, exactly where their camera used to be. When we asked what researchers thought had caused these cases of interference and objections, the most common answer was “fear or concern about what might happen with the images”. This fear can take extreme forms. For instance, one of our respondents in South America said that they had found a woman next to a camera trap who thought it was being used to take pictures of her children with a view to kidnapping them.  Clearly, conservation projects whose cameras are damaged or stolen are going to face the cost of replacing equipment and often also lose data. For these reasons alone, it makes sense to avoid angering or scaring local people. More broadly, the antagonistic relations with local people caused by camera traps can give conservation projects a bad name and even promote damaging activities: several researchers reported that those who objected to the cameras retaliated by killing wildlife. Despite the frequency of these issues, they are almost never discussed in conservation or ecological scientific studies. And although most of our respondents recognised the potential problems with the use of pictures of people, the dilemma of how to best to handle them is not being publicly debated. Luckily, researchers themselves are thinking about this problem. Protocols established by our respondents to manage pictures of people caught on camera included blurring images, or not publicly sharing them. This blurring could even be done automatically, using machine algorithms, so that images containing people are blurred before they are seen by human eyes. Some individual researchers have gone further and have substantially involved local communities in their projects – reassuring them that the cameras were not there for law enforcement purposes, involving them in the process of deploying the cameras and sharing the images with them. But, despite these independent efforts, to date there are no standard guidelines agreed by conservationists.  The implications of camera trap technology for people’s privacy and well-being need to be more widely and openly discussed, and good practice shared. Conservation projects need to make sure they have proper protocols in place to minimise social impacts and stop useful wildlife research tools from damaging both the short and long-term success of wildlife conservation projects."
"
Today I visited my friend Jim Goodridge, former California State Climatologist and the man with a garage full of data going back to before the Gold Rush.
He’s been quietly toiling away in his retirement on his computer for the last 15 years or so making all sort of data comparisons. He gave me two CD ROMS full of data that I’m just now wading through. One plot which he shared with me today is a 104 year plot map of California showing station trends after painstakingly hand entering data into an Excel spreadsheet and plotting slopes of the data to produce trend dots.
He used every good continuous piece of data he could get his hands on, no adjusted data like the climate miodelers use, only raw from Coopertive Observing Stations, CDF stations, Weather Service Offices’s and Municipal stations.
The results are quite interesting. Here it is:

Squint hard and you can see a pattern emerge.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea610a013',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Take a 12 hour ferry north from mainland Scotland, and you’ll reach the Shetland Isles – the northernmost place in the UK. The only part of Britain to be considered “subarctic”, the archipelago of about 100 islands is found at the same latitude as the southern tip of Greenland. It’s so far north that, on most maps of the UK or Scotland, Shetland is cut out and placed in an inserted box somewhere off the coast of Aberdeenshire or the Highlands. Yet this aspect of cartographic design has long been controversial among the 23,000 people who live on the islands. This was a point made recently by the their representative in the Scottish parliament, Tavish Scott MSP, who has passed an amendment to the Islands (Scotland) Bill which now prevents public bodies in Scotland from including the Shetland Islands in an “inset map” (in a box).  The Ordnance Survey, Britain’s national mapping agency, was against the move as it would imply “publishing maps that are mostly sea”, while we must now wait and see how this actually impacts maps produced by Scotland’s various public bodies, which do have a get-out clause if they use an inset, but explain their reasons. Nonetheless, the idea behind the law change is certainly laudable. With Shetland in a box off the coast, it is easy to forget how much of a logistical and financial challenge it is to travel from the island to the mainland. It is a full 125 miles from the northern coast of the mainland to Shetland – about the same distance going from London to Nottingham. Placing the islands in their true geographical location should remind people of this reality. However, Scott has perhaps forgotten why the islands were put in a box in the first place, and this reason hasn’t changed in the hundreds of years that people have been making maps of Scotland. Due to their geographical isolation, including them in a box allows the rest of Scotland to be shown in much more detail. If the Shetland Isles were mapped in their true location, we would end up with a map that is largely sea, with the rest of the Scottish mainland around 40% smaller. When creating any map, whoever is making it has to decide what to include and what not to include. If you had a map of the town or city where you live, would you include the internal layout of your home? Obviously not – marking out the distance you need to travel from your sofa to your kitchen to make a cup of tea would be both impractical and irrelevant for almost all users of the map. So what to include in a map depends on what the map is for. The London Underground map is a great example of one that only includes what it needs to: it lists the stations, the tube lines and is very clear on which lines connect where. (It also includes the River Thames, which is arguably not vital, but helps people orientate themselves and so was reinstated after it was removed in 2009). As a map to work out where to go in Central London on foot it is no help at all, but that’s not the point.  Hexagon-based maps used in parliamentary elections are another iconic example. Most viewers are not interested in the geographic area of each constituency, but do want to easily assess the total number of seats and which party has a majority. Decisions made at the design stage can alter how a map is interpreted, sometimes quite significantly. We can automate some of the process using Geographic Information Science (GIS) computer programs, but we have to use these tools carefully, with the end product in mind. Insets are just one element of a map that can have an impact on the message it conveys. The Worldmapper website uses cartograms to adjust maps to show variables other than space geographically, and Vox has a great video explaining different projection systems used to display the globe. And the Shetland Isles? Some maps of Scotland should include the islands in their true geographic location in order to highlight their remoteness and logistical issues, as Scott says. However, not all maps need to: sometimes this is not relevant to the map, and it is more important to give more prominence to the central belt of Edinburgh and Glasgow where many more people live.  It all depends on what the aim of the map is. The choices made by the map maker have a big impact on the output of the process and how the map is viewed. It is impossible to represent everything on a map, and sometimes what we decide to leave out is much more important than what we decide to include."
"

Suppose a commenter posts a libelous comment here at NorCalBlogs. It’s been known to happen. Can the blogger, Enterprise Record, and its corporate owners be sued for defamation? A federal appeals court just held that no, they cannot. The court noted that a federal law was designed to ensure that ‘within broad limits’, message board operators would not be held responsible for the postings made by others on that board,’ adding that, were the law otherwise, it would have an ‘obvious chilling effect’ on blogger free speech.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8011936',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Anybody who wonders what we are fighting for in the middle east should read the article below in today’s Boston Globe. In case you didn’t know, there are places where you could go to jail for just reading this.

Iran bloggers test regime’s tolerance
Push boundaries of political dissent
By James F. Smith and Anne Barnard, Globe Staff  |  December 18, 2006
TEHRAN — By day, Alireza Samiei covers banking and insurance for an industry newspaper. By night, he writes a daring online blog about Iran’s social and political ills.
In a recent blog entry, he described a scene he saw while talking to a greengrocer about soaring prices: A young child was pleading, ” ‘Mommy, I want watermelon.’ The woman, shy and sorrowful, singled out one broken, small watermelon from the spoiled fruit bin and told the grocer, ‘Just this one, please.’ She put 20 cents on the counter and hurried away.”
Samiei, 27, is among the growing ranks of Iranian bloggers who are relentlessly pushing the boundaries of free expression, making Farsi one of the 10 most popular languages for blogs. The bloggers are testing just how much political and social dissent the nation’s rulers will tolerate on the Internet.
The authorities are pushing back. They have blocked access to thousands of websites in recent years that are deemed to threaten Iran’s Islamic revolution, including the BBC’s Farsi-language site. A trial began this month against four bloggers on charges including propaganda against the state. And in October, the government barred high-speed Internet service in private homes.
Especially threatening, it appears, are sites that create online communities that might allow Iranians to assemble virtually. The government banned the hugely popular Orkut site, an online Iranian social club. The latest casualty this month: YouTube.com, the American site for sharing videos online. Click on it in Iran and the screen reports, “Access denied.”
The Paris-based rights group Reporters Without Borders includes Iran on its list of 13 countries designated “enemies of the Internet.” That organization’s website is also blocked in Iran.
The organization said repression of bloggers has eased somewhat in 2006. But in a report in November, the group said Internet filtering has accelerated, with two political sites, tik.ir and meydaan.com, closed down in recent weeks. Both had criticized the government of President Mahmoud Ahmadinejad.
Bloggers agree that they have found some latitude in recent months. Many have developed a feel for the boundaries, and some are trying to stretch them rather than break them.
Farzana Sayid Saidi, a 29-year-old reporter and colleague of Samiei, has two blogs, one political and the other showcasing her poetry. She has been blogging in her spare time for two years. Her first blog was shut down within three days, she said, after she wrote that school officials were providing access to abortions in clinics for young students.
Now she’s back at it. She blogged a few days ago that while Ahmadinejad wants people to have more children, his economic policies make it difficult for many families to do so. She said she received some obscene and abusive replies. In a poem on her other blog, she compared Iran’s leaders to the pharaohs of Egypt.
She estimates her readers at only in the hundreds, but adds: “We just want to express ourselves. We don’t know how many people are reading.”
Farzana’s colleague Samiei admires her courage.
“Of the things she writes in her blog, only 1 percent would be acceptable in print,” Samiei said.
She and hordes of other Iranian bloggers are pushing the envelope of the permissible. Technorati, a Silicon Valley search engine for blogs, said in October that Farsi has moved into the top 10 languages worldwide for bloggers. Most estimates put the number of active blogs in Iran at 70,000 to 100,000, and growing fast.
Iran has a long tradition of controlling the airwaves and the print media, banning papers and jailing journalists who criticize official policies.
But Iran’s online activists have proved harder to quash. They have used fast-changing Web addresses, proxy sites, and other technological tricks to get around the restrictions.
“They block us and we evade the blocks,” Samiei said. “It goes on every day. They code, we decode.”
The Ministry of Information periodically sends lists to Internet service providers saying which keywords to filter out so that users can’t get access to websites or blogs that contain them. The government contends that the principal target is pornography and other morally offensive material. The word “sex” is among those blocked.
That has some odd consequences. At one point, an Internet café owner said, the word “hot” was blocked. And that briefly prevented access to Hotmail, the popular e-mail program.
Amirhussein Jaharuti, the manager of a major Internet service provider in Tehran, said the government’s restrictions focus on pornography, and he feels that filtering is appropriate.
“This is the demand of Iranian families, that they don’t want their children to use these kinds of sites,” he said. Asked about the political restrictions, he said: “All governments have ways to control their societies. . . . It’s natural that when we see that someone wants to destroy us, we limit them.”
Jaharuti said his client base has doubled in the past two years, to nearly 70,000. He provides dial-up and digital-subscriber line service to home and business customers at a cost of 20 to 40 cents an hour, or about $20 per month.
Internet use in Iran has exploded in recent years, with about 7.5 million users in 2005 in a country of nearly 70 million people.
Some journalists say the Internet has become even more vital in Iran as the government has suppressed other, more easily controllable forms of expression. Several opposition newspapers have been shut down since September, including the prominent paper Shargh.
The editor, Mohamed Atrianfar, said in an interview that the closure of Shargh and other publications and renewed pressure on critical websites reflects the government’s concern that “the more challenges we have, the more agile and fresh the society becomes.”
“All the hard-liners have mustered all their strength to fight this war. I am proud that we have invoked this reaction in them,” he added.
Despite its closure, Shargh has maintained a website to continue coverage of elections last week.
Atrianfar estimated that about 70 to 80 Iranian journalists have their own blogs.
“Websites and blogs have real impact,” he said. “They have been very powerful in forming a word-of-mouth culture, especially for those between 17 and 35.”
An Internet café owner in central Tehran who gave only his first name, Shariar, said the filtering of keywords rather than individual sites often blocks legitimate websites that people need for academic research. He also said limits on credit cards resulting from US financial sanctions against Iran have all but eliminated e-commerce on Iran’s Internet, a major obstacle to economic growth.
Shariar said that while the government contends it is aiming its restrictions at pornography, “I think they are worried about politics. . . . I think they fear everything. They don’t want people to make connections overseas. They are worried about information.”
The authorities also close Western media sites temporarily. Both The New York Times and Los Angeles Times sites were blocked briefly this month.
A 22-year-old university student, Morteza Yeganeh, said the state-owned broadcasters and newspapers “brainwash people, so we need to find ways to educate ourselves.”
But he said the filtering of sites is effective because “even though people can get around the filters, it is difficult and time-consuming and people give up.”
Most Iranian blogs are apolitical, and government members — including Ahmadinejad himself — have their own blogs to convey their views. But those with blogs that challenge the government know they are taking a risk.
Niloufar Taslim, 24, said that three years ago, she was one of Iran’s first bloggers, writing about social and political problems. But she started receiving e-mails signed by a group calling itself the Army of God, listing her name, telephone number, and address and threatening to kill her.
She shut down that site, but now has two new blogs. One talks about social problems without crossing what she also considers political red lines: transportation and environmental problems.
Another blog features her poems. One laments that she has lost her voice, that in “a situation without possibilities” her hands are “in pain because they cannot write.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9745970',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Here we go again. The “sceptical environmentalist”, Bjorn Lomborg, has returned to warn against the excesses of an impending green dictatorship. The latest threat: taking away our burgers! Yes, you’ve heard correctly. According to Lomborg it could even go as far as the “UN dictating what people eat”. A well-known provocateur who now runs a think-tank in his native Denmark, Lomborg first made his name in the early 2000s with a series of outspoken but attention-grabbing attacks on mainstream environmentalism. He argued that the climate is changing but less dramatically than most suggest, and that there is no urgent need for action. He also claimed that forests aren’t disappearing and that species extinction has been wildly exaggerated.  Lomborg walks the line between sensible liberal thinking and outright denialism by cherry-picking or misrepresenting statistics. Though widely criticised by most scientists, Lomborg retains a large following today.  This is why his typically contrarian take on climate change and food attracts so much attention, and why it is worth responding to. Lomborg, a vegetarian for animal welfare reasons, explains that: “Almost all articles on this topic suggest going vegetarian could achieve emission cuts of 50% or more.” But apparently none of them have taken the time to “dig deeper”. As researchers who work in environmental impact analysis, we are acutely aware of the limitations of “food footprinting” studies and the danger of taking figures at face value. So, let’s dig deeper into his claims. Take the “systematic peer review” he cites which found that going vegetarian cuts personal emissions by around 5% rather than 50%. He’s correct that the cuts aren’t close to halving a person’s overall emissions, but there is good reason to believe it is double what Lomborg claims. Only two studies in the review he uses look at the major effect of meat consumption on emissions from deforestation, even though millions of hectares of forest are cleared each year to satisfy the world’s appetite for beef. As forests act as a carbon sink, while beef farms emit lots of greenhouse gases, this has a huge impact on net emissions. Meat consumption is incompatible with limiting deforestation and encroachment into natural land. Hence, we must take into account “deforestation emissions” when tallying the environmental burden of eating meat. Taking the more realistic figure from these studies we arrive at a 10% cut in personal emissions from going vegetarian. To put this into context: a shift to vegetarianism in the UK would be the equivalent of taking 8m (or one in four) cars off the country’s roads. The impacts of veganism would be greater still. In short, the impact of individual actions really does add up. What about Lomborg’s second claim that vegetarians take the money they save from “eating carrots instead of steak” and spend it on other things which have their own environmental impact, offsetting part of the benefits of giving up meat? We dug deeper and found that the paper he cites relies on data from 2006 and also does not factor in emissions from changing land use, linked to deforestation. The paper is a microeconomic analysis of what consumers in Sweden, specifically, might spend their extra cash on if they went vegetarian. Its author warns that her work must be “interpreted within a relatively narrow topical and temporal scope”, and that unrealistic market assumptions concerning fixed supply, demand, and pricing could lead to completely different conclusions when relaxed. Nevertheless, Lomborg does extrapolate the paper’s findings, against its author’s own suggestion, to trivialise the impact of vegetarianism on emissions across the industrialised world. Clearly, this is not just about the environment. It’s about our ability to choose. Lomborg prioritises the right to eat meat over our collective responsibility not to. Many of the world’s poorest are involuntary vegetarians, he argues. Our duty, he implies, is to support their “right to meat”. However, poorer countries stand to benefit from widescale adoption of a plant-based diet. Mortality linked to strokes, heart disease, diabetes, and cancer could fall by 5m to 6m avoided deaths and trillions of dollars could be saved in healthcare costs and by preventing productivity losses.  Moreover, producing meat is terribly inefficient as animals consume far more food than they yield. If we grew crops for human consumption, instead of animal feed, we could increase available food calories by as much as 70%, which could feed an additional 4 billion people, ending global hunger and reducing emissions, one carrot at a time. Lomborg summarises his argument: “Climate change is both trivialised and hampered by unrealistic senses of magnitude, and by silly suggestions that your or my actions can transform the planet.” To suggest you and I can do nothing to help prevent climate change is surely defeatist. This climate defeatism is the new climate denial. Lomborg offers techno-fixes where effective measures already exist. Although he knows consumers will fry the planet before they do lab-grown burgers, prescribing artificial meat helps kick the can further down the road. Although we need systemic change, the climate is also in our hands. Perhaps the only meaningful contribution Lomborg makes to avoiding climate breakdown is choosing carrots over steak."
"
Share this...FacebookTwitterDonations now welcome!
Before we jump into the data, readers may have noticed that NoTricksZone is now accepting donations – after close to 10 years of not doing so. Hosting services and the hourly rates specialists charge for troubleshooting are painfully expensive, and so any support, no matter how small, is very much appreciated.


		jQuery(document).ready(function(){
			jQuery('#dd_a992b20773ff8dfb5303892e42d2859f').on('change', function() {
			  jQuery('#amount_a992b20773ff8dfb5303892e42d2859f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
I’ve gotten some donations since I’ve installed the donation button, and so it’s very encouraging. Thanks to those who have done so.
And no, I’m not nor have I ever been funded by Big Oil, The Heartland Institute, Big Coal, the Koch Brothers or anything of the sort. Everything has been out of my own pocket. Thanks, Pierre
Falling Canada mean February temperatures
By Kirye
and Pierre Gosselin
Today we look at the data from the Japan Meteorological Agency (JMA) mean temperature for Canada. Examined are the 9 stations that have almost complete data going back to 1983.
Nine of the 9 stations show February mean temperatures have had a cooling trend since 1996: 



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: JMA 
Annual Canada temperatures steady since 1994
Looking at the JMA data for mean annual temperatures for Canada, we see that no significant warming has taken place at the 9 stations since 1994!

Data: JMA.
Yes, the climate is changing, but nothing unusual is happening. The changes are well within the natural range of variability and so cannot be wholly attributed to man’s activities.
In fact, using the data and results that are available, one can easily argue that the recent warming is related to ocean and solar cycles. There’s nothing to be alarmed about.
Worry about Corona and political threats, and not climate.
Share this...FacebookTwitter "
"

 _Here we introduce a new feature from the Center for the Study of Science, “On the Bright Side.”_ _OBS will highlight the beneficial impacts of human activities on the state of our world, including improvements to human health and welfare, as well as the natural environment. Our emphasis will typically focus on the oft-neglected positive externalities of carbon dioxide emissions and associated climate change. Far too often, the media, environmental organizations, governmental panels and policymakers concentrate their efforts on the putative negative impacts of potential CO 2-induced global warming. We hope to counter that pessimism with a heavy dose of positive reporting on the considerable good humans are doing for themselves and for the planet._   
  
_**—**_   
  
According to Piao _et al_. (2015), the reliable detection and attribution of changes in vegetation growth are essential prerequisites for “the development of successful strategies for the sustainable management of ecosystems.” And indeed they are, especially in today’s world in which so many scientists and policy makers are concerned with what to do (or not do) about the potential impacts of CO2-induced climate change. However, detecting vegetative change, let alone determining its cause, can be an extraordinarily difficult task to accomplish. Nevertheless, that is exactly what Piao _et al_. set out to do in their recent study.   
  
More specifically, the team of sixteen Chinese, Australian and American researchers set out to investigate trends in vegetational change across China over the past three decades (1982-2009), quantifying the contributions from different factors including (1) climate change, (2) rising atmospheric CO2 concentrations, (3) nitrogen deposition and (4) afforestation. To do so, they used three different satellite-derived Leaf Area Index (LAI) datasets (GLOBMAP, GLASS, and GIMMIS) to detect spatial and temporal changes in vegetation during the growing season (GS, defined as April to October), and five process-based ecosystem models (CABLE, CLM4, ORCHIDEE, LPJ and VEGAS) to determine the _attribution_.   




With respect to _detection_ , this work revealed that most regions of China experienced a greening trend indicative of enhanced growth across the time period studied (see Figure 1). Overall, 56 percent of the area studied experienced a significant increase in greening (95% level) when using the GLOBMAP dataset, compared with 54 and 31 percent using the GLASS and GIMMIS datasets. Those regions with the largest greening trends include southwest China and part of the North China Plain.   
  
_  


![Figure 1. Spatial distribution of the trend in LAIGS over the period 1982–2009 as calculated by the GIMMS dataset \(a\), GLOBMAP dataset \(b\) and the GLASS dataset \(c\). The frequency distribution of the significance level \(P value\) of the trends calculated for the three LAIGS datasets is shown in panel d.](/sites/cato.org/files/styles/pubs/public/wp-content/uploads/chinaidso.jpg?itok=fhC2To8Y)

_



  




**_Figure 1._** _Spatial distribution of the trend in LAI GS over the period 1982–2009 as calculated by the GIMMS dataset (a), GLOBMAP dataset (b) and the GLASS dataset (c). The frequency distribution of the significance level (P value) of the trends calculated for the three LAIGS datasets is shown in panel d._



With respect to _attribution_ , Piao _et al_. report that “the combined effect of CO2 fertilization and climate change with the effect of nitrogen deposition, leads to the conclusion that these three factors are responsible for almost all of the average increasing trend of LAIGS observed from the satellites” (see Figure 2). They also report that “at the country scale, the average trend of LAIGS attributed to rising CO2 concentration is estimated to be ... about 85% of the average LAIGS trend estimated by satellite datasets,” while noting secondarily that the enhanced nitrogen deposition driven by _fossil fuel combustion_ and _agricultural fertilization_ is likely the source of the remaining portion of China’s enhanced vegetation growth, citing the findings of Reay _et al_. (2008), Thomas _et al_. (2009), Fleischer _et al_. (2013) and Yu _et al_. (2014).   






**_Figure 2._** _Trend in China’s LAI GS over the period 1982–2009 at the country scale for the three satellite remote sensing datasets and five process models described in the text above. Significance levels of 95 and 99 percent are denoted with one and two asterisks, respectively. See the authors' original text (Piao et al., 2015) for additional explanation of this figure._



In considering the researchers' several findings, it is clear that the fossil fuel combustion that has resulted in the rise in atmospheric CO2 and enhanced nitrogen deposition over the past three decades has provided a great benefit to Chinese vegetation. As illustrated in Figure 2, led primarily by the increase in CO2, that benefit has been _more than sufficient_ to compensate for the negative effects of climate change that also occurred over that time period. Thus, it would seem far more prudent to _celebrate_ CO2 instead of _demonizing_ it, like so many people incorrectly do these days; for atmospheric CO2 is truly the _elixir of life!_   
  
  
  
**References**   
  
Fleischer, K., Rebel, K.T., Molen, M.K., Erisman, J.W., Wassen, M.J., van Loon, E.E., Montagnani, L., Gough, C.M., Herbst, M., Janssens, I.A., Gianelle, D. and Dolman, A.J. 2013. The contribution of nitrogen deposition to the photosynthetic capacity of forests. _Global Biogeochemical Cycle_ s **27** : 187-199.   
  
Piao, S, Yin, G., Tan, J., Cheng, L., Huang, M., Li, Y., Liu, R., Mao, J., Myneni, R.B., Peng, S., Poulter, B., Shi, X., Xiao, Z., Zeng, N., Zeng, Z. and Wang, Y. 2015. Detection and attribution of vegetation greening trend in China over the last 30 years. _Global Change Biology_ **21** : 1601-1609.   
  
Reay, D.S., Dentener, F., Smith, P., Grace, J. and Feely, R.A. 2008. Global nitrogen deposition and carbon sinks. _Nature Geoscience_ **1** : 430-437.   
  
Thomas, R.Q., Canham, C.D., Weathers, K.C. and Goodale, C.L. 2009. Increased tree carbon storage in response to nitrogen deposition in the U.S. _Nature Geoscience_ **3** : 13-17.   
  
Yu, G., Chen, Z., Piao, S., Peng, C., Ciais, P., Wang, Q., Li, X., and Zhu, X. 2014. High carbon dioxide uptake by subtropical forest ecosystems in the East Asian monsoon region. _Proceedings of the National Academy of Sciences USA_ **111** : 4910-4915.


"
"
Share this...FacebookTwitterIn Berlin today, some 200 scientists shivered at a scientists4future demonstration in front of the Chancellor’s office in order to protest government inaction on combatting global warming.
German activist scientists silently demonstrated before the Chancellor Merkel’s office to protest inaction over warming. Cropped image: ScientistsForFuture.
One prominent attendee was FridaysForFuture activist Prof. Volker Quaschning, who proudly took the day off from lecturing on taxpayer expense. Here’s what he tweeted just before attending the modest demonstration:

Auf dem Weg zur #scientists4future  Schweigedemonstration heute 12:30 vorm Kanzlerinnenamt. Zur #Klimakrise ist alles gesagt. Handel, liebe Regierung!#FridaysForFuture pic.twitter.com/LNycIksdXa
— Volker Quaschning (@VQuaschning) November 15, 2019

Quaschning, a HTW Berlin professor, is seen above at a bus stop on his way to the demonstration, holding the propaganda temperature stripe chart to protest the German government’s inaction on fighting global warming. Unfortunately the professor looks rather silly all dressed up for winter cold in gloves, knit hat, scarf and coat – to protest warming!
200 scientists shiver to protest warming
The activist Berlin professor wasn’t the only scientist trying to stay warm today while protesting climate warming. Two hundred other scientists also showed up in front of Chancellor Merkel’s office, all bundled up in winter clothes, demanding a stop to the warming and that they be listened to – instead of the working class taxpayers.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Low IQ scientists in Berlin outside freezing, dressed in winter clothing, protesting warming! https://t.co/dOwWfFHoEy
— Pierre L. Gosselin (@NoTricksZone) November 15, 2019

The science-is-settled scientists held up signs declaring, “Everything has been said! Act now!” or: “Decades of climate research: Ignored!”
The question is whether they will be taken seriously by the hundreds of millions of Northern Hemisphere inhabitants who are getting socked by a premature frigid winter this year.
Silent protest
The scientists4future all appeared with their mouths taped shut in order to symbolize a “silent protest”. Or perhaps the tape was to keep their teeth from chattering as they shivered in the bitter November cold.
Of course, the scientists didn’t stick around too long. Reportedly they left for an early start to the weekend – in the warm comfort of their homes.
Today the German government voted to pass measures that among other things will make heating and fuel more expensive for ordinary citizens starting next year.
Share this...FacebookTwitter "
"The oil industry is at risk of a global market shock that could halve the value of fossil fuel investments if governments delay setting policies to tackle the climate crisis, according to new analysis. A report by Carbon Tracker, a financial thinktank, warned that a “handbrake turn” in climate policy could have a “forceful, abrupt, and disorderly” impact on the global oil industry by derailing fossil fuel demand.  The report warned that the longer governments wait to set new regulations to drive climate action the tougher they will need to be to avert the risk of runaway greenhouse gas emissions and dangerous global heating. The report modelled the impact of a swift government crackdown on fossil fuels from 2025. It predicted that the impact could cause global oil prices to collapse, wiping out billions of dollars worth of fossil fuel investments. Andrew Grant, a Carbon Tracker analyst and author of the report, said oil companies “risk being left with stranded assets” by assuming that governments will stop short of “forceful action to limit climate change”. The thinktank urged policymakers to act soon to limit new investment in fossil fuel projects which risk being stranded, and warned oil companies to anticipate a step change in climate action. Under existing forecasts oil demand is expected to grow by 0.6% a year over the next five years, but the report found that a crackdown on greenhouse gas emissions could cause demand to shrink by 2.6% a year between 2025 to 2040. “The loss of value is driven not by the oil industry throwing money away but simply by investing based on signals sent by the oil price,” the report said. Many oil companies may have been “lulled into a false sense of security by industry scenarios” which continue to forecast steadily rising demand, according to Carbon Tracker. The thinktank also warned investors against oil market scenarios published by the International Energy Agency which predict a steady decline in demand rather than a sudden collapse. Oil companies in the US, China and Russia are more exposed to a sudden oil market shock than their European peers due to their slow progress in adopting cleaner energy sources alongside fossil fuels. Within the world’s largest listed oil companies, the so-called “super-majors”, ExxonMobil, ConocoPhillips and Chevron are most exposed to an oil price collapse. Europe’s most vulnerable oil company is BP, according to the report, followed by Norway’s Equinor, Paris-headquartered Total, Italy’s Eni and Anglo-Dutch energy giant Shell. However, Saudi Arabia could prove to be relatively resilient to an oil price crash because its reserves are by far the cheapest to produce in the world."
"The US Federal Trade Commission issued a report on the “internet of things” this week. It announced: Six years ago, for the first time, the number of “things” connected to the internet surpassed the number of people … Experts estimate that, as of this year, there will be 25 billion connected devices, and by 2020, 50 billion. Moments later, UK telecoms regulator Ofcom proclaimed a major initiative to ensure the nation “plays a leading role in developing the internet of things”. So the internet is more than just a topic of work, fun, friends, and family that we routinely mention around the kitchen table; it actually happens there, thanks to such canny innovations as “smart chopsticks”. These new wonders can detect rancid food and hence protect animal-eating diners from fish-oil infections. The technocentric promise of this golden age is that electrical appliances will connect to the internet via subscriber identification modules (SIMs) and radio-frequency identification devices (RFIDs). Technology boosters promise a world where digital wallets will replace cheques and credit cards; personalised electronic adverts will compete with static hoardings; and transport, electricity, power and water systems will provide a continuous real time update of their performance and user status. Firms will offer us advice and services built on analysis of such data. The internet of things is described as a marvel – the moment when wireless becomes limitless. Building sensors save energy; homes are automated beyond even the vision of post-war suburban idylls; transportation is effortlessly streamlined; smartphone applications direct daily life; manufacturing is tied to merchandising which is tied to consumption; and healthcare occurs at a distance from the bodies being cared for. The very idea is proclaimed as an expansion of knowledge, convenience, and hence wellbeing. Cybertarians hail a new age of ethical consumption in which customers know the environmental and labour history and future of the devices and services they purchase and have greater control over their own lives than ever before. We are at the peak of what the Gartner research firm calls a “hype cycle”, when expectations of new technologies rise giddily. This is followed by sober realism and everyday use. The Design Museum has rapturously announced that we are entering “A New Industrial Revolution”. Given the costs as well as the benefits connected to the first one – illness, death, pollution, slavery, and war are all there on the downside – one might think the advent of this miraculous new age would provide opportunities to rethink the absurd lightness of being routinely attributed to the internet. For the past two decades of cybertarianism have been an era of ignorance. We have neglected the dirty, material origins and processes that characterise communications technologies like tablets, phones, and laptops. We have forgotten the real story of Cold War militarism, undersea cables, conflict minerals, slave labour, toxic exposure, and illegal recycling. We got such things (the environment, workers, and legality) badly wrong the first time, and risk repeating the mistake due to the obfuscatory claims for a post-smokestack internet age. Then there are the problems with architecture, security, robustness, interoperability, regulation, and privacy across the internet of things – and doubt even encircles the holy fetish of modernity: economic growth. In 1987, the year he won the pseudo-Nobel prize for economics, Robert Solow identified what has become known as the “Solow Paradox”. He came to this insight while reviewing the tide of futurism that accompanied the Cold War and is now reborn via the internet. Solow doubted the wonders of a service economy, which he thought might produce “a nation of hamburger stands and insurance companies”. The memorable phrase he used was: “You can see the computer age everywhere but in the productivity statistics.” Last year, the National Bureau of Economic Research published findings that buttress Solow’s Paradox, a decade after it was supposedly dispatched to the dustbin of history courtesy of the internet. This research suggests that unemployment hastens productivity growth, not information technology. And of course, the internet of things will see labour displaced onto customers, who will become increasingly responsible for work previously undertaken by full-service utilities providers, for example, through properly-employed experts. And what about the instant purchase and upgrade fetishes that will add massive over-consumption to mass capitalism’s inexorable crises of overproduction? The internet of things will create a mountain of junk. Its electronic detritus will be untold. Suddenly your light switch, toothbrush, trousers, tights, kettle, bedroom toys – and chopsticks – will be rendered useless thanks to software upgrades, otherwise known as built-in obsolescence. And that fish-oil prophylactic – do you need it? Why not cease industrial fishing?"
"

Anyone who knows much about me knows that I’ve been a strong advocate for alternate energy, and that I’ve put my money where my mouth is by putting solar power projects on my home as well as at Little Chico Creek School in my former role as CUSD Trustee.
Now I’m going to push an alternate way to do home or business computing.
A couple of months ago I wrote about the upcoming release of Windows Vista, and how I was disappointed that this new release from Microsoft and all of its Digital Rights Management (DRM) nonsense made the operating system turn your PC into a version of George Orwell’s Big Brother.
A friend of mine, school Trustee Rick Anderson recently dumped his PC and bought a Mac Mini because he said he was tired of all the viruses, spyware, upgrades and the like. I pointed out that if all he needed to do was do email, web browsing, word processing, some digital picture work and maybe some video editing, then the Mac Mini would be a good choice since it comes with all those things right out of the box.
It’s an important point, because what I described is what the majority of non technical people need in a personal computer. So why go through the expense and hassle that has become Windows? That got me to looking at an operating system that I once only thought of to be the domain of the uber Geeks – Linux.
For those who don’t know about Linux, or have the view that you have to be one of those people that stares at a computer screen until 3AM and then falls asleep on a stack of empty pizza boxes, that used to be the case. But Linux has grown up. While there are still a few distributions aka distros out there that cater to that some new ones have emerged recently that are as easy to use as Windows. They even come with applications like word processors, spread sheets, etc and the best part is they are free or low priced. Some work right out of the box, requiring only a simple install.

Since Microsoft had me so ticked off because of the expense and corporate control issues in Vista, I recently started experimenting with a Linux distro called Ubuntu. Now don’t let the weird name scare you, this is actually the most amazing thing I’ve seen in awhile. Ubuntu is as African word meaning “humanity to others” and the company that is distributing it bills the software as “Linux for Human Beings”. The company seems to be setup like a philantrophy, they want to bring computers to people whom can’t afford it. With Vista costing upwards to $500 for ll of its features, they are positioned well to do that.
I recently installed it on a blank PC, and was instantly impressed. Best of all it’s free if you know how to download it and burn it to a DVD. If not, they have lost cost media you can buy.
My installation experience was fast and easy. And I had Ubuntu up and running within 30 minutes. It was just as simple as putting in the DVD, answering a few simple questions about configuration, and off it went. It came up, updated itself automatically wiht the web connection and was ready to run.

It comes with everything a home or small office might need. Word processor and Open Office Application Suite, Firefox web browser, Email, and a bunch of other applications including a cool paint program called “gimp” that rivals Adobe PhotoShop. Ubuntu comes installed with a project management application called Planner. The tool allows users to create simple Gannt charts, tasks, and allocate resources. There’s even a Microsoft PowerPoint clone. And you can read or save Microsoft document formats for all their office applications.

There are hundreds of other free aplications available for download.
Ubuntu recently announced they are offering a free video/graphics/audio editor to make DVD’s and edit video from camcorders and sound/music tracks. You can see a preview of Ubuntu Studio here
If you can run Windows you can just as easily run Ubuntu Linux. It’s fast, relaible and virus/spyware free.
I gotta say that if you just want a simple and reliable student, home, or office PC at minimal cost, one that will actually run effectively even on older inexpensive hardware, Ubuntu is it.
Microsoft is going to lose some of their edge to unique companies like this.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8e63d13',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The UK’s onshore wind power industry may have been dealt a huge blow by new government policies announced last week, but this apparent setback should instead be seen as an opportunity. Elsewhere in Europe local, cooperative wind power is flourishing – could your town be next? This wind turmoil comes at a point when politicians are widely struggling with the “energy trilemma” of balancing affordability and sustainability while also securing national supplies. The Queen’s Speech on behalf of the new Conservative government explicitly included a new Energy Bill designed to increase energy security – a clear prioritisation of one side of the “energy trilemma” in the next parliamentary period. Changes affecting the UK’s onshore wind power sector include devolving consent powers for any large onshore wind farms away from the secretary of state to local planning authorities. The government also plans to end its commitment for new subsidies for onshore wind farms. Taken together these measures appear to send out a strong signal that government support for onshore wind power in the UK is ending. Unsurprisingly, reactions to the announcements have been largely negative with companies fearing job losses and the terminal decline of this sector. Industry representatives are concerned about the capacity of local authorities to deal with these new powers in a competent and timely manner. Yet at the same time, these proposals may provide the sector with a much-needed incentive and an opportunity to develop new business models that increasingly rely on and reward local ownership and support given that communities are now in charge. The question is whether the onshore wind sector is able to turn this situation to its advantage, perhaps similar to the way the sector has flourished in countries such as Denmark and Germany. There, onshore wind projects are predominantly in the hands of mid-size firms and owned largely by communities and small investors. In Denmark, too, local municipalities determine whether and where wind turbines can be placed. This suggests that empowering local decision makers and drawing on citizen investment schemes can actually have positive effects on the industry. This is a model that could be adopted in the UK but there are caveats. First, the success and growth of small-scale wind in the past has relied on financial support in the form of feed-in tariffs – a scheme that is now under review and likely to be limited. the industry must now demonstrate the ability to stand on its own feet to support itself financially. In fact, onshore wind is already competitive and one of the cheapest forms of energy production. But it still has to compete with solar, offshore wind and other renewables for financial and governmental support, which leads to media complaints about how much these subsidies are adding to consumer energy bills. Much then depends on project size. Typically, big wind farms tend to be developed by large companies backed by institutional investors. A modern 2.5MW (commercial scale) turbine, on a reasonable site, will generate 6.5m units of electricity each year – enough to make 230m cups of tea. So the large wind farms above 50MW affected by the changes would typically consist of more than 25 turbines. For them, the challenge will be to cooperate with and convince local authorities of the wider benefits of these projects in order to gain planning consent. By contrast, many of the smaller projects have an inherent advantage. Increasingly they are crowd-funded such as one proposal by Yorkshire-based Edgehill, which seeks to raise £2.5m from investors chipping in as little as £50 to build ten turbines in ten different rural locations. With their risks and benefits shared between large numbers of individual investors, these projects are used to keeping locals happy, dealing with NIMBYs, and bringing them on as investors.  The question is which of the two project types and business models will become more successful in garnering local support over the coming years. It is possible that developers with closer links to local government authorities and community organisations are likely to emerge stronger from this political shift. In short, we might see more, but smaller wind farm projects. There is also a regional dimension to this, as the proposed Energy Bill will definitely not apply to Scotland, and perhaps also exclude Wales. The UK currently has 538 projects worth 7,240 MW in the pipeline which will be affected differently depending on where they are located. For onshore wind, devolved powers for Scotland are a good thing. For example, 37 out of 45 proposed sites for large-scale projects awaiting planning consent are based in Scotland and so would remain unaffected by the latest proposals. The future viability of the UK’s onshore wind energy sector will be determined by developers’ skills in attracting capital funding, managing local authorities and obtaining a “social licence to practice” from affected communities. Less clear, however, is how the government intends to square its declining support for the cheapest form of renewable energy and increased efforts to shore up the nation’s remaining oil and gas reserves with its other aim of “seeking to address climate change through ambitious action at home”. It seems that despite prioritisation ultimately the government cannot escape the energy trilemma."
"
Share this...FacebookTwitterBy Kirye
and P. Gosselin
Global warming alarmists like claiming that a certain place is seeing more warming and climate change than everywhere else. Remarkably, they say that about almost everywhere, which of course makes no sense.
Today we look at Canadian temperature trends using the data from the Japan Meteorological Institute (JMA) for stations where they have data available going back to at least the mid 1990s.
First we look at December mean temperatures. What follows is a chart depicting the results of 9 stations across Canada:

Of the 9 examined stations, seven show no warming taking place at all in Canada over the past quarter century for the month of December. Data: JMA. 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The data hardly show the trends you’d expect from a place that is supposed to be “warming faster than anywhere else”.
Canada mean annual temperatures show no warming 
Okay, those are only data for December. How about the annual mean temperatures?
What follows are plots for the mean annual temperatures for the 9 stations: 
Data source: JMA
The plots speak clearly enough: we have been seeing more cooling than warming.
Though the surface of the globe may be have warmed modestly as a whole, nothing unusual is going on. What we are likely seeing are mainly natural oceanic cycles at work, which we still know very little about.
Share this...FacebookTwitter "
"Coral reefs are an invaluable source of food, economic revenue, and protection for millions of people worldwide. The three-dimensional structures built by corals also provide nourishment and shelter for over a quarter of all marine organisms.  But coral populations are threatened by a multitude of local and global stressors. Rising ocean temperatures are disrupting the 210m-year-old symbiosis between corals and microscopic algae. When temperatures rise, the coral animal becomes stressed and expels its algal partners, in a process known as coral bleaching. These symbiotic algae are a critical food resource for corals, and without them corals lose their primary source of nutrition. Fortunately, corals are mixotrophs and not solely dependent on nutrition from their algal partners. Despite their sedentary appearance, corals are voracious predators capable of capturing a wide variety of prey using their tentacles and mucous nets. Knowing how much corals eat via predation is essential for understanding how they can persist in a warming ocean. Numerous laboratory studies have shown that if coral feed, they are more capable of surviving the stress associated with warming temperatures and decreasing pH levels. Feeding can also increase the reproductive capacity of corals, which is key to repopulating reefs that have suffered high levels of coral mortality. Yet, almost 90 years since one of the first published accounts of coral predation, we still do not know much about how coral feeding varies as a function of food availability in the wild. However, our new study sheds light on this longstanding question. We combined field sampling with global satellite measurements and published data to reveal that corals respond to how much food is on their reef. This indicates that corals living in more productive (food-rich) waters consume more food, which changes our understanding of how corals survive and may aid in predictions of coral recovery in the face of climate change.  Studying variation in the diets of corals over large areas is no easy task. To determine if corals will change their feeding behaviour as a function of food availability, we sailed to the remote Southern Line Islands of Kiribati. These islands are ideal for studying variations in coral diets because they lack local direct human impacts (fishing and pollution) and are situated across a natural gradient of food availability fuelled by equatorial upwelling. This process delivers colder, nutrient- and plankton-rich waters to the surface ocean along the equator in the central Pacific. We examined coral diets across five islands using stable isotope analysis. Stable isotopes are atoms of the same element (in this case carbon) that differ in mass due to the number of neutrons in their nucleus. This subtle mass difference allows scientists to determine what an organism is eating based on how similar the isotopic composition of the consumer (coral) is to its food (zooplankton). The isotopic data showed that the corals on the more food-rich islands were capturing and consuming more planktonic prey than corals on islands with lower food availability. These findings suggested that the abundance of food might be important for corals in other locations, which inspired our team to evaluate if coral feeding habits can be used to track global food availability. Satellites can reliably measure the amount of phytoplankton around tropical islands – a useful proxy for estimating food abundance for corals. So, using satellite data from 2004-2015, taken from 16 locations spanning the Pacific and Indian Oceans to the Red Sea and the Caribbean, we compared published isotopic values from corals at each location. What we found was a striking relationship between the chlorophyll content of the water and the feeding habits of corals. Essentially, corals in more productive regions consume more planktonic food. The seemingly simple observation that corals eat more where there is more food has important implications for our understanding of how coral reefs function. It underscores the importance of the physical environment around reefs and suggests that food availability may be an overlooked driver of coral recovery potential. The capacity for corals to feed before or during thermal stress can improve their capacity for survival. These findings lay the foundation to begin investigating the possibility that reefs in more naturally food-rich waters have a greater capacity to resist or recover from disturbance events such as thermally induced bleaching. Reefs do show variations in how they respond to thermal stress events – some reefs bleach less than others – but the exact mechanisms behind these differences remain largely unclear. The relationship between coral feeding and ocean chlorophyll established in this study offers a roadmap to locating potentially more resilient coral reefs around the world. Such knowledge does not replace the need to urgently reduce greenhouse gas emissions and protect coral reefs from the increasing frequency of ocean warming events, however. Instead it should be used to guide strategic management actions in the inevitable interim."
"
Share this...FacebookTwitterProbably the most disturbing aspect of wind energy is its destructive impact the green energy source has on the environment, never mind its sporadic supply, adverse health impacts and high costs.
As Germany ignored all warnings about wind energy’s shortcomings and its threats, government officials charged ahead blindly and obstinately, without a plan, into the uncontrolled development of the power source.
Germany wrecks its landscape
What follows is a photo video montage of how a beautiful natural landscape near the (once) scenic central German community of Wartenberg was tragically transformed into an industrial eyesore.

The photos were taken by Herrmann Dirr
The “Vogelsberg” wind project was carried out by a cooperation consisting of utility HessenEnergie, “scrupelous” forest owners and the community of Wartenberg.
The video posted on YouTube starts by showing a once idyllic scenery the area offered before the wind project came along and stamped it out. At about the 1:20 mark we begin to see the destruction inflicted on nature to build the project and then how today it is an almost inhospitable area for much wildlife.
Ruining communities to “save the planet”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s a classic example of how people professing to have good intentions, yet lacking in vision and judgement, allow themselves to be hijacked by populist green ideology and end up ruining everything they touch around them. They tell us they will rescue the planet, but only end up ruining their communities. It’s a disgrace.
These “leaders” operate according to true populism. They make promises that sound good to the masses who cannot think one step ahead and see the true consequences.
Vermont’s, Bernie Sanders’s disaster
It all reminds me how a few years back my home state of Vermont (whose source of power was in large part green hydro to begin with) deforested and dynamited its green mountain tops which took nature millions of years to sculpture, and then installed large wind turbines. Also see here and here.

How Vermont protects the environment by installing wind turbines. Photos taken near Lowell. 

Silent spring for Vermont wildlife. Making way for green energies. Stupid as stupid gets. 
The source of above photos is mountaintalk.com.
 
Share this...FacebookTwitter "
"At 89, Claudia Andujar still has her work cut out. For five decades she has photographed the Yanomami indigenous people, an Amer-Indian tribe who number 33,000 and live in 192,000 square kilometres of rainforest that straddle the borders of Brazil and Venezuela. Until the early 20th century they had lived almost entirely in isolation from the outside world, but since then disease, deforestation and climate change have taken their toll. The election of Jair Bolsonaro in Brazil has proved a further threat. Vehemently against legislation protecting indigenous lands, last week the far-right president commented: “Indians are undoubtedly changing … They are increasingly becoming human beings just like us.”  “I now put all my efforts into the activism, into addressing Brazil’s political situation,” Andujar says as she prepares a retrospective at the Fondation Cartier in Paris, which shows a selection from a personal collection that runs to several thousand photographs. “I no longer take photographs, but use my archive to show how I saw the Yanomami. This present government has no respect for them. They have no understanding of who they are as people.” The changing nature of Andujar’s work is chronicled in the show. Though the earliest images are straight documentary, the artist notes she was always more interested in striking up a human bond with her subjects than in approaching the work with professional detachment. “I decided very early that I would not photograph if I felt I did not have a connection with the person whose picture I was taking,” she says. “Developing an intimacy with the individual and community came first. The photography was always secondary to that.” Immersing herself in the culture of the Yanomami, it soon became clear that she would never represent their worldview through conventional composition. Instead, employing techniques, including double exposure, long exposures, the use of coloured filters or smearing of vaseline on the lens, Andujar started to produce a body of work a lot stranger and more faithful, she says, to the experience of the Yanomami people. A room of the Paris show is dedicated to the shamanic rituals that are a key aspect to Yanomami cosmology. During these ceremonies the tribe believe spirits – xapiri – descend on the forest leaving trails of brilliant white light in their wake. Andujar represented this by shaking her camera as she took each photograph of the convulsing, gyrating shamans. One image shows a woman with her face obscured by curling trails of smoky white light dancing across the picture’s surface. In another, a man, naked but for streaks of body paint, lies on the ground near a burning ball of light emitting from an unknown source. A funeral ceremony, in which the deceased is encased in a woven casket hung from a tree, is photographed through an otherworldly orange filter. A group of Yanomami are shown in layered multiples, using a double exposure, a way of reflecting the rhythm, movement and intense noise of the indigenous ceremonies.  Davi Kopenawa, a Yanomami activist and shaman, first met Andujar in 1977. “It was quite unusual for a white woman to come, a woman who wasn’t a missionary especially. Claudia took her time to get to know us; she slept in our shabono,” he says, referring to the ring-shaped wooden buildings that the Yanomami live in communally. “When the white people invaded our land, they took us by surprise and we weren’t prepared to deal with that first contact. Non-indigenous people wanted us to vanish, they wanted us to die ... It is still a dangerous battlefield out there for us, but to survive we need to confront it. These pictures are part of that. She was able to show them, show us, to the people of the city.” Born in Switzerland in 1931, Andujar was raised in Oradea, a border town that has switched between Romania and Hungary. Her parents separated when she was nine. In 1944, as the German army closed in on the town, her mother took her to Switzerland, leaving her father, Siegfried, behind. He, and his entire extended Jewish family, perished in Auschwitz and Dachau. This, Andujar says, lies behind the affinity she felt with the Yanomami. “I want to help the Yanomami to survive like my family did not. I think my work is dependent on the suffering of my childhood. My friends from school all died in Auschwitz. Everyone. Nobody, nobody survived.” Kopenawa agrees Andujar’s childhood holds the key to her relationship with the Yanomami. “When the people of the world waged war against each other, Claudia suffered a lot. But that gave her the experience that is required to take images of the Yanomami.” After the war, Andujar moved first to New York, before settling in São Paulo in 1955. “I was conscious that I was looking for something that was missing from my life,” she says. “I never forced the relationship with the Yanomami, but there was something within me that was searching for a connection or purpose that they provided.” The longest of her repeated trips living with the tribe was over a year. Did she feel at home with them? “Yes, yes I did.” The exhibition in Paris makes clear that Andujar’s journey from outsider to champion of the Yanomami was a long one. On one wall, showing her early work, is a fashion shoot from 1970 for Sententa magazine, in which a white model poses against the exoticised backdrop of an indigenous village. Elsewhere, the Yanomami are shown sympathetically, romanticised perhaps, with no hint to what some conservative anthropologists claim to be a violent culture inherent to the tribe. In 1968, the American anthropologist Napoleon Chagnon reported the tribe lived in a “a state of chronic warfare”. While disputes undoubtedly occur, Kopenawa rejects the generalisation, a claim backed by numerous studies since. Andujar says that while she feels close to the Yanomami, and never felt scared, there is a power dynamic between her and them. “There are various beliefs in Yanomami about photography, and the capturing of what we call the soul,” Andujar says. “They have this fear, this suspicion of the camera. I only took pictures of Yanomami who got to know me first, when they knew why I was there. After a while they trusted me and would take no notice of the camera.” In 1976, Adujar was back in São Paulo, struggling to get permission to return to the Amazon, when she heard of an ensuing measles epidemic among the Amerindian people. “I heard whole villages had disappeared, so many I knew died.” Kopenawa says that the disease was part of the ongoing persecution of his people by Brazil’s military dictatorship. “The white men were bringing all these diseases as the Transamazonian road was being built. I saw my mother and father die.” The tragedy precipitated a major change to Andujar’s career. Bruce Albert, a French anthropologist working in the area at the time, recalls that there was a strong desire by the dictatorship to keep news of the situation spreading. “The government agencies were spreading a lot of xenophobic, malicious rumours about us. They were furious we were there and one guy went round telling the Yanomami people we Europeans had come to steal their riches.” Albert recalls Andujar arriving on the scene in a black VW Beetle. “I was asleep in my hammock when I heard this engine noise. Back in Paris I drove a white Beetle and this was the exact same sound. I thought I must be hallucinating, but I got up, went outside and there was Claudia silhouetted against the headlights of the car on the dirt track, composed as if it were one of her photographs. She was someone I had heard about from the Indians but not met. Here she arrived like a character in her own work.” “I was undoubtedly aware that I was causing the government problems,” Andujar says, “They were watching me.” Albert and Andujar, alongside Carlo Zacquini, a liberal Catholic missionary, went on to form Comissão Pró-Yanomami (CCPY), an activist group that campaigned on behalf of the Yanomami, and who were instrumental in pushing for a continuous tract of officially demarcated land for the tribe. Granted in 1992, this now stretches over 96,650 square kilometres. However, despite legal protections, Kopenawa estimates 20,000 illegal gold miners are operating in the Yanomami Park with the tactic permission of Brazil’s government. It was through the CCPY, working with a group of doctors from São Paulo, that Andujar began coordinating a medical programme and the camera became but a tool in this activism. Yet the change of direction gave rise to what have proved to be her most haunting photographs. Traditionally, Yanomami do not give each other names, referring only to their relationships with one another. In order to keep medical records, the team needed a way of identifying each patient. Each of the Yanomami were assigned a number, written on a wood necklace, which Andujar photographed them wearing. In their pragmatic simplicity, and numbering in their hundreds, the images show the Yanomami at their most vulnerable, facing the onslaught of sickness brought in by outsiders. And, while this process was born out of administrative need, it has become Andujar’s most biographical project, the young and old staring back at the camera, their numbers recalling those branded on the victims of the Holocaust. Despite the power inherent in her photography, Andujar says she does not think art has a political agency. Gesturing to her life’s work, the artist shrugs. “This won’t change the attitude of Bolsonaro. All I can hope is when people look at my pictures they experience a connection with these people as people, people who, under this president, are suffering again.” • Claudia Andujar: The Yanomami Struggle is at the Fondation Cartier, Paris, 30 January – 10 May. Now closed to coronavirus, the exhibition can be seen here"
"

A potentially informative and constructive debate about the costs and benefits of global warming has been lost to “political dramatization,” argue the authors of a new Cato Institute book.



In _The Satanic Gases: Clearing the Air about Global Warming,_ climatologists Patrick J. Michaels and Robert C. Balling, Jr., trace the development of global warming, writing that politicians blame the latest thunderstorm, flood, or change in the weather on global warming. They also assert that global treaties, protocols, and other policies are being signed and negotiated despite shoddy science.



Michaels, senior fellow in environmental studies at the Cato Institute and professor of environmental sciences at the University of Virginia, and Balling, director of the Laboratory of Climatology at Arizona State University, analyze the politics of global warming and provide a primer on the science. Acknowledging that industrial emissions of greenhouse gases have warmed the planet and will continue to do so over the next several decades, Michaels and Balling argue that future warming will be moderate, not catastrophic, and will have benign economic and ecological effects. They point out that the effects of climate change are already positively affecting mortality and agriculture, citing data that show the “greening” of the earth may be enhancing plant growth. The year 1998, during which temperatures warmed as a result of El Niño, produced record agricultural output. The authors expect that the earth’s average surface temperature will warm 0.65 to 0.75 °C (1.17 to 1.35 °F) by 2050, resulting in a decline in temperature‐​related mortality and a rise in crop yields that alone would feed one‐​quarter of today’s world population



The authors find that government funding of research has corrupted the scientific process as scientists compete for funding in a politically charged environment. Total federal spending on global climate change research has ballooned from a few million dollars to $2.1 billion annually in the last 15 years.



The book has already received much praise. Frederick W. Seitz, past president of the National Academy of Sciences, says it “should be read by every scientist and layman who has an interest in the topic.”



 _The Satanic Gases: Clearing the Air about Global Warming _can be purchased through the Cato Institute’s online bookstore.



 **Mises, Hayek Examined in _Cato Journal_** __



The latest issue of the Cato Journal (vol. 19, no. 2) commemorates the 50th anniversary of the publication of _Human Action_ by Ludwig von Mises (Yale University Press, 1949) and the 100th anniversary of F. A. Hayek’s birth. Editor James A. Dorn writes, “These two giants of market‐​liberal thought exposed the fallacies of central planning, pointed to the importance of private property rights and limited government in promoting a spontaneous market order, and explained the role of institutions in shaping incentives and behavior.”



Papers by Vernon Smith, Israel Kirzner, Kenneth Elzinga, and George Selgin (along with comments by Lawrence H. White, Gordon Tullock, Frank Machovec, and Richard Timberlake) were first presented at the 1999 meeting of the Western Economics Association in a session titled “Mises’ _Human Action_ : A Critical Appraisal after 50 Years.” Smith discusses how the experimental economics in which he is a pioneer has confirmed Mises’ analysis of cooperation. Selgin and Timberlake examine Mises’ views on the role of gold in the monetary system. Elzinga and Kirzner both note Mises’ understanding that the market is a constantly evolving process, not a path to a particular endpoint.



Stephen Macedo of Princeton University discusses three themes in Hayek’s work: his critique of political utopianism, his emphasis on the interdependence of law and liberty, and his faith in the power of ideas and institutions. Ronald Hamowy, a student of Hayek’s at the University of Chicago, offers some personal reminiscences and an examination of Hayek’s history of liberalism. Those papers were delivered at the Cato Institute on May 8, 1999, the centenary of Hayek’s birth.



Other papers in the Cato Journal discuss the regulation of addictive substances, politics and the IRS, and women’s sports. _Cato Journal_ is published three times a year. Most articles are available at www​.cato​.org; subscriptions and single copies are also available.



 _This article originally appeared in the May/​June 2000 edition of_ Cato Policy Report.
"
"Is the Guardian really arguing that leaders who don’t jet into Davos aren’t committed to poverty reduction and climate action (World Bank chief’s Davos snub dashes hopes of climate consensus, 23 January)? The World Bank Group (WBG) is taking concerted action to tackle global poverty and boost shared prosperity by delivering concrete results for the world’s poorest and most marginalised people, including broad-based and sustainable growth, jobs, debt transparency, rule of law, good governance, human capital and green financing.  In his first year, the WBG president, David Malpass, has met more than 100 leaders around the developing world, in countries from Pakistan and Egypt to Ethiopia and Mozambique. Our strong focus on country results is one reason why the World Bank has repeatedly been given the highest rating among international agencies in the UK Department for International Development’s reviews of multilateral development partners. The WBG is committed to climate action and provides by far the most climate-related finance among multilateral development banks and other international organisations, almost half of the total. In the last fiscal year alone, the WBG committed $17.8bn (£13.6bn) to climate-related investments – and we announced $200bn of climate finance over the period 2021-25. The World Bank Group and our leadership remain focused on the urgent and significant challenges facing more than 700 million people living on less than $1.90 a day.Jeremy HillmanDirector, corporate communications, World Bank Group • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"Gino McDonald, 61, builder (on left), and Patto McDonald, 56, artist, Upper Brogo  Patto We were keeping an eye on the Fires Near Me app. By about lunchtime on New Year’s Eve we’d packed our daughter and grandkids up and sent them off. We stopped by the fire shed on the top of the hill. They seemed to think we were going to be all right, so we thought we’d be back the next day. We grabbed some dirty clothes baskets, a few things. But when we looked at Fires Near Me the next morning, we were horrified at how big it had got overnight. They reckon the fire spread at 60km an hour. It was a shock for days. It wasn’t until the third time we came back that I took everything in: the neighbours’ houses all gone, just a wasteland. It was as if we’d never been here before. We’ve lived here around 17 years. I had a lot of things – my grandmother’s and my mum’s. It’s taken me two weeks of saying, “Oh, it’s just stuff”, until my daughter said, “Stop saying that, Mum, it’s not just stuff.” But it was; that’s been a lesson for me, not to get attached to things, because it doesn’t matter. We don’t have any insurance, so we’ll be starting from scratch again. We started off here in a tent, anyway, so I guess we’ll be going back to one. It’s like Seasick Steve sang: “I started out with nothing, and I’ve still got most of it left.” We lived simply. We had one solar panel that was enough to give us lights, and a gas fridge; we used a lot of recycled materials. So that makes it a bit easier to start over. Our chief concern is our daughter, who moved in with us before Christmas with two babies; we’ll probably rush to get something up for her. Jenni Bruce, 63, artist, Upper Brogo, New South Wales I’ve lived here for 44 years. It’s fringe farming country and we’ve got all kinds of people: tradies, chippies, sparkies; life-changers as well, retirees and artists who have come to live in the bush. It was New Year’s Eve when we knew we’d have to evacuate, but we thought we had a great deal more time – that it wasn’t going to reach us until the next day. I was planning to stay in that night, but some of my friends had already evacuated to Quaama, a nearby village – I thought I’d just pop in and see them. When I turned around to come home, I saw the fire front. It was just a land-eating monster, tearing through with a ferocity like nothing I’d ever seen. I knew there was no going back into that. We waited in Quaama and watched until it became very evident that we needed to move. I helped a couple of locals evacuate, so I was probably one of the last cars out. By the time I was on the highway, the fire was only a few metres away; it was just so hot and so loud – the noise was unbelievable. At 5am we arrived in the tiny beach town of Bermagui and spent the day there. It was chaotic, a thousand people poured into a small space with only four toilets. None of us had much in the way of supplies, but the shops, bowling club, pub – everybody was doing their best to make people comfortable. Coming back home for the first time, after a few nights, was awful. You always hold out that tiny bit of hope that, just maybe, there’s something. But it was all gone. I had a great workshop with tools that I had spent a lifetime gathering. There was a beautiful garden, with lots of blueberry plants and fruit trees. I’d recently sold a lot of things to fix the place up, so I could stay as I grew older, and I’d just finished renovating. I had a huge collection of paints and canvases, and I was working on a new body of work to exhibit. It was just devastating to lose that amount of hard work. All I could do was keen – it’s a really weird noise; I’d only made it once before in my life, but I keened. There’s not a single person in the entire region who is not traumatised – all over the Great Dividing Range. I wish the people in power would listen; I wish they would stop using industries that are so bad for our environment. I hope that the underlying anger, because of mismanagement from our leaders, does not overrule the kindness and compassion that people are showing in the present moment. I don’t really know what my life lesson is: I’m just very glad to be alive. Sharyn Wotton, 61, teacher’s aide, and Tom Wotton (AKA Swampy Tom), 66, retiree, Wandella Sharyn Tom and I have been together 45 years and lived at Wandella for 24. We have 100 acres about 15km from the nearest town. It’s not a hobby farm – we don’t raise animals. We just live here for the peace and quiet, the serenity. We had always agreed we wouldn’t stay and fight a fire. We had used planks of foam, pontoons from Sydney harbour, as insulation for our walls. But the 75 acres we had behind us were wooded, and we’re in a gully, so we thought: we don’t have enough water. On 30 December, our eldest daughter, Carly, texted and said: “Mum, it’s doubled again.” Our two sons-in-law hooked up our 1979 Millard deluxe caravan, and we put a few bits and pieces in there and the car. I remember my daughter Teagan saying, “Mum do you want this, this?”, and I said, “No, I’ll come back tomorrow.” But there wasn’t a tomorrow. After a few days, we went to Canberra, where we got word from our neighbour who had stayed to defend his property. He said a huge firewall just came from nowhere, up one side of their house, and he knew then our place was probably gone. He was alive, and his kids were alive, but there have been so many horror stories. We’ve got friends who ran through firewalls to save themselves and ended up with burns. One family lost a father and son. We might have lost property, but there are people out there who are traumatised for ever. For me, it comes in waves. If I’m not talking about it, I’m OK. When we drove back, I was stunned. I joke that I’m a collector, not a hoarder, but I had a lot of stuff – big bookcases and stuff hanging from ceilings, and it’s all gone. We didn’t have insurance at the time of the bushfire. One of our children, without asking, started a GoFundMe page. When he found out, Tom was really angry and said, “We’re not a charity case.” Carly sat him down and said, “For 24 years, you have watered and fed people. People have come for two days and stayed for five because you make them feel welcome.” She said, “They’re donating to you. They want you to rebuild.” We’ve pissed mother nature off big time, and she’s paying us back. We’ve just been watching it get drier and drier – the whole valley’s been a tinderbox. Nobody heeded the warnings. Surely they’ll listen now.  Fina Montagner, 49, care nurse, and Anthony Montagner, 64, former electrician, with their sons Christian, 10, and Dylan, six, Upper Brogo Anthony We had gone into Bega to do some shopping on New Year’s Eve, and Fina and the boys decided to stay because everyone was talking about the fire. At about 5.30 that afternoon I came back here with the dogs to get the farm ready, thinking I could stay and fight the fire. At eight o’clock, there was no sign of fire. At 10 o’clock I went out to turn the generator off and noticed the glow to the west, 10, 15km out at least. I’ve had fires here in the past and I thought, well, that won’t be here till the morning. Forty-five minutes later, and the fire’s in my next gully. So I barely had 15 minutes to grab the dogs, a couple of cans of fuel, the kids’ bikes, a pair of jeans and I took off. I was only ever 10 minutes from the fire all the way to the coast.  I’ve had fires get away on me in the early years. This was a monster; it was racing as fast as my van would go. And the noise sounded like a couple of freight trains. It was sucking air in from all directions. It was a fire-breathing dragon, spitting hot flames everywhere. But it’s not climate change – I’ve heard of at least 180 people arrested for arson.  I haven’t been insured for 25 years. I’ve built what I had by scrounging. When I met Fina, she got a job so we were able to buy some new materials, but other than that everything in the house was built from bush wood. It was all repurposed junk. But I’m not going to do that again, there’s nothing left here to do it with. I’m not prepared to start scrounging again. We’ve got a caravan for the boys. We’ve got another coming for us. So we’re just going to camp here for the time being, maybe six months, a year.  Fina The little one seems OK, because he’s only six. But Christian is having issues. He gets very angry and then he’s anxious. He’s been really sad in the last few weeks. Anthony I don’t have one photo to show them what their grandmother looked like. I’m hoping my siblings might have some. I don’t have photos of my childhood or anything I’ve done as a teenager. It’s like I’ve never existed.  Fina We’re just hoping to be strong, emotionally, for the boys. They have got no one but us, so we need to show them that we are tough. Ron Corby Snr, 87, farmer; his son, Ron Jr, 67, farmer, and his wife Gloria, 71, housewife; granddaughter Tammie, 42, carer, with Brett Jee, 45, painter, and their sons (from left) Blake, 16, Myles, 12, Beau, 10, Mason, 14, in Wandella Ronald At 2.30am, I received an urgent call: “If you want to get out, get out now!” I looked out the window – everything looked normal and I went back to bed. Ten minutes later, my daughter pulled up and said, “Pop, get in the car and go! My house just burned down and the fire followed me, nearly as fast as I could drive.” When I came back a couple of days later, there was just charcoal. This fire was hotter than anything. I saw cars’ aluminium wheels melted and running down the gutter like a stream. The fire came from all angles, in whirling winds and twisters. It’s 35 years since the bush was last burnt out. The stuff in the mountains has been building up; you couldn’t walk through it, there’s that much rubbish. When the fire came, the whole mountain seemed to explode. I don’t think it was the climate crisis that caused this; I think it was neglect, not keeping the mountains clean.  I grew up in the Great Depression. Back then, it was a bit like this. We had nothing. But when someone was sick, the community would come together. I thought, in this modern age, that was gone – it looked like people would rather cut your throat. But I was wrong. A man who didn’t know me gave me A$500, and he’d probably lost his job the same as anyone. But the human love is there and stronger than ever.  Ron All my life I’ve fought bushfires, but there’s never been one like this: the fireballs, the way it roared, the flames, 50-60ft high. Sometimes it seemed to be above the ground, just burning the air. It sucked the roof off the house and threw our cars a kilometre away.  Tammie Forensics think two fire fronts collided and created their own storm. My brother had a bull that was killed in the fire and is still sitting upright. It was just instantly cooked, mummified. They told us that for that to happen, it had to reach 2,000 degrees. I don’t think it’s climate change; the bush here hasn’t been burned back in 15, 20 years. We have five boys. Mason has muscular dystrophy and we’d set up an aviary for him. He can’t do anything active, but loves birds and wanted to breed them. They were all lost. He was hard hit and cried quite a bit, but we’ve told him we’ll build him a new one.  We didn’t bring the boys straight out here. But our youngest, who’s 10, would not stop asking what it was like. Is this still OK? Is that still OK? So we thought we should show them that there is nothing salvageable. But he found a few garden ornaments and said, “Look, Mum, you can put this in the new garden.” Brett and I have been together 24 years. So when the insurance company says, “Can you write down a few lists of your assets?” there aren’t enough notebooks. I’ve not seen my husband cry like this. Brett’s father passed away a few years ago and he had a lot of his belongings here, tools and stuff you’ll never get again. But we’re just thankful that none of us are lost. All our family’s still here. Veronica Coen, 59, mental health clinician, and Murray Gibbs, 62, trike tour guide, Quaama Veronica We were away, visiting my daughter and her family. But this just wasn’t a fire that human beings could tame. It’s become very political. I call myself a fire refugee, and think about the people in the Pacific Ocean on the low-lying islands, and war refugees – because it’s as if there has been a war.  For a while I had a great deal of hope that the people who run the country would have to get it. That’s dissipating now. There has to be some kind of transformation in the way we govern. This two-party system is combative: they consume too much energy on the ego stuff and don’t respond to human need. A lot of people I’ve spoken to are clearly dealing with post-traumatic stress. I had a meltdown the other day and made someone else really sad. I was able to collect myself the next day and was welcomed back with open arms, because everybody’s falling apart. I’m a trauma-informed practitioner, and it’s a slow process. I’m not someone who usually posts my status on Facebook, but before I knew it, I just put it up there: “Don’t rush my recovery.” That’s really what it feels like.  In my work, we talk about collective trauma. Until now, I’ve only read papers and heard it spoken about in relation to Indigenous communities, for example. We talk about intergenerational trauma and collective trauma – this is what we have.  Pam Sweeny, 63, nurse, Cobargo My husband, Michael, and I had four children, and brought them up here in a way that’s kind to the environment. We’ve been using stand-alone solar power for 31 years. We’ve got our own water, and got rid of our own effluent in a way that’s friendly to mother nature. We had our family of native birds, too. We’d lie on the grass with the kids at night and watch the little gliders glide out. When we came back, the trees were still falling. It felt sad and unsafe. There was a big tree across the driveway, and piles of tin from the sheds that had been burned out. I wake up with tears, but then you just get busy. What we’ve done to Australia in the years that we’ve been here, us whitefellas, we’ve got to learn – we’ve done too much damage and mother nature’s hitting back. We didn’t even have a lock on the door, so we couldn’t get insurance. That is not unusual here. There’s loss, but we can replace it. New year, new beginning. We’ll be fine, but people who were used to spending five minutes in the shower and leaving their lights on [will struggle]. We’ve got to stop being such consumers, and think about where things come from: from mother nature. David Wilson, 59 (on right), and Kyle Moser, 41, both post office workers, Cobargo David I’m the licensee of Cobargo post office. Kyle and I moved here four years ago from the city, and we love the place. It’s the most unique community: 50% old-style farmers, 50% new people, often very musical or artistic. One in three says they’ve lost their house. We’re still hearing of people in hospital. Each one that dies is just a blackness, a real sadness. We didn’t really sleep on New Year’s Eve; we just watched and waited. It got to a point where it was coming in all directions. There’s only one road out, and it looked like the single bridge that leads into town was on fire, too. We headed for the ocean with four dogs, two cats and two rescued wallabies in the car.  The community response has been amazing: we had a relief centre, operational pretty much from that night, and it just grew. There were toilets, showers; they quickly got a generator on site. When Kyle and I came here, we were widely accepted as a gay couple. Quite a few families, although they might seem tough on the exterior, have a gay son or a gay nephew. During the marriage equality debate, we raised the rainbow flag, and Australia Post told us to take it down – they wanted to remain impartial. The response of the community was: “Well, if the boys can’t fly their flags, we’ll do it for them.” Shops put rainbow-coloured clothes over the balconies, the bookshop put rainbow-coloured books in the windows. It was amazing. We’re lucky that we’re insured, but precious things were lost. For my 50th birthday, we had a ring made with diamonds and my date of birth on – that’s gone.  But what upsets me most is the political inaction of the last two years, when they’ve been warned of these conditions. People had been asked for additional firefighting aircraft. They should be held criminally responsible.  Seraphina Leahy, 20, artist and barista, Wandella  I moved out here to get away from everyday life, and to try not to have an impact on the environment. When we heard there were fires coming, we packed up a few things and went to my mum’s house in Bermagui. Then the Rural Fire Service said we needed to evacuate. We went down to the beach, and when you looked back, you couldn’t see the mountain any more: it was pitch dark and a huge glow reflected on the ocean. We camped on the beach, and went fishing to calm our nerves, so we kept ourselves fed. We had my nana with us, who’s in her 80s. We had word that the house was gone, but until we saw it I didn’t really believe it. We have a lot of hard work ahead of us, mentally and physically. The support’s been amazing; if we didn’t have that we probably wouldn’t be doing OK. You’ve got social media, and sometimes it feels as if everyone’s always arguing, but when something like this happens, humans pull together and look after each other. We were doing our best not to make an impact, having a carbon offset with all our trees. In two years it’s gone from lush green pastures and forest, to the leaves on the trees just dropping. The flowers don’t come out, the animals are struggling, everything is yellow and brown.  I think the prime minister needs to have more empathy. The government cut so much funding to forestry and national parks. Everyone’s dry in terms of resources; we need the land managed properly.  • Additional research by Annette Widitz. The Cobargo Community Bushfire Recovery Fund is crowdfunding for the local area. This article has details on how to help with the Australian relief effort more generally. If you would like your comment on this piece to be considered for Weekend magazine’s letters page, please email weekend@theguardian.com, including your name and address (not for publication)."
"

You know the old saying that “no two snowflakes are alike“? Well it may be possible for two snowflakes to be alike after all. There’s a fascinating article in LiveScience that details how this may be possible.
For anyone who studies probability, this seems reasonable, given that the article mentions that 10^24 snowflakes fall in any given year. The article also contains a photo gallery of fascinating snowflake pictures like the one shown above.
From the article: “A typical snow crystal weighs roughly one millionth of a gram. This means a cubic foot of snow can contain roughly one billion crystals … It is probably safe to say that the possible number of snow crystal shapes exceeds the estimated number of atoms in the known universe.”
Kenneth Libbrecht, a professor of physics at California Institute of Technology runs a website devoted entirely to Snow Crystals at www.snowcrystals.com which is also visually impressive.

Here’s an interesting graphic on the formation of Snow Crystals:



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8c39c76',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In an abstract presented at the 26th PACLIM Conference that was published in a recent issue of Quaternary International, Verosub (2015) writes about the challenges of maintaining and utilizing water supplies in California. However, the geologist from the University of California notes that what is often missing from discussions of water security is a consideration of the effects of natural climate variability beyond the historical record. As an example of such variability, Verosub cites the fact that river flow and lake measurements during the 20th century “document the occurrence of several multi‐​year droughts in the past 100 years while tree ring records show that 20‐​year and 70‐​year droughts occurred during the last 300 years.”   
  
  
On an even _longer_ time scale, the scientist reports that “at least once and probably several times in the last few thousand years, there have been droughts severe enough to drop the level of Lake Tahoe by several tens of meters, which allowed Douglas fir trees to grow to maturity on exposed lake beds.” Furthermore, other data indicate episodes of extreme flooding, such as the water year of 1861–1862 that brought extensive rainfall from Oregon down through southern California.   
  
  
In consequence of these realities, Verosub concludes that “the paleoclimate history of California suggests that even in the absence of climate change due to anthropogenic greenhouse gases, decadal, multi‐​decadal, or even century‐​long droughts are a real possibility in the future for California as is flooding on a greater scale than was seen in the twentieth century.” According to Verosub, if such _natural_ events were to occur today, they would easily “wreck havoc with California’s delicately balanced water delivery system” in the case of drought, and “overwhelm the levee system and destroy California’s ability to transfer water from north to south” in the case of flooding. No doubt, such events would quickly be labeled by climate alarmists and advantage‐​seeking politicians as “human‐​caused.” Yet, given the historic periodicity of these events, there would be no way to prove that they weren’t natural. In fact, their mere occurrence would simply confirm that they _are_ natural, recurring over and over again throughout history, human influence notwithstanding. As such, the title of the author’s work provides some good advice for Californians: _Don’t worry about climate change; California’s natural climate variability will probably “get us” first_.   
  
  
  
  
  
**Reference**   
  
  
Verosub, K. 2015. Don’t worry about climate change; California’s natural climate variability will probably “get us” first. _Quaternary International_ **387** : 148.
"
"More than 90% of the £2bn in energy deals struck at this week’s UK-Africa investment summit were for fossil fuels, despite a government commitment to “support African countries in their transition to cleaner energy”. Prime Minister Boris Johnson opened the summit on Monday, citing the climate emergency: “We all breathe the same air, we live beneath the same sky, and we all suffer when carbon emissions rise and the planet warms.”  But the commercial energy deals revealed later were dominated by oil and gas production. The official UK government statement on the summit and a press release failed to mention these, citing only the far smaller support for clean energy. Green Party MP Caroline Lucas said the “hypocrisy of the government’s position is breath-taking”. Johnson also announced that UK taxpayers’ money would no longer support overseas coal-fired power plants and coal mining. Yet MPs on the environmental select committee reported in 2019 that “UK Export Finance (UKEF) has not supported a coal project since 2002”.  A report by Greenpeace and Newsnight also found that UKEF spent billions of pounds abroad supporting fossil fuel projects that will emit an estimated 69 million tonnes of carbon a year. The UK will host a critical UN climate summit in Glasgow in November, at which nations must dramatically increase their pledges to cut carbon emissions to avoid a disastrous 3o-4oC rise in global temperatures. The five oil and gas deals announced after the summit are worth £2.1bn, led by oil company Tullow investing £1.2bn in continued oil production in Kenya. The other fossil fuel contracts span the continent from Nigeria to Mozambique and Tunisia to Cote D’Ivoire. In contrast, just £161m in deals – 8% of the total – were related to clean energy, the biggest being for £80m of solar-powered irrigation pumps for Uganda and £50m to help build a solar farm in Kenya. Before the summit, Lucas asked ministers which companies would attend, but the Department of Trade refused to reveal the names, citing “commercial interests”. “Now we know why the government was so secretive,” said Lucas. “The hypocrisy of the government’s position is breathtaking. It boasts of its green credentials one minute, and then hosts an investment summit which sees billions being invested in carbon-intensive industries in Africa. This government’s promises on climate action are completely hollow. “As hosts of this year’s UN climate summit, the UK needs to show international leadership and bring other countries with us. We can’t do that while UK money and business is supporting dirty energy around the world.” A spokesman for the Department for International Trade (DIT) said: “The UK is committed to tackling climate change and supporting African countries in their transition to cleaner energy. Less than one-third [of the total £6.5bn in deals] were in oil and gas.” At the summit, Johnson said the UK would help African nations “extract and use oil and gas in the cleanest, greenest way possible”, while also delivering “an electro-convulsive lightning bolt through our renewables industry”. Other deals struck at the summit included a total of £170m for aircraft and airports and £224m for a new Kenyan gold mine. The biggest deal was £3.2bn for Bombardier to construct and operate two monorail lines in Cairo, Egypt. The UK taxpayer is directly funding £50m of clean energy projects, including energy storage batteries and energy-efficient housing. “The government is stepping up our offer to work with African countries to unlock their massive renewable energy potential,” said the DIT spokesman. Nick Dearden, director of campaign group Global Justice Now, said: “The supposedly transformative £6.5bn UK investment in Africa includes oil, gas, gold mining and airlines. So much for fostering ‘climate-friendly’ development. “In the 19th century, the ‘scramble for Africa’ was carefully disguised as a humanitarian project. Now, 150 years later, what we saw at the UK-Africa summit was a desperate and unseemly grab for markets, dressed up as ‘development’.”"
"

From the France surrenders just to be safe department :
Some “experts” think we should put the U.N. in charge of our space defense against large meteors or asteroids that could wipe out Earth. Ok, let me ask you a question.
Can you name one thing the U.N. has been able to accomplish with complete success? …..Yeah, I thought so.
If the world needs to deflect an asteroid, or even practice doing it, failure is not an option. So rather than leave the fate of the world in the hands of this, ahem, “capable” diplomatic organization, who you gonna call? (Hint, they have headquarters in Florida and Texas). Please, leave space work to space agencies, and the hand wringing to diplomats.
SAN FRANCISCO (Feb. 18) – An asteroid may come uncomfortably close to Earth in 2036 and the United Nations should assume responsibility for a space mission to deflect it, a group of astronauts, engineers and scientists said on Saturday.
Astronomers are monitoring an asteroid named Apophis, which has a 1 in 45,000 chance of striking Earth on April 13, 2036.

Although the odds of an impact by this particular asteroid are low, a recent congressional mandate for NASA to upgrade its tracking of near-Earth asteroids is expected to uncover hundreds, if not thousands of threatening space rocks in the near future, former astronaut Rusty Schweickart said.
“It’s not just Apophis we’re looking at. Every country is at risk. We need a set of general principles to deal with this issue,” Schweickart, a member of the Apollo 9 crew that orbited the earth in March 1969, told an American Association for the Advancement of Science conference in San Francisco.
Schweickart plans to present an update next week to the U.N. Committee on Peaceful Uses of Outer Space on plans to develop a blueprint for a global response to an asteroid threat.
The Association of Space Explorers, a group of former astronauts and cosmonauts, intends to host a series of high-level workshops this year to flesh out the plan and will make a formal proposal to the U.N. in 2009, he said.
Schweickart wants to see the United Nations adopt procedures for assessing asteroid threats and deciding if and when to take action.
The favored approach to dealing with a potentially deadly space rock is to dispatch a spacecraft that would use gravity to alter the asteroid’s course so it no longer threatens Earth, said astronaut Ed Lu, a veteran of the International Space Station.
The so-called Gravity Tractor could maintain a position near the threatening asteroid, exerting a gentle tug that, over time, would deflect the asteroid.
An asteroid the size of Apophis, which is about 460 feet long, would take about 12 days of gravity-tugging, Lu added.
Mission costs are estimated at $300 million.
Launching an asteroid deflection mission early would reduce the amount of energy needed to alter its course and increase the chances of a successful outcome, Schweickart said.
NASA says the precise effect of a 460-foot object hitting the Earth would depend on what the asteroid was made of and the angle of impact.
Paul Slovic, president of Oregon-based Decision Research, which studies judgment, decision-making and risk analysis, said the asteroid could take out an entire city or region.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8507042',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Air pollution pumped out by factories and power plants in Europe and North America has led to drier spells in the tropics, thousands of miles to the south. Scientists had long suspected this was the case and even had modelled the change in computer simulations, but now for the first time we have direct evidence – straight from a cave in Belize. Most of us, when asked to think about climate change, think of global warming and the unequivocal rise in greenhouse gases. But greenhouse gases aren’t the only pollutants we produce which have the potential to disrupt the climate. Atmospheric sulfate and nitrate aerosols, produced from burning fossil fuels, alter the climate both directly by reflecting solar radiation and indirectly by altering clouds. The impact of these aerosols is to offset the warming caused by greenhouse gases – where GHGs cause the Earth to retain heat, aerosols keep heat out in the first place.  However, as aerosols don’t stick around in the atmosphere for long, their effects are much stronger close to their source. It’s one reason why we see big regional differences in climate change. Indeed, observational and modelling studies provide considerable evidence that aerosols have lowered surface air temperatures in the northern hemisphere, offsetting greenhouse warming. Characterising our influence on the climate is challenging for many reasons. Climate is a complex web of intricately bound variables, difficult to understand and even more difficult to predict. But there is one issue with climate science that complicates things more than any other: the lack of instrumental data. Beyond the past 130 years, instrumental and observational data is sparse and uneven. It’s not very useful when looking to give context to our current climate debate. The study of climate history therefore relies on proxies to reconstruct the conditions at a given point.  These proxies are natural archives such as sediment cores, ice cores, tree rings and rocks – these record certain aspects of the climate in their physical characteristics. The width of tree rings, for instance, or the amount of carbon found inside air bubbles trapped in Antarctic ice for thousands of years. By analysing these archives we can create climate records that extend far beyond the short era of thermometers and rain gauges. In our research, published in Nature Geoscience, we present one such climate reconstruction produced from a stalagmite collected from a cave in Belize in Central America. Stalagmites (the ones that grow upwards from the ground) grow incrementally as saturated water, filtered through the rock above, drips into the cave and leaves behind what becomes new rock.  Every drop of water has a unique chemical signature that is largely controlled by prevailing climate conditions above the cave, meaning that stalagmites record climate changes as they grow. By analysing the geochemistry of these incremental growth layers in a 450 year-old stalagmite, we were able to construct a historical rainfall record for the region. Recently it has become increasingly clear that climate changes in one region can have an impact in a totally different latitude. The IPCC’s 2013 summary for policymakers IPCC 2013 concludes with confidence that man-made changes in the North Atlantic climate are linked to rainfall at lower-latitudes. Precipitation in the tropics, including Belize, is governed by the Intertropical Convergence Zone (ITCZ) – a belt of monsoon rainfall encircling the Earth near the equator that migrates seasonally between the hemispheres. The relative temperature difference between the hemispheres plays a crucial role in controlling the position of the ITCZ and hence, rainfall distribution in the tropics. What we found was a distinct drying trend in Belize since 1850 that coincides with a steady rise in industrial aerosol emissions in North America and Europe. This presents strong evidence that industrial sulfate emissions have shifted the position of the ITCZ through reflecting the Sun’s incoming radiation and therefore moderating warming in the northern hemisphere. In other words, aerosol pollutants have changed the relative thermal contrast between the two hemispheres and subsequently led to the ITCZ moving southward. This means less rainfall for the northern tropics. The role of sulphate aerosols in repositioning the ITCZ was previously identified through computer modelling techniques, but until now no suitable climate record existed to support those ideas. Our claims are backed up by the volcano record. Emissions from volcanoes are similar to those produced by burning fossils fuels – basically lots of sulphur – and we identified short-lived drier spells in the northern tropics following very large volcanic eruptions in the northern hemisphere, such as the Icelandic Laki eruption in 1783.  This provided evidence that any injection of sulphate aerosols into the upper atmosphere, both natural and man-made, can disrupt temperatures and rainfall. These volcanically forced dry periods essentially rule out the possibility that the climate shifts were caused by a previously unknown natural climate cycle or increasing atmospheric carbon dioxide concentrations.  Although warming due to man-made carbon dioxide emissions has long been at the centre of discussions regarding climate change, the shifting of rain belts has significant regional-scale effects. The tropics are heavily populated and extremely reliant on regular rainfall. Linking human-induced changes and natural changes from the past to understand where the climate currently stands, and where it might go in the future, will be as socially important as it is scientifically challenging."
"

In a nod to alternate energy, Ford annouced a new sports sedan with roots in the Mustang design that will run on E85 ethanol fuel. It’s called the Ford Interceptor. It is shown above. The fuel it can burn, E85,  is a mixture of 85% ethanol and 15% gasoline.
Ford says: âOur customer target for this powerful masculine sedan was a man with a family,â? Horbury said. âHeâs essentially a good guy, but a bit mischevious. He loves power and performance. But ultimately, heâs responsible. When he has his family on board, he values new safety technology as well as a powerful engine that runs on E-85 ethanol.â?
You may have read in the ER where Rick Keene went to South America to view how ethanol is produced and used as an alternative to gasoline. In Brazil, ethanol is produced from sugar cane and fuels a major portion of their automobiles. It was Brazils response to the Opec induced oil crisis of the 1970’s, and it’s a good idea.
In the USA, ethanol is produced from corn, but its production efficiency is reportedly not as good as with sugar cane. Nonetheless, demand for ethanol continues to grow, with new production facilities coming online each year.

Ford has issued the following press release:
Building on its legacy of bold muscle cars, Ford is introducing a modern, all-American sedan concept that combines âBuilt Ford Toughâ? attitude with the sporty elegance of its iconic 1960s sedans.
The Ford Interceptor concept comes equipped with a manual six-speed gearbox mated to a Ford Racing 5.0-liter V-8 Cammer engine that delivers 400 horsepower and runs on E-85 ethanol.
âThis concept celebrates the best of American muscle, showing customers what âmodern muscleâ is all about,â? said Peter Horbury, executive director â Design, The Americas. âThe Interceptor concept is much like a Marine in dress uniform. He looks smart and elegant but you can see the raw power that lies beneath.â?
Flexing Modern Muscle
The Ford Interceptor conceptâs exterior design features substantial, sometimes brutish, surfaces and sections that give the concept its modern, powerful look.
The Mustang-based concept features a traditional rear-wheel drive proportion that includes a short front overhang, long rear overhang and extended dash-to-axle ratio.
The Ford Interceptor also has a low cabin and higher beltline, adding to the vehicleâs attitude and sense of mystery.
âThe Ford Interceptor concept is a pure sedan that speaks to performance car lovers everywhere,â? said Freeman Thomas, director, North American Strategic Design. âThese people might need more space, but they still appreciate the power and attitude that cars like this represent,â?
Painted a deep blue, the Ford Interceptor conceptâs strength exudes from its strong, high shoulders. And much like on last yearâs Ford F-250 Super Chief pickup concept, a single character line runs the length of the body side, slightly sloping downward as it reaches the back of the sedan.
This adds wedge to the car, making it dynamic, without detracting from its smooth, clean design.
Signature Ford touches include the horizontal three-bar grille, which has been structurally integrated into the bumper beam, as well as âsquirclesâ? â or professionally square circle-shaped graphics â inside and out.
As a nod to performance purists, the ultimate muscle lies under the powered clamshell âshakerâ? hood, which caps a thoroughly detailed engine compartment that houses a 5.0-liter V-8 Cammer engine.
This is an upgraded variant of the 4.6-liter engine under the hood of the current production Mustang GT. The Cammer modular engine powered Ford Racingâs FR500C race car to the top of the Grand Am Cupâs GS class, achieving five victories on its way to the Drivers, Manufacturers and Team Championships in its first season of competition.
The Interceptor conceptâs Cammer engine is mated to a manual six-speed transmission. The car, equipped with 22-inch wheels, also features a solid rear axle for more hard-core performance feel.
Attitude Within
Inside, the Ford Interceptor concept is sleek and thoroughly modern, completed in contrasting black leather and metal finishes.
The dash, headliner and thick steering wheel are leather-wrapped. Plus, the Interceptor conceptâs four low-back bucket seats are wrapped in thick black belt leather with exposed-edge seams and contrasting caramel stitching. The seats are accented with Ford GT-inspired squircle grommets finished with Titan Metal painted inserts.
Squircle accents are repeated in the conceptâs door trims, floor, console and instrument panel.
Designed within a pair of squircles, the speedometer and tachometer are eye-catching. The needles for both start at center and move opposite each other as the speed and RPM climb.
Other clever touches include retractable headrests that deploy from the roof when the car is parked. They adjust fore and aft, as well as up and down for each occupant. Audio control panel and climate controls also are stowable.
On the other hand, the gated six-speed shifter is exposed, just waiting to be thrown into gear.
âThe Interceptor concept is a sedan â but with the heart and soul of a performance car,â Thomas said. âThis car is about restraint â and not clouding the driving experience with too much technology. There arenât a lot of layers between the driver and the road with this car.â?
Safer travels
For safety, the Interceptor concept incorporates Fordâs patented four-point âbelt and suspendersâ? safety belt design in all four seats and inflatable seat belts in the rear.
While current three-point safety belts are extremely effective in reducing the risk of injury in a crash, Ford Motor Company is researching these two potential safety belt technologies as possible ways to further reduce injury risk in vehicle crashes.
A number of technical challenges still need to be overcome before such restraint systems could ever be used, but these technologies might one day further enhance safety belt effectiveness.
The four-point belt showcases a possible next-generation safety belt that is more comfortable and easier to use than traditional three-point belts, according to consumer research. Additionally, inflatable belts have been included in the rear seat of the concept to help better protect occupants in a variety of crashes.
Ford Interceptor Concept
Powertrain
5.0-liter Cammer V-8
Chassis lengths
Overall length…………………………………….201.6 in.
Wheelbase………………………………………….120.8 in.
Overall width……………………………………….76.4 in.
Overall height at curb…………………………….54.8 in.
Track width
Front…………………………………………………..66.5 in.
Rear……………………………………………………67.8 in.
Suspension
Front………………….. Double wishbone-independent
Rear…………………….3-Link Design with Panhard Rod
Headroom
Front…………………………………………………..37.5 in.
Second Row…………………………………………35.9 in.
Legroom
Front…………………………………………………..42.3 in.
Second Row…………………………………………35.6 in.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea903ff34',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
It’s been a particularly mild winter in Europe this year. But that hasn’t changed the long-term trend over the past 30 years.
Now that the February 2020 data have been coming in, we plot the mean February temperatures for some countries in Europe.
Sweden
Three of 5 stations show February mean temperature in Greta Thunberg’s Sweden have had a cooling trend since 1988! The real data will probably make the climate alarmists upset.

Data source: JMA. 
Great Britain
Twelve of 14 stations in UK show February mean temperatures have had a cooling or no warming trend since 1992! (Note: those 14 stations have data dating back at least to 1980s):

Source: JMA
Finland


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




As a whole this northern European country has seen very little warming over the past 3 decades for the month of February:

Three of six stations in Finland show no warming since the 1980s. Data: JMA.
Ireland
Ireland, situated in the North Atlantic, also shows no warming for the month of February since 1987:

Four of 6 stations with data going back to the 1980s show no February warming. Data source: JMA
Netherlands
The story is the same in the Netherlands for the month of February. All 5 of 5 stations there with data going back to at least 1995 show February temperatures having a cooling trend:

Mean February temperatures have been falling at stations in the Netherlands since 1995. Data source: JMA. 
So it remains a mystery as to why global warming alarmists claim rapid warming is happening.


		jQuery(document).ready(function(){
			jQuery('#dd_78e475dfa13960a3458e73b79c2c48f9').on('change', function() {
			  jQuery('#amount_78e475dfa13960a3458e73b79c2c48f9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

I’ve been saying this all along…the sun is the Big Kahuna of climate change on earth. CO2 effects pale in comparison to the effects of the sun. I’ll have more on this in part 3 of my series on 2006’s Record setting temperature year.
WASHINGTON (AP) — The brightening and dimming of the sun may account for a 1,500-year cycle of cooling and warming on parts of the Earth, a study of ice in the North Atlantic suggests.
Researchers found that a very slight difference in the amount of solar energy reaching the Earth can have a powerful chilling effect on the climate: ice builds up in lands bordering the North Atlantic, the average temperature drops in Europe and North America.
see the full story here from USATODAY
This goes hand in hand with another study by the University of Main Climate Change Center as reported by SPACEREF
And just in case that’s not enough light reading for you, here is a study from Harvard that talks about “Chaos” and the sunspot cycle.
Part of the abstract is quite telling: “…by examining 1500 years of sunspot, geomagnetic, and auroral activity cycles. Sub-harmonics were found of the fundamental solar cycle period during the years preceding the Maunder minimum and loss of phase of the subharmonic on emergence from it. These phenomena are indicative of chaos. They indicate that the solar dynamo is chaotic and is operating in a region close to the transition between period doubling and chaos.”
Translation: The sun can easily tip from one state to another, with resultant changes in solar output.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8f61fb0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 There’s an interesting thorn in the side of the recent planning commission approval of Meriam Park that nobody seems to have brought up or discussed. Maybe there are plans I’m not aware of, but given the issues being raised with a cell phone tower elsewhere in the city, it seems the issue would have been vocalized by now.

There’s a giant cell phone tower jutting into the Meriam Park property. Most people think its a standard radio station. It was, but not anymore. It’s purely a cell phone site. It’s at the intersection of Bruce and Picholine Way, as seen at left.

I’ve also provided an aerial view from Google Earth.

The former KHSL-AM radio towers on Bruce Road no longer broadcast on AM 1290 as they did for 50 plus years. That transmitter was removed a few years ago but the towers remained. The FCC license was and property was sold to McCoy broadcasting and KPAY 1050 was converted to 1290.
In the early 1990’s, one of Chico’s first cell phone services was placed on the East tower and it remains in service today. The West tower is not transmitting anything at all. A few years ago, the Bruce road property was sold to one of the cell phone companies when Clearchannel bought KPAY from McCoy.
The old KHSL Radio tower cell site is probably the best in Chico due to its location and height. I don’t think they’ll be easily persuaded to give it up.
So now, some city councilors that may want to vote against a cell phone tower at the Elks Lodge for “health and safety” issues raised by enlightened citizens may find themselves in a pickle when it comes to approving Meriam Park homes that will be less than 500 feet from those “dangerous cell phone emissions”.
In the graphic below from the Meriam Park planning website, I’ve added the pointer showing where the cell phone tower is in relation to the rest of the plan.

The neighbors right across the street on Picholine fit in that zone already.
I wonder if the neighbors on Picholine Way were ever told of the nature of that tower? I wonder if they even care? Since the cell phone transmitters have been there about 15 years now, I wonder how many of the neighbors are suffering from debilitating health issues as is claimed by some detractors of the Elks Lodge cell tower?
‘Tis a quandary for sure.
UPDATE– It turns out New Urban Builders has purhcased the tower property, see comments.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea64c07c1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGerman broadcaster RTL here reported how Northern Hemisphere snow mass has reached the highest level in years.

Image: Finnish Meteorological Institute
The Finnish Meteorological Institute (FMI) reports the total amount of snow in the northern hemisphere this winter season has been well above the long-term average from 1982 to 2012.
This will come as a surprise to Europeans, who have seen one of the mildest winters on record. According to RTL, “In those places where it was cold enough for snow at all, there was a lot of snow. The snow is meters high, higher than usual.”
Snow cover trending upwards since 1990


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Looking at the northern Hemisphere snow cover charts from Rutgers University Global Snow Lab, northern hemisphere snow cover (area) has been on the rise since reaching a low in 1990.

Chart: Rutgers University Snow Lab.
Record-breaking snow across Montana and South Dakota
Meanwhile weather site Electroverse reports of “record breaking February snowfall” burying Montana and South Dakota. According to Electroverse, “The cold times are returning” due to reversing natural cycles.
Record Breaking February Snowfall Buries the U.S. States of Montana and South Dakota



		jQuery(document).ready(function(){
			jQuery('#dd_51c08b0b0f83667272b40339623e8ea9').on('change', function() {
			  jQuery('#amount_51c08b0b0f83667272b40339623e8ea9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Wind energy has been identified as having an important role to play in the world’s move towards a low-carbon future. But, due to short-term planning rules, it may not have as big a part as it could in the UK’s own sustainable energy generation. To date, when most UK wind farms were under development, temporary planning consent of 25 years was granted. Under the terms of this consent, when the two and a half decade period comes to an end, the turbines have to be removed and the land returned to its previous use. Now, a significant number of the country’s wind farms are starting to reach the end of their permission period, 62 wind farms in England, Wales and Scotland are aged 15 and over and 22 of these are more than 20 years old. If existing sites are removed without replacement this could decrease the overall amount of energy generated from UK renewables.  There are other problems too: the government has warned that there is a risk of equipment being abandoned on some of the oldest sites, because some original planning consents failed to specify the removal of all of the infrastructure. In some cases, large equipment and cables do not have to be removed. And in 2015, the government created major planning hurdles for onshore wind farms and ended subsidies. As a result, there has been a 94% drop in applications to build new wind farms in England alone. But all is not lost. To combat the issue, in July 2018 it was announced that the repowering of existing wind turbines would not be subject to the same planning hurdles as new sites. And our analysis has now confirmed that repowering can massively increase the energy output of the UK’s wind farms. There are three options for wind farms that reach the end of their planning consent. First, they can be decommissioned, the infrastructure removed and the land returned to its previous condition. Another option is for the operational life of the existing turbines to be extended. This involves getting planning permission to keep the turbines in place, usually for another five to 10 years. Or the farm can be repowered, which means old turbines will be replaced by newer ones. So far, most wind farms that have reached the 25 year limit have been repowered and given a 25-year consent for the new turbines. Or they have had the permission for the original turbines extended, allowing them to continue working for up to 10 additional years. Across Britain, 23 sites have already been repowered and at least three have extended their life, while only two have been decommissioned. Repowering sites has proved to be a great opportunity to increase the energy produced. On average repowering has increased the output of sites by 171%. The following table shows the potential increase in power (measured in megawatts) from sites repowering in England, Wales and Scotland within the next 10 years. In all, 54 sites are due to come to the end of their life within five years, and 161 more within ten years. This data is based on 23 sites that on average (excluding sites that repowered early for technical reasons) repowered after 18 years of operation. Although not all farms will repower this early, or have the same increase in output, these estimates show that repowering has the potential to greatly boost wind farms’ contribution to UK energy supply. If all 215 sites did repower at this level within the next ten years the energy increase will be enough to power an additional 3.8 million homes. Repowering doesn’t come without its challenges, however. It can change how a wind farm looks, which is not always popular with the general public. On average, repowering has reduced the number of turbines on a site by 24%, but turbines have got 89.5% taller. This has caused difficulties for local planning authorities when assessing the visual impact of planning applications.  Public opinion and the benefits for local communities that come from repowering should be an important consideration – but this hasn’t played a major part in approval decisions, and the public response to repowering schemes has varied. Some approved schemes – such as St Breock wind farm in Cornwall – have received significant support, while others (Ovenden Moor in West Yorskshire, for example) have come up against local opposition.  Crucially, despite the evident benefits to repowering wind farms, there is still not enough governmental guidance to ensure that decisions can be made quickly and fairly in a way that balances energy production with local environmental, social and economic benefits.  This is not an issue that the country can sit on. With so many wind farms approaching their 25th year, we need to act quickly in order to maximise the potential benefits to energy generation and carbon reduction targets."
"The crisis is not imminent. The crisis is here. The recent infernos in Australia; the storms and floods in Brazil, Madagascar, Spain and the US; and the economic collapse in Somalia, caused in part by a devastating cycle of droughts and floods, are not, or not only, a vision of the future. They are signs of a current and escalating catastrophe. This is why several governments and parliaments, the UK’s among them, have declared a climate emergency. But no one in government acts as if it is real. They operate within the old world of incremental planning for a disaster that has yet to arrive.  Nowhere is this clearer than in the reports of the Committee on Climate Change (CCC), the official body that began with such hope and promise of holding the government to account, but that now seems to have abandoned scientific realities in favour of political priorities. Its latest report, on changing the UK’s land use, is so unambitious that, in some respects, it would take us backwards. For example, it calls for a 10% reduction in cattle and sheep numbers over the next 30 years. But it admits that over the past 20 years, their numbers have declined by 20%, so this would involve a slowing of the trend. Cultured meat and milk could replace these sectors almost entirely by 2050. The report makes no mention of rewilding or natural regeneration. The only means it proposes by which trees should return to the land is planting. This is often a slower, more expensive and less effective way of restoring habitats and sucking carbon out of the atmosphere than removing livestock or controlling deer numbers and allowing trees to return by themselves. Its target for reforestation is so feeble that the UK would still have less than half the average current European forest cover by 2050. One of the reasons for this timidity is its preposterous assumption that if land is unsuitable for commercial forestry, it’s unsuitable for trees. There are plenty of places where trees grow well, store carbon and provide magnificent habitats, but won’t produce straight 50-foot poles. The CCC envisages not wild woods, but plantations, whose purpose is the discredited policy of “bioenergy with carbon capture and storage”. This means growing wood to burn in power stations, then capturing and burying the carbon emissions. It is likely to cause more harm than good. Could the committee’s enthusiasm have anything to do with the fact that one of its members works for Drax, the energy company pioneering this disastrous technology? Throughout the report, business appears to come first; nature and climate last. All this, the CCC says, is consistent with the target it has set for the government, of net zero greenhouse gas emissions by 2050. It tells me that the rationale for this target “remains valid today”, meeting the UK’s obligations under the Paris agreement. This agreement commits governments to seek “to limit the temperature increase to 1.5C above pre-industrial levels”. But in November, the UN published a report showing that preventing more than 1.5C meant cutting greenhouse gas emissions by 7.6% every year between now and 2030: a much steeper trajectory than the CCC’s. The committee has set the wrong target, for the wrong date. But I think the problem runs deeper than this. It’s not just the target that’s wrong, but the very notion of setting targets in an emergency. When firefighters arrive at a burning building, they don’t set themselves a target of rescuing three of the five inhabitants. They seek – aware that they may not succeed – to rescue everyone they can. Their aim is to maximise the number of lives they save. In the climate emergency, our aim should be to maximise both the reduction of emissions and the drawing down of carbon dioxide already in the atmosphere. There is no safe level of global heating: every increment kills. Maximisation is implicit in the Paris agreement: it requires governments to pursue “the highest possible ambition”. In its land-use report, the CCC repeatedly admits that it could go further, but insists it doesn’t need to, because its policies will meet the target. The target has supplanted the ultimate objective, which is to respond appropriately to the climate emergency. This is a classic vindication of Goodhart’s law: “When a measure becomes a target, it ceases to be a good measure.” We are all familiar with the absurdities of target culture. We know how, in many workplaces, the target becomes the task. We know how official targets for depriving people of social security ruined thousands of lives. We know that the Windrush scandal – the persecution and wrongful deportation of people legally entitled to reside in the UK – was caused in part by the Home Office target for “enforced returns”. We know how targets encourage people to game the system, as hospital administrators do with their waiting lists, and cause Kafkaesque nightmares of overzealous officialdom, as David Boyle documents in his new book, Tickbox. But less discussed is the way in which targets can encourage officials to underperform. As soon as you set a target, you pull back from maximisation. Even if you say “this target is the minimum”, as the CCC does, politicians treat it as merely the line they need to cross. At this point, they fulfil their legal duty, even if they fail to fulfil their wider duty of care. Is a policy of maximisation possible? It is not only possible, it’s already happening, in exactly the wrong place. The 2015 Infrastructure Act introduced a legal duty to “maximise the economic recovery” of petroleum in the UK. If drilling companies fail to maximise their extraction of fossil fuel from an oilfield, they will be forced to surrender their licence to operate. In other words, while the government observes a legal minimum (the CCC’s target) for reducing greenhouse gases, it observes a legal maximum for increasing them. The appropriate response to the climate emergency is a legal duty to maximise climate action. The CCC’s board should be disbanded and replaced by people whose mandate is rigorously to explore every economic sector in search of the maximum possible cuts in greenhouse gases, and the maximum possible drawdown. We have arrived at the burning building. The only humane and reasonable aim is to rescue everyone inside. • George Monbiot is a Guardian columnist"
"
Share this...FacebookTwitterEurope storm leads to negative electricity prices
By Die kalte Sonne
(Translated/edited by P. Gosselin)
It almost hurts a little that “specialist for renewable energies” (own claim on Twitter) Prof. Volker Quaschning gets mentioned here so often. This is simply due to the absurd tweets the man continuously puts out.
His latest prank has to do with the storm Sabine, which earlier this week swept across Germany. It supplied a lot of energy in the form of wind, which made the wind turbines rotate strongly.
Even in the otherwise regulated electricity market, the laws of the market cannot simply be levered out. Supply and demand determine the price. If supply is higher than demand, the price falls.
In the case of electricity, even money might be included with the product when this electricity is purchased, meaning negative prices. Electricity is an extremely perishable “commodity”, it must be consumed at the moment of production. However, the “expert” Quaschning does not blame this oversupply and the negative prices on the volatile wind power plants, but rather on nuclear power and coal. They deliver very reliably and not wildly fluctuating like wind power.
Prof. Volker Quaschning tweeted in response to the negative electricity prices from overproduction which Germany saw during the recent storm:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sturm und viel Wind sorgen wieder für negative Börsenstrompreise. Ein klares Zeichen, dass Kohle- und Atomkraftwerke zu unflexibel sind und nicht als Backup für erneuerbare Energien taugen. Wir brauchen darum einen schnelleren #Kohleausstieg. #FridaysForFuture #Scientists4Future pic.twitter.com/w7CRrEruoq
— Volker Quaschning (@VQuaschning) February 10, 2020

In English: “Storm and lots of wind are again causing negative stock market electricity prices. This is a clear sign that coal-fired and nuclear power plants are too inflexible and are not suitable as back-up for renewable energies. We therefore need a faster #coal exit.
#FridaysForFuture #Scientists4Future.”
A logical train of thought actually would have been to realize that highly volatile power sources such as wind and the simultaneous provision of base load are difficult to reconcile. Unfortunately, the energy source gas is also being massively fought by people like Quaschning, although it is more flexible and at the same time more CO2-friendly. In any case, however, it is only a crutch that might have to supply a great deal of energy, namely when we have the well-known lulls in wind and sun.
Every wind turbine and every photovoltaic system needs a backup. And anyone who has ever wondered why the wind countries of Denmark and Germany have such high electricity prices knows the reason. We are paying for a double power infrastructure. The prices will not decrease with an increasing share of renewable energies, but rather will continue to rise.
Share this...FacebookTwitter "
"

It’s not often that I get to have a glass of wine at a restaurant in Chico and have WiFi Connectivity at the same time. Having both of these at the newly opened Market Cafe where Highway 32 meets 99 I decided I’d do my first “Live” blog entry.
Restaurants come and go in Chico, some don’t ever rise above the level of having an “open” sign. Finding new ones with ambiance and class is a treat. Finding one that gives every customer a free appetizing plate after 5 is even better.
This restaurant used to be “The Bean Scene” which was started by a couple of people I used to call “The Evil Blonde Ladies” because they’d come into Bidwell Perk and take notes on that business just before opening up their own gig. They didn’t make it, partly because it took them 20 minutes to toast a bagel.
So seeing a new restaurant in Chico is always a good thing, as on Friday and Saturday nights it’s often impossible to find a classy place to eat that isn’t fully booked. So while I’m never mentioned any Chico business before, I thought this one was worth a mention not only because they are new, but because its run by locals, Bob and Patricia Johansen.
I’m partial to wine blends, as they tend to take the edge off of the aftertaste. Tonight I sampled one of the best blends I’ve had in a very very long time. Its called Falling Star Merlot-Malbec (shown above), and I gotta tell you it beats my former favorite “Clous du Bois Marlstone” by a long shot, and costs about 66% less. ($19/bottle -vs- $60)
It’s worth checking out, if nothing else for the free appetizer and WiFi.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8aef9e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

We all have something to be thankful for today. Whether you are stuffing yourself, or stuffing your agenda, be thankful we live in a country that allows us to even have your choice of stuffing.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9f04bbf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I’ve long argued that enviros don’t have anywhere near the electoral clout most people think and that no one is going to gain much political capital donning the garb of “Mr. Green Jeans.” Today, the trade publication Greenwire (subscription required) agrees. And believe me, these are the last people who want to make this argument. 



**CAMPAIGN 2006: Voters cool to climate issue in torrid midterm races**   
  
  
**Darren Samuelsohn, _Greenwire_ senior reporter**   
  
  
Five Northeastern Republicans facing fierce re‐​election battles turned just before the latest congressional recess to global warming in hopes the issue would boost their chances in their suburban House districts.   
  
  
But the lawmakers apparently got little traction from climate change in a campaign dominated by voter concerns about the Iraq war, President Bush’s unpopularity and overall dissatisfaction with Republican leadership.   
  
  
“It’s been very difficult for any of these incumbents whose problems are bigger than themselves, or whose problems have been themselves,” said Bernadette Budde, a senior vice president for the Business and Industry Political Action Committee. “They have had a hard time changing the subject.”   
  
  
The five — Reps. Curt Weldon (Pa.), Mike Fitzpatrick (Pa.), Christopher Shays (Conn.), Nancy Johnson (Conn.) and Rob Simmons (Conn.) — cosponsored in September what some consider the most aggressive bill to date aimed at limiting heat‐​trapping greenhouse gas emissions. The bill’s lead sponsor is Rep. Henry Waxman (D‐​Calif.), the presumed new chairman of the House Government Reform Committee if his party wins a majority of House seats.   
  
  
“Doing it before Congress goes off to campaign is telling,” said Howard Reiter, chairman of the political science department at the University of Connecticut. He added that global warming is a nuanced subject that comes with an important caveat: It may require constituents to make sacrifices in their day‐​to‐​day lives.   
  
  
“The problem with global warming is its incremental,” Reiter said. “It’s not as if there’s an immediate crisis people can see.”   
  
  
Massie Ritsch, spokesman for The Center for Responsive Politics, a nonpartisan organization that tracks campaign spending, said the recent media frenzy over climate change — from Hollywood‐​style documentaries to mainstream press coverage — did little to stir voters this year. “For all of the attention Al Gore’s movie got, it hasn’t stayed a major election issue,” he said.   
  
  
The lack of voter interest in climate change is not due to a lack of effort from environmental groups .…   
  
  
_Reporter Michael Burnham contributed to this report._
"
nan
"
Warren Meyer, one of the first surfacestations.org volunteers, delivered Tucson for us Saturday. It was discovered during an analysis of climate stations around the USA on the Climate Audit blog that Tucson had the greatest positive temperature trend for any USHCN station after the TOBS adjustment was applied. The TOBS adjustment corrects for differences in local times of observation of temperature by the observer. The picture says it all:

Yes folks, this is an official climate station of record, the temperatures it measures go into our National Climatic Database and are used in research such as the graph produced by NASA Goddard Institute for Spaceflight Studies here:

There’s a British word that has been bandied about to describe the reaction to pictures like this one: “gobsmacked”. The word applies even more so since this station is operated by science faculty members at the University of Arizona.
They are so proud of this station they even had a sign made for it to hang on the chain link fence enclosure:

The complete photo essay is available at the Tucson album at www.surfacestations.org The satellite and aerial photo images there are telling of the environment being measured.

Besides the obvious questions like “why is it in the middle of a parking lot?” and “why would scientists who should know better allow such a bizarre siting for a USHCN climate station of record?” Then there is this burning question: “Why did they go to the trouble of installing a precision aspirated temperature sensor and then not even bother to place it at the standard observing height?”. 

It appears that the Stevenson Screen serves no other purpose except as an equipment holder, as Warren Meyer reports the Stevenson Screen to be empty. Originally the inside standard mounting board for the mercury max/min thermometers were mounted about 1.5 foot higher than the air inlet of the precision aspirated temperature sensor. So the lower mounting height for the precision sensor adds a positive bias.
Is there no diligence left in basic measurement? Is this what they teach in college science departments these days?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4ef3691',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Years ago in the 50’s, nuclear energy was the big idea of the time. Clean and nearly limitless energy for everybody was the promise, and ideas such as having a home nuclear power station were even floated to provide independence from the power grid.. I was so impressed with this idea that I did lots of work in nuclear physics even at my own high school, building a 1 MEV cyclotron (a form of atom smasher) which I powered with my dad’s arc welder.
After winning my state science fair, I went to the national science fair with it, but didn’t win there, partly because I think I terrified some of the judges when I fired it up. Getting it there was quite a job, lugging a 1200 pound steel electromagnet cross country and setting it up again was a good lesson in logistics.
I even started out in college in nuclear engineering, then 3 Mile Island happened and I saw the handwriting on the wall. Nuclear fission, which is what we use now, creates all sorts of radioactive isotopes as byproducts of the fission, and they remain radioactive for thousands of years, making disposal/storage a problem.
Nuclear FUSION on the other hand, doesn’t have such byproduct problems, and its been the holy grail of clean nuclear power for 50+ years, but it’s been elusive because its so difficult to do and still produce a net power gain.
That’s why I found this story from the Detroit Free Press interesting, it hit close to home, building a nuclear fusion reactor in your basement brings back memories of my own tinkering. Maybe someday, we’ll see a safe and clean fusion reactor in your home. If I’m still of this earth, I’ll be the first to buy one.
The work this young man did in his basement was a continuance of work done by eccentric inventor Philo T. Farnsworth, who is credited with inventing television, though RCA clearly stole the idea from him and commercialized it.
Farnsworth’s approach to fusion has been dubbed “Inertial Electrostatic Confinement” or “IEC” for short. (VERY) simply put, the process uses forces within the atomic particles themselves to bring them close enough to fuse. The more common approach uses tremendous external forces to achieve the same effect. These enormous machines employ powerful magnetic fields and the method is called “magnetic confinement” Literally billions of dollars have been spent in the last thirty years with little to show in the way of meaningful results. After thirty years, the “experts” still say that a practical fusion power plant is still – would you believe? – at least another thirty years away.
This article underscores why we need to encourage more science and technology
for our youth. The US keeps slipping behind.


TEEN GOES NUCLEAR: He creates fusion in his Oakland Township home
November 19, 2006
BY GINA DAMRON
On the surface, Thiago Olson is like any typical teenager.
But to his friends, Thiago is known as ""the mad scientist.""

Thiago Olson, 17,
stands near his nuclear fusion reactor, which he calls ""the Fusor,"" at
home in Oakland Township on Friday. After more than two years and 1,000 hours of
research, the Stoney Creek High School senior, with a little help from his dad,
built the machine. (PATRICIA BECK/Detroit Free Press)
In the basement of his parents’ Oakland Township home, tucked away in an area
most aren’t privy to see, Thiago is exhausting his love of physics on a project
that has taken him more than two years and 1,000 hours to research and build —
a large, intricate machine that , on a small scale, creates nuclear fusion.
Nuclear fusion — when atoms are combined to create energy — is ""kind
of like the holy grail of physics,"" he said.
In fact, on www.fusor.net,
the Stoney Creek senior is ranked as the 18th amateur in the world to create
nuclear fusion. So, how does he do it?
Pointing to the steel chamber where all the magic happens, Thiago said on
Friday that this piece of the puzzle serves as a vacuum. The air is sucked out
and into a filter.
Then, deuterium gas — a form of hydrogen — is injected into the vacuum.
About 40,000 volts of electricity are charged into the chamber from a piece of
equipment taken from an old mammogram machine. As the machine runs, the atoms in
the chamber are attracted to the center and soon — ta da — nuclear fusion.
Thiago said when that happens, a small intense ball of energy forms.
He first achieved fusion in September and has been perfecting the machine he
built in his parents’ garage ever since.
This year, Thiago was a semifinalist for the Siemens Foundation’s National
Research Competition. He plans to enter the Science and Engineering Fair of
Metropolitan Detroit, which is in March, in hopes of qualifying to be in the
Intel International Science and Engineering Fair in New Mexico in May.
To his mom and dad, he’s still reminiscent of the 5-year-old who toiled over
a kid-friendly chemistry set and, then at age 9, was able to change the battery
in his older brother’s car.
Now, in a small room in the basement, Thiago has set up a science lab —
where bottles marked ""potassium hydroxide"" and ""methanol""
sit on shelves and a worn, old book, titled ""The Atomic Fingerprint:
Neutron Activation Analysis"" piled among others in the empty sink.
Thiago’s mom, Natalice Olson, initially was leery of the project, even though
the only real danger from the fusion machine is the high voltage and small
amount of X-rays emitted through a glass window in the vacuum chamber — through
which Olson videotapes the fusion in action..
But, she wasn’t really surprised, since he was always coming up with lofty
ideas.
""Originally, he wanted to build a hyperbaric chamber,"" she said,
adding that she promptly said no. But, when he came asking about the nuclear
fusion machine, she relented.
""I think it was pretty brave that he could think that he was capable to
do something so amazing,"" she said.
Thiago’s dad, Mark Olson, helped with some of the construction and electrical
work. To get all of the necessary parts, Thiago scoured the Internet, buying
items on eBay and using his age to persuade manufacturers to give him discounts.
The design of the model came from his own ideas and some suggestions from other
science-lovers he met online.
Someday, he hopes to work for the federal government — just like his
grandfather, Clarence Olson, who designed tanks for the Department of Defense
after World War II. Thiago, who is modest and humble about his accomplishment,
said he knew from an early age what he would do for a living.
""I was always interested in science,"" he said. ""It’s always
been my best subject in school.""
But, his mom had other ideas.
""I thought he was going to be a cook,"" Natalice Olson said,
""because he liked to mix things.""


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9e258bc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Image: Average temperatures warmed in nearly all parts of California between 1950 to 2000. Image credit: NASA/JPL/Cal State L.A Click for Larger Image
Average temperatures in California rose almost one degree Celsius (nearly two degrees Fahrenheit) during the second half of the 20th century, with urban areas leading the trend to warmer conditions, according to a new study by scientists at NASA and California State University, Los Angeles. Results of the study appeared in the journal Climate Research.
But 50 years of temperature trends hardly proves anything relevant about climate change, other than its gotten warmer in the past fifty years. 50 years in terms of our planet and the suns processes is a blink. I have to think that because NASA chose to co-author this paper with researchers at California State University, that some of the statewide “global warming as man-made problem bias” crept into the thinking for the purpose of this paper, i.e. “we need another study to show that its getting hotter so action is justified”.
What is troubling about this study is that many of California’s historical climatological stations, when done on a 100 year trend, rather than a 50 year trend, show a net cooling over the period, or a reversal of trend. The northern Sacramento Valley has very few reporting stations that go back 100 years, so I only have 4 data points, but it makes me wonder just what data the NASA/CSU study used to come to the conclusion that our area has warmed 1.1 degrees F over the last 50 years.
I’ve prepared some side-by-side graphs below of Sacramento Valley stations to illustrate that point:

My data source: U.S. Historical Climatology Network (USHCN) Data Set
Yet the NASA/CSU paper claims “The only area to cool was a narrow band of the state’s mainly rural northeast interior“. None of the stations above are in that area, but are in the North Sacramento Valley.
Even odder than that, cold and snowy Mt. Shasta, where you’d expect to hear about depleted snowpack, it’s melting glacier on the side of the mountain, and other “signatures” of “global warming” shows a significant drop in temperatures over the last 50 years. yet the NASA/CSU study for that area concludes that a 2.1 degree F rise in temperature occurred.

Granted a few data points don’t equal a complete study, but the fact that I’ve been able to find and plot in a couple of hours, several places that don’t match the trends in the NASA/CSU study calls their methodology into question. Note the cities I used are all small rural cities, but the NASA/CSU study plotted major, medium, and minor cities in California to draw their conclusions. From their own paper they admit that the areas that have grown the most have shown the greatest temperature increases:
Southern California had the highest rates of warming, while the NE Interior Basins division experienced cooling. Large urban sites showed rates over twice those for the state, for the mean maximum temperatures, and over 5 times the state’s mean rate for the minimum temperatures. Average temperatures increased significantly in nearly 54 percent of the stations studied, with human-produced changes in land use seen as the most likely cause. The largest temperature increases were seen in the state’s urban areas, led by Southern California and the San Francisco Bay area, particularly for minimum temperatures.
For example, look at Pasadena, CA once a small city itself,  but in the last 100 years it became a dot in the sea of the second largest American City, Los Angeles. It’s temperature trend, unsurprisingly, is sharply upward, for both the 50 and 100 year trends. Its drowning in a sea of asphalt and concrete, is it any wonder it shows a temperature increase?

The inescapable conclusion is that the NASA/CSU study is plotting the effects of urban heat islands, and applying that trend to the entire landmass of California to reach the conclusions they have mapped onto the state map of temperature trend they present.
A simple filtering based on urban growth factors would yield a temperature map with a far different result.
To their credit though, they recognize this fact:  “If we assume global warming affects all regions of the state, then the small increases our study found in rural stations can be an estimate of this general warming over land. Larger increases would therefore be due to local or regional changes in land surface use due to human activities.” 
For the most part, “urban warming” has dwarfed “global warming” in its magnitude, a fact that is lost on some who look at temperature data from weather stations worldwide and treat them all equally in the quest to prove a theory.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea757c22d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Greta Thunberg has criticised world and business leaders for ignoring calls to break away from fossil fuels, as young people protested in Davos over the climate emergency. Speaking on the final day of the World Economic Forum, the 17-year old climate campaigner said leaders were not reacting to the crisis, and were not being held accountable for their inaction. “Before we came here we had a few demands for this WEF and of course those demands have been completely ignored, but we expected nothing less,” said Thunberg, speaking before marching through Davos with fellow climate activists. “As long as the science is ignored, and the facts aren’t taken into account, and the situation is not treated as a crisis, then world and business leaders can of course continue to ignore the situation,” she said. Thunberg, who appeared on two panels at the forum, has called for an immediate end to investments in fossil fuel exploration, fossil fuel subsidies and for investors to ditch fossil fuel assets. But while many delegates spoke about the importance of climate issues, actual action was harder to find. Luisa Neubauer, a 23-year-old German climate activist, said she had met the chief executive of Siemens, Joe Kaeser, at Davos this week to press him to abandon a contract to supply rail signalling to a mine in Australia. The Adani project would be one of the world’s largest coalmines, and Neubauer said such fossil fuel activity needed to be curbed by companies and investors. Otherwise the Paris agreement’s goal of keeping global heating to 2C this year could not be met, she said. Davos is a Swiss ski resort now more famous for hosting the annual four-day conference for the World Economic Forum. For participants it is a festival of networking. Getting an invitation is a sign you have made it – and the elaborate system of badges reveals your place in the Davos hierarchy. The meeting is sponsored by a huge number of international banks and corporations. For critics, “Davos man” is shorthand for the globe-trotting elite, disconnected from their home countries after spending too much time in the club-class lounge. Others just wonder if it is all a big waste of time.  The 2020 meeting is being advertised as focusing on seven themes: Fairer economies, better business, healthy futures, future of work, tech for good, beyond geopolitics and how to save the planet. Young climate activists and school strikers from around the world will be present at the event to put pressure on world leaders over that last theme.  “If you add up all the contracts that have been signed by today on fossil fuel projects, we will be far beyond 2C,” said Neubauer. “Some of those contracts, if not all of them, will have to be cancelled if we want to actually do something about Paris. “This is not a radical demand, this is a rational demand,” she added. On Thursday, the US Treasury secretary, Steven Mnuchin, said climate activists should recognise the impact of fossil fuel divestment on jobs, and said Thunberg should go study economics before offering advice. Thunberg said such barbs, like Donald Trump’s criticism, had no effect on her. “We are being criticised like that all the time. If we cared about that, we wouldn’t be able to do what we do,” she said, before leading a Fridays for Future protest in the ski resort. Some climate actions were announced at Davos, including a push to plant 1tn trees, which Donald Trump backed. Thunberg warned earlier this week that planting trees was not enough to tackle the climate crisis. But Micah White, the co-founder of the Occupy Wall Street movement, said the trillion tree campaign would work as a Trojan horse to help mobilise people to fight the climate emergency. “I love the trillion tree campaign, because hundreds of millions of people will need to be mobilised at the grass roots,” he said on the sidelines of WEF. “To actually mobilise that number of people, and plant that number of trees, will be such a systemic disruption of our day-to-day lives that it will be revolutionary.” White said corporations would have to tell staff, “You don’t have to come into work this week, we’re all planting trees”, and governments will have to deposit trees in all the schools, to actually carry out the plan. White attended Davos with a manifesto titled “An Alliance Of Opposites”. It proposed that protest movements, governments, activists, industry, and civil society could work together on the climate emergency. THIS IS WHAT I SAID TO ELITES AT DAVOSIt was a surreal experience to hand this candidly written activist manifesto to a few of the world’s most powerful people at the World Economic Forum in @Davos. https://t.co/dmuwyU0NJP#WEF2020"
"Sun-kissed walkways in the sky, platefuls of seafood ceviche, a private helicopter pickup from the beach – the Instagram account of Danish architect Bjarke Ingels has unfolded like an escapist travelogue epic in recent weeks, as his adventures in Latin America have taken their place in his dizzying globetrotting itinerary. But there is one photograph he hasn’t been so keen to share with his 730,000 followers: of him standing next to Jair Bolsonaro, Brazil’s far-right president, with the uneasy smile of a man who’s just secured his latest big commission from another unsavoury despot, in this case one who has boasted of being “proudly” homophobic. According to a statement from Brazil’s ministry of tourism, Ingels visited Brazil to tour several states and discuss strategies for developing sustainable tourism on its north-east coast, in partnership with the Nômade Group, which recently built an eco-conscious luxury resort in Tulum, the ruins of a Mayan walled city in Mexico.  “They’ve pioneered this incredible barefoot light-impact environment,” says Ingels, speaking by phone from Santiago in Chile. “It’s a model of tourism development that doesn’t replace the forest or the sand, but strengthens and preserves it. It’s a very welcome alternative to the kind of high-rise hotels that spring up on the beach in many places.” Such an eco-conscious approach doesn’t seem to chime with Bolsonaro’s reactionary regime. Since he came to power last January, Brazil’s president has confirmed every fear that he would be a “Trump of the tropics”. He has appointed climate-change deniers to prominent roles, dismissed deforestation statistics as fake news, and sacked the head of the institute that undertook the research. Last year, he played down the worst fires for a decade in the Amazon, calling the global outcry the result of “deceitful” media hype. He once said he would prefer a dead son to a gay one, rails against Brazil becoming a “gay tourism paradise”, and his recent introduction of abstinence-based sex education has been likened to “going back 40 years”. So what is Ingels, self-styled seer of a progressive liberal world, doing with such a figure? “I’m happy to share the ideas and ideals that I hold with any government willing to listen,” he says, “especially if they hold different beliefs. If there’s a responsibility that comes from the creative platform we’ve created, it is to use that platform to change the world for the better.” He says that the political leaders of the north-eastern Brazilian states they visited are “from the opposite end of the political spectrum” to Bolsonaro, adding that ultimately “great ideas transcend political parties”. Over the last few years, Ingels has gone from being the mischievous young designer of mountain-shaped apartment blocks on the outskirts of Copenhagen, to becoming the go-to celebrity architect for some of the world’s biggest corporations. Now 45, he has recently popped up as the face of everything from Hyperloop to WeWork, and is currently designing a city-sized HQ for Google in California. Then there’s his actual city for Toyota in Japan, right under Mount Fuji.But most audacious of all, he tells me, is the masterplan he is now working on for the whole of Earth – provisionally titled The Masterplanet. His status as big tech’s cheeky go-to boy is perhaps what also makes him so attractive to a growing client base of autocrats. Both kinds of patron share an insatiable desire to shape every aspect of the world to their exacting vision, be it a future of autonomous vehicles, or a world where the Amazon rainforest is seen as a “virgin” that should be “exploited”, as the climate-crisis-denying Bolsonaro has put it. BIG, which stand for Bjarke Ingels Group, is also engaged in several major projects in Saudi Arabia, including a new leisure resort city called Qiddiya, 45km from Riyadh, billed as a new “capital of entertainment”; there’s also a confidential project that Ingels describes as “a human-made ecosystem that is as close to a utopia as you dare imagine”. Would this be a utopia that had – in accordance with Saudi law – flogging for drunkenness, amputation for robbery and extrajudicial killing for anyone criticising the regime? “I do sincerely believe that the urban transformation of Saudi Arabia that we’re taking part in is part of paving a path to a clearly needed social and cultural reform of the country,” says Ingels. Following the state-sanctioned murder of journalist Jamal Khashoggi in 2018, he says his office debated whether to pull out of the projects (as Norman Foster did), but decided they could do more good by continuing to engage. “The road to ethical impact as an architect is to [propose] the future we want to companies and governments,” he says, “even if they have different views. We have to embrace our differences if we want to create a future that is different.” It’s easy to dismiss his words as naively optimistic, like a TED Talk for dictators, but he has a point – architecture and cities long outlast the ideologies that created them. But they do also reinforce the politics of their sponsors. Ingels is by no means alone. French architect Jean Nouvel has become the darling of the autocratic Gulf monarchs, designing the Louvre Abu Dhabi, the National Museum of Qatar and now a luxury resort in Saudi Arabia. He has in the past dismissed the issue of human rights as “an old question”, echoing the response of the late Zaha Hadid, architect of the vulva-shaped stadium in Qatar for the 2022 World Cup. “I have nothing to do with the workers,” Hadid said when challenged about the fact that hundreds of migrant workers had died on construction projects in Qatar. “I think that’s an issue the government – if there’s a problem – should pick up. It’s not my duty as an architect to look at it.” Dutch architect Rem Koolhaas was equally frank about his work in China, when asked about designing the enormous HQ of China Central Television, the chief organ of state propaganda. “A position of resistance seems somehow ornamental,” he said. “On our own, we can at most have good intentions. But we cannot represent the public good without the larger entity, such as the state.” Revealing the allure of totalitarian regimes, he added: “To make matters worse, the more radical, innovative and brotherly our sentiments, the more we architects need a strong sponsor.” Sharing Ingels’ belief that architects can effect change from within, Jacques Herzog compared his work on the Birds Nest – the 2008 Beijing Olympic stadium – to designing a “Trojan horse”. He told German newspaper Der Spiegel that “only an idiot” would have turned down the project on moral grounds, adding that his firm’s design creates “all kinds of niches” and “meeting places” around the edge of its latticework structure, hidden away from government surveillance. This is a recurring theme. Whenever the topic of ethics comes up, most architects who work in repressive climates believe their projects can transcend the abuses of the host regimes and make everyday life a bit better for the people who live there. The dilemma is whether to boycott or engage. Should a well-intentioned architect avoid working with any system they disagree with – or hope that the conditions of workers, or freedom of speech, might be improved by using their platform to raise these issues? “It’s just surface rhetoric,” says Jeremy Till, dean of Central Saint Martins art school in London and author of Architecture Depends, which examined the discipline’s relationship with money and power. “The idea that a single building is going to be emancipatory for a whole totalitarian state is ridiculous.” Till was involved in the RIBA’s recent ethics report, which asserted an “unequivocal commitment” to placing public interest, social purpose and sustainable development at the heart of RIBA activities. He is particularly critical of some of the practices that have signed the recent Architects Declare manifesto, pledging to advocate for low-carbon, eco-friendly development – while also designing airports. “There are some architects who abdicate their responsibility,” he says. “I do understand that – but then they shouldn’t virtue-signal by signing up to these agreements.” As for Ingels’ latest foray into the tropics, Till doesn’t hold out much hope. “It’s a fantasy that by doing a nice greenwash project in the north-east of Brazil, you are going to save the nation from fascism.” Ingels differs. “The proof will be in the pudding,” he says. “I don’t know if we’re going to succeed in bringing positive change, or come up with a great alternative to the traditional developments that destroy the landscape and debase the local community. But we will definitely not succeed if we don’t even try.”"
"

New York Attorney General Eric Schneiderman demands out-of-state charities disclose all donors for his inspection. He does not demand this of all charities, only those he decides warrant his special scrutiny. Schneiderman garnered national attention for his campaign to use the powers of his office to harass companies and organizations who do not endorse his preferred policies regarding climate change. Now, it seems he seeks to do the same to right-of-center organizations that might displease him. Our colleague Walter Olson has cataloged Schneiderman’s many misbehaviors.   
  
He’s currently set his sights on Citizens United, a Virginia non-profit that produces conservative documentaries. While Citizens United has solicited donations in New York for decades without any problem, Schneiderman now demands that they name names, telling him who has chosen to support the group. Citizens United challenged this demand in court, arguing that to disclose this information would risk subjecting their supporters to harassment and intimidation.   
  
These fears are not mere hyperbole. If the name Citizens United rings a bell, it’s because the organization, and the Supreme Court case of the same name, has become the Emmanuel Goldstein of the American left, complete with Democratic senators leading a ritualistic two minutes hate on the Senate floor. In 2010, the Supreme Court upheld its right to distribute _Hillary: The Movie_, and ever since “Citizens United” has been a synecdoche for what Democrats consider to be the corporate control of America. Is it unwarranted to think that their donors might be subjected to the sort of targeted harassment suffered by lawful gun owners, or that Schneiderman might “accidentally” release the full donor list to the public, as Obama’s IRS did with the confidential filings of gay marriage opponents?   
  
The Supreme Court has long recognized the dangers inherent in applying the power of the state against the right of private association. The cornerstone here is 1958’s _NAACP v Alabama_ _._ For reasons that hardly need be pointed out, the NAACP did not trust the state of Alabama, in the 1950s, to be good stewards of its membership lists. “Inviolability of privacy in group association may in many circumstances be indispensable to preservation of freedom of association, particularly where a group espouses dissident beliefs,” wrote Justice John Marshall Harlan II, who went as far as to compare such demands to a “requirement that adherents of particular religious faiths or political parties wear identifying arm-bands.” More recently, Justice Alito pointed out in a similar context that while there are undoubted purposes served by reasonable, limited disclosure requirements, the First Amendment requires that “speakers must be able to obtain an as-applied exemption without clearing a high evidentiary hurdle” regarding the potential harms of disclosure.   
  
But the Second Circuit Court of Appeals has decided it knows better than the Supremes. On Thursday, it ruled that Citizen United’s challenge should be thrown out without even an opportunity to prove their case. In the process, it effectively turned _NAACP_ into a “Jim Crow” exception to a general rule of unlimited government prerogative to panoptic intrusion into citizen’s political associations. While there can be no doubt that the struggle for civil rights presented a unique danger for its supporters, this should not mean that _only_ such perils warrant First Amendment protection.   




The marketplace of ideas is often fraught with contention, and those who support controversial causes must shoulder some risk. As the late Justice Scalia argued, “running a democracy takes a certain amount of civic courage.” But anonymity in such pursuits serves important purposes, and the premise that concealment of one’s identity is a sign of ill-will would have surprised James Madison, who published numerous defenses of the new constitution, convincing his fellow citizens of the virtue of the endeavor; he signed them “Publius.”   
  
In our schismatic political climate, many people could suffer if their political views were made widely known. This could include everything from adverse employment actions to outright violence. Some groups, such as those in the “antifa,” have openly advocated violence against political opponents. It’s odd that some on the modern left find themselves on the same side as the state of Alabama in 1958: arguing that those who support some political views should be disclosed to the state, even if violence might result. Although an appeal has not yet been filed, the Supreme Court should take the case and reverse the Second Circuit, making it clear that a compelling government interest is required before the government can force the disclosure of people’s political affiliations. 


"
"

A recent investigation by the Financial Times says that the new Carbon Credit Industry may already be rife with fraud. Hmmm…now where have we heard that before?
Among the findings:
■ Widespread instances of people and organisations buying worthless credits that do not yield any reductions in carbon emissions.
■ Industrial companies profiting from doing very little – or from gaining carbon credits on the basis of efficiency gains from which they have already benefited substantially.
■ Brokers providing services of questionable or no value.
■ A shortage of verification, making it difficult for buyers to assess the true value of carbon credits.
■ Companies and individuals being charged over the odds for the private purchase of European Union carbon permits that have plummeted in value because they do not result in emissions cuts.
From the article:
Some companies are benefiting by asking “green” consumers to pay them for cleaning up their own pollution. For instance, DuPont, the chemicals company, invites consumers to pay $4 to eliminate a ton of carbon dioxide from its plant in Kentucky that produces a potent greenhouse gas called HFC-23. But the equipment required to reduce such gases is relatively cheap. DuPont refused to comment and declined to specify its earnings from the project, saying it was at too early a stage to discuss.
The burgeoning regulated market for carbon credits is expected to more than double in size to about $68.2bn by 2010, with the unregulated voluntary sector rising to $4bn in the same period. 
Seems like the “green” here is not about Gaia…but all about Benjamins.
There’s no mention of how much these companies pay gamers to have virtual trees planted in video games.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6dacc7b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Carmakers are bracing for a hybrid electric car price war this year as they try to avoid steep EU fines for carbon dioxide emissions. Some carmakers are struggling to hit tough new EU carbon emission rules introduced in January, which force them to reduce the amount of carbon dioxide their vehicles emit. This may force auto firms to slash prices on lower-emitting plug-in hybrid electric vehicles (PHEVs) to encourage consumers to buy them, according to the investment bank UBS. Average CO2 emissions of almost all cars sold in the EU during the next two years must fall below 95g a kilometre, with heavy fines for carmakers that miss individual targets designed to meet the goal. The rules include the UK until at least 1 January 2021. Around €8.4bn (£7.1bn) will be wiped from carmakers’ profits over the next two years as they try to comply with the regulations. That’s €1bn more than previously expected, due to electric car delays and consumers’ desire for more polluting SUVs, according to calculations by UBS analysts led by Patrick Hummel. Sales of PHEVs, which can be charged from an external source without using the internal combustion engine, need to grow sevenfold between 2019 and 2021, they said. “As [carmakers] likely need to aggressively push PHEVs to their customers, we see risk of a price war in the course of 2020,” wrote Hummel. Germany’s Daimler, the maker of Mercedes Benz cars, and France’s Renault are particularly at risk of missing their targets and having to pay large fines. Fiat Chrysler Automobiles is also at risk, despite a deal thought to be worth hundreds of millions of euros to “pool” its emissions with Tesla’s zero-emissions cars. While legal, the deal has been heavily criticised by campaigners. Jaguar Land Rover, owned by India’s Tata Motors, is also exposed, with costs of £179m expected over the next two years. Britain’s largest carmaker, which has just launched a second round of tough cost-cutting as it struggles with electric investments, needs to increase its hybrid sales rapidly, UBS said. The green credentials of some hybrid cars are controversial because of their continued use of polluting internal combustion engines alongside battery motors. Julia Poliscanova, clean vehicles director at Transport & Environment, a Brussels-based thinktank, said there is evidence that plug-in hybrids rarely achieve the emissions reductions predicted by lab tests because users fail to charge them. “In the real world, their emissions are often two or three times higher,” she said, referencing findings from the Miles Consultancy which said many business users never used their charging cables. If plug-in hybrids are not charged, they can even emit more carbon dioxide and air pollution than the equivalent car without a battery, because a smaller engine is pulling more weight and therefore running less efficiently. Nevertheless, plug-in hybrids, which can be charged from external sources, are a key part of carmakers’ emission-reduction efforts, because they can use the same factories in which they have already sunk billions of euros in investment. The emissions limits could hardly have come at a worse time for the car industry, with China’s market suffering two years of recession even before the coronavirus outbreak and sales of their profitable diesel cars in freefall following the “dieselgate” emissions cheating scandal. At the same time, consumer confidence is weakening across some key European markets and Brexit uncertainty has dented sales in Britain. “The new car market is soft at the moment so dealers are having to work hard,” said Mike Hawes, the chief executive of the Society of Motor Manufacturers and Traders SMMT, the UK car industry’s lobby group. “It’s a buyer’s market.” The costs of complying with the regulations are expected to vary across carmakers, which have radically different strategies. For instance, Volkswagen, the world’s largest carmaker by volume, has launched an all-out electric assault, with plans to produce 1m battery electric cars by the end of 2023. Rival executives believe the company panicked after its reputation was sullied by the dieselgate scandal. Rival manufacturers such as BMW and Daimler have instead focused in the short term on hybrids, which are easier to produce using existing factories and avoid problems such as shortages of lithium ion batteries. Car executives argue that hybrids are essential to reduce society’s emissions. However, they are also particularly attractive from a regulatory perspective. All hybrids that emit less than 50g of CO2 per kilometre under test conditions qualify for “supercredits” for the next two years. This means they count double under the EU regulations, so carmakers are equally incentivised to sell hybrids and battery electric vehicles. “Plug-in hybrids are a short-term compliance strategy for vehicle manufacturers,” said Poliscanova. “They are unavoidably costly because you are locked into two engines and a motor. They should not be part of the future.”"
"At 07:30 on July 1, 1916, a private in the British army – let’s call him Tommy Atkins – scrambled out of a trench in front lines and advanced across no man’s land towards the German trenches in the Battle of Albert, the opening phase of the Battle of the Somme. He had covered no more than 30 yards when he felt as if someone had hit him on the back of the head with a mallet. Checking himself dazedly, he realised that he must have taken a bullet in the left shoulder because the whole area was suddenly bright crimson. Private Tommy Atkins had just become one of the 57,470 British casualties on the now-notorious first day of the Somme. His actions in the next few seconds would ultimately save his life. He tore off the field dressing sewn to the inside of his uniform jacket, ripped it open and applied the contents to his wound before making his way back to the aid post. Tommy Atkins was lucky, not merely because the bullet had not killed him. He was lucky because this was 1916 and not 1914, and the nature of his field dressing therefore meant he had a good chance of staying alive rather than subsequently dying of sepsis caused by infection of his wound.  Death as a result of even relatively minor injuries due to sepsis and gas gangrene had been a major problem in the first years of World War I. Appalling conditions in the trenches meant that wounds were generally accompanied by fragments of dirty clothing and the bodily filth plastered about the battlefields.   In 1915, however, an army surgeon named Charles Cathcart recalled that even quite terrible wounds had been successfully dressed on the battlefield in ancient times using a group of bog mosses known as sphagnum. On the basis of some successful trials, he instigated a nationwide programme of bog moss collection to create what would become the standard field dressing issued to all UK and Imperial land forces as an integral part of their uniform. People gathered the humble bog moss across the country, from Bodmin Moor to the far north of Shetland. Moss provided a number of improvements over the cotton-based dressings used in the early period of the war. Sphagnum is primarily adapted to storing water – vast quantities of it – and it was capable of absorbing more than twice as much blood and fluids as cotton wool, thus initially helping to dry out the wound.  It was fibrous like cotton, and therefore helped to seal the wound, but unlike cotton wool it also appeared to prevent infection of the wound in some mysterious way. It was later discovered that sphagnum releases a chemical called “sphagnan” which inhibits nitrogen uptake by decomposer organisms, sending them into a form of stasis. This is what saved Private Tommy Atkins and thousands like him from a lingering death by sepsis even though the original injury trauma may have been successfully treated. Despite this record of success, the medical use of sphagnum was largely discontinued after World War I. In part, this may have been because supply chains relied on the rather laborious process of harvesting the moss in the wild. Its later use in certain sanitary products ended more recently, however, when significant concerns were raised about the environmental damage caused by such wild harvesting.   Indeed, it is the environmental damage caused by the unsustainable harvesting of sphagnum and its semi-decomposed remains – which we know as peat – that has led colleagues and I to investigate a new type of farming called “paludiculture”, namely harvesting wetland products on the re-wetted soils of former wetlands. In their natural state, such peat soils represent long-term carbon storage, often on millennial timescales. If such soils are damaged, this long-term carbon store is progressively oxidised, releasing lots of carbon dioxide into the atmosphere. Studies have indicated that turning peat soils into farmland is potentially the largest source of greenhouse gas emissions per unit area in the UK lowlands. Working with various partners, our research is investigating ways of growing sphagnum as a commercial crop on re-wetted organic soils. We are seeking to produce a sustainable growing medium with a view to replacing the use of peat – which is essentially just ancient sphagnum – within both the professional horticulture industry and the retail trade.  We are also exploring with Greenwich University the possibility of re-establishing sphagnum as a modern medical material for use in wound treatment and anti-microbial action, thereby coming full circle to that day on the Somme when the life of Private Tommy Atkins was saved by the extraordinary properties of the material in his field dressing. Listen to The Anthill podcast on remembering World War I here, or subscribe wherever you get your podcasts."
"


An erupting solar prominence photographed by the Solar and Heliospheric Observatory (SOHO). [More]
In a post a few days ago I mentioned scientists discovering that global warming appears to be happening on Mars in its polar ice caps and that this was likely evidence of a solar linkage that also affects Earth’s climate. Today NASA announced in an article shown below that the next solar sunspot cycle due in 2010 is likely to be one of the historically largest in 400 years of sunspot records.

What does this mean? Well if you follow sunspots and temperature trends on earth you’ll be able to see clear correlations between the ebb and flow of sunspots and Earthly temperature. While global warming proponents brush this off as inconsequential, the fact is that when sunspots happen in larger numbers, Earth warms up, when they disappear, the earth cools, as evidenced by a 50 year cold period in Medieval history with virtually no sunspots known as the Maunder Minimum. Its also called The Little Ice Age.
So, with a big sunspot cycle in the next few years, we can expect many record high summer temperatures and warmer than normal winters. We’ll see melting sea ice, retreating glaciers, and wailing of those saying “We told you so, CO2 is killing the planet!”. Al Gore will probably get elected President by a panicked nation, and general worry and angst will reign supreme. Emergency CO2 emissions measures may be enacted. Perhaps a rationing on driving our cars?
And then, when solar cycle 25 hits ten years later, which will likely be much smaller, the “crisis” will subside and those whom enacted those emergency measures will pat themselves on the back and bask in their “heroism”. Except, it won’t have anything to do at all with changes in emissions. It’s all about the sun. Just take a look at the picture above and notice just how small earth is compared to the sun, or even a large solar flare. Anybody whom thinks the human race has more effect on our global energy balance than an active sun does is just deluding themselves.
There’s a monetary bet out there: two Russian solar scientists are so certain that its the sun driving climate change and nothing else, they have put down a $10,000 bet with a prominent climate change scientist saying we’ll see a cooler of the earth by about 2015.
I want some of that action. Bets anyone?

From NASA, Dec. 21, 2006: Evidence is mounting: the next solar cycle is going to be a big
one.
Solar cycle 24, due to peak in 2010 or 2011 ""looks like its going to be one of the most intense cycles since record-keeping began almost 400 years ago,"" says solar physicist David Hathaway of the Marshall Space Flight Center. He and colleague Robert Wilson presented this conclusion last week at the American Geophysical Union meeting in San Francisco.
Their forecast is based on historical records of geomagnetic storms. Hathaway explains: ""When a gust of solar wind hits Earth’s magnetic field, the impact causes the magnetic field to shake. If it shakes hard enough, we call it a geomagnetic storm."" In the extreme, these storms cause power outages and make compass needles swing in the wrong direction. Auroras are a beautiful side-effect.
Hathaway and Wilson looked at records of geomagnetic activity stretching back almost 150 years and noticed something useful:. ""The amount of geomagnetic activity now tells us what the solar cycle is going to be like 6 to 8 years in the future,"" says Hathaway. A picture is worth a thousand words

Above: Peaks in geomagnetic activity (red) foretell solar maxima (black) more than six years in advance. [More
In the plot, above, black curves are solar cycles; the amplitude is the sunspot number. Red curves are geomagnetic indices, specifically the Inter-hour Variability Index or IHV. ""These indices are derived from magnetometer data recorded at two points on opposite sides of Earth: one in England and another in Australia. IHV data have been taken every day since 1868,"" says Hathaway.
Cross correlating sunspot number vs. IHV, they found that the IHV predicts the amplitude of the solar cycle 6-plus years in advance with a 94% correlation coefficient.
“We don’t know why this works” says Hathaway. “The underlying physics is a mystery. But it does work.”

According to their analysis, the next Solar Maximum should peak around 2010 with a sunspot number of 160 plus or minus 25. This would make it one of the strongest solar cycles of the past fifty years—which is to say, one of the strongest in recorded history
Left: Hathaway and Wilson’s prediction for the amplitude of Solar Cycle 24. [More]
Astronomers have been counting sunspots since the days of Galileo, watching solar activity rise
and fall every 11 years. Curiously, four of the five biggest cycles on record have come in the past 50 years. “Cycle 24 should fit right into that pattern,”says Hathaway.
These results are just the latest signs pointing to a big Cycle 24. Most compelling of all, believes Hathaway, is the work of Mausumi Dikpati and colleagues at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. “They have combined observations of the sun’s ‘Great Conveyor Belt’ with a sophisticated computer
model of the sun’s inner dynamo to produce a physics-based prediction of the next solar cycle.” In short, it’s going to be intense.
Details may be found in the Science@NASA story Solar Storm Warning
“It all hangs together,” says Hathaway.

Picture above – Sunpot numbers have been increasing for the last 150 years and have been at their highest average levels during the last 20 years, which could explain much of the global warming conditions observed on earth.
Note that during the 1970s, sunspot numbers decreased, we had some severe winters, and many scientists and popular press at that time talked of a coming ice age.  You can read a June 24th, 1974 article about a coming ice age in the TIME Magazine archive here:
http://www.time.com/time/magazine/article/0,9171,944914-1,00.html
Even as recently as 1994, TIME was concerned about a possible ice age coming as we see in this article:
http://www.time.com/time/magazine/article/0,9171,980050,00.html
Chances are, we’ll see another dramatic dip in sunspots by 2015 through 2022 and global cooling will set in again as it did in the 1970’s.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea93ed743',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter[Correction: James Taylor is Director of Heartland’s Arthur B. Robinson Center on Climate and Environmental Policy, and not the President of The Heartland Institute]
Today I’m writing about a public relations disaster here in Germany by the Heartland Institute where its president, James Taylor, allowed himself to be fully duped by leftist journalists posing as industry lobbyists offering half a million dollars to his Chicago-based think tank.

Image: Correctiv
Just days ago, flagship ZDF German public television broadcast a segment here — created by “investigative” site Correctiv here — at its popular weekly Frontal 21 magazine. Frontal 21 describes how the two “investigative” journalists from the leftist Correctiv infiltrated Heartland Institute and exposed its “lobbying and climate disinformation campaign”.
The ZDF’s Frontal 21 magazine shows how Correctiv set up a phony PR agency, fake website and printed phony business cards so that two of its reporters could pose as lobbyists representing the German automotive and coal industry. Their Project Veritas-like mission: to infiltrate Heartland and uncover “what goes on in the climate denier scene, how it works, which strategies it’s pursuing in Europe and how influential it is.”
“That’s how we want to find out whether and how to question climate change in exchange for money and to buy influence,” reported Frontal 21.
Unfortunately, James Taylor fell for the entire ploy. The ZDF’s Frontal 21 (and Corectiv) took the material and later pieced together a major hit piece seen nationally on February 4th. A blow to the skeptic scene in Germany.
First meeting in Munich
The two undercover German journalists started by making their way from their base in Berlin to the Munich Climate Conference last November, where they had an easy time fooling James Taylor.
Equipped with hidden cameras, they sat down with Taylor for dinner and asked how he went about convincing people of his mission. ZDF’s Frontal 21 quotes Taylor: “The people cannot be motivated by logical things; you have to argue emotionally.”
$500,000 offer in Madrid


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Frontal 21, after dinner Taylor then personally invited the two “German lobbyists” to Madrid for a later conference, where they met again with Taylor in the Marriott Hotel lobby.
ZDF Frontal 21 reported: “Taylor boasted about his good contacts to the Trump administration. ‘The Trump administration often asked for advice and about what we could do. We worked closely together.'” Taylor even told the undercover journalists that Heartland’s budget was $6 million.”
When asked how a $500,000 donation could be made, Frontal 21 says Taylor told them to donate it directly to Heartland. “And if you want to remain anonymous, then give it to an organization. One of them is Donors Trust.”
“To keep it hidden?” one undercover Correctiv journalist asked.”
“Yeah right, exactly,” replied Taylor, alleges Frontal 21.
Untypical for interviews, Taylor’s voice is completely filtered out by Correctiv editors, so it’s impossible to ascertain whether Frontal 21 translated Taylor’s words accurately.
In the Marriott lobby meeting, Taylor discloses Heartland’s future plans for Germany, where he also brings up young German influencer Naomi Seibt – the “anti-Greta”.
Duped by the mean and nasty media
Weeks later, Taylor sends the two undercover journalists his proposal dubbed “Funding Proposal: Germany Environmental Issues”, outlining Heartland’s strategy for Germany and that Heartland was interested in the funding in order to inform the public about the minimal effects of diesel exhaust and to produce videos on the negative impacts of excessive environmental regulation.
The set-up succeeded, and Correctiv got all the footage they needed, and much more, to allow German ZDF public television to weave a highly distorted, one-sided, yet believable hit piece. All in all, a major PR blow for climate skeptics in Germany.
A little research and precaution on Heartland’s part could have avoided being duped to the extent they were. They were sloppy. And we also have to feel bad that a young, promising German influencer got caught up in it and was so inaccurately portrayed on national television.
In her latest video, a visibly distraught Naomi tells of her experience of being observed by impostors under the table by hidden cameras, yet bravely pledges to keep up the fight for what she believes to be the truth. Now she understands just how mean and nasty the German media really are.

Share this...FacebookTwitter "
"

The Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press. In this special issue, we focus on the climate implications of a carbon tax._   






We calculate, you decide.   
  
Once you make your selections, the calculator will return the amount of global temperature rise that will be averted as a result of your choices by the year 2050 and also by the end of the century.   
  
Try it using this example. Choose a 100% reduction of carbon dioxide emissions from the United States and the IPCC’s sensitivity value of 3.0°C. Hit “Submit.” The amount of temperature savings that results is 0.052°C by the year 2050 and 0.137°C by the year 2100. (Why we are using three significant digits is in the fine print at the end of this article.)   
  
[block module=""carbontaxform"" delta=""carbontaxform""][/block]   
  
Sorry, Major Kong (h/t to “Dr. Strangelove”), those _are_ the figures. That’s the right answer. Assuming the IPCC’s value for climate sensitivity (i.e. disregarding the recent scientific literature) and completely stopping all carbon dioxide emissions in the U.S. between now and the year 2050 and keeping them at zero, will only reduce the amount of global warming by just over a tenth of a degree (out of a total projected rise of 2.619°C between 2010 and 2100).   
  
If you think that a rise of 2.482°C is vastly preferable to a rise of 2.619°C then all you have to do is set the carbon tax large enough to drive U.S. emissions to zero by mid-century—oh yeah, and sell that tax to the American people.   
  
To explore other alternatives, use our handy-dandy calculator.   
  
Have fun!   




*********



 **The fine print:**   




The results from our calculator are produced from climate change calculations performed using the MAGICC climate model simulator (MAGICC: Model for the Assessment of Greenhouse-gas Induced Climate Change). MAGICC was developed by scientists at the National Center for Atmospheric Research under funding by the U.S. Environmental Protection Agency.   
  
We are not creative enough to have made that acronym up. MAGICC is itself a collection of simple gas-cycle, climate, and ice-melt models to efficiently emulate the output of complex climate models. MAGICC produces projections of the global average temperature and sea level change under user configurable emissions scenarios and model parameters. MAGICC is run using its default model parameter settings except for climate sensitivity, which you can choose from between 1.5°C and 4.5°C.   
  
The baseline emissions scenario against which all climate dioxide reductions were measured is scenario A1B from the IPCC’s Special Report on Emissions Scenarios (SRES). Scenario A1B is a middle-of-the-road emissions pathway which assumes rapid carbon dioxide emissions growth during the first half of the 21st century and a slow CO2 emissions decline thereafter. Emissions are prescribed by country groups. Our “Industrialized Countries” group is the OECD90 countries (which includes North America, Western Europe, and Australia, New Zealand and Japan.) In order to obtain the baseline emissions from the United States to which the emissions reduction schedule could be applied, the U.S. emissions were backed out from the OECD90 country grouping. To do so, the current percentage of the total group emissions that are being contributed by the United States was determined—which turned out to be right around 50%. We assume that this percentage will be constant over time. In other words, that the U.S. contributed 50% of the OECD90 emissions in 2000 as well as in every year between 2000 and 2100. In this way, the future emissions pathway of the U.S. was developed from the group pathway defined by the IPCC for the A1B scenario. From these baselines (either the U.S. baseline or the OECD90 baseline), carbon dioxide emissions reductions were applied linearly from 2005 to 2050 to obtain the user-specified total reduction. The new (reduced) emissions were recombined with the other (unadjusted) IPCC country groupings to produce the global emissions total. It is the total global emissions that are entered into MAGICC to yield global temperature projections. The results using the reduced emissions pathway were then compared to the results using the original A1B pathways as prescribed by the IPCC, with the baseline against which temperature changes were calculated set to the year 2010.   
  
We assume that a carbon tax would only be applied to reduce carbon dioxide emissions. In practice however, the only way to reduce carbon dioxide emissions is to reduce the burning of fossil fuels. Reducing the burning of fossil fuels will have co-impacts such as reducing the emissions of carbon monoxide (CO), volatile organic compounds (VOCs), nitrogen oxides (NOx), and sulfur oxides (SOx). The first three chemical compounds generally enhance warming while the latter generally retards it. Sensitivity tests using MAGICC indicate that for the OECD90 countries under the A1B pathway, the effect of collective changes in these co-emissions is largely compensative.   
  
Additional fine print on precision: The temperature savings are presented to three significant digits in order to tell the results apart. In the real world, the impacts from the emissions reduction pathways are not nearly so precise and, in fact, the temperature savings from most of the different carbon dioxide emissions reduction pathways are scientifically impossible to tell apart from each other, and in many cases, are impossible to tell apart from the original A1B scenario, i.e., they are same thing as doing nothing.


"
"
Share this...FacebookTwitterElsevier has accepted a new paper by Lüdecke et al, 2020, showing natural oceanic and solar cycles play a large role in modulating Europe’s climate. Offers new chances for robust midterm temperature prognoses. 
The paper, in press, journal pre-proof, analyzes natural variability in European monthly temperatures on decadal and multidecadal timescales and their possible drivers.
NAO, AMO, sun behind temperature variability 
The authors claim to have established characteristic correlations of temperature with ocean cycles, here NAO and AMO, and solar activity for many regions and seasons. This means it is likely that NAO, AMO and solar activity are the actual drivers of a lot of the temperature variability.
Figure 1 of the soon-to-be-published paper follows:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Image: Decadal and multidecadal natural variability in European temperature, Lüdecke et al, Journal of Atmospheric and Solar-Terrestrial Physics (journal pre-proof).
The authors did NOT look at the anthropogenic component of the long term warming of the past 150 years and its attribution but feel the results will hopefully help to better attribute shorter-term temperature changes and their typical patterns.
Chance for midterm temperature prognoses
This is important because efforts by the scientific community are progressing to better predict NAO and AMO for months and a few years in advance, This opens up new chances for more robust mid term temperature prognoses that also includes natural climate variability which the  paper documents.
The paper’s preliminary abstract (emphasis added):




European monthly temperatures undergo strong fluctuations from one year to the other. The variability is controlled by natural processes such as Atlantic cycles, changes in solar activity, volcanic eruptions, unforced internal atmospheric variability, as well as anthropogenic factors. This contribution investigates the role of key natural drivers for European temperature variability, namely the Atlantic Multidecadal Oscillation (AMO), the North Atlantic Oscillation (NAO) and solar activity changes. We calculated Pearson correlation coefficients r for AMO, NAO and sunspots compared to monthly temperature data of 39 European countries for the period 1901–2015 in order to search for ‘fingerprints’ of the natural drivers. A cross correlation window of 11 months was chosen for AMO, NAO and of 120 months for SILSO to account for possible time lags or phase shifts. The r coefficients were mapped out across Europe on a monthly basis to document regional and seasonal changes of the correlation strength. The NAO dominates European temperature variability during the winter months, with strongest relationship in February. The AMO modulates temperatures in March to November, with best correlations occurring in summer, but also in April. Regions of strongest AMO and NAO impacts shift across the continent from month to month, forming systematic patterns. Direct correlation of the solar 11-year Schwabe cycle with temperatures was identified only in some countries in certain multidecadal intervals during February, March, June and September. Previous studies have suggested a significant solar influence on the AMO and NAO. It is likely that the greatest impact of solar activity on European temperatures is of a non-linear, indirect nature by way of interaction with Atlantic cycles.”






The paper’s graphs show an oscillation of temperature that challenge the sharply rising GISS-like narrative for Europe.


		jQuery(document).ready(function(){
			jQuery('#dd_d64e9e9adc2a2d68f814d56480aff4dc').on('change', function() {
			  jQuery('#amount_d64e9e9adc2a2d68f814d56480aff4dc').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000


Share this...FacebookTwitter "
"
Share this...FacebookTwitterMainstream climate science claims CO2 molecules “slow down the rate of heat-loss from the surface” like a blanket does. And yet the rate at which a CO2 molecule retains or slows down heat loss is, at most, a negligible 0.0001 of a second. A CO2 concentration of 300 ppm versus 400 ppm will therefore have no detectable impact.
SkepticalScience, a blog spearheaded by climate science “consensus” advocate John Cook, is widely considered the explanatory guidebook for the anthropogenic global warming movement.
The blog claims CO2 molecules, with a representation of 4 parts in 10,000 in the atmosphere (400 parts per million, or ppm), collectively function like a blanket does in slowing down the rate at which the human body cools.

Image Source: SkepticalScience
The rate or time lapse involved in this “slowing” of heat loss is problematic to the paradigm that says CO2 drives global warming, however.
Professor Nasif Nahle has mathematically assessed the rate at which heat is retained by CO2 molecules; his work was endorsed by the Faculty of Physics of the University of Nuevo Leon (Mexico).


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Nahle found the “mean free path” for a quantum wave to pass through the atmosphere before colliding with a CO2 molecule is about 33 meters (Nahle, 2011a). Such a wide chasm between molecular collisions would appear to undermine a visualization of CO2 functioning like a blanket does.
Even more saliently, Nahle determined that the rate at which CO2 molecules can retain heat at the surface may only last about 0.0001 of a second (Nahle, 2011b).
If heat-loss is slowed down at a rate of 0.0001 of a second by CO2 molecules, the atmospheric CO2 concentration – whether it’s 300 ppm or 400 ppm – effectively doesn’t matter. The time lapse differential would be immaterial for either concentration.
Consequently, Nahle concludes “carbon dioxide has not an effect on climate changes or warming periods on the Earth”.

Image Source: Nahle, 2011a

Image Source: Nahle, 2011b
Share this...FacebookTwitter "
"
The picture below comes to me via my website www.surfacestations.org from volunteer site surveyor Bob Meyer. It is the USHCN climate station of record for Waterville, Washington.
In addition to the now commonly seen attempts at measuring the temperature of parking lots, this station sports another new feature: volcanic cinder rock under the station to complement the tidy sidewalk. Note the convenient drive through teller window nearby so that you can cash your paycheck while on the way to the Post Office to mail in your COOP observer form to the National Climatic Data Center.

There’s also a nearby building about 10 feet away, and of course, convenient close-by parking just a few feet from the MMTS temperature sensor. Note that published NOAA/NWS siting standards require a 100 foot distance from buildings.

The USHCN “high quality” set of climate monitoring stations keeps getting curiouser and curiouser.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea594507e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 
In Today’s Chico News and Review, the cover story is about Internet Radio and all
the trouble the Copyright Royalty Board recently caused with a draconian ruling
on the cost to Internet Radio Stations. Regular over the air
broadcasters don’t have such limits because they are seen to ""promote the music
industry"" Its an alliance as old as payola.
But good news comes today. A bill introduced in Congress today could nullify
the new rates set by the Copyright Royalty Board (CRB) which advocates say would
put Internet Radio webcasters out of business, such as our own local
Radio Paradise.
Rep. Jay Inslee (D-WA) and Rep. Don Manzullo (R-IL) have presented the ""Internet
Radio Equality Act"" which aims to negate the controversial March 2nd
decision which puts royalty of a .08 cent per song per listener, retroactively
from 2006 to 2010 on internet radio.
Advocates of Internet Radio have dreaded the CRB ruling, which they say could
raise rates between 300 to 1200 per cent for webcasters. Earlier this month, the
CRB threw out an appeal by commercial webcasters, National Public Radio and
others to review the new rates and postpone a May 15 deadline for the
introduction of the royalty schedule.
If passed, today’s proposed bill would set new rates at 7.5 per cent of the
webcaster’s revenue — the same rate paid by satellite radio. Alternatively,
webcasters could decide to pay 33 cents per hour of sound recordings transmitted
to a single user.
This bill is a critical step to preserve this new growing medium, and would
present a level playing field where webcasters can compete on the same royalty
terms with satellite radio. It would also reset royalty rules for non-profit
radio such as NPR. Public radio would be required present a report to Congress
on how it should determine rates for their internet streaming media.
I hope this passes, not so much because local radio needs more competition,
but because this insane CRB ruling makes it nearly impossible for local broadcasters to
compete on the Internet at all. This would give everybody a fair chance and at
the same time bring in millions, perhaps billions in royalties for artists.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6ce3daa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Cities may only occupy about 2% of the world’s habitable land, but they are big drivers of global climate change. Cities are usually hotter than rural areas, and get referred to in the jargon as “urban heat islands.”  Cities are hotter for a number of reasons. Traffic pollution creates a greenhouse effect that keeps heat in at night. Cutting down trees means you lose their ability to absorb heat and convert it into nutrients. Paving and tarmac quickly release the heat they retain back into the air, and rainwater has to be drained away in sewer systems, which deprives the area of the cooling effect of rain-soaked soil. Then there are people. They both generate body heat and heat buildings to keep themselves warm – or use air-conditioning to cool them down. Aircon means they are transferring warmer air into the streets outside, so it adds to the city’s warmth just as much as heating systems.  As cities expand in size and more people live in them, these warming factors have gradually been exacerbated. In the south of England, the difference between rural areas and London is as much as 6°C. In Glasgow, even though the population has subsided until recently, the difference can still be as much as 8°C.  In hotter parts of the world, this is reaching breaking point in some cases. Colombo in Sri Lanka has seen people migrating away in substantial numbers to live in cooler areas, for example. The searing heat in Phoenix, Arizona, may prevent the city from continued expansion. Even in more temperate cities like London or Paris, unexpected heatwaves can kill hundreds and even thousands of people.  Discussions about global warming tend to overlook the contribution of urban growth to the problem, and instead concentrate on what is happening to the temperature across the world.  And policy developers aiming to fight global warming overlook the fact that by focusing on ways to make cities cooler, they might contribute in a big way to the solution – perhaps much more than focusing on global carbon-reduction agreements that either fail or end up badly watered down. Given the forecasts for climate change out to 2050, this looks like a vital trick that is being missed.  The good news is that cities have been living with the effects of local warming for decades. By observing different cities around the world, we can see what needs to happen – the problem is getting those cities that do less to focus on doing more.  The solutions in hotter and cooler climates are different. [Research](http://researchonline.gcu.ac.uk/portal/en/persons/rohinton-emmanuel(c687e6ca-47ef-47f5-810c-b662ff2eda72/publications.html) in warm, humid Colombo shows excessive amounts of solar radiation. But because of the availability of abundant water from year-round rainfall and a large amount of urban vegetation, there is much less temperature difference between the city centre and rural surroundings. This suggests that were this not in place, the migration problem could be even worse.  The research found that you a big difference can be made to the climate in tropical cities, whether humid or arid, through shade. This requires an ethos of urban design that turns on its head the old idea of “thou shall not cast shadow on thy neighbour’s property” and instead says, “thou shall cast shadow on public spaces.” This is not about shading buildings per se (nor is this desirable) but to encourage an urban geometry that makes the spaces between buildings naturally shaded without compromising buildings’ ability to draw in the sunlight as and when required.  Achieving this when the tropical sun is so high in the sky means that you have to use an intelligent combination of building heights and geometry together with elements such as canopies, awnings and urban vegetation.  With care and attention to detail, built-up areas can combine good shading with generous urban vegetation to cool neighbourhoods to temperatures that are even below those of rural areas. This is good news given the continued acceleration of urban growth in many tropical cities and rising concentrations of people. And even a couple of degrees’ difference can make a city unbearable in an area that is already hot.  London and New York are a good examples of what cities in cooler areas can do to make a difference. Their heat island policies include things like planning requirements to plant trees, reduce paved areas in parking lots and reduce traffic. But these sorts of policies are still quite rare across the board, and neither do you see much similar action in hotter climes. Singapore is one of very few tropical cities that prioritise traffic control for example.   Finally a word on colder cities such as Glasgow, where I am based and have been involved [in work](http://researchonline.gcu.ac.uk/portal/en/persons/rohinton-emmanuel(c687e6ca-47ef-47f5-810c-b662ff2eda72/publications.html) to look at ways to make it cooler. This may not seem very necessary when the temperature is not particularly high but we need to bear in mind that it is likely to get hotter in the coming decades, so it will still contribute to global warming. For example our simulations suggest that if you increase tree cover by 20%, you could eliminate a third to half of the expected urban heat increase by 2050. This sort of intervention looks well worth considering. "
"

What do the numbers 923, 930, 935, 941 and 944 have in common? Answer: They’re different names for the same sunspot, this one shown above.
Greg Piepol of Rockville, Maryland, took the picture yesterday using a Solar Max Solar telescope/camera. It shows sunspot 944 coming around the sun’s eastern limb–for the fifth time! Usually sunspots form and dissolve in a matter of weeks, but this spot has endured for more than five 27-day solar rotations. By long and idiosyncratic tradition, a sunspot receives a new number each time it reappears and is visible to earth.
Sunspot 944 may not seem impressive now, but one month ago as “941” it was a lovely spiral. Three months ago as “930” it produced one of the strongest solar flares of the past 25 years and Northern Lights as far south as Arizona. What will it do this time?
Even though we are in between peaks in our 11 year sunspot cycle, we still seem to have quite an active sun. The trend over the last century has been that our solar cycle has had more activity than centuries before.

Of course, that couldn’t possibly have anything to do with global warming.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea821d20f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Deer, bison and pronghorn traverse the plains in large herds…scavengers quickly sniff out carrion, sockeye salmon leap upstream, wolves attack in packs surrounding their prey, geese fly in fixed formations, possums play dead, rodents scamper into tree hollows, grizzly bears bluff charge when threatened and birds of prey soar on thermals. That may sound like a mountie’s report on the Canadian wilderness, but it’s actually how Rockstar recently promoted Red Dead Redemption 2 – its critically acclaimed game, which transports players to a sprawling and immersive Wild West. Red Dead Redemption 2 features more than 200 species of animal in a variety of habitats, and its record breaking success  suggests that authentic natural environments which mimic the ecology of the real world will become a mainstay of future titles.  Video games have grown in scale and complexity to the point where intricate virtual ecosystems of this kind are now possible, with flora and fauna living and behaving in these virtual worlds as they do in ours. As of 2018, the worldwide games industry was estimated to be worth around £100 billion. To put that into perspective, it’s 1.5 times bigger than the movie industry and five times bigger than the music industry, with one in three people on the planet being a gamer. Not bad for an industry that is only around 50 years old. Alongside the huge financial success of modern games is the ever-growing size of “open-world games”, in which players are free to explore vast and interactive virtual worlds. These virtual environments have gone from simple mono-block representations of landscapes to dynamic and interactive ecosystems. They have plants that can be foraged and a variety of wildlife that demonstrate complex AI-driven behaviour, interacting with the player and each other.  Within Red Dead Redemption 2, apex predators such as alligators lurk patiently underwater, anything (including other animals) in the game that venture too close to the water’s edge quickly meets its demise. Deer will also react to unseen predators, alerting the player to cougars lurking in nearby grass. Horses, one of the most important animals in the game, also react to other wildlife – bolting at the sign of a bear or hidden rattlesnake – demonstrating authentic animal intelligence. Guerrilla Games’s open world role playing game Horizon Zero Dawn features machine as well as organic “animals”. The machine animals in particular exhibit behaviours that don’t primarily rely on the player’s interaction. “Corrupted” machines will often attack their non-corrupted counterparts, with the player often coming across the bodies of dead machines, alluding to a dynamic world that exists outside the player’s attention.  The bodies of fallen machine animals, like in any real ecosystem, are not wasted. If not engaged in combat or roaming territory, “scrappers” (machines resembling hyenas), and “glinthawks” (giant vulture-type machines) will consume fallen machine animals they detect nearby – replicating decomposition and nutrient cycling.  Nintendo’s open world game Zelda: Breath of The Wild uses “virtual foraging” which is required to progress through the game. However just like the real world, players also need to be careful as flora and fauna can be easily over-foraged, forcing the player to  wait for stocks to replenish. All of this is more impressive when we consider that it has all been achieved in a single generation. Video games as a medium are relative newcomers – the industry only emerged in the 1970s. After the same length of time, films were still black and white. One can only wonder what gamers will be playing ten, 20 or even 50 years from now. Ecosystems in games are increasingly dynamic and “lived-in”, which opens the potential for education. Anna Groves, an American ecologist and gamer explained:  A kid who loves lighting the Hyrulian grassland on fire just might get excited about grassland restoration ecology when they find out it involves lighting real-life grasslands on fire. As games increasingly use ecology as a core gameplay feature, its value and relevance as a subject field will inevitably increase – exposing children to an academic subject in an accessible and enjoyable manner. Video games offer unparalleled creative freedom to explore subjects like ecology. Designers can create environments filled with long extinct species or pristine ecosystems that recreate how wilderness may have looked before human intervention. Children may “play” with imagined scenarios of the natural world in an intuitive, immersive and fun manner, far surpassing what is possible in traditional educational approaches.  As a result, they may gain a deeper appreciation of what natural states are possible through conservation than even a student engaging with depleted ecosystems in the real world could. With the advent of virtual ecology, video games are increasingly functioning as “conduits” to other disciplines. Landscape architecture and psychology are increasingly feeding into contemporary game design. In the future, disciplines such as engineering, geology and even medicine could start to inform the next generation of games.  When designing the worlds we play in, future game designers might increasingly be educated in “traditional” elements of landscape design, including ecology and architecture. With this also comes the opportunity for people in different fields to collaborate in shaping the worlds of future video games, radically reshaping both professions in the process."
"Coalition MPs and the Australian newspaper have accused the New Zealand government of hypocrisy over its objections to Australia’s use of what are known as carryover credits to meet climate targets under the Paris agreement. Are they right?  The Australian published comments by four Coalition backbenchers suggesting New Zealand’s climate change minister, James Shaw, was a hypocrite for saying there had been an “allergic reaction” in the international community to countries wanting to use an accounting measure to meet their 2030 climate target under the Paris agreement. There is no question Shaw was referring to Australia: it is the only country planning to use the measure – carryover credits claimed for having emitted less than previous self-set targets – under the Paris agreement. The backbenchers said New Zealand government was itself relying on carryover credits, in this case to meet its 2020 target under the Kyoto protocol. The newspaper followed up with an editorial on Wednesday headed “Jacinda Ardern’s climate policy virtue signal exposed”, saying among other things that New Zealand was not on track to meet the 2030 target it lodged in Paris. Jason Falinski, a moderate from New South Wales, said NZ had shown a “lack of consistency and standards” in criticising countries for using the credits. James Paterson, a Victorian senator and former staff member at climate change-denying Institute of Public Affairs, said it was appropriate for Australia to “reserve the right to get credit for its success in reducing emissions”. The former deputy prime minister Barnaby Joyce said New Zealand should not be lecturing Australia about environmental policy. Gerard Rennick, a Queensland senator who does not accept mainstream climate science, suggested Australia could reduce its emissions by deporting 600,000 New Zealanders. The Australian editorial adopted the language of the Morrison government, including claiming it was “working towards” meeting its 2030 target (a 26% to 28% cut below 2005 levels) “with or without Kyoto carry-over credits”, while New Zealand was forecast to miss its target. At one highly qualified level, yes: New Zealand says it will use 27.7m tonnes of carry-over credits to meet its 2020 target under the Kyoto protocol. Like Australia, it “beat” its target under the first stage of the Kyoto protocol, the precursor to the Paris agreement, which included targets for developed countries up to 2012. It means that, like Australia, it could carry those credits forward and count them against its target for the second stage of the Kyoto deal, which ends this year. Unlike Australia, New Zealand would need the credits to meet its 2020 Kyoto target (a 5% cut below 1990 levels). Australia will meet its 2020 target without them. Not really. But before we get to that, a couple of caveats. Firstly, New Zealand did not actually sign up to the second period of the Kyoto protocol. The former National party government opted not to make its 2020 target legally binding, but said it would still meet the goal using UN rules. Jacinda Ardern has focused on the Paris agreement, including committing the country to net zero emissions by 2050, with a lower target for methane from agriculture. Secondly, it is arguable whether Australia is actually meeting its 2020 target. The government usually describes its 2020 target as a 5% cut below 2000 levels. But, according to government projections, national emissions this year will be only 0.3% lower than in 2000. The reason that, despite this, it can still claim to be beating its 2020 target is explained here. New Zealand deserves criticism for not meeting its Kyoto target. But under the most important test – is what the countries are doing enough? – both Australia and New Zealand have failed under the Kyoto protocol. Both set inadequate targets based on what scientists say would be their fair share of emissions cuts would be under a meaningful climate deal. Both used creative accounting and loopholes to further limit action. New Zealand is not meeting its low 2020 target and assessments suggest it is not on track to meet its 2030 target (a 30% cut compared with 2005). But it recently passed a zero-carbon act for 2050 and is revising its emissions trading scheme. Australia is meeting its low target using accounting rules, but has not reduced its emissions below what they were 20 years ago. Despite what the government says, it has no evidence it it is on track to meet its 2030 target without using carryover credits. It has no long-term target or, at this point, new policies. Shaw is not being hypocritical in suggesting Australia shouldn’t use Kyoto carryover credits to meet its 2030 Paris target for one, simple reason: the agreement struck in the French capital is a completely different treaty to Kyoto. The rules of the Kyoto protocol are clear: countries can claim credit against future targets if they beat earlier targets. The Paris agreement does not mention carryover credits, and is legally separate to the Kyoto agreement. Credits from Kyoto have no formal status under it, and New Zealand has no plan to use them its 2030 target. The implied goal of the Paris deal is net zero emissions, which means boosting cuts over time, not finding ways to limit them. While the Morrison government claims otherwise, Australia’s attempt to use Kyoto credits under Paris has been widely criticised by other countries and remains a point of unresolved disagreement at UN negotiations. Laurence Tubiana, an architect of the Paris accord, told the Financial Times: “If you want this carryover, it is just cheating. Australia was willing, in a way, to destroy the whole system, because that is the way to destroy the whole Paris agreement.” Shaw’s claim that there has been an “allergic reaction” to Australia’s planned reliance on carry over credits might be colourful, but is not hypocritical nor inaccurate. With at least 100 countries signed up to groups that oppose them, it seems a pretty reasonable call. Bill Hare, from policy and science institute Climate Analytics, puts it more bluntly. He says New Zealand should be applauded for rejecting the use of carryover in the Paris deal, and the hypocrites in this exchange are the Australian politicians who profess to be concerned about climate change while promoting policies, including the use of carryover credits, that will result in higher emissions."
"

From the ""you’ve GOT to be freaking kidding me"" department:
Dell’s Virtual Plant a Tree for Me program into the computer game Second Life has many tech savvy people wondering if this represents a new low in Earth Day marketing tie-ins. It looks like in the rush to pander to green-ness, some Dell executives maybe didn’t think beyond the boardroom door.
You may wonder, too, after reading Dell’s invitation to its Earth Day Party at Dell Island in the Second Life game  where they say proudly “get your own tree sapling to plant in Second Life!”.
Yes that’s’ right, you can plant a virtual tree in a video game for Earth Day. And, Dell is only too happy to take a couple bucks from you in the process as well for their real tree planting program designed to assuage your guilt at using a computer that uses electricity.
You have to wonder just how hypocritically lazy some people might be to take this offer seriously, though with 5.7 million ""residents"" in the Second Life game, I suppose its hard to deny that this offer would have an impact.
Just how much electricity is used by PC’s in pursuing this pointless exploit in ""green-ness""? And with Dell soliciting and online Earth Day Party, that will tie up PC’s, routers, and Servers nationwide, using even more electricity. There’s no mention in Dell’s press release of the expected carbon footprint on this bogus promotion. Maybe Gore will fly in on his private jet to make a “virtual appearance” to preach to the faithful.
But since some people nowadays seem incapable of disconnecting themselves from the virtual world of gaming, it stands to reason that a virtual eco-delusional activity might very well appear valid to them.
Maybe next the researchers at Berkeley can tap into the seti@home background processing idea and instead of searching for intelligent life in radio-telescope signals, we could program our wasted CPU cycles to grow virtual trees on a screen-saver. It could boast onscreen counts of virtual carbon sequestered, and virtual O2 produced. I can smell the virtual fresh air already!
We’re doomed.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea705a03b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Whew! It has been a busy week, with lots of things to report. So rather than
making a blog entry for each one I thought I’d condense them all into one entry
with links.
The emphasis this week seems to be on the sun, and the fact that maybe its
really the sun which has been driving climate change after all. That’s what I’ve
been saying for years, because its just unrealistic to ignore the largest and
single most important contributor to our planets energy balance and to only
focus on made-made CO2 and nothing else.
Here are some headlines and links to the reports:
Former Tennessee Senator Fred Thompson

comes out against Gore – cites the sun – from the National Review

Sun Blamed for Warming of Earth and Other Worlds – from LiveScience
Gore testifies on Global Warming before congress –
video from
C-SPAN – free RealPlayer required,

download here
Greenlands

Ice pack measured accurately, shown to be shrinking, but alternate cause
suspected – from NASA
NASA Finds

Sun-Climate Connection in Old Nile Records – Pharoahs apparently made some
accurate records

Sun’s Output Increasing in Possible Trend Fueling Global Warming – From a

Duke University paper and Space.com
 

Global Warming expedition to north pole called off due to extreme cold

Biggest solar storm in fify years is expected – from NASA


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7929fda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterI feel obligated to upgrade a reader comment by Jim Lakely, Communications Director, The Heartland Institute, to a post. He says I was “being very unfair” yesterday.
My main gripe is that these two very dishonest Correctiv “journalists” should have never been allowed to get as far as they did. Now we independent skeptics here in Germany have to deal with being smeared nationally again.
In Germany, the skeptics are outnumbered by like 20 to 1; it’s not a 50-50 deal like in USA. German MSM journalists like those at ZDF are very nasty, unfair and dishonest to legitimate climate science critics.

Jim Lakely, Director of Communications, The Heartland Institute
I have other comments, but will hold them for another time.
I’d love to see a point-by-point rebuttal from Heartland to the nonsense made by ZDF Frontal 21 and Correctiv so that we can defend ourselves here.
============================================================



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Pierre,
You are being VERY unfair to James Taylor and Heartland, and Naomi Seibt — who is not “distraught,” but angry (as is Heartland) at how this has been able “to weave a highly distorted, one-sided, yet believable hit piece.” You had it right on “distorted,” and not so right with the rest.
First: James is surely flattered by your battlefield promotion, but he is not the president of The Heartland Institute. He is the Director of Heartland’s Arthur B. Robinson Center on Climate and Environmental Policy.
Second: They “worked” James spottily over several days in two cities over two weeks before he sat down for a brief talk. That’s a lot of squeeze for no juice. Their big “get” was a big nothing. They tried to bait James into agreeing for a “pay to play” arrangement, and the reporters say … “he didn’t say no.” Really? After two weeks of work, they finally get around to asking their “money question” and that’s it?
There’s a reason James didn’t say “yes”: Because “pay to play” is not what Heartland does, unlike many think tanks on the left. You’re seriously faulting James for steering a (fake) potential donor away from that “gotcha” dishonorable idea into discussing honorable like-minded work with a donor? By that standard, no one in the climate realist movement would ever be able to raise any funds to support our work.
Ask yourself: If these “journalists” had a genuine “smoking gun,” wouldn’t they have presented that instead of that lame “gotcha”?
Third: To compare this lame, days-long caper to the work of Project Veritas is a massive insult to the latter’s fantastic work.
I hope, Pierre, that you will consider giving Heartland some space on your site to more fully rebut for your readers your unfair and uncharitable take on this. Considering all Heartland has done to promote climate realism over many years, I hope you don’t consider that an unreasonable request.
Jim Lakely
Director of Communications
The Heartland Institute”
Share this...FacebookTwitter "
"
An astute letter to the editor writer in Arkansas has found the reason for global warming:

This actually happened, as attested to on the rumor/urban legend verifcation website Snopes.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6b2c1e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

You may recall that back in May I did a simple preliminary experiment to give me guidance on a hypothesis: That changes in paint on Stevenson Screens over time make a measurable difference on the temperatures recorded inside them. This stems from the fact that when the Weather Bureau commissioned the design in 1892, whitewash was specified. But whitewash is no longer commonly available, and the National Weather Service changed the specification in 1979 to be semi-gloss latex paint.
But, cured whitewash is composed of Calcium Carbonate, while latex paint uses Titanium Dioxide as a pigment. While they both appear “white” in visible light, they have vastly different properties in infrared.
My first simple experiment used thermistors in boreholes into 3 wood slats; 1 bare wood as a control, the other two painted with whitewash and latex, showed me that there was a measurable difference in the temperature of the wood by as much as 2-4 degrees at times. I needed to do that experiment before I embarked on the full scale test, since each of the Stevenson Screens you see here in the pictures cost me about $1000.00 Since I’m doing this out of pocket, with no funding or grants, I had to try a small scale test first.
The photos show 3 standard Stevenson Screens as used today in the United States. One is bare wood, unpainted, as a control, the middle one is latex, as sent from the supplier, and the third is painted with a historically accurate (for early 20th century) whitewash mixture that I obtained both materials and formula from the head chemist at the National Lime Company.

The device on the tripod is a stacked plate IR shield with a small fan to pull air through, commonly called an aspirated shield. It is the air temperature reference and placed at the same exposure height as the thermistiors in the screens. Also nearby but not shown is a pyranometer to measure solar insolation and wind speed/direction sensors that are being datalogged as well.
Each Stevenson Screen and the air temperature reference sensor are fitted with matched, calibrated thermistors, NIST traceable with certificates, that are connected to a calibrated data-logger, also with a certificate. The resolution is .01 degree Fahrenheit with an accuracy of +/- 0.1 degree over the range.
I expect that the air temperature differences inside the screens will be less than the 2-4 degrees I observed in the paint slat test. It’s possible that there will be no significant difference at all. I won”t know until I run about a months worth of datalogging.
The site, while not ideal due to the trees, is the best I could get permission to use.  Fortunately the trees do not directly shade the screens except for a short portion of the day. It’s also out of the way, so vandalism will not be likely. Since it had to be an unwatered grass field, concerns over fire danger were raised from some I asked because of the electronics package, so I had limited choices. Perhaps later I’ll be able to find a better site but for now it will have to do.
The whitewash on the third Stevenson Screen is still curing, as the chemical reaction is not yet complete to convert Calcium Hydroxide to Calcium Carbonate. In about a week, I’ll make the data available via a web link in near real-time.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea536d1ac',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGrand media deception
So typical of climate science. Everything in and around it gets wildly exaggerated in order to feed media consumption and deceive the public. Reports are emerging that the “500,000 people” crowd awaiting Greta in Madrid was in fact as low as 15,000, according to Spanish federal police.
That would make the 500,000 claimed figure a 3000% exaggeration!

Ms. Thunberg, media, claim crowd in photo was 500,000 people. But Spanish federal police say the crowd was closer to 15,000. Image: Greta Thunberg.
=========================================
Greta demonstration in Madrid: Crowd number – 97% is exaggeration?
By A. R. Göhring
(German text translated/edited/supplemented by P Gosselin)
After Greta Thunberg’s arrival in Madrid, there was an “unprecedented demonstration” for climate protection in the Spanish capital, supposedly involving 500,000 participants.
But the federal police say there were about 15,000 (!) demonstrators.
At German news outlets, people could read that Greta Thunberg at first had to interrupt participating at the demonstration in Madrid – because of so many people and security could not be guaranteed. Later she stood on a stage with famous Hollywood actor Javier Bardem and gave a speech.
There were, as usual, no scientific facts to be found, but a lot of feeling, self-righteousness and general demands to an ominous elite, who officially stand behind Thunberg’s FFF.

The leaders are betraying us. Enough is enough. Change comes whether you like it or not. We want to see action.” [translated from the German]

Bardem seconded: “We only have ten years to mitigate the worst effects of climate change.” He also hit the Mayor of Madrid, José Luis Martinez-Almeida, and President Donald Trump for their “stupid” measures against climate change.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sources revealing much smaller numbers
The Spanish media, but also the Austrian ORF, are now discussing falsified numbers of participants. The mass media had spoken of 500,000 demonstrators. That would mean that Greta caused one of the largest climate protection demonstrations on the planet in Spain.
Largely empty seats in Katowice in 2018
Those who remember the coverage of the world climate conference COP24 in Katowice, Poland, know that Thunberg’s “breakthrough”, an emotional and panic-stricken speech in front of a “large” audience, had already been manipulated. In fact, the 15-year-old Swede spoke in front of largely empty rows of chairs, which was not visible in the media due to the skillful selection of photographic and film perspectives.
Police figure: only 15,000
Therefore the brash falsification of the numbers is not surprising. The organizers of the demonstration spoke of half a million participants, but the large newspaper El País spoke of 25,000 to 35,000, and the Spanish federal police of only about 15,000 people with the help of helicopter images.

Excerpt screen shot from El Pais.
Experience has shown that the police always give quite low numbers of demonstrators, but the unusual use of aerial photographs makes this figure seem convincing.
If one calculates the real number percentage of the FFF fantasy figure, then you come up with three percent. 97% of the claimed figure is presumably a lie. Demonstration organizers like to give slightly higher figures, but 30 times above reality is already extraordinary, especially in a globally important political PR campaign filmed by numerous cameras.
The knowledgeable climate sceptic is also quite familiar with the 97% figure – with respect to ex-President Barack Obama and climate psychologist John Cook. One sees how huge and audacious exaggerations / counterfeits are part of the climatic profiteering business. Isn’t that risky? Wouldn’t it be better to manipulate more cautiously so that there’s less to be accused of?
Obviously, those who don’t panic enough achieve less. Moreover, even critical citizens often would not imagine that someone would publicly falsify in such a brazen manner.
Share this...FacebookTwitter "
"When released in 1975, Jaws not only transformed the face of cinema, it would also change the way many of us perceived the ocean. We were exposed to a vengeful, human-eating, boat-destroying great white that although fictional, would end up haunting our relationship with sharks for decades to come. It would appear that the 40-year-old shark is still stalking many of us. The iconic tones of the theme song is still heard by many as they enter the ocean: “Da-dum, Da-dum …” A dorsal fin breaking the surface still encourages imagery reminiscent of a puppy wrestling a chew toy, only with added clouds of crimson and shrills of terror. Jaws resonated so strongly with audiences because Peter Benchley, author of the original novel, took inspiration from real-life events. Shark incidents did happen and with expanding populations, more and more people were encountering them while enjoying the ocean. Stories of “rogue” sharks caused headlines that added kindle to the embers of fear that Jaws had started. The shark had quickly become the villain that we loved to hate. Much like the chiming bell of a buoy, the film’s tagline – “Don’t go in the water” – rang loud for many people. The reality was that even with increased shark accidents the number of annual incidents was low. In 2014, only three people died from shark bites. Dogs, cows, driving and even vending machines would kill more people in 2014 than sharks did. Yet, without fully understanding the shark, many beaches would employ destructive measures in an attempt to reduce the risks. Gill nets, hunts and shark culls were adopted in an attempt to reduce the risk to ocean users, all of which would end with the death of our new “villain”. As exposure of shark incidents increased, so did our fascination with this “monster”. While no doubt responsible for the fear felt by most, the film was also a catalyst for progressing scientific research within the field of shark biology. Primarily set on mitigating the occurrences of shark attacks, we slowly began to unravel the enigmatic world of these “big fish”. Portrayed in the film as an indiscriminate killing machine capable of seeking revenge and sinking our boats, we slowly began to see there was much more behind the shark’s dark, “doll eyes” than first appeared. Research groups around the world began dedicating their work to uncovering the cryptic behaviour of these animals. We learnt of their incredible feats of transoceanic migrations, complex 3-dimensional movements and intricate population structuring. They weren’t too dissimilar to us with social interactions, learned behaviours, and even feeding preferences. These revelations were made across the shark species, from freshwater to the deep-sea.  Turns out, not all sharks were formidable apex predators and we very quickly realised that few species actually posed any threat to us whatsoever. The most alarming of all scientific discoveries however, would capsize everything we thought we knew about our relationship with sharks. Sharks are key predators within ocean habitats and are, in turn, vital components within these environments. Removal of sharks from the oceans can have disastrous effects that reverberate throughout an ecosystem. Unfortunately, sharks were under decline. With post-film popularity in trophy hunting and an on-going global expansion of fisheries, sharks were being removed from the ocean at an unprecedented rate. The fate of many would be in the soup bowls on Asian tables, where a dish called shark fin soup was being consumed on a growing scale. With estimates of 63 to 273 million sharks being killed every year, the once feared villain was now the imperilled victim. Despite scientists unanimously warning of the importance and vulnerability of shark populations, many countries are still failing to provide adequate fisheries management and regulatory enforcement to inhibit further declines. In the 40 years following the release of Jaws, it is quite remarkable the advancements we have made in understanding the ocean and its sharks. Yet, despite these advancements, countries continue to retaliate to shark incidents in much the same way that the residents of Amity Island did in Jaws: with shark hunts and culls. A recent study that evaluated the kill-based strategies adopted by Western Australia (and other countries), exposed the ineffectiveness of these destructive methods. Instead, hazard mitigation policies should focus on expanding scientific research, developing non-lethal mitigations and further improving education, outreach and public awareness/opinion. Our relationship with the shark is one riddled with complexities. After four decades of research by people inspired by these animals, education, ecotourism and fisheries regulations are improving. However, fantastical media inaccuracies and economic greed are preventing us from truly escaping the misconceived clutches of one of the ocean’s most misunderstood monsters."
"This week, among the private chalets and deep snow of Davos, the world’s leading politicians and businesspeople have been spending their time at the World Economic Forum (WEF), and they’ve been talking about the climate crisis. Greta Thunberg and Prince Charles have given stark speeches warning of the dangers of a warming world, and CEOs and presidents have promised long overdue action.  For those of us in the climate movement, this shift of focus is no surprise. Last year changed everything. From the wave of schoolchildren going on climate strike, to large scale non-violent uprisings like Extinction Rebellion, 2019 proved to be the year that people in power couldn’t hide any more from the need to act. And then Australia caught fire: no amount of money could put those fires out. With this sudden focus on climate it’s no wonder that attendees of the WEF have come under fire for their choice of transport in getting there. Prince Charles flew to the summit in a private jet, before demanding new green taxes. And he wasn’t the only one. Figures from last year suggest over 300 private jets landed for the talks – somewhat overshadowing the summit’s eco-credentials, including the carpets made from “end-of-life fishing nets” and the rooms painted with renewable resources “like seaweed”. But though the egregious use of private jets is both deeply hypocritical and climate-wrecking, it shouldn’t be the main focus of our ire. Instead, let’s look at what those sitting around this year’s mountaintop tables have on their climate record. And let’s be real about how much big corporates can plan for a carbon-free future, in light of the fact that just 100 companies have been the source of more than 70% of the world’s greenhouse gas emissions since 1988. Just 20 companies have contributed to 480bn tonnes of carbon dioxide equivalent since 1965 – that’s a third of all emissions. Many of them – including the two largest emitters on Earth – are listed as “affiliates” on the WEF website. And it’s not just big oil – the financial institutions participating in the WEF are responsible for pumping at least $1.4tn into fossil fuel investments since the Paris climate agreement was signed. And then there’s the governments. Prince Charles’ speech was a powerful challenge to those in power, asking if “we want to go down in history as the people who did nothing to bring the world back from the brink?” But the UK government was this week revealed to be supporting fossil fuel mega projects around the world with the combined emissions of a country the size of Portugal. Such climate failure is not a byproduct of the economic system advocated by the WEF, it is baked into their model. An ideological commitment to free markets – advocated by those at Davos every year for the summit’s 50-year history – has been a consistent block to progress on the climate crisis. Just look at the world emissions data – an endless pursuit of economic expansion, of marketising every aspect of our lives, of extracting oil and gas to the point of no return, has put us on course for climate catastrophe. Climate data isn’t, as Thunberg says, “anyone’s political opinions or views”: it’s just a statement of fact. And that’s why over the last week I trekked across the mountains to Davos. Alongside 1500 people I spent three days walking from Landquart to Davos in possibly the coldest and most high-altitude climate justice march of all time. On the final day, we even managed to block the road to Davos, which has been blocking progress for a fairer and more just world the past 50 years. While Australia is burning and frontline communities all over the world are threatened by the very real consequences of the climate crisis, Davos-style meetings will never give us the answers we need. In truth, it would be foolish for anyone to expect a private club whose 1000 member companies have paid between 60,000 to 600,000 swiss francs to be a member (the more you pay the more access you have) should be trusted to solve an issue they created. A crisis of this scale needs scalable and just solutions – whether that’s a Green New Deal, divestment from fossil fuels, or decarbonisation targets in richer countries being rapidly brought forward. If the rich and famous at Davos really want to tackle the climate crisis, they should make this the last World Economic Forum. • Payal Parekh is an international climate activist and media spokesperson for the Swiss based Strike WEF Collective"
"A whistleblower from Bristol Zoo has revealed a series of unfortunate accidents involving endangered species. One involved a golden-headed lion tamarin, a small monkey native to Brazil, which fell into a pond and was killed by otters. In another incident, a male warty pig attacked his mate and killed his offspring. These critically endangered pigs are found on just two islands in the Philippines. The knee-jerk media reaction to such accidents is to directly or implicitly blame the zoo for what happened. “So much for conservation!” said the Daily Mail, which also said the zoo was “under fire”. Gawker even dubbed it the British Zoo of Horrors. However such reactions ignore something very important – it is impossible to completely eliminate accidents. Yes we should be asking whether an accident was foreseeable and therefore preventable. But we also need to consider the probability of it occurring in the first place. It is of course possible to imagine a monkey falling off a branch into a pool, but it is not highly probable.  Primates make thousands of movements per day in trees and yet rarely fall. It’s the same with us humans. I once fell on ice and broke my ankle but I knew the risk and I’d still take my chances again. Imagine a world or a zoo where we do not factor in probability when managing accident risk – a world of strict liability. Perhaps zoos should keep monkeys away from their monkey bars because of the risk of falling. When one reads the news reports about such accidents it is clear that probability does not matter – it should not have occurred full stop. Yet I wonder how many of these journalists or interviewees will be campaigning for the elimination of stairs. After all, these dangerous devices are responsible for millions of accidents and thousands of deaths each year. There is a serious issue here: if we ignore probability and try to eliminate all accidents in zoos then we will need to keep animals in padded cells. And social species could not be kept together in case one injures another one. It is after all foreseeable that these animals will compete aggressively with one another – male chimpanzees will fight for the opportunity to mate, for example. Animals living in an accident free world would have a terrible time and these animals would have minimal value for us humans in terms of education, conservation, research or leisure – the goals of modern zoos.   The case of the warty hog is similar. Males of many species can present a threat to females and their offspring. One way to eliminate this risk would be to use artificial insemination – but of course this is usually more risky than allowing natural mating.  Allowing accidents to happen in zoos simply mirrors conditions the animals experience in the wild. Monkeys may be well adapted for swinging between trees, but I have seen them fall to the ground on several occasions. Indeed, surveys have found that a third of wild primates have broken bones in their lifetime.  One of my friends saw a bear fall out of tree, which unlike my falling monkeys did not scream and remained expressionless as it fell.  The monkeys and bear all bounced got up and carried on with their lives, apparently none-the-worse for their accidents. Bears, big cats and polar bears are among the many species of mammals known to commit infanticide in the wild. My research team in Brazil even observed one incidence in wild titi monkeys.  Infanticide can even be one of the greatest sources of infant mortality for some animals. Yet there are no headlines about wild monkeys falling out of trees or polar bears eating their babies because we don’t feel responsible for the welfare of wild animals. The consideration of probability shows how absurd it is to believe we could create accident free zoos and that, even if we could, such zoos would not be in the interest of animal wellbeing."
"
One of the really odd discoveries that I’ve made while surveying climate monitoring stations around the USA is the fact that many of the official stations are located at sewage treatment plants. For example, the one in Colusa, CA is at their sewage treatment plant. I’ve visited it.
A couple of volunteers for www.surfacestations.org have been going around Washington and Oregon locating stations there and have also reported a number of stations at waste-water treatment facilities. I’ll get to why locating a temperature monitoring station at these facilities is a really bad idea later, but first I want to tell you why many of them are located at these places.
It has to do with the fact that somebody must read the thermometer once a day, write down the max and min temperatures for the last 24 hours in a logbook, then send in the page of the logbook to the National Climatic Data Center (NCDC) once a month. When stations were assigned to cities, they needed to locate them at a place where there was somebody 7 days a week. Sewage is a 24/7 operation. Police and fire stations have some stations for the same reason, somebody is always there.
Ok this picture comes in today from from surfacestations.org volunteer Steve Tiemeier, who visited the climate station of record located at the Urbana, Ohio Waste Water Treatment Plant:

The small item in the center of the picture labeled “MMTS” is the temperature sensor that is used to submit monthly climate reports to NCDC.
Now in case you don’t see some of the obvious problems with this location and why its a terrible place to measure temperature, I’ll list them one by one:
– Sensor is attached to the building, just mere inches away from brickwork
– Sensor is near windows, which radiate heat from heated interior rooms in winter
– Sensor is directly above effluent grates for waste-water, Waste-water is often warmer than the air many months of the year
– Sensor is between three buildings, restricting wind flow
– Sensor is between three buildings, acting as a corner reflector for infrared
– Several exhaust fans near sensor, even though one is disable, there are two more on the walls (silver domes)
– Air conditioner within 35 feet of sensor, enclosed area will tend to trap the exhaust air near sensor
– Sensor is directly over concrete slab
– Refrigeration unit nearby, exhausts air into the enclosed area
– Shadows of all buildings create a valley effect related to sunlight at certain times
– There are two nearby digester pools, which release heat and humidity in the sensor vicinity
– Heat and humidity plume over the site from digesters is often tens of degrees warmer than the air in the wintertime
Here is wider view that shows the temperature sensor in relation to the digester tank:

More picture on my image database here: http://gallery.surfacestations.org/main.php?g2_itemId=5322
I don’t know if any readers of this blog have ever driven by a sewage treatment plant in the winter, in the midwest, as I have, but I can tell you from experience it looks like a hot springs with steam rising into the air.
Talk about your urban heat island effect…not only that, sewage treatment plants effluent volume is a direct indicator of population growth. So as more water is treated, more local effects from the heat/humidity plume occur, which can affect the temperature readings.
There are dozens, possibly hundreds of USHCN climate monitoring stations sited at sewage treatment plants around the USA. I’ll have more reports on this in the future.
Who knew? I’ve been working in meteorology 25 years and I didn’t until this week.
here are some other stations at a sewage treatment plants:
http://gallery.surfacestations.org/main.php?g2_itemId=1489
http://gallery.surfacestations.org/main.php?g2_itemId=4658
http://gallery.surfacestations.org/main.php?g2_itemId=4388


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5d45db9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Polychaetes are a class of segmented worms that live under a wide range of oceanic conditions. Often, they are the dominant organisms found living in the sea floor, but they also thrive in the open ocean. According to Ricevuto _et al_. (2015), although knowledge of the potential response of these organisms to ocean acidification is growing, much remains to be learned, including “how their trophic behavior might change in response to low [less basic, or more acidic] pH.” In an effort to fill this informational void, Ricevuto _et al_. thus set out to examine food-chain interactions of three polychaete species ( _Platynereis dumerilii_ , _Polyophthalmus pictus_ and _Syllis prolifera_ ) and their organic matter (food) sources (macroalgae, seagrass and epiphytes) in a naturally acidified region of the Mediterranean Sea.   




The location for their study was a shallow water reef area on the north-eastern coast of Ischia, an island off the coast of Italy known for volcanic features, including underwater vents that release copious quantities of CO2. The vents produce a pH gradient in the area that provides “a natural laboratory for ocean acidification studies,” which the researchers further describe as “an ideal model system to conduct experiments investigating the effect of climate changes (particularly ocean acidification) on benthic community composition and structure, as well as on functional aspects, such as tropic interactions,” which was the focus of this study. And what did the study show?   
  
After collecting data and conducting a series of complex analyses, the three Italian researchers report “increased pCO2 did not alter the trophic interactions dramatically,” adding “there seems to be a resilience in the trophic pattern, possibly due to the tolerance of the target species to acidification and potential local acclimatization and/or adaptation (see Calosi _et al_., 2013).” Such “phenotypic plasticity” (the ability to alter biochemical reactions based on environmental changes such as increasing temperature or acidity) observed in the three polychaete species studied, according to Ricevuto _et al_., “may allow them to respond well to alterations in the environment and eventually offset near-future ocean acidification scenarios.” Thus, as the researchers ultimately conclude, “for some species, like the ones considered in this study, ocean acidification may not represent a dramatic stress.” And that’s good news worth reporting.   
  
  
  
**References**   
  
Calosi, P., Rastrick, S.P.S., Lombardi, C., de Guzman, H.J., Davidson, L., Jahnke, M., Giangrande, A., Hardege, J.D., Schulze, A., Spicer, J.I. and Gambi, M.C. 2013. Adaptation and acclimatization to ocean acidification in marine ectotherms: an _in situ_ transplant experiment with polychaetes at a shallow CO2 vent system. _Philosophical Transactions of the Royal Society of London B Biological Sciences_ **368** : 20120444.   
  
Ricevuto, E., Vizzini, S. and Gambi, M.C. 2015. Ocean acidification effects on stable isotope signatures and trophic interactions of polychaete consumers and organic matter sources at a CO2 shallow vent system. _Journal of Experimental Marine Biology and Ecology_ **468** : 105-117.


"
"Britain’s energy regulator has said it will change how it governs the industry to help meet the government’s climate targets, after coming under fire for failing to prioritise the climate emergency. The regulator published a wide-ranging climate action plan on Monday, which aims to help get 10m electric vehicles on our roads by 2030 and support a fourfold increase in offshore wind generation, while protecting homes from rising energy bills. The nine-point manifesto also includes plans to support low-carbon home heating, tariffs that encourage homes to help balance the energy system, and a crackdown on “greenwash” energy deals. Ofgem’s incoming chief executive, Jonathan Brearley, set out the regulator’s climate manifesto after critics warned that its outdated statutory duties were not aligned with the government’s climate policies. Ofgem was set up to regulate energy companies and safeguard consumer interests, often against price increases. The regulator admitted that it faces trade-offs between supporting ambitious green investments – paid for through energy bills – and the need to protect homes from rising costs. The Guardian reported last year that Britain’s biggest business group, the CBI, was concerned that the regulator’s existing mandate sent “negative signals” to low-carbon investors. The CBI called for a legal change to the regulator’s statutory duties to directly prioritise tackling the climate crisis. On his first day as the regulator’s new boss, Brearley signalled that Ofgem plans to balance the tension between green investments and energy bills without a legal overhaul. “We are taking an approach that recognises that our role protecting consumers includes achieving net zero,” he said. Gillian Guy, the chief executive of Citizens Advice, said: “Ofgem has set out the right challenges, it now needs to deliver on them. If we don’t get the difficult decisions about the low-carbon transition right, it will ultimately be those who can least afford it who end up hardest hit.” The first point in Ofgem’s new climate manifesto will change how the regulator controls the spending of energy companies, in order to avoid stifling investment in low-carbon technologies with a rigid eight-year spending plan. The regulator came under fire from the National Audit Office last week for allowing energy networks to rake in bigger than expected profits by not being tough enough in the price-control plans. Under Ofgem’s new plan, network companies may have the chance to ask for their spending plans to change during set windows within the eight-year period, so that they can adapt their green investment plans as the sector evolves. The change appears to answer fierce criticism from the boss of Scottish Power, Keith Anderson, who last year accused the regulator of hindering the UK’s electric vehicle rollout due to its “colossal disconnect” with Britain’s climate policies. Scottish Power had hoped to make extra investments in car-charging in anticipation of an electric vehicle boom, but Ofgem said the plan would not offer good value for money if the demand failed to materialise. Brearley said: “As low-carbon renewable energy grows and more transport goes electric, the energy system needs to be more flexible to respond to peaks and troughs in both supply and demand. Our new price controls for network companies will clear the path for this, providing the incentives for investment for the future.” Ofgem will also create a fund specifically to unlock investment in innovation on tackling greenhouse gas emissions, and help find ways to encourage homes to use their car batteries to help balance the energy system and use more renewable energy when it is available. Nicola Shaw, the UK boss of National Grid, said: “It’s critical that the regulator, government and industry are aligned to decarbonise the energy sector in the journey to net zero at the lowest cost to consumers, and we both welcome and share Ofgem’s commitment to achieving this.” Make price controls more adaptable to help firms invest in clean energy. Set up a regulatory fund to help invest in climate-change solutions. Explore ways to create a “lowest cost” offshore grid to support wind power. Work with government and industry to decarbonise heating. Make UK energy systems fit for a net-zero future. Create a more flexible electricity system to help move towards net zero. Develop a regulatory strategy to help get 10m electric cars on the road by 2030. Support energy firms to create low-carbon products and services for consumers. Change its regulatory approach and take “big decisions” on decarbonisation faster."
"It is 2020, the world is on fire, and some of us are in the midst of a gripping debate about exactly what we should have in our cereal. As we learn about the environmental impacts of mass cattle farming, the future is looking bleak for the beef and dairy lobbies. US citizens need to cut milk by 60% and beef by 90% to prevent global heating changes, according to researchers. If dairy’s out, then, what milk is best for the world? You can now get milk from soy beans, cashews or tiger nuts - just to name a few. All are far better for the planet than dairy, according to a 2018 study. (The Guardian’s own analysis of vegan options was published this week.) But the dairy lobby is not going down without a fight, going as far as demanding that milk alternatives cannot be called “milk”. Today, Virginia’s legislature has proposed to make alternative milks parading as real milk “unlawful”. What is real milk, you ask? According to them, it is “the lacteal secretion of a healthy, hooved mammal.” (Yum!) This has led to existential questions such as “do we have to change the name of peanut butter?” and “what about breast-milk?” There is, in fact, an exception in the bill that accepts that breast-milk is the real deal, but the fact that a loophole needs to exist does make you wonder what this whole thing is really about. In the words of Shakespeare, “a rose by any other name would smell as sweet.” Perhaps that’s the problem. This article was amended on 31 January 2020 to correct that the Virginia legislature has proposed the change, but it has not been signed off by the Senate and governor. "
"MPs in the UK recently voted against a moratorium on hydraulic fracturing but Lancashire, the local county council under most pressure, agreed it needed more time to make a decision. MPs also agreed that sensitive areas should be excluded, ruling out many of the UKs shale areas. The Environmental Audit Committee concluded that development was still some way off and that under the Climate Change Act, emissions from this source should fit within Britain’s 2020 obligations. Since coal-fired stations will cease under EU directives with or without fractured gas, the Tyndall Centre concluded that Britain’s reserves of shale gas might be unburnable under carbon dioxide  targets. With public concerns still unmet, any rationale appears less favourable; however the underlying factors have changed little. Between January 2014 (blue on chart) and January 2015 (red), the spot price of oil for immediate delivery fell more than 50% from above $100 to near $50 per barrel. However the price of oil for future delivery (in the centre region, say from 2017) has fallen only 30% from about $95 to $65.  Longer horizon prices of oil (on the right for 2020) remain around $80 per barrel; these prices are much less variable and offer better indicators of future demand, the need for supply and the pressures that balance the two. Thus the current situation of “contango”, with spot or immediate prices lower than those on the longer horizon (upward sloping red curve), is indicative of a short term oil glut that is not expected to persist. Oil and gas production from the North Sea is declining.  Until new energy forms and generation such as renewables can catch up, other domestic sources will be necessary or the UK will have to import more from overseas – further increasing its exposure to world market prices. With its offshore experience, the UK has a good track record on drilling and recovery and a skilled labour force. The level of environmental scrutiny and remediation is increasing for used oil rigs. Sharing this expertise with shale well developments would partially address the environmental issues; certainly it is more appropriate to have greater scrutiny of onshore work. Compared to offshore, the environmental risks of fracking are concentrated in the first year, after which residual issues are lower compared to oil platforms which need removal. Unions see the strength of this. Alongside continuity of employment and training, they have also made the case for new energy projects to be located domestically rather than in developing countries where there may be easily accessible energy but poor protections for workers and residents. In the longer run renewables will make the biggest change. However sun, wind and waves are intermittent sources without storage mechanisms for that calm winter night so complementary forms of generation are likely to be required. Gas is the most flexible of fossil fuels; it is already widely liquified for transport and import but when piped into homes for heating, it can also be used for electricity generation via Stirling engines or fuel cells which can also power cars instead of petrol. If fracking is to make a positive impact within tough carbon budgets, its gas will have to be used more efficiently and creatively than before. Rather than burning it in power stations, its biggest contributions could be to generate electricity in the home (with the heat as a by-product) or in car transport (replacing petrol and oil imports). If home and car owners have incentives to adapt their energy usage in these ways, gas will prove the transition fuel that many seek."
"A recent online video took what seemed like an inspirational moment viral. The video, shot by Dmitry Kedrov using a drone, shows a baby bear climbing up and falling down the side of a mountain near Russia’s Sea of Okhotsk. After repeated efforts, the cub finally reaches the top, joining his mother and winning the hearts of viewers around the world. But after the initial enthusiasm for the video came some controversy when scientists pointed out that the incident may have been caused by the drone risking the cub’s life by interrupting its efforts to climb to safety. This prompted some online commenters to call for drones to be banned on grounds of environmental impact, while others defended the responsible use of this technology. My colleagues and I have been researching the impact of drones on wildlife, and found that they pose very similar kinds of threats as other disturbances such as people, cars and conventional aircraft. This suggests that rules and guidelines that took animals into account would make a big difference to how much harm remotely and autonomously controlled aircraft could cause to wildlife through their noise and visual presence. When animals come into contact with drones, they may experience physiological changes such as an increased heart rate, behavioural responses such as running or flying away, or even suffer stress that could disrupt their reproductive process. If they decide to avoid specific areas as a result of frequent disturbing drone encounters, this could fragment and ultimately damage the whole population.  Unfortunately, there is no reliable indicator that can give us an idea of the extent to which these flights are affecting wildlife. But this does not mean that there is no need to worry, because drone use is expected to increase in coming years. Exactly how serious the threat from drones is depends on how often and how intensely they disturb the animals. If they are frequently disturbed, the animals will likely abandon the area, but they could also eventually become used to the drones. At worst, if drones fly too close to animals, collisions or attacks can cause wounds or death. Also, not all animal species nor individuals react to drones in the same way, and they may be more vulnerable in certain moments, such as breeding season, or in areas without protection or escape routes. With all this in mind, drone operators should try to minimise the impact they have on wildlife. To start with, they should consider why they want to fly into or near an animal’s habitat and whether they really need to. When scientific projects are planned, they have to be approved by ethical committees and the potential disturbance has to be justified by the interest of the project. Risking the life of an animal to create a popular online video is unacceptable. But if you are trying to gather data for a conservation project, a small disturbance to wildlife in order to protect it can be justified, and drones may be the least impactful way to do it. After all, any method of gathering animal data involves a certain degree of disturbance. Cars and manned aircraft are substantially more noticeable and noisier than drones. Monitoring a bird breeding colony on foot causes considerable chaos. Trapping animals involves considerable risks and equipping them with sensors or GPS can harm them. If using a drone is the best option, it’s best to minimise the risk of disturbance and accidents by using an experienced pilot and a reliable, small and low-noise drone that doesn’t resemble the shape of a predator. Missions should be as short and at the highest altitude possible, using a regular, back-and-forth flight pattern over the animals and not complicated manoeuvres directed towards them. If it’s convenient, drones should take off and land at least 100 metres away from the animals and avoid disturbing them during breeding periods and at times of day when they may be most vulnerable. It’s also important to monitor the target animals during flight so you can check if they are being disturbed and abort the mission if necessary. You should also avoid protected areas, flying over sensitive species or abundant wildlife. Flying drones around animals requires basic knowledge and respect for wildlife. But the reality is that drone users without a wildlife background may not be aware that they are flying into a raptor breeding territory or that the drone noise may disturb animals on the ground. We encourage drone manufacturers to help by including this kind of basic advice with the instructions that come with the drones.   But what about people who choose not to follow these kind of guidelines? I think we need a legal framework so that appropriate actions can be taken when wildlife is negatively affected by irresponsible drone operators. Then perhaps people won’t be so keen to risk disturbing animals for the sake of YouTube views."
"

Dr. Gavin Schmidt, a lead researcher with NASA’s Goddard Institute for Space Studies (GISS) that does leading climate change studies, replied to one of my posts and made an assertion that the USHCN and GHCN stations and station data being discussed here in my blog are not used in validating climate models. This is surprising to me.
Here is the full correspondence:
Schmidt’s first post:
> Don’t let me get in the way of your efforts here, but please stop saying that “This data is in fact used in climate modeling to predict our climate future”.
>
> This is simply not so.
>
> You’ve downloaded the GISS model – perhaps you’d like to show me where these station data are used? You won’t be able to because they aren’t.
>
> Observational data at large scale (not individual stations) are used to evaluate the models after they’ve been run – but again generally only at the continental scale and above. The evaluation is not just with trends but
> with patterns of variability (El Nino responses, NAO etc.) and obviously, the better the data the more reliable the evaluation.
>
> Note that the climate model hindcasts for this area are around 0.5 over the 20th Century – significantly less than this individual station. Should this record therefore be shown to contaminated, it would actually improve our confidence in the models, not lessen it!
I responded to this on June 21st 2007 as follows:
> Gavin,
>
> I thank you for commenting on my blog, Watts Up with That? I’m honored
> that you would take the time. Rather than reply immediately, I thought
> I’d give some thought and research to my response, hence the delay. I
> also thought you’d appreciate a direct reply rather than a blog post.
>
> You wrote on the blog:
>
> “You’ve downloaded the GISS model – perhaps you’d like to show me where
> these station data are used? You won’t be able to because they aren’t.”
>
> I did some looking at a paper you authored, I found Schmidt et al 2006,
> from BAMS, which is also posted on your website:
> http://pubs.giss.nasa.gov/docs/2006/2006_Schmidt_etal_1.pdf
>
> You wrote on page 168 of the BAMS article:
>
> “We endeavor to compare the model simulations to as many suitable
> datasets as possible. … . Where useful gridded datasets exist of
> selected in situ data we use those.”
>
> After reading through your paper, I agree that you did not show any
> comparisons to GISS gridded data and I will withdraw any implication
> that you used GISS station data. However, I must say that I’m surprised
> to learn that GISS gridded data did not meet the standards of Schmidt et
> al 2006 of being either “useful” or “suitable”. Thank you for drawing
> this to my attention.
>
> However, later in the article, on page 176, you show comparisons of
> model output to CRU surface temperature data on two occasions:
>
> “Surface air temperatures (SATs; Fig. 17) show a general warm
> continental bias in comparison to the updated Climate Research Unit
> (CRU) data (Jones et al. 1999).
>
> Figure 23 on page 187 shows Taylor diagram comparisons among the
> selected models for the December-February (DJF) and June-July (JJA)
> extratropical NH CRU surface air temperature (SAT)”
>
> It is my understanding that CRU uses GHCN station data, which includes
> the USHCN sites discussed here in my blog. So, my answer to your
> question is that Figures 17 and 23 of Schmidt 2006 et al use the station
> data discussed here via the CRU gridded data. It has always been my
> understanding that adjusted GHCN and USHCN surface station data (also
> listed on the GISS webpage) including the ones I show plots of, is
> applied to a gridded data scheme for use in the computer models, such as
> model E. If I am in error in that assumption, I welcome you pointing out
> that error.
>
> If you felt that I was speaking of a specific station data being “used
> to predict our climate future” that of course is not my intent. If that
> was the case, I’ll revise the wording to make it clearer.
>
> Regarding your mention that “contamination of station data would improve
> your confidence in your model”, I must say that I’m a bit surprised at
> this. I’m not really in a position to dispute this yet, but would
> appreciate some additional clarification as why you are so certain of
> this without even seeing the impact of contaminated data. I surmise the
> opposite to be true, but I welcome further understanding.
>
> Again I thank you for your comments, and I welcome any correspondence or
> suggestions you may have.
>
> Best regards,
> Anthony Watts
Dr. Gavin Schmidt replied on June 22nd, 2007 with:
My comments stand. The station data are not used *in* climate models, and
they are not used to predict future climate. So yes, the sentence you have
is just wrong. I’m not sure how you could edit it to make it correct.
We compare the models to the gridded products that deal with individual
station problems as best they can. We have used the GISTEMP and CRU
products to do so. (Semantic note, ‘compare to’ is not the same as
‘include in’). For the specific station you have highlighted, the grid
point trends in the products (~0.5 deg – eveballing it, since I’m on
travel) are significantly less than the trend you show (2 deg or so).
Climate model results for the 20th C are similar (i.e. 0.5 deg). Thus
reductions of the trend at this station would actually improve the match
to the model – always being clear that you shouldn’t really compare
model grid boxes to individual stations…
If you are of the opinion that this station is contaminated, then you have
to admit that the process designed to remove artefacts in the GISS or CRU
products has in fact done so – (i.e. that grid box in the product does not
have a 2 deg/Century trend).
Improvements to that process and the data are always welcome, but do not
ascribe consequences to your project that clearly do not follow.
Gavin
*——————————————————————–*
| Gavin Schmidt NASA/Goddard Institute for Space Studies |
| 2880 Broadway |
| Tel: (212) xxx-xxxx New York, NY 10025 |
…| |
*——————————————————————–*
[email address and tel# removed by Anthony for privacy/spam purposes]
So one has to wonder.
If Dr. Schmidt’s point is only the observation that they do not reconcile their models with every individual station (as opposed to gridcell composites calculated by GISS and CRU), then there is no misunderstanding.
However, it is very clear that the NASA GISS and CRU (Climate Research Unit) use this station data in arriving at their gridcell values which are what is presumably used in testing the models.  From 53 USHCN site surveys done so far we know that a number of stations do not meet published WMO (World Meteorological or NOAA (National Oceanic and Atmospheric Administration published standards.
There is no evidence at present that NASA GISS or CRU have made any effort to verify quality control standards at these USHCN stations. Whether these quality control issues will have a significant impact on overall averages remains to be seen.  The only way to tell for certain is by examining individual stations though the site survey process as is being done on www.surfacestations.org and then doing an assessment of how pervasive the quality control problems are and what the potential impact of these problems may be.
But, any problems in individual USHCN stations will affect gridcell values. For non meteorologists, a gridcell is a box on a map that has been divided up into a x-y  lines and specific data applied to each box. This helps in computer modeling because with computer programs it is easier to divide into cells, then calculate and display. Below is an example  map that may help you visualize gridcells:

Whether it’s a big problem or a little problem remains to be seen, but it’s odd for Dr. Schmidt to pretend that it’s not a problem because they use the gridded version of the data.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5f332f6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Stephanie Kelton’s _The Deficit Myth_ is quite the talk of the town. To quote Amazon’s webpage:



It’s an attractive vision, but it doesn’t work.



I am reminded of Einstein’s time at the Swiss Patent Office where he used to check applications to patent perpetual motion machines. They don’t work, but the fun is working out why. The same applies to proposals to bring about prosperity that depend on loosening the monetary spigots. MMT is a perfect example.



MMT is a macroeconomic school of thought in the post‐​Keynesian tradition. Its central tenets: fiscal deficits don’t matter; monetary policy should be subordinate to fiscal policy; and the monetary authorities should be willing to issue base money to finance government spending. MMT is associated with large‐​scale government spending, a focus on ending involuntary unemployment, and programs to alleviate poverty and fight climate change.



Kelton’s book builds on earlier work by Warren Mosler and Randall Wray but has its roots in Abba Lerner’s system of “functional finance,” which goes back to the 1960s. She builds on Lerner primarily by adding a federal job guarantee that would eliminate involuntary unemployment and provide an automatic economic stabilizer.



MMT makes _big_ promises. It would “build a more just economy that works for the many and not just the few” and put “people and planet first.” “MMT’s lens enables us to see that another kind of society is possible, one in which we can afford to invest in health care, education, and resilient infrastructure. In contrast to narratives of scarcity, MMT promotes a narrative of opportunity.”



But does MMT deliver? Let’s see what she says.



“The idea that taxes pay for what the government spends is pure fantasy,” writes Kelton. Really? Let’s go back to basics. The government must finance all its expenditures. In a world in which it does not issue debt and does not issue currency, and assuming away any gifts it might receive, all its expenditures must be financed by current taxation.



If the government can issue debt but not issue currency, then it can finance its expenditures by current taxation or by issuing debt. But to issue debt is to pass on the obligation to repay that debt to future taxpayers. If that debt is to be repaid, then it must be repaid out of future tax proceeds.



If the government can issue its own currency and monopolizes the issuance of currency, then it can also pay off its debt obligations as they come due by issuing additional base money (“printing money”). Does this mean that printing money allows the government to avoid the need to raise taxes? No, because printing money lowers its value against goods and services, and so operates as a tax on money holdings and other holdings of wealth that are fixed in nominal terms (such as level annuities). So, barring gifts, all government expenditures must be financed by taxation in one form or another.



Kelton explains:



This sounds great: involuntary unemployment eliminated and everyone willing to work gets a high federal minimum wage or more, to the extent that market wages are forced higher to compete. But hold on. If it is such a good idea, why not raise the minimum wage beyond the $15 an hour she suggests? Why not $30 an hour? Or $50? The problem is that there are a raft of jobs that are profitable to provide at existing wages but would disappear at higher wages.1 It is not just the existing unemployed who would end up on federal payrolls but these newly unemployed too, and many of their employers. Think of the restaurant sector. That sector and others in the same position could only survive by hiking their prices: dining out would become a lot dearer. Ordering in, too. The federal government, the employer of last resort, would find itself with the problem of what to do with all these people also turning up for guaranteed jobs. The feds would have crowded out much of the labor market and wiped out the lower paid sectors of the economy.



“Why does the financing have to come from Uncle Sam?” asks Kelton. “Simple. He can’t run out of money.” Imagine that the government pays debts coming due by handing over dollar bills that it has in a chest in the basement. If it runs out of bills, then it will default the next time a payment comes due. But then imagine if it can also print money. If it runs out of bills, it can avoid default when the next payment comes due by printing more. It does not follow, however, that the government can _always_ meet its payment obligations by printing more money.



Suppose the government prints money at an accelerating rate and we end up with an accelerating hyperinflation. The traditional tax‐​collection apparatus will break down because the tax revenue will be worth almost nothing by the time it comes in. Similarly, the government will effectively be unable to borrow in its own currency because the borrowed funds would also be worth next to nothing by the time they come in. As the hyperinflation accelerates further, the real value of the revenue from printing money also goes to zero. The government then faces the prospect of default despite being able to print any amount of its own money. To give an example, by the end of the Hungarian hyperinflation of 1946, the total value of all Hungarian notes in circulation was a thousandth of a U.S. cent (see Judt 2006: 87). The Hungarian government didn’t have a cent, let alone a dime. The Hungarian government would have been unable to make repayments denominated in other currencies or make inflation‐​linked payments in its own.



The mistake is to presume that what is correct at the margin (i.e., that the government can avoid default by issuing a few extra dollar bills) is also correct under any circumstances, that is, at any scale. There is also the related point that issuing a small amount of money will have a negligible impact on prices but issuing a lot of money will not.



It is a “myth,” writes Kelton, “that deficits will burden the next generation.” This claim is also wrong. Suppose Congress passes a Boomers Boomtime Act to provide for a humongous 75th birthday payout to each surviving member of the first Boomer cohort born in 1946. They will reach 75 in 2021. These payments are to be financed by a zero‐​coupon bond with a 40‐​year maturity. Since none of the beneficiaries will be around to pay taxes when the bond is due to be repaid, they get a free handout.



Who bears the burden of paying for it? When the Boomer bond comes due in 2061, the government faces the following choices: (a) pay it off by raising taxes, (b) pay it off by issuing money, (c) default, (d) pay it off by rolling over, that is, by issuing a new bond.



If (a), then the burden is borne by taxpayers in 2016.



If (b), the subsequent price level is higher, so the burden takes the form of a tax on money holdings and other instruments of fixed nominal value.



If (c), default, the burden is borne by those who suffer the adverse consequences of default.



If (d), then the rollover will mean that there will more debt after 2061 than there would otherwise have been and we have the same choices again when the new payments come due. If the decision is to roll over each time, then the debt/​GDP ratio will hit a level at which the government defaults sooner than otherwise.2 Thus, however the government responds when the Boomer bond matures, some group born after 1946 bears a burden from it.



More generally, any arrangement that involves one group issuing a debt that another group is expected to pay for _necessarily_ burdens the second group. The injustice is all the worse because the second group has no say in the matter.



There are also the government’s entitlement programs, Social Security, Medicaid, etc. This takes us to Kelton’s “myth” that “entitlements are propelling us toward a long‐​term fiscal crisis.… There is absolutely no good reason for Social Security benefits, for example, to ever face cuts. Our government will always be able to meet future obligations because it can never run out of money.”



These programs however are just another form of debt insofar as they create obligations on the government’s part to make future payments. Consequently, my earlier argument, that programs that create future obligations burden future generations, applies here also.



Entitlements are large, so the corresponding burdens would be large as well. To illustrate, there are perhaps $210 trillion in entitlements, and possibly more.3 If these entitlements are to be paid for by future taxation, then that is a lot of future taxation. If they are to be paid for by rolling over, then we would anticipate the ratio of debt (including entitlements) to GDP rising considerably, possibly to default levels.



Then there is the option of meeting those obligations by printing money. Given that the current stock of base money is just over $5 trillion, that response implies a possible 42‐​fold‐​plus expansion of the monetary base. That, in turn, implies a considerable increase in prices. Making entitlement payments is one thing, but the purchasing power of those payments is another.



We have here another instance of the margin vs. scale issue. The government can increase entitlements a little with next to no impact on their real value. But if the government creates huge entitlements to be financed by printing money, then those entitlements are going to be greatly devalued in purchasing power terms. And what the government must absolutely _not_ do is create huge entitlements that are inflation‐​linked and then rely on printing money to finance them. If it does that, it will produce both hyperinflation and default. The “myth,” that the national debt is a problem, is not a myth.



We can break down MMT into a set of policy _ends_ (what the government spends _on_ ), and a set of policy _means_ (how the government finances its spending), which in the case of MMT would involve large deficits and a lot of borrowing and money printing. The expenditure and the financing of that expenditure are two different issues. Bernie Sanders might use MMT to advance a more right‐​wing version of the Kelton agenda, but Donald Trump might seize upon the spending opportunities promised by MMT to advance his own, even more right‐​wing, agenda, for example, to promote policies that work for the few and not the many.



My point is that it is short‐​sighted and potentially counterproductive to promote a particular policy package such as MMT because _you_ can use it to finance projects that _you_ like, because someone else might use it to finance projects that you do _not_ like. To the extent that MMTers persuade people that MMT‐​based government finance is a good idea, they can hardly restrict that message to people who share their own political views. If you think of MMT as a government‐​financing package, then that financing package can be used to finance any government spending program, whatever its political hue.



A deeper issue is that any policy that gives policymakers the appearance of being able to spend a lot without having to bear the unpopularity of the high taxes needed to finance that spending is a dangerous one and invites abuse. Any such policy entails a major shift in power away from the legislative branch to the executive branch, because it gives the latter additional means of finance that bypass constitutional constraints against government overspending. To elaborate, the Constitution says that fiscal policy, the power to tax and spend, is constrained by the need to obtain congressional approval. If there were no Fed, or if the Fed were genuinely independent, then Congress could deny appropriations for spending projects of which it does not approve. But if the executive branch has the power to print money, then it has a potential means to circumvent Congress. If Trump wants his wall and Congress denies the appropriations, he can then order the Treasury secretary to print the necessary money instead. From this perspective, MMT is something of a constitutional abomination.4



It is a fundamental principle of constitutional political economy that economic policymakers operate under rules that constrain the decisions they make, and that these rules should be designed in ways that prevent undesirable behavior on their part. Under this way of thinking, the rules operate as bulwarks that constrain policy makers in order to protect everyone else from the misuse of the powers entrusted to those policymakers.



For proponents of MMT and for many other advocates of big government, however, those rules serve no real purpose and merely constrain policymakers from achieving the lofty ends that they seek to pursue. Yet they fail to appreciate that lofty ends do not justify giving those in power unrestrained discretionary powers. As Adam Smith observed:



Should MMT be adopted, then the Fed would become subservient to the Department of the Treasury, but in a more nakedly obvious way than it was during the years before the 1951 Treasury‐​Fed Accord, and without operating under the constraints of the Bretton Woods watered‐​down gold standard. Personally, I would suggest that, if MMT were adopted, then the government should make the new monetary policy arrangements transparently obvious. The Fed’s independence, such as it was, would be history and there would be no point pretending otherwise. The government should nationalize the Fed and make it a division of the Treasury, whose responsibilities would then be to issue currency and manage the national debt. Nationalizing the Fed would highlight the underlying chartalism of MMT (i.e., the idea that money is a creature of the state),5 and would also simplify analysis going forward because it would cut out the need to consider Fed/​Treasury interactions that only mask the underlying reality. We could then talk openly about the _government printing money_. If the United States is going to embark on a monetary policy worthy of a banana republic, then it should look the part.



Suppose then that the government goes full throttle MMT à la Kelton. This spending must be financed, however, and the government would have to do so by some combination of levying taxes, borrowing, and printing money. In essence, she proposes high government spending and a big deficit financed by borrowing (“debt finance”) and printing money (“monetary finance”).



If such a policy were launched at a time when there is considerable unemployment, then one would suppose that unemployment would fall. But there must eventually come a time when the economy returns to more or less “full” employment, whether because of those policies or despite them being a separate question. What happens then? To examine this question, I built a model along “unpleasant monetarist arithmetic” (see Sargent and Wallace 1981) lines and got some interesting results.



Let’s consider the following three possible MMT policies: (1) the government pursues pure debt finance; (2) the government pursues pure monetary finance; and (3) the government pursues debt finance for as long as possible, up to the point where it is about to default, and then switches to monetary finance.



Let _d_ be the long‐​term growth rate of the deficit and _g_ the long‐​term rate of economic growth. It is reasonable to suppose that MMTers would want _d_ considerably in excess of _g_ , so let us assume that this is so. Under a policy of pure debt finance, the debt/​GDP ratio would grow relentlessly and must at some point reach a level at which the government defaults. Hence, debt finance makes government default inevitable if pursued for long enough—that is, pure debt finance is unsustainable.



Under a policy of pure monetary finance, _d_ becomes the key driver of the inflation rate. If _d_ is steady in the long‐​term, then the inflation rate will converge to a long‐​term steady state. But if _d_ itself grows, so the deficit grows at an _accelerating_ rate, then long‐​term inflation will also accelerate.



Under the third policy, the debt/​GDP ratio rackets up to the brink of default, then the government switches to monetary finance with similar long‐​term outcomes as pure monetary finance.



It is important to emphasize that under monetary finance there is no way in which the government can simultaneously pursue an inflation target. Instead, the inflation rate becomes a residual outcome from the government’s fiscal policy and the government loses all control over the inflation rate. To make matters worse, the inflation rate also becomes the macroeconomy’s main shock absorber, so any shocks that produce unexpected increases in the deficit will lead to unexpected increases in inflation, which makes inflation highly uncertain too. A policy of monetary finance is therefore dangerous, because it can easily lead to runaway inflation or even hyperinflation. Externally, these effects on prices and inflation will be reflected in a falling, volatile and uncertain exchange rate.



In sum, we have a variety of possible long‐​term consequences, ranging from merely bad (highish and uncertain inflation, loss of control over inflation, a volatile exchange rate) to positively catastrophic (huge levels of national debt, high future taxation, national default and all that might entail, runaway inflation, hyperinflation). I could go into a long discussion of why any other fiscal‐​monetary policy mix would produce better long‐​term outcomes. On the fiscal side: a balanced budget or lower deficit financing. On the monetary side: a monetarist rule, a nominal GDP target, a Taylor Rule, a gold standard, whatever.



Suffice to say that the poor performance of MMT is not a coincidence. On the fiscal side, it encourages a much more rapid run‐​up of the debt/​GDP ratio than any alternative, which has got to be the worst possible fiscal policy in the long term. On the monetary side, it throws away any attempt at controlling inflation or maintaining monetary stability by making monetary policy subservient to runaway government spending. MMT performs so badly _precisely because_ it represents the extremes of fiscal and monetary excess.



Indeed, MMT does not even work on its own terms. Kelton herself indicates that an MMT policy package is constrained by the requirement that inflation should not rise. Unfortunately, she hasn’t thought it through.



Current U.S. inflation is 2.3 percent. Is she suggesting that the government should pursue MMT subject to the constraint that the inflation rate should not rise above 2.3 percent? If so, she should advertise the fact to help dispel the concerns of those who might have gotten the impression that MMT is some sort of funny money scheme. If she did so, much of the knee‐​jerk opposition from sound‐​money people would dissipate. The problem, however, is that a commitment to maintaining inflation at no more than its current rate would severely constrain the ability of MMT to deliver on its promises.



A looser interpretation of her “inflation shouldn’t rise” constraint would apply to inflation in the long run. But then consider my unpleasant monetarist arithmetic results. Since pure debt finance is fiscally unsustainable, any MMT package must involve some element of monetary finance. But, _ex hypothesi_ , the monetary finance must be constrained by the need to avoid rising inflation in the long run. A necessary condition to prevent rising inflation is that the rate of growth of the deficit should not itself increase. This constraint is not as severe as requiring that current inflation should never rise, but it is a severe constraint nonetheless. The problem is that it is not at all clear how much of what she promises can be delivered while satisfying this constraint. That she does not address this issue is the central failing of her book.



In effect, she offers us the prospect of a bunch of goodies but doesn’t explain why those goodies fall within the economy’s production possibility frontier, that is, are actually attainable—an intriguing oversight for an economist.6



She offers a revealing anecdote when she recalls a discussion between James Tobin and President Kennedy:



“Everything else is just talk” captures it perfectly. Indeed, it undermines the entire book.



We shouldn’t forget what subsequently happened. Government spending on the Vietnam War and the Great Society overdid it, and the United States ended up with rising inflation and the monetary troubles of the 70s and early 80s. MMT’s fiscal proposals are akin to the Great Society and its monetary proposals are akin to relying on the pre‐​NAIRU, or fixed, Philips Curve that was discredited by Milton Friedman. From this perspective, MMT has a distinct sixties feel to it.



In both cases, the root problem is the same: the absence of a coherent theory of inflation. The Keynesians of the sixties didn’t have one and neither does Kelton. Now, as then, the solution to that problem is the same: their macro model needs some version of the quantity theory of money to connect the money supply to the price level, and thence to the inflation rate. If the central bank or the government pursue policies that lead to too much monetary growth, then the result will be a higher than desired inflation rate. The solution to that problem is also the same as it was then: to rein in the rate of monetary growth. In short, MMT is not particularly modern and the monetary theory has too much money and not enough theory.



Kelton might object that these negative outcomes would not occur under MMT because counter measures would be taken once (or even before) inflation started to rise. So, what counter measures would she pull from her MMT toolbox? The answer is a rise in taxes.7



One can imagine the howls that would follow a proposal to raise taxes “merely” because inflation had gone over some “arbitrary” threshold. Why abandon The Project? Why stop policies that were on the verge of making the world a better place just as the going gets tough? MMTers attempting to stick to the “if inflation rises” script on which The Project was predicated would be cast into the role of fiscal conservatives. Don’t they know that deficits don’t matter, etc.? They then reap the downside of overpromising.



If and when taxes were increased, it is doubtful that doing so would get inflation back down again. We know from monetarism that the inflation rate will only come down once the underlying monetary growth rate has slowed. But since the MMTers lack a decent model of inflation the likely policy responses would be some muddle akin to what we experienced in the late 1960s and much of the 70s, and with similar results: rising inflation followed by stagflation, a new Keynesians vs. monetarists controversy, and inflation only being brought under control again when policymakers relearn the lessons learned then—namely, the importance of the quantity theory of money.



The poor long‐​term performance of MMT under my simulations is a perfect illustration of why policymakers need to be constrained by rules. To illustrate the benefits of such rules, consider the following. Recall that we can think of MMT as a set of policies that break down into two subsets:



(1)MMT = {MMT spending program; MMT financing program}.



The former is about what goes out of the government’s coffers and the latter is about what goes in.



It seems to me that for most people inclined toward MMT, the big attraction is the MMT spending program. For the sake of argument, let’s hypothetically agree with that spending program and then ask if we can replace the MMT financing program with something better.



The MMT financing program consists of a combination of high deficits, tax, borrowing, and monetary finance. This financing program is a key reason why MMT performs so badly, so let’s replace it with an alternative financing program that is a combination of, let’s say, more restrained deficits, tax and borrowing, and no monetary finance. The monetary side of this program would be taken care of by some monetary policy or rule that focuses on a stable inflation rate or something similar. We then come to:



(2)Alternative to MMT = {MMT spending program; alternative financing program}.



My models indicate that this “Alternative to MMT” would produce considerably better outcomes, including a less rapid run‐​up of national debt and lower inflation.



Why then would you not prefer the “alternative” to MMT? The “alternative” still delivers the spending goodies you want, but in a less damaging way in the long‐​term. But the “alternative” is old‐​fashioned tax and spend!



My point is that even if you support MMT because of its spending program, there is no good reason to support MMT in preference to some tax and spend policy mix with the same spending program. Whatever your preferred government spending program, MMT is a poor way to finance it.



So, if you are a hard‐​left socialist who supports the Kelton government spending platform, you should support tax and spend, not MMT. And if you do not support her spending platform, say because you are not politically hard left or because you support sound money, then you would also not support MMT. Whatever your politics, MMT is not for you; MMT is just bad economics.



In the end, MMT comes down to this: the government spends a lot, issues a lot of debt, and prints a lot of money. It is not as if it hasn’t been tried before.



Haskins, R. (2015) “The Federal Debt is Worse Than You Think.” Brookings Institution blog (April 8).



Judt, T. (2006) _Postwar: A History of Europe Since 1945_. New York: Penguin.



Sargent, T. J., and N. Wallace (1981) “Some Unpleasant Monetarist Arithmetic.” _Federal Reserve Bank of Minneapolis Quarterly Review_ (Fall): 1–17.



Smith, A. (1994 [1776]) _An Inquiry into the Nature and Causes of the Wealth of Nations_. New York: Modern Library.



Kevin Dowd is an adjunct scholar at the Cato Institute, and a professor of finance and economics at Durham University in England.



ShowHide

Endnotes



1 “There’s no reason every job—all the way down to retail clerk or fast food worker or janitor in a luxury Chicago hotel—can’t be a good job, with dignified pay, hours, security, and benefits,” she says. The question however is how many of these jobs would still exist.



2 I implicitly assume, as seems reasonable in this (MMT) context, that the rate of growth of the national debt, including entitlement commitments (see below), exceeds the economic growth rate. In that case, the ratio of debt (including entitlements) to GDP will keep growing, and default is then inevitable unless the government resorts to (a) taxation or (b) printing money.



3 This figure comes from Laurence Kotlikoff’s testimony to Congress in 2014 (see Haskins 2015).



4 Few presidents would have the self‐​restraint to refrain from taking advantage of such powers, but the point is that if the Constitution were properly followed, we wouldn’t have to rely on his or her self‐​restraint in the first place.



5 One might even go as far as to say that MMT is the apotheosis of chartalism and I do not mean that as a compliment. Chartalism maintains that the state is entitled to monopoly privileges regarding the issue of currency. In response: a government monopoly is always a bad idea, period.



6 As she says, “The real challenge lies in managing your available resources—labor, equipment, technology, natural resources, and so on—so that inflation does not accelerate.” This passage describes the problem nicely but does not give the solution to it.



7 Wrong again. The only way to reduce inflation is to rein in the excessive monetary growth that is the proximate cause of rising inflation. So, Kelton’s statement that “MMT … offers a more sophisticated array of techniques for managing inflationary pressures than what we have today,” does not instill much confidence.
"
"
Share this...FacebookTwitterGerman online weekly FOCUS here reports how cuts by wind energy giant Enercon will lead to 3000 layoffs. According to Enercon chief executive Hans-Dieter Kettwig, “politicians have pulled the plug on wind energy.”

German wind energy industry in turmoil. Photo: By Pierre Gosselin
Subsidies cut
Once lavished with huge incentives, the German wind industry is being hit hard after the government recently ended the huge subsidies that were once aimed at expanding the installation of wind energy capacity. Power grid operators had been struggling to keep the grid stable due to erratic feed-in and the subsidized feed-in of wind energy caused German electricity prices to become among the most expensive worldwide.
Fierce opposition from hundreds of protest groups
Moreover, hundreds of citizen protest groups have sprouted and since become a formidable force pushing for the stop of proposed wind projects. Not only have wind parks scarred the German landscape and destroyed biotopes nationwide, they have also been shown to be a real health hazard to humans living in their proximity through the low frequency infrasound they emit. Enough is enough, citizens say.
3000 job cuts in the works


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




FOCUS reports: “The crisis in the German wind energy industry is worsening. According to the ‘Süddeutsche Zeitung’, hard cuts at the largest German manufacturer Enercon will cost 3000 jobs.”
Next year Enercon will also cut contracts with suppliers, sending a wave of job losses across the industry. “If supply contracts are terminated as planned, many of these companies are threatened with extinction,” FOCUS reports.
FOCUS notes that the layoffs will hit regions that are already economically weak. “At the Aurich and Magdeburg locations, 1500 jobs will be cut, according to the company. At the company headquarters in Aurich, 250 to 300 jobs are affected.”
Stricter regulations for wind parks, greater setback distances
Not only have the subsidies for German wind parks been cut back, but also setback rules will become more strict in order to protect homes and residents from landscape blight and infrasound. In the future, wind parks will need to keep a greater distance away from residential areas. The current  CDU/CSU/SPD government wants to keep at least one kilometer between wind power installations and residential areas in the future. This will make many proposed projects impossible.
German Greens demand the industrialization of scenic landscape
The stricter setback rules have been sharply criticized by the wind industry, and particularly by German Green party leader, Annalena Baerbock: “The planned distance rules for wind turbines are devastating,” she told network Germany RND.
“Contrary to all public announcements, the federal government is thus making further expansion of wind power impossible. This is tantamount to a boycott of the Paris Climate Treaty and its own climate targets.” Baerbock also told RND: “The federal government claims to want to get out of coal but at the same time is stopping the expansion of wind power.”
Share this...FacebookTwitter "
"Palm oil can be found in food and cosmetics everywhere: in fact, half of the world’s population uses palm oil in food. But public awareness about the loss of wildlife through deforestation caused by palm oil crops is growing, and there’s mounting pressure on retailers to reduce their sales of palm oil products, or boycott them altogether.  The debate has become especially heated since a Christmas advert by UK-based supermarket chain Iceland – which dramatises the link between palm oil, deforestation and the death of orangutans - was barred from being broadcast in the UK, on the basis that it would have breached political advertising laws, because the animation was originally produced by Greenpeace.  In the first four days of its release, the video was viewed 13m times. A petition to overturn the advert ban has so far attracted more than 720,000 signatures. But while Iceland’s campaign has been a great way to bring more public attention to food sustainability issues, an outright boycott on palm oil products could actually lead to more problems for forests and wildlife. 


      Read more:
      Iceland Christmas ad: barred, but it will help 2018 go down as the year of 'corporate caring'


 A recent report by the International Union for the Conservation of Nature, concluded that boycotting palm oil would merely shift – rather than counter – losses to rainforests and wildlife caused by agriculture. Put simply, boycotted palm oil would need to be replaced by other types of vegetable oil to meet global demand – and that could actually make matters worse.  This is because, compared to other common sources of vegetable oil – such as rapeseed and soybeans – palm oil crops yield four to ten times more oil per unit of land, and require far less pesticide and fertiliser. In fact, palm oil makes up 35% of all vegetable oils, grown on just 10% of the land allocated to oil crops.  So, if other crops such as soybean replaced a shortfall in palm oil, this would not only shift more production to the Amazon (a major soy-producing region), it would also require more land, leading to more deforestation. Indeed, soybean farming is already responsible for more than double the deforestation of palm oil. In the context of other food sources, livestock and beef production has led to more than five times the amount of deforestation, compared to palm oil.  Certification – a mechanism by which consumers pay higher prices for more responsibly sourced products – is one way to help safeguard rainforests, and the wildlife which lives in them. Palm oil certification is spearheaded by the Roundtable on Sustainable Palm Oil (RSPO), who are leading the market toward environmentally and socially responsible palm oil that doesn’t contribute to deforestation.  As the RSPO meets to renew its sustainability commitments, one major challenge facing the sector is that less than 20% of the world’s palm oil is currently certified as sustainable.  There is little incentive for producers to seek certification – or for retailers to promote environmentally and socially responsible products – as long as the debate continues to focus on boycotting palm oil altogether. As a result, only about half of sustainable palm oil is actually sold as certified, because a large proportion of the market is not willing to pay the premium for sustainable products.  Despite this, many large retailers and leading brands (including Nestlé, Unilever and Palmolive) and supermarkets (such as Morrison’s, Waitrose and Sainsbury’s in the UK) are already using certified palm oil in their products, but cannot heavily promote this due to the persistent negativity toward any type of palm oil. To help the palm oil industry to safeguard wildlife, conservation scientists are working with certification bodies and producers to improve how palm oil cultivation affects biodiversity. It can be as simple as growing the crop on non-forested areas. But it can also involve protecting forests along rivers, such that they join up patches of high quality forest within the palm oil landscape, allowing wildlife to move more freely. If certification of palm oil becomes more popular, it will improve prospects for wildlife, including orangutans. This is why major conservation organisations – including leading orangutan charities and Greenpeace – continue to support certified palm oil, rather than a boycott. And now, environmentally conscious consumers can check where they can buy products that contain responsibly sourced palm oil. Hopefully the interest sparked by Iceland’s advert will bring positive changes for rainforests and their wildlife. But a boycott is not the best answer. The best thing retailers can do is support their suppliers to bring more responsibly sourced products to the supermarket shelves this Christmas."
"

Given this gadget matches my blog namesake, you’d think maybe I invented it. Alas, though I’ve made lots of other inventions, this is not one of them.
For those of you interested in sustainability or renewable energy, your first and best defense against power waste is to look for energy that is being wasted in normal everyday use. You’d be surprised at how many of our modern electronic devices that appear to be “off” are actually wasting power and you don’t even know it. TV’s, radios, game consoles, some computers, and many rechargeable devices waste a huge amount of power.
There’s two places this happens:
1. Instant on devices: TV’s and stereos are especially bad. The convenience of having the device turn on immediately causes it to operate in standby mode, drawing a small amount of power 24/7 Some PC’s also operate this way.
2. Devices with AC plug transformers. Often called “wall-warts” these small transformers convert the 120 volts AC to a safer 6-15 volts DC to power the electronic device. Even if the device is unplugged from the transformer, the transformer continues to waste power!
The Watts Up meter can help you identify and quantify where power is being wasted and how much it is costing you. You’d be surprised.

A simple solution to the problem of home energy waste is a $5 power strip. For example I have a TV set and satellite receiver on my workshop which I use to keep abreast of news while tinkering. If I left these two devices plugged in 24/7, they’d waste about $12/year in electricity. I plugged both of them into an inexpensive power strip, enabling me to separate them from the AC power source. Since I don’t use my workshop every day, the small inconvenience of waiting for the satellite reciever to initialize (about 1 minute) is well worth the money I’ll save over the years.
Chargers are another place this could work. Cell phone chargers, MP3 player chargers, etc can all be placed on a single power strip. When the devices are fully charged or disconnected, simply turn of the power strip to end power draw.
For more info on alternate energy, see the website I designed for the North State Renewable Energy Group at www.nsenergy.org


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9b53788',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

From the “us” against “phlegm” department:
There’s a DailyMail article about the possibility of a revolutionary flu vaccine that could work against all strains of the Influenza A disease. This ‘holy grail’ of vaccines would work on everything from the annual ‘winter flu’ to the ‘bird flu’. The best part is that just a few vaccinations may provide complete immunity, unlike the annual boosters are current defenses require.

From the article:
“The new jabs would be grown in huge vats of bacterial ‘soup’, with just two pints of liquid providing 10,000 doses of vaccine. Current flu vaccines focus on two proteins on the surface of the virus. However, these constantly mutate in a bid to fool the immune system, making it impossible for vaccine manufacturers to keep up with the creation of each new strain. The universal vaccines focus on a different protein called M2, which has barely changed during the last 100 years.”
The brainchild of scientists at Cambridge biotech firm Acambis, working with Belgian researchers, the vaccine will be tested on humans for the first time in the next few months.
A similar universal flu vaccine, being developed by Swiss vaccine firm Cytos Biotechnology, could also be tested on people in 2007 – and the vaccines on the market in around five years.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9120cc2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A stunning little fish that eats salmon parasites could revolutionise aquaculture by providing an eco-friendly alternative to fish medicines.  The Atlantic salmon (Salmo salar) is one of the world’s favourite fish, and there’s big money to be made farming it – Scotland alone exports £400m worth per year. As in most agrifood sectors, the big rewards on offer have caused the industry to consolidate into fewer, larger players. It’s also lead to major technological breakthroughs and most of the big salmon farming challenges have now been addressed, except one: sea lice. Sea lice are small crustacean parasites that live on salmon, damaging their scales, reducing growth and helping diseases spread. The industry in Scotland is spending more than £30m per year on mitigation with approved veterinary medicines being a core element of parasite control.     There is a better way. My research group, in collaboration with leading salmon farming companies, has focused on an alternative “greener” approach – the use of cleaner fish, especially ballan wrasse. Controlling parasites with a biological agent – the cleaner fish – would be unique among livestock farmed for human food, but there are good reasons to think these wrasse are up to the task as we’ve known about their cleaning powers since the 1970s. Previously evidence was mainly anecdotal however, based on observing what happened when wild cleaner fish broke into salmon cages.  If we want scale things up, to harness these powers to make fishing more sustainable then the challenge now is to farm the cleaner fish themselves. This should protect numbers in the wild as well as being a safer option for salmon farmers – farmed wrasse would be disease-free, with a standardised size and robustness.  However, we first had to determine farming (domestication) would not mean these fish stopped going after the sea lice. Would a farmed wrasse still be interested in foraging on sea lice if it had never seen one during the two years spent in fish tanks and fed on pellets? Two Stirling colleagues and I recently published a study confirming the strong innate delousing behaviour of farmed ballan wrasse. Surprisingly, even though the wrasse were each able to eat up to 175 lice per day we didn’t find their hunger levelled off. The fish enjoy eating parasites, but they still don’t feel full. Supplementary feed is still needed. We also found size did not really matter, larger fish not being more effective than smaller ones. This means we can figure out the optimum wrasse deployment – numbers and size relative to salmon and sea pens and so on – and it also means the wrasse can be fed while cohabiting with salmon. Ballan wrasse aren’t the tastiest fish, though they are often used in the French “bouillabaisse” soup. However the species does have a number of striking features – and not just its looks. Their reproduction strategy is unusual. All ballan wrasse are born females and then undergo a sex inversion into males at a later stage of their life cycle (a phenomenon known as hermaphroditism protogynous). Their behaviour can be odd: they truly sleep at night which is not common in fish, the younger fish have a habit of grouping together in a ball, and of course they enjoy delousing. The fish also have no stomach, and a bile pigment causes their plasma to be blue. All this means scientists find ballan wrasse fascinating but it doesn’t make farming them any easier. In marine aquaculture, the first stages of life are generally the toughest to get right as the supply of eggs can be unreliable and lots of fish die off before they are fully developed. But things are improving. We can now better manage breeders and optimise egg production and fish survival, thanks to a range of tools developed to identify gender and manipulate sex on demand if required, assess reproductive conditions, remove the stickiness from the newly spawned eggs which simplifies the disinfection from potential pathogens, synchronise hatching and successfully produce robust fish in the hatchery. All of these resulted in the first commercial batches of farmed ballan wrasse in the hatchery and significant numbers of fish deployed to salmon farms.  Over the past few years we’ve made significant progress and the industry has even completed full 16-18 month salmon production cycles at sea without using any medicine to control sea lice. We’ve just been given a grant to continue our research so look out for further developments. It’s clear that cleaner fish really can be an effective way to keep salmon clean and healthy."
"We are only just beginning to learn how aquatic organisms will respond to climate change, and the effect that this will have on their communities and ecosystems. One way to find out more is to look at whether species will be able to compensate for changes in their environment. Particularly if they can survive any immediate fluctuations in temperature, and reductions in ocean pH brought about by increasing levels of atmospheric CO₂.  Coastlines and estuaries are already challenging places for marine organisms to live. The physical properties of seawater – salinity, temperature, pH and oxygen levels – vary frequently. And with further environmental fluctuations due to climate change, they are becoming even more demanding. Patterns of sea surface salinity are changing, as fresh water input increases, due to exceptional storm events and runoff from flooding.   Scientists have started to examine the combined effects of global warming and a reduction in seawater pH – otherwise known as ocean acidification – on marine communities. To date, it has appeared that multiple factors have more of an effect on these creatures than each factor in isolation. Together they influence the ability of species to compensate and survive the changes.  However, not much is known about the combined effects of ocean acidification and seawater dilution on these organisms. This is important as changes in salinity tolerance are known to influence distribution patterns of marine species and their community structures.  For our newly published study we decided to look at this combination of factors by focusing on two species of marine crabs: the edible crab (Cancer pagurus) and the shore crab (Carcinus maenas). Both are common to UK waters, but experience different degrees of environmental variation in their natural habitats. For edible crabs, home is typically the low intertidal shallow shelf waters for juveniles, and down to 100 metres for adults away from the influence of freshwater. While shore crabs typically live in estuaries and experience dilute seawater on a regular basis.  We studied how the crabs reacted to what are predicted to be the business as usual levels of CO₂ in 2100 (1,000 micro-atmospheres) and a biologically relevant reduction in seawater salinity. We were interested to see whether the edible crab will be less capable than the shore crab which regularly experiences salinity variations. We were also keen to find out why one species is likely to be more vulnerable than the other by investigating the ways they naturally compensate for environmental changes. We exposed juveniles of both species to the different CO₂ and salinity conditions for up to one year. The crabs were fed regularly and they continued to grow by moulting throughout the exposure time. We found that the shore crab was fully capable of surviving the conditions for up to a year, but the edible crab struggled.  The shore crab – which is a widely invasive species in countries outside Europe – increased its response to a stimulus (upregulated) its capacity to exchange bicarbonate ions across the gills. This mechanism helps buffer changes in body fluid pH associated with increased CO₂ in seawater. The edible crab, meanwhile, showed no such upregulation, and had limited ion transporting capacities. Instead, this species accumulated CO₂ within its haemolymph (crustacean blood) supply.  There was some attempt at compensating for the conditions, but remarkably the edible crabs were better off in dilute seawater. This was a surprise as the edible crab typically spends all of its adult life living in marine environments separated from the influence of freshwater. The reason behind it is difficult to explain, but it may come down to passive changes associated with exposure to dilute seawater making the haemolymph more alkaline.  Our work demonstrates that the juvenile edible crabs could survive elevated CO₂ conditions by moving into freshening seawater – but only for limited periods. This species also proved to be vulnerable to longer term exposures to dilute seawater. Our study helps us appreciate that there are fundamental differences in the biological capacities of marine species to compensate for climate change. Even within a taxa of crustaceans that is generally regarded as being relatively tolerant to change.  Fully marine species, such as the edible crab, with its preference for stability, are poorly equipped for survival in a variable natural environment. They are likely be to more vulnerable to climate change and further studies on this and similar species are urgently needed."
"Over the last few years Arctic scientists have reported a surprising finding: large areas of the Arctic are turning brown. This is in part due to extreme events linked to winter weather, such as sudden, short-lived periods of extreme warmth. These events are occurring as the climate warms, which is happening twice as fast in the Arctic compared with the rest of the planet. Extreme events are therefore happening more and more often, with increasingly severe effects – including widespread damage and death in Arctic plants. This “browning” of plant communities has happened over thousands of square kilometres or more. However, until recently we knew very little about what this might mean for the balance between carbon uptake and release in Arctic ecosystems. Given that the Arctic stores twice as much carbon as the atmosphere, this is a pressing concern. Now, our study has shown that extreme climatic events can significantly reduce the ability of Arctic ecosystems to take up carbon –- with implications for whether the Arctic will help combat climate change, or accelerate it. To understand how extreme events are affecting Arctic heathlands, we travelled to the Lofoten Islands in northern Norway where coastal, sub-Arctic plant communities act as a bellwether for future climate change in the far north by exhibiting the effects of warming in the region first. Here we found the effects of two extreme winter weather events. First, “frost drought” had caused extensive plant dieback. Frost drought occurs when the insulating layer of snow which usually protects plants from the harsh Arctic winter is melted, typically by unusually high winter temperatures. If plants remain exposed to cold, windy conditions for long enough, they continually lose water and are unable to replace it from the frozen soil. Eventually, they succumb to drought. The second event was “extreme winter warming” – a sudden burst of high temperatures during winter which melts the snow and tricks evergreen plants into preparing for spring by shedding their cold tolerance. When the warm period is over, the return of cold temperatures usually kills the plant. In this case, however, we found something unexpected. Heathland plants had survived this extreme winter warming event, but were showing evidence of severe stress, visible as a deep, persistent dark red colour in shoots and leaves. We measured how much carbon dioxide was being taken in and released by the plants in three vegetation types: damaged heathland (where the dominant evergreen species had been killed by frost drought), stressed heathland, and healthy, green heathland which had escaped the effects of either extreme event. This was done in three measurement periods across the growing season. We found that these extreme winter conditions reduced how much carbon was absorbed in heathland ecosystems by up to 50% across the entire growing season. This is a huge reduction in the ability of a widespread Arctic ecosystem to remove carbon from the atmosphere.  Surprisingly, this was the case both in damaged heathland, where a large part of the vegetation had been killed, and in stressed heathland. Although the processes driving this change were different in each type of heathland, this clearly shows that we need to consider the role of plant stress in limiting plant carbon uptake to fully appreciate the consequences of extreme climatic events. What does this mean for the Arctic? We now know that extreme climatic events could significantly reduce the ability of Arctic ecosystems to take up carbon and combat climate change. This is especially concerning as the impacts of browning are in stark contrast to those of a better understood response of Arctic ecosystems to climate change: “Arctic greening”, or the tendency for plants to become taller and more productive as Arctic summers warm. Many climate models currently assume arbitrary levels of greening across the Arctic,  and therefore that Arctic ecosystems will take up more carbon in the future – slowing climate change. The scale of the browning we’ve seen in recent years combined with the negative impacts on carbon uptake reported here suggests that the reality may be more complex, calling into question our understanding of the role of the Arctic in the Earth’s climate. What does this mean for us? The impact of extreme weather events in the Arctic has global consequences. It is clear that our current efforts to tackle climate change are dangerously inadequate, but ambitious action now could cut how much the Arctic is expected to warm by as much as 7°C. This is critical to minimising the impacts of climate change both in Arctic ecosystems and worldwide."
"Western liberal democracies believe that in difficult political decisions science serves as a referee and arbiter of truth. Scientific knowledge can indeed inform and narrow the scope of policy choices, for example in the teaching of evolution in public schools. But a staunch belief in a fully rational society, together with a political culture of adversarialism and the scepticism of vested interest groups can also create a fertile soil for controversy and a political deadlock. While we have come to know a lot about interest groupsʼ smoothly oiled denial campaign to influence public opinion on climate change, very little has been said about the institutional mechanisms exacerbating the political stalemate between Democrats and Republicans. In order to gain political support in the US, scientists are frequently asked to articulate, represent and defend their knowledge in congressional hearings. To this end, Democrats and Republicans choose experts independently. They then put the scientists under oath and start their cross-examination. Truth, they insist, would emerge only from aggressive testing in an adversarial forum. Of course, the purpose of congressional hearings on science most often is not to actually expand or clarify the scope of choice available to decision makers, nor to convince neutrals or to win over the other side to one’s point of view. Rather, these hearings are meant to show and confirm solidarity with one’s own side. In this sense, they mark a breakdown of democratic deliberation. In their floor speeches Republicans and Democrats subscribe to the so-called linear model of science and society. This describes a sequential process by which basic or fundamental research results in technical innovation and public policies. There’s little empirical evidence this is how things actually work but nonetheless it remains the organising principle of congressional hearings on scientific matters. Paired with the belief that truth emerges from aggressive testing, congressional hearings create the perfect conditions for a political opponent to deconstruct the basic of fundamental research. Such often openly hostile hearings date back to at least the DDT controversy in the 1960s when Democrat representatives invited legendary ecologist Rachel Carson to testify on the petrochemicalʼs detrimental effects on the environment. Democrats wanted Carson to make a scientific case for the regulation of the petrochemical industry, and thereby (unwittingly?) provoked the sceptical deconstruction of environmental science. When Republicans invited experts who questioned the presented consensus, a political debate quickly turned into a narrow technical one about the scientific method, uncertainties, and the scientistsʼ alleged conflict of interests. DDT sceptics from the right adopted a strategy the left had pursued for many decades: they employed a Marxist critique of the social and economic foundation not of capitalist but of environmentalist science. The US Environmental Protection Agency (EPA) was able to frame its eventual ban as a rational decision, circumventing a value-laden discussion about the merits of a system that has allowed a few industries to benefit at the cost of the general public To be sure, the Democrats succeeded because their political programme of action could be brought into alignment with science: the decision made in the 1970s was politically and economically viable as the industry moved overseas to create new markets for their products. Science can’t always be brought into alignment with those wider political and economic rationales. Public opposition to major companies has kept GM food out of European supermarkets, for example. It doesn’t matter that the purported health risks cannot be scientifically substantiated. For politicians, the potential loss of credibility from being seen to go along with Monsanto and co is not worth the scientific upside.  Congressional hearings on climate science continue in that tradition. Since the late 1980s the Democrats have convened hearings and invited “their” experts in the hope that science would legitimise their policy proposals. We have all heard of climate scientist James Hansen’s emphatic 1988 testimony that “it’s time to stop waffling so much and say that the evidence is pretty strong that the greenhouse effect is here.” In turn, the Republicans invited experts who issued statements questioning respective claims. This happened frequently under the Bush administration, for example in hearings convened by the Republicans James Inhofe, Ed Whitfield and Joe Barton. Seizing the Republican majority in both chambers, their hearings on the so-called hockey stick climate reconstruction functioned as a veto put on a legislative process that has been facing resistance long before esoteric scientific questions attracted politicians’ attention. Unsurprisingly, when the Democrats regained a majority they fought back. Two of the latest hearings featured briefings grandly entitled “A Rational Discussion of Climate Change: The Science, the Evidence, the Response” and “Undeniable Data: The Latest Research on Global Temperature and Climate Science”. Convened by Democrats Edward Markey and Henry Waxman these hearings should set the scientific record straight and reinforce the difficult legislative process. But subpoenaing testimony from intimidated scientists in order to influence the policy process has proven ineffective at best – since neither party takes their opponent’s expert advice seriously – and counter-productive at worst – since it simply reinforces the stalemate between Democrats and Republicans. On a discursive level these hearings havenʼt achieved much. The idea that truth is best served through adversarialism and the clash of competing viewpoints before a judge and jury turns esoteric scientific controversies into fully fledged public debates. This adversarial procedure is typical for how the litigious US society warrants scientific knowledge for policy-making. It has come to characterise the climate change debate. By contrast, in the UK the assumption of trust and mutual respect still guides the relationship between scientific advisers and government. The option of subpoenaing testimony from scientists is rarely exercised. Instead, in a consensual decision, parliament invites and asks for advice a chief scientist who is recognised as the authoritative and trusted voice on scientific matters of fact. This does not mean that the goals set in the UK Climate Change Act are achievable. They are probably not. But the procedure by which the UK uses science to influence policy doesnʼt drive a wedge between scientists and politicians. What humble and sane climate scientist would still want to accept an invitation to Congress?"
"Red Dead Redemption 2, a new video game about an outlaw gang on the American frontier in 1899, has been met with huge adoration. Journalists have lauded it as a “landmark” title, a “technological masterpiece”, even a “watershed moment” in entertainment. Much of the praise has focused on how developer Rockstar Games has coded a “living” game world that oozes character and aesthetic richness. However, now that the digital dust has started to settle, that same world has come in for criticism. Gamers have dubbed the title “boring” and “slow,” with their enjoyment of the game noticeably impeded by “clunky controls” and the lack of easy “fast travel” between destinations. Matt Reynolds for Wired recently complained how Red Dead Redemption 2 ultimately “feels like a chore.” The ability to kill a life-like female suffragette in the game also courted controversy, with YouTube first banning then restricting the gratuitous footage. Most of the criticism reflects a growing problem with video games: the pursuit of realism. For decades now, mainstream developers (with the notable exception of Nintendo) have committed their energies to crafting visually realistic game worlds. More than any other studio, Rockstar has dedicated itself to the holy digital grail of verisimilitude, throwing millions of dollars (and controversially long work hours for its staff) chasing reality.   In the 1970s, arcade Westerns such as Midway’s Gun Fight featured barely-animated stick figures “firing” dots at each other across a landscape populated by a single pixelated cactus. By contrast, in Red Dead Redemption 2, game character Arthur Morgan’s hair, wardrobe and waistline grows and stunning digital vistas rival Albert Bierstadt’s paintings of the 1800’s American Interior.  In Rockstar’s stunning quest for the life-like, more than 170 species of animal wander the digital terrain. In the 1870s, Eadweard Muybridge, seeking to answer the question of how horses gallop, captured their movement through a series of photographs. Nearly 150 years on, Rockstar uses 21st-century motion capture to perfect literally hundreds of “living”, “breathing”, and galloping horses on the digital screen, to such detail that, as lad magazines recently enthused, even their testicles shrink in the cold. Not only has Rockstar strived for visual realism, but the studio has constructed a world of realistic player activities and responsibilities. From their inception, video games have digitised the “every day”. From delivering newspapers in Atari’s Paperboy (1984) to collecting airport baggage in Apollo’s Lost Luggage (1982), video games have transformed mundane tasks into moments of digital revelry. Released back in the 1990s, Sega’s Shenmue (1999), set in a “living” Japanese city, incorporated a range of daily tasks including driving a forklift for money. Such titles have provided moments of “playful realism”.  Red Dead Redemption 2 represents a new scale-up and seriousness to the enterprise. As the character Arthur Morgan, the player is expected to hunt and skin wild animals, maintain his guns and wares fastidiously, and feed and groom his horse. In the process, Red Dead Redemption 2 undoubtedly offers a more meaningful adventure. But the game also tests boundaries over notions of play and digital experience. Unfortunately, performing work-like tasks and living the “every day” in games can easily test our patience. The closer a game gets to any semblance of reality, the greater the player notices its flaws. In “reality”, most of us (at least on a basic level) can choose when to do things, perform tasks freely and organically, and process multiple sensations while doing them (such as the weight of an item, or our own limited strength).   In ultra-realistic games, those expectations are quickly frustrated: we push a complex sequence of buttons to perform simple actions (such as drawing a gun), we lose authorial control (and voice) to orchestrated story arcs (Red Dead’s set missions), and narrow visual cues become an excuse for human experience. In-game realism is quite a different property, then, to the world outside. Too much realism also rails against the basic appeal of games: to escape, to play, to indulge in fantasy (in other words, the “unreal”) and most of all, have fun.  Writing in Homo Ludens (1938) on the concept of games, Johan Huizinga declared “the fun-element” crucial to “the essence of play”. In the case of Red Dead Redemption 2, Rockstar has pushed the boundaries of gamic realism to new heights of maturity and sophistication. Rockstar shows how we can find beauty in a digital place and a digital moment, and is actively testing what “gameplay” means. However, while a landmark title, few reviewers have applauded the game as enjoyable. The true limit of gamic realism may not come in terms of technological hardware, programmer hours or dollars spent, but in our basic human desire for games to be fun."
"

Scott Brown’s stunning upset in the Massachusetts special election may have done what the best policy arguments could not – defeat the Democrats’ plans for a massive government takeover of the U.S. health care system.



Democrats will undoubtedly offer a variety of excuses for Brown’s win. The Democratic nominee, Attorney General Martha Coakley, was a poor candidate. The “political climate” was bad. The dog ate their ballots. But in reality, there can be no denying that this election was a clear cut rejection of the Democratic health care bills.



There were no blurred differences on this issue. Scott Brown made his opposition to the bill a centerpiece of his campaign. He promised to be the 41st vote to sustain a filibuster and kill the bill, even signing autographs as “Scott41.” Coakley, on the other hand, pledged to vote for the bill.





[T]here can be no denying that this election was a clear cut rejection of the Democratic health care bills.



The issue was featured in ads, debates, and public discussions. In the end, according to polls, in the home of Ted Kennedy, more than half of voters opposed the version of health care reform being rushed through Congress. Voters knew what they were saying. And what they were saying was a resounding “No!”



What do Democrats do now? House Speaker Nancy Pelosi says that they will pass health care reform “one way or another.” Those “ways” are:



 **Hurry up and stall:** New York Democratic congressman Anthony Wiener both named and defined this strategy. Democrats would slow‐​walk certification of Brown’s victory, preventing him from taking his seat in the Senate. Massachusetts Secretary of State William Galvin has hinted that he won’t certify election results for at least the 10 days that local officials have to report on absentee and overseas ballots and has noted that state election law gives him as long as 50 days beyond that. Meanwhile, Pelosi and Harry Reid will rush their negotiations to merge the House and Senate bills, allowing the appointed interim Massachusetts Senator Paul Kirk to vote on the bill before Brown takes his seat.



Democrats legitimately fear that such a blatant disregard for the democratic process would spark an enormous backlash. There would be no way to pretend it was anything other than the most corrupt power politics. After previous special elections the winners have been seated within days. In fact, when Kennedy first won the seat in a 1962 special election, he was sworn in the very next day. Would Democratic moderates, already frightened by the election outcome, be willing to go along with such an approach?



 **The House Surrenders:** Democrats could try to avoid a Senate vote altogether by having the House simply pass the already Senate approved measure. But that would require the House to accept the Senate bill with no changes at all. Pro‐​life Democrats like Bart Stupak (D‐​Mich.) would have to accept much weaker Senate restrictions on government funding of abortions. Liberals would have to accept the so‐​called “Cadillac tax” on high value insurance plans — without the exemption demanded by labor unions. And what about nervous moderate and blue dog Democrats? Are the Massachusetts results going to make them more or less likely to go out on a limb for health care reform? Remember, the House only passed their bill by a three vote margin, and one of those, Rep. Robert Wexler (D‐​Fl.) has since resigned his seat.



 **Let It Snowe:** Democrats could try to reach a compromise with Republican moderates like Olympia Snowe of Maine. Snowe did vote for a version of health reform in the Finance Committee and has spoken positively of the need for reform. But she also voted in favor of a resolution declaring that the individual mandate was unconstitutional and has raised a number of other objections. Any bill she agreed to would have to be substantially different than the ones currently being considered. That would almost certainly jeopardize already tenuous support from liberals. If the current Senate version is hard for them to swallow, just imagine how they will react to one watered down even further.



 **Go for 51:** The last, desperate gasp would be to use an arcane procedure known as reconciliation to pass health care reform with just 51 votes. But doing so would require Senate Democrats to overcome all manner of procedural hurdles. Reconciliation cannot be used for policy as opposed to budgetary issues. That means Democrats would have to drop some of their more popular proposals like the ban on preexisting conditions. They would be left with a bill that did little more than expand Medicaid and other subsidies, raise taxes, and cut Medicare. How popular would that be?



So far Democrats have been willing to do almost anything, cut any deal, sacrifice any principle, to force this bill through. But they may be running out of options at last. And for that, we can thank Scott Brown and the voters of Massachusetts.
"
"Electricity generated by fossil fuels is increasingly unsustainable and a shift towards renewable energy – principally from the sun and wind – is vital. Renewable generation is already less expensive per unit than its polluting counterparts, but the fact the sun doesn’t always shine and the wind doesn’t always blow presents an obstacle to a serious takeover of the energy sector. Energy storage could overcome this pressing “intermittancy” issue. If storage was available at sufficiently low cost and high performance, renewable energy would rapidly displace all other generation forms. Energy is already stored, of course, in batteries or various other technologies. Even reservoirs can act as huge stores of energy. However nothing that exists or is in development can store energy as well, and as cheaply, as compressed air. The concept seems simple: you just suck in some air from the atmosphere, compress it using electrically-driven compressors and store the energy in the form of pressurised air. When you need that energy you just let the air out and pass it through a machine that takes the energy from the air and turns an electrical generator. Compressed air energy storage (or CAES), to give it its full name, can involve storing air in steel tanks or in much less expensive containments deep underwater. In some cases, high pressure air can be stored in caverns deep underground, either excavated directly out of hard rock or formed in large salt deposits by so-called “solution mining”, where water is pumped in and salty water comes out. Such salt caverns are often used to store natural gas. Compressed air could easily deliver the required scale of storage, but it remains grossly undervalued by policymakers, funding bodies and the energy industry itself. This has stunted the development of the technology and means it is likely that much more expensive and less effective solutions will instead be adopted. At present, three key problems stand in the way of compressed air: The above description of how it works is an over-simplification. CAES is, in fact, not a single technology but a wide family that includes compression machinery, expansion machinery, heat exchangers, the design of air stores and the design of thermal stores. These all require meticulous engineering to get right.  At the moment, wind and solar still make up only a small proportion of the overall sector. As electricity generated from fossil fuels can cover the overcast or wind-free days, renewable energy is often used straight away and only needs to be stored for short amounts of time. For these situations, batteries work quite well and can be economically viable.  Large-scale decarbonisation will require us to store energy for much longer periods, however, for instance from a sunny day to use on a cloudy day. CAES is especially suited for storage durations of some hours through to several days.  All affordable energy storage involves converting energy from the form of electricity to some other form and storing it in that other form. For pumped-hydro storage, for instance, the other form is water that has been lifted up to a great height. For CAES, that other form includes both heat and high-pressure air. For such systems, there are separate costs for the equipment that does the conversion and for the storage itself. Systems like CAES and pumped-hydro involve relatively expensive equipment for the power conversion but very inexpensive provisions for the storage of energy. These systems, where small amounts of power can fill up very large amounts of storage, are therefore very economical for storing energy over a long period. Private investment requires high rates of return. An indirect effect of this is that investors place less value on what utility may be left in an asset in the longer term.  In most CAES systems, costs are concentrated in things that naturally have very long lifetimes. For example, a solution-mined cavern in a salt deposit might reasonably be expected to operate for at least 100 years, while high power machines for compressing and expanding air can typically operate for 50 years or more. With returns over such a long timescale, there is a strong argument that at least some large-scale compressed air installations should be treated as national infrastructure projects financed by governments. Two large compressed air plants were built decades ago, one in Huntorf, Germany and the other in McIntosh, Alabama. Both are still working extremely well. Many refer to these two plants to draw conclusions about how efficient CAES can be and how much or little it can cost.  But this is misleading and pointless. Both plants were designed with very different priorities from those relevant today. It is imperative that we now think again about compressed air energy storage and evaluate it properly in light of what can be achieved by exploiting modern methods and knowledge."
"For millions of livestock owners across Africa, a disease in the herd can mean hardship or even ruin. Access to effective veterinary treatment is therefore essential for many people’s livelihoods.  Many factors can make getting good treatment difficult, such as a shortage of vets and the high cost of care.  To make things worse, counterfeit or substandard drugs have penetrated many marketplaces, and are often hard to tell apart from the real thing. While regulators scramble to catch up with this problem, livestock owners must work out their own ways of coping. The quality of medicine for humans has long been a major public health problem, particularly in developing countries. Products are sometimes sold without little or no active ingredient or without correct labelling – sometimes imitating the packaging of well-known companies. Such deceptions can be as simple as a shopkeeper diluting bottled medicines, or involve sophisticated international criminal networks.  The true scale of the problem is difficult to pinpoint, though some research indicates that fake and substandard drugs are a significant proportion of all drugs sold globally, and are a multi-billion dollar industry. In the past decade, there have been impressive efforts to tackle the human drug quality problem. However, the issue of veterinary drug safety and security remains largely below the radar.  The International Federation of Animal Health estimates that the trade in sub-standard and non-registered drugs in Africa is worth US$400m a year – the same size as the official market. The manufacturers, sellers, and customers I’ve spoken to over believe that the problem has been around for decades, but has got much worse in the last five to ten years. Some research has tried to estimate the quality of drugs that treat trypanosomiasis and other animal diseases. But it remains frustratingly difficult to get a clear view of the problem. This is in part because testing drug quality is difficult – even where facilities exist, it can cost several hundred US dollars to test a single sample.  As considerable progress has been made in cleaning up the human drug market, why has Africa’s veterinary drug problem been left behind? One reason is that since the 1980s, public veterinary services in many African countries have been greatly reduced. This decline has had far-reaching consequences, not least in the loss of network of veterinarians well-placed to identify and report dubious products. More broadly, the withdrawal of public veterinary services means that many livestock owners are forced to improvise when looking after their animals. Diseases must be tackled with a patchwork of private veterinarians, “paravets” and livestock workers (with variable levels of training), and local knowledge. A lot can go wrong in this Wild West free-for-all: diseases with similar symptoms can be easily misdiagnosed, mistakes can be made in the tricky task of judging doses correctly, and even good quality drugs can be rendered useless by being stored incorrectly or used after their expiry date. Because of this, fake or substandard drugs are just one possible cause amongst many for treatment failure – meaning that it is difficult to definitely prove that a particular batch of drugs is substandard. My own research with Fulani pastoralists in Nigeria’s middle belt demonstrates that cattle owners are well aware of the presence of dubious products. Customers are not always able to reliably identify dubious products – a fake pill does not come apart at the seams like a fake pair of Levi’s. Instead, they systematically experiment with small quantities of several drugs to identify effective ones, and use medicines in careful combinations to avoid the risk of failure in any one product. Nigerian urban elites often discuss Fulani cattle owners with a certain condescension. But the Fulani are far from passive victims: building on their expertise and efforts to secure good quality treatment would be a good place for regulators and veterinary authorities to start. To tackle the human drug quality problem in Nigeria, the National Agency for Food and Drug Administration and Control (NAFDAC) has followed a uncompromising strategy of prosecutions and well-publicised seizures and burnings of drugs. While these efforts have been rightly celebrated, improving veterinary drug safety may require a different approach. Many in Nigeria prescribe and sell veterinary drugs without the necessary licences – but it would achieve little to harshly prosecute such vendors when government support and regulation has been ineffective for so long. A better approach might emphasise training for both vendors and customers of drugs, alongside more rigorous licensing and investigations. NAFDAC launched a new veterinary and animal products directorate in 2013 to address these issues, which along with the good reputation of the agency is cause for optimism.  There is urgent need for more research across the continent, cheaper ways of testing drugs, and most of all, better use of livestock owners’ own knowledge and skills."
"

Image above: Dubbed the “Swan” this X-ray image shows massive energy releases from the sun’s magnetic field, even while we are at the solar minimum in between sunspots cycles.
Last week, on the same day Al Gore was giving testimony to congress on made-made CO2 being the sole cause of Global Warming, NASA called a press conference in Washington DC to announce some spectacular new findings about the sun. Of course everybody in the press was so busy covering Gore’s big day, there was hadly any mention of what NASA announced.
What they announced was that a new X-ray imaging satellite called HINODE, launched in September 2006, has seen the first images that  explain one of the biggest mysteries of the sun: why the corona is hotter than the suns surface. Magnetic reconnection seems to be the key, and these images go a long ways towards proving the theory.
But even more importantly, scientists expected to see a very quiet sun with the new x-ray imager, since we are at solar minimum right now. NASA announced we’d reached solar min on March 6th. The fact that the HINODE scientists saw huge explosive energy bursts even while the surface of the sun is nearly devoid of sunspots tells them that the suns magnetic field is still tremendously active. The suns magnetic field has been getting more active for the past hundred years, coincidentally at the same time CO2 on earth has been increasing along with the global mean temperature.

But it seems that coincidence makes CO2 a Red Herring.
The linkage between changes in the suns magnetic field and earths climate has been well documented. Global temps closely track solar cycles as measured by sunspot intensity. Sunpots are proxy indicators of changes in the suns magnetic field. The Danish Meteorological Institute first reported the correlation in a study going back centuries. Historic data reveal that whenever the sun got more active, the earth heated up, and vice versa. The best correlation was the Maunder Minimum.

But until now, we could not see energy being transported away from the sun via its magentic field, which is why many in the environmental community doubt the role of the sun in climate change. We couldn’t visualize the sun’s magnetic output. This new tool is going to open a whole new era of understanding how the sun works, and more importantly how changes on the sun link to climate changes on earth.
Of course I’m sure Mr. Gore will find a way to explain this away, since we can’t have any new science getting in the way of a “consensus” and a “debate thats over”.
Inconveniently, NASA also announced last week a new study that shows a clear sun-earth linkage in records kept by Eqyptians of the Nile river, rainfall, and auroral activity which is a direct indicator of solar activity. It seems the sun-earth climate linkage has been around way before SUV’s.
So what’s easier to believe as the cause of climate change? That a trace gas called CO2 that has increased on earth from about 280 PPM to 380 PPM in the last 100 years is the cause, or that the giant nuclear fireball a thousand times bigger than earth a mere 8 light-minutes away has been getting more active during the same period is the reason?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea783b761',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIt’s certainly no misperception that global warming alarmists have become far more shrill over the past months with ever wilder, more sensational claims about how the climate is getting more dangerous.
Making up facts
For example, in his COP25 opening speech, many of the claims made by UN Secretary General Antonio Guterres outright flunked the reality test, were exposed to be outlandish and seemed to be completely made up.
At Twitterm, Timjbo tweeted some Sky News footage of a statement made by the UN Secretary General, followed by a fact-check:
 Climate related disasters are becoming more frequent, more deadly, more destructive with growing human and financial cost.”

Many of Mr Guterres’ assertions in his opening speech #COP25 didn't stand up to scrutiny, including his claim a record number of people are dying due to climate-related disasters. @CraigKellyMP #Outsiders 
Not true, the #UN is after cash.
Full interview https://t.co/vCrHGqmOuY pic.twitter.com/vriSgQb2Ej
— Timjbo 🇦🇺 (@pleaseuseaussie) December 9, 2019

“Incredible 98% reduction” in climate deaths since 1930
The footage of Guterres is then followed by a Sky News panel analysis at Sky News, which featured MP Craig Kelly, who said that Guterres told three untruths in a single sentence.
First, Guterres claims a greater number of deaths from climate-related disasters, but this is untrue as the data chart show:

Lie no. 1: More climate-related deaths. Reality: 98% reduction since 1930!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Guterres is saying the exact opposite of what is true
Kelly shows that climate-related deaths are “at the lowest level in history”.
“There’s been a 98% decline, an incredible 98% decline since the 1930s,” said Kelly. “And this year we’ll get the lowest number of deaths from climate-related  disasters, ever!”
Kelly slams the UN Secretary General for saying the “exact opposite of what is true.”
Lie number 2: increasing number of climate related disasters
Also shown by Sky News is a chart depicting the number of reported natural disasters since 2000.

It’s a lie that climate-related disasters have increased.
Lies for cash = fraud
Here as well climate-related disasters have fallen steadily since 2000.
Kelly believes that money is behind all the falsehoods. “The UN is after cash,” Kelly says. He adds: “Now when someone makes false and misleading statements asking for cash, that is called fraud.”
Lie no. 3: “The costs of disasters is increasing.”
The third lie in the Secretary General’s whopper statement was saying the costs of disasters was rising. But here as well the data tell the opposite story:

Watch the whole Sky News segment here.
Share this...FacebookTwitter "
"In a bid to reduce our carbon footprint, confront greenwashing and increase our focus on the climate crisis, the Guardian this week announced it will no longer run ads from fossil fuel extractors alongside any of its content in print or online. The move will come into immediate effect, and follows the announcement in October last year that we intend to reduce our net emissions to zero by 2030. Once upon a time, a newspaper was a rather straightforward business. You generated enough material of interest to attract a significant number of readers. You then ‘sold’ those readers to advertisers happy to pay to get their ideas, products or brands in front of consumers with cash to spend.  Of course, digital disruption over the past 20 years has upended that model, but advertising remains an important part of the media business ecosystem. At the Guardian, it is still responsible for about two-fifths of our income. But what happens when the readers don’t like the adverts? What do you do when the message that advertisers want to spread jars awkwardly with the work your journalists are doing? What if your journalists are some of the best in the world at revealing and investigating the deepening climate catastrophe and the disaster that is fossil fuel growth, while some of your advertisers are the very people digging the stuff out of the ground? This contradiction has bothered us - and some of you - for some time. We came up with a rather bold answer this week: turn away the money and double down on the journalism. “It’s something we thought about for a long time,” says Anna Bateson, the interim chief executive officer of Guardian Media Group, the Guardian’s parent company. “We always felt it was in line with our editorial values but were cautious for commercial reasons.” She said it was the logical next step after the Guardian committed last year to becoming carbon neutral by 2030 and was certified as a B Corp – a company that puts purpose before profit. But she added that the move had to be weighed carefully, given the fact that the Guardian only recently returned to breakeven after years in the red. “You have to be careful you are not making cavalier decisions,” she said. “ We are still having to fight for our financial future. But because of the support we get from our readers, it is less of a risk.” On the advertising side of our business, Adam Foley said there were no complaints at all that potential customers were suddenly off-limits, adding that staff felt that “being part of a company that shares their values” was the biggest motivation for his teams. “A statement like this reaffirms to all of us that we’re contributing to a business that really lives those values - to the extent where it is prepared to sacrifice profit for purpose.” The response from the wider world has been a pleasant surprise. Hundreds of you have written in, pledging your support, and in some cases, one-off contributions to start making up the shortfall.  The environmental movement was instantly appreciative, with activists quickly urging our peers to follow suit. “The Guardian will no longer accept advertising from oil and gas companies,” Greta Thunberg tweeted. “A good start, who will take this further?” Greenpeace called it “a huge moment in the battle against oil and gas for all of us.” Some readers have been calling for the Guardian to go the whole hog and forsake advertising from any company with a substantial carbon footprint. Bateson said that was not realistic, adding that such a move would result in less money for journalism. She said the fossil fuel extractors were specifically targeted because of their efforts to skew the climate change debate through their lobbying effort.“We are committed to advertising,” she said. “It will continue to be part of our future. We want advertisers who want to be appear alongside our high quality journalism.” And how will we know if this has worked?“We will listen to our readers, we will listen to our advertisers. The response so far has been gratifying. If we continue to hear positive noises from our readers and supporters, then it will have been a success.” Responses from our supporters That is such a brilliant decision and it will be tough, but it is the correct one and I am very proud of The Guardian. Barbara Syer Following the Guardian’s decision to ban ads from fossil fuel companies I’m making a monthly contribution to support its fearless journalism: reader support is essential for independent scrutiny of the powerful in business, finance and politics. Titus Alexander, Hertfordshire, England I live at present in Canada, home to the Alberta Tar Sands: another name for ecological devastation resulting from fossil fuel extraction. I fully support The Guardian’s action in ceasing to be a vehicle for advertising by fossil fuel extractive companies, and I’m proud to be a supporter. My monthly donation is small, but when I can I will make it much greater. Rosemary Delnavine, Canada Congratulations. At this time it may be a bold step, indeed, within this industry, but true leaders have to take bold steps for the betterment of the quality of life, and more importantly for the life of future generations. I applaud this decision, and will spread the word. Raphael Sulkovitz, Boston MA What a bravery! This is what the life on earth needs, thank you. Karri Kuikka, Finland  Keep it up. Here in Canada, we’re still trying to have it both ways -- sell the product internationally but discourage buying domestically. As I recall, it was the same with tobacco. Eventually, it took a change in public opinion to solve the problem. As a news source, your efforts are part of this solution. Robert Shotton, Ottawa I applaud your decision to”walk the talk.” I will therefore continue to contribute to The Guardian. Bob Wagenseil Bravo yr decision to eschew $ from the FFI. Please do continue to hold to the fire(s) the feet of the deniers and the willfully ignorant. Sydney Alonso, Vermont, US I am very happy to hear that good news. It’s quite courageous on your part, and I’m happy to support you! Have a great year ahead, you’ll have my continuous support! Julien Psomas I completely support your plan to refuse ads from fossils, despite the  financial hit to the Guardian. I have made a donation to help out. David Thompson A very commendable decision, very much in keeping with the Guardian’s position as leader of green issues to leave a better planet for following generations. Richard Vernon, Oxford Yay! I’m so proud of the Guardian! We can no longer support or fund in any manner the fossil fuel industry if we have any chance of survival as a civilization on this planet. You’ve taken a courageous and moral step that will hopefully embolden others to join you. Good on you! Best, Carol Ross, Missouri, US Good decision. I’ll support you as much as I can, which unfortunately is not much as I live on age pension only. Keep up the good work, we need it desperately! Ursula Brandt, South Australia I am absolutely delighted by this decision. So many people pledge to do something about Climate Change, but few actually are willing to get uncomfortable and DO it. I am very proud of you as my favourite source of Information and this only makes a case for me to donate next time to you again. Christiane Gross It was great reading what The Guardian is doing re the climate. As a Guardian on-line reader from The Netherlands I’m going to contribute monthly now instead of ‘now and again’. The amount will be relatively small as I do not have a great income. I really hope more of your supporters will do so, because it is really great what you are doing.  With kind regards, Aleida Oostendorp, Netherlands I congratulate you and your team on taking this step regarding fossil fuel companies. The Guardian’s stance on the environment and its excellent coverage of related stories and events is the major reason for my support. Well done, and good luck in the future. Deirdre Moore Love your new policy about accepting money from fossil fuels. Will contribute more to help make up for the shortfall. Todd Misk I live on a fixed income with a strict budget so my continuing support of your excellent news organisation represents my commitment to the fight to address climate change. Every step counts. Barbara Hirsch, Texas, US Only when we speak truth to power can change take place. thank yo for your courageous and expensive decision. Nancy Shepherd, Vermont, US Love your journalism, especially your investigative work and the climate change topic. And with the bold statement about not receiving any more sponsorship from the fossil extracting companies? Well, the already great newspapers became even more impressive now. Keep up the good work. Miroslav Řezníček, Czech RepublicThank you for taking the bold step of refusing advertising from fossil fuel extractive companies. I think it is the right thing to do & hope many more companies do the same. We must all work together if we want to save our planet. It is one of the most important issues of our times. Ginger Comstock, New York, US"
"As I write these lines, I tell myself there is no reason to fear. Far-right politics are on the rise globally – yet the universities in Brazil that were recently raided in a clear attack on freedom of expression seem a long way from the UK. This has not prevented, however, my personal and professional selves becoming increasingly entangled as I watched Jair Bolsonaro, one of the most obnoxious figures in Brazilian politics, become the country’s president-elect.  As a Brazilian citizen, and an academic interested in ethics, social justice and sustainability, I have caught myself asking how to resist in times of “Bolsonism”. After all, in my everyday life I live by values which are in direct opposition to it. The answer I found, which might make some fellow scientists (including social scientists) raise their eyebrows, is to be overtly political.  If our practices as researchers and educators are already value-laden, we should then ask ourselves which values we want to pursue. As a form of academic activism, we should be ready to lobby – not necessarily for funding, which is already the most established form of scientific lobbying – but, through our research, for diversity, human and environmental rights. Many would argue that science and politics operate on different grounds and work by different logics. Scientific research is, however, just as instrumental for policy and regulation, as dependent on them. For instance, scientific evidence has been the centre of attention in recent cancer lawsuits, while governments are considering supporting research on artificial intelligence for military systems.  Ideals of scientific neutrality and disinterestedness have been challenged and the politics of knowledge production were unveiled a long time ago. Science is not immune to human values precisely because it is a human endeavour. As with politics, science not only is highly fallible, it is also shaped by specific interests, it delivers on specific goals and it is subject to cultural standards.  Of course, it wouldn’t be a surprise to see some supporters of Bolsonaro take advantage of this narrative and use it to undermine science’s authority. Such a strategy seems to have already worked well for climate change deniers. So where would this leave us? If there is very little, or nothing at all, that separates science from other human practices, is there anything special about science in the context of far-right politics? Scientific actors are powerful players in the fabrication of collective imaginaries in modern societies. The interaction between “pure” science and its application is fundamental in the creation of shared ideas on how society should be and which goals we should pursue. The science studies scholar Sheila Jasanoff has shown that anything from mundane devices through to high-tech projects on nuclear energy or bioengineering, are all motivated by broader sets of values and norms which are adopted and shared within communities or even nations. For example, increasingly individualistic societies will favour scientific and technological developments that allow for the reproduction of these values, such as self-driving cars or smartphones.  In her book Dreamscapes of modernity Jasanoff writes that such “sociotechnical imaginaries” might be held collectively but they “can originate in the visions of single individuals or small collectives, gaining traction through blatant exercises of power or sustained acts of coalition building”. This reminds us of the importance of humility in scientific and technological decision making. The projects we push forward help make certain views of the world become reality and generate benefits and harms which may be unequally distributed among different societal groups. In times of Bolsonism, we should therefore ask ourselves what kinds of imaginaries we want to build and to which social, environmental and political objectives our research is contributing.  How can we ensure, for example, that industrial biotechnology will not undermine the livelihoods of subsistence farmers or contribute to agricultural models that cause deforestation? That the scientific solutions to tackle diseases and improve human well-being will also prioritise the needs of the poorest? Or that the research we produce will help debunking certain beliefs on class, gender and race hierarchies? Such questions are especially urgent when, as with Bolsonaro, those are the kinds of concerns that will likely be ignored. With this comes great responsibility and a duty of care, for which an unashamed and committed politicisation of science is required. Human values intersect our practices and shape our vision of public good. How should we respond to neo-liberal development policies which are detrimental to the environment, to social conservatism and the violation of human rights? Can we scientists, researchers and academics ask, every day, how are we promoting a culture of care through our professional selves as we typically ask of our personal selves? The philosopher of science Isabelle Stengers has reminded us that resistance is achieved by an intelligent, inclusive and collective endeavour against the establishment. Brazil’s future president supports vile beliefs in men’s superiority over women, hate towards the black, indigenous and LGBT communities, as well as the weakening of long-fought protection of Brazilian natural resources and of the autonomy of leftist educators. One way to resist his discourse and agenda is by saying and doing otherwise: by preaching responsibility over complacency and arrogance, and care over violence and destruction."
"Former US vice-president Al Gore has tried to mobilise the global elite to fight the climate crisis by comparing it to some of history’s greatest battles, from Agincourt to Dunkirk. Gore told delegates at the World Economic Forum in Davos that the scale of the climate emergency was much worse than people recognise, and getting worse much faster than people recognise.  “The burden to act on the shoulders of the generation of the people alive today is a challenge to our moral imagination,” said Gore, during a session on protecting the Amazon and developing sustainable markets. “This is Thermopylae. This is Agincourt. This is the Battle of the Bulge. This is Dunkirk. This is 9/11,” declared Gore. “We have to rise to this occasion.” Gore, who has campaigned on environmental issues since losing the 2000 presidential race, added that the world currently lacks the requisite political will to tackle the emergency. “But remember, political will is itself a renewable resource,” he added, to applause from the audience. Jane Goodall, the English primatologist and anthropologist, called for more education and funding to help young people tackle the crisis. “My hope is that we can increase the level of education and understanding, not only but especially among our youth, and increase the level of funding so they can do more,” she said. “The young people know what needs to be done, but very often there aren’t the resources for them to actually do it,” Goodall added."
"
This picture, taken by www.surfacestations.org volunteer Don Kostuch is the Detroit Lakes, MN USHCN climate station of record. The Stevenson Screen is sinking into the swamp and the MMTS sensor is kept at a comfortable temperature thanks to the nearby A/C units.

The complete set of pictures is here
From NASA’s GISS, the plot makes it pretty easy to see there was no discernible multi-decadal temperature trend until the A/C units were installed. And it’s not hard to figure out when that was.

But hey, thy can “fix” the problem with math and adjustments to the temperature record.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4e14bff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The former prime minister, Malcolm Turnbull, has said he “can’t explain” Scott Morrison’s behaviour during Australia’s unprecedented bushfire crisis and that his successor had “downplayed” the catastrophe and had not behaved the way a prime minister should. Turnbull made the extraordinary criticism of Morrison during an interview with the BBC on Wednesday, in which he also blamed News Corp and rightwing thinktanks in Australia for promoting climate change denialism.  He “could not explain” why Morrison had refused to meet former fire chiefs – who early last year attempted to warn him of the risks posed by the coming fire season – and then decided to holiday in Hawaii in the midst of the crisis in December. “Everybody knew we were in a very dry time and as a consequence the fire season was likely to be very bad,” Turnbull said. “So rather than doing what a leader should do and preparing people for that, he downplayed it – and then of course chose to go away on holiday in Hawaii at the peak of the crisis. “I can’t explain any of that. It’s not consistent with the way in which a prime minister would and should act.” Morrison cut short his family holiday to Hawaii after two volunteer firefighters in New South Wales died. He came home two days early and apologised, saying he “deeply regretted” any offence caused. But he was then heckled on a visit to the fire-ravaged town of Cobargo in NSW, where three people died, and mistakenly said that nobody had died on South Australia’s Kangaroo Island, where two people had died. “I do not know why Scott Morrison has acted the way he has,” Turnbull said. “I’ve known him for 20 years, at least. I can’t explain his conduct.” Morrison’s personal approval rating and rating as preferred prime minister fell in the first polls after the bushfire season began. “I can’t explain why he didn’t meet the former fire commissioners who wanted to see him in March last year to talk about the gravity of the threat,” Turnbull said. In November the former fire chiefs said Morrison had turned down a meeting with them because his government “fundamentally doesn’t like talking about climate change”. Turnbull also criticised the role of rightwing politics in Australia and the Murdoch press in promoting climate denialism. “If you go to any of the rightwing thinktanks or read the Murdoch press it is just full of climate denialism,” he said. “And it is designed to deflect from the real objective which has to be to reduce our greenhouse gas emissions. “To be a climate change denier is a badge of honour on the right wing of politics here and in the US, and it is mad.” Turnbull said Australia was “in the frontline of the consequences” and needed to act on the climate crisis to show the world that it was important. “How many more coral reefs have to be bleached, how many more million hectares of forest have to be burned?” he asked. “How many more lives and homes have to be lost before the climate change deniers acknowledge they are wrong? “If a country like Australia is not prepared to grapple with this issue seriously, itself being in the frontline of the consequences and being an advanced, prosperous, technologically sophisticated country, with the means to do so, then why would other countries take the issue as seriously as they should?” On Tuesday Morrison said reducing fuel loads with hazard reduction burns was at least as important as reducing carbon emissions to prevent future bushfire disasters. “Hazard reduction is as important as emissions reduction and many would argue, I think, even more so because it has an even more direct practical impact on the safety of a person going into a bushfire season,” he told the Australian. In November former heads of the NSW, Queensland, Victorian and Tasmanian fire services said they were not allowed “to utter the words ‘climate change’” even though it contributed to longer, more intense bushfires. “Bushfires are a symptom of climate change,” said Neil Bibby, the former chief executive of Victoria’s Country Fire Authority. “Firefighters are the immune system that gets rid of that symptom. But [the problem is] still there.”"
"
Share this...FacebookTwitterVictoria Falls ignoring IPCC science: Sometimes more water, sometimes less
By Die kalte Sonne
(German text translated by P. Gosselin)
German Spiegel Online on 7 December 2019:
Victoria Falls in Zimbabwe and Zambia: “It’s the longest dry period that we have ever had.”
The Victoria Falls are considered to be the widest waterfall in the world. But instead of the usual quantities that plunge into its depths, there is a drought – tourists have also gone absent. The mood there is gloomy.”
And, of course, it is being immediately attempted to explain the water flow with man-made climate change. The Guardian wrote on the same day:
Data from the Zambezi River Authority shows water flow at its lowest since 1995, and well under the long-term average. Zambian president, Edgar Lungu, has called it ‘a stark reminder of what climate change is doing to our environment’.”
Spiegel calls it the worst drought ever. The Guardian calls it the worst drought since 1995. That’s a small difference.
As always, it’s best to look at the hard data. Here we look into a report by Richard Beilfuss from 2012 (pdf here), which has the following exciting title:
A Risky Climate for Southern African Hydro: assessing hydrological risks and consequences for Zambezi River Basin Dams “
The report checks whether dams along the Zambezi River are always supplied with sufficient water. In the event of water shortages, the turbines would quickly stop and the production of electricity would fail. Therefore, Beilfuss looks back into the hydrological past of the Zambezi region to better understand the variability of rainfall. He embeds his research in the usual climate change narrative, which we can overlook.
We are particularly interested in the facts presented by Beilfuss. From his summary:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Zambezi River Basin has one of the most variable climates of any major river basin in the world, with an extreme range of conditions across the catchment and through time. Average annual rainfall varies from more than 1,600 mm per year in some far northern highland areas to less than 550 mm per year in the water-stressed southern portion of the basin.
Runoff is highly variable across the basin, and from year to year. The entire Zambezi River Basin is highly susceptible to extreme droughts (often multi-year droughts) and floods that occur nearly every decade. Droughts have considerable impact on river flows and hydropower production in the basin. For example, during the severe 1991/92 drought, reduced hydropower generation resulted in an estimated US$102 million reduction in GDP, $36 million reduction in export earnings, and the loss of 3,000 jobs.”
Oh, man. The Zambezi area is already well known as a highly variable rain area. Can a drought be surprising at all? Why is this drought man-made in 2019, when there have apparently always been droughts in the past?
Not CO2 related
A little later in the report we also find a flow diagram for the Victoria Falls:

Figure: Water flow rate-volume at the Victoria Falls an den 1907-2006. Source: Beilfuss 2012 (immediately pdf)
We see a strong variability from year to year. On a scale of several decades, the period 1940-1980 is characterized by particularly high flow rates. The early 20th century was rather dry. A coupling to the 60-year-old ocean cycle offers itself. The Pacific is far away, but the wet Zambezi phase fits quite well into the negative PDO:

Figure: The Pacific Decadal Oscillation (PDO). Source: By Giorgiogp2 – Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=13297650
Working hypothesis: Whenever the PDO is positive, the flow at the Victoria Falls decreases. And what is PDO doing right now? It is positive. It fits!

Abbildung: PDO bis Ende 2019. Quelle: daculaweather.com
It’s a pity that so few take the trouble to first check the natural precipitation dynamics and possible correlations.
It’s so much easier to simply hold the universal rogue CO2 reflexively responsible for every observed rain anomaly. Naturally, this quick fix is not sustainable. In this specific case, it was probably just a matter of dark climate alarmist background music for the climate conference in Madrid.
Share this...FacebookTwitter "
nan
"

The Bush Administration caught unshirted hell from environmentalists last month for ordering a 20 percent increase in energy efficiency standards for air conditioners sold after 2006. The Clinton administration, you see, had proposed a 30 percent increase in standards, so the Bush administration is once again being hammered for relaxing environmental standards, promoting wasteful energy consumption, accelerating global climate change, and risking California‐​style blackouts during hot summer months for years to come.



While the Greens would have us believe that Bush was once again carrying water for corporate America, the president was actually doing us all a small favor. 



First, the Clinton standards would have imposed huge costs on consumers. The U.S. Department of Energy (DOE) estimates that the Clinton standards would have increased the price of your typical air conditioner by a whopping $332 to $435. The Bush administration standards would increase prices by only (!) $144 to $213. 



Second, the DOE had failed to adequately consider the possibility that some manufacturers would be driven out of business by the Clinton standards, thus reducing competition that helps restrain prices. So even those expected astronomical price increases were almost certainly low‐​ball estimates. 



Third, the DOE concluded that over 40 percent of consumers buying air conditioners that meet Clinton’s standards would never recover the higher costs through energy cost savings. Under the Bush administration standards, only about 25 percent of consumers would be net losers. 



The Greens are right, however, to point out that the Bush standards will accomplish little on the environmental front, but the Clinton standards would not have achieved much either. Consider: The DOE estimates that the Bush standards will reduce growth in energy consumption by “about 3 quads” between 2006–2030, a figure deemed by the department as “significant.” The nation, however, is expected to use about 3,200 quads of energy from 2006–2030, so “3 quads” equals but 9/100 of 1 percent of projected energy use. The DOE’s claims about avoided emissions of carbon dioxide and nitrogen oxides are equally overblown.



While we should be grateful that the Bush administration has adopted efficiency standards that are somewhat less costly than those planned by the Clinton administration, the unwarranted economic burden and restrictions on consumer choice remain too great. Consumers, not bureaucrats, should have the final say about what’s in the home.



The chance of recovering the higher cost of air conditioners through reduced energy consumption, after all, depends heavily on electricity prices and the frequency with which the air conditioner is used. Given that electricity prices vary widely throughout the United States (highest unit prices are more than double the lowest), and that consumers’ usage patterns vary for dozens of reasons (e.g., geographic location, family size, tolerance for warm temperatures), paying more up‐​front at the cash register to reduce operating costs by a small amount for years to come makes sense for some but not for others. 



While that observation should be rather obvious, it is — for practical purposes — ignored by appliance efficiency standards that apply to everyone whether it makes sense or not. The result is that many consumers are forced to incur higher costs than they would if allowed to base their decision on factors affecting their energy bills. The requirement is roughly equivalent to an attempt to restrain hat costs by requiring that everyone wear a size seven hat. 



Moreover, the entire regulatory exercise is fraught with uncertainty. The government, for instance, assumes that it can accurately forecast the conditions affecting future appliance manufacture, sales, and use during the next 30 years, including changes in technology, markets, energy prices, consumer and appliance manufacturer behavior, raw material, labor and overhead costs, and wholesale and retail markups. How likely is that, particularly when much of the data comes from appliance manufacturers with a direct financial interest in the DOE’s decisions? 



The upshot is that all analytic uncertainties in the exercise are resolved in favor of tighter standards because only the well organized — typically those positioned to gain the most from government action — can affect the rulemaking process. Big appliance manufacturers, for instance, are happy to have the government impose regulatory barriers to entry into their market, barriers that serve to cartelize the industry. Bureaucrats are always on the lookout to expand their power and reach. Green political lobbies consider energy conservation a religious virtue regardless of economic realities and are in the business of delivering such mandates in return for contributions from the faithful. 



Everyone wins but the poor consumer who is for the most part oblivious to these regulatory machinations undertaken at his expense. The administration did us a favor by minimizing the hunk of flesh taken out of our economic hides. But it would be better to junk these standards and make the consumer, once again, king of his own pocketbook. 
"
"

Many in the energy and environmental industries thought Donald Trump’s victory in November meant certain death for the Clean Power Plan (CPP), a piece of low‐​hanging fruit in Trump’s promise to revitalize coal country. This regulation, which many argue is one of the most expensive in American history, was key to Obama’s climate legacy and, indeed, the President’s Executive Order issued this week does kill the CPP. Until, that is, the environmental activists file for a stay, which could happen any day now.



As with Trump’s promises for the revitalization of coal country, all of this will be more complicated than suggested.



Legally, the Supreme Court’s 2007 decision, _Massachusetts v. EPA_, held that if the EPA determined carbon dioxide is a pollutant causing harm to human health and welfare, then it is empowered to regulate it under the 1992 amendments of the Clean Air Act.





The conversion from coal to cleaner burning natural gas has led to the decoupling of economic growth from an increase in carbon emissions — something many said would only be possible through government coercion.



Trump’s executive order cannot call on the EPA to cease and desist from its Clean Power Plan until it somehow determines that carbon dioxide, after all, does not cause endangerment, or that the science is simply not there to show that it does. As science moves slowly, and with the federal government itself providing a vast majority of all climate science funding, this will be a difficult battle.



Undoing regulations is typically more difficult than creating them. However, the selection of Scott Pruitt, who defended the rights of Oklahomans to set their own environmental standards, shows the Trump administration is serious. While many left‐​leaning environmentalists tend to believe Pruitt is “against” the environment, the truth is that most Republicans strongly value the environment — they just wish to regulate it at a state level, where local knowledge and values can be applied. Pruitt is not an anti‐​environmental zealot; as for the EPA, he’s said “Clearly the mission of the EPA is to protect our natural resources, protecting our water quality, improving our air.”



And, as many have noted, even the elimination of the Clean Power Plan will not itself bring coal back to anything like its former life. The major reductions that the US has made in its greenhouse gas emissions stem not so much from a war on coal (indeed, the previous administration was surely belligerent toward the industry), but from the market itself.



Dramatic advances in geolocation and hydraulic fracturing have made natural gas, which only emits half as much carbon dioxide as coal when used for power generation, and the equipment used to burn it, cheaper than coal. It also burns much cleaner, so the expensive scrubbers and bag houses required to capture coal’s bad residuals are not necessary.



This conversion from coal to cleaner burning natural gas has led to the decoupling of economic growth from an increase in carbon emissions — something many said would only be possible through government coercion. Instead it was accomplished by greed and genius.



It’s hard to predict the legal fate of Mr. Trump’s latest executive order. What we do know, though, is it will be a long time before the dust settles, and unless many fundamental changes occur legally, diplomatically, and scientifically, any new administration can bring Obama’s policies back to life with a pen and a phone.
"
"

Above: Tifton, GA Sewage Treatment Plant – a good place to measure climate?
There have been some claims on the blogosphere of limited or no value to the taking of pictures for the www.surfacestations.org project. This is my view of why pictures are vitally important to an assessment of the accuracy of the near surface temperature record gathered by USHCN and other weather stations, where the data gathered is used in climate studies.
Photography is well established as a diagnostic tool in many fields. Take astronomy for example. If data and computer models of the universe is all that was needed to move the science forward, we certainly wouldn’t need the Hubble Space Telescope.
Do pictures work very well in illustrating problems that need correction?  Well I say ask any doctor who uses xrays, or MRI images, or ultrasound. Do you think doctors can define an illness solely on chart data such as BP and body temperature? No of course not, they need pictures. They DEMAND pictures.
Or how about the NASA’s loss of the space shuttle Columbia in 2004? The spacecraft is covered in sensors, yet after a photo showed foam striking the shuttle during booster burn, engineers pleaded to get photos under the wing from Department of Defense DOD. NASA Engineering made three separate requests for DOD imaging of the shuttle in orbit to get photos to determine if there was damage. NASA management did not honor the requests for DOD photos and in some cases intervened to stop the DOD from assisting.
On reentry, sensors on the shuttle started showing problems, and flight controllers struggled to understand what was happening. Photos and video taken by amateurs on the ground showed clearly what had happened. I don’t recall CNN showing pictures of sensor data in announcing this failure to the world.
Given NASA’s unwillingness to listen to engineers first with Challenger (frost and o-rings) and Columbia (possible wing damage – just get us a picture so we can be sure) I have even less respect for the NASA armchair UHI analysis called “lights = x” ironically done by counting the number of streetlights near weather stations using DOD nighttime photos. This method can give an approximation of the urbanization around a weather station, but it can’t possibly discern the nearby microsite effects like asphalt and air conditioners that have seen so far.
The worlds of science, engineering, medicine, forensics, astronomy, biology, and many more use photos to cross check gathered data or to confirm observations or theory. Climatology shall be no exception.
We are getting pictures of stations, lots of them, and we’ll get every one if possible. Then we are going to analyse them against existing published standards, and then we will publish the results of that analysis. And unlike some prominent climatologists, the pictures, the methods, the code, and the results will be publicly available to anybody, be it scientist, layman, or citizen. And, it will be done without wasting once cent of taxpayer money.
Then after that, critics can determine just how useful the pictures are.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea50af6b3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Claire O’Neill, the former UK energy minister who was to lead the UN climate talks this year in Glasgow, has been removed from the post. Her sacking comes as Boris Johnson prepares to launch the UK’s strategy for hosting November’s crunch climate talks, known as COP26.  O’Neill, under her Twitter handle of @COP26President, wrote on Friday evening: “Very sad that the role I was offered by Boris Johnson last year has now been rescinded as Whitehall ‘can’t cope’ with an indy COP unit. A shame we haven’t had one climate cabinet meeting since we formed. Wishing the COP team every blessing in the climate recovery emergency.” The dramatic last-minute change of plan follows murmurings over the past month that O’Neill, known under her previous married name of Perry when she was a minister, lacked the gravitas for one of the most important jobs in international politics this year. A source in the COP26 unit said: “Claire has seriously underperformed, including at Davos and on a recent ministerial visit to India. She had said ‘the Paris agreement is dead’ in key meetings to the surprise of everyone. “She didn’t seem to get that this is a diplomatic job. The senior team of officials in the unit couldn’t work with her and her erratic behaviour and poor performance has spooked key stakeholders in the UK and internationally. She had to go. The PM now needs to show he is taking this seriously by appointing a heavy-hitting minister.” Governments must come to COP26 prepared to scale up their commitments on cutting greenhouse gas emissions, or risk the failure of the 2015 Paris agreement on the climate. Achieving the consensus needed among world governments will be a mammoth task – many countries, including the US, Brazil, Saudi Arabia and Russia, are now hostile to the Paris agreement, and others including China and India have been unwilling to make big new public commitments under the accord. O’Neill’s previous experience peaked at junior minister, which was regarded by some diplomats as not enough to gain her the respect needed to get meetings with premiers and top officials around the world. Observers of the UN talks interpreted her removal as a sign that Johnson was taking COP26 more seriously. He will make his first public intervention on the issue next Tuesday, when he will launch the UK’s COP26 strategy, at an event with Sir David Attenborough, the climate expert Lord Stern, the outgoing governor of the Bank of England, Mark Carney, the UN’s climate chief, Patricia Espinosa, and a host of dignitaries. “A good COP president makes all the difference between success and failure,” said one former high-level diplomat and COP veteran. “They direct the negotiations, they play the key role in determining the outcome.” Tom Burke, the co-founder of the environmental group E3G, said he thought that either William Hague or Michael Howard, both former Conservative party leaders, could be possible choices to replace O’Neill. “If this is a sign that the government really wants to signal its intent to make a success of COP26, it has to appoint some more senior people, and those two candidates come to mind,” he said. The Cabinet Office, which has been leading on the COP26 talks, declined to say whether a successor had been chosen. There have been rumours among climate activists that Zac Goldsmith could take on the role, and O’Neill’s tweet appeared to suggest that responsibility for COP26 could be taken away from the Cabinet Office. The Cabinet Office put out a statement: “Claire Perry O’Neill will no longer be UK COP26 president. The prime minister is grateful to Claire for her work preparing for what will be a very successful and ambitious climate change summit in Glasgow in November. Preparations will continue at pace for the summit, and a replacement will be confirmed shortly. Going forward, this will be a ministerial role.” That could leave the way open for a House of Lords appointee. Responsibility for the UK’s participation in the annual talks was previously held by the Department for Business, Energy and Industrial Strategy, after the closure of the Department of Energy and Climate Change. Another COP expert said whoever took on the job must have the clear and public backing of the prime minister, and that Johnson must take a much stronger public role on the issue. In her parliamentary career, O’Neill was best known for having to apologise when, unable to catch the Speaker’s eye, she wondered aloud in the House of Commons to whom she needed to “give a blowjob” in order to get her say. She also issued a putdown to David Davis when he confused her with another female Tory minister, Caroline Nokes. Referring to Davis’s previous campaigning slogan, she is reported to have told him: “David, let me help you: Caroline is a C cup, I am a double D.” Her exuberant manner also led to trouble. In November 2018, three unions wrote to BEIS, where she was a minister, to raise allegations of shouting and bullying civil servants. O’Neill started out as a political radical. The environmentalist George Monbiot recalls her at Brasenose College, Oxford, where she studied geography, as “a leftwing firebrand who wanted to overthrow capitalism and nationalise the banks. She was impressive and persuasive, and had some influence on my thinking. You can imagine my disappointment when she took a City job and became a Tory.” She entered parliament with the coalition government in 2010, her no-nonsense approach belying the patronising “Cameron cutie” label. O’Neill announced her decision to resign as an MP last September, citing the pressures of the COP26 presidency, and she stood down in December’s election. That left her free to jet to foreign capitals, but outside cabinet discussions. Johnson’s government is vulnerable to charges that ministers are not prepared to make the hard decisions required against vested interests in fossil fuels and finance. There was a major hiccup at the Africa summit last week when, as Johnson pledged not to invest in coal in Africa, the Guardian drew attention to the almost £2bn finance from the UK pouring into African oil and gas."
"
Share this...FacebookTwitterDr. Knut Wittkowski, the former head of the epidemiology department at Rockerfeller University, says doing nothing would have been more effective – and ultimately cost fewer lives – than the “containment” strategy now in operation across the world.
By restricting movement and confining people in their homes we are unnecessarily prolonging or widening the curve instead of just flattening it.
The only way to eliminate any respiratory virus is not by developing vaccines or with pharmaceutical intervention, but by natural herd immunity. This means we should be allowing children to attend school.
When 80% of the population becomes infected – and the vast majority of the population won’t even know it because they won’t have symptoms – a common coronavirus like this one can be exterminated within about 4 weeks.
By trying to contain the virus, we are practically ensuring there will be a “second wave” of infections in the Northern Hemisphere fall, as not enough people will have been infected in recent months to exterminate this particular coronavirus strain.
Dr. Wittkowski asserts he is able to talk candidly about what should have been done in response to this COVID-19 outbreak because he is not paid by the government and therefore he is able to “actually do science”.

Transcribed commentary (and image) from a YouTube interview

Share this...FacebookTwitter "
"Encompassing swathes of Ethiopia, South Sudan and Kenya, the Omo-Turkana Basin is one of the oldest landscapes in the world that is known to have been inhabited by Homo sapiens and is now one of the world’s most extraordinary examples of ethnic diversity. In the lower Omo Valley alone, a varied history of cross-cultural encounters has played out to produce eight distinct ethnic groups, speaking many languages from Afro-Asiatic to Nilo-Saharan. In a cattle camp on the bank of the ancient Omo River a Mursi elder implored me to “tell our story so that others might know us before we are all dead in the desert”. Where the river ends in Lake Turkana, this sentiment was echoed by local fishermen: “You will find our bones in the desert.” The story of the Omo-Turkana Basin is now that of the Ethiopian state exploiting its periphery in the name of “development”, trampling on the human rights of its citizens in the process. Over the past decade, the Ethiopian government has pushed ahead with a huge hydro-electric dam on the Omo, known as Gibe III. Without any meaningful consultation with the communities affected, the state has also appropriated grazing lands and freshwater, threatening their vital resources and local heritage.  All of this has happened despite the area gaining the status of a UNESCO World Heritage Site in 1980. As Richard Leakey, the Kenyan paleoanthropologist, conservationist and politician put it, “these happenings are profoundly disturbing”. The completion of Gibe III, Africa’s tallest dam to date, has eliminated the annual flood and radically reduced the Omo’s flow, which produces 90% of Lake Turkana’s freshwater input. In doing so, it has reduced sediments and nutrients critical for traditional agriculture, riverside pastures and fish habitat.  Over 30% of the lake inflow will be diverted for commercial irrigation projects. The result could be a fall in lake level comparable to that of Central Asia’s Aral Sea, which has shrunk by over two thirds since the 1960s because of irrigation abstractions  and which has been called “the world’s worst environmental disaster”. To make way for the commercial plantations planned for the Omo Valley, tens of thousands of hectares of land will be expropriated and thousands of local people displaced. The need to see “development” as more than a simple matter of an increase in GDP is well established. In his seminal work, Development as Freedom, the Nobel Prize winning economist, Amartya Sen, demonstrated that sustainable development must be based on universal access to social and economic necessities as well as political and civil rights. The many communities in the Omo-Turkana Basin have suffered a systematic curtailment of their most basic and essential rights.  International agreements which the Ethiopian government signed up to, such as the 1993 International Convenant on Civil and Political Rights and the International Covenant of Economic, Social and Cultural Rights require it to protect and promote the rights of minority cultures and ensure the “right of everyone to take part in cultural life”. Since 1948, Ethiopia has also been signed up to the Convention on the Prevention and Punishment of the Crime of Genocide. Article II provides against the destruction of “a national, ethnical, racial or religious group”. Raphael Lemkin, who coined the word “genocide”, famously defined the specific need to protect against the “disintegration of the political and social institutions of culture, national feelings, religion, and the economic existence of national groups”. It is difficult not to conclude that what we are seeing in the Omo is the wholesale disregard of these commitments by the Ethiopian government. Its development policies are not only transforming landscape and heritage but destroying complex systems of sustainable living that have endured for millennia. The huge injustice of all this is that the ecological costs will be borne by local communities while the profits will be enjoyed by central and international corporations.  Meanwhile, centuries of collective wisdom relating to livestock diversification, flood dependant cultivation and customary obligations and mechanisms of livestock exchange, will be made redundant. This is not to deny, of course, that development, in the sense defined by Sen, is a laudable and necessary enterprise. But we must also recognise that large-scale infrastructure projects are likely to have far reaching consequences for the lifestyles and cultural identities of those they displace.  Projects which set out to increase economic growth without regard for social justice and individual rights are not worthy of the name “development”. Development must benefit locals and for this to happen their voices must not only be heard but also given a central and determining role in any discussions about the future of their lands and livelihoods.  Both cradle and crucible of our species, the Omo-Turkana Basin is unique and precious. Its heritage and history, as well as responsibility for its future, are shared by us all."
"
Share this...FacebookTwitterMany of us have been noticing there’s a not-so-subtle hate-campaign against the elderly going on.
Not long ago we saw a German production which featured kids singing “my mother is old environmental scumbag“or how Brexit voters were portrayed as incontinent scum.
At a recent performance in Dortmund, Leftist hip-hop band K.I.Z. told a cheering crowd of predominantly women:
People are scared of some stupid virus. The truth is, only old white men die!”
“Corona is practically healing the planet”
Now we have a production again by ARD/ZDF German Public Television, Browser Ballet, that tops it all. The satire is titled: “Corona is rescuing the planet”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





In the skit the moderator begins by telling viewers:
We here at Browser Ballet say ‘yes’ to Corona because this virus is practically healing the planet by itself. Interesting is how fair this virus is. It’s ravaging the elderly, but the youth are withstanding this infection almost without effort. That’s only just because it is the generation 65 and over that has run this planet into a wall over the past 50 years.”
Especially responsible, says the moderator, are the overweight and ill adult Americans, who have been recklessly practicing earth-destroying “turbo capitalism”:
Maybe the Corona virus is merely a response to turbo capitalism, and it is working. Air travel has collapsed, production has been cranked back, consumption is declining. There couldn’t be better news for this planet. Air pollution in China, thanks to Corona, has decreased in a very short time. If that continues, then we may experience a new green paradise!”
He adds:
And isn’t it the problem that there are just too many of us? Less people means less shortages of resources, which means less hunger, which means less war, and that means less causes for refugees. So, probably the Corona virus is simply a nice reflex of nature to tell people who’s the boss here. That’s why: enough with this silly egoism. Corona is here simply because we don’t deserve anything better. “


		jQuery(document).ready(function(){
			jQuery('#dd_c281537b1a33a0b9fbc65907b25ba34a').on('change', function() {
			  jQuery('#amount_c281537b1a33a0b9fbc65907b25ba34a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The picture above of a thunderstorm over Upper Bidwell Park was taken by my Bidwell Ranch Weather Station Webcam at http://www.bidwellranchcam.com this summer.
Weather in the mountainous portion of upper park can sometime be pretty unpredictable, with storms forming quickly due to upslope winds which cause an Orographic Lifting effect aiding the quick formation of a storm.
You may have read in the Sunday ER about the new Outdoor Planetarium  at the Kiwanis Chico Community Observatory and some of the hi-tech gadgetry there.
Today I’m pleased to announce that I completed the installation of the first ever weather station in Upper Bidwell Park at the Observatory and it is available for online access at the Observatory web page at: http://www.chicoobservatory.com/weather.htm It displays new weather data every 15 minutes plus logs the entire day’s weather for use later. I often get requests for rainfall data for Big Chico Creek watershed, and there is virtually none. This will help.

The weather station will soon have its own live webcam, the picture in the weather graphic online now is from the Bidwell Ranch Cam.
When the new webcam is installed, it will overlook the Horsehsoe Lake fishing pier. Besides the camera being a bit of deterrent for rascally behavior, the new upper park weather station will also give hikers, bikers, and golfers an ideas of what the weather is like there right now, which will aid in choosing what to wear and what gear to bring. It will help observatory staff determine if viewing is likely that night. It will also display inside the observatory, and be linked to a network of weather stations around town for an even bigger project I’m working on to create a temperature and wind model to assist in an important public health service need. More on that in the future.
This is a free community service. Enjoy it with my compliments.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7eaa0897f8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSeveral new studies use evidence from temperature-sensitive plant species and megafauna remains to reconstruct an Arctic climate that was 6°C to 22°C warmer than today when CO2 concentrations lingered near 300 ppm.
Navigating the Arctic Ocean
William Barentsz discovered Arctic Svalbard as he sailed through an open-water Arctic Ocean using a wooden boat in early June, 1596.

Image Source: Wikipedia
In September, 2019 (the month of the year with least extensive sea ice), 16 scientists needed to be rescued by helicopters because the massive ship they were using to study climate change couldn’t cut through the ice-covered waters near Svalbard.
In the 1500s, the Western Arctic was sea ice free for about 4-5 months of the year. Today – and steadily since 1800 – the Western Arctic is sea ice free only about 2 weeks of the year (Porter et al., 2019).

Image Source: Porter et al., 2019
In fact, according to Rosel et al., 2018, Arctic sea ice was actually thicker in 2015 (1.56 m) and 2017 (1.65 m) than it was in 1955 (0.94 m).

Image Source: Rosel et al., 2018
The paleoclimate Arctic record
1. Voldstad et al., 2020  A much broader distribution of thermophilous (warmth-dependent) plant species suggest the sea surface temperatures near Svalbard were as much as 6°C warmer than they are today earlier in the Holocene, which effectively means the Arctic was sea ice free throughout the year.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Voldstad et al., 2020
2. Kirillova et al., 2020  About 115-100,000 years ago, “semi-dwarf” wooly mammoths thrived in an Arctic Siberia teeming with lakes, rivers, aquatic plants, grassy meadows, and forests. July temperatures were 8-10°C warmer than today’s. CO2 levels ranged between 260-280 ppm.

Image Source: Kirillova et al., 2020
3. Schenk et al., 2020  Plant species’ thermal tolerance limits (Finland) suggest July temperatures had already reached “near-modern” (~16°C vs. 17°C) levels 15,000 to 11,000 years ago, or during the last late glacial, when CO2 hovered near ~220 ppm CO2. By about 10,000 to 9,000 years ago, Greenland was “4-7°C warmer than today”.

Image Source: Schenk et al., 2020
4. Fedorov et al., 2020  The Eurasian Arctic was up to 8°C warmer than today during the last interglacial (130,000 to 110,000 years ago). Extensive forests lined the Arctic Ocean coast from 10,000 to 3,000 years ago, when the region was also much warmer than today. Today’s Eurasian Arctic coast is treeless tundra. The cold-preferring collared lemming has 25% more habitat now than it did earlier in the Holocene due to today’s tundra expansion and cooler climate.

Image Source: Fedorov et al., 2020
5. Rybczynski et al., 2013  Giant camels and horses were hedged by plants, forests, and wetlands in a balmy High Arctic until at least the late Pleistocene (~79k yrs ago). CO2 ranged from 190 to 280 ppm back then (Pleistocene) and averaged about 300 ppm during the Pliocene, but yet the Arctic climate was 18.3°C warmer than today.

Image Source: Rybczynski et al., 2013
6. Fletcher et al., 2019  From 2 to 4 million years ago, as CO2 pivoted around 300 ppm, the Arctic was 15-22°C warmer than it is today.

Image Source: Fletcher et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_bf2a70f196bcfaac008638389ee41e60').on('change', function() {
			  jQuery('#amount_bf2a70f196bcfaac008638389ee41e60').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterNesting red kite shot dead because of wind energy?
By Die kalte Sonne
(Text translated by P. Gosselin)

Red kites have little chance against wind turbines. Image: Thomas Kraft (ThKraft) – Own work, CC BY-SA 2.5
Red kites and wind power just do go well together. These predatory birds can find good prey especially where farmers mow meadows or plow fields. Lethal are cases such as the one in Baden-Württemberg, where areas with green fodder have been planted in the immediate vicinity of a wind park.
When these fields are mowed, the red kites search for food within the hay. It is ideal for them, but also possibly deadly because they cast their view downward when hunting, and not forward. The Hilpensberg wind farm was even approved in a red kite area. Now one of the beautiful animals has fallen victim again, as the Nature Conservation Initiative reports:
According to biologist Immo Vollmer, the conclusion can only be that we should not build any more wind turbines in areas where red kites nest or where buzzards often seek food. Otherwise the red kite, which has its largest distribution center in the world in Germany, will have no future here, because the loss rate is already almost in the same order of magnitude as the rate of offspring.”
And another sad case has just been reported in North Rhine-Westphalia. A female, nesting red kite was shot dead near Paderborn.
In an earlier trial, a judge even gave the controversial wind projects approval – precisely where the shot bird was found – under the condition that no protected species be proven to exist there. Now that the animal has been executed, this condition has been met. Probably just a coincidence, or maybe suicide, to make the wind turbines possible and to get out of the way?


		jQuery(document).ready(function(){
			jQuery('#dd_9aefd3a91f352f2c5132dfecb2a10391').on('change', function() {
			  jQuery('#amount_9aefd3a91f352f2c5132dfecb2a10391').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The major criticism that East Asian officials would make of the outgoing Bush administration’s foreign policy would be Washington’s focus on the geostrategic problems in the broader Middle East in the past eight years, and the resulting sidelining of China and most of East Asia on the US global agenda. This neglect of China needs to change. 



Secretary of State Condoleezza Rice’s recent trips to South Asia (to try to defuse Indo‐​Pakistani tensions in the aftermath of the Mumbai terrorism) and to the Middle East (to attempt to re‐​energise Israeli‐​Palestinian negotiations) have been highlighted in leading US newspapers. Treasury Secretary Henry Paulson’s meetings in Beijing, as part of the ongoing Strategic Economic Dialogue, have, however, only been minor news as far as the US media is concerned. 



After president‐​elect Barack Obama recently unveiled his national security team, most of the discussion among Washington’s pundits centred on how the selection of Hillary Rodham Clinton as secretary of state and the retaining of Robert Gates as defence chief would affect US policies in the Middle East. China and East Asia were largely ignored. 



Earlier, in the foreign policy debates during the presidential election campaign, when China was mentioned, it was mostly in the context of criticising its trade policies, and warnings of its rise as a geoeconomic “threat” to US interests. 



Certainly Mr Obama needs capable people in his administration to manage the challenges in the Middle East. But he and his foreign policy aides must realise that all the major geostrategic and geoeconomic problems facing the US in the next four years, including energy policy, climate change, nuclear proliferation — and the current global economic crisis — will require co‐​operation with Beijing. 



During his discussions with officials in Beijing, Mr Paulson expressed concern that lowering the value of the yuan to stimulate the Chinese economy could worsen the US slowdown by keeping Chinese export prices relatively low and import prices high, which would hurt US exporters. In the past, US lawmakers have threatened to punish China if it refuses to change its exchange rate policies. But it is unlikely that Congress will risk a trade war now, given that America’s effort to spend itself out of recession will depend so much on the willingness of the Chinese to continue financing the US deficit. 



This dilemma highlights the need for a long‐​term strategy to manage the Sino‐​US relationship in a way that encourages China to assume greater leadership in multilateral economic organisations like the International Monetary Fund. 



During the cold war, the first question on the minds of America’s allies and rivals following the election of a new president was: “How is he going to handle Moscow?” Today, and in the future, America’s friends and adversaries should be more concerned about the approach a new White House occupant will take towards Beijing. 
"
"Britain is highly dependent on imported food. By value, imports make up more than 90% of the fruit and vegetables consumed in the UK and half of the meat. Brexit is expected to increase trade costs and make food imports more expensive, something that could lead to changes in diets and dietary risk factors that influence health. In fact, Brexit could lead to up to 5,600 diet-related deaths per year by 2027, additional healthcare expenditure of £600m, and increase the GDP losses of Brexit by up to 50%. That’s according to estimates my colleague Florian Freund and I have published in a new Oxford Martin School Working Paper. Analysing the potential implications of Brexit is a tricky business. The concrete details of Brexit remain unclear. Proposals range from various forms of “soft Brexit” that include a new trade agreement with the EU, to a “hard Brexit” in which the UK falls back on the (higher) tariffs set out by the World Trade Organization. We evaluated these opposing ends of the spectrum and compared them to a no-Brexit (“remain”) scenario. We used an agriculture-economic model to simulate the impacts that changes in tariffs and regulatory measures could have on the agricultural sector in the UK. And we used a national disease model to estimate what the resulting dietary changes would mean for mortality from chronic diseases, such as coronary heart disease or cancer.  Currently so-called “dietary risks”, including not eating enough fruits and vegetables or eating too much red and processed meat, are the second biggest risk factor for mortality in the UK, after tobacco. Our analysis suggests that Brexit could further increase those dietary risks. As a result of increasing trade costs from customs checks, new regulation, and higher tariffs in the case of a hard Brexit, we estimated that prices for most foods would increase. Foods that are critical for good health would be especially affected. Fruit and vegetable consumption could be reduced by up to one portion each per person per week under a hard Brexit, and by half a portion each under a soft Brexit. The consumption of nuts and legumes could decrease by up to 7%. Other foods would be affected as well. Dairy consumption could go down by up to one portion per week, meat consumption by half a portion, and total calorie intake could decrease as well. Some of those changes have health benefits, such as reduced intake of red meat or, for overweight people, reduced calorie intake. But we estimated that those potential benefits would be outweighed several times by the reductions in health-promoting foods. According to our estimates, the Brexit-related changes in food consumption could lead to 5,600 additional deaths per year under a hard Brexit, and to 2,700 additional deaths under a soft Brexit. This represents an increase in overall mortality of 0.9% in the Hard Brexit scenario (about 610,000 people are projected to die in the UK in 2027), and 0.4% in the Soft Brexit scenario. For premature mortality (before age 70), the increases are slightly higher. Most of the additional deaths would be due to cancer, coronary heart disease and stroke, which are associated with reduced consumption of fruits, vegetables, and nuts. The health impacts of Brexit also have economic implications. In our analysis, we quantified the economic costs of the additional diet-related deaths by summing up the extra healthcare expense, and by using valuation techniques commonly used by government when looking at the costs and benefits of projects that could affect mortality, such as new nuclear power plants or roads. Accounting for the costs to healthcare and related services resulted in increases in healthcare-related expenditure of £600m per year under a hard Brexit, and of £290m under a soft Brexit. And valuing the changes in mortality using cost-benefit analysis – based on the willingness of society to pay to reduce risks to life – led to costs close to £12 billion under a hard Brexit, and £6 billion under a soft Brexit.  The Brexit bill for health adds to the impact that leaving the EU is expected to have on other sectors which will also face higher costs of trade and production. We and others have estimated losses in real GDP (a measure of output corrected for inflation) to range from 0.5% for a soft Brexit to 1.4% for a hard Brexit (in nominal terms, that would be 1.6-3.6%). The health costs of Brexit amount to 0.3% to 0.6% in terms of real GDP, and therefore increase the overall economic losses of Brexit by 40-50%.  Given the UK’s import dependence, in particular for fruit and vegetables, any Brexit-related increase in trade costs will make it harder to get hold of foods that are critical components of healthy diets and chronic-disease prevention. Whatever form Brexit might take, our analysis suggests that it will significantly impact the British food system and negatively affect the health and welfare of British citizens."
"Bitcoin recently turned ten years old. In that time, it has proved revolutionary because it ignores the need for modern money’s institutions to verify payments. Instead, Bitcoin relies on cryptographic techniques to prove identity and authenticity. However, the price to pay for all of this innovation is a high carbon footprint, created by Bitcoin mining.   Fundamental to that mining process is a peer-to-peer network of computers, referred to as validators, who perform Proof of Work. In essence, this involves computers solving computationally-intensive cryptographic puzzles that prove blocks of transactions, which are recorded in a public asset ledger, known as a blockchain. This ledger is publicly viewable by all computers, which helps the system achieve consensus in an unreliable network of participants. Validators are called miners because the computer, or node, that successfully validates one of those blocks is rewarded with “mined” Bitcoin. Thus mining is also the process by which Bitcoin adds new coins to the network. But these processes consume a vast amount of power. In my 2016 article, Socialism and the Blockchain, I estimated Bitcoin mining’s annual energy use at 3.38 TeraWatt hours (TWh), which I equated to the total 2014 annual consumption of Jamaica. Recent estimates show the currency’s annual consumption rising exponentially, currently reaching an incredible 55TWh. Indeed, a new paper in Nature Sustainability suggests that the energy costs of mining cryptocurrencies exceed the costs of mining physical metals. Furthermore, the paper estimates that Bitcoin emitted between 3m and 13m metric tonnes CO₂ in the first half of 2018. A team in Hawaii even suppose that, if Bitcoin’s adoption continues to rise, within a couple of decades, such emissions could help push global warming above 2°C. However, both the study in Nature and the team in Hawaii make assumptions about the means of energy generation. In the light of the recent disturbing UN 1.5°C Report, humanity would be wise to act on the recommendation for an “unprecedented shift in energy systems”. The hope is that such a shift towards large-scale renewable energy does occur, thus invalidating the assumptions made in those papers.  Nevertheless, concerns over Bitcoin’s energy consumption remain, so Ethereum, another cryptocurrency, is investigating a more energy efficient consensus algorithm known as Proof of Stake. This method differs from Proof of Work because miners on this network use their economic stake to prove transactions and therefore, they are not performing energy intensive calculations. That introduces some complications – not least, how to ensure that people in this network act honestly, as they would have nothing to lose by behaving dishonestly? Ethereum’s proposed solution is to introduce penalties through measures such as penalising miners for simultaneously producing blocks on two versions of the blockchain. After all, only one of those blockchains is valid. Bitcoin’s Proof of Work overcomes such problems implicitly because it includes natural penalties since miners have to expend energy to prove transactions. In economic game theory, a Nash Equilibrium is said to be reached when a system stabilises because no one gains by changing strategy from that which produces the stable state. Since Bitcoin rewards are given to miners only if their blocks help form the valid Bitcoin blockchain, the most profitable outcome, or the Nash Equilibrium, is for each miner to act in consensus with the majority. As a result, Bitcoin’s Proof of Work algorithm has proven effective, despite the excessive energy consumption. In essence, my work looks at whether blockchains are a rebuttal to the hierarchies of capitalism. If Bitcoin promotes a way of organising that does not rely on capitalist consumption, might that indirectly drive down society’s energy use and help lessen its environmental impact? After all, consider the recent alarming WWF report, which all but blamed capitalism for the dramatic decline in wildlife populations. We need alternatives. Perhaps, then, Bitcoin’s revolutionary offer, as an alternative to capitalism, means its energy use is a price worth paying? That argument holds some weight if it drives down consumption in other areas of society because Bitcoin mining is not the primary driver behind climate change. However, even then, given the urgency of environmental degradation, if we continue to produce energy in a manner that creates so much warming CO₂, that argument may provide scant consolation.  Perhaps alternative consensus schemes, such as Ethereum’s Proof of Stake, provide part of the solution. However, Bitcoin or not, if humankind is to avoid climate catastrophe, we need to take urgent action and find solutions that produce clean, sustainable energy. If we do that, humanity will benefit, and as a by-product, so will Bitcoin."
"
I decided I’d drop some more fun with entropy your way. Here is the USHCN
station of climate record in Redding, CA GISS number # 425725920010 and used in
the climate modeling database
It is now operated by the US Forest Service at their HQ located at the
Redding Airport. It used to be operated by the National Weather Service, but
that WSFO closed in the mid 90’s.
Like Marysville, the site is surrounded by asphalt, and the surface is
unnatural – its wood chips over weedmat, and I’ll have to say it was hot as heck
to walk on during mid-day..
But the kicker is the “accessories” they’ve added for convenience of running
the hygrometer and for night observations. Yes it is another fine high-quality
USHCN climate recording site. I wonder how many times they forgot to turn off
the light? It looks like there might be room for a hot plate to keep your coffee
warm while making observations.



The blower is used to run air past the wet bulb hygrometer…its not the
correct way
to do it (manual aeration by rotation is specified).

Here is the satellite picture from Google Earth



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea62cfaea',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Think of it as a hypnotist’s trick, because that’s what it is. Scott Morrison says it over and over: there is no dispute about the need to take action to reduce Australia’s emissions. No dispute. You are getting sleepy. No dispute. In fact there is a dispute, and a serious one. By failing to do what is necessary, the government Morrison leads maintains a serious dispute with what the climate science tells us needs to be done both in Australia and internationally to avert the most dangerous risks associated with global heating.  When it comes to mitigation, Morrison’s government is in dispute with the facts. It is in dispute with the evidence. It is in dispute with the truth. Day in, day out. So despite his gritted-teeth soothing and head-patting from the podium at the National Press Club on Wednesday after a summer of calamity, there’s a dispute alright, and it’s one of the most important disputes of our time. This dispute is about the future, and how our government shapes it on our behalf. The dispute turns on whether climate change is now all about adaptation, about adjusting and adapting to the new reality – or whether Australia, as a responsible, reformist, middle power, is still at the policy and diplomatic barricades trying to avoid the worst case scenario. Dear prime minister. That’s a big dispute. Big with a capital B. And this is a dispute that every citizen of this country, and every citizen of the world, has a direct stake in. Here’s one piece of advice for all you good folks who watched Morrison’s scene setter at the press club: do not consent to having your head patted. Do not consent to the high stakes hypnotism. When the prime minister says “taking action is agreed”, do feel free to say to yourself or to anyone around you: “No, it isn’t agreed.” If so inclined, add this: “Climate action remains contested, and it remains contested because the Coalition chose to weaponise climate change at the precise moment in history when we needed to get about solving it.” If you feel on a roll at this point, add this: “And this political party still weaponises climate change against its opponents if it feels there’s an electoral advantage in doing so.” Morrison told us, blithely, on Wednesday, that solving the challenge of climate change goes beyond targets and summits. It will be driven by technology, not by taxes – a convenient sort of rationalisation – as if this was all unavoidable, inexorable, a fixed set of conditions, rather than a choice, and a choice the Coalition has been at the epicentre of. Some facts. When we had a “tax” on carbon in Australia (that wasn’t a tax, either then, or now), emissions came down, which is what the science tells us needs to happen. When the Coalition repealed that “tax” (that wasn’t a tax, then, or now), emissions crept up, which is what the science tells us is dangerous. As my colleague Adam Morton has reported countless times, national emissions peaked in 2007, the last year of the Howard government, came down each year under the Rudd and Gillard Labor governments, and have flatlined since the Coalition was elected in 2013. Reversing that positive trajectory of abatement was a choice the Coalition made. It wasn’t something that the universe imposed arbitrarily on the government, a bit of happenstance. It was a choice these people, including Morrison, made. Eyes wide open. Another fact. When Australia went to the United Nations climate talks late last year and argued we should use carryover credits from the Kyoto period – an accounting fix that means Australia will promise to reduce our emissions by 26% but in practice only reduce our emissions by about half that headline number – that was a choice too. We made a choice to do less than is necessary, hurting ourselves in the long term, and making it harder to sustain any sort of global consensus for ambitious action. These things aren’t happening because of strange, alien forces beyond our collective control. They are happening because our government is choosing to fail on climate change mitigation. I’ll say it again because it’s important, and inexplicable, no matter how long you look at the same set of facts hoping to comprehend the incomprehensible: our government is choosing to fail, and trying to make a virtue of it. Our government has access to the science, to the best advice available, and yet it continues to shirk the mitigation challenge. It continues to gamble with the future in the worst way imaginable. Morrison still has time to turn this appalling behaviour around, and there are some interesting markers if you can penetrate his various maxims and misdirections. Working with the states on bilateral agreements to reduce emissions is one hint worth watching. I suspect our prime minister is still hedging his bets and refining his thoughts. But until actions follow words, there is only one reliable measure to judge the government on. Its record."
"
Share this...FacebookTwitterScientists suggest relative sea level changes are well-correlated with natural variability and accelerated sea level rise is a “recurring feature” of what has been observed for over 300 years. Five of six studied regions along the North American Atlantic coast show declining sea level rates (mm/yr) in recent decades.
After retreating into the sea until about 1960, for the last five decades the Atlantic coast of North America has, on net, reversed course, expanding at a rate of about 5 centimeters per year (Armstrong and Lazarus, 2019).
This is likely the exact opposite of what would be expected given the reports of accelerated sea level rise for this region in recent decades.

Image Source: Armstrong and Lazarus, 2019
The lead author of a new study, Professor Roland Gehrels, has previously found much more rapid rates of sea level rise prior to 1950 than in recent decades in Southern Hemisphere locations, such as along the coasts of Tasmania and New Zealand (Gehrels et al., 2012).

Image Source: Gehrels et al., 2012
In a new study, Gehrels et al. (2020) also found rapid rates of sea level rise reaching up to 3 millimeters per year during the 1700s along the Atlantic coast of North America. He suggests “those rapid episodes of sea level rise on the north east coast of North America in the 18th Century have a natural cause”.
Interestingly, of the 6 locations chosen for the study, only 1 (Connecticut) indicates sea level rise rates have been steadily accelerating throughout the second half of the 20th century and in recent decades. The 5 others (Nova Scotia, Maine, New Jersey, North Carolina, and Viðarhólmi) all show the millimeters-per-year rates of sea level change have either not been rising or even rapidly falling.
This would not appear to be consistent with a driving anthropogenic influence in sea level rise trends since the 1950s, or since CO2 emissions have risen dramatically.

Image Source: Gehrels et al. (2020)
Share this...FacebookTwitter "
"Smartphones may have been around for a while but demand for digital traffic is still growing fast. During the new year celebrations, mobile providers in the UK and Australia reported a 50% increase in traffic compared to last year, while in Taiwan mobile data more than doubled.  This is because mobile device availability, functionality and connectivity are increasingly creating the possibility of sending greetings to many others in different ways.   It isn’t just “special events” like New Year’s Eve or the record-breaking traffic generated by Apple’s iOS upgrade that we should worry about. Underlying such sudden demand peaks is an ongoing, steady increase of total demand. In Europe, recent years have shown typical increases of 30% every six months for home broadband and 15% for mobile (cellular) data. But given the resources marshalled to support continued rises in data volume and ever-faster speeds, we should also be debating what kinds of digital services have real social importance. It’s great that it’s so easy to keep in contact with close friends and family, almost anywhere in the world. Yet a time when we are battling to keep carbon emissions under control, can we really justify the energy consumption involved in streaming cat videos in ever-higher definition? Much of the energy and carbon burden of more data and more speed is buried in network infrastructure and hidden away in data centres, easily ignored by people in bedrooms and offices, but not by the global climate.   But what is all this movement of data for?  Ostensibly, about half of it is for staying in touch with others, and watching and listening to media. The rise of Facebook, YouTube and Skype is supported by larger data allowances, faster speeds, and more reliable and affordable home and mobile connections. This goes hand-in-hand with increasing numbers of smartphones, tablets, and internet-connected televisions. More data to more devices all day and night pushes us to faster infrastructure that is active a greater proportion of the time.  This in turn means more energy demand and greenhouse gas emissions.  Figuring out exactly how much more demand and emissions is difficult as it involves many unknowns, from the originating data centre right through to the computer or phone at the other end.  But we do know the following: sharing photos or videos is more energy-intensive than sending a text message or making a phone call; that video streaming tends to require more energy than broadcast television; and that listening and watching on smartphones and tablets adds to, rather than wholly supplants, the hours that TVs are watched.  Demand for consumption and the means to consume are joined at the hip – they constitute each other.  This has resulted in escalation: higher-speed and more widespread infrastructure has made feasible new data-hungry services such as Netflix. This in turn has spurred higher speeds and more pervasive infrastructure allowing higher demand.   Indeed, fast connections mean people have made more intensive use of on-demand streaming services. No one binge-watched a whole TV series through a dial-up connection. But enterprising companies are also making use of faster connections. Unsatisfied with regular high-definition video, Netflix developed “super HD” which increases data demand by 11-50% and led to significant increases in traffic for some internet service providers. More recently, the introduction of Facebook’s “autoplay”, which streams shared videos and adverts automatically, caused massive traffic surges.   Innovations such as these mean that even if the time we spend digitally sharing, chatting, or watching doesn’t rise, the amount of data demanded does. Real and anticipated rises in demand tend to be framed as needs which must be met, despite the fact that there was no such need before. Connecting those without broadband is important, but most of the focus remains on speeding up everyone else. Look, for instance, at the talk about the need for “ultrafast” broadband of up to one gigabit per second, when industry regulator Ofcom’s own report shows that just 10 Mbit per second is what we might expect for a home. Thus far the debate over this ever-increasing connectivity has primarily been about who should pay for, or profit from, the traffic, and equality of (speedy) access. But the increasing emissions this generates are a global problem which demands a solution far beyond the ins and outs of broadband policy. We need more purposeful planning of capacity and speed, and their justifiable limits – which in the end, there surely must be. By contrast, the current strategy – predict-and-provide, followed by more predict-and-provide – can only make January 1, 2016 a chance to set another new record."
"
Share this...FacebookTwitter

War on windpark-blocking red kites?
Authorities are offering a €1000 reward for information leading to solving 11 cases of dead red kite protected birds. Nine of the deaths were due to a long-banned poison. 

Protected red kites being poisoned in north Germany. Image: Thomas Kraft (ThKraft) – Own work, CC BY-SA 2.5
The German Presseportal.de here writes that a total of eleven dead red kites have been reported to the LLUR (State Office for Agriculture, Environment and Rural Areas) since 2017 from the area south of Neumünster in northern Germany.
“Nine of these rare birds of prey died of a banned insect venom,” the Presseportal.de reported. Now the Hunting Association of Schleswig-Holstein e.V. (LJV) is offering a reward of 1,000 euros (1,100 US dollars) for information leading to solving the cases.
“Banned poison”
Authorities say “the 9 red kites died from an insect poison which had been banned for many years” and that four dead red kites with suspected poisoning have been reported since March alone.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Three birds were found close together by a local hunter in the community of Rendswühren in the Plön district. The police departments Segeberg and Kiel have taken over the investigation,” according to the Presseportal.de.
More than half of the worldwide population lives on the territory of the Federal Republic of Germany, where wind energy proponents have been actively lobbying to build wind parks. Red kites and other protected species have ling been obstacles against the construction of wind parks.
A few days ago we reported here how one red kite had been found shot dead in an area where a wind park could be built under the condition that “no protected species be proven to exist there”.
1000-euro reward
In 2008, authorities and wildlife clubs jointly signed the Kiel Declaration on the Protection of Birds of Prey, under which the costs of examining dead birds of prey can be borne by the state,” the Presseportal reports. “Anyone who finds a dead bird of prey where the circumstances of discovery indicate an illegal act is requested to contact the LLUR (+49) 4347-704-0 or the Lower Nature Conservation Authority of the respective district.”
The Presseportal.de also informs that persons may also contact the Landesjagdverband Schleswig-Holstein e.V., which is offering a reward of 1000 euros for information that would lead to solving the cases. Evidence from citizens are accepted under (+49) 4551/884-0 (Police Bad Segeberg UVS) or (+49) 431/160-1503 (Police Kiel UVS).
Your contact at the Landesjagdverband Schleswig-Holstein e.V (State hunting association of Schleswig-Holstein e.V.) is
located at Böhnhusener Weg 6, 24220 Flintbek. Telephone: (+49) 4347-9087-0.


		jQuery(document).ready(function(){
			jQuery('#dd_25ae9455058c240ae34b8e284e82d5e9').on('change', function() {
			  jQuery('#amount_25ae9455058c240ae34b8e284e82d5e9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000


Share this...FacebookTwitter "
"
Share this...FacebookTwitterPaleoclimate evidence shows there is little to no link between atmospheric CO2 concentration and relative sea level.
Venice, the treasure of Italy, is a city built on a mud swamp. Consequently, in the last 100 years it has sunk about 25 cm, or 2.5 millimeters per year (Munaretto et al., 2012).
The lagoon city had its worst flooding event ever recorded in 1966, when CO2 concentrations were still about 320 ppm.
Last week Venice flooded again, and, as expected, journalists blamed climate change and rising atmospheric CO2.
However, when we consider sea level rise rates for Venice averaged 2.6 mm/yr during 1872-1969, but then decelerated to 0.7 mm/yr for 1970-2000 (Munaretto et al., 2012), these trends are the opposite of what would be expected if CO2 emissions were driving sea level rise.

Image Source: Munaretto et al., 2012
Pisa’s history provides a sea level perspective
Italy’s Pisa is famous for its leaning tower.
The city was originally built on the coast of the sea about 13 centuries before the common era (~3300 years ago). At that time, sea levels were meters higher than they are now despite the low (~270 ppm) CO2 concentrations.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




During the Roman Warm Period, Pisa was still close enough to the sea coast (~4 km) to be a busy harbour (Huissen and de Graauw, 2019), accessible by canal. Dozens of ships dating to Roman times have been found buried beneath the city in recent decades.
In the last 2000 years, however, sea levels have retreated so thoroughly that Pisa now sits 9.7 km from the sea coast.

Image Source: Huissen and de Graauw, 2019
Italy’s sea level during the last interglacial
About 130,000 to 120,000 years ago, or during the last interglacial, CO2 levels peaked at 280 ppm.
Yet along the coasts of central Italy there are marine mollusck shells buried in silty sand and clay 12-35 m above today’s sea levels dating to this time period (Marra et al., 2019).

Image Source: Marra et al., 2019
In sum, the record of coastal changes throughout both ancient times and in the modern era do not support the conclusion CO2 levels are a driver of sea level change.
Share this...FacebookTwitter "
"Ecosystems will continue to collapse around the world unless humanity listens to the expertise of indigenous communities on how to live alongside nature, a prominent Amazon leader has warned. Tuntiak Katan of the Ecuadorian Shuar people, who is vice-president of the pan-Amazon organisation representing communities in the river basin, said governments were spending millions of dollars on environmental consultants while largely ignoring the land management skills of the planet’s indigenous people that could help combat the climate crisis and biodiversity loss.  Speaking to the Guardian from the Ecuadorian Amazon, Katan, who became the first indigenous representative at a UN climate action summit last year, said environmental “catastrophes” such as the fires that devastated the world’s largest rainforest in 2019 would continue unless the contributions and human rights of indigenous people were respected. Indigenous communities support around 80% of the planet’s biodiversity despite accounting for less than one twentieth of the human population, according to the World Bank. Katan’s warning came as a new study revealed that parts of the Amazon rainforest under the stewardship of indigenous peoples sequester carbon better than areas with little protection, leading to less deforestation and degradation. “We are the defenders of nature, of the life of the forests, of our territories,” said Katan, vice-president of Coordinator of the Indigenous Organizations of the Amazon River Basin (Coica). “The world is investing lots of money to implement public policy to combat climate change, help conservation and restoration. But these policies are made in offices by technical experts with little or no knowledge of the Earth.” Biodiversity loss was named as the third biggest risk to the world in terms of likelihood and severity this year by the World Economic Forum, ahead of terror attacks, infectious diseases and interstate conflict. Despite the concerns expressed by the global elite in Davos, there was no indigenous representation at last week’s forum in the Swiss ski resort, according to Katan. He said he would welcome the opportunity to attend next year’s forum to outline an indigenous economic model based on maintaining the health of the world’s soils, rivers and the forest. “If the proposals, knowledge and management practices of indigenous people are not listened to, there will be more big catastrophes. The issue of fires in the Amazon will continue, the degradation of forests and water will continue, deforestation will continue,” Katan added. This month, the UN unveiled the draft of a Paris-style agreement on nature calling for a commitment to protect at least 30% of the planet, dramatically reduce pollution and promote the participation and practices of indigenous people. Katan said: “We are well-coordinated with our brothers and sisters from Indonesia, the Congo, communities in the Arctic and from the Pacific. We’ve been discussing issues with our brothers and sisters from all parts of the world. “In Indonesia, for example, they also have a lot of knowledge about how to manage tropical forests. But the same story is being repeated here as in other parts of the world: the lack of recognition of their knowledge and the lack of respect for the human rights of indigenous populations.” Anti-indigenous sentiment is increasing in some parts of the planet. This month activists said they would sue Brazil’s far-right president Jair Bolsonaro for his latest racist comments in which he questioned the humanity of indigenous communities. In one of his weekly Facebook broadcasts, Bolsonaro declared: “Indians are undoubtedly changing … They are increasingly becoming human beings just like us.” The new study from the Woods Hole Research Center in Falmouth, Massachusetts, found that between 2003 and 2016, 90% of net emissions came from outside protected lands in the Amazon. Scientist and lead author Wayne Walker said: “Our work shows that forests under the stewardship of indigenous peoples and local communities continue to have better carbon outcomes than lands lacking protection, meaning that their role is critical and must be strengthened if Amazon basin countries are to succeed in maintaining this globally important resource, while also achieving their commitments under the Paris Climate Agreement.” The findings add weight to the recommendations of a report on land use and the climate crisis by the Intergovernmental Panel on Climate Change (IPCC), which found that areas held or managed by indigenous peoples had much less human impact on the environment. The report also highlighted the lack of consideration of indigenous views and knowledge in understanding large regions and ecosystems. Find more age of extinction coverage here, and follow biodiversity reporters Phoebe Weston and Patrick Greenfield on Twitter for all the latest news and features"
"The bus company Greyhound Australia has ruled out any extension of work on the controversial Adani coal project after a backlash from climate change campaigners. On Sunday the SchoolStrike4Climate group launched a campaign to boycott travel with the company until it publicly ruled out working on the mine.  Guardian Australia revealed last week that Greyhound had written to staff warning they could be caught “in the crossfire” of anti-Adani campaigners after the company took a three-month contract at the coal project, with an option to extend. The Indian-owned Adani mine and railway project is the first to begin work to extract the vast coal reserves of Queensland’s Galilee basin. Greyhound is providing transport to workers for the construction company BMD, which is building the railway to take the coal to Adani’s Abbot Point port. In a statement, Greyhound Australia said it had “received numerous messages, emails and phone calls from people expressing their thoughts both for and against the Carmichael Rail Network and Adani Carmichael project”. It said: “Following considered deliberation, and in the best interests of our staff, customers, and partners, Greyhound Australia has decided to not enter into a contractual agreement with BMD to service construction of the Carmichael Rail Network beyond our preliminary 31 March 2020 commitment.” The company declined to comment further. Within hours of Guardian Australia’s report on the contract, the conservation group Citizens of the Great Barrier Reef Foundation announced it had terminated a partnership with Greyhound. Greyhound’s chief executive, Alex de Waal, also resigned as chairman of the foundation. #STATEMENT from Greyhound Australia re. Carmichael Rail Network. pic.twitter.com/wW2WRjIRnI Varsha Yajman, a spokesperson for SchoolStrike4Climate, said the group had launched a campaign to boycott Greyhound buses on Sunday. “We thank Greyhound for not throwing young people under a bus by continuing to help Adani build their climate-wrecking coalmine,” she said. After a summer of “bushfires and heatwaves”, she said, Greyhound’s decision had given her hope. “It shows that we can push companies to be part of the solution to climate change and consider the impact of their actions. “Thanks to Greyhound listening to young people, students can now still go to school camps and on excursions by Greyhound buses.” The climate activist group Galilee Blockade told Guardian Australia it had now cancelled a planned protest targeting Greyhound Australia at Brisbane’s Roma Street travel interchange on Wednesday. Ben Pennings, spokesman for Galilee Blockade, said: “Greyhound took a stupid risk but quickly saw sense. Most Australians don’t want the Adani mine and every single company with a retail brand has listened to their customers and dumped Adani. “We’re already experiencing climate chaos and corporations simply have to take heed of an angry public increasingly willing to risk legal sanction for a liveable climate.” In January the German technology company Siemens said it would honour its Adani contract after it reviewed a reported $30m contract with the project. The engineering firm GHD ended a 10-year association with Adani in December after that company was also targeted by campaigners. Julien Vincent, the executive director of Market Forces, a group that tracks the relationships between corporations and fossil fuel industries, told Guardian Australia: “As Greyhound have discovered, and Siemens are in the middle of discovering, Adani’s climate-wrecking mine is a surefire way to destroy your reputation. “Now we need the likes of Siemens to realise what an appalling and ill-informed decision they have made to work on Carmichael and also pull the pin on their relationship with Adani.” A BMD spokesperson said in a statement: “Our contracts with clients and subcontractors are commercial in confidence and, as such, we do not disclose them publicly.”"
"

 _ **Editor’s note**_ _: In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security, _an edited volume of papers originally presented ata Cato conference the previous year. In each chapter, experts on international security assessed, and put in context, the supposed dangers to American security, from nuclear proliferation and a rising China, to terrorism and climate change._



 _As part of ourProject on Threat Inflation, Cato is republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. The first of these, by Brendan Rittenhouse Green, appeared ___here__ _last week._



 _Paul R. Pillar, a n_ _on‐​resident senior fellow at the Center for Security Studies of Georgetown University, and a non‐​resident fellow of the Quincy Institute for Responsible Statecraft, provides his thoughts below. His reflections on hischapter are informed by his 28‐​year career in the U.S. intelligence community, and his voluminous writing and research, including his most recent book, _Why America Misunderstands the World: National Experience and Roots of Misperception _(Columbia University Press, 2016), whichhe discussed at Cato in late 2016. _



—–



Prevailing American thinking about substate threats—and more specifically the thinking that shapes U.S. policy—exhibits at least as much of a disconnect between perception and reality as when _A Dangerous World?_ was published six years ago. The policy players and their principal bugbears have changed, but broader patterns my earlier essay identified persist. Perhaps the most glaring demonstration of this persistence is the continued presence of U.S. troops in Afghanistan—more than eighteen years after the original intervention, in what has become America’s longest war. A major impediment to withdrawing those troops continues to be the notion of Afghanistan as a unique “safe haven” for terrorists who, because of that haven, are supposedly more likely than they otherwise would be to inflict harm on Americans. The result is an interminable military expedition that in important respects is doing more harm than good.



The evolution of international terrorism during the last six years has challenged other common but flawed thought patterns about terrorism. The biggest development in that evolution has been the rise and, as a territorial entity, fall of the Islamic State or ISIS. This group’s split from, and competition with, Al Qaeda underscore the error of the earlier tendency to treat violent Sunni radicalism as monolithic, with the accompanying habit of applying the label “Al Qaeda” to the whole phenomenon. ISIS’s history also further refutes the thinking about terrorist safe havens. When ISIS had its mini‐​state in Iraq and Syria, it was focused primarily on running and maintaining that entity and less focused on international terrorism than it has been when lacking such a territory.



The Trump administration appears to have centered its threat perceptions more on states than on substate phenomena. Nonetheless, its foreign policies demonstrate some of the patterns identified in the earlier essay, including the tendency to divide the perceived world simplistically into competing camps of good guys and bad guys. A prime example is the administration’s idea of a NATO‐​like security alliance in the Middle East that would unite the United States, Israel, and some favored Arab states against a presumed bad guys’ bloc led by Iran. Nonstate actors such as Lebanese Hezbollah, the Houthi movement in Yemen, and some militias in Iraq are placed in the bad guys’ camp because of their association with Iran. The idea hasn’t gotten anywhere partly because it does not correspond to the more complicated lines of conflict and competition in the Middle East.



The administration’s obsession with Iran also illustrates a corollary to a pattern the earlier essay identified regarding perceptions of revolutionary violence and regime change. The pattern is the habitual assumption that regime change in any state the United States currently considers a friend or ally is assumed to be a threat to the United States. The corollary is that any regime change in a state the United States considers an adversary is assumed to be good. Thus, the Trump administration presses on with its “maximum pressure” campaign against Iran, which, in the absence of feasible demands or constructive diplomacy, can only be aimed at collapse of the current Iranian regime. It presses on—and in so doing raises the risk of escalation to a wider war—oblivious to the likelihood that a replacement regime, such as a Revolutionary Guard dictatorship, would be even worse than what Iran has now.



Now the United States and the world are confronting a nonstate threat, in the form of the COVID-19 pandemic, that is inflicting death and damage orders of magnitude beyond what was ever inflicted by the substate actors that for years have been the focus of American threat perceptions. Unlike with, say, terrorism, there certainly has been no problem of previously prevailing threat perceptions exceeding the reality. With terrorism, more sober voices have had to point out that in most years more Americans drown in bathtubs than fall victim to terrorism. Even after an outlier event such as 9/11, the casualties have been many times fewer than, say, the number of Americans who die in traffic accidents. But in only a couple of months, COVID-19 has left bathtub drownings in the dust and has killed more Americans than a year’s worth of traffic deaths.



One pattern applicable to other nonstate threats that does apply to the current pandemic is the tendency—a characteristically American tendency—to overstate the newness of a threat. The novel coronavirus may be novel in terms of virology, but infectious disease epidemics certainly are not. Plagues go back to ancient times. A failure to think in such terms is one factor underlying the inadequacy of preparations to deal with the likes of COVID-19.



Some of the U.S. responses to COVID-19 can be attributed to Trump’s habits, such as the flagellation of China as a way to deflect blame and attention away from the administration’s performance. But a more general American tendency is in play as well. COVID-19 is a nonstate threat, but it also is a nonhuman threat. As such, it does not conform well with the way Americans habitually think of their _bêtes noires_. Americans have long looked for monsters to destroy, but they expect the monster to have a face, in the form of a loathed leader, regime, or substate group. They have difficulty thinking ahead about meeting faceless threats such as a disease or a changing climate.



This is one reason to temper silver‐​lining hopes that the pandemic will get people and their government to think more about threats that are most likely to kill them and less about foreign regimes or groups that are unlikely to do so. Just look at how the Trump administration has continued with its maximum pressure campaign against Iran. As thoughtful and expert observers on both sides of the Atlantic have observed, any nation’s inability to get the virus under control impedes efforts to contain the pandemic globally and thus threatens other nations’ citizens. A prudent step, therefore, would be to ease the U.S. sanctions that are impairing Iran’s ability to contain COVID-19. At a time when tens of thousands of American deaths ought to make control of the pandemic an overriding priority, the Trump administration ignores this advice.



- Paul Pillar, Washington, DC
"
"

Last week, the _New York Times_ delivered the worrisome news that a team of scientists has concluded that maximum hurricane winds will increase 6 percent by the 2080’s, thanks to global warming. I was very upset to read that news, but not because I’m afraid my great‐​grandchildren will get blown away. My concern is what those scientists’ work says about the state of climate science.



The researchers reached their conclusions using a series of climate models called General Circulation Models. They assumed that the concentration of atmospheric carbon dioxide–the main global warming gas–will increase by 1 percent per year, compounded yearly. That would warm the ocean, which would create slightly stronger storms.



But there’s a problem: Any atmospheric scientist who is worth his or her salt knows that atmospheric carbon dioxide is not increasing at that rate and has not been doing so for decades. And that makes a real difference in the modeling results.



The increase has been about four‐​tenths of a percent per year, averaged over the last 30 years–not 1 percent. Charitably, throw in another tenth of a percent because of other human “greenhouse” emissions (though the two major ones, chlorofluorocarbons and methane, are declining or holding steady). That means that the researchers’ models are envisioning twice the actual increase in carbon dioxide as has been occurring for decades. 



The reason that carbon dioxide is growing so slowly is because the world is gradually becoming more energy‐​efficient as its people become more affluent. That results in both a reduction in per‐​capita emissions and a reduction in the number of “capits” that are born, as rich folks have fewer kids. Among big countries, this trend started in the United States. It is now spreading globally as the enriching world buys more‐​efficient cars and power plants.



This trend isn’t going to change anytime soon. That means the growth rate in carbon dioxide over the next few decades is likely to be the same as the rate for the last few. Using the more realistic rate delays the time that hurricane winds will increase by 6 percent from the 2080’s to the 2180’s–175 years from now.



And it’s pretty hard to speculate what impact humanity will have on nature over nearly two centuries in time. To understand that, let’s go backwards in time 175 years, to 1830, and think about the changes in energy and technology that have occurred since then.



The fact is that, just as folks in 1830 could not possibly imagine the many technological changes we have today (cars, planes, rockets, nuclear bombs, computers and Viagra come to mind), so can we have absolutely no vision of 2185. The only reasonable bet is that it will be dramatically different than today, and our fossil fuel‐​powered society will seem as remote in the future as one driven by horses and slavery seems remote to us today. So why would anyone make a prediction of what effects humanity will have on the environment some two centuries from now, based on what we’re doing today?



Or, in the case of the researchers’ exaggerated percentage increase in carbon dioxide, what we’re not doing today? That leads to an interesting question: Because carbon dioxide increases have been bouncing around four‐​tenths of a percent per year for three decades, why do climate modelers insist on using the wrong number? It seems peculiar that people who have the equivalent of doctorates in applied physics (which is what climate science is) would somehow be perfectly happy to do something they know is wrong.



I began asking that question at scientific meetings a decade ago. At that time, I asked Kevin Trenberth, a highly visible atmospheric scientist from the U.S. National Center for Atmospheric Research, who often testifies to Congress on climate issues. He told me it was done because it was “convention.” 



That answer doesn’t set well with me, because it’s awfully easy to program a computer to increase a variable by half a percent instead of 1 percent per year.



That leads to the final, nagging question. There are literally hundreds of scientific papers out there in which climate models use this wrong number. Each of those papers gets sent to three outside peer‐​reviewers. The fact that 1 percent continues to be used only means one thing: when it comes to global warming, hundreds of scientists must prefer convention to truth.



But why? Is it because, when the real numbers are put in, there’s no story for the _New York Times_ to report? 
"
"
Share this...FacebookTwitterTo all the morons out there who think we ought to defund the police and drag them through the mud. How stupid can you be?







<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->









Readers are welcome to add your own videos of police saving lives.
Share this...FacebookTwitter "
"If you ask the climate negotiators gathering in Bonn this week for their last get-together before the Paris conference in December who they are doing all this for, the reply would probably mention future generations. You can be as cynical as you like about what actually drives them but most people, including the negotiators themselves, no doubt think that the point of the exercise is to protect the interests of the planet as a whole. But is it? And whose planet are they trying to protect? Worryingly, the basic negotiating framework of the UN climate process increasingly favours the interests of people alive today over future generations, in ways that perhaps even the negotiators themselves do not understand. At the heart of the problem is the challenge of setting priorities between two very different kinds of climate pollutant. On the one hand, we have cumulative pollutants such as carbon dioxide, which build up in the climate system like heavy metals in the food chain. On the other, we have short-lived pollutants like methane and soot, which are “washed out” naturally within a few weeks or years. Action on short-lived pollutants has become very fashionable. It has an entire movement devoted to it, the Climate and Clear Air Coalition, enthusiastically supported by the United Nations Environment Programme. The reasons are clear: unlike carbon dioxide, many of these emissions can be reduced cheaply, with massive immediate benefits to human health and agriculture in precisely the countries where these emissions come from. Who could possibly object to measures that would save the lungs and lives of women in developing countries and at the same time could help improve our prospects of keeping global temperatures below two degrees? But there is a problem. As I explain in a new report released by the Oxford Martin School, as long as carbon dioxide emissions are still rising – and last year’s blip notwithstanding, they are – emissions of short-lived climate pollutants are almost entirely irrelevant to peak warming. The reason is simple: because carbon dioxide accumulates in the system, stabilising temperatures at any level means we have to get net global carbon dioxide emissions to zero. Even on the most heroic assumptions about future reductions, until carbon dioxide emissions are falling, and falling fast, net zero is still many decades off – by which time today’s methane and soot emissions will have long since washed out of the climate system. So why are countries so enthusiastic about them? Part of the reason is obscurely technical. For reasons long since forgotten, the whole UN process is based on the notion of “carbon dioxide equivalent” emissions, with equivalence measured in terms of a kind of “exchange rate” called the “100-year global warming potential”. Given the name, you might be forgiven for thinking this was something to do with global warming over 100 years, but it isn’t. It turns out GWP100 actually measures the relative impact of different emissions on warming rates over the next 30-40 years. And on this timescale, cutting short-lived climate pollutants could indeed have some impact.  Halving global methane emissions immediately, for example, could cut expected global temperatures by a couple of tenths of a degree by 2050 – which would be useful, but only comparable to natural fluctuations on these timescales. Halving global CO2 emissions would have a much bigger impact (and an immeasurably bigger impact after 2050), but would also be far, far more difficult and expensive. So much of the UN process is constructed around a measure of the impact of different emissions that explicitly focuses on the interests of the generation of decision-makers alive today. To be fair, some countries, like Brazil, have been calling for years for GWP100 to be replaced. But just replacing it with another exchange rate won’t help, because any rate that works on one timescale will fail on another. The solution is extremely simple. Countries need to acknowledge the need to get net carbon dioxide emissions to zero and limit the total amount we dump in the atmosphere in the meantime. So until CO2 emissions are falling, and falling fast enough that there is a realistic prospect of getting them to zero in the foreseeable future, we avoid the temptation of pretending that action on short-lived climate pollutants is helping to limit peak warming. This issue arouses strong passions: colleagues and I were recently accused by fellow scientists of putting “tens of millions of lives” at risk by calling for a delay in including short-lived climate pollutants in the UN climate process.  Let me be clear: I am all in favour of cutting soot emissions in developing countries. My first job, back in the 1980s, was developing clean and efficient wood-stoves in East Africa. I well recall the infernal conditions experienced by women cooking over open fires in 40 degree heat: we don’t need a global climate treaty to need a reason to do something about this kind of thing.  But until carbon dioxide emissions are falling, we shouldn’t pretend cutting soot emissions is helping to stabilise global temperatures, because it isn’t. So a crucial test of whether the UN climate process is actually working in the interests of future generations is whether the negotiators in Bonn, and in Paris in December, acknowledge the need for net zero carbon dioxide emissions. Watch this space."
"

This year’s installment of the U.N.‘s annual holiday party has come and gone from Cancun, with little to show for it except the massive carbon footprint of thousands of attendees — official delegates of member nations, plus representatives of Greenpeace, the World Wildlife Fund, and their ilk.



Technically, what we have just witnessed is the 16th Conference of the Parties to the United Nations’ Framework Convention on Climate Change, a.k.a. “COP16.” These conferences always take place at this time of year, and often in the tropics or in the Southern Hemisphere, where it is now summer. Next year’s confab will be in Durban, South Africa.



It’s really poor planning to throw these parties in December. The weather in the Northern Hemisphere lately has been downright uncooperative. Cancun witnessed a 100‐​year record low temperature one day as the delegates got down to “business” for two weeks, actually accomplishing little of substance. Last year, when COP15 was held in Copenhagen, the weather was miserable, with frequent snow and unseasonably cold temperatures. The quintessential images from that conference were of Barack Obama pronouncing the meeting a great success, and then rushing back to Washington, only to land in a blinding snowstorm.



Copenhagen was an abject failure. The great “success” was a requirement that each nation submit a schedule for reductions in carbon‐​dioxide emissions. But that requirement was waived by the executive secretary of the Framework Convention, Yvo de Boer. Then he quit.



He was replaced by Costa Rica’s Christiana Figueres, who welcomed the crowd to Cancun, invoking Ixchel, the Mayan goddess of weaving and creativity. Rather appropriate for an organization whose last climate report was made up out of whole cloth.



Predictably, Figueres pronounced the festivities a roaring success. “Cancun,” she said, “has done its job — the beacon of hope has been reignited.”



Sure — as in, Third World nations hope that the developed world’s governments will magically decide to donate them a trillion dollars over the next decade to “cope” with climate change.



Indeed, the delegates did agree to this “green fund,” but they failed to explain where the money will come from. All they agreed upon was how the nonexistent moneys are to be distributed.



Nor did they agree to anything that would commit any nation to reductions in carbon‐​dioxide emissions. However, you are free to submit a schedule that you don’t need to adhere to.



I’m sure that the U.N. can’t wait for the U.S. response on this one. It will appear in the form of some directive from the executive branch, specifically the EPA. But let us not forget — as I’m sure he has not — that the president’s party just got pummeled in the midterm elections. The more stringent the directive is, the more likely it is that Mr. Obama will leave Washington a private citizen in January 2013.



Even if he approves big (and impossible) cuts, it won’t do a measurable thing about global warming unless China and India agree to similar reductions. In fact, they have already informed the world — at both Copenhagen and Cancún — of their intent to raise emissions. China’s are on track to double in the next decade, and India’s look to increase threefold. Together, those two countries could easily be responsible for half of global emissions over the next two decades. The U.S. is currently responsible for about 20 percent of the total, and that percentage is steadily dropping. In 2009, China emitted a whopping 27 percent more than the United States.



The Cancun partiers couldn’t agree on any treaty or protocol to replace the failed Kyoto Protocol, which expires at the end of 2012. That one was supposed to “legally bind” the industrialized world to reduce its emissions to about 5 percent below 1990 levels by now. That language really worked, didn’t it? Emissions rose by more than they were supposed to fall. And even if all nations met their “obligations” under Kyoto, they were so insignificant that their effect could never be found by thermometers.



Kyoto was so unpopular that it was never brought up for ratification by the U.S. Senate. After the unceremonious death of cap‐​and‐​trade, are there going to be the 67 votes necessary to ratify something even more politically damaging?



It’s not easy to see the need for all these annual gatherings. In this fiscal climate, the developed world isn’t going to send a trillion bucks to Africa and a few tropical islands. Nor is any agreement going to be enforceable.



If the U.N. delegates were serious about global warming, at least they could meet by Skype and GoToMeeting instead of burning hundreds of thousands of gallons of Jet A in pursuit of perennial failure.
"
"

Here are some notes on the tax proposals in the new federal budget: (See Table S-6; All figures are 10‐​year totals) 
"
"It’s been a good year for apples. Across Europe the apple harvest is the biggest it has been for a decade. But the handful of apple types you see on supermarket shelves only tells part of the story. There are actually 7,500 varieties of eating apple grown all over the world, and growers and scientists are making efforts to conserve and extend this. Many people will have heard the story of Granny Smith apples, every one of which can reportedly be traced back to a single seedling plant found growing in Australia in 1868. Though not all plants have found the fame that this crisp green apple has, there are numerous varieties that are – like the Granny Smith once was – peculiar to a local region and rarely, if ever, grown elsewhere. The UK has more than 3,600 registered apple varieties recorded in the National Fruit Collection (NFC). Though it was once thought that 200 types were being grown in Wales, only about 50 (with investigations ongoing) are known to exist today. That this number is not lower is thanks to the pioneering endeavours of the likes of nurseryman Ian Sturrock, who began propogating the now world-famous Afal Enlli – along with other rare Welsh heritage apple trees – after its rediscovery in 1998. All Afal Enlli trees now sold come from one tree that may have been cultivated by generations of monks who lived on Bardsey Island at the tip of the Llyn Peninsula around 1,000 years ago.  Sometimes it is easy for experts to identify a type of apple as being an existing variety, or even a new variety never seen before, when they have a very distinct look. But that can’t always be done. So modern researchers have been turning to DNA profiling technology, similar to that used by forensic scientists. DNA profiling has become an essential tool for characterising the genetic diversity of apples and shaping collection strategies. The technique works by identifying small sections of DNA called simple sequence repeats (SSRs). These stretches of DNA do not code for genes, but the number of repeats within them varies between individuals. By analysing a number of SSRs, a unique “fingerprint” for each individual can be built up. These fingerprints are then compared to the profiles in the NFC database and either matched to an existing variety or, when there is no match, we can be sure it has never been characterised before and is possibly a newly discovered or rediscovered variety.  This is precisely what the Welsh Perry and Cider Society’s Jayne Hunt has been doing, as part of the first concerted effort to identify and conserve old unknown varieties of apple and pear trees growing in Wales. Hunt’s team extracted DNA from hundreds of apple leaves collected from derelict orchards throughout Wales, created a genetic profile for each tree, and compared them with the NFC database.  Though the work did identify previously declared Welsh Heritage varieties as having pre-existing duplicates in the NFC database (enabling current collections to be rationalised), the results have overall been fascinating and many unique trees have been found. Of course, there is a proviso. Just because a variety is declared unique, it doesn’t make it useful and worth conserving. The nature of breeding means that every seedling grown from a pip will be unique, combining characteristics from the mother tree and its pollinator. Often the seedlings are but poor reflections of the parent trees. This is why another part of Hunt’s project is really important: histories and anecdotes from farmers, growers and members of the public have been documented, to share their intimate knowledge of the trees and their uses. With these verbal historical records and further investigation of the variety’s properties it is possible to determine if the newly discovered or rediscovered apple variety is a real gem worth conserving. At Aberystwyth University, we are currently planning extension projects to take this work further. We have been sampling trees from derelict orchards on university land that are at least 60 years old. Our own DNA profiling has found that, by and large, the trees are existing varieties popular at the time of planting and were most likely obtained from the catalogues of English nurseries – varieties such as Bramley’s Seedling, Cox’s Orange Pippin and Blenheim Orange and then the slightly more unusual Allington Pippin and Lady Sudeley. These were likely to have been planted simply because they were popular at the time and not necessarily because they were suited to the local weather conditions.  But two trees have been found to be unique and we are currently evaluating their properties. Whether their existence lies in their specific adaptation to the local climate and aspect, or is more due to random chance, we don’t really know – but they certainly extend the genetic diversity of our global resource. Never has it been more important to preserve our crop genetic diversity, not only for our increased pleasure but, more pressingly, to provide a reliable and economically sustainable source of food in the face of climate change.  And maybe one of the newly identified Welsh varieties will become a global success like the Granny Smith."
"
Russ Steele is out on vacation and doing several surveys while traveling. This one below is from St. George, UT. Here we see an MMTS measuring the temperature near the surface of an elevated parking lot. The effect of the asphalt and vehicles that park near it, engine forward, probably dwarfs the effect of the nearby a/c unit. The shading may help daytime temps some, but the asphalt likely biases Tmin the most. The complete photo survey is available on surfacestations.org




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4b945b5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFor those wondering how Greta Thunberg managed to get so off the rails  look no further than her climate advisors. They are among the most alarmist worldwide.
According to the online, German-language Merkur.de here, citing Swedish daily Expressen, 16-year old climate activist is advised by Professor Stefan Rahmstorf, among others, of the Potsdam Institute for Climate Impact Research (PIK) in Germany.
Wild science claims

Professor Rahmstorf and the Potsdam Institute are well known among the climate science community for their highly alarmist projections and off-the-mark science. For example, the PIK’s former director, Prof. Hans-Joachim Schellnhuber, once preposterously claimed the Himalayan glaciers would disappear by 2030 and the world would explode with 9 billion people on it.
Alarmism as a business model
According to the Merkur, the PIK has “created a business model with climate change”, adding: “It advises state institutions and municipalities on how they can protect themselves against the severe climate changes predicted by PIK.”

The Merkur reports how Greta and Professor Rahmstorf “have met on several occasions” and how the 16-year old “calls the professor for advice.” Rahmstorf praises the teenager, saying “she knows a lot about climate science” and argues that politicians too should adopt Greta’s doomsday visions.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Source: Twitter
“Holy anger” and “religious exaggeration” 
But now some are speaking up against the excessive alarmism.
Not only are some media questioning Rahmstorf’s alarmist climate science, though it makes for good headlines, but so are other experts, such as renowned geologist Stefan Kröpelin, or Prof. Fritz Vahrenholt, or veteran meteorologist Jörg Kachelmann.
Swiss meteorologist Kachelmann calls Rahmstorf a “climate-Ratzinger”, alluding to former Pope Benedict XVI, earlier known as Cardinal Joseph Ratzinger. Kachelmann, a warmist, has become turned off by the fiery Potsdam professor’s “religious exaggeration” and “holy anger” in the climate debate.
“Climate howler”
The two heavyweights recently even argued on Twitter about whether a piece of broken glass could trigger a forest fire, as Rahmstorf claims, or not. The sharp-witted, media-savvy Kachelmann recently characterized claims made by Rahmstorf as “potsdaft” – an allusion to his place of work –  and labelled him a Klimaheuler, a ‘climate howler’.
Surrounded by alarmists, rational voices not welcome

Greta Thunberg’s other advisors include Swedish Professor Johan Rockström (53) and British Professor Kevin Anderson (57), both devout alarmists. Non-alarmist climate experts and rational voices are not welcome by Thunberg and her advisors. Why ruin a good business model?

Share this...FacebookTwitter "
"
Share this...FacebookTwitterLarge regions of the globe have  been cooling or not warming in recent decades according to several new scientific papers.
A new paper shows the West Antarctic Ice Sheet (WAIS), sea surface temperatures near southern Chile, and the entire region between 50-70°S have cooled or not warmed since the early 1980s (Collins et al., 2019).
The region was more than 2°C warmer 1000 years ago and today’s temps (12.1°C) are the coldest of the last 2300 years (Collins et al., 2019).
Other new papers indicate the North Atlantic sea surface temperatures (SSTs) between southeast Greenland and Denmark have cooled at a rate of 0.78°C per decade since 2004 (Fröb et al., 2019).
The North Atlantic region (Labrador Sea to Icelandic Basin) hasn’t warmed overall (net) since the 1950s (Buckley et al., 2019), which includes no net warming of winter temperatures in Northern Europe and North America since the 1980s (Chen and Luo, 2019, Gan et al., 2019).
Should it be called “global warming” if it isn’t actually global?

Image Source: Collins et al., 2019


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Collins et al., 2019

Image Source: Collins et al., 2019

Image Source: Collins et al., 2019

Image Source: Fröb et al., 2019

Image Source: Buckley et al., 2019

Image Source: Chen and Luo, 2019

Image Source: Gan et al., 2019
Share this...FacebookTwitter "
nan
nan
"Donald Trump’s treasury secretary has clashed with Greta Thunberg after responding to the activist’s call for immediate fossil fuel divestment by telling the 17-year-old to go to college and study economics. In an attempt to slap down the climate emergency movement, Steven Mnuchin pretended not to know who Thunberg was, before dismissing her concerns as ill-informed. Asked whether calls for public and private-sector divestment from fossil fuel companies would threaten US growth, Mnuchin jibed: “Is she the chief economist? Who is she, I’m confused” – before clarifying that he was joking. “After she goes and studies economics in college she can come back and explain that to us,” Mnuchin added, at a press conference at the World Economic Forum in Davos. Davos is a Swiss ski resort now more famous for hosting the annual four-day conference for the World Economic Forum. For participants it is a festival of networking. Getting an invitation is a sign you have made it – and the elaborate system of badges reveals your place in the Davos hierarchy. The meeting is sponsored by a huge number of international banks and corporations. For critics, “Davos man” is shorthand for the globe-trotting elite, disconnected from their home countries after spending too much time in the club-class lounge. Others just wonder if it is all a big waste of time.  The 2020 meeting is being advertised as focusing on seven themes: Fairer economies, better business, healthy futures, future of work, tech for good, beyond geopolitics and how to save the planet. Young climate activists and school strikers from around the world will be present at the event to put pressure on world leaders over that last theme.  Thunberg, 17, responded by tweeting a graph from a UN report showing how the world’s remaining carbon budget will be used up by 2027 unless global emissions are curbed. “My gap year ends in August, but it doesn’t take a college degree in economics to realise that our remaining 1,5° carbon budget and ongoing fossil fuel subsidies and investments don’t add up,” she pointed out. My gap year ends in August, but it doesn’t take a college degree in economics to realise that our remaining 1,5° carbon budget and ongoing fossil fuel subsidies and investments don’t add up. 1/3 pic.twitter.com/1virpuOyYG Mnuchin’s comments expose the huge gulf that still exists between climate activists and the White House. Pressed on the climate emergency, Mnuchin insisted that environmental issues are “clearly complicated”. He said: “When I was allowed to drive I had a Tesla. I drove in California. I liked it.  “But nobody focuses on how that electricity is made and what happens to the storage and the environmental issues on all these batteries.” He also claimed the US was showing leadership in tackling emissions – but through its private sector rather than government. President Trump believes in clean air and water, and a clean environment, Mnuchin insisted, but also believes that more attention should be paid to the environmental damage caused by China and India. People who call for divestment should remember there are “significant economic issues, issues with jobs”, he continued. “Many economies are transitioning to more efficient and cleaner energy. That doesn’t have to be all renewables.” Angela Merkel, though, spoke warmly about the work of the new generation of climate activists. “The impatience of our young people is something that we should tap,” the German chancellor said. In a special address to the WEF, Merkel called for more international cooperation to tackle climate change. “I am totally convinced that the price of inaction will be far higher than the price of action,” she declared. Thunberg has used this week’s gathering in Davos to push for radical change on the climate emergency. She called for an immediate exit from fossil fuel investment, an end to subsidies for the industry and a halt to investment in fossil fuel exploration and extraction by companies, banks, institutions and governments. “You might think we’re naive but if you won’t do it, you must explain to your children why you’ve given up on the Paris agreement goals and knowingly created a climate crisis,” she told delegates on Tuesday. Mnuchin’s comments clearly show the White House has yet to heed the call. The US labour secretary, Eugene Scalia, called for a balanced approach. The energy sector has been an important source of jobs and fully divesting from fossil fuels would also harm US pensioners, he said. On Tuesday, Trump told Davos that delegates should be optimistic. “To embrace the possibilities of tomorrow, we must reject the perennial prophets of doom and their predictions of the apocalypse. They are the heirs of yesterday’s foolish fortune tellers,” he said."
"Ameth Diagne points to a single tree submerged in the ocean. It is barely visible from the patch of land where he is standing, 50 metres away. The few branches emerging from the water mark the place where he proposed to his wife 35 years earlier. It used to be the town square of Doun Baba Dieye, a vibrant fishing community on the outskirts of Saint-Louis in northern Senegal. The village has been wiped off the map, with only the tree and crumbling walls of an abandoned school remaining as testament to its existence. Everything else is 1.5 metres under water.  “This was home. I was born here. Everything which was important to me happened here,” says Ameth, the former village chief. Doun Baba Dieye is in the southern part of Langue de Barbarie, a thin, sandy strip of land protecting Saint-Louis, former colonial capital of Senegal, from the ocean. Saint-Louis, a city of 230,000 and a Unesco world heritage site, is nestled between the mouth of the Senegal river and the Atlantic. The French chose Saint-Louis as the capital because of its strategic location, which allowed the city to flourish in colonial times. But today the “Venice of Africa” is being eaten up by the rising waters. Crossing the Faidherbe bridge, which connects the colourful city centre to the mainland, it seems as if you can almost touch the water. This state of a permanent flood alert has become the city’s new normal. In Saint-Louis, the consequences of climate crisis are tangible: thousands of people uprooted; houses destroyed; hundreds of children attending classes in the evening instead of in the morning because their school has been swept into the ocean. The World Bank, which recently allotted €24m (£20m) to combat the effects of climate change in Saint-Louis, estimates that 10,000 people in the city are either already displaced or live within 20 metres of the waterline, the high-risk zone. And this is just the beginning. According to a study commissioned by the Senegalese government, 80% of Saint-Louis territory will be at risk of flooding by 2080, and 150,000 people will have to relocate. Most of west Africa’s coastal cities, home to 105 million people, face a similar threat. Mangone Diagné, from the regional division of Senegal’s environment ministry, puts it bluntly: “Saint-Louis is surrounded by water and is incredibly vulnerable to climate change. But the damage was caused both by nature and by men.” He is referring to an engineering mistake, which contributed to the deterioration of the Langue de Barbarie. In 2003, heavy rainfall caused the Senegal river to rise rapidly, putting Saint-Louis at risk of flooding. As a quick fix, local government dug a four-metre-wide breach, or canal, cutting through the Langue de Barbarie. The effect has been the opposite of the one intended. Although at first the river level dropped, the breach quickly started to expand. It is now 6km wide and has cut off part of the peninsula, turning it into an island – and flooding Doun Baba Dieye. It has also upset the delicate balance of the local ecosystem. The canal brought seawater into the river, increasing its salinity level. This has affected the population of rare bird species and river fish – forcing fishermen to venture into Mauritanian waters, which is dangerous and illegal – as well as wiping out the coconut trees and mangroves that once protected the shores. Local crops, already destabilised by irregular rainy seasons and sand storms, were further damaged. Ameth, along with more than 800 former inhabitants of Doun Baba Dieye, are among the victims forced to move away from the peninsula without any support from the state. “We are fishermen, we know the ocean, so we left at the good moment – and nobody got hurt,” says Ameth, before correcting himself: “We were fishermen.” Since his community was dispersed, Ameth’s fishing boat, a pirogue, remains largely unused: “We live too far from the ocean now. I was not only deprived of my home, but also of my livelihood.” Inhabitants of the Langue de Barbarie are from the Lebou ethnic group who have been fishermen for centuries. Boys learn the necessary skills from an early age. But Ameth wants his sons to have an education: three of them are in primary school and two are at university. “For us Lebou, the ocean is our lifeblood. But it is hard to make a living as a fisherman today. This is why so many young men from our community are migrating. I want my sons to have a choice.” North of the Langue de Barbarie peninsula, the fishing district of Guet N’dar was hit by a 4-metre wave one night in September 2017. The ensuing flood affected more than 100 houses, a school, a mosque, and part of the cemetery. The homeless were put in a makeshift camp next to the airport. The ocean eventually receded, leaving a scene of devastation. Against the backdrop of crumbling walls and half-wrecked buildings, French president Emmanuel Macron visited in February 2018 and pledged an extra €15m to build a sea wall to protect the remaining infrastructure. But by April of that year, a part of the sea wall had collapsed, letting the ocean take over. Ameth is counting neither on the World Bank, nor on Macron. Thanks to financing from the UN, he returns once a year to the place where his old village stood, and plants trees. He hopes that mangroves and filaos, an exotic species of pine, can stop the destruction of the shoreline. “Although we had to move physically, my mind and spirit stayed here. And I hope one day I can move back,” he says."
"
Share this...FacebookTwitterThough the media like to tell their audience that man-made climate change is leading to more extreme weather, the data don’t support it. In fact, one could easily argue that Japan’s climate is more agreeable today.
By Kirye in Tokyo
and Pierre Gosselin
No trend in long-term annual precipitation
Over the past 100 years, for example, annual precipitation has not trended in an particular direction over the long term, showing rather some cyclical attributes:

Data source: Japan Meteorological Agency (JMA). 
If anything, precipitation has been rather steady for the better part of the past 2 decades, and even resembles what was observed about 60 years ago, in the 1950s.
Note how the extremes in precipitation occurred in the 1970s and 1980s when most of the climate talk was about global cooling. But overall, there’s been no trend change in precipitation in Japan.
Typhoons trending downward modestly!
Typhoons forming, and those striking Japan, also show no worsening, as is otherwise often claimed by climate alarmists. What follows is a plot of typhoon landfalls for Japan and typhoons formed, since 1951:
Data: JMA here and here. 
The data suggest the number of typhoons forming and those striking Japan have declined modestly over the past 70 years, which is in line with the trends for global tropical storms. So there’s nothing alarming happening.
Japan sea level rise “no long-term trend”



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Also nothing dramatic is happening with regards to Japan and sea level rise. This is the official conclusion of the JMA! Their site states:
A trend of sea level rise has been observed in Japanese coastal areas since the 1980s, but no long-term trend of rise is seen for the period from 1906 to 2018. Variations with 10- to 20-year periods (near-10-year variations) are seen for the period from 1906 to 2018.




 Time-series representation of annual mean sea level values (1906 – 2018) 
 The 1981 – 2010 average is used as the normal. 
Annual sea level anomaly time series (comma-separated value file: 3 KB)
The graph indicates annual mean sea level anomalies for each year averaged among the four tide gauge stations shown in the map on the left below for the period from 1906 to 1959, and among the four regions shown in the map on the right below for the period from 1960 onward. The solid blue line represents the five-year running mean of annual sea level anomalies averaged among the four stations, while the solid red line represents the corresponding value for the four regions. The dashed blue line represents the value at the four stations for the same period shown by the solid red line (from 1960 onward) for reference.”




Japan annual temps steady 80 years, before peculiar 1990 jump
Finally, we look at Japan’s mean annual temperature trend over the past 100 years. Though we see an overall rise – it had remained more or less steady for some 80 years, from 1918 to 1990. But suddenly in 1990, the mean temperature jumped to new plateau.

Data source: JMA. 
Perhaps this may have in part been due to a change over to electronic measurement systems and urban heat island effect, along with station siting. One thing can be ruled out: Any CO2 effect would not act so instantaneously.
Japan’s climate has not really worsened
Overall in terms of weather extremes, cold and storms, things in Japan have not gotten worse. In fact one could easily argue things have tamed just a bit.
Share this...FacebookTwitter "
"

Sen. Dianne Feinstein has introduced the Bot Disclosure and Accountability Act, a proposal to regulate social media bots in a roundabout fashion. The bill has several shortcomings.   
  
  
Automation of social media use exists on a continuum, from simple software that allows users to schedule posts throughout the day, to programs that scrape and share information about concert ticket availability, or automatically respond to climate change skeptics. Bots may provide useful services, or flood popular topics with nonsense statements in an effort to derail debate. They often behave differently across different social media platforms; Reddit bots serve different functions than Twitter bots.   
  
  
What level of automation renders a social media account a bot? Sen. Feinstein isn’t sure, so she’s relinquishing that responsibility to the Federal Trade Commission:   




The term ‘‘automated software program or process intended to impersonate or replicate human activity online’’ has the meaning given the term by the [Federal Trade] Commission



If Congress wants to attempt to regulate Americans’ use of social media management software, they should do so themselves. Instead, they would hand the hard and controversial work of defining a bot to the FTC, dodging democratic accountability in the process. Moreover, the bill demands that the FTC define bots “broadly enough so that the definition is not limited to current technology”, virtually guaranteeing initial overbreadth.   
  
  
While the responsibility of defining bots is improperly passed to the FTC, the enforcement of Feinstein’s proposed bot disclosure regulations is accomplished through a further, even less desirable delegation. The Bot Disclosure and Accountability Act compels social media firms to adopt policies requiring the operators of automated accounts to “provide clear and conspicuous notice of the automated program.” Platforms would need to continually “identify, assess, and verify whether the activity of any user of the social media website is conducted by an automated software program”, and “remove posts, images, or any other online activity” of users that fail to disclose their use of automated account management software. Failure to reasonably follow this rubric is to be considered an unfair or deceptive trade practice.   
  
  
This grossly infringes on the ability of private firms, from social media giants like Facebook to local newspapers that solicit readers’ comments, to manage their digital real‐​estate as they see fit, while tipping the balance of private content moderation against free expression. Social media firms already work to limit the malicious use of bots on their platforms, but no method of bot‐​identification is foolproof. If failure to flag or remove automated accounts is met with FTC censure, social media firms will be artificially incentivized to remove more than necessary.   
  
  
The bill also separately, and more stringently, regulates automation in social media use by political campaigns, PACs, and labor unions. No candidate or political party may make any use of bots, however the FTC defines the term, while political action committees and labor unions are prohibited from using or purchasing automated posting software to disseminate messages advocating for the election of any specific candidate. It is as if Congress banned parties and groups from using megaphones at rallies. Would that prohibition reduce political speech? No doubt it would. How then can the prohibitions in this bill comport with the constitutional demand to make no law abridging the freedom of speech? They cannot.   
  
  
Feinstein’s bill attempts to automate the process of regulating social media bots. In doing so, it dodges the difficult questions that attend regulation, like what, exactly, should be regulated, and foists the burden of enforcement on a collection of private firms ill‐​equipped to integrate congressional mandates into their content moderation processes. Automation may provide for the efficient delivery of many services, but regulation is not among them. Most importantly, the bill does not simply limit spending on bots. It _prohibits_ political (and only political) speech by banning the use of an instrument for speaking to the public. Online bots may worry Americans, but this blanket prohibition of speech should worry us more.
"
"Roland Barthes proclaimed the death of the author in 1967, arguing that once a text is produced it is an independent entity to be interpreted and understood by the audience without the author’s intentions, idiosyncrasies and personal history getting in the way. The Journal of Controversial Ideas is Barthes’ idea made manifest – it proposes to allow academics to publish papers on controversial topics under a pseudonym. The hope is that this will allow researchers to write freely on controversial topics without the danger of social disapproval or threats. Thus the journal removes the author’s motivations, conflicts of interests and worldview from the presentation of a potentially controversial idea. This proposal heralds the death of the academic author – and, unlike Barthes, we think believe this is a bad thing. First, we need to distinguish between anonymous and pseudonymous authorship: a paper is anonymous when it does not list a name, and it is pseudonymous when it lists a name which is not the author’s given name. Both practices have long histories in academic research. The Philosophical Transactions of the Royal Society – the world’s longest-running scientific journal – was initially published without the names of researchers who carried out the experiments. It was only after the development of the legal institution of authorship in the 17th century that named authors become the norm. The Victorian bestseller, Vestiges of the Natural History of Creation, which put forward an early version of evolutionary theory, was initially published anonymously. Its readers had to wait 40 years and 12 editions to discover that it was written by Robert Chambers. Malthus’s An Essay on the Principle of Population, which develops his theory of population growth, was also first published anonymously. More recently, there are some notable examples of pseudonymous authorship. Starting in 1939, a rotating group of mathematicians have used the collective pseudonym “Nicolas Bourbaki” to publish the ongoing Elements of Mathematics series, which has 11 volumes published over 70 years. The Polymath project, which crowdsources solutions to mathematical problems, has also published some of its papers pseudonymously, under “D.H.J Polymath” – the initials standing for the density Hales-Jewett theorem, the first problem the project worked on. Other examples include the philosopher David Lewis, who published a response to one of his own papers under the nom de plume Bruce Le Catt. Indeed, just this year, three academics in the US had seven papers, written under pseudonyms or borrowed identities, accepted at various humanities journals as part of an elaborate hoax. In many cases, the identity of these writers was an open secret at publication – everyone knows who is behind the Polymath project, for example. Even when the intention is to conceal, the writer’s identity is typically revealed. How best to assign authorship is currently an open question. Issues such as ghost authorship, hyperauthorship, and hyper-productive researchers all challenge traditional notions of authorship – and there are numerous revisionary proposals in the air. The Journal of Controversial Ideas should be understood as part of a conversation about what authorial practices are best suited to the aims of academic inquiry. To consider the viability of pseudonymous authorship, we need to think about what the point of having authors is. One important reason for a researcher to attach their name to a paper is to enable them to claim the credit for the paper. Although details are sketchy, the editors of the proposed journal seem to have planned a system that would allow writers to claim ownership of papers for the purposes of hiring and promotions.  Likewise, writers may also choose to conceal their papers for the purposes of hiring and promotion, if not claiming ownership would be advantageous to their careers. For the reader of a paper, attaching authors to papers is important to help them decide how seriously to take the results. Here the difference between anonymous and pseudonymous authorship becomes important: if an author uses the same pseudonym over a period of time, the academic community can begin to get a sense of how good their work is (consider the Bourbaki pseudonym, which has been in use long enough to get a track-record), but if a publication is anonymous, the audience must rely solely on the credibility of the publishing journal and its editors. But the most important function of having authors is to facilitate responsible publishing. If the 1998 Lancet paper linking the MMR vaccine to autism had not listed Andrew Wakefield as its lead author, it would not have been possible to hold him to account for producing fraudulent work, or for contributing to a dangerous anti-vaccination narrative. Authorial responsibility has both an intellectual and a moral flavour: we want to hold people responsible both for producing shoddy research, and for the moral consequences of their publications. Holding authors responsible functions as part of academic quality control. If researchers know that producing bad work has social and career consequences, this incentivises more careful and diligent work. Similarly, holding authors morally responsible motivates a healthy degree of caution on topics which might cause real harm. We worry that pseudonymous authorship is likely to lead to a problematic asymmetry between praise and blame. If the The Journal of Controversial Ideas can reliably keep researchers’ identities secret – and we have our doubts, given the history of pseudonymous authorship – researchers will be able to publish papers, claim them if they get a positive reaction, and disown them if they do not.  Perhaps the journal’s interest in establishing a good academic reputation will lead them to put in place reviewing procedures to ensure that only high-quality work is published, but their interest in controversy suggests that the journal may be much more lax when it comes to moral responsibility. We worry that The Journal of Controversial Ideas will respond to moral criticism by pointing out that their mission is precisely to publish controversial and dangerous ideas. Defenders of The Journal of Controversial Ideas see it as a forum for true academic freedom. While academic freedom is important, it is not an unlimited right. Freedom without responsibility is recklessness. It is a lack of regard for the danger or consequences of one’s ideas. True academic freedom does not mean that writers get to choose when to avoid controversy. The pseudonymous authorship proposal allows authors to manipulate the credit and blame systems of the academy in the name of academic freedom. When it is working well, academic inquiry is a conversation. Researchers make claims and counterclaims, exchange reasons, and work together to open up new fields of inquiry. A conversation needs speakers: we need to keep track of who is talking, what they have said before, and who they are talking to. Pseudonymous authorship is an opt-out from the conversation, and the academic community will be worse off if its members no longer want to engage in intellectual conversation."
"I love Australia. It’s not a thing you hear too often from progressives. Mostly this is because we don’t go in for the pathetic jingo-nationalist, quasi-militaristic “love it or leave it”-style patriotism that John Howard attempted to link with a love of country.  But I do love Australia. I get an absurd amount of irrational pride when I hear of Australians doing well. When I read stories that Indigenous rock art might be among the oldest in the world I get excited and think, yeah suck it, caves of Cantabria! I can still remember where I was when John Aloisi scored the winning penalty against Uruguay (jumping up in my home in Cairns and cutting my hand on the overhead fan), and like all sensible Australians I let out a deep groan whenever I hear someone start yet again an “Aussie, Aussie, Aussie” chant at the tennis. But my love has nothing to do with Australian Day and no, this is not an article about Australia Day. I mean, of course we should change the date. As one who grew up in country South Australia from German ancestors, the English landing in Sydney has never resonated for me as anything more than New South Wales Proclamation Day. Thanks for the holiday and the cricket at Adelaide Oval, but otherwise ... Keep the public holiday – make it the last Monday in January – it is nicely timed to signal an end to the summer holidays. Call it “Summer Day” or some such and then find another day to actually celebrate the nation. Better still, become a republic and make it that day. But I digress. This is not about Australia Day, but climate change. Because I love Australia, and the real question is why don’t conservatives who refuse to do more on climate change love Australia? Because climate change will destroy much of what we love about this country of ours. Much of what makes Australia unique and beloved by those of us lucky enough to live here is linked to the extremes of our land and climate. In my lifetime I have mostly lived in country areas – in South Australia on the Murray River, in Redlynch just north of Cairns, and now in the northern suburbs of the bush capital that is Canberra. So I love our biggest river and the farming areas of wheat, sheep and dairy around my home town to the grapes and fruit in the Riverland where during uni I picked fruit in the summer holidays. Then there are the tropics with Mossman Gorge, the glorious drive from Cairns to Port Douglas, the Great Barrier Reef and the late afternoon rains. And yes, I love the surrounds of Canberra – where I can live near a national park with kangaroos and echidnas and be able to see the snow caps on the Brindabellas in winter and fiery red of the trees in autumn and the blossoms in spring. As anyone who has spent any time overseas knows, there is something about the sky in Australia that is different – that shade of blue so perfectly captured by Tom Roberts in his painting A Break Away!. That gorgeous clear, crisp blue. I must admit I don’t love Dorothea Mackellar’s My Country. I find the poem rather maudlin, but perhaps I am biased because I am sick of hearing climate change-denying politicians recite it as though it is evidence that climate change has not occurred. Because here’s the thing: when she published that poem in 1908, the average annual temperature in Australia was about 2C lower than it is now. And those conservatives who recite the line about “a sunburnt country” ignore that climate change is going to wreak havoc with everything we love – that tenuous balance of droughts and flooding rains, the ability of agriculture to exist on “thirsty paddocks”, our rivers, our wildlife where “orchids deck the treetops”, even the crisp air and “pitiless blue sky”. This summer has shown how precarious our Australian lifestyle is – the bushfires that have not stopped since September; the mix of fires, smoke, dust, hail (and that is just in Canberra in the past fortnight). We are a nation on the extremities, where climate change will affect and strip away what we love much sooner than will occur in Europe and North America. No patriotic Australian can be anything but angry to read stories of a billion or more animals killed in the fires – especially when you realise the koalas on Kangaroo Island are chlamydia-free and are essentially the best protection against their extinction. Our wildlife is so exceptional and precious that the upset in balance that comes from climate change will render some habitats unliveable. To read that the platypus is facing extinction due to human activity exacerbated by climate change should have every patriotic Australian filled with rage. Conservative patriots love to talk up Australia “punching above its weight” on things such as sport or business or war, but they turn to self-hating cowards when it comes to climate change. Yes, Australia “only” accounts for around 1.3% of emissions (of course well above what you would expect given our population), but given the fragility of our ecosystem, any political leaders who profess to love Australia should be energising our diplomatic networks and using every economic and political lever we have to cajole, convince and encourage other nations to act on climate change. We should do this even if it is out of purely selfish reasons of loving our country and wanting it to remain in the same state that has caused that love. If you love Australia, climate change should scare the hell out of you because the reef, our rivers, our wildlife, our fresh air, even, as we have seen since December, our relaxed summer holidays are going to be stripped away from us. Our government has more reason than most others outside of the Pacific Islands to be demanding global action on climate change. Given our wealth we should be leading the way – leading by example rather than leading to ruin as our current government has been at the most recent climate change conferences. What we love about Australia will be taken by climate change well before other nations who emit much more greenhouse gas will feel great changes. And that should enrage us and our representatives, and it should drive their actions. I love Australia and so I want action on climate change. And if you love Australia, so should you. • Greg Jericho writes on economics for Guardian Australia"
"
Share this...FacebookTwitterGreenland’s largest glacier (Jakobshavn) has quite abruptly thickened since 2016. The thickening has been so profound the ice elevations are nearly back to 2010-2011 levels. The nearby ocean has cooled ~1.5°C – a return to 1980s-era temperatures.
The world’s glaciers have not been following along with the CO2-driven catastrophic melting narrative.
Alaska

For example, in a study of 50 Alaskan glaciers for the warming period between 1972-2012, researchers (McNabb and Hock, 2014) found there was

“…no corresponding change in the number of glaciers retreating nor do we see corresponding acceleration of retreat rates. To the contrary, many glaciers in the region have advanced…”

Image Source: McNabb and Hock, 2014
Antarctica
In the Southern Hemisphere, an accumulating collection of (29) referenced studies (Lüning et al.,2019) indicate that not only has the Southern Ocean, Antarctic Peninsula, West Antarctica, and East Antarctica been cooling or not warming in recent decades, but many regional glaciers have begun advancing again.

Image Source: Lüning et al.,2019
Greenland
Greenland’s ice sheet mass losses have significantly decelerated since 2013 – a reversal from the rapid retreat from the 1990s to 2012 driven by cloud forcing and the NAO (Ruan et al., 2019).
The 47 largest Greenland glaciers also experienced a “relatively stable” period of rather insignificant retreat from 2013 to 2018 (Andersen et al., 2019).


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Only 21 of the 47 Greenland glaciers retreated in 2018, 12 advanced, and the other 14 showed no trends in either direction (Polar Portal, 2019).
Greenland’s largest glacier, Jakobshavn, earned headlines in 2019 for it’s surprising and non-predicted rapid thickening in recent years.


Image Source: BBC, 2019
New Study
A new study (Joughin et al., 2020) finds that the Jakobshavn glacier thickening that began in 2016 has continued apace, and ice elevation has now nearly completely returned to 2010/2011 amplitudes.
The authors attribute much of the glacier advance to the rapid 1.5°C ocean cooling impacting the region in recent years.
Ocean temperatures have returned to 1980s-era levels.

Image Source: Joughin et al., 2020
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterAn observational analysis of photometric evidence suggests solar forcing of Earth’s atmosphere could vary by as much as ±4.5 W/m² since 1750, which is “far larger than the IPCC estimate of −0.30 to +0.10 W/m²” (Judge et al., 2020).
A 2017 study suggested the solar activity during the “modern maximum period from 1940 to 2015” is a “relatively rare event, with the previous similarly high levels of solar activity observed 4 and 8 millennia ago” (Yndestad and Solheim, 2017). Variations in solar activity since the 18th century were shown to have ranged between about 1357.5 W/m² and 1362 W/m² (~4.5 W/m²).
In contrast, the total radiative forcing due to the increase in the CO2 concentration since 1750 is suggested to be 1.82 W/m² (Feldman et al., 2015).

Image Source: Yndestad and Solheim, 2017
A new study (Judge et al., 2020) also affirms our highly uncertain estimations of solar forcing variations since 1750 may be “of the order of 3 W/m², far larger than the IPCC estimate of −0.30 to +0.10 W/m²” and also greater than the uncertain IPCC estimates of total anthropogenic forcing (+2.2 ± 1.1 W/m²) since 1750.
Large estimate ranges for solar forcing variability should reduce the certainty that Earth’s radiative forcing has been dominated by anthropogenic activity in recent centuries.

Image Source: Judge et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_d215e380a9bc673ec0197eb9c75076b2').on('change', function() {
			  jQuery('#amount_d215e380a9bc673ec0197eb9c75076b2').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"If we want to relieve the strain of a globally growing – and commuting – population, we need to rethink how and where we work. Working more flexibly – both in timing and location – could have a massive impact on transportation and electricity production, two of the main contributors to greenhouse gas emissions. How does it work? Organisations adopt a four-day working week, the daily head count in the office drops by approximately 20% and the number of cars on the road drops by at least a fifth. It’s a win-win-win scenario for employees, employers and the environment.  If the four-day week catches on in Auckland, for example, and organisations across the city cut down on their daily in-office head count by 20%, the number of cars on the road each day drops by at least a fifth, and by up to 40% if parents are routinely permitted to work five shorter days in order to do school drop-offs and pick-ups. A 2017 report by the New Zealand Institute for Economic Research on the benefits from Auckland road decongestion means we know exactly what this decrease in traffic volume would mean for the city’s economy. Productivity could be boosted by at least NZ$1.3bn per annum (1.4% of Auckland’s GDP), the authors say if use of the road network could be optimised. Additionally, if the average speed across the Auckland network was close to or equal to the speed limit (known as free-flow), the benefits of decongestion during weekdays were estimated at around NZ$3.5 m per day, or between NZ$1.4 and $1.9bn (between 1.5 and 2% of Auckland’s GDP). Imagine these results extrapolated for New York City or London or Buenos Aires. The intensity of congestion and the lengthening of commutes are a byproduct of the way we work today, with billions of hours, dollars, fuel gallons and pounds of carbon dioxide expended each year in developed countries, where the term “rush hour” has been part of the lexicon for as long as anyone can remember. Even if we leave aside the climate change question and apply a pure economic lens, a widespread model of working which prioritises productivity and efficiency over a robotic adherence to working hours (which were once dictated by the sun and are now mostly arbitrary) is a no-brainer. When we turn our minds to the welfare of the planet, the answer is just as obvious. The human resources department of UC Davis in 2018 bluntly made the environmental case for work flexibility: “Not going into work could be one of the most environmentally sustainable things you can do as an individual employee.” In another 2018 study, researchers analysed data from the US Bureau of Economic Analysis and Bureau of Labour Statistics and found households with longer work hours have significantly larger carbon footprints. According to UC Davis, the two main contributors to US greenhouse gas emissions are transportation (29%) and electricity production (28%), with about 135 million Americans commuting to work. 50% of those workers have jobs they could do remotely some of the time, and the emissions-reduction value of those workers avoiding their normal commute on half of their usual work days is equivalent to removing 10 million cars from the road. A flexible work arrangement programme at UC Davis has provided options for employees such as flexitime (altering the start or end times of the work day); a compressed week of fewer, longer days at work; and remote working for part of the week. Every option means skipping the commute or evading rush hour at least some of the time. That could put a big dent in transport emissions: University of Reading researchers asked business leaders and owners how a four-day week would affect their commuting habits. When scaled up across the United Kingdom, workers estimated that they would drive 557.8m fewer miles per week if they worked fewer days. Of course, none of this evidence matters unless our political and business decision-makers are willing to upend the status quo in service of our planet’s viability. Changing to a four-day working week won’t by itself solve the climate crisis, but combining it with other progressive policies will be part of the global climate mobilisation we indisputably need. Andrew Barnes is the CEO of Perpetual Guardian. The 4 Day Week: How the Flexible Work Revolution Can Increase Productivity, Profitability and Well-being, and Create a Sustainable Future is published by Little, Brown on 6 February 2020. https://4dayweek.com/ "
"

An article today in BRIDGES Weekly Trade News Digest ( _What? You don’t subscribe??_ ) contains an explicit rejection by India’s trade minister of the idea that carbon border tax adjustments belong in the WTO’s agenda. Border tax adjustments in this context refers to _de facto_ tariffs that would “level the playing field” for domestic producers competing with foreign producers not subject to climate change policies of an equivalent rigour, also called “border carbon adjustments” or variations on that theme.   
  
  
While Minister Khullar predicts that these sorts of measures will be in place in 2–3 years time, he rejects that the WTO is the forum to deal with environmental issues.   
  
  
Furthermore, countries introducing such measures can expect litigation: 



India and other developing countries will undoubtedly challenge the true impetus behind the [border carbon adjustment] measures.



“Such measures imposing restrictions on imports on the grounds of providing a ‘level playing field’, or maintaining the ‘competitiveness’ of the domestic industry, etc are likely to be viewed as mere protectionist measures by the developed world to block the exports of the poorer nations,” [a recent report from an Indian think‐​tank closely connected with the Indian government] reads. “This is because there is little empirical evidence that companies relocate to take advantage of lax pollution controls.”   
  
  
The [report] argues that such unilateral trade measures will inevitably lead to tit‐​for‐​tat trade retaliation that could spiral into an all‐​out trade war. Such warnings have also been raised by China and several think tanks following the issue.



I’ve written before on the dangers of introducing climate change issues into the WTO (and Dan Griswold has written more broadly on why labor and environmental standards don’t mix well with the aim of freeing trade) but this is yet another firm, unequivocal warning to developed countries that their proposals (and they are still just proposals at this stage) will have consequences. Developed country politicians who insist on forcing rich‐​world standards on the poor world should listen carefully.
"
"

 _ **Editor’s note**_ _: In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security _an edited volume of papers originally presented at_ _a Cato conference_ _the previous year. In each chapter, experts assessed and put in context the supposed dangers to American security, from nuclear proliferation and a rising China to terrorism and climate change._



 _As part of our_ _Project on Threat Inflation_ _, Cato is republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. You can view previous entrieshere, here, and here and on the Project on Threat Inflation homepage._



 _This week’s entry comes from Christopher Fettweis, a professor of political science at Tulane University, and the President of the Board of the World Affairs Council of New Orleans._



In “Delusions of Danger,” I made the case that fear in the United States is out‐​of‐​proportion to the dangers it faces. In the time since I wrote, I have tried to explain why this is so, where that fear comes from and how the nation can be reassured. Then my country elected Donald Trump to be its president, and I was tempted to delete the whole project and join my cousins in the drywall business.



Donald Trump is a manifestation of American fear. He is the nightmarish personification of everything that scares us, from immigrants to crime to job loss, and he stokes those fears on a daily basis. Even the COVID-19 crisis has provided opportunities for him to remind us of dangers, this time those emanating from China, the WHO and (apparently) arms control treaties. Trump’s entire existence is defined by his enemies, who for his supporters become the enemies of the nation. Voices of reason are easily drowned out by the cacophony of madness. Fear will be the defining feature of American politics as long as we are led by Donald Trump.



The COVID-19 crisis had the potential to mitigate American fears. We could have used this opportunity to recognize our common humanity, to realize that we faced the same challenges and work together toward solutions. Empathy toward the plight of others, whether in China or Iran, could have grown. A better, more rational, less fearful world could have emerged from this ongoing tragedy. Unfortunately we are led by a man incapable of thinking in such directions.



Perhaps it is not, however, time for despair and drywall. Surely it is possible for U.S. foreign policy to emerge stronger and wiser after Trump leaves office. It may prove useful for this country to have its fundamental assumptions challenged, in order to examine its most sacrosanct beliefs and evaluate their value. Trump is the equivalent of what political scientists call a systemic shock, and there is no reason to believe that what follows him will be worse. Perhaps the wreckage he leaves behind can be reconstructed in new ways; perhaps the final chapter in the story of this most pathological of administrations will be a positive one, a happy ending in which the assumptions that drive our decisions can be re‐​thought. The first few post‐​Trump years will be crucial: Do we return to the same fearful delusions that led to war after war (and to Trump), or do we seek to improve U.S. policymaking? Can we learn from our hideous national mistake?



Cato’s Project on Threat Inflation could not have come at a better time. With it and the establishment of the Quincy Institute, momentum is building toward more rational foreign policy, and perhaps a better post‐​Trump world. I worry that restraint will be wrongly tainted with the Trump stench – an odor that will not wash off easily – but that is the subject for another essay. Much of what ails American foreign policy in the short term can be cured with one election. Long‐​term trends, however, will not change unless acted upon by a force. Such a force may be growing in DC policy circles, but whether it can overcome the power of fear remains to be seen.



-Christopher J. Fettweis
"
"
I just finished a 150+ mile round trip from Boulder to get Dillon, CO and Cheesman Reservoir USHCN sites in addition to the Boulder NIST/NOAA site.
Cheesman had recently been flooded due to heavy runof from forest fire, the roads were mudpits, and even with 4WD I rented couldn’t get there before sunset. So gave up and returned to hotel at DIA for flight out tomorrow.
Had Vietnamese food with Pielke’s group last night, and that didn’t help my day either. I’m pretty toasted. But it was a heckofa good day even so.
So I’m signing off for a couple days for travel back home and some R&R.
The good news; While driving back on US285 I had another citizen science project idea to disprove Parker’s  2004 and 2006 papers essentially saying “UHI is minimal or doesn’t exist”, which I believe is unsupportable. I think it will work. Got to mull it over. Check back in a day or two.  Pictures and presentation coming when I get back to normal schedule.
Anthony out


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea40bc5a3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBoth during the last interglacial (~120,000 years ago) and from roughly 2000 to 7000 years ago, relative sea levels were from 6-10 meters to 1-3 meters higher than they are today, respectively.
For a list of over 100 other scientific papers indicating sea levels across the world were multiple meters higher when Earth’s CO2 concentrations were about 150 ppm lower than they are today (~260 ppm), see our database here.
The Mid-Holocene, 2000-7000 years ago
Lopez-Belzunce et al., 2020 (Mediterranean)
“Regarding the stabilization of the RSL [relative sea level], our data show it to be 1.20 m above the present-day level at 3000 cal yr BP and 1 m higher at 2000 cal yr BP.”
Burley et al., 2020  (Polynesia)
“At the time of first Lapita arrival at Nukuleka, sea levels were 1.2–1.4 m higher than present (Dickinson 2007).”
Lopes et al., 2020 (Brazil)
“The late Pleistocene-middle Holocene post-glacial marine transgression (PMT) that started around 18 ka b2k in response to the melting of ice caps and glaciers, together with increased precipitation, would have led to another lake highstand (Figure 3A). Sea-level curves obtained from several sites along the Brazilian coast show that a mean sea level (m.s.l.) equal to the present one was reached at ~7 ka b2k, and continued to rise until reaching up to +5 meters between 6 and 5 ka b2k (Martin et al., 2003; Angulo et al., 2006). In the CPRS the PMT formed the Barrier IV, and the estimates based on geologic and fossil records indicate that it reached amplitude of about 2-3 meters above the present m.s.l. (Barboza and Tomazelli, 2003; Caron, 2007; Lima et al., 2013; Dillenburg et al., 2017).”
“The altitude of the terrace T3 above the fossils of Toxodon found in situ indicates this was cut by the Holocene sea-level highstand that reached a maximum altitude of 3 meters [above present] between 6 and 5.1 ka b2k. At that time Mirim Lake was invaded by the Atlantic Ocean through Taim and São Gonçalo channel, becoming a large paleo-lagoon with conditions suitable for its occupation by marine organisms, including sharks, rays, teleost fishes and whales. The coastal waters were warmer than today, as indicated by the presence of fossils of the shark Carcharhinus leucas, common in tropical areas.”

Image Source: Lopes et al., 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Brocx and Semeniuk, 2020 (Western Australia)
“The Holocene stratigraphy in the Walpole–Nornalup Inlet Estuary shows that mean sea level was 1 m higher than present some 2900–1200 years BP (Semeniuk et al., 2011).”
Helfensdorfer, 2020 (Australia)
“This study presents a well-constrained model of the geomorphic evolution of the lower Murray River and Murray estuary with a specific focus on the response of the system to the Holocene sea-level highstand. Hydrodynamic modelling of the lower Murray River and Murray estuary was conducted to evaluate the primary drivers of palaeo-environmental change during the Holocene and constrain the plausible response of the Murray estuary to the +2 m higher-than-present sea level of the Holocene sea-level highstand.”
Martin et al., 2020 (Western Australia)
“Sea level high stands (~2 m higher than present) occurred at ~7 and 4 ka (Gouramanis et al., 2012) that likely caused seawater intrusion events into the aquifer”
The Last Interglacial (LIG), ~120,000 years ago
Muh et al., 2020  (Bahamas, Bermuda)
“Corals with closed-system histories collected from patch reefs on NPI have ages of 128-118 ka and ooids/peloids from beach ridges have closed-system ages of 128-116 ka. Elevations of patch reefs indicate a LIG paleo-sea level of at least ∼7 m to ∼9 m above present. Beach ridge sediments indicate paleo-sea levels of ∼5 m to ∼14 m (assuming subsidence, ∼7 m to ∼16 m) above present during the LIG. …. Results of this study show that at the end of the LIG paleo-sea levels could have been as high as 11-13 m above present (at localities close to North American ice sheets) to as little as 5-8 m above present (at localities distant from North American ice sheets).”
Helm et al., 2020  (South Africa)
“Around 126 ka, sea levels were 6.6-8 m higher than present levels on the Cape south coast [of South Africa]. … Chronological context11 suggests an age of MIS 5e (the Last Interglacial). As sea levels during MIS 5e in this area were up to 6-8 m higher than at present, a warmer climate capable of supporting large reptiles on the Cape south coast can be inferred.”
Share this...FacebookTwitter "
"From wildfires to rising tides, the climate crisis is already bringing many threats. Now scientists say it may also bring a shortage of many popular wines. Researchers looked at the land suitable for 11 popular varieties of wine grape and found that 2C (3.6F) of warming above pre-industrial levels – a rise the world is on track to exceed – would result in a 56% loss of suitable land within current wine-growing regions compared with the 1970s, before the most serious impacts of global heating. The white grape variety ugni blanc (also known as trebbiano toscano) is expected to lose 76% of its suitable growing area, and riesling 66%. The red grape grenache is predicted to lose 31% of the area currently deemed suitable for growing the variety. But the team said the glass was not necessarily half empty. Ignacio Morales-Castilla, the co-author of the study from the University of Alcalá, Spain, said: “The positive message is that we can still adapt viticulture to climate change – and diversity is a very interesting tool to do that. But the warning … is we should limit warming [as much as] possible, because the more warming we have, the fewer options for adaptation.” Writing in the Proceedings of the National Academy of Sciences, Morales-Castilla and colleagues report how they built a computer model that takes into account the timing of processes such as budding and fruit ripening for the 11 different varieties, as well as the climate in areas where these varieties are currently grown. From this, they identified areas within current wine-growing regions suitable for each of the 11 varieties. The model suggests global heating may hit the wine cellar hard: if no action is taken, a 2C rise would result in a 56% loss in land for the 11 varieties . A 4C rise would mean 85% of these areas would be lost. While Morales-Castilla said that was partly down to factors such as changes in rainfall, the main driver was heat. He noted this might damage plants, or speed up ripening and make the grapes too high in sugar. But the model also shows that if these areas could be replanted with a more suitable wine grape, or newly suitable areas planted, only 24% of growing area within current regions would be lost under a 2C temperature rise – a reduction in loss of more than a half. Under a 4C rise, 58% of such an area would be lost if varieties were switched or newly suitable areas planted – about a third lower than if no such action was taken. For example, many areas of wine land suitable for pinot noir, including in South Africa and Burgundy, will need to be switched to grapes such as syrah, monastrell and grenache, which produce fruit later in the year and are better able to tolerate a warmer climate. The team said some countries might be more affected than others, with countries already warmer and less able to compensate for future losses: land loss for the varieties could hit 90% for Italy and Spain under 4C of heating. And there’s more: the team found that new areas around the world – including parts of the UK – would become suitable for wine grapes as the planet continues to heat, with early-ripening varieties such as pinot noir moving north. The study has limitations, including the fact that it only looks at a handful of the more than 1,100 varieties of wine grape. Morales-Castilla suggested other varieties might offer greater potential for adaptation as the climate continued to heat up. The team said mitigation efforts were not without their problems: replanting or regrafting vineyards is expensive. There are also complex rules about how wines are labelled: for example, the name “champagne” can legally be used only if the sparkling wine comes from the Champagne region of France. Prof Steven Penfield, of the crop genetics department at the John Innes Centre, who was not involved in the research, welcomed the study. He said: “[It shows] that if growers are willing to adapt by changing the varieties they grow, there are ways to maintain yields in the face of rising temperatures, at least in the less extreme emissions scenarios. “The challenge for the industry will be that local varieties often add distinctive characters to wines, and there will be a reluctance to let go of traditional varieties, especially in areas with strong cultural heritage. Can you imagine a burgundy without a pinot noir grape, for instance?”."
"
Share this...FacebookTwitterBy Die kalte Sonne
The past winter in central and northern Europe was quite warm. Why is that? The Norwegian Centre for Climate Research CICERO explains it in an article from 6 January 2020:
Unseasonal temperatures for Norway
The unusual warm temperatures this winter and forecasts indicating milder winter conditions for January, February and March in Europe are partly due to an atmospheric circulation pattern called the North Atlantic Oscillation, or NAO. This atmospheric circulation pattern explains well the weather we get in Europe, especially in winter.
As explained in a CICERO-article from November 2019, seasonal forecast models are sometimes able to correctly forecast the phase of the NAO. 6 different seasonal forecast models are run at the beginning of each month by their respective weather centres from around the globe. The October simulations gave us a hint that we might get a positive phase of the North Atlantic Oscillation for November, December and January. The signal was quite strong, and 4 out of the 6 models were clearly in that direction, while 2 suggested normal winter conditions. The November simulations gave similar results, the majority of the models showed a positive NAO for December, January and February. Experts were a bit puzzled, as at the same time the snow cover over Siberia was already quite extensive, and the Arctic was very warm, two things that usually suggest a cold winter for Europe.
And then came the December simulations, the most recent ones, where all six models hinted to a positive NAO for January, February and March, and therefore milder conditions than normal for northern Europe in particular.
Read more at CICERO. An excellent report, which also applies to Central Europe.
For those who don’t know it yet: The NAO (North Atlantic Oscillation) controls the winter temperature in Northern and Central Europe. Positive NAO brings warm winters, negative NAO brings cold winters. Please note!
Now some of you will ask, what is this NAO actually? Well, it is the difference in air pressure between Iceland and the Azores. If the difference is big (pronounced low and pronounced high), then the NAO is positive (NAO+). If low and high are a bit thin, so the difference is smaller, then the NAO is negative. It’s as simple as that.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Why is it important? This pressure difference pushes the westerly winds a little bit more to the north or south. In an NAO+, the westerly wind belt lies further north and meets central Europe. This is where the humidity (wet February 2020, does anyone remember?) and the relative warmth of the Atlantic occur.
In a negative NAO (NAO-), the westerly winds blow further south and discharge their humidity as rain in Portugal and Spain.
For those who want to know more, the NAO website of the British MetOffice is recommended. We take the liberty of reproducing the two most important graphs of the website here. And this is how it looks with a positive NAO:

And this is how a negative looks:

And now, of course, you want to know where to check the current NAO status. To do so, simply google NAO and NOAA, or click on this NOAA page. There you can follow the last months of the NAO in high resolution. There is also a forecast for the next 2 weeks.
We see: In fact, the NAO was mostly positive during the winter. The forecasts for the next 2 weeks are not consistent. Bad luck. But if all models show a sharp downward trend in winter, you urgently need to buy road salt.
If you understand the NAO, you will get along better in life and in the climate change labyrinth. Finally we allow ourselves the question:
Why can’t the DWD German Weather Service explain such contexts to us?

Source: NOAA
Stay healthy!


		jQuery(document).ready(function(){
			jQuery('#dd_16efd3924a8804ec558ac63db78e3d5e').on('change', function() {
			  jQuery('#amount_16efd3924a8804ec558ac63db78e3d5e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Cape Town’s recent drought brought into sharp relief how people behave and the choices that they make when resources become scarce. The South African city was teetering on the brink: a sustained dry spell meant that residents were faced with “day zero” – the very real possibility that water supplies could be turned off, leaving them queuing for access to this most important of natural resources. Day zero was averted in large part by people’s willingness to use dramatically less water than normal, while waiting for rain to fall. This was at once a worrying story about the impact of these “mega droughts”, which, according to top scientists from around the world, have become substantially more likely due to climate change. At the same time, Cape Town told an optimistic tale of a society pulling together in a crisis. My research, recently published with colleagues Oliver Vitouch and Judith Glück from the University of Klagenfurt, shows that we perhaps shouldn’t be so surprised by the way that the people of Cape Town responded to water scarcity. In fact, people may be more willing to share scarce resources they really need than apocalyptic Hollywood movies may suggest. As such, the results offer hope that, in a world facing a dramatic population increase and a potentially greater struggle for resources, it will not be every person for themselves. Our research asked how a crucial resource is shared by individuals when it is critical to their physical needs and well-being. The premise was simple: are people more or less generous with a “primary” resource such as water than with a “secondary” one like money, where needs may strongly vary across individuals and which is also more abstract?  The results revealed that even when people themselves were in need of water, they still acted in an altruistic way by sharing the very valuable and very limited supply of water they had equally with others. This was true even when they didn’t know who they were sharing their water with. This also contrasted with how they shared money. In our study, participants were invited into the lab in pairs (but they did not have a chance to meet or get to know each other). In separate rooms, they were asked to ride a stationary bike for half an hour to work up a thirst. They did not have access to a drink for an hour before, during or after the study.  We then used a used a common experiment that’s used in experimental economics research called the “Dictator Game”. One person was made an “allocator”, in control of giving out money and water. The other was the “recipient”.  The pairs were placed in two experimental conditions. In one condition, allocators received water and money as a windfall to give anonymously to recipients. In the other, those doing the allocating were informed that they had “earned” the water and money during the training session. Recipients in both conditions indicated how much they expected to receive. This was designed to test if their expectations were linked to whether or not allocators had “earned” the resource. The results were intriguing: the allocators who earned the small amounts of water gave about the same as those who hadn’t earned it. In both groups, they were willing to share water more generously than money. Similarly, recipients expected to receive more water than money. This goes against the idea that people are purely self-interested. One may argue that thirsty allocators were less generous because water simply had less value compared to money. But findings from other research suggest that what we observed may be guided by increased empathy towards others with a similar need which makes altruistic behaviour more likely. This may also explain why a related study conducted at UCL showed that when thirsty recipients were allowed to reject water offers, they were less likely to accept the ones that were very small and may thus have been perceived as unfair. Perhaps such a strong sense for fairness also speaks to the age-old wisdom that despite money’s value, we know deep down that it’s no substitute for the basics we and others need day to day. Ultimately, a thirsty man knows he can’t drink the money in his pocket; and others can’t either. Research such as ours aims to better understand what drives people’s behaviour – and how likely they are to cooperate – when resources are scarce. This may allow us to predict how individuals will behave before they run out of a resource. We now need to learn how sharing varies in different contexts, with different resources, when scarcity occurs in the real world and over long periods of time, like during the drought in Cape Town. The events in Cape Town are another warning about the consequences of climate change and the increasing need to preserve the world’s natural resources. But our results may also indicate that it is when crises are at hand and resources are scarce that our shared humanity is more likely to show."
nan
"
Share this...FacebookTwitterBy Kirye (photo right)
and Pierre Gosselin
Today we post before-and-after mean annual temperature charts for 6 US stations in the midwest region with a low brightness index (BI), meaning low impact from the urban heat island (UHI) effect, which arises from widespread asphalt, concrete and infrastucture.
The low BI index tells us that the stations are sited in a rural-type environment. Five of the six stations have a BI of 0, while one (Thibodaux, LA) has a relatively low BI of 11.
Shown will be comparisons of NASA GISS Version 4 unadjusted, versus Version 4 adjusted. In each case the unadjusted data showed a cooling or little warming, while the adjusted data all ended up to show warming.
First we plot the station at Plainville, Kansas, which I already posted at Twitter. Shown is the plot going back over 100 years, before NASA adjustments and after adjustments.

It's obvious, as far as Plainville's temperature goes, NASA made the false warming trend.Why do many media pretend to know nothing about that?https://t.co/35Itt16sN0~#地球温暖化? #温暖化？ #気候変動 #ClimateChange pic.twitter.com/WTD380BGxN
— キリエ (@KiryeNet) June 7, 2020

Data source: NASA GISS.
As the 2 plots show, the data from the past were changed by NASA and made cooler. The new result: a warming trend! In other words, a cooling climate was fudged into one that is supposedly warming.
Next we move to the station of Hobart, Oklahoma. A slight cooling trend there was transformed by NASA into a warming trend:

Data source: NASA GISS.
The third station we look at is Carrizo Springs, Texas:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: NASA GISS.
Originally in the V4 unadjusted, the past at Carrizo Springs was warmer than today. But NASA didn’t like that, and so they adjusted the temperatures from earlier in the 20th century downward. Again a modest cooling trend was changed into warming.
The story is the same at the station at Conception Missouri:

Data source: NASA GISS.
At Conception, Missouri, we originally saw no warming over the past 130. years. But then NASA fiddled with the data and now tell us there’s been warming at there as well.
Looking at the temperature charts for the station at El Dorado, Arkansas:

Data source: NASA GISS.
Note how warm it was in the 1920s. But NASA said that this couldn’t be right, and so cooled the mean annual temperatures in the early 20th century byalmost a whopping 2 degrees! Result: (fake) warming!
Finally we plot the NASA GISS data from the station located at Thibodaux, Louisiana – i.e. the U.S. South:

Data source: NASA GISS.
Above we see how the unadjusted V4 data were changed to create more warming.
NASA changed the data several times until they got the warming they want to us believe is taking place. The original data tell us there has been any real warming over the past century at these 6 stations.


		jQuery(document).ready(function(){
			jQuery('#dd_9484675fc1afd13abd685b1cca163d6c').on('change', function() {
			  jQuery('#amount_9484675fc1afd13abd685b1cca163d6c').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"In his acceptance speech, Brazil’s president-elect Jair Bolsonaro said he will “change the destiny of Brazil”. He may be right. For the Amazon and its indigenous residents, things will get worse before they get better, but this election may also be a tipping point for change. The staunchly anti-environment Bolsonaro may sow the seeds for radical environmental politics both in Brazil and worldwide. When the night comes, I wonder what might happen, who will be next, and hope that the dawn will come soon. With these words, Kum'tum, a leader of the indigenous Gamela people in the heart of the Amazon, shared his fear of being an environmental defender in Brazil. His community had been trying to occupy a portion of their ancestral lands claimed by farmers but, in April this year, they were attacked by men armed with machetes and firearms. Some had their hands cut off. Others like Kum'tum were shot. Indigenous groups have good reason to be scared. Over the past decade, Brazil has been the most dangerous country in the world to be a land or environmental defender and 57 of these people were murdered last year alone. According to the NGO Global Witness, this kind of violence stems from sections of the agribusiness sector, in particular parts of the Amazon’s cattle industry which is the largest single cause of deforestation globally. The Amazon and its indigenous residents may now be persecuted with even more ferocity, as Bolsonaro promises a new alliance between the security state, agricultural interests and far-right political power. Fortunately, where there’s hegemony, there’s resistance. NYU sociologist and theorist Steven Lukes famously broke power itself down into “three faces”: decision-making power, non-decision-making power, and ideological power. The Amazon’s future and that of its residents will be fought across these three key battlegrounds. First, Bolsonaro’s government will have to win the visible battles of decision-making processes shaping Brazil’s environmental governance. For instance, Bolsonaro would have to break free from present institutional and legal frameworks tying Brazil to its environmental commitments, such as the Paris Climate Agreement and the Sustainable Development Goals. And that’s tricky because of the global backlash. Perhaps this is why Bolsonaro has now scrapped his pledge to quit Paris.   Other challenges to Bolsonaro’s decision-making power will come from below. While Brazil’s balance of power has historically favoured big corporations like those in the agribusiness sector, the country also hosts massive grass roots movements of resistance. These include the Marxist-inspired Landless Workers’ Movement (MST), with an informal membership of 1.5m people and, in the Amazon itself, the Cainquiama, a resistance coalition of tens of thousands of persecuted indigenous peoples and other communities.  In this case, the “non-decision-making power” concerns Bolsonaro’s ability to set the development agenda. The country’s new leader wants to reverse green agreements and return to a form of violent pro-growth neoliberalism which is blind to both democracy and environmental concerns.  But the Amazon acts as the world’s lung and is a globally shared priority. The existential threats it faces would be met in a similar combative spirit as those facing the melting Arctic – by environmental defenders, pro-sustainable development governments, global institutions and civil society. Collectively, these actors have the moral power to set the agenda and pressure for legal, institutional and economic sanctions and limits to stop the predatory march of Brazilian agribusiness further into the forest. If they can manage to frame actions against the Amazon as crimes against humanity and the planet, they could even be effective. The third conflict is the invisible ideological battle to control Brazilian hearts and minds. By democratically electing a far-right candidate, Brazilians have sent a crystal clear message that they want radical change for the better. The problem is what people understand by “better” and what this “better” means for the environment.  The “Bolsonaro way” is ideologically powerful because it is ambiguous about what it is, but sharply clear on what it is not. Just like Trump, it is unashamedly pro-growth regardless of the collateral damage to the environment, but Bolsonaro’s vague development utopia lacks depth. It resurrects zombie ideas of dictatorship style governance and pro-growth violent neoliberalism, fuelled by popular disillusionment with corruption and a failure to lift people out of poverty. Another ideological battle will concern the church, a powerful institution in the world’s largest Catholic country. In his 2015 encyclical message Laudato Si, Pope Francis attacked environmental degradation and climate change and called for “swift and unified” global action. That, and his later claim that to harm the environment is to sin, may prove a powerful ideological tool for environmental activism. The rising wave of Brazilian evangelicals may also question how Christian it would be to slay the Amazon and its people. A final key ideological battle concerns the normalisation of indifference towards the environment. According to the World Values Survey and Latinobarometro, the largest opinion polls that have investigated what Brazilians value as important, the only topics ranking high in everyday life are corruption, the unstable political situation and the economy. The challenge is that too many Brazilians fail to see how crucial the environment, the Amazon and its indigenous inhabitants are to their own well-being and development."
"

This week’s report, by Elizabeth Thomas and colleagues from the British Antarctic Survey, that snowfall has been increasing in Antarctica is hardly surprising. What is different that it is much more comprehensive than previous studies, which were largely limited by a virtual lack of pre‐​1957 data. That was the “International Geophysical Year”, in which systematic observations of Antarctica’s climate began.   
  
  
The new study looks at the last 200 years of snowfall trapped in 79 ice cores taken from around the continent. It supplements other recent findings that also made headlines.   
  
  
Determining Antarctica’s overall ice balance has been, well, slippery. One favored method has been to look at gravitational data measured by satellite. Thicker ice means more mass, which means greater gravity. These studies usually come up with a net loss, translating to from 6/1000 of an inch of sea level rise per year to 12/1000 (both values being rather small beer). But different measurements show otherwise. Three years ago, Jay Zwally and his colleagues at NASA used satellite‐​based altimetry and concluded Antarctica was undergoing a net gain in ice.   
  
  
Common sense dictates that it should be snowing more in Antarctica. Think of it as Buffalo on steroids when it comes to snow. In the fall, when Lake Erie isn’t frozen, cold air passing over it from the west picks up evaporated moisture and dumps it on the land in the form of snow squalls. The warmer the water and/​or the colder the air is, the more is snows. Unlike a mere Great Lake, Antarctica is surrounded by a largely unfrozen ocean, and when any atmospheric disturbance sends moisture onshore, it snows too.   
  
  
Around Antarctica, there’s been a slight—meaning a couple of tenths of a degree—warming of the surrounding ocean, which means that the air blowing over it picks up a bit more moisture than it used to. Unlike Lake Erie, the Southern Ocean is huge, and any atmospheric disturbance that shoves more oceanic air up onto the continent is going to be pushing a substantial stream inland with ever more moisture, even for a very slight ocean temperature rise.   
  
  
The “surface mass balance” of a glacier or an ice sheet is the difference between accumulated snowfall and what either melts or evaporates. In anticipation of increased snowfall, the last (2013) scientific summary by the United Nations’ Intertgovernmental Panel on Climate Change shows that the projected 21st change in the Antarctic mass balance to be weakly _positive_. That’s why it’s perplexing that the new finding is so newsworthy.   
  
  
But now we know that the snow has been increasing down there for the past 200 years…and that the increase started before the major emissions of atmospheric carbon dioxide.
"
"A heatwave over India that started on May 21 and has produced India’s highest recorded temperatures in two decades has claimed more than 2,000 lives and caused widespread devastation. It would be easy to dismiss this as a freak event, something so far out of the norm that there’s little chance of preparing for it. But this isn’t quite true. Even the most extreme events can be predicted – and prepared for. I specialise in a statistical approach called extreme value theory which draws inferences about very rare events. Among other uses, it can be used to estimate the probability of a heatwave occurring.  Where most statistical methods are great at modelling average values – what temperature is it likely to be next week? – they tend to be poor at modelling the extremes (what is the highest temperature we’ll see this decade?). Our model instead focuses on extreme values themselves, in this case the hottest days of the year, which are the values that can cause the greatest devastation. This also means we can figure out the rough probability of events larger and longer than any that have gone before. Just because we haven’t seen consecutive 40℃ days in the UK, or snow in the middle of the Sahara, doesn’t mean it can’t happen. Extreme value theory helps assess these probabilities.  We can use it to assess how likely the current heatwave is by analysing daily temperature maximums for Delhi over the years 1972-2013. Extreme value models can be used to estimate the temperature expected to be attained on average once a year (called the one-year return level). In Delhi the one-year return level is 45℃. However, single hot days do not have a great impact on excess mortality. Consecutive runs of hot days are more dangerous since the human body is subjected to heat stress over many days without being given the chance to recover. Fortunately the same type of extreme value models can be used to estimate the probability of runs of consecutive days above a certain temperature.  During the current heatwave Delhi recorded seven consecutive days above 44℃. That’s an entire week above an exceptionally high temperature – such an event is expected to happen on average once every 30 years. However, care must be taken with such results since they are often subject to large uncertainty as they are based upon very few rare events in the observational record. An El Niño may make such heatwaves more likely. The El Niño is one of two opposing phases in the El Niño Southern Oscillation (ENSO), a large-scale fluctuation of sea surface temperatures in the equatorial Pacific. Right now a large El Niño (warmer waters) is currently building and will peak later this year. This generally means higher temperatures in India. Could the current ENSO be contributing to the current heatwave event? Under current conditions, the probability of observing seven days above 44℃ in Delhi is slightly increased, with the event expected to happen on average once in every 25 years. But it is difficult to definitively say that El Niño is having an effect since this result is again subject to a great deal of uncertainty. One important question for decision makers concerns whether heatwaves are going to become more likely under climate change. Answering this question becomes difficult since there are numerous sources of uncertainty – most obviously, we don’t even know exactly what “climate change” will entail. For example, different scenarios are generated, most famously by the IPCC, that aim to predict how future emissions will evolve. For each particular scenario, different climate research centres run different models that produce different replications of each future scenario.  The most common way to account for uncertainty is to take an ensemble of many different climate models – yet even this can leave a big margin for error, especially when analysing rare events such as heatwaves. In 2003 a month-long heatwave struck Europe, causing at least 20,000 deaths and possibly as many as 70,000. My colleagues and I have previously analysed this particularly hot summer to assess the effect of climate change on heatwaves. We wanted to understand whether such an event would become more likely under future climate change and whether the behaviour of heatwave events would change.  We found that generally temperatures would increase, an increase of 1℃ in global temperatures would lead to an increase of, at most, 2.6℃ in the one-year return level at Orleans in central France. However, despite the fact heatwaves like in 2003 are expected to become more likely, it is not expected that such events will tend to last longer. Statistical approaches have great potential to help decision makers mitigate for devastating events such as the current Indian heatwave. By providing reliable estimates of how often they will occur better preparation measures can be undertaken which will reduce the number of deaths.  Looking to the future, the effect of climate change is the largest uncertainty. Further collaborative research between statisticians and climate scientists presents the best chance to understand the potentially devastating effects that climate change will have on all types of extreme event."
"A report by the WWF published on October 30 reveals how our actions are degrading the natural world – the very basis on which our livelihood depends. The Living Planet Report 2018 shows that between 1970 and 2014, vertebrate – mammal, fish, bird, amphibian and reptile – population sizes have been reduced by 60%. South and Central America have been hit particularly hard, suffering population declines of 89%.  The report is one of the most comprehensive global analyses of biodiversity, yet it does have its limitations. It only tracks vertebrates, sampling is not standardised across different biomes, and it ignores genetic diversity.  It’s also worth noting that other global studies have reported different figures for biomass decline. A study in Nature looking at plant and insect species, estimates declines in species abundance of around 11%, and a study from Germany found a 75% decline in flying insect biomass in the 27 years up to 2016.  These are large discrepancies and clearly this topic needs further exploration. However, all these studies support the conclusion that we are losing biodiversity at an alarming rate. There are two main strands of argument when it comes to the loss of wildlife. The first is that the loss of nature is a necessary and acceptable consequence of human progress. Historically, our wealth has increased through exploiting the natural environment, and it has allowed us to live richer lives with more freedom of opportunity.  Counter to this, the argument runs that we can only push biodiversity loss so far before we threaten the life support systems of our small planet – the capacity of the biosphere to regulate our climate, pollinate our crops, purify our water and decompose our waste. The biologist Paul Ehrlich once made the analogy that losing species in an ecosystem is like progressively removing rivets from an aeroplane: the plane may fly on for a while, but eventually it will fall out of the sky. Such concerns have led to attempts to quantify “safe limits” of biodiversity loss, or so-called planetary boundaries that we must not cross else we risk a catastrophic tipping point. Although a compelling concept, there remains serious issues in implementing it. One is the uncertainty in the extent of biodiversity loss, the other is in the impact these losses will have on human livelihoods.  To make a comparison with climate change, many governments only committed to action after the likely economic impacts were quantified through meticulous analysis combining climate science and economics. Therefore, new approaches to more precisely quantify risk are urgently needed in order to galvanise action. But even if we can ascertain the risks, will we actually be able to stop biodiversity loss?  We know with some confidence the risks of global warming, yet countries are struggling to stick to their Paris commitments, let alone the even greater emission reductions needed to avoid a warmer world.  I was recently involved in an interdisciplinary analysis of the global food system (one of the major culprits of biodiversity loss), which identified a range of mechanisms that keep our food system “locked” into an unsustainable trajectory.  People often feel powerless to change such global systems and point to factors at the level of government policy, such as the upcoming extension and renewal of the Convention for Biological Diversity. Although wise governance is essential, many factors that contribute to a decline in biodiversity operate at the individual level, such as our dietary and consumer choices. Also, the structure of our institutions ultimately reflects our individual mindsets, so we have the opportunity to initiate positive change by acknowledging our dependency on nature. Rising levels of individualism, however, have encouraged an economy that provides for private interests at the expense of nature.  Through our purchases we can destroy the environment on the other side of the world, which is why the WWF report calls for better data to connect consumers to the consequences of their actions. On the positive side, our increasingly connected world could allow for social contagion of positive and responsible ways of acting. Small individual changes can cascade and cause a different kind of “tipping point” towards a more sustainable way of life.  If we really want to halt biodiversity loss and ensure a safe course for current and future generations on Spaceship Earth, we need to think beyond government, and forget the selfish “I” – the solutions start with “us”."
"The global crisis of climate change is one of the most complex and wicked of problems we currently face. It is a physical, technological and economic challenge, and one that raises questions right at the heart of our relationship with the environment in which we live. In the light of the IPCC’s most recent report, we face difficult decisions that will change every aspect of how we live. Yet providing people with more scientific information has been shown to have little effect on the degree to which people care about the climate or understand the impact of human activity. Something else is needed to jolt us out of our current trajectory. I am exploring the role of art as a route to knowing the environment in an alternative way. For my latest work, The Matter of the Soul, I hacked the electronics of lab equipment to transform them into musical instruments that play the sounds of melting ice. In the musical compositions and sculptural installations for this work, I explore the possibility of art to engender empathy with the Arctic ecosystem, and how dispersal of water, human movement and digital identity are three intrinsically interlinked processes of transformation relevant to climate change. I began the research for The Matter of the Soul sailing around Baffin Island in the Canadian High Arctic. I set up a temporary studio on the top deck of the ship Akademik Sergei Vavilov, where I could tinker with all of my equipment and explore the aesthetic of this fascinating place, so different from my European home. The light in the Arctic was incredible. It felt like the sun touched me differently. But what struck me most was how much personality the water there had. Lumps of ice and mini-icebergs are strewn across the barren landscape, perching on top of rocks like seagulls or floating next to tiny islands as if they’re biding their time, waiting for their chance. The meltwater from glaciers crashed joyously down rock faces, surging into the ocean. Icebergs created a time all for themselves, distorting our reality when we intersected them. Around the ship, and on land in the open, rocky landscapes, I interviewed visitors to and residents based around Baffin Island. But I also captured the voice of the ocean and ice. I wanted to draw an analogy between bodies of water and human culture. To capture the voice of the water, I decided to explore the chemical consequences of ice melting. Ice and seawater are the coming together of individual water molecules, just as culture emerges from the coming together of individual human beings. Water in ice both has its behaviour shaped by its environment and constructs this environment, just as we human beings are shaped by and construct our culture. When glaciers and icebergs melt, individual water molecules begin an adventure of dispersal that could take them as far as Mexico. The behaviour and trajectories of molecules are changed by the ocean that they have joined and become part of, just as when we travel – either for tourism or migration – we change and exchange with the cultures we encounter, and within ourselves. There are plenty of scientific instruments for gathering data on how water changes due to climate change, but not many pianos that do it. I decided to merge artistic and scientific methods of encountering the world, by hacking the scientific instruments to make sounds that I could use in my musical compositions. I took a pH meter and conductivity meter, which measures the water’s saltiness, with me to the Arctic. To get sound out of these scientific instruments I used a process called circuit bending. This process allows me to make audible the changes in voltage in the equipment when they measure the physical properties of the water. I can then record them. It took a while for me to succeed in this. Immersed in the Arctic, having spent days in open water due to fog and ice, I had been struggling to find the sound in the circuit of the conductivity meter. Hunched in my little studio, dipping the probe in and out of my sample of “Open Seawater 2”, I suddenly heard it. A faint hum, followed by a diminishing click, with periodic surges in the sound. I had found a temporal representation of the salinity – the sound of the the dying off of the saltiness of the sea as icebergs melt. The sound of the dying of the ice. Importantly, these recordings are not directly representative of the value of the measurements. They are rather derived from what happens inside the machines during the process of measurement. As such, the recordings are not a sonification of data, but rather a reflection on the process of measurement as a passive way of knowing, which can evoke the same sense of achievement as acting (and therefore can act as a placeholder for real action). On October 23, I played my hacked instruments at Howard Assembly Rooms at Opera North in Leeds, accompanied by pianist Matthew Bourne and cornet player Alex Bonney. The hour-long symphony tells the narrative of transformation and dispersal in the Arctic: of ice and seawater, of changing culture and climate change. The raw material and compositions from the project will also be released online under Creative Commons, with a call for others to remix the work’s identity as it disperses through the internet. I am also releasing all compositions of The Matter of the Soul on audio cassette, with the full symphony coming out on cassette in November. I chose this physical format because it’s easy to remix it while preserving the audio quality. We are at a tipping point, a moment where the decisions we make will drastically affect the future well-being of humanity. A wicked problem like climate change can’t be fixed by one action alone. It takes a collection of actions to tip the system. I hope that by connecting in an embodied way with the processes of transformation in the Arctic, it might be possible for a few of us to connect with transformations within ourselves, enabling us to take action without."
"
Share this...FacebookTwitterThat’s right, last week a panel, made up of 4 pompous linguists and one journalist, chose “climate hysteria” as Germany’s taboo word (un-word) of 2019.

Image: PatriotRetort.com
Discriminatory, disguising or misleading
The Unwort des Jahres (un-word of the year) is a new or recently popularized term used in Germany which a panel deems “violates human rights or infringes upon Democratic principles”.
According to Wikipedia, “The term may be one that discriminates against societal groups or may be euphemistic, disguising or misleading. The term is usually, but not always, a German term. The term is chosen from suggestions sent in by the public.”
Over the years, like so many other institutions, the volunteer panel has leaned to the left and has been choosing words that tend to cast conservatives and the right political spectra in a negative light. The panel’s announcement of the un-word of the year gets broad media coverage.
Last week the panel selected “climate hysteria” as the un-word of the year.
Taboo because it “defames climate protection efforts”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Wikipedia, the panel – which has no scientific expert on it at all, chose “climate (change) hysteria” as the un-word of 2019 because it “defames climate protection efforts and the climate protection movement, and discredits important discussions about climate protection.”
Climate science dissent is no longer welcome, the panel wants to tell us.
According to Wikipedia:
The expression [climate hysteria] was used by many in politics, economics, and the media in 2019 – by the Frankfurter Allgemeine Zeitung as well as by entrepreneurs and especially by politicians of the Alternative for Germany party. It dismisses the increased commitment to climate protection as some kind of collective psychosis. Moreover, in light of scientific findings regarding climate change, this word is misleading and irresponsibly supports anti-scientific tendencies.”
Yet, thankfully, some media have grown critical of the panel of volunteer linguists and single journalist, and all the media attention it gets. For example, Bild newspaper wrote:
As if it were the decision of an important institution, the decision of a privately organised group is reported: Four linguists and a journalist who volunteer once a year to play linguistic police. According to the motto: Listen up, citizens, the language committee has decided, this word is taboo from now on!
Ironically, in 2011 the panel chose “alternativlos” (no alternative) as the un-word of 2010 in politics because they claimed it was “undemocratic”, as any discussion on a subject “would be deemed unnecessary or undesirable”.
Today the panel appears to have forgotten about that earlier choice.
In any case, skeptics and dissenters should instead ramp up the use of the term “climate hysteria” to describe the FFF and XR movements, and all the nutty doomsday scientists who like telling us there’s no alternative to decarbonization.
Share this...FacebookTwitter "
"

Have you ever wanted a reliable way to test your Internet connection speed? I have, and there’s been a bunch of tests devised…but not all work that well. I even saw one once where after the test a picture of Al Gore would pop up and say “I invented the Internet and your speed on the information superhighway is: xxxx bytes/second.”
This test http://www.speedtest.net/ is one of the coolest looking and most accurate ones I’ve ever seen. It figures out where you are in the world and provides test points to servers all over the globe. It does an accurate download and upload speed test by doing a file transfer to/from your PC. and displays it in a nice dashboard representation as shown below.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9fcaccc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The death of famed “daredevil” climber and base jumper Dean Potter has once again raised the idea that all high-risk sportspeople are hedonistic thrill seekers. Our research into extreme athletes shows this view is simplistic and wrong. It’s about attitudes to risk. In his famous Moon speech in 1962, John F Kennedy said: Many years ago the great British explorer George Mallory, who was to die on Mount Everest, was asked [by a New York Times journalist] why did he want to climb it. He said, ‘Because it is there.’ Well, space is there, and we’re going to climb it, and the moon and the planets are there, and new hopes for knowledge and peace are there … Humans have evolved through taking risks. In fact, most human actions can be conceptualised as containing an element of risk: as we take our first step, we risk falling down; as we try a new food, we risk being disgusted; as we ride a bicycle, we risk falling over; as we go on a date, we risk being rejected; and as we travel to the moon, we risk not coming back. Human endeavour and risk are intertwined. So it is not surprising that despite the increasingly risk-averse society that we live in, many people crave danger and risk – a life less sanitised.  Dean Potter exemplified that craving. He was a pioneering climber and base jumper, well known for scaling huge vertical rock faces without ropes and with only a parachute for protection. On May 16 Potter and fellow climber Graham Hunt died in Yosemite National Park after attempting a dangerous wingsuit flight, where base jumpers wear a special suit that enables them to “fly” forwards and control their fall. Potter’s endeavours and those of George Mallory seem motivated by something very different from hedonistic thrill. Over the past ten years we have interviewed dozens of high-risk sports people and studied their profiles in detail with a view to trying to find out what that “something different” is. Our findings are surprising. For example, it is now clear that sensation-seeking explains very little about the motive for many of these people. Many high-risk sportspeople do not crave excitement at all – yes they seek out risky environments, but only with a view to minimising any additional risk so that they can remain in control despite the apparent danger of dangling off cliffs or jumping out of planes. But there are two more striking features of our recent risk-taking research. The first is something we call “agentic emotion regulation”. Feeling agency is similar to feeling in control, but more akin to the feeling “I want to be the person who decides how my life pans out”. Some high-risk sportspeople purposefully seek out danger in order to make some sense of their feelings of lack of agency. In other words, in everyday life they do not feel like the chess player of their life but more like the pawn on the chessboard – they feel emotionally constrained and passive.  Legendary climber Patrick Berhault, who later died traversing a steep face of Switzerland’s highest mountain without a safety rope, once said he didn’t think he’d do it if there wasn’t the notion of risk. “Ordinary life lacks intensity and attraction for me”, he said, “I can’t stand it; I believe we should live!” The fascinating feature of this finding is that the lowest sense of agency is in relationships that are the most emotional: with loving partners. This feeling of low agency is made worse by the difficulty with expressing their emotions. In this way, the relationship with risk serves as a proxy for the relationship with a loving partner, except that the risk-taker is rewarded – rather than penalised – for not expressing emotion. The primary emotion to overcome in risk-taking activities is fear. If a person has difficulty experiencing and expressing emotions then the risk-taking arena becomes a rewarding place.  It is rewarding because they have moved from a feeling of inadequacy, “why can’t you tell me how you feel??” to a sense of achievement, “wow, that was amazing how you achieved that scary feat … ” In this way, the relationship with nature is more rewarding than their relationship with other humans. My vision turns black and white except for the searing red line. 
Sounds fade. I feel faint, face flushed with heat. My muscles tense, but I hold calmness in my centre and loosen my arms from the shoulders to my fingertips. The moment sickens me, and my mind tries to stop it, but I command myself to walk. – Dean Potter on facing fear and going ropeless. The second surprising thing we found in our research is that the difficulty with emotions leads people to take greater risks and to have more accidents in the high-risk environment – where accidents have serious consequences. The link between emotional expression and accidents is our most recent finding and one that was so intriguing that we ran three different studies on various high-risk sports to see if we found the same thing. Each time, we found a strong link between the difficulty in expressing emotions and the chances of being in an accident. We now understand this link. People who have difficulty identifying and describing their emotions seek risky extreme sports because they provide the experience of a more easily identifiable emotion: fear, perhaps the purest emotion of them all.  The continued search for fear (and overcoming that fear) leads people to take further risks, which in turn eventually leads to a greater likelihood of an accident. This finding was novel because the established view in emotion research is that people do not typically repeatedly approach situations that induce fear. However, extreme sportsmen and women are attracted to risk because it provides an opportunity to experience the negative emotion of fear and to turn that fear into a fantastically rewarding and positive experience (often in retrospect). Extreme sportspeople learn something about themselves by taking risks and by embracing the full spectrum of their emotions. It is a construction of the self that is played out in nature with all its inherent dangers.  They expect more from life. A craving for life in its purest, simplest, and sharpest form. Life in direct juxtaposition to death; to live fully or to die trying. In that respect, adventurers such as Dean Potter can teach us all how to embrace life and to turn directly to face our fears."
"A novel “floating pipe” to recover plastic from the ocean has just arrived on its maiden voyage to the Great Pacific Garbage Patch. Run by Dutch start-up Ocean Cleanup, the scheme involves a 600m-long floating pipe connected to a net, which herds plastic into place before it is gathered and taken to shore by specialist boats.  The question of why we should bother to clean up the oceans may seem obvious to you but, as an economist who studies these things, I like to put a number on it. We can therefore say that plastic in the ocean has a direct financial impact through things such as lost tourism, damaged ships or fewer fish to catch. But it also has a wider and harder to quantify economic impact on lost marine life or reduced beach and water quality.  These damages, estimated at US$1.25 billion annually, imply that recovering marine plastics is worthwhile. But my research suggests that it might not be financially viable to do so. This is partly because the clean-up is so expensive. Unsurprisingly, towing a massive boom out to the middle of the ocean and then periodically transporting plastics to and from it is not cheap.  The Ocean Cleanup’s own 2014 feasibility study suggested that, once a full fleet of 100km of these floating barriers was deployed at a cost of US$372.73m (currency converted by myself in August 2018), it would collect plastic at around US$5.32 per kilogram.  This wouldn’t be a problem if discarded plastic was more valuable. The scheme could even pay for itself. But the clean-up will remain unprofitable for the time being because the market price for discarded plastic remains incredibly low. I looked at four possible options for recovered plastic: 1. Landfill: This is the easiest option although it leads to actual net losses rather than any benefit. Revenue per kg: -$0.12 2. Incineration: Burning all waste generates electricity which is reportedly as much as 60% cleaner than a fossil fuel equivalent. However, this negates the possibility of recycling or reusing the plastics. Revenue per kg: $0.10 3. Pyrolysis: Similar to incineration, except the plastic is heated in the absence of oxygen, so it doesn’t burn. Instead, the process generates oils which can be refined and sold. However, the viability of pyrolysis is dependent on economies of scale which may not suit it to the infrequent collection of marine plastics. Furthermore, at a low level, it is unlikely that the generated oil from plastics can be price competitive with conventional oil sources. Revenue per kg: $0.27 4. Recycling: This is the preferred option, as it is a more efficient use of existing resources. But volatile recycled plastic prices and low virgin plastic prices suggest that this, too, is unlikely to be a profitable option. Revenue per kg: variable, but a weighted average of $0.15 is a reasonable assumption All this means it costs more than $5 to gather a kilo of plastic from the ocean, while that same plastic will only be valued at – at best – 30 cents. With about 8 billion kilos (8,000 tonnes) of plastic added to the ocean each year, the costs – and losses – involved are huge. Of course, such a massive loss is not likely to persist in the long term. For instance, increasing oil prices will push up the price of virgin plastics hence making recycling a more valuable option. Furthermore, the high cost expectations are primarily due to this being the first system of its type. With further experience, research and development in recovering debris at sea, it is likely that a lower-cost method will arise. However, there is no immediate indication of such improvements, and so big losses are expected to persist in the short to medium term. The financial loss contrasts with the economic benefits of recovering marine plastics. Even conservative underestimates of the costs of marine plastics suggest annual damages to be in the billions.  Using estimates of how much plastic is in the ocean, I was able to estimate that removing each kilogram would lead to a net benefit of at least US$7 and as much as US$38. But that still leaves us with a direct financial loss of nearly US$5 per kilogram recovered, versus a more than US$7 net benefit to society for every kilogram recovered. So we are at an impasse. It is nowhere near profitable to recover marine plastics, yet it is imperative to do so. Thankfully, a few solutions are at hand.  The first is crowdfunding. This avenue worked before for the scheme in question which managed to raise more than $2m in just 100 days. But there are doubts about how viable such a fickle and inconsistent funding source is for a longer-running scheme that would operate at a global scale.  Philanthropy is another option. Conceivably, with the support of just a few wealthy benefactors, viable ocean clean-up could be a reality. However, it is unclear whether leaving ocean clean-up to the whim of a few individuals is a sustainable model in the longer term. Finally, the international nature of plastic pollution suggests no single government is going to foot the bill – especially if all the financial benefits are privately appropriated. However, one suggestion is that if taxpayers are already paying environmental charges aimed at reducing plastic pollution, such as the plastic bag tax or proposed latte levy, then perhaps these revenues could be earmarked to fund the scheme.  So future policymakers must pay particular attention to the various mechanisms and agreements that may bridge the gap between financial losses and economic benefits. Indeed, evidence suggests a healthy degree of public support for cleaning up the environment, but whether the public feels strongly enough to support efforts via crowdfunding or earmarked taxes remains to be seen.  Plastic causes $13 in damages per kilogram per year. The race is on to determine how we can clean up the world’s oceans without bankrupting ourselves."
"

The image above is from Google Earth, which had its photography for Chico updated just within the last two weeks. The picture is the closest zoom available and shows the new City Plaza under construction. Based on other construction landmarks around town and the colors of foliage, plus the fullness of Lake Oroville at the time, I’m estimating the image was snapped by satellite sometime in late June 2006.

Just looking at it I immediately found out something I didn’t know; That the four corners of City Plaza are compass points. The North corner, which points to Duffy’s Tavern is almost exactly true North, with the other corners being East, South and West. I added the letters to the image for your convenience, they don’t exist on the sidewalks.
The previous imagery for Chico on Google Earth was taken in 2003, and quite fuzzy. Partially due to the image being taken while smoke and haze from a wildfire somewhere covered the town. Part of that imagery remains in the Nord Avenue section on the west side and when you load images with that view, you can see a sharp cutoff where the smoke ends and fresh image begins.
Another thing thats been updated is integration of vegetation colors at lesser zoom levels, which clearly show where agriculture exists. See the image below.

But the coolest thing is the 3D capability, and in the image below, you can take in all of Chico and the foothills in one image. This view is looking Northeast from about 23,000 feet altitude. The Chico Airport is in the upper left. Click the link below the picture for a larger more detailed image.

Click to view larger image
If you haven’t tried Google Earth yet, its really a lot of fun. Free too. See http://earth.google.com/ It’s a great tool for figuring out things or for just looking around at things you’ve never seen from the air. The Pro version, which I use, allows integration of overlays and drawings, and allows measurements. When I was running for County Supervisor this past year I used Google Earth partly to devise a solution to the Keefer Slough flooding problem. I’ll post that solution in a future blog.
Another thing that can be seen in Google Earth is the city owned stormwater basin just south of the Chico ER building that was the subject of problems this summer related to mosquitos and a West Nile virus outbreak. That was a hot topic on Jack Lee’s blog this summer.
But the most odd new image in Chico is the one below. Can anybody guess what it is?



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea922807c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"If you’ve ever eaten fish from the sea, especially an older or larger fish, you’ve probably been exposed to the pollutant mercury. It’s invisible, odourless, and dangerous. When ingested by humans, mercury is a neurotoxin, attacking the brain and nervous system, and the development of babies and infants can be particularly hampered.  Whereas most of us are at liberty to adapt our diets, people living in the Arctic strictly rely on marine wildlife for food and, unfortunately, mercury levels in animals such as seals, beluga whales and polar bears are among the highest worldwide. It’s harming birds too – recent research shows that endangered ivory gulls have 50 times more mercury in their feathers than when records began 130 years ago. The stuff we’re really concerned about is methylmercury, the most toxic form of the element that accumulates in those animals. And there’s a mystery here – while emissions from factories and power plants have pumped a lot of mercury into the Arctic, we still know little about exactly how this is converted to methylmercury.  One idea is that methylmercury is produced in the oceans. Inorganic mercury, natural or man-made, sticks to algae in the surface waters. When these algae sink to the deep ocean, microbes are already waiting to eat them. We believe some of these microbes can convert the inorganic mercury to methylmercury.  Methylmercury is then passed along the food chain via a process known as bioaccumulation. Algae pick it up from the water, are eaten by zooplankton (krill) which are eaten by smaller fish, which in turn are eaten by bigger fish – at each step, methylmercury gets many times more concentrated, reaching dangerous levels in top predators such as seals, polar bears or even humans. But where does all this mercury come from? Mercury is unique – it is the only heavy metal that is present as gas in the atmosphere, where it stays on for an average of about a year. As a consequence, it can travel across the globe, including to the remote Arctic. One theory is that the Arctic is a global sink for increased man-made mercury emissions from North America, Europe and now Asia, and possibly causing the high mercury levels in arctic animals. However long-term data on mercury levels in Arctic animals don’t always match up with increasing man-made emissions. Other factors must be at play.  Though scientists have generally focused on atmospheric mercury sources over the past decade, models suggest that atmospheric emissions can’t account for all the mercury. A large source of mercury to the Arctic Ocean is missing.  Rivers could provide such a source, especially during spring flood of Siberian rivers. While this finding is exciting, Siberian rivers and the Arctic Ocean itself remain under-sampled. We actually have to go to Siberia during the spring floods and measure mercury to know what is coming out of the rivers, and how far it travels into the sea.  That’s why we are now investigating the mercury discharge, especially during the spring flood, of one of the largest Siberian rivers, the Yenisei. We must find out where, how and what is turning inorganic mercury – natural or man-made, from the atmosphere or from rivers – into its most toxic and bioaccumulating form, methylmercury. We already know that inorganic mercury in the oceans has increased because of man-made emissions. At the same time a warming climate and melting sea ice is likely to play havoc with Artic algae and methylating microbes. But what will this mean in the future? A previous Polarstern cruise in 2011 gave us some first insights. In research published in the journal Scientific Reports my colleagues and I presented the first full-depth high resolution profiles (> 5 km-depth) of total mercury and methylmercury in the central Arctic Ocean (79-90°N).  Our findings suggested methylmercury production in the Arctic Ocean is highest in the area of thinner and younger sea ice, probably due to a higher accumulation of algae in these areas. Methylmercury concentrations peak shallower than in the other oceans (150 m in the Arctic versus roughly 1000 m in the Atlantic). The shallow methylmercury production, close to the surface where algae thrive, likely results in enhanced biological uptake at the base of the arctic food chain. While these first few results might hint an alternative explanation for the high methylmercury levels of Arctic wildlife, many questions remain open. That’s why this summer I’ll spend two months investigating mercury changes in the Arctic while on board the German icebreaker Polarstern sailing to North Pole. Polarstern will be joined by research ships from the US and Canada – the operation, organised within the international program GEOTRACES, will be the largest exploration of mercury (and other elements) in the Arctic Ocean.  This effort involves many different research teams and will take a whole lot of coordination. At sea we also need to keep each other up to date about what we’re doing. To master these efforts the mercury teams on the three research ships connect online and will be keeping each other and the interested public informed on ResearchGate, a professional network for scientists. The three mercury teams will map mercury and methylmercury distributions throughout the Arctic Ocean, and merge their data. This is critical to understand marine methylmercury production and to predict the impact of ongoing climate changes on the Arctic mercury cycle. After all, global warming is bad enough as it is, without contaminated fish to worry about, too."
"
We had ""yellow"" level geomagnetic activity on the sun last night, and more may
come tonight and tomorrow night. Its coming from Sunspot 953, which is about 3 times the size of the Earth.

Sunspot 953 is crackling with mild
B-class solar flares. Credit: SOHO/MDI 

Image of sunspot 953 taken today by Sebastien Kersten of Le Cocq, Belgium:
Here is the dispatch:
From: solarxactivity@bbso.njit.edu
Date: April 28, 2007 9:24:59 AM CDT
To: xxxx@rice.edu
Subject: BBSO Solar Activity Warning 28-APR-2007 14:19:18 UT
Region NOAA 10953 is currently beta-gamma magnetic class, and may increase in complexity.
The region is bright in H-alpha as well. This region has a chance of producing M-class
events.
NOAA 10953, S10 E41.  Beta-gamma region. Position as of April 28, 2007 at 13:30 UT.
And this is in the middle of our solar minimum, indicating our sun still has a few belches to pass out before completely settling down.
One of the best tools we have is the ACE Spacecraft, which monitors the sun 24/7 and provides us with a plethora of real-time data, of the magnetic
field, the solar wind, and  inter-galactic cosmic ray counts.

For the latest ""dial"" info (including our ""space weather stoplight"") go to
http://space.rice.edu/ISTP/dials.html
For the latest 10-minute averages of the Boyle Index from realtime ACE
spacecraft data, go to http://space.rice.edu/ISTP/wind.html
Some guides to interpret the gauges
If the hourly-average of the Boyle index exceeds 110, then Kp 4-6 storms
will likely occur within the next three hours
If the hourly average of the Boyle index exceeds 200, then major magnetic
storms will occur within the next three hours
If the hourly average of the Boyle index exceeds 250, major low-latitude
auroras will occur within the next three hours.
A magnetic storm generally occurs about an hour or two after the CME arrives at Earth, which is roughly 26-48 hours *after* a major solar flare. The Boyle Index is derived from real-time ACE spacecraft data, which gives about 45 minutes of warning before it hits the Earth.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6a441e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The climate activist Greta Thunberg has said she has applied to register her name and that of the Fridays For Future movement she founded in 2018, which has gone global and catapulted her to international fame. The move would allow legal action against persons or companies trying to use her name which are not in line with her values or that of her movement, she said. “I assure you, I and the other school strikers have absolutely no interests in trademarks. But unfortunately it needs to be done,” she said on Instagram on Wednesday. Thunberg said she had also applied to trademark Skolstrejk for klimatet (school strike for the climate in Swedish) – the wording on the placard she has held since she started her protest outside the Swedish parliament in 2018. “My name and the #FridaysForFuture movement are constantly being used for commercial purposes without any consent whatsoever. It happens for instance in marketing, selling of products and people collecting money in my and the movement’s name,” she wrote on the social network. Thunberg, who took centre stage at the World Economic Forum in Davos this month, and her fellow young activists in the movement want politicians to listen to climate scientists and take action to tackle global heating."
"Hedgehogs have acquired a surprisingly cuddly reputation for a nocturnal predator festooned with spines and fleas. Perhaps it is their habit of curling up, their classic cute faces with turned up noses, or their bumbling manner. While foxes are cunning, hares are mad and weasels are weaselly the hedgehog is not at all threatening, but instead they are portrayed as either a stout yeoman of olde England or, if female, a maternal Mrs Tiggy-Winkle.  All of this makes hedgehogs the ideal icon for wildlife hospitals such as St Tiggywinkles or road safety adverts. My memories of 1970s hedgehogs are not so much of the stout, aproned Mrs Tiggywinkle but what appeared to be a nightly suburban back garden lawn swingers party. Hedgehogs were routine visitors to our small 1960s housing estate garden, their noisy romantic trysts dismissed as “oh its just the hedgehogs at it again”. The prospect that hedgehogs would now warrant specific conservation action would have seemed far fetched. Judging by the noise they seemed determined to conserve themselves with gusto.  Times have changed. In 2014 hedgehogs joined the list of UK Biodiversity Action Plan species, under their more official name of the Western European Hedgehog, and, more informally, 2015 sees the launch of the first Hedgehog Improvement Area in the Birmingham suburb of Solihull. Hedgehog numbers in the 1950s were put at more than 35 million, but contemporary surveys suggest that there may not be even one million. On the BBC’s Springwatch series, hedgehogs now star not as a familiar creature we can all see but as one of the endangered, heading towards extinction. Most species declines are complicated and the hedgehog’s is no exception. The animals have lost countryside habitat as the fine grain of small fields, woods and old hedgerows was industrialised, and in cities their garden homes have been tided, paved and surrounded by impenetrable fences. Perhaps we can also point to climate change, declining prey populations and predation by badgers. In addition hedgehogs are notoriously vulnerable as they cross the road – hence their resonance for road safety campaigns.  Hedgehogs would do well to heed the advice of their animated road safety avatars and a recent study of a remnant population in Regent’s Park in London suggest that they might be doing just that. The park staff, working with The Royal Zoological Society of London, have been tracking their hedgehogs’ nightly movements using radio and GPS trackers. The results show the hedgehogs avoid crossing the surrounding roads and don’t stray beyond the safety of the park. All living creatures would do well to avoid city roads. It is not hard to see the rapid selective advantage of not crossing. Oddly, road kills can be evidence of improving circumstances as revealed by hedgehogs in the Republic of Ireland. You may have been perturbed by the earlier mention of badgers as a threat to hedgehogs, after all badgers are meant to be brave and noble. They are also indifferent to a hedgehog’s spines and chomp through them.  In Ireland badgers have been subject to a cull to try and limit the transmission of bovine TB. In areas where badger numbers have been culled the numbers of hedgehog road traffic victims has increased compared to control regions with no badger reduction. Similar results are coming through from the UK’s own badger cull. In the UK the 1990s saw marked increases in otter road casualties as they recolonised much of England and beavers may be next as their populations expand. Road kill may not be the ideal methodology but can be a useful and effective bell weather of change. The hedgehog’s fate is not the only recent decline of a once familiar creature. Just look at house sparrows, starlings or wall brown butterflies – all of these creatures were once so commonplace you’d not give them a second glance, and all are now in trouble. From a species point of view it pays to be rare. We are not bad at the last ditch, highly expensive defence or reintroduction of white tailed sea eagles and large blue butterflies. Meantime the hedgehog’s supporters are rallying. This year will see the first “Day of the Hedgehog”, a follow up to a national hedgehog survey. If only they hadn’t made the event sound like a 1970s low-budget horror movie. Or maybe they know something about those back garden noises best left unsaid."
nan
"
Share this...FacebookTwitterThe two recent dry summers seen in Europe have led to alarmists believing that the climate doomsday has arrived. But The European Institute for Climate and Energy (EIKE) looks at the past to see if this sort of thing is really unusual.
=======================
German forests growing much faster today than 1000 years ago. Photo: NTZ
=================
Dry summers as a doomsday scenario – are they really something new?
By Axel Robert Göhring
(Text translated/edited by P. Gosselin)
Drought completely normal during High Medieval Ages.
Researchers from the German Department of Ecology and Ecosystem Dynamics at the University of Greifswald have shown that drought in the High Middle Ages was completely normal during the summer. Even if hardly any real scientist dares to say anything against the climate madness, many do their work properly and deliver many small mosaic pieces for dismantling all the fraud.
Last spring, however, one could hear “top physicist” Harald Lesch at the Markus Lanz’s ZDF show claim how climate change would hit quite badly in summer, how the drought of the “record summer” 2019 would have violent effects, especially on the holy German Forest (forest die-off scare came knocking again…).
So what about drought in the holy German Forest 2019? Is it real, or “interpreted”?
Well, it’s probably real. But why not? In summer it is hot and dry even in the temperate climate zone of Europe. Mr. Lesch & Co. showed a heat peak and claimed it is man-made climate change. And when a cold peak appears, then it is only weather – or even proof of climate change. Weather extremes are somehow more frequent today.
Dry summers in Europe not uncommon, new study


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Biologist Martin Wilmking and his team from the University of Greifswald in the German state of Vorpommern now have shown that dry summers a thousand years ago were not uncommon in northern Germany. In fact, they were much warmer than today – and this without combustion engines, industry and motor traffic.
Prof. Wilmking and his biologists evaluated so-called proxy data, i.e. verifiable effects of climate in animate or inanimate nature. Specifically, the team worked on annual rings in living beech trees and thousand-year-old archaeological timber: the long established field of expertise is dendroclimatology (Greek: dendron – the tree).
Trees growing faster today
The authors prove once again that our forests are growing much faster today than in the past, because agriculture (also traffic & industry) provides them with a lot of fixed nitrogen (ammonium salts).
The modestly increased CO2 content of today’s air also allows the trees to open the stomata of the leaves for a shorter period of time, thus limiting water losses. In other words, our industrial civilization is considerably HELPING the forest by supplying it with building materials and  even water indirectly. This is nothing new for avid EIKE readers, as we have pointed out more than once that the planet has become much greener in recent decades.
Often dry in the prosperous High Middle Ages
If one includes the faster growth of today’s trees, one can conclude in comparison using the annual ring curves of the historical woods that it was often dry in summer in the High Middle Ages. Even the Rhine, the largest river in Europe, became dry near Cologne. Yet the High and Late Middle Ages were not a phase of decline like the Early Middle Ages. On the contrary, the courtly knightly culture flourished in Western and Central Europe. There are tens of thousands of stone castles in Germany, Switzerland, Bohemia and Austria – several in the Saale valley near EIKE’s home city of Jena.
And if you take a boat trip on the Rhine or Moselle rivers – ruins of stone castles can be seen everywhere. They all date back to the time after 1000 AD. The stone castles are a testimony to a significant increase in Europe’s economic performance, which can be traced back to the warmth of the Climate Optimum. The development lasted until the late Middle Ages, when it then became much colder again.
Politically correct theories assume that the booming economy overtaxed nature and thus undermined itself. Certainly not wrong, but without the cold, nature could have recovered faster from the overexploitation.
Warmer is better
Also findings from graves, for example near Berlin, prove that Brandenburg citizens from the Renaissance period were significantly sicker than their ancestors of the High and Late Middle Ages. The saying rings true: “Cold is bad, warm is good.”
Share this...FacebookTwitter "
"

The California legislature may want to revisit the wording of their proposed ban on incandescents (AB 722). California assemblyman LLoyd Levine, a Democrat from Van Nuys in Los Angeles, wants to make California the first to ban incandescent light bulbs (by 2012) part of its new initiatives to reduce energy use and greenhouse gases blamed for global warming. But somebody hasn’t thought this through completely.
Why do I suggest a change? Two reasons: 1- There’s a new efficient challenger to the old tungsten filament light bulb. 2- The Compact Flourescent Lamps touted as “Eco Bulbs” have a small amount of mercury an other heavy metals in them, making disposal a problem. Some landfills won’t take them!
GE has announced an advancement in incandescent technology that promises to increase the efficiency of lightbulbs to put them on par with compact fluorescent lamps (CFL).

The new high efficiency incandescent (HEI(TM)) lamp, which incorporates innovative new materials being developed in partnership by GE’s Lighting division, headquartered in Cleveland, Ohio, and GE’s Global Research Center, headquartered in Niskayuna, NY, would replace traditional 40- to 100-Watt household incandescent light bulbs, the most popular lamp type used by consumers today.
The new technology could be expanded to all other incandescent types as well. The target for these bulbs at initial production is to be nearly twice as efficient, at 30 lumens-per-Watt, as current incandescent bulbs. Ultimately the high efficiency lamp (HEI) technology is expected to be about four times as efficient as current incandescent bulbs and comparable to CFL bulbs. Adoption of new technology could lead to greenhouse gas emission reductions of up to 40 million tons of CO2 in the U.S. and up to 50 million tons in the Eeropean Union if the entire installed base of traditional incandescent bulbs was replaced with HEI lamps.
So take note California assemblymen and assemblywomen, how about mandating a level of lighting efficiency for bulbs rather than assuming that innovation of older technology can’t happen?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7e64554',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"Proposed changes to building regulations in England are likely to make buildings less energy efficient not more, a group of leading architects and engineers has warned.In a damning assessment of proposals to reform Part L of the Building Regulations, which sets the minimum energy performance standards for new dwellings, a growing coalition of professionals has described the changes as “a step backwards, in a climate where we need a huge leap forward”.“The proposals are framed as an improvement, but they actually represent a reduction in the energy performance standards of buildings,” says Clara Bagenal George, a building services engineer at Elementa Consulting and founder of the London Energy Transformation Initiative (Leti), a voluntary group of more than 1,000 architects and engineers that has been calling for radical changes to how building energy consumption is assessed.Under current regulations, all new building designs are assessed against a “notional” benchmark design, using parameters such as the thermal performance of materials, the orientation and size of the windows, airtightness and heating and ventilation systems (pdf). The proposed building must meet the performance of the notional design to pass the test. Critically, the new changes to the regulations would remove something called the Fabric Energy Efficiency Standard, meaning a building designed next year could be allowed to perform much worse than one built in 2013, when the current standards were introduced. Similarly, a building that would fail to meet the current regulations would pass under the new system. Part L applies to all buildings, although the current consultation covers only dwellings.Secondly, Leti warns that a proposed emphasis on overall carbon footprint will help to mask the actual energy performance of new homes. The government intends to introduce a new factor into the assessment, related to the energy efficiency of the grid, not the building itself. Because the National Grid has been rapidly decarbonising over the last few years, as more renewables have been connected, the new method of assessment would show that a home produced lower carbon emissions than before – despite the design being exactly the same.“The new proposals totally mask the actual energy efficiency of a home,” says Clare Murray, head of sustainability at architecture firm Levitt Bernstein and member of Leti. “They make the building look like it is performing better, when the reality is it could be much worse.”  At a time when the built environment accounts for 40% of UK carbon emissions, a growing number of architects, engineers, planners and developers are demanding urgent change to the regulations to insist on more stringent energy standards. The proposed reforms of Part L are a missed opportunity to properly address the climate emergency, they say.“It’s a massive disappointment,” says Joe Giddings, co-founder of the Architects Climate Action Network (Acan), a campaign group formed last year to address the twin crises of climate and ecological breakdown. “From disregarding the performance of a building’s fabric to ignoring the embodied energy of materials, the proposals represent a total loosening of regulations. And it’s all hidden in a dense consultation document that seems designed to confuse.”Both Leti and Acan have also criticised proposals to remove local authorities’ existing powers to insist on greater energy efficiency than the national building regulations demand. Around 65% of local authorities have declared a climate emergency, and many have already introduced more stringent energy standards into the planning process, but all this would be overwritten by the new lower national regulations.“Not only are the building regulations going backwards,” says Murray, “but local authorities won’t be able to set their own rules appropriate to their areas. It will roll the whole country backwards.”Leti wants to see a fundamental shift towards monitoring how buildings actually perform in use, away from the current system of simply measuring abstract design factors in a vacuum, before the dwelling is built. Their research has shown that many buildings don’t come anywhere near meeting their purported Energy Performance Certificate (EPC) band, the official rating of a property’s energy efficiency. Equally, the way the rating system calculates heat loss and gain means it can’t detect factors such as more insulation and better airtightness, so good buildings end up being rated lower than they should be.“The current system is simply not fit for purpose,” says Bagenal George. “It’s the wrong set of tools, as it was never designed to achieve net zero carbon.” The UK has a legal commitment to achieve net zero carbon by 2050, but Leti believes the new building regulations will knock it hopelessly off course. It is urging people to respond to the consultation on Part L by the 7 February deadline. On Monday, it will launch its own 150-page Climate Emergency Design Guide, along with an equally hefty Embodied Carbon Primer, which it hopes will provide practical solutions for architects, clients and local authorities to achieve a zero-carbon future.“At the moment, our regulations simply aim for a ‘percentage better than bad’,” says Bagenal George. “We want to redefine what ‘good’ looks like and set out how we can get there.” • This article was amended on 27 January 2020. An earlier version said the proposed changes to building regulations applied to “England and Wales”. Housing is a devolved matter and the Welsh government has launched similar, but separate, proposals."
"As the Brexit negotiations wrap up and Theresa May’s deal is lambasted by Remainers and Leavers alike, it’s still far from clear what the future holds for the United Kingdom. On March 29 2019, it is due to leave the European Union. Brexit is the first time a member state has voted to withdraw from the EU and it has caused a geopolitical earthquake, unleashing uncertainty in the UK and abroad. We don’t know what the impact on the UK will be when (and if) it actually leaves the EU. If it does so on poor terms, or via the still possible “no deal” eventuality, there are a wealth of devastating projections which may materialise. The only thing that we can be sure of is that Brexit represents a moment of huge social, political and economic rupture. However, history tells us that such moments are also moments of opportunity for radical departure from the status quo. Let’s be frank, Brexit is not a progressive endeavour. It threatens social and economic turmoil in which the most vulnerable in society will – as always – be the hardest hit. Leaving the EU could jeopardise benefits to UK citizens in the form of workers’ rights, environmental protections and food standards. The political climate outside of the EU also offers an increasingly undesirable community of potential allies and traders, dominated by the rise of the far-right in North and South America. On the other hand, uncritical adoration of the EU overlooks the reality of what the Greek economist Yanis Varoufakis has described as “a regressive set of vile institutions”. It cannot be denied: the EU is a large and brutal force for neoliberalism. From this critical stance on the EU, I still voted Remain in the referendum. I believed then – and still believe now – that regressive forces will profit from the UK’s exit and that the vulnerable will suffer. So, as we approach the March deadline, if the UK does indeed crash out of the EU, the left needs to be prepared with visions of alternative futures, and be ready to fight for them.  The realities of a post-Brexit UK appear bleak, certainly in the short term. But separation opens the door for alternatives to the dominance of free-market fundamentalism. We could move from a society centred around financialised capital and the City of London, to one that promotes social and environmental justice in the UK and internationally. Reports claim that Brexit will mean lower levels of economic growth for the UK. For politicians this is a horrifying prospect. But falling growth need not be feared, if it is integrated within a broader transformation of society. The degrowth movement emerging amongst academics and activists argues that the logic of infinite growth is driving ecosystem collapse and climate breakdown. As stated in the latest IPCC report, we now have only 12 years to radically restructure society to cap global temperature at 1.5 degrees Celsius above pre-industrial levels. If we fail, we will face catastrophic climate impacts.  Degrowth argues that the wealthy and heavily polluting countries of the global north – such as the UK – must undergo a phase of managed and socially equitable economic contraction. This is necessary to downscale rich economies to within safe ecological limits. The need for endless economic growth pushes us to produce more, consume more and make more profit. It has left our society overworked, over-stressed and plagued by extreme levels of inequality.  These dire social conditions have been blamed for the Brexit vote itself and unlimited growth also fuels climate breakdown, with the UK as a big contributor to global carbon emissions. Simply, our slavish devotion to growth is making us miserable and destroying the planet. Degrowth could liberate us by arguing that more growth is not the solution, but the problem. We can and must live better with less, shared more fairly. Degrowth would rid our society of pointless production and consumption. We could say goodbye to “bullshit jobs” – the pointless make-work that keeps workers stressed without any obvious value to society beyond enriching corporate elites. Production and consumption could be organised in service of social and environmental well-being rather than profit. This degrowth transition could be pursued through ideas which confront the relentless treadmill of work, such as a four-day week. Poverty and inequality could be tackled by implementing a universal basic income and a maximum income. A fundamental decentralisation of the UK’s political and economic landscape could end London’s dominance by distributing more democratic autonomy to the regions. Degrowth thus acknowledges that liberating society from the growth imperative is not only an ecological necessity, but also loosens the grip of the capitalist wage-labour market. This frees people to dedicate more of their lives to the things that really matter to them. Is degrowth a likely future for the UK after Brexit? Certainly not in the short term. But, as Brexit and climate breakdown destabilise our politics, nothing much is certain. Only that we must be prepared with visions of a better future, and be ready to fight for them."
"One of the principal arguments in favour of HS2 was the positive effect it would have on the environment – and this was rooted in the belief that high-speed electric trains could help the UK cut its carbon emissions. The rail project, supporters have argued, will lessen demand for carbon-intensive air travel, road freight and car journeys by linking northern England – and potentially Scotland – with the Midlands, London and HS1 to the Channel tunnel and continental Europe. Providing low-carbon alternatives is urgently needed because transport (mostly road) is now Britain’s largest greenhouse gas emitting sector, accounting for 28% of all GHG emissions in 2017. The government has a target of net zero emissions by 2050, but there is disagreement over whether HS2 will really help deliver. At face value, the project should be a good way to cut carbon. Electric trains, particularly if powered by renewable energy, provide low-carbon transport. HS2 claims it can achieve 8g of carbon emissions per person per km. The same journey by car would generate 67g of emissions – and by plane, 170g. But carbon modelling – predicting a new infrastructure’s carbon emissions – is, like all future-gazing, imprecise and dependent on computer modelling. The government’s own calculations for HS2 suggest its carbon emissions could exceed potential savings, even over the railway’s projected 120-year lifetime. HS2 will not cut carbon emissions. According to HS2’s own forecasts, even over 120 years, its overall construction and operation cause carbon emissions of 1.49m tonnes of carbon dioxide equivalent. This represents just 1.18% of Britain’s annual transport emissions, but critics say it is still an increase when emissions need to be falling rapidly to reach net zero. HS2’s construction requires vast quantities of concrete and steel, as well as diesel-powered machines moving millions of tonnes of earth. A 2019 report for the High Speed Rail Group (HSRIL), which represents companies with an interest in high-speed rail, estimates construction could be reduced by 20% to 30% with low-carbon innovations, such as hydrogen-powered floodlights and hybrid excavators. HS2’s calculations show its emissions will be offset by the wider decarbonisation of transport it helps create. Some road freight will move to rail as HS2 will enable the wider rail network to take more freight. There will also be a “modal shift” – people choosing to travel by HS2 instead of driving or flying – but this is likely to be small. The Department for Transport suggests only 1% of HS2 passengers will be people who would have flown, and 4% those who would have driven. Some analysts argue that HS2 has presented an overly pessimistic carbon-saving forecast because it is obliged by law to give worst-case scenarios. The HSRILG report points out HS2’s forecasts of 4% and 1% are based on assuming that driving and flying become more affordable while rail fares increase above inflation. In practice, the European average for high speed rail modal shift is 15% from cars and 30% from planes. Furthermore, HS2’s forecasts do not include carbon-saving from increased use of local passenger rail, with HS2 freeing up lines such as the west coast mainline to provide better local services. Julia King, the deputy chair of the committee on climate change, reviewed HS2’s carbon forecasts for Tony Berkeley’s minority report on HS2 and judged them “sensible and conservative” – they do not include highly speculative future scenarios, such as HS2 leading to fewer new roads. If HS2 was well integrated with the European high speed network (and critics say it is not, because it does not connect to HS1) it could become part of a European-wide system saving up to 5m tonnes of carbon dioxide equivalent by 2050 if all journeys under 1,000km moved to rail. Potential emissions Construction: • Huge quantities of steel and concrete including concrete slab-track • Moving construction materials to site • Tunnelling (more carbon-intensive than open-air construction) • Construction machines • Removing soil by truck • Manufacture of rolling stock • Journey to work of HS2 employees Operation: • Power source not guaranteed to be renewable. Speed of HS2 requires more power • Ongoing maintenance • Increased car journeys to HS2 stations • HS2’s better airport connections could increase flying • Domestic flights reduced by HS2 could lead to increase in international routes Potential savings • Increased rail capacity shifts freight from road to rail • Increased capacity leads to more local/regional rail journeys • Modal shift with travellers choosing rail over more carbon-emitting road • Travellers also switching from flying to high-speed rail • Carbon sequestration from tree-planting • HS2 could be powered by all-renewable energy • HS2 prevents other carbon-intensive infrastructure projects Sources: HS2 Limited, 2019, High Speed Two phase 2a, Informationa Paper, E27: carbon Friends of the Earth, Opportunity Costs of HS2, 2019 Lord Berkeley’s Dissenting Report, 2020 [Size of emissions not included because different scenarios give different estimates; figures have not been modelled for all these factors] But Lord Berkeley and other critics argue there are risks that HS2 could increase emissions. More people might drive to HS2 “parkway” stations. And by connecting airports more effectively in London, Birmingham and Manchester, HS2 could lead to an increase in flights. If HS2 cuts demand for domestic flights, says Friends of the Earth, that could simply encourage the aviation industry to switch domestic flight slots to more profitable, and carbon-intensive, international routes. HS2 could be made less carbon intensive. Critics such as Berkeley point out that if HS2 was not built to such a high specification (with top speeds supposedly in excess of continental high speed rail), emissions would be reduced both in operation and construction; it wouldn’t require carbon intensive concrete slab-track, for instance. An independent report by the rail consultants Greengauge in 2012 said HS2’s sustainability would be maximised by reducing its top speed, fully using freed-up capacity on the existing rail network, and creating city-centre stations rather than edge-of-town parkways. Supporters claim HS2’s plans for parkway stations are already morphing into “sustainable urban extensions” with high-density housing and tramways. Ultimately, HS2’s future emissions will depend on wider – and more joined-up – transport and energy policies. It would use less carbon if there was a decarbonisation of electricity supply (rather than another dash for gas), and coherent national policies to reduce car use and air travel.  Friends of the Earth and other critics highlight the “opportunity cost” of HS2: its £106bn price (at least £5bn a year for 20 years) will lead to less investment in more effective carbon-saving transport, such as regional rail, buses, cycling and walking. Berkeley concluded that electrifying existing railways would have a much greater environmental benefit. Friends of the Earth suggest an even less glamorous low-carbon option: making all UK buses free would cost £3bn a year."
"Scientists, environmentalists and animal rights activists have said it for many years. Now conclusive analysis has confirmed their argument.  The global meat industry not only damages our health and is ethically dubious – it is unsustainable because of the damage it does to the environmental prospects of our planet.   Yet politicians have little desire to do anything meaningful about it. Instead, they have a long history of ignoring or suppressing inconvenient evidence that is detrimental to the major industries of a free market economy.  But we need them to speak out. Back in 1722, when Dutch explorers landed on Easter Island in the south Pacific, they found a human population in terminal decline. The Rapa Nui people had deforested most of the island, and the variety of plant and fauna had considerably decreased.  Left alone for centuries, and without governmental legislation to protect the environment from human behaviours, the inhabitants of the island had been slowly committing ecocide against the terrain that sustained their very existence.  This historic example – and our current relationship with the environment – present interesting questions about human denial, idleness and avoidance within the individual and collective psyche. It also suggests that only compassionate authoritarianism, which holds that our ecosystem is more important than individual and collective egos, can prevent us from our current path towards global ecocide.  The meat industry is one of the largest political lobby groups across the world, providing financial support to many mainstream political parties and their candidates. Politicians, many of whom are meat eaters, stay clear. They likely conclude that confronting such a powerful interest group is not in their career (or meal time) interests.  The mainstream media also regularly falls short. Earlier this year, the BBC’s Today programme included an interview with a sheep farmer from Northumberland. It followed publication of a study which pointed to its detrimental impact on the environment. The interview was essentially the meat industry’s PR response – broadcast on Radio 4’s flagship news show.  In the programme, the farmer was referred to as a “shepherdess”, and her work described in terms of romantic walks on windswept moors. The discussion was emotional rather than rational, with the main argument of the interviewee being that if the meat industry was to decline it would be “very sad”.  She was then given a platform to make claims, without challenge, about what she thought were the main causes of environmental degradation (unsurprisingly, not the meat industry).  Such instances regarding environmental issues are unfortunately all too common. The Today programme could have (instead, or in addition) interviewed an independent academic on the matter. But it appears to have considered the opinion of a commercial sheep farmer to be at least on a par with the latest scientific evidence.  Indeed, my collaborator Rachael Hillyer and I have found that most politicians and mainstream media continue to place environmental concerns in a sphere of debate where industry interests are presented as having equal importance as the future of the planet.  This at a time when scientific evidence on the environmental effects of the meat industry, and how those effects can be reduced, has been compellingly presented. That this debate continues to sit in the sphere of “legitimate controversy” is a bit like having a debate in 2018 on whether smoking is detrimental to your health.  But then democracy has never been very good at tackling the global issue of environmental degradation. Instead politicians often go to great lengths to avoid the topic. When they do engage, they do so begrudgingly, putting all their rigour into a division of responsibility that excuses themselves to the greatest extent.    On the whole, democracies are dominated by chronic short term decision making. And while they often act as safeguards to individual human liberties, democracy, and its preference for compromise, are often part of the problem when it comes to the environment – the biggest issue of them all.  Politicians avoid the reality that only immediate alterations to human behaviour can prevent this crisis. Put simply, the planet urgently needs more compassion for the environment and much less individual ego. The weight of democratic political experience also sits heavy on the minds of politicians. Previous democratically elected leaders have tried to persuade their electorates to think more collectively and to consider the environment before their own selfish pleasures.  US President Jimmy Carter (1977 – 1981), for example, was a keen but moderate environmentalist. Despite his considerable personal wealth he led by example by living modestly, and tried to encourage Americans to lower their carbon footprint and energy consumption.  However, it turned out that America did not like being told to rein in their habits – and Carter was decisively beaten by Ronald Reagan in 1980.  Reagan’s neo-liberal campaign message was: “Make America great again.” Yes, the very same message used by Donald Trump during his presidential campaign. It was a campaign which emphasised the primacy of the economy over the environment.  To this end, democracy cannot fix our environmental issues. Because for every democratically elected environmentally conscious politician in a leadership position, there is another one waiting in the wings to denounce and depose them for economic weaknesses. And in doing so, to relinquish the voters of their duty towards the upkeep of this planet."
"

Hillary Rodham Clinton, the secretary of state who no doubt thinks of herself as “fourth in the line of succession,” tells a European audience how the Obama administration will pass an agenda that Americans have previously rejected: “Never waste a good crisis … Don’t waste it when it can have a very positive impact on climate change and energy security.”   
  
  
As I’ve written several times, governments throughout the decades have taken advantage of wars and economic crises to expand their size, scope, and power. Bob Higgs wrote about “Crisis and Leviathan” long before Naomi Klein called it “The Shock Doctrine.”   
  
  
But the striking thing about the Obama administration is that they openly acknowledge that’s what they’re doing — using a crisis to ram through their entire policy agenda while people are in a state of panic. Projects like national health insurance, raising the price of energy, and subsidizing more schooling — the three prongs of President Obama’s speech to Congress — have nothing to do with solving the current economic crisis. But the administration is trying to push them all through as “stimulus” measures. And they keep proclaiming their strategy.   
  
  
First it was Rahm Emanuel: “You never want a serious crisis to go to waste. And this crisis provides the opportunity for us to do things that you could not do before.” Then Joe Biden: “Opportunity presents itself in the middle of a crisis.” Not to mention Paul Krugman and Arianna Huffington. And now Hillary.   
  
  
Not since George Bush the elder told the media that his campaign theme was “Message: I care” has a president been so open about his political strategy. But these people are displaying a contempt for the voters. They’re telling us that we’re so dumb, we’ll go along with a sweeping agenda of economic and social change because we’re in a state of shock. They may be right.   
  
  
But voters and members of Congress should remember Bill Niskanen’s sobering analysis of previous laws passed in a panic.
"
"
Share this...FacebookTwitterNot long ago one (right wing) politician warned before the German Parliament that the bicycle as a means of transport was extremely dangerous – especially for children – and thus ought not be promoted.
“Highly impractical and dangerous”
This of course brought ridicule from the infallible leftists and greens – and yes, even from German centrists who have long become all drugged up on green and “climate protection”.

AfD parliamentarian Dr. Dirk Spaniel told before the Parliament: “Soberly considered, bicycles are highly impractical and dangerous.”
According to Spaniel, a transportation expert, a child transported on a bicycle is exposed to greater danger than in a car. On the Green Party’s vision of a bicycle utopia in Germany and the world, Spaniel mocked: “They want to draw an ideal fairy tale world here with bicycles, which do not exist in this form.”
Twice as likely to die on a bike
As much as the left and greens like to ridicule Dr. Spaniel’s claim, it is backed up by most studies.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




For example, the Washington Post here writes that “bikes are the most dangerous way to get around with the exception of motorcycles” and that in the USA, “you’re more than twice as likely to die while riding a bike than riding in a car, per trip” and riding a bicycle is “about 500 times more fatal than riding in a bus”. Here the WaPo cited according a 2007 study led by Centers for Disease Control and Prevention epidemiologist Laurie Beck.
Why so many German politicians are now striving to transport children using such a dangerous mode of transport remains a mystery. It’s one of the side effects of being drugged on green. In their doped minds, addicts dismiss all the risks and amplify the promised benefits.
Bicycle deaths rising in Germany
As bike riding increases in Germany, so do the accidents and fatalities. According to Spiegel, citing the Federal Statistical Office in Wiesbaden: “445 people died in accidents on a bicycle – 63 cyclists more than in the previous year and the highest number since 2009.”
“A total of 88,850 cyclists were involved in accidents on German roads in 2018,” Spiegel wrote earlier in 2019.  “That is around 11 percent more than in the previous year.”
25 times higher risk of injury
According to AfD Parliamentarian Dr. Dirk Spaniel: “Parents who transport their children on bicycles increase the risk of injury to them 25 times more than those who transport them by car.”
Time for parents to be responsible for their kids and to stop pretending they can be responsible for the climate.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterUsually shunned by the German mainstream media, today moderate, rational voices on the issue of climate change are beginning to be heard on the air waves once again. This may be temporary. We’ll just have to wait and see.

Leading German climate science critic Prof. Fritz Vahrenholt. Image: Die kalte Sonne
For example, just days ago, leading climate science critic Prof. Fritz Vahrenholt was interviewed by NDR German public radio’s Anke Harnack on the topic of climate change. As protests by yellow vests in France and angry farmers in the Netherlands intensify, perhaps the establishment in Germany is having second thoughts about going down the hysterical climate rescue path that has been forcefully advocated in Germany over the recent months.
Prosperity based on “reliable energy supplies”
In the interview Vahrenholt, a leading founder of Germany’s modern environmental movement, tells the NDR that following the demands made by Greta Thunberg would put global prosperity at risk and exacerbate world hunger. He says the amazing improvement human society has seen over the last 100 years is thanks to “reliable energy supplies”.
“Huge, huge difficulties”
“Shutting these down in 12 years would indeed throw us into huge, huge difficulties.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Vahrenholt says all the recent “panic is leading policymakers into making errors and will lead to disappointment for the youth because it is not doable.” He adds changing over the green energies is needed ultimately, but this cannot be done over a short time period of a decade or two. He says “we need two generations” to get off fossil fuels and that it’s going to require “more innovation and research in order to get CO2 emissions down to acceptable levels by the end of the century.”
Vahrenholt, the former director of renewable energies company Innogy, says he is also puzzled over why Germany refuses to do research on fusion and remains so fixated on unstable sun and wind. Vahrenholt was one for the 500 scientists who recently signed a letter to the UN declaring that the planet was not facing a climate crisis.
97% consensus claim distorted
On the claim 97% agree that man is behind global warming, Vahrenholt says this figure has been completely misrepresented, and that it is in fact “only a handful of scientists” who say that man is 100% responsible. Many scientists say that man is only partly responsible.
Chinese are laughing
The outspoken German professor of chemistry says giving in to the demands of the radical greens would lead to a deindustrialized Germany: “In the end what’s left is a deindustrialized Germany, and the Chinese are laughing their heads off.”
Leaders lack courage
On the large Fridays for Future protests, Vahrenholt says: “It’s not surprising because currently hardly a teacher, hardly a journalist, hardly a scientist has the  courage to say: ‘Dear millions of people, we find it nice that you’re concerned about the climate, but let’s really discuss among each other what really needs to be done, and how much time we have.’ This really annoys me. I may not always be right, but I’m pretty sure that the alarmists are not right.”
Share this...FacebookTwitter "
"

On Tuesday the Supreme Court will hear arguments in _South Dakota v. Wayfair,_ which will force it to decide whether to end the ban on states charging sales taxes on goods sold via the internet from retailers without a nexus in the state. Should it effectively reverse a pair of previous Supreme Court decisions and permit states to do such a thing, it will constitute a significant change in our economy—but those changes won’t include rescuing embattled terrestrial retailers or filling states’ coffers with new tax revenue.



The main outcome will be slower economic growth.



The Supreme Court’s 1992 ruling in _Quill v. North Dakota_ imposed a prohibition on states taxing the sales of remote retailers that exists to this day. The Court found at the time that it was impossibly complex for a remote retailer to know or compute the sales tax owed in thousands of different jurisdictions, and determined that if a retailer had no operations–such as a store or warehouse—in a state, then it did not make sense for it to pay taxes on sales made in that state.



The timing of the _Quill_ case proved to be propitious: Less than two years after its decision Jeff Bezos introduced Amazon to the world. Today, of course, internet retail is enormous, and many of the terrestrial retailers that are struggling these days blame the company for their woes.





The Supreme Court will decide whether to end the ban on states charging sales taxes on goods sold via the internet from retailers without a nexus in the state.



The states have been enthusiastic in having the court—or Congress—end that ban as well: They have blamed the expansion of internet retail for everything from their budget woes to climate change.



However, the notion that states will reap a revenue bonanza by taxing remote retailers is as dubious as the Glengarry leads. Every estimate that has previously been made of the revenue to be gained from such a tax vastly overstated reality, simply because most sales on the internet are already taxed (roughly two thirds, according to a recent study) and internet sales still comprise a small fraction of all retail sales—roughly 11 percent in 2016.



The Government Accountability Office estimates that ending _Quill_ would raise an additional $8 to $13 billion for the states, or roughly 1–2 percent of the $700 billion of revenue the states anticipate collecting in 2018. Or, put in a different context, it roughly amounts to Walmart’s annual tax bill.



And that revenue comes at a significant opportunity cost to the economy. A plethora of research over the last two decades has found that it has been the hyper‐​competitive retail economy of the U.S. that has driven much of the productivity gains that the country has achieved since the mid‐​1990s. Both Walmart and–next–Amazon have ruthlessly pursued methods to reduce costs and increase worker productivity, and they have forced their suppliers to follow suit.



Productivity growth is important because it ultimately determines a nation’s standard of living, economists believe, and the retail sector plays an outsized role in its determination. William Lewis, the founding director of the McKinsey Global Institute, and former Obama CEA chair Jason Furman have both concluded that the nation’s competitive retail sector distinguished us from most other developed countries, and that this is an important reason–if not _the_ main reason–that we have had greater productivity growth the last three or four decades.



Right now, Amazon is winning that productivity race hands down–its sales per employee exceeds $100,000, or twice that of Walmart. It is likely that as Amazon expands its terrestrial offerings and Walmart beefs up its online presence that this gap will shrink, but no one thinks it will disappear anytime soon.



If Amazon can withstand any competitive threat from Walmart, Target, or other big‐​box retail stores, then who _is_ the next retailer who has a chance of beating it at its own game? We can safely venture that it will originate as an internet retailer, much like Amazon did.



However, taxing remote retail constitutes a barrier to future internet retail startups. Amazon undoubtedly benefited in its early years from not having to pay sales taxes in most states when it was competing remotely from a relatively small number of distribution centers and faced more difficult shipping and return logistics, and that ultimately came to benefit consumers–that advantage helped Amazon to hasten the internet marketplace.



Today, Amazon now has a presence in all fifty states in the form of its logistics network and therefore pays sales taxes in each state that has one—to provide the level of service it feels compelled to offer in order to compete against Walmart and Target it has to have warehouses and stores and pickup facilities near its customers.



The retail sector of the economy has, of course, changed dramatically in the last quarter century and in ways that no one anticipated then, or even a decade later. Until the last decade or so retailers were, generally, either solely remote retailers or else sold their wares only via their stores. Today, almost all retail sales are completed by an entity that is an omnichannel retailer, with both an internet presence as well as one or more physical stores.



The notion—implicit in the plaintiffs’ argument—that there are distinct internet and distinct non‐​internet retailers does not at all reflect the reality today. Most existing internet‐​only retailers are small mom and pop businesses selling goods that aren’t readily found elsewhere–a chia pet in the shape of Jerry Garcia’s beard or vintage Buffalo Braves shirts, for instance. A sales tax on these operators reduces the breadth of goods available to consumers without increasing demand for anything from their terrestrial “competitors.”



And for all of the bluster and talk of Amazon acting like a monopoly, it is worth remembering that its annual sales are only one third that of Walmart and behind CVS as well, and it is unclear whether its retail operations—which it doesn’t break out from its other operations—have ever turned a profit for the company; its most lucrative division appears to be its cloud services.



The history of retail shows that a company can dominate the field for only a short period of time. Before Walmart and Amazon there was Sears, Kmart, JC Penney, Montgomery Ward, and Woolworths, each of which innovated the retail environment in some way and became the top dog in the market before being supplanted by a competitor. Today, these companies are defunct or nearly so.



It is a history that Bezos is keenly aware of: He constantly refers to his company being at Day One of its history, explaining that his goal is to forever maintain that same competitive culture so that it can resist the new competitors that will inevitably arise. It is a noble goal but an impossible one.



But one way Bezos can put off its future rivals is to make it more difficult for small companies to grow—and imposing a retail sales tax on even the smallest remote retailer presents a significant barrier for retail start ups. That is undoubtedly the reason why Amazon now _advocates_ for a sales tax on all internet retail.



Imposing an internet sales tax on remote sales won’t make us like France, which carefully circumscribes the retailers’ hours, prices, number of sales, and other behaviors in order to keep at bay the nonexistent bogeyman of unfair competition, but it’s a step in that direction.



State governments with a fiscal problem—and there are many, despite the fact that we are in the ninth year of an economic expansion and unemployment rates are nearing record lows—agitate for the right to tax out of state retailers because they need to place blame somewhere besides their profligacy and inability to coherently govern. Taxing out‐​of‐​state retail sales won’t fix anything–and it will hurt the economy to boot.



Despite the ancient principle of _stare decisis_ , the betting seems to be that this court will indeed rule with the plaintiffs and effectively overturn _Quill._ If so, the U.S. economy would be worse off for it in the long run.
"
"
Share this...FacebookTwitterTemperature stations along the coast of the Antarctic Peninsula indicate “marked statistically signficant cooling” has occurred since 1991, with the Larsen Ice Shelf cooling at a rate of -1.1°C per decade.

Image Source: Bozkurt et al., 2020
Bozkurt et al., 2020
“Observed near-surface temperature trends indicate important contrasts between summer and autumn for the period 1991−2015. A notable summer cooling exists on the northern peninsula (Frei and Marambio stations) and leeward side (Larsen Ice Shelf station). The largest summer cooling trend is observed at the Larsen Ice Shelf station [−0.92°C (10 yr)−1, p < 0.05]. On the other hand, in autumn, San Martin station on the central windward coasts exhibits the largest warming trend [+0.64°C (10 yr)−1 , p < 0.05]. Autumn warming is also notable at the other stations except the Larsen Ice Shelf station. At the annual time scale, there is a clear warming trend at San Martin station [+0.52°C (10 yr)−1 , p < 0.05], whereas at a close latitude on the leeward side the Larsen Ice Shelf station exhibits a marked statistically significant cooling [−1.1°C (10 yr)−1].”
Share this...FacebookTwitter "
"With their own sense of dark irony, the bushfires that have ravaged much of Australia over summer are now closing in on Canberra – just as our leaders prepare to return to the capital to put the nation back together. The profound human cost of lives lost, sacrifices made, wildlife destroyed and dreams shattered has moved the nation, the work of the volunteer firefighters tapping a spirit of mutuality and civic-mindedness that we thought we’d lost. The economic cost of the fires is still being reckoned: from insurance to tourism to ruined local economies, the numbers guys are starting to draft their response to Bill Shorten’s unanswered pre-election question: what is the cost of climate inaction? But what is less clear is the political impact the fires will have on climate policy.  Here are a few things we do know. The prime minister has taken a serious hit to his personal authority for his dereliction of duty before, during and since the fires. He limps back to parliament a diminished leader, facing twin crises of a looming pandemic and an orchestrated vote-buying scandal.  Yet as I pointed out last month, the PM is holding on to his base, aided and abetted by those who create a parallel universe where the fires are the work of greenies and arsonists and nothing the experts say will convince them differently. Despite these determined efforts to gaslight the debate, there is a sense that the summer carnage will shift the needle on climate change and our global laggard of a government will be compelled into more meaningful action. To understand the moment we find ourselves in, I’ll defer to a construct that political scientists use to model how political activism can deliver meaningful policy change: the Overton window.  The Overton window of political possibilities holds that for every issue, there are range of policy responses that sit on a spectrum from radical to sensible and back to radical. As a rule, support for these policy options will follow a bell curve: policies seen as sensible will also be the most popular. When a policy is seen as sensible and popular, it will offer a window for political action. In a static world this would hardly be a revelation – indeed it would be a recipe for both incrementalism and stasis. Except there’s a twist: the Overton window moves over time, influenced by public activism, policy advocacy and external events. That was the idea first laid down by Joseph Overton, head of a libertarian Michigan thinktank in the mid-1990s when trying to articulate a plan to deregulate education in that state. Rather than championing radical, unpopular ideas direct to legislators, he urged the long game, advocating instead to extend the window of what was acceptable, and only when it was in your zone would you pounce through it. Overton died in a plane crash a few years later and never lived to see the malign impact of his theory, but its thinking has driven the right’s success for more than three decades, from mainstreaming market deregulation to marginalising healthcare reform. The left has been less inclined to adopt the window, too often pushing for policy purity rather than the long game of moving the policy spectrum, although the successful campaign for marriage equality is a great example of how this type of fenestration can work. Indeed, the failure of the 2019 election campaign can be explained in part by Labor’s failure to situate their policies within this framework. Instead of demanding policy be set within the Overton zone, Labor presented a patchwork of radical deals without the necessary ballast to make them viable. So is there an Overton climate window? And if so, have the bushfires extended it? We have known for many years that there is passive support for climate action. Our benchmark polls show about 60% of Australians accept climate change is real and a similar number believe the government is not doing enough.  The problem has been when concrete policies have become contested: support for market mechanisms, when compared to the perceived costs in jobs and prices, left the Overton window shut. There has also been consistent support for renewable energy, but the cashed-up efforts from the fossil fuel lobby and their ties to media and government have provided an effective counterpoint to seriously scaling the industry in Australia. But as results in this week’s Essential Report suggest, now there is overwhelming support for concrete action to back renewables and increase targets, while a number of policies that have sat on the fringes of the debate now have majority support. On first blush these are strong numbers for comprehensive climate action, but I would lay down a couple of caveats before we break any glass. First, while the combined support for measures looks strong, only the accelerated development of renewable energy has more passionate supporters than passive. Additionally, by choosing not to offer the chance to give a “no response”, we were forcing a choice on the issue, rather than allowing a shrug of the shoulders. This in itself is an element of the political challenge. Other points worth contemplating are that longer-term options are more popular than shorter-term targets (compare the 2030 and 2050 targets), while proposals for the government to actually ban new coal mines garner much stronger opposition. Based on these findings, the Overton window then would be open around aggressive support for renewables and long-term emissions targets, perhaps with some specific measures to ensure mining companies contribute to the cost of fires. This is reinforced when you look at support for measures via voting intention, where a majority of conservative voters appear to be within the edges of the window. They are right on board with renewables, and while there is majority support for the next layer issues, that support slips below majority once targets become more ambitious and government action more punitive. The idea of pushing for centrist, reasonable and sensible policies may chafe when the world is on the brink. It does not dispel the need to campaign hard at the margins – climate rebellions and school strikes are essential to shifting the window to make other policy change possible. But the risk is to confuse the movement with the moment. If political change is the answer and Australia can’t wait until 2022, then locating the Overton window and finding a way through it now seems the only viable way forward. • Peter Lewis is an executive director of Essential, a progressive strategic communications and research company"
"
Share this...FacebookTwitterGerman climate blogger Snow Fan here presents some background on Australian bush fires. 
It turns out that the 1974/75 bush fires were considerably larger in area than the 2019/20 bush fires we have been witnessing.

The Australian bush fires of 2019/20 have seen an area as big as southern Germany (see above). But in 1974/75, they covered an area as large as France and Spain combined! Source: www.wetteronline.de. 
Snow fan writes:
On the completely exaggerated climate alarm in the German media on the current bush fires in Australia, a pleasantly objective report from WetterOnline: ‘In the summer of 1974/1975, an area in Australia burned to the tune of about the size of Spain and France. For the sake of perspective: Bush fires are generally nothing unusual in the Australian summer. Often large areas are affected. The last time a huge fire raged was in February 2009. The so-called Black-Saturday-bush fires killed over 170 people and destroyed 1800 houses. […] Since the beginning of the great bush fires in October 2019, more than 100,000 square kilometres of land burned throughout Australia, which is roughly the size of Bavaria and Baden-Württemberg combined. Thousands of houses were destroyed.'”
Bavaria and Baden-Württemberg have a combined area of around 105,000 square kilometers, so there’s no doubt this season’s bush fires have been devastating.
But WetterOnline reminds Australia has seen much worse:
 In the summer of 1974/1975 the flames burned over an area of about one million square kilometers. This corresponds to an area about three times the size of Germany.”
That means an area that is nine times greater than what has been affected this year! Back in 1975, however, atmospheric CO2 concentrations were BELOW the “safe” 350 ppm.
Share this...FacebookTwitter "
"At the southern tip of the Maldives, on the tiny island of Villingili, a patch of ground rises to tower a whole 2.4m above the sea. It’s the world’s lowest high point.  With most islands just a metre or so above the sea level, it is often suggested that the world’s lowest country may drown beneath rising sea levels by the end of the century. For tourists, this ranks the Maldives atop bucket lists of destinations to visit before they disappear. For the 400,000 people who live on the islands, things are rather more serious: rising sea levels could render them climate change refugees.  However, such scenarios of inundation and drowning assume that the land surface remains static and unchanged. But, what if the land could build vertically as sea level rises?  This is what colleagues and I have been examining in our research, now published in Geophysical Research Letters. We studied five reef islands in the southern Maldives and found that they were actually built when sea levels were higher than they are today. The Maldives is a nation of around 1,200 coral reef islands. Reef islands are unique landforms in that they are formed entirely of sediments produced by organisms such as corals, molluscs and gastropods that live on coral reefs in the surrounding waters.  However, this reliance on the coral reef for island-building sediments, combined with elevations rarely more than a few metres above the sea, means that reef islands are often considered among the most vulnerable environments to climate change, particularly to sea level rise. This is of particular concern for nations such as the Maldives that are built entirely on reef islands, and have nowhere else to go.  


      Read more:
      Venice flooding is getting worse – and the city's grand plan won't save it


 To improve predictions of how reef islands may respond to future environmental change, it is important to understand how they responded to environmental change in the past. To this end, we reconstructed the island-building histories of five islands in the southern Maldives. We first collected 28 reef island “cores”. This essentially involved sledgehammering an aluminium pipe into the reef island until it reached the island “foundations” – a point lower than the live coral in the surrounding ocean. The cores enabled us to access the layers of sediment that have built up throughout the island’s history. We then analysed these sediments under the microscope to find out what exactly the island is made of. In addition, we radiocarbon dated the sediments to determine the when the various layers were created. Results showed that the key phase of reef island building occurred between 4,200 and 1,600 years ago, when sea levels reached around 0.5m higher than they are today.  In addition, this was likely under the influence of large wave events caused by distant storms. These waves would have had the power to break pieces of coral off the reef. Over time, these pieces of coral, as well as sand from the the reef, built up to form the islands. Climate change will mean rising sea levels and even stronger large wave events. It may therefore recreate conditions that are conducive to reef island building, which may enable these islands to keep growing vertically.  This would make the islands more resilient and may even be necessary simply to keep pace with rising sea levels. Our work complements other studies which are showing that islands are in fact dynamic landforms that are able to move and adjust in response to environmental change. All this should make reef islands in the Maldives more physically resilient. However, large waves can also make islands less habitable for humans, for instance by damaging buildings and farmland, or by dumping salt into supplies of fresh water. Reef island nations will have to develop infrastructure that can withstand, or be adaptable to, such powerful waves. Such infrastructure must still allow natural processes to take place however, so that reef islands can maintain active connections to their surrounding coral reefs. While our study suggests that rising sea levels could benefit reef islands in some regards, they still remain at risk. For instance, we also found the islands in our study were made predominantly (about 75%) of coral. This means a healthy reef will be vital if the islands are to keep growing in future, and the Maldives are to remain above the waves. However, coral reefs are also threatened by climate change, not just by rising sea levels, but also by warmer and more acidic oceans. Under climate change, we may therefore end up in an odd situation where we have the perfect conditions to build coral reef islands, but an absence of any building materials."
nan
"
Share this...FacebookTwitterFew places virtue signal green as much as Germany.
So not surprisingly a number of cities led by socialist/green governments have attempted to implement electric public transportation buses, declaring they are the future of clean mobility.

Electric powered buses still struggling to be successful. Image: Flixbus
But Tichy’s Einblick just recently reported on the results of attempted electric bus fleets across Germany. They are not pretty.
Electrically driven public transport by bus is still a long way off.
FlixBus suspends electric bus after “repeated technical problems”
One example, Tichy’s Einblick cites, is German intercity bus carrier FlixBus, which worked with Greenpeace to promote the electric bus on the route between Mannheim and Frankfurt as a showcase project – all accompanied by ample fanfare and slogans such as “sustainable travel” and “the mobility of the future is green”.
But last April Greenpeace reported the discontinuation of the first nationwide electric long-distance bus line: “The long-distance bus provider announced on Wednesday that there had been repeated technical problems during the pilot project between Mannheim and Frankfurt with the vehicle of a Chinese manufacturer. These problems with the bus of Chinese manufacturer BYD must have been so massive that the project was suspended.”
Tichy’s Einblick reports the “only one thing that was really sustainable about the project was the disappointment of the travelers.”
Wiesbaden: 45 million euros, only on flat routes
Tichy’s Einblick also looked at the city of Wiesbaden where city bus operator ESWE Stadtwerke ordered 56 electric buses in April of this year and the first five were to run in October, with another five to follow in November. But so far none have “made any further progress yet”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Wiesbaden plans a total of 140 electric buses, all to be supported by 45 million euros in taxpayers’ money from the Federal Environment Ministry. Yet Tichy’s Einblick reports that only the “flat inner city stretches are to be served” and that “they do not dare to venture onto the steeper streets on the outskirts.”
Also: “A battery charge should last 150 kilometers, but less if the temperatures are freezing and the bus is full: 100 kilometers.” That would mean about 3 hours of service before a recharge becomes needed.
Nürtingen electric bus pilot “a flop”…battery 80,000 euros!
In the city of Nürtingen, “The electric bus pilot project is a flop,” says Tichy’s Einblick. “The battery on a bus was broken after two and a half years of operation, and a new one costs 80,000 euros. Too much – that’s why the operation is stopped.”
Trier: buses taken out of service after just 2 weeks
Tichy’s Einblick also sarcastically reports that since July, “The electric buses in Trier have proven to be truly quiet and environmentally friendly: They are idol.”
“Already after two weeks the first electric bus had to stay in the workshop. Reason: Problems with the battery. An end of the problems is not in sight,” according toTichy’s Einblick.
Bremen backs off electric bus plans: “many disadvantages”
The failures of the electric buses on German streets has not gone unnoticed. Even the Green/Socialist government of the northern city of Bremen has made “a 180-degree turn”.
Earlier the city had planned to purchase five electric buses, 40 percent of which were to be funded by the federal government. But the Bremen city government opted out of the plan. “The reason: It is still unclear whether the electric drive really is the technology of the future.”
According to Tichy’s Einblick: “Bremen Mayor Maike Schaefer (Green Party) says that e-mobility has many disadvantages: ‘The batteries need cobalt, which comes from mines in the Congo. Exploitative child labor prevails there.'”
“Currently, the electric vehicles also have such a short service life that their carbon footprint does not represent any real progress compared with conventional technologies,” writes Tichy’s Einblick.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMonster carbon footprint: Six German researchers fly all the way to Ecuador to study how humans are impacting the earth during the Anthropocene – and do lots of hiking at the expense of the public – on a 17-day “expedition”. Some of them, including a musicologist, are of questionable scientific disciplines. 

Chimborazo in Riobamba, Ecuador, Photo David Torres Costales, CC BY-SA 4.0, via Wikipedia here. 
In spring 2020, six members of Die Junge Akademie from the Berlin-Brandenburg Academy of Sciences and Humanities and the German National Academy of Sciences Leopoldina from a range of disciplines – departed on “Expedition Anthropocene”.
“One of the focal points of the expedition,” the website says, “is climate change and its consequences for the environment as well as the transition of a region over the past 200 years.”
“In this, humans are consistently viewed as the instigators, those affected and the observers of these events.”
Lots of hiking
Their research took them to Ecuador and the volcanic mountain Chimborazo. Due to its location close to the equator, the summit of Chimborazo is the highest point on the planet when measured from the Earth’s center.
Impacts from “advancing climate change”
According to the six German researchers, “Together with our local partners, we go in search of traces of human activity in this environment” by using “methods from glaciology, biology, chemistry, acoustic ecology, computer science and medicine, we will investigate the human impact on Chimborazo at different altitudes – from advancing climate change and its consequences for humans, glacial retreat and biodiversity, to acoustic ecological changes and the question of whether microplastics can be detected in the snow and ice.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A junket disguised as science?
But already some are criticizing the expedition. Not only because of the carbon footprint the long travel and extensive accommodation, but because of it has the appearance of a junket disguised as a scientific expedition.
“The first stop on our expedition is Quito, the capital of Ecuador and the world’s highest capital city. We will spend a few days in the city to give ourselves time to acclimatize to the altitude, and set out from here on our first day trips,” the site explains.

Locations to be studied by among others,  a musicology professor. Chart source: Expedition Anthropocene.
The team were planned to hike to the active volcano Pichincha and the inactive volcano Chimborazo – the highest point above Earth’s center – as well as to the Ambato and the Llanganates National Park where they would “spend a few days in a complementary vegetation zone to the mountainous regions of the Andes.”
Not a single climate-related scientist – one musicologist
Of course long hikes to sites are all part of many expeditions and thus perfectly legitimate. But controversy swirls concerning the background of members of the German team.
Many have nothing or little to do with climate science. German science site Die kalte Sonne here noted: “No single geologist, geographer or glaciologist is involved. Instead there is a musicologist, a medical doctor (okay, maybe because of the altitude), a computer scientist and (EVEN!!) a physicist. The physicist comes from the PIK!”
“Six young people simply claimed that they would study the consequences of climate change and without further ado they jetted off to Ecuador. A nice example of a real-life satire: young up-and-coming artists (some of them really artists) are “researching” climate change in South America in Humboldt’s footsteps,” Die kalte Sonne writes.
Share this...FacebookTwitter "
"
Boulder is home to National Institute of Standards and NOAA’s research lab…big government facility and probably the most secure weather station in the USA, I had to go through metal detectors, have mirrors run under my vehicle, be photographed, and my drivers license verified.
Took 2 hours…on the road at the moment to get another station in Colorado, blogging via WiFi from Starbucks
Will post new pix soon.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea41b293b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
nan
"The latest Living Planet report from the WWF makes for grim reading: a 60% decline in wild animal populations since 1970, collapsing ecosystems, and a distinct possibility that the human species will not be far behind. The report repeatedly stresses that humanity’s consumption is to blame for this mass extinction, and journalists have been quick to amplify the message. The Guardian headline reads “Humanity has wiped out 60% of animal populations”, while the BBC runs with “Mass wildlife loss caused by human consumption”. No wonder: in the 148-page report, the word “humanity” appears 14 times, and “consumption” an impressive 54 times. There is one word, however, that fails to make a single appearance: capitalism. It might seem, when 83% of the world’s freshwater ecosystems are collapsing (another horrifying statistic from the report), that this is no time to quibble over semantics. And yet, as the ecologist Robin Wall Kimmerer has written, “finding the words is another step in learning to see”. Although the WWF report comes close to finding the words by identifying culture, economics, and unsustainable production models as the key problems, it fails to name capitalism as the crucial (and often causal) link between these things. It therefore prevents us from seeing the true nature of the problem. If we don’t name it, we can’t tackle it: it’s like aiming at an invisible target. The WWF report is right to highlight “exploding human consumption”, not population growth, as the main cause of mass extinction, and it goes to great lengths to illustrate the link between levels of consumption and biodiversity loss. But it stops short of pointing out that capitalism is what compels such reckless consumption. Capitalism – particularly in its neoliberal form – is an ideology founded on a principle of endless economic growth driven by consumption, a proposition that is simply impossible. Industrial agriculture, an activity that the report identifies as the biggest single contributor to species loss, is profoundly shaped by capitalism, not least because only a handful of “commodity” species are deemed to have any value, and because, in the sole pursuit of profit and growth, “externalities” such as pollution and biodiversity loss are ignored. And yet instead of calling the irrationality of capitalism out for the ways in which it renders most of life worthless, the WWF report actually extends a capitalist logic by using terms such as “natural assets” and “ecosystem services” to refer to the living world. By obscuring capitalism with a term that is merely one of its symptoms – “consumption” – there is also a risk that blame and responsibility for species loss is disproportionately shifted onto individual lifestyle choices, while the larger and more powerful systems and institutions that are compelling individuals to consume are, worryingly, let off the hook. The WWF report chooses “humanity” as its unit of analysis, and this totalising language is eagerly picked up by the press. The Guardian, for example, reports that “the global population is destroying the web of life”. This is grossly misleading. The WWF report itself illustrates that it is far from all of humanity doing the consuming, but it does not go as far as revealing that only a small minority of the human population are causing the vast majority of the damage.  From carbon emissions to ecological footprints, the richest 10% of people are having the greatest impact. Furthermore, there is no recognition that the effects of climate and biodiversity collapse are overwhelming felt by the poorest people first – the very people who are contributing least to the problem. Identifying these inequalities matters because it is this – not “humanity” per se – that is the problem, and because inequality is endemic to, you guessed it, capitalist systems (and particularly their racist and colonial legacies). The catch-all word “humanity” papers over all of these cracks, preventing us from seeing the situation as it is. It also perpetuates a sense that humans are inherently “bad”, and that it is somehow “in our nature” to consume until there is nothing left. One tweet, posted in response to the WWF publication, retorted that “we are a virus with shoes”, an attitude that hints at growing public apathy.  But what would it mean to redirect such self-loathing towards capitalism? Not only would this be a more accurate target, but it might also empower us to see our humanity as a force for good. Words do so much more than simply assign blame to different causes. Words are makers and breakers of the deep stories that we construct about the world, and these stories are especially important for helping us to navigate environmental crises. Using generalised references to “humanity” and “consumption” as drivers of ecological loss is not only inaccurate, it also perpetuates a distorted view of who we are and what we are capable of becoming.  By naming capitalism as a root cause, on the other hand, we identify a particular set of practices and ideas that are by no means permanent nor inherent to the condition of being human. In doing so, we learn to see that things could be otherwise. There is a power to naming something in order to expose it. As the writer and environmentalist Rebecca Solnit puts it: Calling things by their true names cuts through the lies that excuse, buffer, muddle, disguise, avoid, or encourage inaction, indifference, obliviousness. It’s not all there is to changing the world, but it’s a key step. The WWF report urges that a “collective voice is crucial if we are to reverse the trend of biodiversity loss”, but a collective voice is useless if it cannot find the right words. As long as we – and influential organisations such as the WWF, in particular – fail to name capitalism as a key cause of mass extinction, we will remain powerless to break its tragic story."
"Until relatively recently, lots of different massive mammals roamed across our planet. Mastodons, mammoths, giant elk, rhinoceros-sized marsupials, sabre-toothed cats, marsupial lions, dire wolves, American cheetahs … the list goes on and on. Then modern humans spread throughout the world and the vast majority of those large species disappeared. Our planet’s large mammal biodiversity is a shade of what it once was.  Sadly, research we’ve carried out shows that the large mammal extinctions of the past 2.5m years are continuing today – and smaller species are now also threatened. Our new study, published in Science Advances, reviewed the threats, status and ecosystem services provided by the 74 largest terrestrial herbivores (exceeding 100kg in body mass), and the conservation effort required to save them from extinction.  Our results are highly concerning. The vast majority of these large herbivores are declining in distribution and abundance, such that 60% are now threatened with extinction. These include well-known and iconic species such as elephants, hippos, all species of rhino, European bison and Indian water buffalo, but also less well-known species such as takin, kouprey, mountain and lowland anoa, and tamaraw. The situation is likely to get worse and we risk leaving empty landscapes unless urgent and drastic action is undertaken. Hunting, habitat loss and competition for food with livestock are the major threats to the world’s large herbivores. Simply identifying these threats is perhaps the most optimistic result of our study, as these are all issues that can be managed and reduced, provided there is sufficient human will to do so.  While Africa supports the greatest number of large herbivore species, south-east Asia retains the most that are threatened. The region’s woodlands are facing empty forest syndrome – where they seem intact, but there are few large animals left within them.  Overwhelmingly, it is developing countries that host the remaining megafauna – they are largely gone from the developed world. Consequently, these poorer nations bear the costs of protecting large herbivores, as well as the missed opportunity costs of setting aside large areas of land for conservation rather than food production. The developed world offers paltry support. Research efforts also suffer from this same disparity. Data deficiency is the bane of conservation management, yet the most-studied large herbivores are the common game species. We know next to nothing about large and highly threatened wild pigs such as Oliver’s warty pig or the Palawan bearded pig, for instance. Without adequate and targeted funding, it is hard to see this research occurring before it is too late for many of the developing world’s big herbivores. A world without elephants, tapirs, hippos, giraffes or gorillas would be a much poorer place. Large herbivores are inspirational, and huge numbers of tourists travel the world to observe them. Yet these species also perform fundamental roles in the ecosystems they inhabit and their loss would substantially alter the natural world. African elephants knock over trees enabling shrubland to develop, for example. This shrubland benefits browsing species such as impalas and black rhinos. Elephants also make great seed dispersers and there are concerns that this ecosystem service is being lost in parts of Asia and Africa where they are becoming scarce. Other large herbivores have also been shown to have a disproportionate impact on their environment, such that their decline is likely to have repercussions right along the food chain. The return of bears and wolves to Europe illustrates that developed countries can succeed in conserving wildlife. These large carnivores can also play fundamental roles in their ecosystems, often by limiting numbers of common herbivores such as rabbit or deer, yet globally carnivores are also still in decline.  There are plans to reintroduce beavers, lynx and wild boar in the UK, as wolves have been returned to Yellowstone National Park in the US. But what about the mega-herbivores? Why don’t we bring back herds of wild cattle (the ecological equivalent and modern variant of the extinct aurochs) to the UK? Governments are inherently risk averse when it comes to conservation initiatives, but they must start acting before it is too late for these majestic creatures."
"Coastal floods are a major global hazard. In 2008, Cyclone Nargis generated a five-metre storm surge along the coast of southern Myanmar. This swept seawater 50km inland, killing a staggering 130,000 people. In 2013, Typoon Haiyan swept across the central Philippines, killing 8,000 people and destroying a million homes, with much of the damage due to high sea levels. The past decade witnessed two of the most costly natural disasters in US history: Hurricane Katrina in 2005 and Sandy in 2012. Coastal flooding from these two events resulted in 1,000 deaths and billions of dollars in damage. In the UK, an unusually severe sequence of coastal flooding caused enormous damage during winter 2013-14. The world’s coastal population keeps growing and these floods are a reminder of the risks. Coastal flooding is caused by combinations of high tides, storm surges and large waves. A storm surge is a big rise in sea level caused by strong winds pushing sea water towards the coast where it “piles up”; and by low pressure at the centre of storms which “pulls” the sea surface up by about a centimetre for every millibar. The worst coastal flooding often occurs when the timing of the peak storm surge coincides with high spring tide. The fact that coastal flooding is caused by high sea levels is obvious enough. But surprisingly, we currently do not record which extreme sea level rises spill over into extreme floods.  To improve our understanding of coastal flooding, and to assess just how unusual 2013-14 was, we have compiled a new database and described in Scientific Data. Our work provides a systematic UK-wide record of coastal floods over the past hundred years. It currently contains data on 96 major floods, with information for each on the storm that generated it, the high-water level reached, and the severity of coastal flooding. We have also developed a website called “SurgeWatch”, to make the information easily accessible, and freely available to scientists, coastal engineers, and planners. We’re aiming to expand the database and we want you to help. Do you have any photographs of coastal flooding from recent or past events which you are willing to share? Photos can be easily uploaded to our website. We want to investigate these in order to improve understanding of exactly which areas were flooded and to what water depth. The database has allowed us to identify which historic storms resulted in the worst coastal flooding over the last 100 years, and we have mapped the specific paths of the storms responsible for these events. What is particularly evident is that coastal floods can “cluster”. That is, you get seasons and even decades of calm with relative little happening, and other periods when floods occur in rapid succession. Winter 2013-14 was particularly unusual, featuring seven out of the 96 floods in the 100-year database and two of the top ten; no other season comes close. Now that we know more accurately which seasons and decades had the largest number of coastal flooding events, we are examining properties (such as sea surface temperature) of the North Atlantic ocean the year before, to see if this provides clues as to how stormy the following season may be. If there are links, and the ocean contributes to the clustering of storms in a period, then there is scope to develop seasonal forecasting that could supplement the short term forecasting currently provided by the UK Coastal Monitoring and Forecasting Service, which provides warnings of impending high sea levels, helping people to prepare for flooding emergencies. The UK coastline has been subject to terrible floods throughout history and accurate forecasting would have been useful. In 1607, up to 2,000 people were drowned around the Bristol Channel, the greatest loss of life from a natural catastrophe in the UK during the past 500 years. The worst natural disaster in modern times was the “Big Flood” of 1953. In south-east England, 307 people were killed and 24,000 people fled their homes, and almost 2,000 lives were also lost in the Netherlands and Belgium. This event was the driving force behind the Thames Barrier which protects London from storm surges, and other flood defence schemes around the country. It also led to the establishment of the UK monitoring service.  The fact the 2013-14 damage was so limited compared to the tragedy of 1953 is thanks to significant government investment in coastal defences, flood forecasting and sea level monitoring. Our work helps create a direct link between the latter two."
"My mother died recently and at the funeral home I was asked if I had any ideas what kind of coffin she would like. For some reason I said something environmentally friendly. These words came out of my mouth more out of nervousness than anything previously discussed with my mother. Duly the undertaker showed us a catalogue of wicker coffins and we chose one made of banana leaves. I often think of my carbon footprint – I have not owned a car in more than 15 years, for example – but I had never thought about my “green obligations” in death. My mother may not have requested an environmentally friendly coffin, but she did state she wished to be cremated. Due to the lack of space in the UK around 80% of people request cremation – and if we think about green space being at a premium this makes ecological sense. However the energy required to cremate a single person is equal to the energy they would use in a month if they were alive. In the UK this translates to a yearly energy consumption of a town of 16,000 people.  In Asian countries where cremation is very popular there is considerable interest in using solar power to reduce such energy consumption. Another problem with cremation is air pollution, which obviously depends on the filtering system being employed. Until recent times cremations were one of the major sources of mercury pollution in the UK due to the amalgam fillings in people’s teeth. A group of environmental NGOs recently called on the EU to curb mercury emissions from human cremation. Furthermore, the clothes worn and use of embalming fluids may also increase air pollution. Humans have buried their dead for at least 100,000 years. Therefore, not wishing to throw the baby out with the bathwater, I looked into different burial options. A woodland burial initially appealed to me.  However, I would only really approve of this if it resulted in the maintenance of a high-quality conservation area and wildlife refuge. And I wonder if it became popular enough if it could result in major reforestation of the UK. But bodies would still be rotting in the ground releasing globally warming methane gas. Surely, there must be greener options than a standard burial or cremation?  Coming from a family of fishermen I thought about burial at sea, as the fish could recycle my body quickly.  But there are only three registered places in the UK and only around 50 such burials per year. As a biologist, I find the idea of becoming fish food strangely appealing. This is not a new idea: I remember reading of man who macabrely wished the meat from his body fed to the residents at Battersea Dogs Home. Not surprisingly this strange offer was declined. As a conservationist the idea of recycling my body after death appeals: some Asian cultures have what are called sky burials, where a dead human body is laid out on a mountain top for scavenging animals such as birds of prey to feed on.   From a biological point of view I cannot see anything wrong with this, providing deceased people do not have contagious diseases. Burials in the ground are more to do with people not wishing the body disturbed by animals than hygiene considerations – hence being buried six feet. Unfortunately, as much as I like to imagine my deceased body on the top of Ben Nevis being recycled by golden eagles, I can never see it being allowed in the UK. I suppose what really appeals to me is being fully recycled in a short time-frame. The problem is that cremation does not fully recycle the body and burials can take years for the recycling process to occur. Thus, if my body could be fully recycled quickly into the nutrient cycles, thereby allowing the burial plot to be constantly reused then I may have found a biologically acceptable method to dispose of my body when the time comes. A company in Sweden has tested a concept of eco-burial on dead pigs (pigs are good models for the human body), whereby the animal is frozen in liquid nitrogen at -196℃, which makes the body become brittle and disintegrate. In the case of a human, the disintegrated body would be filtered for metals (such as tooth fillings) and then buried in a shallow grave.  In tests with pigs the remains become rich compost in six to twelve months.  Plus this sort of eco-burial does not release greenhouse gases such as methane (from traditional burials) or carbon (from cremations) into the atmosphere.  The only problem being it is still in development."
"

No thats not the title of a new Godzilla movie, but “Deep Fried Rodan” could be.
My friends in journalism say news goes in cycles. If that is so, this must be the year of the creepy crawly restaurant.
Today I see on the TV news the shocking video (a frame of which is shown above) of the Kentucky Fried Chicken combo Taco Bell in New York City’s Greenwich Village that has been taken over by rats and closed down by the health department.
What’s in those buckets anyway? Just kidding, and the trademark bucket in the picture above had a little help using Photoshop. But it makes you wonder just how many restaurants in America are as bad as this?
Oddly, it was exactly one year ago today that we had the China Star meltdown, where police and fire responders to a burglar alarm found a restaurant so incredibly filthy and pest ridden, it defied description.
In his ER article last year,  reporter Ari Cohn and Chico PD officer Melody Davidson’s incident report did an admirable job in conveying the heebie jeebies via the written word to anyone whom ever ate there. Today reading the news reports online and then seeing the videos, it was “like Deja Vu all over again”.
I wrote a letter to the editor last year suggesting we need to have color coded health inspection reports posted in the entrance of every restaurant showing its last inspection status. Green for Pass, Yellow for some minor violations, and Red for get the heck outta there ! I still think its a good idea.
Some progress has been made, as now you can get inspection reports online at  Butte County’s Health Department. Here is the link: http://www.buttecounty.net/Default.aspx?tabid=312
Reading through the list of inspection reports at the Butte Health Dept website, I was surprised to learn that even some well known and considered “classy” Chico restaurants had some major violations in the last year. If you eat out a lot, this website is worth a look. Any enterprise that sells packaged food, serves food or food samples, including school cafeterias, coffee houses, country clubs, fraternal clubs, and even liquor stores get inspected by the County Health Department.
Here’s a surprising fact: Indian Casinos and their restaurants are exempt from inspections, because tribal operations are considered their own sovereign nation. That may be so, but I think any place that could potentially make people sick through sloppy food handling shouldn’t get a free pass on a legal technicality.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea83ee422',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterEarth’s atmosphere is made of 78% nitrogen (N2) and 21% oxygen (O2). The “consensus” view is N2 and O2 are not greenhouse gases (GHGs) and don’t absorb infrared radiation (IR). But scientists have been saying N2 absorbs and radiates IR since 1944 and more recent (2012, 2016) studies have found N2 and O2 are “radiatively important” greenhouse gases with IR temperature absorption capacities similar to CO2.
It’s been known for 75 years that nitrogen – the Earth’s most prevalent atmospheric gas – absorbs and “strongly” radiates infrared energy (Stebbins et al., 1944)

Image Source: Stebbins et al., 1944
Methane (CH4) is thought to be an 84 times more potent greenhouse gas than CO2.

Image Source: Environmental Defense Fund
Nitrogen, oxygen are “natural greenhouse gases”
Scientists (Höpfner et al., 2012) publishing in Geophysical Research Letters dispute the “common perception” that nitrogen and oxygen – accounting for 78% and 21% of the Earth’s atmospheric gases – do not contribute signficantly to the Earth’s greenhouse effect.
They assert N2 and O2 are “radiatively important” “natural greenhouse gases” primarily because their concentration is “about 2000 (550) times higher than that of CO2 and about 4.4 × 105 (1.2 × 105) times more abundant than CH4.”
Nitrogen, oxygen combined are more potent GHGs than methane
The atmospheric abundance of N2 and O2 compensates for their relatively weaker IR function (when directly compared to CH4).
For example, “the natural greenhouse effect of N2 and O2 would be larger than that of CH4 by a factor of 1.3” when considering their combined isolated GHE influence.
Further, the reduction in the atmosphere’s infrared transmission amounts to 25.7% for N2, 14.2% for O2, and only 6.9% for CH4.
Nitrogen’s greenhouse gas influence also rivals CO2’s
Höpfner and colleagues also suggest N2 reduces outgoing longwave radiation (OLR) by 4.6 W/m² compared to CO2’s 5.1 W/m² when assesing their solo absorption capacity. This would appear to be a rather minor difference.
If the number of N2 molecules in the atmosphere were hypothetically doubled, it would produce a 12 W/m² longwave greenhouse effect forcing.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Doubling CO2 from 280 ppm to 560 ppm only yields a 3.7 W/m² radiative forcing.
The authors reject the “view that the radiative forcing of N2 increase operates only indirectly by broadening the absorption lines of other gases.” Instead, N2 has a “direct impact” (as well as an indirect impact) within greenhouse effect forcing.

Image Source: Höpfner et al., 2012
Experiment: nitrogen, oxygen absorb IR to about the same limiting temperature as CO2
A real-world experiment (Allmendinger, 2016) assessing the efficacy of CO2’s IR-absorption temperature capacity relative to air (N2, O2) and Argon (Ar) further establishes CO2 is not the “special” GHG it is commonly thought to be.
Twin styrofoam Saran-wrap-sealed tubes exposed to sunlight were used, one with pure (1,000,000 ppm) CO2 and the other with air (N2, O2) and/or Ar.
The results were admittedly “surprising” given expectations CO2 would operate as a radiatively distinct GHG.
The tube absorbing IR with N2 and O2 (air) and Ar warmed to a temperature limit quite similar to (55°C to 58°C) the temperature limit in the 100% CO2 tube (58°C).
There was no remarkable or  “special” heat absorption capacity for CO2 relative to air observed. And Argon – not considered a greenhouse gas – absorbed IR to the same temperature limit as CO2. With a concentration of 9300 ppm, Ar is the third-most abundant gas in the Earth’s atmosphere.
Because there is so little to distinguish CO2 from the most abundant gas molecules in Earth’s atmosphere, Dr. Allmendinger assesses “a significant efect of carbon-dioxide on the direct sunlight absorption can already be exluded.”
Further, “the greenhouse theory has to be questioned.”

Image Source: Allmendinger, 2016
Share this...FacebookTwitter "
"It is widely agreed that today’s global agriculture system is a social and environmental failure. Business as usual is no longer an option: biodiversity loss and nitrogen pollution are exceeding planetary limits, and catastrophic risks of climate change demand immediate action. Most concede that there is an urgent need to radically transform our food systems. But the proposed innovations for more sustainable food systems are drastically different. Which we choose will have long-lasting effects on human society and the planet. Suggested innovations in food systems can be broadly understood as either seeking to conform with – or to transform – the status quo.  Some want to keep the agriculture industry as close to existing practices as possible. This is true of the increasing number of corporate and financial actors who seek to solve the food crisis by developing new technologies. These technologies are envisaged as being part of what is being called the “fourth industrial revolution” (4IR). The “answer” here is thought to lie in a fusion of technologies that blurs the lines between physical, digital and biological domains. For example, the World Economic Forum is currently supporting agricultural transitions in 21 countries through its “New Vision for Agriculture” initiative. This initiative supports “innovation ecosystems” to re-engineer food systems based on “12 transforming technologies”. In this imagined future, next generation biotechnologies will re-engineer plants and animals. Precision farming will optimise use of water and pesticides. Global food systems will rely on smart robots, blockchain and the internet of things to manufacture synthetic foods for personalised nutrition. Like previous green revolution technologies in agriculture, this effort is designed by and for powerful agricultural giants. These technological innovations reinforce the concentration of political and economic power in the hands of a small number of corporations. Indeed, the latter have a growing monopoly control over the “12 transforming technologies” protected by patents. Most notably, the spread of these technologies will expand the technosphere at the expense of the biosphere. Flying robots will pollinate crops instead of living bees. Automated machines will replace farmers’ work on soil preparation, seeding, weeding, fertility, pest control and harvesting of crops.  These hi-tech innovations radically depart from most farming practices. They are moving us towards an increasingly people-less food system. Yet they show a remarkable continuity with the logic of capitalist accumulation – hence their staying power despite their significant risks. The spread of automated, de-localised and digitalised production and commercialisation of food is part of the “financialisation” of the global food system. Financial markets play an increasing role in controlling food systems from a distance. This generates huge social and human risks. For example, the significant growth in the sale and purchase of financial products linked to food commodities was one of the determining factors in the 2008 world food crisis. But there is an alternative to this future. Agroecology involves the application of ecological principles for the design and management of sustainable agroecosystems. Our research on agroecology focuses on how it can contribute to food sovereignty, which emphasises the democratisation of food systems. Agroecology’s contribution to the Sustainable Development Goals is now recognised. In contrast to the technological vision described above, agroecological innovations promote circular systems that involve recycling, reuse and combining resources to reduce dependency on external inputs, in particular fossil fuels. They mimic natural cycles and the functional diversity of natural ecosystems.  Farming systems are designed in a way that is based on beneficial interactions between plants, animals and environments. Trees and shrubs might be planted amongst or around crops, say. Or two or more crops might be grown in proximity. Agroecology reduces the dependence of food producers on expensive external inputs, distant commodity markets and patented technologies. This is achieved by relying on appropriate biodiversity to ward off pests and increase farm yields. At broader scales, agroecology involves circular systems that combine food and energy production with water and waste management. Pollution is minimised and synergies achieved by carefully clustering industries into functional wholes. The re-localisation of production and consumption within territories enhances local economic regeneration and sustainability.  Agroecological innovations in transitions to sustainable food systems are being driven largely from the bottom up by civil society, social movements and allied researchers. In this context, priorities for innovations are ones that increase citizen control for food sovereignty and decentralise power. This is in direct contrast to the monopoly control enabled by 4IR technologies. Government, civil society and private sector representatives will soon meet in Rome at the United Nations Food and Agriculture Organization to discuss the future of farming. Who controls the global governance of innovation will be a hotly debated topic. But given these highly contested views on innovations for food and agriculture, it is vital that everyone is able to exercise their right to have a say on the future of their food supply. Deliberative and inclusive processes such as citizens’ juries, peoples’ assemblies and community-led participatory processes are urgently needed to decide priorities for food and agricultural innovations. This is all the more important in today’s context of rapid global change and uncertainty.  So. Do you want to live in a world in which artificial food is produced by intelligent robots and corporations that put profits before people? Or one where agroecological innovations ensure we can nourish ourselves and our communities in a fair, ecologically regenerative, and culturally rich way?"
"
Share this...FacebookTwitter
Image: NASA Earth Observatory. Public Domain
Prof. Fritz Vahrenholt’s Monthly Solar Report
The global mean temperature in April 2020 was again significantly lower than in February and March, at 0.38°C above the average from 1981 to 2010. The average temperature increase on the globe from 1981 to February 2020 was 0.14°C per decade. The further development promises to be interesting, especially since a number of research institutes expect a higher probability of a cooling La Nina in the Pacific towards the end of the year. March’s solar activity was very low with a sunspot number of 1.5.  Activity in April rose slightly to 5.4. The first sunspots of the new cycle are showing.
What causes the sun to have an 11-year cycle?
Since the Dessau pharmacist Heinrich Samuel Schwabe discovered in 1843 that the sunspots of the sun increase and decrease in an 11-year cycle, science has been puzzling over the reason why this cycle lasts 11 years and why the solar magnetic field also changes its polarity in this rhythm: the north pole becomes the south pole and vice versa.
In July last year, scientists at the Helmholtz Centre in Dresden Rossendorf made a little-noticed but exciting discovery. Every 11.07 years, the planets Venus, Earth and Jupiter are aligned quite precisely. At this point in time, their gravitational force acts jointly in one direction on the Sun.
“The agreement is amazingly accurate: we see a complete parallelism with the planets over 90 cycles,” explains Frank Stefani, one of the authors of the publication published in Solar Physics. Just as the gravitational pull of the Moon causes the tides on Earth, planets could move the hot plasma on the surface of the Sun. But the effect of a simple gravitational force is too weak to significantly disturb the flow in the Sun’s interior, so the temporal coincidence has long been ignored.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now the researchers assume that the layers of the plasma are subject to a Taylor instability. The Taylor instability is known from the behavior of liquids of different densities at their interface (we know the turbulence that occurs when milk is poured into a cup of tea).  Taylor instability is sensitive to even very small forces. A small burst of energy is enough for the polarity of the solar magnetic field to swing back and forth every 11 years. The necessary impulse for this could be provided by the tidal action of the planets – and thus ultimately determine the rhythm in which the sun’s magnetic field reverses its polarity.
The tidal forces of the planets could have other effects on the Sun in addition to their role as pace-setter for the 11-year cycle. For example, it would be conceivable that they could change the stratification of the plasma in the boundary area between the inner radiation zone and the outer convection zone of the Sun, the tachocline, in such a way that the magnetic flux could be more easily dissipated.
Under these conditions, the strength of the activity cycles could also change, just as the “Maunder Minimum” once caused a significant decrease in solar activity over a longer period, the researchers write on the Helmholtz Center website. It is an unusual idea that the activity of the sun is controlled by the planets, including the earth itself. This sounds like astrology – but it is the latest in solar research.
One of the first researchers who assumed an influence of the solar activity by the planets was Theodor Landscheidt, who already in 1988 in his book “Sun-Earth-Man” predicted the decreasing strength of the solar cycles 22 and following. However, he assumed a different mechanism, according to which the planets cyclically move the sun out of the center of gravity (barycenter) of our solar system. Landscheidt died in 2004.
And also in our book “The Forgotten Sun” we had invited Prof. Nicola Scafetta for a separate chapter, who already then interpreted the conjunction of Saturn and Jupiter as the cause of a 60-year cycle. In a publication published in Solar Physics in February 2020, he also relates the longer-term oscillations (Hallstatt -2400 years ,Eddy – 1000 years, Suess-de Vries – 210 years) to influences of the large planets of Jupiter, Saturn, Uranus and Neptune. The long version is accessible here.
Fritz Vahrenholt


		jQuery(document).ready(function(){
			jQuery('#dd_51502fcc0d8cc70a6aed31f4b468444e').on('change', function() {
			  jQuery('#amount_51502fcc0d8cc70a6aed31f4b468444e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe COVID-19 pandemic has exposed how scientific dissent is not only being suppressed and marginalized in Germany when it comes to climate science, but with virology and public health.

The online German national daily Die Welt here writes, “Virologists and physicians fear for their freedom of expression in the Corona crisis.” Climate scientists are getting some company!
Scientific freedom “under threat”
Citing the results of a recent survey, to Die Welt reports “many experts believe that freedom of expression in science is under threat” and “virologists have begun to change their attitude towards the measures taken by the German government.”
As the economy reels from the stringent restrictions enacted by authorities across Europe, the discussion about how to respond to the spread of the virus has become bitter. Public health experts opposing the restrictions and lock downs have found themselves marginalized and attacked by the media, other virologists and most politicians.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Shift: COVID-19 alarmism waning, frustration growing
Using a survey, “Researchers at the University Hospital Eppendorf in Hamburg (UKE), the Society for Virology (GfV) and the University of Tübingen have tried to determine the mood among experts,” Die Welt reports. “178 experts from the fields of virology, immunology, hygiene, internal medicine and intensive care medicine” were surveyed anonymously. The results show that “more than more than 70 percent of the participants support the distance rule of two metres and the prohibition of major events” but that the other restrictions were “far more controversial”.
One third feel freedom in science is “being threatened”
What’s surprising: “One third even see freedom of expression in science as being threatened” and today 63 percent think “it would be sensible to restore public and economic life”. Also: “social distancing” is apparently losing support, according to the survey.
Overall, the findings show that the pandemic skeptics are finally beginning to assert their views to the public.
Unfortunately this is not even close to happening in climate science, where in Germany the ultra-alarmists continue to control the message. That freedom to express science is under threat, “is probably what many climate scientists secretly think,” Die kalte Sonne site here commented.
Perhaps COVID-19 tells us how moods can change quickly once restrictions start eroding freedom and prosperity.
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterA new study assesses a reduction in tree cover via urbanization or by clearing forests for cropland can warm up a locality by 1°C within 10 years. In contrast, transitions from croplands and urban centers to forests leads to cooling. Europe has been cooling recently (1992-2015) from land cover transitions to forests.

Image Source: Huang et al., 2020
Urbanization adds multiple degrees of warming over decades
A few years ago a compelling analysis (Levermore et al., 2018) found the urban heat island effect can reach intensities of 8°C warmer temperatures than nearby rural sites.
Further, reducing the green (trees and vegetated areas) in an urban center by as little as 11% can lead to a 0.21°C per decade (non-climatic) warming trend in the local thermometer record.

Image Source: Levermore et al., 2018
Global warming can be reversed via land cover changes
While forest losses can heat up local temperatures by as much as 1°C within 10 years (Alkama and Cescatti, 2016), a new study (Huang et al., 2020) has assessed the opposite can occur too.
From 1992 to 2015, there were about 70 million hectares (Mha) of land cover changes (LCCs) occurring across the European continent.
A substantial portion of these LCCs were “cropland-to-forest” transitions due to agricultural abandonment.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




When a region returns to forest and tree cover, cooling ensues.
And with a growing percentage of European forested areas returning, a “predominant regional biophysical cooling” with “an average temperature change of −0.12 ± 0.20 °C, with widespread cooling (up to −1.0 °C) in western and central Europe in summer and spring” has swept across Europe due to LCCs in recent decades.

Image Source: Huang et al., 2020
The substantial impact of land cover changes
The implications of this study are profound.
First, the human effect on CO2 concentration changes appears to have minimal effect on local and regional temperatures relative to the much larger impact from land use changes.
More importantly, if reducing global warming is indeed the goal of policy makers, then denuding forests so as to install hundreds of steel-and-concrete wind turbines would appear to achieve the opposite of what it is claimed to do (reduce warming).
So why on Earth are we doing this?

Image Source: The Telegraph
Share this...FacebookTwitter "
"
The picture below is from Oregon State Climatologist George Taylor. You may have heard of him, the Governor of Oregon tried to get him fired for not jumping on to the global warming bandwagon because he doesn’t see enough supporting evidence.

The picture is of Forest Grove, Oregon, and the temperature plot below shows how it is warming. But George says:
“Yes, it’s a window air conditioning unit to the east and the edge of a large asphalt parking lot to the north, northwest, and west. The pic is shot looking northeast. For those of you that may not immediately realize this, air conditions exhaust hot air to the outside.

Not only that, but Forest Grove is located in Washington County, Oregon’s fastest-growing county (in terms of population growth, not percentage) for the last 40 years. No wonder it’s seeing unprecedented high temperatures…”
It looks like the air conditioner may have been installed around 1985, notice the sustained 1 degree jump that started about then and sustained a plateau.
And this is a station of record, a US Historic Climatology Network station that is used in global climate models by NASA, in fact the plot is from that database.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea603b6f9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I just completed my first meeting of the City of Chico Sustainability Task Force today and here are a few observations.
First, it seemed to be pretty well rounded, we had public and private sector, business, building industry, CSUC, and regular citizens represented by the 15 appointees.
Second, so far the focus seems to be doing things better, more efficiently, and at less cost. I’m all for that.
Third, everybody seemed to get along, no shouting matches or fistfights broke out.
While the group is still feeling their way, I expect that given the makeup of it, we’ll get some useful suggestions and ideas from it that may very well get implemented as policy someday. I was worried that we might have a group of folks who were so focused on the goal of “green” that we’d see odd policy come from it like our famously silly nuclear weapons ban in the city limits.
I’ll keep you posted. I have a few ideas of my own that I’ll discuss here.
Some folks ask me how I can be against the idea of man-made global warming but for alternate energy. Its simple really, if its more efficient, pollutes less (on any venue) has no social cost, and has a lower operating cost, I’m for it. Mostly I’m for alternate energy becuase California has essentially legislated out the ability to build any traditional forms of energy generation, such as coal, hydro, and nuclear. That leaves wind, solar, and conservation as the future of energy in California. whether you beleive in man-made global warming or not, our future energy needs have to come from some source, so we’d better get started now. If they have beneficial side effects, all the better.
One thing I’m not for is a carbon credits/trading programs. I think the whole idea is simply a cop out and designed to benefit the few that setup these programs. See why in this post.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7711e26',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"When a 9.0 magnitude earthquake hit Japan in March 2011, I was on the 9th floor of a 20-storey Tokyo hotel. The quake was one of the five most powerful since modern record keeping began in 1900, and it lasted for around three minutes – an unusually long time.  Countless lives were saved by Japanese engineering. The tsunami that followed – and the resulting Fukushima meltdown – was a terrible tragedy, but the earthquake itself could also have caused far greater loss of life. It actually did very little damage relative to its magnitude. The fact that buildings like the one I was in, and thousands of others, remained intact shows just how important good earthquake engineering is. In Nepal, which was recently struck by a 7.8 magnitude earthquake, the problem is quite different. A combination of poverty, a lack of specialised knowledge and poor regulation mean most houses were never built to such standards in the first place. Therefore it is these houses that retrofitting efforts – and my research in the country – must focus on. As recent events have reminded us, it is collapsing structures that kill people during earthquakes and rarely the shaking itself. Engineers, through intensive and persistent analysis of past earthquakes have mastered the art of building houses to resist shaking and to avoid collapse.  However, such earthquake-resistant buildings are expensive and need specialised knowledge and skills – it’s not something any old builder/architect combo can knock out overnight. There are specialised MSc courses and even doctorates in the subject – and engineers need continuous professional development to keep pace with the latest research. Japan has these skills and resources in abundance; a poorer nation such as Nepal, not so much. Houses in Nepal are built with traditional knowledge and often without any engineer’s visit – the technical term is non-engineered buildings – and it is difficult to make them withstand earthquakes of large magnitude.  In such scenarios, it is often prudent to rein in expectations and aim for the “least bad” outcome by increasing the time it takes for the house to collapse. If, instead of two seconds, the building collapses in 12 seconds it may give the occupants enough time to escape. The collapse of non-engineered masonry buildings is one of the greatest causes of casualties in major earthquakes around the world. Yet by definition these non-engineered structures remain largely outside of the scope of modern engineering research, focused as it is on new technologies and new buildings – fancy new quake-proof skyscrapers command significantly more funding than the unglamourous task of seismic retrofitting. This means that the majority of those at risk often remain so.  Even where research is focused on non-engineered housing, there are still significant social and economic challenges before implementation. It’s all very well asking people in Tokyo to pay a premium for seismic proofing, but Nepal’s gross national income per capita is US$730 – just two dollars a day. My research is aimed at developing retrofitting techniques which will prevent or prolong the collapse of adobe (mud brick) houses in strong earthquakes. We used common plastic packaging straps to form a mesh, which is then used to encase structural walls.  Tests showed that the proposed technique effectively prevents brittle masonry collapse and the loss of debris. We then trained rural masons in Nepal, gave a public “shake-table” demonstration and retrofitted a real house.  This implementation project proved effective at reaching rural communities but highlighted the fact that government subsidies are still required to give low-income people the incentive to safeguard their homes against the next big earthquake."
"A group of more than 200 scientists will on Monday urge returning parliamentarians to urgently reduce Australia’s total greenhouse gas emissions, and work diplomatically to achieve coordinated global climate action, after a catastrophic summer of fires. In an open letter timed to coincide with the resumption of the parliamentary year in Canberra, the group says scientific evidence unequivocally links human-caused climate change to the increasing risk of frequent and severe bushfires in the Australian landscape.  It says that same science tells us “these extreme events will only grow worse in the future without genuine concerted action to reduce global emissions of greenhouse gases”. The letter says the science suggests a need for immediate action to reduce total greenhouse gas emissions, and manage a rapid transition to net zero emissions by 2050. One of the signatories to the open letter, the Australian National University climate scientist Nerilie Abram, says the letter is “the product of despair as scientists witnessed the deadly fire season unfold”. “Scientists have been warning policymakers for decades that climate change would worsen Australia’s fire risk, and yet those warnings have been ignored,” she said. Given the catastrophic summer, with the bushfires triggering a barrage of commentary about the inadequacy of the Morrison government’s current policy, a number of Liberal MPs are returning to Canberra for the opening session parliament with the view that the government needs to do more on climate change and energy policy. Cabinet has also discussed the issue. The strongly worded appeal by scientists comes as a new energy market analysis by Reputex predicts Australia will hit 50% renewable electricity by 2030, despite the lack of a federal energy policy. But it also warns a slump in new investment in wind and solar investment could threaten a continued decline in wholesale electricity prices. Reputex says the current drive to 50% renewables by 2030 is being driven by state renewable energy targets and rooftop solar schemes that are predicted to make higher cost gas and coal-fired power less competitive. It says that transition is expected to deliver a decline in wholesale prices in the national energy market from around $80 per megawatt-hour (/MWh) in 2020 toward $70/MWh over the next three years, which translates as a fall of 15% from today’s levels. But it warns investment in renewable energy plunged by 50% last year compared with the year before, and it predicts that decline will continue – a development that will put upward pressure on wholesale electricity prices, particularly as major coal-fired facilities begin to close. “In the absence of an effective policy framework to guide new investment, the decline of our ageing generation fleet will lead to higher electricity prices before the new supply is developed, hurting both businesses and consumers,” the assessment says. In an effort to emphasise a positive message on the issue, the treasurer, Josh Frydenberg, told the ABC on Sunday the Morrison government’s 2030 emissions reduction target of 26% on 2005 levels by 2030 was a floor and not a cap, “and we hope to beat our target”. But while declaring the government wanted to beat the 2030 target, Frydenberg said the target wasn’t going to be adjusted. “We took to the Australian people a very clear target, so we’re not about to lift that target,” the treasurer said. “What we are endeavouring to do is to meet our commitments now.” Frydenberg did not mention that the government intends to use an accounting measure, carryover credits from the Kyoto period, to meet the 2030 target, with carryover credits, not practical abatement, supplying about half the pollution reduction load to 2030. The treasurer also played up the government’s commitment to renewable energy. Frydenberg said on Sunday that more than $7bn was invested in renewables last year. He omitted the Coalition’s efforts under Tony Abbott to gut the renewable energy target (RET), and the fact the RET winds down from this year – with policy uncertainty triggering the decline in new investment captured in the Reputex analysis. Asked whether he believed there was now a climate emergency, the treasurer hedged. “Climate change is a significant challenge, a global challenge that needs a global solution,” he said. “We’re doing our bit and we’re also working on the international stage.” Pressed to explain how climate change was not an emergency, Frydenberg said “it’s an important issue, but as the prime minister has outlined, there are a lot of things that we can do with mitigation and adaptation to try to reduce the impact of climate change on the Australian community”."
nan
"
Share this...FacebookTwitterGerman online site Stromreport writes that since the year 2000 the average electricity price for private households has risen from 13.94 to 30.43 euro cents per kilowatt hour (2019).
German electricity prices for households are among the highest worldwide.

Image: Statista.com
The price increase has little to do with demand or markets, but almost everything to do with government interference. According to Stromreport, “Taxes, charges and levies have tripled since 2000 [from 5.19 to 16 cents]. In total, German government charges now account for more than half of the electricity price [52.5%].”
Electricity becoming a luxury
Annually hundreds of thousands of German households see their power cut off due to unpaid power bills. For example in 2018, the Tagesspiegel here reported: “In the past year, almost 344,000 households in Germany had their electricity turned off. This is according to the monitoring report of the Federal Network Agency on the electricity market.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Of course the high prices hit the poor the hardest.
More price hikes in 2020
And things are not going to improve for Germany’s overburdened power consumers in 2020. Stromreport writes that 403 suppliers have already raised electricity prices by an average of 5.3% this year already, bringing the price to a whopping 30.43 cents per kilowatt-hour. “A 3-person household currently pays almost 89 euros for its electricity. That is 27% more than 10 years ago [69.09 euros].”
Now comes the CO2 charge
The price in 2020 is expected to reach 31.47 cents per kilowatt-hour. Also the wholesale prices for electricity are expected to rise in 2020, due to “rising CO2 prices”…”which will make electricity from coal and gas more expensive on the electricity exchange,” says Stromreport.
Another major component of the German power price are the green energy feed-in tariffs for power coming from, for example, wind and sun. German consumers pay 6.756 cents for kilowatt-hour.
Hat-tip: Die kalte Sonne.


		jQuery(document).ready(function(){
			jQuery('#dd_79cf92e8191e2141d3be52723b8df435').on('change', function() {
			  jQuery('#amount_79cf92e8191e2141d3be52723b8df435').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitter
Within the last few years, over 50 papers have been added to our compilation of scientific studies that find the climate’s sensitivity to doubled CO2 (280 ppm to 560 ppm) ranges from <0 to 1°C. When no quantification is provided, words like “negligible” are used to describe CO2’s effect on the climate. The list has now reached 106 scientific papers.


Link: 100+ Scientific Papers – Low CO2 Climate Sensitivity


A few of the papers published in 2019 are provided below.
Krainov and Smirnov, 2019  (2X CO2 = 0.4°C, 2X anthroCO2 = 0.02°C)
“The greenhouse phenomenon in the atmosphere that results from emission of its molecules and particles in the infrared spectrum range is determined by atmospheric water in the form of molecules and microdrops and by carbon dioxide molecules for the Earth atmosphere and by carbon dioxide molecules and dust for the Venus atmosphere. The line-by-line method used the frequency dependent radiative temperature for atmospheric air with a large optical thickness in the infrared spectral range, allows one to separate emission of various components in atmospheric emission. This method demonstrates that the removal of carbon dioxide from the Earth’s atmosphere leads to a decrease of the average temperature of the Earth’s surface by 4 K; however, doubling of the carbon dioxide amount causes an increase of the Earth’s temperature by 0.4 K from the total 2 K at CO2 doubling in the real atmosphere, as it follows from the NASA measurements. The contribution to this temperature change due to injections of carbon dioxide in the atmosphere due to combustion of fossil fuel, and it is 0.02 K. The infrared radiative flux to the Venus surface due to   CO2 is about 30% of the total flux, and the other part is determined by a dust.”

Image Source: Krainov and Smirnov, 2019

Ollila, 2019 (2XCO2= 0.6°C)
“If a climate model using the positive water feedback were applied to the GH effect magnitude of this study, it would fail worse than a model showing a TCS value of 1.2°C. If there were a positive water feedback mechanism in the atmosphere, there is no scientific grounding to assume that this mechanism would start to work only if the CO2 concentration exceeds 280 ppm, and actually, the IPCC does not claim so. The absolute humidity and temperature observations show that there is no positive water feedback mechanism in the atmosphere during the longer time periods. … The contribution of CO2 in the GH effect is 7.3% corresponding to 2.4°C in temperature. The reproduction of CO2 radiative forcing (RF) showed the climate sensitivity RF value to be 2.16 Wm-2, which is 41.6% smaller than the 3.7 Wm-2 used by the IPCC. A climate model showing a climate sensitivity (CS) of 0.6°C matches the CO2 contribution in the GH effect, but the IPCC’s climate model showing a CS of 1.8°C or 1.2°C does not.”

Varotsos and Efstathiou, 2019
“The enhancement of the atmospheric greenhouse effect due to the increase in the atmospheric greenhouse gases is often considered as responsible for global warming (known as greenhouse hypothesis of global warming). In this context, the temperature field of global troposphere and lower stratosphere over the period 12/1978–07/2018 is explored using the recent Version 6 of the UAH MSU/AMSU global satellite temperature dataset. Our analysis did not show a consistent warming with gradual increase from low to high latitudes in both hemispheres, as it should be from the global warming theory. … Based on these results and bearing in mind that the climate system is complicated and complex with the existing uncertainties in the climate predictions, it is not possible to reliably support the view of the presence of global warming in the sense of an enhanced greenhouse effect due to human activities.”

Image Source: Varotsos and Efstathiou, 2019Share this...FacebookTwitter "
nan
"

I don’t know why I’m posting this other than its how I feel today.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7356857',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Sometimes you can see the end of the old world and the beginning of the new one as clearly as a seam. Transformations that were once barely perceptible, recognisable only after the fact, this summer have become akin to a crossing. You can see the line as you step over it. It’s the first summer of this new decade. Welcome. It’s the summer of ash washed up on the beaches, like a long, deranged message unfurled from its bottle. It’s the summer of a billion animals killed by flames and starvation, it’s collapsed biospheres, charred forests, epic dust storms, hailstones and racing clouds carrying fire.  It’s the summer of the worst air quality in the world, of breathing masks and of keeping children indoors. It’s the summer of cancelled holidays, of anxious evacuations on jammed roads out of coastal hamlets. It’s fire raining on beaches, and skies that glow red at night and darken in the day. It’s the summer that Australia’s colours on Instagram went from saturated to sepia, starting in early December when people in Sydney shared photos of the weird red sun. The image at first seemed singular and disorienting until it was repeated so often (so many skies, in different cities, on so many feeds) it just came to be disorienting, and later very familiar. It’s the summer of asking about people’s holiday plans and also their fire plans. It’s the summer of both generalised and specific anxiety – where despair and dread coalesced into a sort of collective bad vibe and common understanding that this is a terrible summer. It’s the summer when faith in the political class entered some new, nihilistic nadir, where the national hero was the volunteer firefighter who said on camera that the prime minister should “get fucked” and if we believed in such a thing as a centre we would say this was the summer the centre no longer held. But it’s also the summer where we held and kept faith with each other: people saving their neighbour’s home at risk to their own safety; people driving hundreds of kilometres with food for fire-affected regions; people donating water to drought-stricken farms; every coffee shop and concert and celebrity hosting a fundraiser. It’s the summer when what we thought of as the union between humans and the natural world seemed suddenly and irrevocably rent. The contract was nullified. The collapse appeared total at times. A billion animals died. Some species may never return. It’s the summer when climate change stopped being something we talked and argued about, an abstract thing to be debated. Instead it was the ash we breathed into our bodies, the devastation we saw with our eyes and pain felt in our hearts. When you feel it there, you can no longer deny it. * * * Those of us several degrees removed from the fire zone – who had stayed in the cities or on the unburnt side of Victoria – experienced fires as an anxiety-generating, feral echo. It was the smoke in the air and sudden drops in temperatures – 46 degrees one day, 20 the next, dust storms and large lumps of hail. This summer, Sydney was a ghost town, populated by sad looking people in breathing masks. The lockout laws were repealed but it seemed like a bad time to restart the party. People stayed at home and ordered UberEats. The world’s media reported our situation with alarm. For a nation that craves attention we should have been pleased to be first at something. But being on the frontline of a major climate catastrophe didn’t feel so good. Expats felt a sudden need to return home, the way you rush to the airport and board a red eye to visit a relative on their deathbed. Some immigrants who had come for the “lifestyle” were similarly rethinking their choices, and talking of making the journey in reverse. The good life in Australia wasn’t looking so good any more. For Australians, a collective grief emerged. We yearned for the past. All summer, we spoke of previous summers – long, dusky days and evenings at the beach, unbroken weeks of sunshine and clear water, skin browning, and the days and weeks merging like a hot, salty dream – as if they were scenes from another epoch, a lost idyll. Part of the anxiety of this summer was that the idyll might be gone forever. In its place was strange, unpredictable weather and humans and animals that moved through this weather tentatively and with a measure of fear. It’s the summer where for the first time I heard people, young people not usually prone to hyperbole or paranoia, speak plainly about the world ending. My Sydney GP told me he had seen four people already that day with climate change-related anxiety and depression. He was offering them antidepressants. At a lunch in Bondi, someone said we have 60 harvests left before the world will run out of food. That everyone she told was alarmed and also accepting of this, and that also no one seemed to know exactly what a harvest was (is it annual, we asked – like yearly??) seemed to be both an allegory and an explanation for how we had got here. After some weeks of weird weather and non-stop fire news around the start of January, there were detectable Weimar republic vibes. Strange currents moved between unlikely people. Friends who lead stable middle-class lives had uncharacteristic nights of chaos, and found themselves going home with strange men they’d met in bars, or on drug binges with kids they had mentored, or discussing with their friends whether they would or would not kill themselves if the climate catastrophe got too bad. I found myself deep in plans with strangers to join some agrarian farming collective they were starting in the Otways and accepting detailed advice about how to build a bunker on my property, which I was scared enough to actually contemplate because my fire plan involved escaping on a bicycle. Meanwhile, all throughout this summer, the smoke moved around us, like a viral social media campaign, documented and populating the social media feeds of people first in Sydney, then Canberra and later Melbourne. One day in mid-January I woke up in my cottage in central Victoria and there it was – a heavy taint in the air that had the previous day smelt so pure and clean, of oat grass dried in the wind. That day it was hot and a friend and I went swimming in a reservoir out of town. The water was brackish and warm and all around was the bush, dry, silent and crackling in the 35-degree heat. As we rounded the bend (dense forest on either side, the tall gums bent and meeting like a cathedral roof) the scene was not one we interpreted as picturesque but through the lens of the Pyrocene. “Imagine all this on fire,” my friend said as we drove through it. How much of this summer is not just imagining yourself in another’s place but is the collective collapse between imagination and reality? This summer we no longer have to imagine any more. • Brigid Delaney is a Guardian Australia columnist"
nan
"
Share this...FacebookTwitterHardcore German left-wing activist Tom Radke – citing selfish leaders, fraudulent science, psychological manipulation and a cult-like atmosphere within the German Greens – has had enough of the Fridays For Future (FFF) Germany movement and has announced his resignation.  

German left-wing activist, former FFF participant Tom Radke quits movement after experiencing its inner working. Image cropped from Tom Radke.de.
In a statement published at his site, Tom Radke wrote he wishes “to concentrate on left-wing patriotic politics: environmental protection, a strong welfare state and peace” instead of “Green voters and ‘green’ corporations”.
FFF not about the environment
Radke writes how he “found out through very negative practical experiences with Fridays for Future that many people never cared about environmental protection” and that he “misjudged the other ‘climate activists”.
He wrote: “Of all activists at FFF Hamburg, only a few were really interested in preserving our environment, clean air and healthy food.”
Radke blasts the FFF leader “Longhaul Luisa” Neubauer who he believes was “obviously in it for a career” and “her luxury life” while she called on others to save money.
Greta “taken advantage of”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Radke also had the impression that FFF movement leader Greta Thunberg was “being taken advantage of by her family but that she was “personally a nice person”.
Climate science “largely manipulation and fraud”
He recognized “the nonsensical and anti-people content of the climate movement” and that the movement’s “climate science is largely manipulation and fraud”.
Movement based on “pure emotions and blind faith”
Radke also explains how he came to realize that the FFF activists in fact had very little knowledge about climate science itself and that “their fanaticism is largely based on pure emotions and blind faith” and “also based on fear-mongering”.
“The young people are told that they, their families and everyone they love will die if we don’t act immediately.”
“Leaving the cult”
Radke accuses the German Green Party Using “a lot of psychological pressure” to coerce donations from followers despite the fact that “there are major donors in the background, who are completely unknown to the ordinary members.”
Radke also writes that the FFF movement “has nothing to to do with real environmental protection” and that “the CO2-tax serves to squeeze even more out of ordinary people.
He summarizes his departure as follows: “I feel a little like a person who is leaving the cult. It is a liberating feeling. Through my experience I will try to help other students to leave FFF and the climate religion.”


		jQuery(document).ready(function(){
			jQuery('#dd_47acf6f0d6d4644f09cfbc459ffd60db').on('change', function() {
			  jQuery('#amount_47acf6f0d6d4644f09cfbc459ffd60db').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterInternational and NASA solar scientists find their Total Solar Irradiance reconstruction extending to 1700 can “correlate well” with Earth’s global temperature records, including a positive net TSI trend during 1986-2008. A new Grand Solar Minimum is expected to commence during the 2030s.
Surface climate records that have been uncorrupted by coastal (ocean-air)/urbanization biases suggest there has been a long-term oscillation in temperature since 1900, with peaks during the 1920s-1940s and again during recent decades (Lansner and Pepke Pedersen, 2018).


Image Source: Lansner and Pepke Pedersen, 2018
An analysis by Soon et al. (2015) (full paper) indicated Northern Hemisphere surface temperatures from rural locations (unaffected by artificial urban heat) aligned well with trends in solar activity since the 19th century. However, models of greenhouse gas forcing did not correlate well with the long-term hemispheric record.

Image Source: Soon et al. (2015)

Image Source: Soon et al. (2015)


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A new paper (Scafetta et al., 2019) also finds the global temperature record aligns well with trends in TSI when using the observation-based ACRIM satellite data rather than the model-based (and IPCC-preferred) PMOD data for trends in recent decades.

Image Source: Scafetta et al., 2019
“By adjusting the TSI proxy models to agree with the data patterns before and after the ACRIM-gap, we found that these models miss a slowly varying TSI component. The adjusted models suggest that the quiet solar luminosity increased from the 1986 to the 1996 TSI minimum by about 0.45 W/m² reaching a peak near 2000 and decreased by about 0.15 W/m² from the 1996 to the 2008 TSI cycle minimum. This pattern is found to be compatible with the ACRIM TSI composite and confirms the ACRIM TSI increasing trend from 1980 to 2000, followed by a long-term decreasing trend since.”
“This model was extended using the ACRIM composite since 1981 and an average between VIRGO and SORCE TIM since 2013. This particular TSI model appeared to correlate well with the Earth’s global surface temperature records since 1700 [Hoyt et al., 1993, . … The TSI data from 1978 to 1981 appeared too corrupted because of uncorrected degradation of theNimbus7/ERB sensors during the solar maximum of cycle 21. For this reason, it was more appropriate to dismiss the data from this period because modifying TSI data using proxy models, as done by PMOD, would be arbitrary. We proposed that any reliable TSI composite should begin from late 1980 with the ACRIM1 record.”
“The same harmonic solar model suggests that the sun may now be heading toward a new grand solar minimum in the 2030–2040 time frame. Final evidence that TSI may have increased from 1980 to 2000 comes from Earth’s climate studies. Secular climate records correlate well with TSI curves such as the one depicted in Figure 13 and on longer ones covering the entire Holocene [1,23,60,64]. In particular, the warming observed from 1970 to 2000, followed by a temperature standstill since 2000, is a good fit for a natural 60-year cycle prediction superimposed to other contributions [20]. This pattern correlates better with a TSI evolution similar to the ACRIM composite [17–21,62,65] than with the CMIP5 general circulation climate model predictions of continuous anthropogenic warming [22]. The CMIP5 climate models use a high climate sensitivity to CO2 forcing and low secular TSI variability proxy models, such as the one proposed in [3], which was calibrated using the PMOD TSI composite model after 1980.”

Image Source: Lansner and Pepke Pedersen, 2018
Share this...FacebookTwitter "
nan
"Drones, robots and unmanned submarines used to be for military use only. But these days the technology is rapidly advancing and becoming more available for emergency services, farmers, film-makers or the public at large. Those of us working in wildlife research and management also recognise the potential of unmanned vehicles. Across the world today these machines are being used to monitor migrating birds, spawning salmon and orangutans, to map breeding habitats of endangered species, to track threatened caribou and polar bears in the far north, to examine nest contents of birds breeding in inaccessible locations, and to deter poachers in Africa.    As technology and industry continue to develop and the regulatory procedures begin to loosen, we’ll see conservation drones used in even more different ways in the field of wildlife biology. It’s something we’ve been researching at McGill University in Montreal since 2007. Here are a few of the things we are engaged in. We spent our first years using a small fixed-wing drone to successfully fly over and count flocks of Canada and snow geese.  Next, we used the plane to count terns nesting in a large colony in New Brunswick, and were also able to map their favoured nesting habitat. We managed the same for threatened least bitterns in southern Quebec. Efforts to learn about and protect birds of prey are sometimes hampered by their nesting locations – often found in remote areas, on rocky ledges or high up in tall trees. The number one source of mortality on the job for wildlife biologists is dying in a light plane or helicopter crash. A better way to monitor the contents of raptor nests is to use a small GoPro camera attached to a light-weight rotary-winged drone. So far we have used such a machine to survey the nests of ospreys, bald eagles, ferruginous hawks and red-tailed hawks – 113 flights in all. We obtained high-quality images of the nest contents in the vast majority of flights, allowing for an accurate count of eggs or nestlings, as well as useful estimates of the nestling ages.  More importantly, at each nest, we measured how the parents responded to the unexpected visitor. We recorded key behaviour such as how far the birds initially flew away or whether they demonstrated aggressive defensive behaviour, calling out or attacking the drone.   Finally, we approached active nests on foot without flying the aircraft, but still recording parental behaviour. This allowed us to sort out the amount of disturbance caused specifically by the drone versus only human presence at the nest site.  Our work demonstrates that drone aircraft can be a valuable tool for monitoring raptor nests, allowing for a flexible schedule of quick checks with minimal disturbance. Drones are also safer and more accurate than nest checking from light manned aircraft.  Drones can help conservation efforts in non-avian animals too. In Goose Bay, Labrador on Canada’s remote north eastern coast, we attempted to compare the use of a fixed-wing drone with a manned helicopter to acquire images of woodland caribou. The imagery obtained was sufficiently high quality for us to find and identify caribou and even differentiate between adults and calves.  Of course sometimes we may miss animals because they are obscured by trees or  boulders, but we are hoping to be able to factor this into our analyses. We are currently working on estimating the necessary corrections by conducting some detectability studies using plywood sheets as surrogate caribou targets placed in various kinds of habitats, using open versus dense. Eventually we hope to use drones to survey large areas and estimate caribou numbers.    Drones aren’t just used for conserving endangered species – they might also help manage unwanted invaders such as the European starling. These birds were first introduced to North America in 1890 when 60 birds were released in New York. Today, they number 200 million across the continent. Starlings compete with native birds for food and nest sites and a flock can decimate food crops, especially vineyards.  Scaring starlings away from grape crops using stationary propane cannons is ineffective and shooting them with guns is destructive, but what if dispersing these birds could be done non-lethally by using drones? We have designed a mount to attach 4 bear-bangers (loud noise-makers) to a small octacopter. We have conducted some banger firing tests to confirm that everything works safely, and we will begin test flights in several vineyards in the Okanagan Valley in British Columbia later this year. Another area of interest to us is tracking wildlife bearing radio transmitters. The objective is to mount an antenna/receiver that will not only pick up signals from the transmitters on the animals so as to identify individuals but also relay the information to a ground station.   As a test case, we are attempting to develop a receiver small and light enough to be affixed to a rotary drone so that we can pick up signals from song sparrows nesting on the Gulf Islands in British Columbia to know that the birds have returned from their wintering grounds. Thus far, we have been able to mount a receiver on a rotary drone and successfully determined that it can pick up signals from transmitters.  Field tests on the wild sparrows begin in 2015."
"Curious Kids is a series for children of all ages, where The Conversation asks experts to answer questions from kids. All questions are welcome: find out how to enter at the bottom of this article.  What is a species? – Finlay, age four, London, UK Thanks for the question, Finlay. In the past, it seemed like a sensible and simple idea to put living creatures – including animals, plants, fungi, bacteria and so on – into different categories called “species”.  Scientists mostly told different species apart from the way they looked, or where they could be found. But sometimes that proved very tricky indeed.  For example, it’s clear that giraffes and mice are very different groups of creatures, and that you can easily tell them apart; in other words, they are two different species. But what about those two little brown birds, which look so similar?  It was also easy to say that emus and ostriches were probably different species, even though they look a bit similar, because they live on different continents, so they must be different groups of birds. As time passed and scientists got to study more and more creatures, they realised that some creatures could be quite different, but still be part of the same group.  Have you seen a peacock and a peahen next to each other? In the animal kingdom, mums and dads can look quite different, but they should definitely be part of the same species.  So, by the end of the 1800s, the scientists realised that you can’t always decide if creatures belong to the same species, just by how they look.  Around this time, a man called Charles Darwin started to convince other scientists with his idea of evolution: he showed how creatures change over time, to become better at living in their environment.  One example is how the peppered moths in the UK changed from light to dark colour, after pollution from factories darkened the tree trunks where the moths like to hide. Light coloured moths became easy to spot and got eaten by birds, while the dark coloured moths survived and had more dark coloured babies.  By the beginning of the 1900s, scientists also started to understand that a baby looks like their mum and dad because some kind of information is passed between parents and their children, inside their bodies. Nowadays, we know that this information is called DNA.  With this knowledge, scientists decided that it’s better to define a species as a group of living things that can exchange DNA, by creating “viable offspring”. “Viable offspring” means a baby that can survive, and make babies of its own later on.  This is important, because some species can make babies together, like a zebra and a horse. But this baby – called a zorse – is sterile; it cannot have babies of its own.  This “viable offspring” definition of species is useful, and it’s the one that scientists rely on most often today.  But if you want to have some fun and see a scientist get very hot under the collar, you could mention that sometimes it’s possible to bring together, which would never meet without the help of humans, and that they can produce a viable offspring. This is the case with tigers and lions. They do not exist in the same location, but humans have bred them together to produce the “liger”. Now what? Are they the same or different species?  As you can see, defining species can get tricky… But I think most scientists will agree that if these two groups wouldn’t meet and have babies without humans getting involved, they are probably two different species.  So how can we define a species? Well, in most cases the “viable offspring” test will work just fine.  You just need to remember that groups of creatures are constantly evolving, so sometimes the differences between species might become a bit blurry.  Hello, curious kids! Have you got a question you’d like an expert to answer? Ask an adult to send your question to us. You can: * Email your question to curiouskids@theconversation.com 

* Tell us on Twitter by tagging @ConversationUK with the hashtag #curiouskids, or

* Message us on Facebook. Please tell us your name, age and which town or city you live in. You can send an audio recording of your question too, if you want. Send as many questions as you like! We won’t be able to answer every question, but we will do our best. More Curious Kids articles, written by academic experts: How do moths eat our clothes? – Albie, age five, Australia Why do leaves change colour? – Isaac, age eight, Guildford, UK Why do we need food? – Milo, age five, Cowes, Australia"
"Expanding Heathrow airport is unlikely to be compatible with the UK’s target of net zero carbon emissions by 2050, leading scientists have said, adding that government policies are lacking in many other key areas from home insulation and transport to carbon capturing. Achieving the net zero goal will require sweeping policy changes, but scientists are concerned that little has so far been forthcoming from ministers.  Home insulation and energy efficiency are essential to reducing carbon emissions from heating, but have been left in a “policy vacuum”, said Dr Charlie Wilson, a reader in energy and climate change at the University of East Anglia. Systems to capture and store carbon dioxide would also be needed, but previous government attempts to kickstart the technology over the past two decades have been abandoned, said Jim Skea, professor of sustainable energy at Imperial College London. Skea said the UK had many advantages over other countries in CCS, such as spent oilfields in the North Sea to use as storage, the skills and infrastructure from oil and gas exploration, and public backing for the technology, but needed the government to step in. Corinne Le Quéré, professor of climate change science at UEA, called for the government to prioritise the net zero goal across all policy areas. “Every minister should have a plan for their own [policy area] on how to reach net zero,” she said. “Some infrastructure must be phased out. Ministers should be preparing in a way that is well coordinated and fair, so that the public are brought on board.” No part of government, and no part of the economy and society, could be left unaffected if the push towards net zero was to be successful, said Skea. “We really need to do it all,” he said. “Nothing can be wiped off the table. No sector can be left to not contribute – really, this needs contributions from absolutely everything.” There is no cabinet minister with responsibility for achieving the net zero target, and no intermediate goals before 2050, apart from the advice on carbon budgets from the Committee on Climate Change (CCC), the government’s statutory advisors. On Thursday, the CCC called for major changes to the UK’s land use, including more trees, the restoration of peatland and an end to the burning of grouse moors, to improve the UK’s carbon sinks. The expansion of Heathrow, which was given a green light under Theresa May, was not consistent with the net zero goals, a group of six experts said on Friday. “I find it difficult to imagine we can control emissions from aviation if we continue to build airports,” said Le Quéré. “We absolutely need ambitious plans for mobility and transport. We need a plan that covers roads and airports.” Pierre Friedlingstein, professor of mathematical modelling of climate systems at Exeter University, said: “This is a clear example of not going in the right direction.” Wilson said: “We desperately need consistent, concerted direction [from the government] and building new airport capacity is not that.” Skea said it was possible for people to continue taking flights under a net zero target, but that the rapid growth of flying in recent years must be curtailed. “It’s not that people must stop flying, but that the increase in flying should be less than it otherwise would be,” he said. One way of achieving this fairly would be through a frequent flyer levy, according to Lorraine Whitmarsh, professor of environmental psychology at Cardiff University. “The sense that it is a fair policy is very important,” she said. “These policy interventions are likely to be quite effective.”"
"
Share this...FacebookTwitterHat-tip: Die kalte Sonne
In 2019, weather-related events in Germany caused insured damage to houses, household contents, commerce, industry and motor vehicles amounting to 3.2 billion euros. This is the result of preliminary figures published in a press release by the German Insurance Association (GDV).
The level is thus at the previous year’s level and below the long-term average of around 3.7 billion euros.
“Despite the storm and hail damage to motor vehicles, the overall natural hazard balance is slightly below average”, said GDV President Wolfgang Weiler.
What follows is the GDV annual chart for weather-related damage (in 2019- based euros):









Source: GDV
Insured damage has been below the average for 6 consecutive years, despite, the alarming tones one reads in the GDV press release.
There were also fewer losses due to storms and heavy rain in property insurance. Windstorm and hail and other natural hazards such as heavy rain caused damage amounting to EUR 2.2 billion, which is below the long-term average of EUR 2.7 billion.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“The below-average balance should not hide the fact that there have been repeated heavy local rains with high damages”, Weiler said. “All in all, the year 2019 stands for a number of severe storms, great heat and severe local flooding and is therefore characteristic of extreme weather in Germany as well.”
Experts: Central Europe weather “not more extreme”
Meanwhile Die kalte Sonne site here comments:
Fact: The weather in Central Europe has NOT become more extreme. The only exception is heat waves. The Austrian Central Institute for Meteorology (ZAMG) states that a trend towards more extreme weather in Austria is generally not noticeable:
‘It should be anticipated from the detailed discussion of the development of extreme values in the following sections heat (air temperature) heavy precipitation (precipitation) and storms (wind) that all in all the climate has not become more extreme in the last 200 years. According to the only suitable basis for this assertion – long and quality-checked measurement data – climate variability in Southern Central Europe remained the same or even decreased’.
A similar assumption can be made for the neighboring country Germany. For transparency reasons, the German Insurance Association (GDV) should finally admit this to its customers. Instead, the press release concludes with an advertising message that citizens should please insure themselves even more comprehensively against extreme weather.”
!!!
Share this...FacebookTwitter "
nan
nan
"
Share this...FacebookTwitterThe “Streetscooter”: Electric Mobility’s First Large Bankruptcy

Image: From Superbass – own work, CC BY-SA 4.0, commons.wikimedia.org
By AR Göhring, European Institute for Climate and Energy (EIKE)
(Text translated and summarized by P. Gosselin)
Manufacturer “Streetscooter”, purchased by Deutsche Post (German Post) in 2014, will be scrapped. The German media of course blame it on “bad management” by the large company.
Which city dweller doesn’t know the small, yellow electric scooters of the German post office that the postmen and women deliver letters and small packages to citizens comfortably and efficiently? Not long ago I received news via Facebook on how the e-delivery-vehicles just barely made it back to the post office, especially in winter, and only when the heating is off.
Now the management of the Swiss Post is also following suit and ending the experiment with delivery street scooters.
The company used to be a small startup, a young dynamic private company in a “sexy” field – just like artificial intelligence or climate protection technology. Deutsche Post bought the company with the benevolent support of the eco-loving press and used it to polish up its otherwise staid image a bit.
However, any PR coup based on electro-chemistry ultimately has to prove itself in everyday life over years. The post office scooters obviously couldn’t. Pushing an electric vehicle still loaded with letters back to the local depot when the battery is empty is not possible: the scooter is too big and heavy for that. Or you have to plan shorter routes (in winter), which reduces efficiency. Since letters are only delivered during the day, the scooters can be conveniently charged at night. But if you have to reload during working hours, it takes hours, and you don’t have the time for that.
Berlin e-bus failure


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Take, for example, the Berlin E-bus experiment: the lithium buses run from 8 to 12 a.m., then the diesel vehicles take over. Our speaker Prof. Alt talks in this context about a double infrastructure, which is of course also roughly twice as expensive. Presumably Deutsche Post had to manage a similarly inefficient double fleet of about 13,000 street scooters. The scooters broke down more often and then soon had to be repaired, and replaced by diesel-powered delivery vans.
A commentator from ntv television, however, blames it on the slow management of Deutsche Post: A project like an electric fleet of electric cars has to be run by flexible start-up managers with heart and attitude, then it would work.
The Streetscooter deserved a dynamic, creative and risk-taking management – and the opportunity to obtain the necessary funds independently on the capital market.”
This claim is not convincing. Whether it’s a startup or the Deutsche Post, both must adhere to the main laws of physics and economics. One thing must never be forgotten: Deutsche Post is a business group that has to make money.
The city administration of Berlin, on the other hand, can waste money at will with misguided planning. They work with funds from taxes levied by force. And Berlin’s eco-socialist politicians, who are poor in arithmetic, are elected and are not held accountable for their failures with their own private assets.
Of course some will claim that Tesla has achieved what the N-TV quote above calls for. But this is not true: Elon Musk is an eco-media darling who has already received billions of dollars in US subsidies. Without these billions he would have long since gone bankrupt or become a mini-manufacturer for a niche.
We Germans are now experiencing the same thing in Brandenburg: because Merkel’s “grand coalition” wants to have a share in the media sexiness of Tesla, the “Gigafactory” is being heavily subsidized there.
The fact that an entire forest is being cut down and cheaper Polish workers have to be hired is of no consequence to someone like Federal Economics Minister Peter Altmaier. The press as well.


		jQuery(document).ready(function(){
			jQuery('#dd_3a04978ab8a5ce78325435c2a3e12065').on('change', function() {
			  jQuery('#amount_3a04978ab8a5ce78325435c2a3e12065').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"This is an article from Head to Head, a series in which academics from different disciplines chew over current debates. Let us know what else you’d like covered – all questions are welcome. Details of how to contact us are at the end of the article. Sharon George: Plastics are ingrained in our everyday lives. Since 1950, it’s estimated that we have produced billions of tonnes of plastic, and most of this is not recycled. Plastics have spread around the world through oceans, rivers and the air to every part of the planet. In rivers and oceans, plastic moves vast distances and is now found right through the water column of the oceans, from the surface to the deepest trenches. We don’t yet know how long this material will prevail in these environments but it will certainly be longer than the lifespan of the person who used it. And it’s accumulating. This impact, like rising CO₂ levels, is man-made. No amount of recycling schemes and ocean clean-ups are going to totally remove the mark that we have left. Plastics are a scar that will hopefully warn future generations of the folly of unsustainable over-consumption. Matt Edgeworth: As well as spreading through air, rivers and ocean currents, plastics are finding their way into soils, landfills and deep ocean sediments. As such, they are infiltrating into strata – layers of rock and mud in the ground – thus becoming part of the archaeological and geological records. They are not only to be found in surface environments, where they are highly visible, but are getting into subsurface layers too.  In some of these buried environments, plastic objects stand a good chance of being fossilised – a process whereby the hard material may decay or dissolve, but its exterior form survives as a mould which then gets filled with other minerals and becomes a cast of the original object. In this way, familiar forms of commonplace plastic items such as drinking straws may survive as traces in the ground not just for a few hundred, but for millions of years. SG: Polymers from natural materials were used by people as early as 1200BC, when the Olmec people used latex and vine extract to create rubber. In the 1840s, sulphur was used to vulcanise rubber, stabilising it and making tyre production possible. A closely-related material, gutta-percha, is a natural latex. This early thermoplastic was used from the mid-1800s, enabling telegraph wires to be laid at the bottom of the sea and electrical wires to be insulated. Other natural polymers resembling modern plastic were developed from cellulose, a natural polymer found in wood. The first, parkesine, was developed to produce celluloid in 1870, a medium for cinema film. But it was during the 20th century that plastics really took off. In 1907, Leo Baekeland invented the first synthetic plastic, bakelite, from fossil fuel-based chemicals. These revolutionary plastics were easy to mould and could be quickly mass produced. These materials were popular, cheap and built to last. The pace of development increased and by 1935 other polymers, such as polystyrene, polyester, PVC, polythene and nylon were all being manufactured from fossil fuels. ME: That’s an excellent historical summary of plastics as a modern material. But as an archaeologist, I see the development of plastics as part of a much longer and broader set of technological trends, extending back over the last 20,000 years or so to the first appearance of “novel materials” in strata.  Novel materials are characterised by their geological novelty, being unprecedented in earlier deposits. Made by humans rather than by natural process, they are entirely new in the four-and-a-half-billion-year history of the Earth. Ceramics appeared first – then bricks and tiles, glass, metal alloys, concrete, paper and so on. Look in any rubbish dump today and you will find all these novel materials and more in the profusion of items thrown away.  Plastics may be a relatively new development, but they are part of this trend towards greater diversification of humanly manufactured materials, all of which eventually find their way into the ground. SG: Despite their relative novelty and permanence, we continue to produce and pour plastics into the environment. Today it is thought that around 80% of the 8.3 billion tonnes of plastic ever made is still somewhere out there. ME: A large proportion of plastic waste is being dumped in landfill, where it remains “out of sight, out of mind”. What we see in the oceans is only the more visible tip of a largely invisible iceberg. Let me give an example. Close to my home in south Bedfordshire are a series of landfilled quarries, now low artificial hills. The old clay pits, some of them over a kilometre wide and up to 55m deep, made convenient receptacles for landfill waste from London and other nearby cities and towns from the 1980s on. Even when the pits were full to the brim, more landfill was mounded up to form hills.  If you stand on top of the highest of these newly created hills, where the former quarry was deepest, over 65m of compacted landfill lies directly beneath your feet. In landfills of similar age in the USA, the proportion of plastic was found to be 20-24% by volume when sorted, reducing to about 16% by volume when compacted in the ground. Assuming the same proportion of plastic at this location, that would be the equivalent of a layer of plastic 10m thick.  Meanwhile, landfill material by no means always stays where it has been deposited. For example, many thousands of landfills are situated in lowland situations and therefore at risk from marine incursion, especially in view of anticipated sea-level rise due to climate change. Coastal erosion, river flooding and tsunamis can decimate landfills, leaving the heavier material where it is but taking the lighter and more mobile material such as plastic away. A significant proportion of plastic currently in the oceans derives from inundated landfills. SG: Once plastic escapes from landfill it will continue to degrade into smaller fragments. It can be ingested by creatures like birds and fish and get into drinking water. Microplastics and nanoparticles are already showing up in our food chain and tapwater.  The risks to human health of ingestion of nanoplastics is not fully known. But with rising numbers, our exposure is bound to increase and people reliant on fishing from highly polluted regions will be more exposed. The pollution is wreaking havoc on wildlife, with animals being entangled or ingesting the plastic. Around 90% of seabirds have ingested plastic. We are adding around 8m tonnes of waste every year to the ocean and unless this stream of waste is cut off, the problem is going to get much worse. ME: But with all the focus today on the harm caused by plastics in rivers and oceans, is there not a danger that putting plastic waste into landfill might be seen as a less controversial alternative? In conveniently burying it out of sight, often oblivious to the fact that it might get released back into the wider environment at a later date, are we just storing up problems for future generations? Should we not be focusing on the dangers of disposal of plastics in earth as well as in water? SG: Yes, we should be focusing on the dangers of plastic disposal in earth as well as in water. I think there is a real danger that people could assume that locking it in landfill means it will stay put until it finally degrades. ME: It is generally assumed that plastics will decay in just a few hundred years or less, but more scientific research needs to be undertaken on this crucial point. It seems likely that plastic in the ocean will break down relatively quickly into microparticles, to be ingested into the food chain or otherwise to sink into the sediment on the ocean floor.  But plastics buried in earth could prove to be much longer lasting. Archaeological studies show that some modern landfills are so tightly sealed and capped they protect material within from erosive forces, effectively mummifying them. Neither rain nor sunlight nor air can penetrate in, and decomposition processes typically slow down after 20 years or so. In such an artificially sheltered environment, materials like paper and plastic may survive for surprisingly long times. SG: Deep sea environments at low temperature, low light preventing photo degradation and higher pressure are thought to have a preserving effect too. But landfill could be just preserving this waste as well? It’s amazing to think that future archaeological finds could be the everyday items and gadgets we are using today.   ME: Well, a recent excavation of Atari computer game products buried 10m down in a New Mexico landfill in 1983 revealed that the plastic game cartridges with associated packaging and shrink-wrapping showed little sign of decomposition after 30 years in the ground! Those cartridges which were not crushed were still playable, and were sold on eBay for thousands of dollars. SG: Despite its durability, we are using plastic as if it is disposable. Efforts to recycle have been shown to be seriously flawed, with supply chains for waste not all that they seemed.  Earlier this year it was reported that the UK sends out around half of its recycling abroad with insufficient checks on what was actually happening to it. The system was found to be open to fraud, leading to concerns that instead of being recycled the waste was being dumped in landfill, rivers and oceans. Exporters of UK waste were sending out contaminated and worthless mixed waste and fraudulently claiming the recovery notes that they would then sell.  It is clear that recycling alone, in its present form, does not work. We have to either stop using plastics or find alternative routes to dispose of plastic waste in a more sustainable way. Consumers are much more aware of the impact of plastics in the environment and are receptive to change, but there are limited choices. ME: Yes. We need to explore possible alternatives, for example by re-using plastics in large-scale engineering projects such as road and building construction. There are promising projects currently underway looking at the feasibility of using processed plastic waste as a replacement aggregate in concrete manufacture. SG: Using plastic waste to replace raw materials makes a lot of sense. Pyrolysis is a really good way to break down the plastic to produce raw materials.  We could also develop new ways to break it down faster into useful chemical components. One way would be to digest the plastics using enzymes. Fungi and bacteria have been found that have a taste for plastic and can break it down to be able to use the carbon from it. Scientists in Portsmouth tried to reproduce the enzyme that PET-eating bacteria uses, and accidentally produced an even more efficient enzyme.      Reprocessing the plastic like this would help cut the amount of waste building up as pollution. Long-term, even if we stop pouring this waste into our environment, we will still shed millions of fibres and microplastics through washing synthetic clothes and wearing down tyres.  The problem is that conventional plastics are just too cheap. A price increase would level the playing field to make the use of new plastics more expensive compared to recycled plastic. Because the price of plastic should include the true downstream disposal costs. If this was the case, then we could afford better waste sorting and more viable recycling facilities to prevent the exporting of waste. ME: We’re both agreed that the amount of plastic being produced and discarded without being recycled is doing irreparable harm to other creatures and habitats. So much plastic is choking river and ocean environments that many forms of life are threatened. So much plastic is entering geological cycles that it is creating a substantial stratigraphic signal of the Anthropocene in its own right. Even when the plastic itself has decomposed, the forms of some of the plastic objects that now litter the ocean floor may be preserved as trace fossils in sedimentary rock.  We might imagine picking up a stone in tens of millions of years’ time and finding – instead of the shells of former sea creatures – the shapes of cotton buds, coffee spoons, fishing nets, CD cases, water bottles, biros … SG: and game cartridges. If there’s a specific topic or question you’d like experts from different disciplines to discuss, you can:"
"

The Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   




The new paper’s lead author is Jonathan Gregory of the U.K.’s University of Reading, and the other authors are a who’s who of sea level researchers (repeating my professions nauseating belief that putting a large number of authors (most of whom have—at best—just read the manuscript) somehow makes it more persuasive). The paper concludes that the causes of sea level rise, and its temporal variations, across the 20th century were many, and that a link to anthropogenic global climate changes has been weak or absent over this period. Basing future sea level rise projections on a presumed historical relationship between anthropogenic global warming and corresponding sea level rise turns out to be a bad idea.   
  
Here is how Gregory et al., 2012 put it:   




The implication of our closure of the [global mean sea level rise, GMSLR] budget is that a relationship between global climate change and the rate of GMSLR is weak or absent in the past. The lack of a strong relationship is consistent with the evidence from the tide-gauge datasets, whose authors find acceleration of GMSLR during the 20th century to be either insignificant or small. It also calls into question the basis of the semi-empirical methods for projecting GMSLR, which depend on calibrating a relationship between global climate change or radiative forcing and the rate of GMSLR from observational data (Rahmstorf, 2007; Vermeer and Rahmstorf, 2009; Jevrejeva et al., 2010).



And here are the main conclusions, now seriously questioned, from the semi-empiricical citations included in the above quote:   
  
Rahmstorf (2007):   




When applied to future warming scenarios of the Intergovernmental Panel on Climate Change, this relationship results in a projected sea-level rise in 2100 of 0.5 to 1.4 meters above the 1990 level [by 2100].



Vermeer and Rahmstorf (2009):   




For future global temperature scenarios of the Intergovernmental Panel on Climate Change’s Fourth Assessment Report, the relationship projects a sea-level rise ranging from 75 to 190 cm for the period 1990–2100.



Jevrejeva et al. (2010):   




With six IPCC radiative forcing scenarios we estimate sea level rise of 0.6–1.6 m, with confidence limits of 0.59 m and 1.8 m.



Seems like three strikes against projecting those high rates of sea level rise.   
  
For a little reality check, the current rate of rise is somewhere in the range of 1.8 to 3.5 mm/yr (0.07 to 0 .14 in/yr) depending on the time period over which you calculate the trend.   
  
Further, as we have previously written, it doesn’t look as if the recent increased rates of ice loss from Greenland and Antarctica are sustainable—much less going to linearly increase to the end of the century. All of this strongly argues that the 21st century sea level rise is not a problem that we can’t keep up with.   
  
**References:**   
  
Gregory, J., et al., 2012. Twentieth-century global-mean sea-level rise: is the whole greater than the sum of the parts? _Journal of Climate_ , doi:10.1175/JCLI-D-12-00319.1, in press.   
  
Hansen, J.E., 2007. Scientific reticence and sea level rise. _Environmental Research Letters_ , **2,** doi:10.1088/1748-9326/2/2/024002   
  
Jevrejeva, S., et al., 2010. How will sea level respond to 1019 changes in natural and anthropogenic forcings by 2100? _Geophysical Research Letters,_ **37** , L07703, doi:10.1029/2010GL042947.   
  
Rahmstorf, S., 2007. A semi-empirical approach to projecting future sea-level rise. _Science_ , **315** , 368–370, doi:10.1126/science.1135456.   
  
Vermeer, M. and S. Rahmstorf, 2009. Global sea level linked to global temperature. _Proceedings of the National Academy of Sciences,_ 106, 51, 21527–21532, doi:10.1073/pnas.0907765106.


"
"
I just watched a presentation Elsi Sertel from a university in Turkey showing how easy it is to introduce true land cover data into a climate model. Her study area was around the Black Sea near Istanbul, and used LANDSAT imagery along with a pixel by pixel truthing technique to determine the type of land cover, sea, forest, urban, etc and apply it to use in a GCM.
Her premise was that current GCM’s use land surface info that isn’t fully representative, out of date, and in some cases just plain wrong.
Her study showed a technique that allowed for a significant amount of automation to the process, to allow improved and current land surface types to be easily integrated into the grid cells of a GCM. Unfortunately, some GCM gridding schemes are too coarse to handle such data.
From what I’ve seen in this conference so far, and I’ve seen presentations now from Europe, Turkey, China, Australia and the USA, it is becoming more clear that land use is a major driver of climate change, and perhaps dwarfs even GHG effects. That’s just a hunch. One study from Australia showed the effects of removing a woody type bush over a large area over the past century, and the results on rainfall and temperature were profound.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4691079',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

You know you’ve reached critical mass in an argument when you start having editorial cartoons drawn about you.
In this weeks Chico Beat, the editorial cartoon above appeared. While editor Tom Gascoyne would not admit to it being my caricature that was used, a call to artist Steve Ferchaud in Paradise confirmed he used me at Tom’s suggestion of my name.
I consider it high praise to be drawn by Ferchaud, but not so high to be in the Chico Beat.
In any event, by the end of the year 2017, ten plus years from now, we’ll know for sure who’s right. I think it will start to be cooler due to the solar cycle starting to dampen.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea812990b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterEven NASA says it:
Without the Earth’s greenhouse gases (GHG) in the atmosphere, the planet would be on average a frigid -18°C.
But because of the preindustrial 280 ppmv CO2 and other GHGs in our atmosphere, the average temperature of the Earth thankfully moves up by 33°C to +15°C (see chart below), based on the Stefan Boltzmann Law.

 
Global warming theorists say the Earth’s surface preindustrial temperature was supposed to be 15°C. And today CO2 is supposed to have warmed it another degree, to 16°C. Image: www.klimamanifest.ch.
And because CO2 has since risen to about 410 ppmv today, the global temperature supposedly should now be about another 1°C warmer (assuming positive feedbacks) bringing the average earth’s temperature to 16°C.
And once the preindustrial level of CO2 gets doubled to 560 ppm, later near the end of this century, global warming alarmists insist the Earth’s temperature will be near 18°C, see chart above.
So we are now supposed to be at 16°C today and warming rapidly. But what is the globe’s real average temperature today? 15.8C? 16.0C? 16.5°C?
Answer: astonishingly the official institutes tell us it is only 14.7°C!
For example, data from the World Meteorological Organization (WMO) shows us the global absolute temperatures for the previous 5 years:

Image: www.klimamanifest.ch, data source: WMO in Geneva.
As the image above shows, the global absolute temperature last year was just 14.68°C.
This is 0.32°C COOLER than the 15°C we are supposed to have with 280 ppmv, and a whopping 1.32°C cooler than the 16°C it is supposed to be with the 410 ppmv CO2 we have in our atmosphere today.
So why are we missing over 1.3°C of heat? Why is there this huge discrepancy between scientists?
German scientists say Earth temperature was 15.5°C – in 1990!
In May, 1990, even the German government stated in its major report on climate (BT-DRS 11/8030, p. 29) that the global average temperature back then was 15.5°C and that the natural temperature was supposed to be 15°C — like NASA says. The German government reiterated that figure in 1992.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In 2003, renowned German climate scientist Prof. Mojib Latif also confirmed in his book that 15°C was the “optimum temperature” for the Earth.
And in his 2003 doctorate dissertation, Tim Staeger wrote that the natural Earth’s temperature without man-made impacts is “about 15°C”.
In fact, practically all German textbooks used at schools today say that the “natural temperature” of the Earth due to the greenhouse effects of the atmosphere is a “life-friendly average of 15°C” instead of -18°C.
PIK scientist: “15°C in 1850”
Moreover, Potsdam institute for Climate Impact Research (PIK) scientist Anders Levermann testified before the German Parliament last year, and confirmed that the global mean temperature of the Earth back in 1850 was 15°C, which means today it is supposed to be well over 16°C. So why is the WMO telling us it’s only 14.68°C and others like the PIK and NASA saying it’s about 16°C?
This massive discrepancy needs to be explained.
IPCC 2007 report in line with WMO
The IPCC indeed contradicts all the scientists and media who claim the global average temperature was 15°C back in 1850. Here’s the IPCC chart from the 2007 report:

How can 14.68°C be “too hot”?
As the chart above shows, the IPCC stated that the global mean temperature in 1850 was a relatively frigid 13.7°C – i.e. well below the “life-friendly” average of 15°C.
Today we are still below the 15°C, and so how can it be too hot?
Summary
There’s no doubt it’s gotten warmer since 1850, the peak of the Little Ice Age. But it’s clear nobody knows what the globe’s real average temperature is. Figures are being wildly tossed around. If we are to believe the IPCC’s 14.7°C figure, then we are still too cool and there is absolutely no warming crisis.
Scientists need to answer this question behind this huge discrepancy quickly. Would the real global temperature please stand up!
============================
Hat-tip: Das Klimamanifest von Heiligenroth
Share this...FacebookTwitter "
"Climate breakdown and the global crisis of environmental degradation are increasing violence against women and girls, while gender-based exploitation is in turn hampering our ability to tackle the crises, a major report has concluded. Attempts to repair environmental degradation and adapt to climate breakdown, particularly in poorer countries, are failing, and resources are being wasted because they do not take gender inequality and the effects on women and girls into account.  Campaigners called for governments and institutions to take note, saying that the impacts on women and girls must be at the heart of any viable strategies on the climate and ecology. The International Union for the Conservation of Nature (IUCN) carried out what is understood to be the biggest and most comprehensive study yet of the issue, taking two years and involving more than 1,000 sources of research. “We found gender-based violence to be pervasive, and there is enough clear evidence to suggest that climate change is increasing gender-based violence,” said Cate Owren, a lead author of the report, published on Wednesday. “As environmental degradation and stress on ecosystems increases, that in turn creates scarcity and stress for people, and the evidence shows that, where environmental pressures increase, gender-based violence increases.” Six in 10 respondents to a survey by IUCN, with more than 300 responses from organisations around the world, said they had observed gender-based violence among female environmental rights defenders, environmental migrants and refugees, and in areas where environmental crimes and environmental degradation were taking place. More than 80 case studies clearly showing such links were uncovered as part of the research. Gender-based violence includes domestic violence, sexual assault and rape, forced prostitution, forced marriage and child marriage, as well as other forms of the exploitation of women. The report found human trafficking rises in areas where the natural environment is under stress, and links between gender-based violence and environmental crimes such as wildlife poaching and illegal resource extraction. “Gender-based violence is one of the most pervasive but least talked-about barriers that face us in conservation and climate work,” said Owern. “We need to take the blinders off, and pay this concerted attention.” Owren found abundant examples of the close links between gender-based violence and the exploitation of women and girls, and the competition for resources engendered by the impacts of global heating and our destruction of the natural environment. For instance, sexual abuse was found in the illegal fishing industry in south-east Asia, and in eastern and southern Africa fishermen reportedly refused to sell fish to women if they did not engage in sex. The illegal logging and charcoal trade in the Democratic Republic of Congo is linked to sexual exploitation, and in Colombia and Peru illegal mines are strongly associated with an increase in sex trafficking. There have also been numerous examples of gender-based violence directed against environmental defenders and activists, who try to stop the destruction or degradation of their land, natural resources and communities. Sexual violence is used to suppress them, undermine their status within the community and discourage others from coming forward. Yet few projects that are aimed at conservation and improving the environment, or tackling the climate crisis, display any recognition of these issues, according to the report.  Global heating puts pressure on resources, as extreme weather, including heatwaves, droughts, floods and fiercer storms, grows more frequent and devastating. In most parts of the world, women are already disadvantaged and lack land rights and legal rights, so are vulnerable to exploitation. When the additional stresses caused by the climate crises bite, they are the first to be targeted. For instance, in some communities, young girls are married off as early as possible when the family faces hardship exacerbated by the climate. Globally, about 12 million more young girls are thought to have been married off after increasing natural disasters, and weather related disasters have been shown to increase sexual trafficking by 20-30%. Women and girls are also burdened with tasks such as drawing water and finding firewood, which are becoming more scarce in many areas under the ecological impact of our scramble for resources, and which expose them to further dangers of violence. Grethel Aguilar, acting director-general of the IUCN, said: “Environmental degradation now affects our lives in ways that are becoming impossible to ignore, from food to jobs to security. This study shows that the damage humanity is inflicting on nature is also fuelling violence against women around the world – a link that has so far been largely overlooked.” At the UN climate conference in Madrid last December, governments were criticised by campaigners for ignoring the plight of women and children and the threats they face.  Some governments are moving to put action for women and girls into their climate and development policies, and the UN in Madrid moved to include a gender action plan as part of the climate negotiations. Campaigners and some countries are hoping for even greater focus on the issue at the crunch UN climate talks in November, to be hosted by the UK in Glasgow. The UK’s department for international development said it was already factoring in gender issues in climate change funding, including a large-scale study on violence against women and girls during the humanitarian crisis in South Sudan, where about three quarters of women and girls who had been in a relationship experienced violence at the hands of their partner. A spokesperson said: “Women and girls can be disproportionately affected by climate change. This is why we’re spending UK aid on helping to promote gender equality, as well as leading the fight against climate change.” Bob Ward, policy and communications director at the Grantham Research Institute on climate change and the environment at the London School of Economics, who was not involved in writing the report, said: “This report highlights the complex but clear link between growing climate change impacts and violence against women and girls.  He pointed to the role that female campaigners were playing in bringing the world’s attention to the problems. “When we see the inspiring leadership of female activists like Greta Thunberg, we should recognise that the lives and livelihoods of women and girls around the world are particularly threatened by climate change,” said Ward. “The empowerment of women and girls and their protection from the direct and indirect consequences of climate change must lie at the heart of the just transition to zero-carbon and climate-resilient societies.” The report also provided a timely reminder that “concerted action to tackle inequality can unlock new opportunities for climate action and women’s empowerment”, added Mary Robinson, chair of The Elders. “We need to recognise the unequal effects of the climate crisis on women, but also that women’s participation brings with it creative and sustainable solutions to both the climate emergency and social injustices. Tackling climate change and environmental degradation without the full inclusion of women will not succeed: gender equality is a prerequisite to the collective effort needed to address the climate emergency.”"
"How would you move through a space when you can’t see the obstacles ahead? For example, how would you find your way out of a maze if you were blindfolded? You could either use your other senses, such as touch, to find your way out – or better yet, you could get someone who can see the way out to direct you. But either way you need information. For birds, non-visual information can provide the same helping hand while flying. Even though they can see the world around them, the air is a dynamic, invisible environment – and airflow is much more complex environment to move through than the ground with its static obstacles. Imagine you are hanging from a glider and racing to a finish line. You can see two other paragliders ahead of you, one looks like they are having a smooth ride and moving quickly, the other looks like they are in trouble and finding it difficult to control their glider. You would choose to follow the first one, right? By observing the other pilots around you and responding in accordance to what happens to them, you tap into information that helps you make a good decision and keep up with the race. Similarly, it makes sense that an animal may do the same to move through their environment – observing those around them that have the same objective. Soaring birds not only move through the air, they rely on updrafts, such as thermals (a column of warm rising air), to gain lift rather than flapping their wings. It’s a bit like a big game of invisible snakes and ladders – but the costs of sliding all the way down without finding the next ladder are high. They must reach that next ladder before they hit the ground. Like the gliders, birds that rely on soaring – including vultures – often share the air with other birds. But until now we didn’t know if soaring birds do indeed observe each other to “see” these invisible thermal ladders. For our recently published study, we designed an experiment that would test this idea. We tracked the movements of each bird in a small group of vultures at a bird of prey centre in the mountains of France, and recorded their behaviour. Only by using the latest tagging technology could we investigate this concept. Each bird had a backpack with a GPS logger, a movement logger and camera recording all aspects of their movement. In the movement logger was a range of sensors, sensitive to different movement types – an accelerometer to pick up wing beats, a magnetometer for directional changes, and an airspeed sensor.  There is a well known theory, by aeronautical engineer Paul MacCready, which states that birds and gliders should glide at high airspeed when they have just left a strong thermal and expect to be approaching another strong thermal. But gliding quickly is risky, as the flyer is more likely to hit the ground before reaching the next thermal. So our hypothesis was that these vultures, and other soaring birds, are able to take this risk and glide quickly when they have clues provided by the soaring of others, on the whereabouts of the next thermal.  When we mapped the movements of all the vultures and analysed their gliding airspeeds, we found that – on making a decision to leave one thermal and glide to the next – vultures which had tapped into this extra information by “eavesdropping” on the movements of others (they weren’t flocked together but were watching how the other vultures were acting) chose to take the risk and adopt significantly higher airspeeds than those going it alone.  This finding helps us to understand what is going on in the invisible world above us and just how these birds make decisions to navigate this challenging environment. We are all quite used to seeing birds such as pigeons or starlings flying in flocks, and it may be quite reasonable to assume that by flying together these birds are interacting with each other. But our work reveals for the first time that even when birds are not flying together in a flock, they may observe others to sense the world around them. This suggests, for vultures at least, that it is important that there are other birds in the sky with them, as numbers may be needed to maintain a healthy network of information."
"

The diagram above is central to the paper’s examination of the “spiral” nature of the earth to sun distance relationship, which affects noit only seasons, but longer term climate cycles.
Every once in awhile some thing comes along that really “clicks” with a  lot of people in the science community.
A new paper from New Zealand titled: Linkages between solar activity, climate predictability and water resource development is one of those that has “clicked” with a lot of people recently. It is the first scientific paper I’ve ever seen that pulls all the interdisciplinary fields of solar physics, astronomy, meteorology, hydrology, and climatology together to prove that in fact the sun is the major driver, even with its “small” fluctuations often ignored by climate scientists as being too small to matter.
It does matter, I’ve written about it many times, and this paper really has strong evidence supporting it. This is not just another paper talking about sunspots and the maunder minimum, no this one has some strong empirical evidence that directly links climate changes on earth to a myriad of changes in the sun-earth relationship.
What’s even better, this paper is readable. It’s not written in techno-speak with accents on using words 99% of the general population doesn’t use. It’s refreshing. Read it here (Adobe PDF)
The abstract reads: “This study is based on the numerical analysis of the properties of routinely observed
hydrometeorological data which in South Africa alone is collected at a rate of more than
half a million station days per year, with some records approaching 100 continuous years
in length. The analysis of this data demonstrates an unequivocal synchronous linkage
between these processes in South Africa and elsewhere, and solar activity. This confirms
observations and reports by others in many countries during the past 150 years.
It is also shown with a high degree of assurance that there is a synchronous linkage
between the statistically significant, 21-year periodicity in these processes and the
acceleration and deceleration of the sun as it moves through galactic space. Despite a
diligent search, no evidence could be found of trends in the data that could be attributed
to human activities.”
My hat’s off to these scientists: W J R Alexander, F Bailey, D B Bredenkamp, A van der Merwe and N Willemse


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5c2b952',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Lon Glazner, a fellow blogger and local electronics engineer made some comments about my post on the NASA/CSU study on California temperatures. Well that got me started…so below are Lon’s comments and my reply along with a fun technical challenge. For those of you that read this blog, but disagree with my views, I invite you to read this carefully.
Anthony,
You make a number of good points.  Particularly in the fact that the writers may have applied changes in urban temperature measurements over large regions for graphical impact.
As someone who has designed and built electronic temperature sensors I have certain concerns about the data itself.
Unless temperature sensors are regularly calibrated I think it is unreasonable to expect accuracy of greater than a couple of degrees.
Even some that are calibrated may not have good accuracy.  The LM34 which is a commonly used semiconductor for measuring temperature is +/-2 degrees F.  This is pretty typical of analog or digital semconductor sensors.  The temperature error for this part is also non-linear, and so it’s not a simple offset that you have to account for during data collection.  Furthermore, there are lots of additional errors that can creep into a temperature measuring device beyond the sensor itself.
http://www.national.com/pf/LM/LM34.html
One could argue that numerical analysis done on data points would tease out errors.  But if a scientist doesn’t know the exact accuracy of a temperature sensor then they couldn’t account for errors in their system.
Some of the temperature sensing stations may be  very accurate and regularly calibrated.  But maybe they’re not?
I have a hard time trusting that the data is accurate to the level of identifying 1 or 2 degree changes over decades.  This is especially true since the techniques of making these measurements have changes over that time frame.
Lon

Lon, thank you for the comments. FINALLY somebody who understands the kind of biases that creep into temperature measurements!
I’m innately familiar with National Semi’s LM34 and it’s accuracy problems. One of my early jobs at my university as a research assistant was to create remote electronic weather stations. I soon learned how inaccurate many electronic devices can be in temperature measurement.
The problem with the National Weather Service temperature data sets (and world data sets too) is that they are full of biases and errors that I’m not sure have been accurately accounted for. People such as Jim Price, from CSUC who is on the IPCC say they have been, yet nobody has shown me any hard evidence of such. I’d be a lot less skeptical if I could see how the IPCC accounted for temperature measurement biases. But they won’t share.

Some people that I try to explain this to accuse me of splitting hairs. But these bias problems in temperature measurement are quite real.
What works against my arguments about the difficulty in getting accurate temperature records is the everyday simplicity of temperature and its common measurement. We live by temperature, we have it reported constantly, we all have thermometers at home, we measure our childrens fevers with thermometers, we barbeque with thermometers.
Measuring temperature is easy right? You just stick the thermometer in whatever gas, liquid, or solid you want to measure the temperature of and voila’  there it is. People tend to think of thermometers as perfect devices. Some very expensive calibrated thermometers, are close to perfect, especially when taking measurements in a closed system, like a fermenatation vat at Sierra Nevada.
But in an open system in our atmosphere, there are many many more biases that can affect the measurement within a few inches or feet of the thermometer. Here’s just a few:
– Reflected sunlight from nearby building or objects
– Re-radiated infrared from nearby cement or asphalt surfaces or the ground itself (which is why airports make terrible places for temperature measurement)
– The structure that the thermometer is mounted to, can conduct heat to the thermometer
Now add to that:
– Accuracy of the thermometer itself
– Linearity of the thermometer over its measurement range
– Long term repeatability of the thermometer’s accuracy
– Long term repeatability of the thermometer’s linearity
And then we have urban effects such as:
– Localized vegetatation removal or addition over time
– Localized building changes over time
– Localized asphalt or concrete surfaces addition or removal
And finally within the global temperature records data set we find instances of:
– Changing the location of the weather station and/or its thermometer
– Changing the thermometer itself at some point – i.e. repair/replace
– Changing the thermometer type, from mercury, to electronic (thats been done at thousands of weather stations worldwide)
– Variations in temperature measurement devices from country to country, even though the World Meteorological Organization has specifications, they are not always followed.
– Changes in thermometer shelter, different types of paint over time, all which have different absorptive and reflective properties.
– Changes in the observer recording the temperature, some may round up, others round down numbers. BTW for about 75 years, all temperature records were manually recorded.
Ok with all these biases and possible errors that you have to account for to make long term temperature measurement reflect the true temperature of the location, can you be absolutely sure of the data integrity? Especially when you are looking for trends that may be 1 degree or less over 50-100 years? I can tell you that I’ve looked at these climatological data sets, and NONE of them come with a calibration record for the thermometer, or even a description of the make/model used at that location. There are notations in the records that say things like “station relocated to accomodate construction” or “thermometer replaced” which can give clues to the data integrity possibly changing but the climate researcher is left to make a judgement call on the viability of the data without anything to gauge the sensor or its local environment.
Or lets try a thought experiment Lon, you’ve been commissioned by the IPCC to make a new thermometer for use around the world at climate measurement stations. As an electrical engineer, could you design an air temperature thermometer that is:
– Linear to within 0.1% over a temperature range of -20F to 120F
– Accurate to within 0.1 degree F over that same range
– Repeatable in linearity and accuracy defined above for a period of 20 years. Or even 10 years.
– Identical withing the specs above, so that if one fails, it can be immediately swapped with another one from parts stock with no worry about introducing bias
Ok there’s your challenge. Could you do it?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7248471',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Intense floods and storms around the world could double in frequency within 13 years, as climate breakdown and socioeconomic factors combine, according to a new study. The authors of the analysis say it’s the first to incorporate historical local and global climate data and information about population density, income and poverty to estimate how many hard-hitting disasters to expect. They counted floods and storms that would affect 1,000 people or kill 100 people.  Broadly, the researchers also see governments around the world as critically unprepared. The authors found very high risks for countries such as Australia, Bangladesh and China. Risks are highest for countries that are already seeing far more extreme events than the global average. The study is published in the peer-reviewed Climate, Disaster and Development Journal. Co-author Vinod Thomas, a visiting professor at the Asian Institute of Management in Manila who has held senior posts at the World Bank, said policymakers for the most part do not yet incorporate climate effects into their preparedness efforts. “On the one side, there needs to be climate adaptation efforts, such as relocating people from highly exposed coastal regions or building better disaster preparedness that would withstand extreme hurricanes,” Thomas said. “Equally, there is a strong case for stepping up climate change mitigation in decarbonizing their economies.” Thomas said the findings of an “unmistakable causal link between carbon emissions and more intense floods and storms come at a crucial time,” as forest fires burn through Australia and floods and powerful storms hit the US and Europe. He said countries like Thailand, which saw massive flooding events that killed hundreds and affected millions in 2011, could not handle twice as many catastrophes each year. He said they need to conduct stress tests to understand what they can withstand. The study examined how disasters have increased as greenhouse gases have accumulated in the atmosphere over 60 years. It then projected that same trend into the future and considered how much a continued increase in floods and storms would affect regions based on how populous they are and whether residents are financially secure and prepared for disasters. The research used an economic approach, rather than relying on climate modeling – which uses computers to calculate likely outcomes based on a range of inputs. The journal publishing the study is based in Manila and not widely known. The authors said they first attempted to publish their work in the well-read journal Science. Don Wuebbles, a professor of atmosphere sciences at the University of Illinois who worked on the 2018 US National Climate Assessment, said the study might be underestimating future disasters by assuming disasters will continue to increase at the current rate. He said he appreciated that the researchers considered population growth and density but that he was “not sure they adequately considered the changes in climate for the future”. Ramón López, the lead author of the study who is a professor at the University of Chile, acknowledged the methods used might not account for the likelihood that severe events will increase at a faster rate than we have seen in the past."
"

It was with sadness and surprise that I learned today that Bombers Baja Grill off East Avenue has closed their doors forever. They served their last Mexican food ordnance today, 12/15/06.
Bombers was known for the biggest burrito ever made (at least in Chico). There was the “missile”, the “bomber” and the “atomic bomb” which between the habeneros, beans, and the calories, was a nearly 2 pound explosive combination.
I’m not sure where they got the idea to mix bombs and burritos, but here is an early picture from the experimental days of the restaurant where they were loading planes at the Chico Army Air Base (now the Chico Airport).

Bombers was an original, so original that those corporate franchisers at Chipotle knocked off the idea I think. But unlike Chipotle, which has food served in prison cafeteria style with steel and glass ambience, Bombers food was great, and the ambience had history you could feel.
I suppose it was only a matter of time though, they had a terrible location, and competition was springing up around them. They were also nearly invisible, which goes to show it pays to advertise.
I’ll miss Bombers.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea980e1fb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Our comment primarily concerns the Department of Energy’s (DOE) use of the social cost of carbon (SCC) in the cost/​benefit analysis of the Energy Conservation Program: Energy Conservation Standards for Walk‐​In Cooler and Freezer Refrigeration Systems proposed rulemaking. The DOE’s determination of the SCC is discordant with the best scientific literature on the equilibrium climate sensitivity and the fertilization effect of carbon dioxide—two critically important parameters for establishing the net externality of carbon dioxide emissions. It is also at odds with existing Office of Management and Budget (OMB) guidelines for preparing regulatory analyses. It is based upon the output of Integrated Assessment Models (IAMs) which have little utility because of their great uncertainties, including uncertainties within the critical physical parameters upon which their simplified climate model are built. They provide no reliable guidance as to the sign, much less the magnitude of the social cost of carbon. Additionally, as run by the Interagency Working Group (IWG) (whose results were incorporated by the DOE in this action), the IAMs produce illogical results that indicate a misleading disconnection between climate changes and the SCC value. Further, we show that the sea level rise projections (and thus SCC) of at least one of the IAMs (DICE 2010) is not supported by the mainstream climate science.



Until this entire situation can be properly rectified, the SCC should be barred from use in this and all other federal rulemaking. It is better not to include any value for the SCC in cost/​benefit analyses such as these, than to include a value which is knowingly improper, inaccurate and misleading.
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
Northern Europe and the Arctic show signs of winter cooling over the past decades. Could the global warming theory be in for an upset?
Looking at January data over the northern Europe, we see no real warming trend for the month, according to data from the Japan Meteorological Agency (JMA).
When plotting mean January temperatures for Norway for stations where data are available since 1988, we see that 7 of 11 stations show a cooling trend since 1988, despite rising CO2:

Data: JMA. 
January cooling in Finland
The story is the same in Finland, a country that stretches into the Arctic:

All stations with data going back to 1988 show a cooling trend in Finland for the month of January. Data: JMA
Ireland cooling more than warming


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Ireland, situated in the North Atlantic, we also see signs of cooling mid winters. warming has been AWOL for some time now:

 
In Ireland 4 of 6 stations with data going back to 1988 show a cooling trend for the month of January. Data: JMA
Arctic sea ice rebound
Next we move to the Arctic. This year’s winter is seeing an impressive rebound in sea ice, tweets meteorologist Chris Martz, reaching the 3rd highest level in 15 years:

Arctic sea ice extent is currently 3rd highest in the last 15 years, the only years higher are 2008 and 2009. It just surpassed 2013 and is about to pass 2004 within the next day or two. It isn't too far behind 2009 either. This isn't good news for the narrative, I will say that. pic.twitter.com/RyCRMnI8tK
— Chris Martz Weather (@ChrisMartzWX) February 11, 2020

“Dramatic recovery” for Arctic sea ice
Obviously the situation in the Arctic is nowhere as dire as alarmists like to deceive others into thinking. According to meteorologist Justin Berk here, “Arctic Sea Ice has made a dramatic recovery and expansion this winter.”

Image: National Snow and Ice Data Center
Share this...FacebookTwitter "
"
Share this...FacebookTwitterClimate alarmist scientists refuted
Distinguished climate expert Roger Pielke Jr. tweeted on recent findings contradicting alarmist claims that tropical storms have slowed down (thus stick around longer and wreak more devastation) or are more frequent and intense.
First, lets look at frequency and intensity.
No detected upward intensity/frequency trend at all
In an article appearing at Forbes, Pielke writes together with atmospheric scientist Dr. Ryan Maue how they and University of North Carolina-Wilmington professor Jessica Weinkle used datasets available around the world on tropical cyclones to create a historical record of storms of at least hurricane strength that made landfall.
Fifty years of global landfalls of tropical cyclones of hurricane strength, based on the Saffir-Simpson hurricane scale, were analyzed.
According to the findings published earlier here:
The analysis does not indicate significant long-period global or individual basin trends in the frequency or intensity of landfalling TCs of minor or major hurricane strength. The evidence in this study provides strong support for the conclusion that increasing damage around the world during the past several decades can be explained entirely by increasing wealth in locations prone to TC landfalls, which adds confidence to the fidelity of economic normalization analyses.”
Shown below is an updated chart from the Pielke et al 2012 paper, which was extended to 2019. It shows global tropical cyclone landfalls at hurricane strength from 1970 to 2019:





<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Pielke and Maue at FORBES: “There are a lot of ups and downs in the data, but no obvious trends.”
Tropical storm translation speeds have not slowed down
Pielke also tweeted about a new study appearing in Nature here. The University of Colorado scientist commented:
He added that the claim that tropical cyclones have now slowed down are “supported by both observations and modeling” and that “there is no reason to expect a slowdown” in the future:

And, from this study under RCP8.5 (!!) there would be no reason to expect a slowdown in hurricane translation speeds in the future pic.twitter.com/RE1aG9OCBC
— Roger Pielke Jr. (@RogerPielkeJr) January 9, 2020

The distinguished professor also says it’s: “Time to retire the notion that hurricanes are slowing down (much less the attribution claims).”
Share this...FacebookTwitter "
nan
"Seven new species of miniaturised frogs have been found in the Brazilian Atlantic rainforest. Among the smallest vertebrates on the planet, these colourful creatures can fit comfortably on a human thumbnail.  To understand just how tiny these frogs are, consider this: the largest living vertebrate is the blue whale, measuring around 26m, while the smallest was believed to be a fish (Paedocypris progenetica) with an adult size of 7.9-10.3mm.  Then two years ago a new species of miniature frog was described, and it was a record breaker. Paedophryne amauensis,  which lives on the island of New Guinea, only grows to an average size of just 7.7mm and is now considered the world’s smallest vertebrate.  The latest discoveries in Brazil, announced in the journal PeerJ, are slightly larger, at between 9 and 13mm. But they’re still tiny compared to pretty much anything else with a backbone.  The frogs belong to the genus Brachycephalus, sometimes labelled “pumpkin toads” due to the often bright orange skin of some members. They live only in permanently-foggy patches of mountainside in Brazil’s Atlantic rainforest known as “cloud forest”.  The first Bracycephalus species was described as early as 1824 but most of the currently recognised 21 species have only been discovered in the last 15 years. The recent finding of seven new species and the difficulty of exploring the inaccessible habitat in which these animals live, suggests the actual diversity in the genus is considerably higher. Most miniaturised frog species have simplified things as a consequence of their reduced size. They have fewer vertebrae than their larger relatives, and fewer skull elements. They also often have reduced numbers of digits; regular frogs generally have four fingers and five toes, whereas miniature frogs have just three and two respectively.   Miniaturised frogs share a number of ecological traits – they are found in wet tropical regions, primarily in forests, living near the ground in the moist leaf litter. This makes sense for such tiny amphibians. After all, their high surface-to-volume ratio makes them vulnerable to desiccation (drying out) and thus they are very sensitive to water loss.  Many, however, are not dependent on water for reproduction. In fact, some species entirely lack the larval tadpole stage typical of most frogs.  Females in these cases instead produce a small number of large eggs that develop directly into small independent froglets. Although this elimination of a tadpole stage may have paved the way for the exploitation of new niches and miniaturisation, the relationship between miniaturisation and terrestrial breeding is not well understood.  It is believed that miniaturisation has evolved independently at least 11 times in terrestrial frogs and species measuring less that 13mm include representatives from 5 families and 9 genera. The Atlantic cloud forests of Brazil have the sort of diverse and humid microclimates in which frogs thrive. The forests are home to more than 400 different species, around 8% of the world’s frog and toad species. The region’s frogs are noted for the extraordinary diversity in  reproductive modes,  In fact, they exhibit  27 different reproductive modes in total including, of course, the typical reproductive cycle characterised by aquatic eggs (or “frog spawn”) that develop into tadpoles that in turn metamorphose into four-legged frogs.  But in many other species, including several found in the cloud forests, eggs and/or tadpoles are partially or completely removed from water. For example, some frogs lay their eggs in foam nests that float on the surface of ponds or on water accumulated on plants. The eggs hatch into tadpoles that complete development in water.  The recently found species, and most of their closest relatives, have gone a step further and completely removed their eggs from water and in the process eliminated the tadpole stage completely.  Decimation of the Brazilian Atlantic forest is one of the most alarming and desperate conservation problems in the world. In the year 1500 at the beginning of European colonisation, the area covered by the forest was approximately 1,300,000 km2.  Today the forest has been reduced to 7.6% of its original extent and the remaining forest is still under severe anthropogenic pressure.  Deforestation causes areas to dry out and eliminates those species, like miniature frogs and toads that depend on humid forests in order to breed successfully. Of the frog and toad species occurring in cloud forest, 81% occur nowhere else on earth.  No doubt there are many other frog species in Brazil that have not yet been discovered. Sadly, given the current rate of destruction and species extinction, it is possible they never will be."
"Hopes of using Davos to forge a new international consensus to tackle poverty and the climate crisis have been thwarted by the decision of the World Bank president, David Malpass, to boycott the event. To the surprise of the other multilateral institutions, Malpass turned down his invitation to attend despite being in Europe this week for the UK government’s Africa investment summit in London. In the past, World Bank presidents have played a prominent role at the annual meeting of the World Economic Forum, taking the opportunity to make the case for concerted action to tackle global poverty. The heads of other major international organisations, including the International Monetary Fund (IMF), the UN, the Organisation for Economic Co-operation and Development, the World Trade Organization and the International Labour Organisation, attended the Davos event. One source said Malpass’s decision not to attend the World Economic Forum reflected the Bank’s go-it-alone approach under his presidency. “He has effectively declared UDI [unilateral declaration of independence],” the source said. “We saw it at last year’s G7 summit in France. President [Emmanuel] Macron wanted a collective statement from the international organisations but Malpass vetoed it. He wouldn’t have the word multilateralism in the statement.” Davos is a Swiss ski resort now more famous for hosting the annual four-day conference for the World Economic Forum. For participants it is a festival of networking. Getting an invitation is a sign you have made it – and the elaborate system of badges reveals your place in the Davos hierarchy. The meeting is sponsored by a huge number of international banks and corporations. For critics, “Davos man” is shorthand for the globe-trotting elite, disconnected from their home countries after spending too much time in the club-class lounge. Others just wonder if it is all a big waste of time.  The 2020 meeting is being advertised as focusing on seven themes: Fairer economies, better business, healthy futures, future of work, tech for good, beyond geopolitics and how to save the planet. Young climate activists and school strikers from around the world will be present at the event to put pressure on world leaders over that last theme.  A spokesman for the World Economic Forum said Malpass, who was Donald Trump’s nominee to head the World Bank, had been invited. The next two most senior officials at the Washington-based institution – Axel van Trotsenburg, the bank’s managing director, and Philippe Le Houérou, the head of the bank’s private sector arm, the International Finance Corporation – are also notable absentees from Davos. By a tradition that stretches back to the 1940s, the US chooses the head of the bank while the European nations pick the managing director of the IMF. Since taking over less than a year ago, Malpass’s prioritisation of programmes for individual countries has led to suspicions in other international bodies that the bank is in effect being run from the White House. “There is no attempt to reach out to other institutions to look for system-level policy coherence,” according to one official in Davos. Those more sympathetic to Malpass said he was a shy man who did not know how to do smalltalk and hated events such as Davos. A World Bank spokesman said: “The World Bank Group is focused squarely on efforts to alleviate poverty and boost living standards. We sent senior technical experts on climate and water to the World Economic Forum to participate in discussions on how to improve development outcomes in these priority areas. “World Bank Group president Malpass has just returned from the UK-Africa Investment Summit 2020, where he met with over 10 leaders of several African countries to discuss challenges they face. He also engaged with private-sector clients and partners on local currency financing, capital markets, green financing and country-specific reforms that could be made to unlock private investment in developing countries.”"
nan
"
This afternoon there will be several presentations that embrace the measurement systems used for the near surface temperature and precipitation records.
Of great interest to me is a presentation outlining the new US CRN (Climate Reference Network) by Bruce Baker of NCDC. Another is by Glenn Conner, former Kentucky State Climatologist whose talk will be about the role of station histories in identifying biases in climate records.
My presentation follows those two – it should be a lively afternoon.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea450bc7f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A senior adviser to the federal government on threatened species has backed calls for the creation of a national scientific monitoring system after the bushfire crisis to help fix Australia’s “very uneven” record in protecting endangered wildlife. Helene Marsh, chair of the national threatened species scientific committee and an emeritus professor of environmental science at James Cook University, said the scale of the ecological tragedy had made Australians more aware of the risks facing the country’s unique animals and plants and provided an opportunity to improve conservation.  With fires still burning, scientists warn it is too early to have a clear picture of the devastation, but preliminary government data suggests more than 100 threatened animal and plant species have lost at least half their habitat and more than 300 have lost more than 10%. The impact on most species not currently listed as threatened is yet to be assessed. Birdlife Australia estimates nearly 80 birds have lost at least a third of the area in which they live, and that the superb lyrebird may have plunged from being a common to a threatened species. Marsh said the threatened species committee planned to review the decision-making process for officially listing species as vulnerable or worse within the constraints of existing national environment laws. She said the protection offered to species after they were listed should also be reconsidered as the existing model of recovery planning had not worked. Fewer than 40% of nationally threatened species have recovery plans in place. She said she was encouraged by the level of goodwill between federal and state governments, scientists and conservationists following the fires, including the response by Sussan Ley, the federal environment minister, who has set up and met with an expert scientific panel to advise on what needed to be done. The federal government has allocated $50m in wildlife recovery funding, with a promise of more to come, and officials from across fire affected areas are due to meet on Tuesday to continue work on a national response. Marsh said the creation of a national scientific monitoring facility, proposed by fire scientists David Bowman and Ross Bradstock to fill critical gaps in bushfire knowledge, made some sense. She said it could include wildlife surveys. “I’ve been quite concerned about the way we monitor our biodiversity in Australia, it’s a huge job, and I think if we are going to understand the impact of fires that is a very interesting idea that needs consideration,” she said. Given not everything could be monitored, she said there would need to be clear priorities that would likely be defined by the 12m hectare bushfires. But she said it was important to consider “biodiversity arks”, key areas that had avoided the fires, as well as burned country. Marsh said it should utilise technology, including remote sensing and drones, as well as on-ground field work by scientists, Indigenous rangers and possibly community groups and citizen scientists. “I do believe that the importance of monitoring the impact of the fires will be a catalyst for doing this work,” she said. “It is very important that it be well designed and scientifically robust.” While she backed the development of a national system, Marsh said monitoring the impact of the fires could not wait for a new system and needed to begin as soon as it was safe. An analysis by environment groups found the Coalition had cut funding for environmental programs, monitoring and staff by about 40% since being elected in 2013. Marsh said changes would be needed to the national environment laws, the Environment Protection and Biodiversity and Protection Act, which is being reviewed by businessman Graeme Samuel. Asked how successful the laws had been, she said the government’s state of the environment report, which among other things found climate change was altering the structure and function of Australia’s natural ecosystems and affecting heritage, economic activity and human wellbeing, “spoke for itself”. Marsh said the review of the laws should consider whether it was appropriate to adopt more advanced criteria used by the International Union for the Conservation of Nature to assess whether an ecological community was threatened.  Government ministers including Ley last year stressed the review would focus on cutting “green tape”. Scientists have expressed hope it may now address the impact of the fires and what was needed to avoiding an extinction crisis that scientists said was worsening before the devastating fire season. Scientists last year said three native Australian species had become extinct in the past decade and another 17 could follow in the next 20 years. More than 1,800 Australian plants and animals are listed as threatened with extinction. Scientists warned in a letter to the government before the fires that this was likely to be an underestimate."
"
Share this...FacebookTwitterUnusual cold and snow are expected to sweep across North America and Europe over the coming days, thus threatening crops. 
Not that mid May is approaching, global warming alarmists tell us we should already be expecting heat waves. But right now the opposite is in the forecast: snow and extreme cold! Who would have thought?
Snow for Boston and new York?
Meteorologist Dr. Ryan Maue tweeted that both New York City and Boston might see snow on Saturday as a “rare & powerful May ‘bomb cyclone’ Nor’easter” is projected to develop off the east coast.
Over a foot of snow might fall in Maine, Maue asserts:

While snow won't stick on the ground for long, if at all, parts of Maine might see more significant accumulations over a foot. ❄️
Flurries for NYC and Boston perhaps on Saturday as rare & powerful May ""bomb cyclone"" Nor'easter develops off New England coast. pic.twitter.com/aOHT4APBfp
— Ryan Maue (@RyanMaue) May 7, 2020



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Maue earlier had tweeted that the snow and cold could act as a “triple whammy” because “hard freezes limit agriculture, forces people to remain indoors w/central heating & provides outdoor environment favorable for coronavirus.”
Snow to blanket parts of northern/central Germany 
Not only the Northeast has to worry about winter striking so late in the season, but also an intense cold front will be sweeping across a vast swath of northern Europe, reports German weather site daswetter.com here.
One can also call it an unusual cold snap for almost mid-May. The air in the north will warm up to only 7 to 12 degrees. […] With the polar air, the temperatures will plunge, and so will the snowfall line. When the cold air reaches the south in the night from Monday, it will snow slowly until the middle of the day. Around 400 to 600 m, up to 10 cm of fresh snow is possible in the middle of May. Even up to 300 m wet snow can fall.”
“Five to 6 nights of frost warnings” in UK
At Twitter David Birch tweeted a GIF animation showing the projected movement of the cold front, writing that Britain might see 5 or 6 nights of frost warnings next week:

Likely the UK could see 5/6 consecutive frost warnings throughout next week. pic.twitter.com/paLeD4vvOK
— DavidIBirch 🇬🇧 (@dbirch214) May 7, 2020



		jQuery(document).ready(function(){
			jQuery('#dd_91b09e226f3683c039c0d47c6040e31f').on('change', function() {
			  jQuery('#amount_91b09e226f3683c039c0d47c6040e31f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Big mining firms in the Democratic Republic of Congo are worried. For the past decade they’ve made good money from the country’s huge reserves of cobalt, diamonds, gold and copper, and now the government wants to grab more of the action: a document leaked to Bloomberg reveals plans to raise royalties and profit taxes, and increase the state’s share in any new ventures. This is so-called “resource nationalism” in action, and the DRC is far from alone in seeking greater economic control of its natural resources. The state is back, the theory goes, and it’s taking on the multinational. From Scotland to Namibia, Zambia to Ecuador, resource rich nations throughout the world are rhetorically reclaiming gas, oil and minerals as their own. The trend is widely reported as the enemy of trade, investment and energy security alike. In the UK, for example, the Telegraph called it a “spectre” and government economists have labelled it as both a “threat” and “anti-competitive”. On the other side of the coin, governments argue they are simply ensuring foreign businesses don’t unfairly benefit from resource extraction. Take Zambia, for instance. The landlocked African nation is a major copper exporter yet most of the population still lives below the poverty line. After the government looked to crack down on tax avoidance by multinational mining firms, one senior politician defended the move: “The situation is win on one side – only the shareholders are winning; the people of Zambia are still in abject poverty”.  The question of whether resource nationalism really is something to be feared is therefore a whole lot more complicated than it would first seem, for the three following reasons. Governments, most prominently those of Sub-Saharan countries like Ghana, Sierra Leone, Guinea or Tanzania, have argued for huge tax hikes on mining, oil and gas contracts in the name of the “national interest”. However, move beyond the rhetorical strength of such statements and resource nationalism is less the enemy of big business than a cover for a business-as-usual bias towards the interests of neo-liberal, foreign investment.  In Tanzania for example, recent discoveries off the coast of East Africa have led to predictions that the region will become one of the world’s biggest exporters of natural gas. As a result, “nationalist” laws are currently being drafted which begin: “Natural resources found in Tanzania belong to the [Tanzanian] people”.  At the same time, however, a recently signed memorandum of understanding between the UK and Tanzania promises, according to former foreign secretary William Hague, to “offer significant opportunities for British businesses in the energy sector”. Indeed, BG Group, as well as Norway’s Statoil and other big players have already been granted licences. The state is striking back in rhetoric only; it is business that still holds the real power. Like a game of Risk, our idea of national control tends to be fixated on owning resources found within neatly defined borders. In today’s world however, this doesn’t make sense.  Better technology, modelling and visualisation techniques means extraction frontiers are constantly being moved further afield and deeper underground. Mines such as one in Mponeng, South Africa, can reach nearly 4km deep and have more than 230 miles of tunnels, all to mine a 30 inch wide seam of ore. This should complicate our understanding of the idea of resource nationalism. How, for example, do we make sense of competing, contemporary claims to the deep sea off Namibia or Papua New Guinea? Similarly, questions over resources and sovereignty might even make us ask who owns the moon? Finally, geopolitical debates over extraction rights in the Arctic provide further worrying evidence of the ways in which national and private interests are always in competition. In all cases, the physical and metaphorical boundaries of the nation state have to be questioned as law tries to keep pace with technological advancement. Whatever the context, resource nationalism makes its claims by promising a country’s citizens “fair” and equally-distributed access to its resources. However this fails to account for politics. Mining and oil contracts are often negotiated in secret. Protests against these deals can be suppressed through state sanctioned force, and “national” policies often marginalise groups based on account of gender, race or sexuality. It is precisely this sort of identity politics which sparked violence over sovereignty in Mtwara, Tanzania, where the region’s population claims that they are marginalised from a policy that favours the urban elite hundreds of miles away.  The “national interest” never means the same thing to everyone within a nation: different people place different values on nature and its resources. Brazil’s recent draft bill aiming to “nationalise” the Amazon is a good example – made at a governmental level, it doesn’t necessarily consider the views of indigenous communities. And the idea of the “national interest” can’t adequately describe this complexity. From “African” oil to “Scottish” gas, those that fear “resource nationalism” would do well to remember this and not overly simplify the debate."
"

Another parking lot being measured for climate change: Newport, TN
My surfacestations.org project has reached an important milestone.
With the submission of #222, Lexington, VA, submitted by John Goetz, we are now below the 1000 mark (out of 1221) stations left to survey. It was a 3 -way race to #222 between power surveyors John Goetz, Kristen Byrnes, and Don Kostuch.
Thanks to ALL of the wonderful volunteers for helping to reach this important benchmark! We currently stand at 231 surveyed stations and 990 left to go.
I still need help in the midwest and the south, particularly Kansas, Nebraska, Montana, the Dakotas, Oklahoma, Mississippi, and Alabama. If you live in the areas want to make a lasting contribution to science, please visit www.surfacestations.org and sign up as volunteer. Its easy to do, and it makes for a fun science learning experience.
To see more weather stations like this one, see my “How not to Measure Temperature” series on this blog. This is only a small sample of the 231 surveyed to date, but it will give you an idea of the problems that have been seen so far.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4d334ac',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterCorona shows the stark attitude differences between the sciences of climate and virology. While one arrogantly claims to monopolize the truth, the other acknowledges the great uncertainties.

Image: CDC
By Dr. Sebastian Lüning, Die kalte Sonne
(Text translated, edited by P Gosselin)
The corona virus with all the effects is currently pushing all other issues completely to the sidelines. This also includes the climate topic.
Climate activists like Professor Volker Quaschning or Professor Stefan Rahmstorf know it. Currently they fear for their livelihoods as they try to resist this with all their might, sometimes with absurd tweets or, as in the case of Professor Stefan Rahmstorf, with an article in Spektrum der Wissenschaft: Denial of science in times of corona“.
In his article, Professor Rahmstorf (Oceanography and Paleoclimatology!) also fancies himself a corona expert, and he puts those who do not agree with him in climate research in the same bag with the scientists who are critical of the corona crisis and approach. This is done completely without any basis because these are two completely different issues.
Climate activists seeking attention
It is a contemptible attempt to desperately link corona and climate, no matter what. This is being done simply because corona is the top issue right now. And before his own issue gets completely washed away, Rahmstorf is stooping to such means to try to get some attention. This blog here admits that it lacks the expertise to properly judge the corona debate.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Beholders of the truth?
But something completely different is crucial here. Anyone who looks at or listens to the regular statements or podcasts of experts such as Berlin virologist Professor Christian Drosten will see the pleasant difference to climate alarmists like Professors Quaschning or Rahmstorf. Unlike them, Professor Drosten does not consider himself to be the sole beholder of the truth. On March 20, 2020, he explained in his podcast:
“There is no research data on long-range curfews. Caution is also called for when dealing with numbers.” …”Summer can have at least a small effect on the virus.”
Uncertainty is acknowledged
This has been the case since the beginning of his podcast series. Professor Drosten has stated more than once that science does not yet know certain things or that certain findings have since become obsolete. Imagine if he applied the popular killer argument “the science is settled” to corona in the same way as Professors Rahmstorf or Quaschning do with climate.
Models have failed
Yet, climate models that fail to reflect reality still remain the basis for future scenarios. They are taken at face value, even though their calibration fail when past data are applied. And even worse, people are being told that there is a control over the climate that is now being lost.
Is this a lack of knowledge, a lack of respect for nature, or just their own blunt agenda?
Alarmist scientists don’t contribute to healthy science
The climate debate will also fail because of the fact that the aforementioned people are sitting in ideological trenches and hurling grenades in the direction of the other side. This is something you can do in a war, but unfortunately it does not lend to a social discourse.


		jQuery(document).ready(function(){
			jQuery('#dd_99d26721b518a0ab9826796b8202153e').on('change', function() {
			  jQuery('#amount_99d26721b518a0ab9826796b8202153e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSo far no signs of another super hot-dry summer for Europe, which media have been alarming about. 
Veteran Swiss meteorologist Jörg Kachelmann tweeted the 45-day projections for Europe.
In terms of precipitation, Europe saw drought conditions over the past two summers (2018 and 2019) and climate alarmists claimed this would be the new normal. And recently the European public got bombarded by media reports stemming from the World Meteorological Organization (WMO) of a blistering super hot and dry summer this year.
But look what the ECMWF now projects for the next 45 days (upper chart). Kachelmann comments: “Even according to the latest 46-day trend of the ECMWF, which runs to mid-July, the drought summer seems to be completely called off for the time being.”

Ein ""Hitzesommer"" war noch nie in den Vorhersagen drin und das scheint zumindest bis Mitte Juli auch so zu bleiben, auch wenn es später im Juni wieder sommerlicher wird. pic.twitter.com/6G97p1NJmY
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) June 5, 2020

And in terms of temperature, see the lower chart, nothing unusual is projected to happen over the next 45 days. The Swiss meteorologist notes: “A ‘hot summer’ was never in the forecasts, and it seems it’ll stay that way at least until mid-July, even if it gets more summery again later in June.”
Keep in mind these long range forecasts come with much uncertainty, and change with every run. But right now it looks like all the recent doomsday projections of a scorched euro-summer were overblown.


		jQuery(document).ready(function(){
			jQuery('#dd_ffb24e69e1916c9a909c9e37aa3ef08a').on('change', function() {
			  jQuery('#amount_ffb24e69e1916c9a909c9e37aa3ef08a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGermany’s onslaught on its famed automotive and production industries appears to be taking an economic toll as the country pushes ahead to go green by phasing out internal combustion engines and coal power plants.
Recently we reported how electricity prices are again slated to increase this year, and thus will continue to make German power among the most expensive worldwide.
A wave of green activism has led to tighter regulations against the internal combustion engines and to a planned phase-out of coal-fired power plants.
Teetering on recession
Just recently German online business daily Handelsblatt reported here that there are “new concerns about an economic slump in Germany” as “surprisingly weak figures are fueling new worries about a downturn”.
“Horrible numbers”…a “disaster”
“Experts spoke of ‘horrible numbers’, a ‘disaster’. Industry, construction, and energy providers produced a full 3.5 percent less in December than in the previous month,” the Handelsblatt reports.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




December production plummets 6.8%
The economic bloodbath was even worse in the production sector which “fell even more sharply, with output falling by 6.8 percent – the sharpest drop since the end of 2009,” writes the Handelsblatt. “Concerns are growing again that the German economy may be in more difficult waters than expected.”
For Germany, “2019 was not only the worst year for industrial orders since 2008, it was also the first time since 2002 that German order books shrank for two years in a row,” reports Yahoo here.
Massive automotive layoffs
The German auto sector has been hard hit. For example, car maker Opel recently announced 2,100 job cuts in Germany. Late last year Daimler, owner of Mercedes Benz, announced plans “to ax at least 10,000 jobs,” Volkswagen’s Audi said “it would slash up to 9,500 jobs or one in ten staff by 2025 and car suppliers Continental and Osram announced staff and cost cuts.”
The Financial Times reported today that Daimler suffered its “worst results in decade” and that its earnings “plunged 60% in 2019 amid ‘Dieselgate’ woes.” Daimler also “refused to deny reports” that an additional 5,000 jobs could be cut.
The Financial Times adds: “Daimler is being forced to spend heavily on electric vehicles and plug-in hybrids in order to avoid fines from Brussels for breaching new emissions regulations.”
Other reasons cited for the poor German economic results are the ongoing global trade disputes. Figures are expected to come under even greater pressure due to the spreading corona virus in China.
Share this...FacebookTwitter "
"The US and Europe have clashed over the threat posed by global heating as Donald Trump’s finance minister downplayed the risks of a climate crisis during the final session of the World Economic Forum in Davos. Steve Mnuchin, the US Treasury secretary, said the debate should be about “environmental issues” rather than climate change, that the costs were being over-estimated and that climate was only one of several concerns that needed to be discussed.  He reacted strongly after the president of the European Central Bank, Christine Lagarde, said it was vital to include climate risks into economic forecasts and Germany’s finance minister, Olaf Scholz, said his country was stepping up its fight to reduce the use of carbon. “We don’t know how to price these risks and so we are over-estimating the costs”, Mnuchin said. “If you want to put taxes on people go ahead and put a carbon tax. That is a tax on hardworking people.” Mnuchin said technology would provide the solution to reducing carbon emissions and that the costs of action would be lower in 10 years time. “Environmental issues have an impact on the economy but it is one of many important issues.” Departure from the Paris climate agreement In June 2017 Trump announced his plan to pull the US out of the Paris climate agreement saying"" “I was elected to represent the citizens of Pittsburgh, not Paris”. He claimed the agreement promising to cut greenhouse gas emissions to keep global heating below 2C, unfairly disadvantaged the US and negatively impacted jobs and factories. Shrinking national monuments and animal protections In December 2017 Trump announced plans to slash the size of two national monuments in Utah. Bears Ears was cut from 1.5m acres to 228,784 acres and Grand Staircase-Escalante almost halved from approximately 2m acres to 1,006,341 acres – marking the biggest elimination of public lands protection in America’s history. In late 2018 the administration then announced plans to remove key provisions from the Endangered Species Act – prompting conservationists to warn it could put vulnerable plant and animal species in more danger. Rollback of the Clean Power Plan The Environmental Protection Agency is in the process of finalizing plans to dismantle the Clean Power Plan, an Obama-era rule intended to cut emissions from power plants and encourage them to move towards natural gas and renewable power. Cuts to clean water protections The Trump administration plans to remove protections from thousands of America’s streams and millions of acres of wetlands, which is feared will harm wildlife and enable pollution to enter drinking water. Currently, protected waterways provide drinking water to approximately 117 million people. More methane In September 2018, the Trump administration announced its plans to repeal rules that aim to restrict methane leaks on public and tribal lands. The Obama administration tried to cut leaks by forcing oil and gas companies to capture methane (a key gas involved in global heating), but Trump's Department of the Interior has branded the rule 'flawed' and 'unnecessarily burdensome on the private sector'. Miranda Bryant Mnuchin cited the Coronavirus health emergency, Iran’s nuclear ambitions and the possibility of oil prices hitting $120 (£92) a barrel, and the need to power development in poor countries as other problems that policy makers needed to be concerned about. “There are way too many people in the developing world who don’t have access to electricity. We need to create an environment where they have better lives,” he said. Mnuchin found himself in a minority of one on a panel that included the managing director of the International Monetary Fund, Kristalina Georgieva, and the Bank of Japan governor, Haruhiko Kuroda, as well as Lagarde and Scholz. Georgieva said the investments that would be needed to transform economies could be the “silver bullet” that would lift the world economy from four lows: low productivity, low growth, low inflation and low interest rates. “We need to think of policies that are good for the economy, good for the environment and good for people” she said. Green investment could provide a use for the world’s big pool of unused savings. “Not everything is doom and gloom.” Lagarde said that pricing the cost of making the transition to a low carbon economy coupled with pressure on companies to disclose their exposure to climate crisis risk would persuade companies to move faster. The ECB head said that, according to the marketing industry, there were three things that made people move: sex, fear and greed. “I don’t want to talk about the first but the fear factor is there. Look at the complete disasters that are going on around the world.” Turning to greed, Lagarde said the recent announcement from Larry Fink, the executive director of BlackRock, that the world’s biggest asset manager was moving to sustainable investment was an indication that companies were starting to worry about the impact of the climate emergency on their bottom line. Scholz said Germany’s decision to pay out more than €40bn to coalmining regions and power companies to phase out coal plants was justified by the scale of the threat. “There is climate change and it will hurt us. It will have negative effects on the economy. Britain’s departure from the EU next week had not entirely lifted the threat from Brexit, Lagarde said, noting that the negotiations this year over the new EU-UK trading arrangement had the potential to cause trouble. “Brexit is a little bit less uncertain, but we still have that possible cliff edge in December of 2020. We don’t’ know exactly what the trade relationship will be. And it’s a big partner for the euro area, so that’s certainly a question mark.” Scholz expressed relief that there had not been a “hard Brexit” and predicted that Germany would not take an economic hit from Britain’s departure. “It will be more difficult for the UK because it needs its business model to be reorganised” Scholz said. He added that the City of London would be affected by Brexit. “I think we will find solutions”, Scholz said, while warning the UK that there might not be a “special, competitive advantage of being outside” the EU. "
"

That’s the topic of my _Washington Examiner_ column this week. In it, I discuss last week’s budget battle and the failure of “policy riders” designed to rein in the Obama EPA’s attempts to regulate greenhouse gases without a congressional vote specifically authorizing it. The Obama team believes it has the authority to implement comprehensive climate change regulation, Congress be damned. Worse still, under current constitutional law–which has little to do with the actual Constitution–they’re probably right. Thanks to overbroad congressional delegation, “the Imperial Presidency Comes in Green, Too.” At home and abroad, the legislative branch sits on the sidelines as the executive state makes the law and wages war, despite the fact that “all legislative powers” the Constitution grants are vested in Congress, among them the power “to declare War.”   
  
  
Yet, as I point out in the column, Congress retains every power the Constitution gave it–powers broad enough that talk of “co‐​equal branches” is a misnomer. Excerpt: 



The constitutional scholar Charles Black once commented, “My classes think I am trying to be funny when I say that, by simple majorities,” Congress could shrink the White House staff to one secretary, and that, with a two‐​thirds vote, “Congress could put the White House up at auction.” (I sometimes find myself wishing they would.)   
  
  
But Professor Black wasn’t trying to be funny: it’s in Congress’s power to do that. And if Congress can sell the White House, surely it can defund an illegal war and rein in a runaway bureaucracy.   
  
  
If they don’t, it’s because they like the current system. And why wouldn’t they? It lets them take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the bureaucracy when it imposes costs in the course of deciding what those statutes mean.



Last year, in the journal _White House Studies_ [.pdf], I explored some of the reasons we’ve drifted so far from the original design: 



_Federalist_ 51 envisions a constitutional balance of power reinforced by the connection   
between “the interests of the man and the constitutional rights of the place.” Yet, as NYU‘s Daryl Levinson notes, ―beyond the vague suggestion of a psychological identification between official and institution, Madison failed to offer any mechanism by which this connection would take hold.… for most members, the psychological identification with party appears greatly to outweigh loyalty to the institution. Levinson notes that when one party holds both branches, presidential vetoes greatly decrease, and delegation skyrockets. Under unified government, “the shared policy goals of, or common sources of political reward for, officials in the legislative and executive branches create cross‐​cutting, cooperative political dynamics rather than conflictual ones.”



Individual presidents have every reason to protect and expand their power; but individual senators and representatives lack similar incentive to defend Congress’s constitutional prerogatives. “Congress” is an abstraction. Congressmen are not, and their most basic interest is getting reelected. Ceding power can be a means toward that end: it allows members to have their cake and eat it too. They can let the president launch a war, reserving the right to criticize him if things go badly. And they can take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the executive‐​branch bureaucracy when it imposes costs in the course of deciding what those statutes mean.   
  
  
In David Schoenbrod’s metaphor, modern American governance is a “shell game,” with We the People as the rubes. That game will go on unless and until the voters start holding Congress accountable for dodging responsibility.
"
"

As many readers know, the www.surfacestations.org effort has been gaining a lot of attention, and also volunteers. I’m now at over 130 volunteers nationwide.
The results of the effort attracted national attention. I never went seeking it, but when Bill Stiegerwald of the Pittsburgh Tribune stumbled across it, he wrote a column about it. Little did I know his column was nationally syndicated. Last week I found myself being asked to give radio interviews. One interview, at KIRO in Seattle surprised me when I found myself being co-interviewed with Dr. Thomas Peterson of the National Climatic Data Center (NCDC) the keeper of weather records, including weather station records. The exchange was congenial and stuck to science. That was Thursday June 21st. I am certain NCDC is aware of the effort that is going on to document the stations. Part of the reason the effort exists is that NCDC has been pressed to do this by scientists that want to do exactly what I’m doing, studying the measurement environment, and NCDC has failed to do it. We’ll come back to that.
Part of the method I and volunteers are using to do this project relies on a database of weather station information provided by NCDC. In some cases stations are at airports, fire stations, sewage treatment plants, and ranger stations. In other few cases, they are at the residences of observers that have volunteered to record weather data and submit it to NCDC. Since the latitude and longitude provided in the database is fairly coarse, volunteers have to rely on a database entry called “Managing Parties” to find the name of the location, be it a fire station of the name of the volunteer observer.
You can access the database yourself, its a public record: http://mi3.ncdc.noaa.gov/mi3qry/login.cfm
Use the “Guest Login” button
I last used the NCDC database system this way to locate stations on Sunday evening, June 24th it went down Monday Morning June 25th and displayed a message:
“You are not authorized to view this information. Your IP address has been logged”
When it came back up Monday afternoon, the “managing parties” field identifying the location of the weather station was gone. I would note that I shared a radio interview with Dr. Thomas Peterson of NCDC last week, so I am certain NCDC is aware of the effort.
No notification was given, nor even a professional courtesy to advise of the change, nor any notice on the website. The records were simply removed from public view where they existed before. Given the timing, and because the this same data had been visible on the same system for years It seemed this was a response to the efforts to photograph and document the USHCN network.
Without this information, its is very difficult to locate the stations, and in some cases where the official climate station is in some one’s backyard, completely impossible. For example, fellow blogger and surfacestations.org contributor Russ Steele had a very difficult time locating the official station for Ft. Bragg, CA. The observer did consent to having photos posted by the way. Had Russ not been able to contact the observer, the station would likely never have been found as it’s surrounded by trees and garden.
One of my volunteers wrote a query to NCDC and got this back:
Your inquiry was forwarded to me by our webmaster. I’m glad you’ve found
MMS to be a useful tool in your research.
MMS is our primary source of station metadata for National Weather Service
Cooperative Observer and several other networks, and we are
actively working to provide increased detail for a larger number of stations.
It sounds as though you’ve used the system enough that once you’ve located
a station using the search, you’re clicking on the station name hyperlink
and opening a separate station details window. The managing party for a
station has always been visible by clicking on the “Other Parties” tab. In
the case of NWS Coop stations (the USHCN research network relies upon a
subset of stations in the NWS Coop program), this is usually the NWS office
that administers the site. This information was previously included at the
bottom of the Identity tab’s “form view,” but was removed from that view
early this week because in some cases it also revealed the name of the
Cooperative observer.
Cooperative observers are volunteers who donate their time in the interests
of the public good with a reasonable expectation that their personal
information will remain private. It is the NCDC’s policy to protect
observer details, based upon Freedom of Information Act (FOIA) Update, Vol.
X, No. 2, 1989, which exempts the application of FOIA in certain cases and
establishes privacy protection decisions in accordance with the Privacy Act
of 1974 (2004 edition). This exemption applies when the personal privacy
interest is greater than any qualifying public interest for disclosure.
If you have other questions regarding MMS, please feel free to contact me.
I am often away from my desk, so my response may not be immediate.
I was shocked to say the least. So were others in the scientific community.
Data which was once public for years, has now been removed, and the timing is very suspect.
The claim that it was done to protect the privacy of observers doesn’t stand up to certain tests:
1) COOP weather observers are gathering climate data which is published and publicly available. The program is publicly funded. Data and methods from a publicly funded program that is not classified for national security reasons should be available for public inspection. Clearly results from surefacestations.org so far show some problems with the climate measuring network.
2) That published data is used in a multitude of publicly funded research. Some of that research guides policy decisions. The effects of a public policy decision based on data gathered by a volunteer individuals can affect millions of people. The right of the individual to FOI privacy is trumped by the greater need of the general public’s right to know if the data produced by that observer is accurate.
3) The data has been publicly available for years, removing it now is clearly in response to the effort to examine a public program given the timing of it having been removed four days after an NCDC official became aware of my efforts.
4) The data that has been removed also includes locations of public entities such as fire stations, police stations, sewage treatment plants, park headquarters, state run agricultural experiment farms, and many more. These locations are public entities and have no expectation of privacy whatsoever.
I can understand wanting an individual volunteer’s privacy protected. But the method used so far has been to contact the observer ahead of time, tell them what the project is about, and ask for consent. If consent has not been given, no visit is made, and no photographs are taken. See the rules that each volunteer to surfacestations.org must follow
So you have to wonder this: Is NCDC asserting that the privacy interests of police and fire stations, park headquarters, waste water treatment plants, and a handful of individuals, outweighs the public interest in examining quality of data produced in NCDC records and subsequent NOAA reports and publicly funded research? 

Does this waste water treatment plant measureing temperatures for the climate record really need privacy protection?
I said earlier we’d get back to something.
Dr. Roger Pielke, a senior climate researcher, of the University of Colorado, posted on his blog, his outrage at this action, calling it a “cover up”. Those are strong words coming from a congenial scientist. He also posted something even more shocking:
Pictures of these weather stations already exist, but they are being held from public view. Apparently some time ago weather service offices were issued digital cameras and told to do this work. The pictures were submitted to NCDC, and an archiving process begun, then stopped again for “privacy concerns”.
This is my position:
Given what has been seen so far at weather stations that have been inspected by myself and volunteers, it is clear that parts of the USHCN climate monitoring network are out of compliance with published siting standards and in disrepair. Given that the output of this network drives in part NOAA’s climate assessment, the public should demand a full and open accounting of the condition and data accuracy. If volunteer observers using NOAA equipment at private residences do not wish to have their location and the data it produces scrutinized by quality control methods, they have that right. But the data [produced by these stations should be removed from the climatic dataset because it will be unverifiable.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea58423f3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Today I had to do a round trip drive to San Jose to inspect some video transmission equipment and back to Chico all in a few hours. Coming back, I was in stop and go traffic coming across the Benicia-Martinez Bridge (which is being rebuilt)which carries I-680 across Suisun Bay and had a fair amount of time to look at the Ghost Fleet kept by the Navy there.
Officially known as the National Defense Reserve Fleet (NDRF) they have all sorts of ships there including the WWII battleship USS Iowa, merchant ships, and an aircraft carrier. There are also some WWI steam ships there too, many in a state of decay.
I was reminded of a boat trip I took down the Delta a few years ago where I got up close and personal with these ships. Some were impressive, others downright spooky. I also remeber finding the crossing of the old Sacramento Northern Electric Railway which went all the way to Chico, and up the Esplande. I’ll tell that story another time.
In the meantime here are some pictures and links of the Ghost Fleet.

Several destroyers and merchant ships, plus a tug.


The USS Iowa BattleShip BB61, soon to be moved to Stockton for restoration and display

Another view of the USS Iowa

Carrier USS New Orleans and merchant ships


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7b3e327',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe leading media worldwide cranked up the volume when it spread the news of how a statement had been published in the journal BioScience. The statement was a collaboration of “over 11,000 from 153 nations”.
The Guardian, for example reported: “The world’s people face ‘untold suffering due to the climate crisis’ unless there are major transformations to global society.”
Like most major media outlets around the world, the Guardian handled the “statement” as if it were the final confirmation needed to finally end any further discussion and hesitation on rapidly moving to a new, transformed “global society”.

Slick sales job. Dr. Thomas Newsome falsely claiming over 11,000 “scientists” support the climate statement. Image cropped from video by University of Sydney. 
The statement more hoax than scientific declaration
Days later, after a more careful scrutiny of the list – which the media failed to carry out, it was uncovered that the list of signatories was a declaration of scientific and media sloppiness and deception. One of the signatories was even cartoon character “Mickey Mouse”. But it gets worse than that.
11,224 list analyzed by Japanese blogger
Since then Japanese climate science skeptic and blogger, Kirye, spent dozens of hours thoroughly compiling and evaluating the 11,224 signatories using an Excel spreadsheet. Her findings have added greater clarity and exposed the true extent of the once media ballyhooed statement now turned hoax.
Kirye’s spreadsheet here.
5 of 11,224 a “climate scientist”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Of the 11,224 signatories, JUST FIVE (5) claimed to be a “climate scientist”.
Only 4 were meteorologists.
A vast number did not even state PhD or professor as their professional title/discipline. Only 2,796 (24.9%) had “professor” in their title. 1,481 (13.2%) of the signatories stated some form of PhD, including PhD “candidate”.
A total of 1,021 had “doctor in their title, i.e. only 9.1%. Many in an unrelated field.
303 of the signatories listed no professional title at all!
34 names had to be discarded altogether because they were invalid.
New climate experts: nephrologists, philiologists, pharmacists!
The vast majority were active in fields totally unrelated to climate science, such as “philiogist”, psychologist, CEO, political scientist, pharmacist, medical doctor, primatologist, physiopathology of the mitochondria, sociologist, industrial systems, nanoscientist, genetics, nephrologist, economist. biotech engineer, foreign language teacher, etc.  In other words, it’s a list hyperinflated by unqualified climate activists. Others were affiliated with environmental activist groups.
“Disservice” to science …”blow to credibility”
Thie list and media handling were in fact so sloppy that it compelled German geologist and hjournalist Axel Bojanowski to write at Cicero here how the statement and list of signatories were “a disservice” to climate science and “a deep blow to the credibility of research (and the media), not only because the list of signatories has apparently been published without verification.”
“Mocks media quality control”
The former Der Spiegel science journalist added: “The fact that numerous representatives of environmental associations are among the signatories and many others without a professional title makes one doubt their scientific character” and that it “mocks” the “media’s quality control function.”
Share this...FacebookTwitter "
"For months leading into this bleakest of summers, all anyone out this way could talk about was the dry. It crunched underfoot, was carried on the furnace-blast of a breeze, dust and dry leaves rattling down the wide avenues and riverbanks of the once mighty Murray River like a premonition of loss. It was written in the furrowed brows of farmers poring over district forecasts like they were scripture – ethereal in their promise of deliverance or damnation. Dry as the bones, pronounced, beneath the hides of animals starving and stunned; landscape bleached as a fossil.  Flying into Sydney in early December as the fires ringing the city had just started to burn in earnest, I had a sense of something momentous and profound unfolding, although it still seemed like a distant crisis, one that was happening elsewhere, to other people. The smoke was so thick every aircraft coming in was instructed to land with full instrumentation, so we spent 40 minutes in a holding pattern over enormous fire fronts, swinging lazy circles over great billowing plumes of ember and soot. Walking across the tarmac from the turboprop, I glanced back over my shoulder at the blood orange sun, ash sprinkling from the amber sky. When speaking at an event that evening, I joked that back home we had no water but the air was clean. Go west, where the skies are blue. I left the stash of P2 masks I’d picked up, only half-seriously, on my way to the airport with a friend who said she was taking an Uber to meetings a block away because her asthma was so bad she couldn’t walk more than a few hundred metres. Even as I was going through the motions, paying lip service to the Anthropocene, it still seemed surreal, somehow. I got on to the plane and flew back over the Great Dividing Range, vast eucalypt tracts fizzing smoke, descending once again into the arid, but breathable, air of home. There was a certain sense of foreboding in the hills in the lead-up to Christmas, as though the kindling landscape was holding its breath. The smoke swirled in from hundreds of kilometres north and settled for the festive season. We ate under grey skies, the house shuttered and our heads aching from the fumes, nights punctuated by the kind of toddler asthma attacks that leave you wired until dawn. What if he stops breathing? Hard-earned pennies were pinched and scraped to install an air purifier in his room, and in the nursery too. We didn’t need to leave our beds to know what kind of day it would be. The ochre and rust of another hazy sunrise spilling through the blinds heralded more of the same. Asphyxia. Inertia. Well before it visited itself upon us, the taste was everywhere. Death. New year brought conflagration. The text messages began rolling in. Are you OK? Sure, why wouldn’t we be? Turn on the news. It’s a gut-churning thing to see the names of places you love, those territories of the self and heart, suddenly headlines in a story of unspeakable horror. A tornado of flame, generated by infernos of such ferocity they create their own weather, rolling a fire truck. An unborn child who will never know their father, only his sacrifice. As small communities, we are constellations of people and place, oriented and anchored by and to one another. Talmalmo, Jingellic, Walwa, Corryong are more than just names in a news broadcast or an emergency warning. They are home to our families, our patients, our colleagues, our friends. For the diverse nations who live and gather along the tributaries of the Murray, the Milawa Billa, there could not be a grief more piercing than watching centuries of colonial mismanagement culminate in this ultimate desecration of country, with totem and traditional routes, sacred and spiritual places pared to ash. We all know someone who has lost something; everything. Photos from friends of a wall of fire racing across paddocks, engulfing life as it was once known. Everything is gone. The scars on our landscape will heal, but will we? Instead of celebrating, we rang in the new decade listening to emergency broadcasts, refreshing the Country Fire Authority and Rural Fire Service feeds, and watching Australian Defence Force water tankers flying low over our house. People gathered, muted and pale, in their yards to watch the midnight fireworks a few blocks away as the southerly swept in, acrid with ash. Hours before the Victorian premier, Daniel Andrews, called his late-night press conference to announce an unprecedented state of disaster and wholesale evacuations of East Gippsland and the north-east, we’d glanced at each other uneasily as our phones had chirped, simultaneously, with an emergency text. You need to leave were the first four words. Surely we’ll be safe here, we said, images of what had played out in Mallacoota that week vivid in our minds. The closest bush was 2km away, the fire station on the corner. Still. The rule book appeared to have been torched. We’d been asking for years that the landlord clear the gutters; all it would take was an ember on the right sort of wind. Where would we go if it came to it? Should we pack a bag? If we had to leave everything behind, what would be essential to take? We laughed at ourselves as we had these conversations – the river would be safe, if we could drive. The showground, if we had to walk. We found the passports. There were other matters to consider as thousands of people from surrounding villages and communities streamed into town, towing caravans, horse floats, trailers crammed with all things practical and precious. The supermarkets began selling out of water, servos between here and more distant towns ran out of petrol. It seemed prepper paranoid, but we agreed I should go out and fill up the car and do a massive shop. Just in case. Venturing out into the eerie still of the morning at sunrise, it could have been winter, a mephitic fug enveloping the town like mist. By the time I came out of the supermarket, where the water shelves were stripped bare and the aisles were hazy, the smoke had rolled in so thick they had disabled the automatic doors and taped a sign over the glass. Auto doors closed due to smoke. Please use side doors. Above the blood bank the sun rose, sanguineous; soporific. The sports stadium down the street opened to evacuees, along with the showground. I’d never seen so many people in town, their apprehension as palpable as the baking heat. The outpouring of support for displaced and dispossessed families and for firefighters was astonishing. Within hours of the RFS issuing their daily list of required items for affected communities and crews, it would be filled. Traffic was banked up for kilometres along the bush blocks adjoining the RFS HQ, an operation of military precision marshalling locals and their stocked boots to the driveway to unload. Dozens of volunteers sorted goods on to pallets and into shipping containers for distribution to outlying communities where people were without power, water or food. Some had lost the roof over their heads, their world reduced to ash. A ute sat in the RFS driveway, its tray overflowing with fruit and vegetables, a pallet stacked with eskies dropped off the by the environment minister, Sussan Ley, alongside. We handed over half a dozen shopping bags, and a drawing by our three-year-old son for the fireys. A burly volunteer crouched down to shake his hand and pinned it up, pride of place, on the noticeboard. As we prepared to leave, a pensioner pulled up in a clapped-out Commodore, a plastic bag full of odds and ends proffered from the passenger seat. It isn’t much, I’m sorry. I blinked away tears. The day of the firestorm, 4 January, we hit a record-breaking 46C in town. Clumsy with foreboding, I stepped awkwardly off our back deck and broke my ankle, lying dazed and thirsty in the heat. While the worst of the winds swept across the firegrounds I was in hospital, nauseous with morphine and still gripping the “green whistle”, though it had long since been sucked empty. By sunrise, I was sober, but the air outside was so toxic that, propped up in bed with towels stuffed around the doors and windows, I still had to wear a mask. Our 10-month-old was so hoarse and distressed I gave her Ventolin; she isn’t asthmatic. The PM2.5 reading (something I had only a passing acquaintance with before this summer) climbed steadily throughout the day to almost 3,000 – 15 times the level considered hazardous – before the local monitoring station went offline for the best part of a day. It seemed symbolic of the cataclysm that had unfolded in the surrounding towns and valleys, where the skies were a filthy orange and visibility reduced to metres at best. Days stretched into another week of wheezing confinement, while the prime minister bleated on about Australia being the best country in the world to raise kids. I cried when, during one of those impromptu toddler confidences, my son told me that one day he’d be older than me. I cried because I wonder if he will. To live to be my age, he will have to make it to 2055, when climate change will have rendered much of the planet inhospitable to human life, driven mass extinctions of almost every kind of species, and unleashed civil unrest over resources like food and water on a scale at which there is no precedent in human history. Moreso than the anxiety or political fury of this summer, I have been gripped by an insurmountable grief for my children and the life I took for granted which they will never be able to share. In a matter of days, 9 January, there was another evacuation order, more fires. Several straddling state borders joined up to become a “megablaze”, terminology I’d never had need for before this grim new decade. Millions of hectares of pristine national park went up, places as familiar to me as laboured breath. The roof of Australia aflame, the Alps snowy at the height of summer, with ash. As ever-increasing tracts of Mount Buffalo burned we could, on a rare clear evening, see the great fungating shadow of a pyrocumulonimbus cloud looming over the horizon, a sign that what raged below was so intense it was generating its own weather. There are many images that will come to define this summer: smog settling so thick over Sydney Harbour its iconic landmarks were obscured, the starved and scorched koalas of Kangaroo Island, a masked schoolkid steering a tinny offshore as Mallacoota was devoured by an avaricious inferno, Scott Morrison hanging a shaka on the Waikiki waterfront. But if you’ve lived in a fire zone, the pyroCb or cumulonimbus flammagenitus will stay with you forever, as a harbinger of the Anthropocene. Ignis aurum probat. Ten days into the decade, the alerts sounded uncomfortably close to home. A grass fire on the Beechworth-Wodonga road – a route we would have travelled more times than you could count – was suddenly racing out of control towards the semi-rural outskirts of Wodonga, our sister city, home to some 40,000 people, just across the Murray River. As quickly as it had started, fuelled by a wild southerly change with lightning and 100km/h winds, the blaze was declared emergency level. You are in danger. Act now to protect yourself. It is too late to leave. The safest option is to take shelter indoors immediately. Those in the area who could evacuate made their way into town to shelter at The Cube performance space; others hosed down their roofs and gutters, watching anxiously as water bombing aircraft flew sorties overhead, trying to contain the advancing flames. Defence crews were mobilised as the blaze spotted towards a string of barracks and bases where hundreds of unfortunate people displaced earlier in the week had taken refuge. In the next valley, residents of a relatively new development known as White Box Rise were urged to get out. In among the frantic messages from loved ones as we again hit the headlines was a text from friends who had just bought in White Box and had been scheduled to return from Melbourne that week from their Christmas holiday with their newborn baby. We’d convinced them to delay due to the smoke. This is the other side of our hill, they captioned a screenshot of the emergency alert. Sweet Jesus. Our house. As night fell, the winds eased, and a smattering of rain brought some much-needed reprieve. Evacuees were, in waves, permitted to return home. There was a nagging, contradictory sense that the gun was never really loaded, but we’d dodged a bullet nonetheless. Reams have already been written about this grim summer; an unwelcome but imperative clarion call to the millions of us sleepwalking toward the abyss. Just two months ago I sat in a climate-themed medical conference where they discussed the need for field hospitals, colocation of emergency triage and primary care, new ways of doing business when the climate rendered business-as-usual obsolete. The fires were burning, even then, and Prof David Bowman warned us that this was a season like no other. Yet it all seemed so abstract; so rhetorical. The climate emergency was something that was happening elsewhere, to other people. Until it wasn’t. Andrews has described this as a summer of firsts; there’s been so many now they seem to have lost impact as time goes on. But there are snapshots of this season I won’t forget. Toddlers receiving commendations for bravery on behalf of fathers who will miss a lifetime of milestones. Stepping on to the tarmac under that ominous, orange sky, the scarcest smattering of ash on the breeze. Evacuation sirens; smoke so dense it cancels out the sun. The fear in my son’s eyes as he struggled to catch a breath. Thousands upon thousands of livestock charred and scattered by the road; millions upon millions of native animals – likely entire species – incinerated. We need new words for collective grief of this scale. There are political observations to be made, and urgent agendas to be advanced if – and indeed, it feels so precariously like an if – we wish to survive. Already, the goalposts have shifted to “the new normal”; summers spent indoors lest the air chokes us all, Christmas under slate and noxious skies, evacuation orders covering ever-larger concentric circles until, at last, there is nothing left to burn and nowhere left to run. If we are to take anything from this season of solastalgia, it must be the immense grace and goodwill, courage and conviction that abides in our communities and comes to the fore when it’s needed most. As our climate becomes more hostile, perhaps the single greatest risk is that, in tandem, so do we. The learned helplessness of neoliberalism not only invites us to believe that we, as individuals, are powerless, it depends on it. But we’ve seen something else entirely forged in these long months: leadership from the grassroots, the ability of communities to rally around one another not thanks to political action but in spite of it. Ignis aurum probat, miseria fortes viros. As gold is tempered by fire, so strong men are tempered by suffering. Collectively, our strength is infinite. Now, more than ever, we must live as testament to that. • Amy Coopes is a Croakey editor, journalist and medical student • This piece was originally published in Croakey and is republished with permission. It is part of Croakey’s ongoing contribution to the Covering Climate Now initiative, co-founded by The Nation and the Columbia Journalism Review (CJR), in partnership with The Guardian"
"

Could it be the _Washington Post_? Bannered across the top of the _Post_ ’s op‐​ed page today is a piece titled “Copenhagen’s political science,” titularly authored by Sarah Palin. I’m delighted to see the _Post_ publishing an op‐​ed critical of the questionable science behind the Copenhagen conference and the demands for massive regulations to deal with “climate change.”   
  
  
But Sarah Palin? Of all the experts and political leaders a great newspaper might call on for a critical look at the science behind global warming, Sarah Palin?   
  
  
What’s even more interesting is that the _Post_ also ran an op‐​ed by Palin in July. But during this entire year, the _Post_ has not run any op‐​eds by such credible and accomplished Republicans as Gov. Mitch Daniels; former governors Mitt Romney or Gary Johnson; Sen. John Thune; or indeed former governor Mike Huckabee, who might be Palin’s chief rival for the social‐​conservative vote. You might almost think the _Post_ wanted Palin to be seen as a leader of Republicans.   
  
  
I should note that during the past year the _Post_ has run one op‐​ed each from John McCain, Bobby Jindal, Newt Gingrich, and Tim Pawlenty. (And for people who don’t read well, I should note that when I call the people above “credible and accomplished,” that’s not an endorsement for any political office.) Still, it’s the rare political leader who gets two Post op‐​eds in six months, and rarer still the _Post_ op‐​eds by ex‐​governors who can’t name a newspaper that they read.
"
"

As the air’s CO2 concentration rises in the years and decades to come, the negative impacts of drought on wheat biomass and grain yield should diminish, a conclusion that can be derived from the recent work of Dias de Oliveira _et al_. (2015).   
  
The five-member Australian research team noted that “elevated CO2 and high temperature are climate change drivers that, when combined, are likely to have an interactive effect on biomass and grain yield,” leading to three possible outcomes: (1) a “reduced positive effect of elevated CO2,” (2) an “amelioration of the effect of high temperature, or (3) a “synergistic effect where high temperature increases the positive effect of elevated CO2.” They also note that the resultant response “may be influenced by [plant] genotypic differences.” In an effort to study these interactions and possibilities, Dias de Oliveira _et al_. designed a field experiment to determine the interactive effects of CO2 and temperature, as well as those of a third variable—drought—on two pairs of sister lines of wheat ( _Triticum aestivum_ L.) over the course of a growing season, where one of the contrasting pairs of wheat sister lines differed in tillering, or branching (free vs. reduced), while the other differed in early vigor (high vs. low). The experiment was conducted out-of-doors in Western Australia in poly-tunnels under all possible combinations of CO2 concentration (400 or 700 ppm), temperature (ambient or + 3°C above ambient daytime temperature), and water status (well-watered or terminal drought post anthesis). So what did it reveal?   




After presenting a very long list of findings, Dias de Oliveira _et al_. summarized their results as follows: (1) elevated CO2 “increased grain yield and aboveground biomass,” (2) terminal drought “reduced grain yield and aboveground biomass,” but elevated CO2 “was the key driver in the amelioration of [its negative] effects,” (3) “temperature did not have a major effect on ameliorating the effects of terminal drought,” and (4) although “the mechanisms by which [the CO2-induced] enhancements were brought about differed in each pair of sister lines,” there was “no difference in aboveground biomass or grain yield within each pair.” Thus, it would appear that the overall outcome the researchers observed in this study was one in which elevated CO2, acted alone, overpowered the negative effects of a debilitating environmental stress (drought). Consequently, as the air’s CO2 concentration rises in the years and decades to come, the negative impacts of drought on wheat biomass and grain yield should diminish. And that is good news worth celebrating!   
  
**Reference**   
  
Dias de Oliveira, E.A., Siddique, K.H.M., Bramley, H., Stefanova, K. and Palta, J.A. 2015. Response of wheat restricted-tillering and vigorous growth traits to variables of climate change. _Global Change Biology_ **21** : 857-873.


"
"
The recent photo submissions at surfacestations.org have demonstrated that many NOAA/NWS climate monitoring stations feature convenient close-by vehicle parking.
Not to be outdone, the Paso Robles USHCN Climate Station of Record features freeway on-ramp access to California’s Highway 101. The weather station is just feet from the street, with the temperature sensor placed just high enough to catch full view of vehicles over the fence.
 My thanks to surfacestations,org volunteer Ed Hahn for this photo. His complete photo essay is available here
Here is the NASA GISS plot for Paso Robles:

Curiously the GISS database still classifies this station as a “rural area”.
I find it interesting that the temperature was trending down in the 70’s then a huge offset occurred just about 1980. I wonder if that was when the freeway access was added? Nothing in the MMS records seem to indicate a station move or other change at that time. Or maybe that’s when somebody got the bright idea to pour a concrete slab under the the station?
From NOAA’s own siting specs: “The sensor should be at least 100 feet from any paved or concrete surface.”
Close enough for government work…


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea547028d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From the ""almost a
Darwin Award winner""
department
and from
WISN-TV
in Milwaukee…TV News Truck Breaks Through Ice:

Even though the temperatures have fallen, the ice on many bodies of water is
not thick enough to support vehicles.
A crew for a local television station drove its news truck onto a channel to
Big Muskeo Lake Sunday and broke through the ice. Crews for WDJT-TV in Milwaukee
were reportedly shooting a story about thin ice when the truck fell through.
The driver reportedly mistook the channel for a road when the accident happened.
The truck was approximately 150 yards off the boat launch, according to a
release issued by the City of Muskego Police Department.
The driver of the truck was the sole occupant and was able to get out without
injury, but the truck remains partially submerged.

Moderators note: Having worked in TV news myself, I’m
not that surprised. Often the only thing on reporters and producers minds is the
story deadline. Caution and common sense sometimes take a backseat in the news van.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea880e97d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"We’re in the midst of fevered discussions about communications and security. Cybertarian campaigners want to stop collusion between corporations and governments to intercept citizen chat; attention-grabbing adolescents at Anonymous want to disrupt murderers who dislike mockery of their principal prophet and the gilt-edged grown-ups in national security services want to listen in on plans to revenge such blasphemy. But away from these dramatic debates over speech, privacy, the state, a less exciting conversation is underway, beyond the third-sector moralism of cybertarians, the attention span of adolescents, and the Olympian speechifying of spymasters. This conversation touches on security and communications in a less spectacular way. Electronic waste (or e-waste) is the largest source of materials left in municipal dumps around the world.  A high proportion of it is derived from the gadgets you are reading this article on: phones, tablets, and computers, which quickly move from being vital sources of everyday life to discarded garbage once an upgrade becomes available. Where did that old fat-screen analogue television go when it was replaced by the slim, flat-screen digital version? Where are those phones you threw out? A vast proportion of these deadly gizmos, with their lethal cocktails of carcinogenic gases and chemicals, end up being unsafely recycled by the poorest of the poor, the most vulnerable of the vulnerable. Pre-teen girls in Chinese and Indian villages are expert at the dangerous work of extracting recyclable minerals from our detritus. Increasingly, of course, the trade in e-waste is domestic. Asian middle classes are booming and as keen as their so-called “Western” counterparts to fetishise the fresh and new by dumping the toxic and the old in the villages and bodies of the desperate. The result is horrendous disease, a poisoned water table, and drifting air pollution. Because this trade occurs in what is politely known as “the informal sector” – translation, where there are no taxes, benefits, health and safety protections, or retirement payments – it is difficult to trace the precise dimensions of e-waste. But the International Telecommunication Union, a technology booster if ever there was one, acknowledges the existence of a problem as annual amounts reach a reported 53 million metric tonnes (2013), with an additional 67 million metric tonnes sold in various forms. And after recycling? E-waste ends up in some very interesting places. Two years ago, the Pentagon and the US Senate Armed Services Committee reported that vital components of the nation’s military hardware routinely include “counterfeit” electronic materials, from China (70%), the UK (11%), and Canada (9%). The Pentagon defines counterfeiting as recycled materials wrongly sold as new, or misuse of others’ intellectual property. Often these counterfeits come from recycled e-waste – the committee estimated more than a million counterfeit parts were in service in US planes. It’s no surprise much of this can be traced back to China, where counterfeiting operates at an industrial level, with factory floors populated by thousands of workers dedicated to the task. Take the Boeing P8 Poseidon, a plane used by the US Navy to drop torpedoes, depth charges and carry surveillance equipment. In 2011, Boeing reported it had discovered a faulty ice detection system in the aircraft, according to the senate committee report. Further investigation revealed the part was previously used, and made to appear new.  After tracing the parts through companies in California and then Florida, it turned out the ice detection equipment had originally come from “an affiliate of A Access Electronics in Shenzhen, China.” And before that? Who knows? Investigators from the senate committee wanted to find out but were denied Chinese visas. Now the story is in the news again. Forbes has just published a column on the topic as did National Defense magazine, an obvious mouthpiece for the military industrial complex. The principal beneficiaries of the complex are warning that this malfeasance continues unabated, and at a massive level, despite 2012 legislation designed to quell it. The senate committee report refers to “risks to national security and the safety of US military personnel” posed by this trade in counterfeit e-waste. There is no mention of the risks posed to people all over the world by the United States’ very use of matériel, of course. But the fact that e-waste is on the agenda in such powerful quarters bodes well for real reform towards managing it properly. The technology is available to recycle our electronic pleasures in a much safer way than is generally the case. In many instances, laws exist to mandate that. What we need is proper use of the technology and serious enforcement of such legislation. But beyond that, we need an end to the built-in obsolescence and advertising-fuelled clamour surrounding innovation. It would help if the cybertarians, adolescents, and spies were able to let go of their shared obsessions for a moment and question the very devices and systems on which they rely for their self-anointed roles. Then they might recognise that the much-vaunted internet of things is also and equally an internet of junk."
"São Paulo’s ongoing water crisis has left many of the city’s 20m or more residents without tap water for days on end. Brazil’s largest metropolis is into its third month of water rationing, and some citizens have even taken to drilling through their basements to reach groundwater. Most commentators agree that the crisis is to blame on multiple factors, but few have questioned the role of the water company in charge: Sabesp. The utility, responsible for water and waste in São Paulo and the surrounding state of the same name, has clearly failed its public service remit. Yet, it’s not even clear whether public service is the highest priority for part-privatised Sabesp, whose directors have just awarded themselves bumper bonuses despite millions of their customers going thirsty. São Paulo’s water will go from crisis to crisis so long as Sabesp prioritises profits over long-term investment. Clearly there are human-induced environmental factors at play: climate change, deforestation of the Amazon, pollution, as well as overconsumption. The pressures we put on nature are likely to increase water shortages worldwide, perhaps leading to conflicts and wars. However, at the same time, there have always been droughts. Historical records going back hundreds of years show how cities and regions have struggled and often coped with extreme water shortages. So, periods without much rain are nothing new. But if that is the case, shouldn’t it be the responsibility of water utilities to plan for such events, putting in place contingency measures to manage possible water shortages? São Paulo’s extraordinary growth in recent decades has overloaded the Cantareira, the city’s water supply system. But the rapid increase in water usage was hardly a surprise; it’s something that could have been managed and planned for. Sabesp has failed to do exactly that. One of the world’s largest water utilities, Sabesp was founded as a public institution in 1973. Since part-privatisation in 1994 the state of São Paulo has maintained at least half of the company’s voting capital, though shares are also traded on the New York and São Paulo stock exchanges. While The Economist and others were keen to point out that Sabesp is “majority-owned by the state government”, this doesn’t tell the whole story. The utility is neither a public organisation concerned with providing a public service, nor a private company facing competition from other companies and controlled by regulatory agencies. Just like the “natural monopolies” enjoyed by water companies in the UK, Sabesp has a publicly guaranteed monopoly, yet its profits are part-privatised – earlier this year it paid out R$252m (US$83m) in dividends. São Paulo’s water is just one of many public utilities that have been privatised throughout the world over the past few decades. Governments have followed the ideological belief that, in order to conserve and manage water properly, it is essential to put a price on what used to be a public good. In 1992, the UN adopted the Dublin Principles, declaring that putting a price on water and establishing a “participatory approach” – which is about involving users, planners and policy-makers at all levels – was the best way to reach a sustainable and equitable governance of water. The principles were quickly adopted by Brazil’s government, and implemented first in, you guessed it, São Paulo. The Dublin Principles call for the establishment of “basin committees”, formed of government, water companies, local residents and civil society. These committees are supposed to be responsible for deciding on water use in a particular watershed. Yet, 23 years after this mechanism was supposedly implemented by Law 7663 in São Paulo – and after 17 years of a similar rule at the national level – we still do not know who participated in these committees. On paper these committees exist, but in practice they are not empowered by state structures. Dysfunctional governance in São Paulo state has left the part-privatised utility, Sabesp, to mainly follow the principles of the market and the interests of its private shareholders. This inevitably skews its strategy towards the short-term. When deciding whether to make the necessary investments to prepare for possible water shortages, Sabesp has had to choose whether to safeguard the public supply or increase the value of its shares. The company did invest US$4 billion from 2005-2013, but that is still not enough. Many of the necessary measures to prevent the current crisis – such as upgrading the Cantareira system – were not implemented because they would be unprofitable to Sabesp’s shareholders . The company’s lack of transparency since the crisis kicked off highlights its planning failure. For many months Sabesp denied that water was being rationed. Then state governor, Geraldo Alckmin, acknowledged that there was lack of water, but said they were “isolated and private” cases. Then a bonus offered to those who economised water in their daily use, later turned into a fine for those who “waste” water. The most essential resource of all has now become a struggle in São Paulo. Yet, ever deepening inequality has turned a water crisis into a social and economic crisis – communities on the periphery of the city and slums were inevitably the first to have their water rationed. Responsibility for this crisis lies with Sabesp and two decades of running water supply as a for-profit service. It is a failure of public-private partnership. As climate change and other environmental factors make water crises more likely, we better rethink the way water is managed worldwide."
nan
"According to the UN’s Intergovernmental Panel on Climate Change, urgent and unprecedented changes are needed to avoid a climate change catastrophe. Although efforts are already being made to reduce the production of greenhouse gasses, they are by most estimations not enough.  It is therefore critical that we find ways to drastically reduce the amount of pollutants in the atmosphere. Ecosystems capable of absorbing and storing large amounts of carbon dioxide know as “carbon sinks” are ideal for this.  In principle, all living organisms – all animals, plants, algae and bacteria – consist of carbon and so function as a carbon sink. For example, as long as a tree lives it will absorb and store carbon. Given the sheer volume of all the trees contained in tropical forests, it’s no wonder most people imagine such forests when they think of a carbon sink. However, once chopped down and turned into firewood, the carbon in those trees will be released and emitted back into the atmosphere as CO₂. So while a forest is a moderately efficient carbon sink, its capacity to retain carbon in the forest floor is limited.  In fact, new research by colleagues and I has found that such forests are actually only the fifth most efficient ecosystem in the carbon storage cycle behind salt marshes, mangrove forests, seagrass meadows and, best of all, tundra. Tundra is found in polar or mountainous regions where temperatures are too low for trees to grow, and the landscape is dominated by grasses or moss. As a large part of the carbon is stored in the frozen soil and so is harder to disturb, it makes a very efficient sink. However, rising temperatures are melting the tundra in many parts of the world, releasing stored carbon back into the atmosphere, and as a consequence its capacity to store carbon is decreasing.  While forests and tundras are losing capacity for carbon storage, another often forgotten ecosystem may hold the answer: seagrass.  Seagrass plants have an excellent capacity for taking up and storing carbon in the oxygen-depleted seabed, where it decomposes much slower than on land. This oxygen-free sediment traps the carbon in the dead plant material which may then remain buried for hundreds of years.   Seagrass meadows are, for the most part, in recession across the globe due to human activity. As a result the re-establishment of these meadows will make it possible to greatly increase the carbon storage potential of our oceans. Many factors influence the exact amount of carbon that can be taken up by a seagrass meadow, but rough calculations show that if we restore one hectare of seagrass, it would correspond to at least ten hectares of dry-land forest and even as much as 40. Planting vast areas of seagrass meadow is also an eminently doable task as these plants are not seaweeds, but plants with flowers, leaves and roots just like plants on land. This means they produce seeds that can be sown in the seabed or small shoots that can be planted by divers. To develop new techniques for actually planting all this seagrass on a massive scale, colleagues and I have been involved in the Novagrass project, which trialled seagrass planting in the coastal zone around Denmark. We tested various techniques, involving both seeds and seedlings, and had the most success when planting seedlings in chequerboard patterns on the seabed. The lessons from this project are now being applied in a larger scale trial, where muddy seabed is topped up with a layer of sand before seedlings are planted. We are waiting on the results, but so far this technique appears to be a promising way to re-establish eelgrass in coastal areas. There are about 60 seagrass species in the world to choose from, but we focused on common eelgrass (Zostera marina). It cannot tolerate warm seas but it’s the most common species in temperate areas and grows well around coasts in the northern hemisphere. Seagrasses thrive in coastal zones, they have the potential to grow all over the world (except Antarctica) and are even expanding into the Arctic as the ice recedes.  There is some evidence of natural recovery after excessive nutrients from fertilisers and other human pressures have been relieved. But much more action is needed to avoid further loss – and indeed new growth – of these valuable ecosystems."
"

 ** _Editor’s note_** _:_ _In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security _an edited volume of papers originally presented ata Cato conference the previous year. In each chapter, experts on international security assessed, and put in context, the supposed dangers to American security, from nuclear proliferation and a rising China, to terrorism and climate change. _



_As part of ourProject on Threat Inflation, Cato will be republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. _



_The first response comes from Brendan Rittenhouse Green, an assistant professor at the University of Cincinnati, and a recently namedCato adjunct scholar. _



——-



Many world leaders today could tell you, earnestly and genuinely, that their country faces major security threats. Historically, such threats have been endemic to the international system, and they have tended to consume most of the time, attention, and social resources of national policymakers. Moreover, statesmen from the past and present alike could probably adopt a common definition of what a “security threat” is: the possibility of outside actors using large scale violence to menace a state’s sovereignty, territorial integrity, or the physical safety of a substantial portion of its populace; or the emergence of a state that could obtain enough material power to do these things.



But the modern United States does not have this kind of problem. To be sure, its foreign policy discourse has been suffused with the language of security threats for a hundred years. The regnant American grand strategy, which I term primacy, is justified largely—though not exclusively—on security grounds. Yet no state with enough military power to reach inside the Western Hemisphere is likely to emerge any time soon. In short, there is a major disjunction between the language sometimes used to explain and justify American foreign policy commitments and the actual purpose of its strategy.



This, at any rate, was the premise of my essay “Security Threats in Contemporary World Politics.” In it, I made three basic arguments. First, I tried to show that America’s most powerful rival, China, looks nothing like the most plausible past security threats faced by the United States—the Nazi and Soviet empires. Indeed, China would have to jump over a series enormous hurdles before it even came within shouting distance of such dangerous states. Second, I claimed that the political commitments entailed by primacy had only a small prospect of reducing competition in China’s backyard below what it otherwise might be. That is, primacy has a “goldilocks problem”: the highly revisionist states that would propel any East Asian competition are likely to be either absent, or too highly motivated for American power to discourage them from risky behavior. Third, I argued that American political commitments were themselves the most plausible sources of threats to national security. Though unlikely to successfully depress regional competition, primacy’s political connections provide several mechanisms by which America could become involved in a major war.



Looking back on this essay from nearly a decade’s distance, I continue to endorse its major claims. Though I might make a few marginal changes here and there, my views are still roughly the same. But national security discourse, recent history, and my own intellectual temperament have all been altered in important ways. These changes would make for a very different essay, were it written today.



For one thing, the essay’s overwhelming focus on security issues seems less necessary today. Over the past decade, national security discourse has increasingly centered on the defense of the “liberal (or rules‐​based) international order” as the key object of American foreign policy. I think the idea of the “order” borders on conceptually incoherent. But it does have a key virtue: it has enabled more and more analysts to admit that American grand strategy is concerned with something other than traditional security problems. It has therefore made the trade‐​off at the heart of American grand strategy more obvious: American leaders are risking major war, and thereby making the American people less secure, for the purpose of shaping the international environment in ways they consider favorable. Was I re‐​writing this essay today, I would devote more attention to examining the supposed benefits of the international order. Essays by Daniel Drezner and Eugene Gholz from _A Dangerous World?_ provide excellent examples of this kind of analysis.



Another idea I would emphasize more is the idea of “tail risk.” The world today is living through a global pandemic, which will probably kill hundreds of thousands of people and induce the worst economic crisis since the Great Depression. This turn of events was unexpected, even though the potential for a devastating global pandemic has been well‐​known for decades. Nevertheless, most countries were underprepared.



Many rare phenomena pose a similar problem. Society lacks the data that would justify the assumption that certain kinds of apparently rare events are in fact extreme outliers on a bell‐​shaped curve of event frequencies, rather than merely uncommon results of some other kind of frequency distribution.. In fact, it turns out that many rare events—for example, earthquakes, rogue waves, and importantly, war—do not follow a normal distribution. In many cases, the statistical likelihood of such events is far greater than the traditional bell‐​shaped curve would imply—the tail ends of the actual distribution of events are “fat.”



Societies are therefore likely to underestimate the risk associated with rare events. I suspect that the probability that America’s primacy strategy will produce a major war is similarly underestimated. The probability may be relatively low, but the scale of disaster would be very large. Over the long‐​term I worry that the chances of such a war would exceed the tolerance threshold of even the most aggressive strategist. Considering and analyzing this possibility seems like an especially salient task in light of recent events.



Finally, if I wrote the essay today, I would focus more attention on the idea of “second best” strategies. Early in the last decade, I still had something of the zeal of youth about me. I retained hopes that normal politics might produce non‐​trivial change in American grand strategy. After all, the country had been somewhat chastened by its exhausting wars in Southwest Asia. The Tea Party, whatever its faults, was a live political force that had managed to achieve temporary restraint in the defense budget, a feat whose last occurrence had required the collapse of the Soviet Union. Obama was pursuing a second‐​term foreign policy that, if not exactly worth defending, at least challenged the elite consensus on grand strategy in a couple of respects.



Well, there is nothing that the world likes better than nice, tasty hopes. The forces enumerated at the end of Christopher Preble and John Glaser’s lead essay turned out to be significantly stronger than I estimated. American power has proven so extensive that a grand strategy explicitly justified in terms of many varied goals like the liberal order is now plausible to the foreign policy establishment. The material and ideological consensus in favor of primacy among the national security elite has proven so robust that American commitments have been able to resist the election of a president like Donald Trump, who is no one’s idea of an internationalist. The American people turned out to give even less of a damn about foreign policy than I expected.



Today I believe that the probability of normal politics producing a genuinely restrained grand strategy is exceedingly slight. The best hope for a major change is probably a crisis that exposes the unexpected risks and costs of primacy. For this reason alone, the task of making the case for restraint remains vital: policymakers will need to have good ideas lying around if and when the bankruptcy of primacy is revealed.



However, I increasingly believe that more effort should be devoted among partisans of restraint to “second‐​best” policies, in case my pessimistic political assessment proves out. And I am not confident that the standard answer—that the second‐​best policy is “less of whatever primacy is proposing”—is always true.



For instance, if we are not going to abandon American alliances, I am not certain that loosening those ties is worthwhile, as it may encourage bad behavior among allies and adversaries alike. If we are going to retain American political commitments, then I suspect that will require more robust military capabilities than I would like as a matter of first preference. I worry that grand strategies may best be plotted on a U‐​shaped curve, where the tail strategies of primacy and restraint both produce reasonably coherent and stable outcomes, but where the strategies in the middle—“off-shore balancing,” “selective engagement,” and “liberal internationalism”—turn out to be ineffective and destabilizing to world politics.



But working out whether there is anything to these concerns would be the subject of a completely different essay. And the present essay, I believe, retains real value. Its fundamental conclusion is still true: “the United States spends hundreds of billions of dollars a year—and risks war—largely to stop other people from fighting among themselves. The common story that reducing regional competition abroad makes America more secure at home is close to being backwards.”



My essay is not the most original or brilliant exposition of this basic point — but as bottom line conclusions go, I think one could do a lot worse.



–Brendan Rittenhouse Green



Cincinnati, OH
"
"
Share this...FacebookTwitterLest anyone has wondered why the climate movement has shifted its focus over to children, it is because too many adults just refused to buy into the climate-Armageddon hoax.
It’s a fact: Children are very easy to manipulate and deceive.
Children – and adults with stunted intellectual development – are much easier to convince than adults who have been around the block of life a few times. Children are naive, inexperienced, lack insight and highly impressionable. This makes them vulnerable and thus really easy targets for climate radicals.
88 victims of sadism
Nothing illustrates this better than a recent story appearing in the Daily Mail here, where it is reported how a German sadist, via Skype, was able to successfully convince 88 young women to give themselves potentially lethal 230-volt shocks!
If one sicko is able to convince people to practically electrocute themselves, then imagine how easy it is for the media/organized activists to convince kids a climate doomsday is coming. It’s all based on the same bloody. The approach is the same in both cases:
1) First there’s an offender who derives pleasure through power over the victims
2) The offender claims to have great authority
3) The offender requests that their victims submit and obey
4) The offender claims it’s for a good cause
5) The offender promises the victims great reward for submitting
6) disaster results
1. Pleasure from sense of power
Just as the German ‘socket sadist’ derived his pleasure from the sense of power over his victims and sexual gratification, the climate radicals derive their pleasure from the control they have over today’s children.
2. Claim of authority
While the German socket sadist claimed to be a researcher conducting an experiment that would advance science and thus the common public good, climate radicals falsely claim to possess the scientific truth, and that they have everything under control and can be trusted.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“The victims believed he was a scientist and there was no danger to them to carry out the experiment, that’s why they agreed,” the Daily Mail quoted prosecutors. “But he appeared so serious,” one victim later said.
The socket sadist refused to be challenged. If his victims resisted cooperating, then they were made to feel guilty and inadequate. With climate radicals, they label dissenters as deniers and villains. The climate radicals also do not tolerate any questioning or dissent.
3, Request to sacrifice
The German socket sadist asked his young female victims to hurt themselves – all in the name of science. The climate radicals demand that their followers collectively subject themselves to hardship and accept going without the amenities we enjoy, while exempting themselves.
4. Do it for a good cause
The socket sadist promised his victims it was in service of science, a good cause they could feel good about. Likewise, the climate radicals falsely promise kids they will see a much brighter future-  but only if they submit and do as they’re told. It’ll save the planet if they do, they are told.
5. False promises of reward
While the “socket sadist” allegedly made false promises of money (up to €3,000) to his victims and assured them they were participating in the noble cause of advancing science, the highly organized and authoritarian climate radicals promise the kids that if they do as they are told, the planet will be rescued, will become a green paradise, and peace will reign.
And they won’t have to school on Fridays.
6. Will turn into a mess
Just as the socket sadist case turned into a disaster, so will the extreme climate movement of zero carbon emissions by 2050.
Ironically, defense lawyers for the socket sadist, Klaus W Spiegel and Matthias Bohn, are now claiming their client had diminished responsibility for his actions as he suffers from Asperger Syndrome and autism. Sound familiar?
Share this...FacebookTwitter "
"

Our lovely model above appears to be boarding up her cabana in preparation for the next hurricane.
Nails, as inventions go, have a legacy back to before the time of Christ. And after two milliennia, they are still pretty much the same; a piece of iron wire with a wider top to be driven into two pieces of wood to hold it together.
Sure there’s been improvements in steel, in manufacturing, and in making better hammers, but the nail itself hasn’t really changed much.
So its with a surprise that I read in Popular Science that a new nail became one of the ineventions lauded in 2006. Popular Science is naming its Best of What’s New.
It’s not your average nail though, the HurriQuake nail spent six years in development. Its designed to help building withstand hurricane force winds (over 71MPH) and earthquakes that rock wood structures apart. I expect our own local best hardware store, Colliers, to start carrying this sometime soon…I mean they HAVE to, they have EVERYTHING.
From the article:
“As the Bostitch team tweaked the head-to-shank ratio, Sutt and metallurgist Tom Stall worked on optimizing high-carbon alloys, trying to find the highest-strength trade-off between stiffness and pliability — the key to preventing snapped nails. ‘Meanwhile,’ Sutt says, ‘we were focusing on how to keep the nail from pulling out.’ The team machined a series of barbed rings that extend up the nail’s shaft from its point, experimenting with the size and placement of the barbs. ‘You want the rings to have maximum holding power,’ he says, ‘but if they go up too high, it creates a more brittle shank that shears more easily.'”
Now if they can just invent the thumb-proof hammer, we’ll really have something.
I guess you could file this invention under “global warming” as the company references the recent increased frequency of hurricanes as an impetus to the invention. Personally I think the linkage between global warming and hurricanes doesn’t wash, as do many hurricane experts like Dr Neil Frank, former director of the National Hurricane Center and Dr. William Gray, a world renowned predictor of hurricanes. The 2006 hurricane season ends December 1st, and so far this year has been an.. er. wash out in big hurricanes with only 7 in the Atlantic, compared to 2005’s hurricane season with 7 major storms, including hurricane Katrina. Katrina became the media poster child for the global warming to hurricane link, among other things.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9c0cb1c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Ok the next session is starting in a few minutes, less than 2 hours from now I’m going to know if the work I and all of the volunteers at www.surfacestations.org has been scientifically fruitful, or if I’m going to get pelted on the stage with rotten fruit.
My presentation is updated with some late breaking photos Russ Steele got yesterday from St. George, UT, loaded into the presentation laptop, and my remote control has been tested. I’m as ready as I’ll ever be.
At the very least, after sitting through a bunch of Powerpoint presentations, my use of the same software I use for doing TV weather presentations should break the mold.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea442c16c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

My friends at coffee this morning got a huge laugh out of Chico Peace and Justice Center member Sherri Quammen’s claim in a vitriol filled letter to the editor that I’m the “real WMD”.
For somebody who professes “peace and justice”, she sure seems to have a lot of anger to vent. She’s sent letters to all three newspapers, the ER, Chico Beat, and you’ll see the same letter come Thursday at the Chico News and Review I’m sure. Lately, the message of “peace on earth” seems to have lost the accessory clause of “goodwill towards men”. Though its hard to tell through her rant just what she dislikes about me most, it appears that my views and research into climate change must be the main factor.
I sent her a nice note last week, offering to meet and get aquainted over coffee or tea someday, (since we’ve never met) after the letter appeared in the Chico Beat, so far no response.
But that’s OK, being a public person, criticism comes with the territory. It’s an occupational hazard. I guess I should be honored that my threat level has been elevated. Poor Al Gore takes all sorts of flak daily.
Sooo….since I’ve been labeled a WMD, I think that I’ll have to look over my shoulder a lot to make sure I’m not being followed by police officers intent on giving me a ticket in case I go off in the Chico city limits. That’s a $500 fine you know.
To make it easier for people to spot me, I think I’ll get a T-shirt that says simply “BOOM”.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6794214',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
By Kirye (photo)
and Pierre Gosselin
Today we look at the mean annual temperatures of western USA stations that have a Brightness Index (BI) of 0, meaning they are not subjected to urban heat island impacts.
Many people are claiming that temperatures worldwide are rising due to greenhouse gas emissions from human activities.
First we begin with the station located at the town of Fort Bragg in California. Using NASA data, we plot the annual temperatures going back to 1935!

Data source: NASA GISS 
Above we plot the V4 unadjusted versus the V4 adjusted data. Neither show any warming since 1935. The adjusted data, however, turns a cooling trend into one of no cooling.
Recently I tweeted an animation that compares the v4 unadjusted data to the V4 adjusted data for the Beowawe station in the state of Nevada:

GHCN V4 Unadjusted data show Beowawe, State of Nevada has had a cooling trend since 1891!Needless to say, NASA changed the data by a large margin.https://t.co/XfEZRXWoFl~#地球温暖化? #温暖化？ #気候変動 #ClimateChange pic.twitter.com/bDIleXLYuB
— キリエ (@KiryeNet) May 10, 2020

Note how the Beowawe data of the past was substantially altered (reduced) in order to create a warming trend from a previously cooling trend. Here the warming is man made – but statistically by researchers at NASA.
The story is similar for 4 other stations located in the western US.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the Manti station in Utah, modest warming was adjusted to created more warming:

Data source: NASA GISS
The same is true for the Seligman, Arizona station:

Data source: NASA GISS. 
The mean annual temperatures measured by the Cheesman, Colorado station used to show a cooling trend since 1903, before NASA tampered with the data and changed them into a warming trend: 

Data source: NASA GISS.
Finally we look at the data from the station for Hachita, New Mexico:

Data source: NASA GISS.
Here for Hachita, NASA changed the data so that modest warming was changed to produce greater warming.
Why do the new, adjusted data plots always end up warmer and never cooler? This seems to be Deep State science, and not real science which the public expects to get and is owed.


		jQuery(document).ready(function(){
			jQuery('#dd_016dc3091d95560b9168dfc61dda50bc').on('change', function() {
			  jQuery('#amount_016dc3091d95560b9168dfc61dda50bc').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The new US stealth fighter, the F-22 Raptor, was deployed for the first time to Asia earlier this month. On Feb. 11, twelve Raptors flying from Hawaii to Japan were forced to turn back when a software glitch crashed all of the F-22s’ on-board computers as they crossed the international date line.
The delay in arrival in Japan was previously reported, with rumors of problems with the software. CNN reported that every fighter completely lost all navigation and communications when they crossed the International Date Line. They reportedly had to turn around and follow their tankers by visual contact back to Hawaii. According to the CNN story, if they had not been with their tankers, or the weather had been bad, this would have been serious.
I have to think there’s going to come a time when wars are fought by warrior hackers, each trying to take down the other sides computers. Or there may come a day when an airliner falls out of the sky because software failed on all the redundant systems. I sure hope not.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7c4cb19',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterRecently I wrote here how Germany’s now infamous record-setting weather station in Lingen was producing readings that were 2-3°C hotter than surrounding stations, yet the German DWD weather service refused to acknowledge the station was likely producing bad data. Today they admit the station has problems and that they will be moving it to a better location.
Last year’s all-time record high is now in question.
Lingen’s heated readings
Last summer the Lingen station, located in northwest Germany near the Dutch border, smashed the country’s all-time record high when the ‘mercury’ rose to a scorching 42.6°C during a late July heat wave. The previous all-time high for Germany was a comparatively cool 40.3°C.

Lingen’s readings of late July 2019 compared to other stations in the surrounding region (July 23 – July 27).
Today, t-online.de reports that Germany DWD national weather service has now reversed and realized that something may be very wrong with the Lingen station after all, and so will relocate it and examine its recorded data.
“The weather station in Lingen: The DWD will not publish the temperatures measured here anymore – there are doubts about the data,” T-online reports. “Now it is being relocated.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moreover, the temperature measurements previously recorded at the station will be checked, and there are now doubts whether last year’s all time record will be allowed to stand.
“Astonishing temperature values”
“We don’t know yet whether the value will stand,” admitted a DWD spokesperson on Friday. “Again and again and more and more frequently astonishing temperature values” had been coming from Lingen.
The DWD experts believe the distorted values are linked to “certain weather conditions,  especially on hot summer days when there is little wind. The station in Lingen is located in a depression and is now surrounded by trees, which means the air gets stuck in place and so heats up.
The decision to stop using data from the current Lingen station and to relocate it represents a position reversal by the DWD. Earlier the DWD had told Bild daily how it planned to stick to the Lingen readings, deeming them to be of good scientific quality and that an earlier site assessment had found that the station conditions had “no serious influence on the temperature measurements” and therefore “did not contradict the WMO standards.”
The DWD now acknowledges the station has siting issues and its data are suspicious after a number meteorologists criticized the station’s poor siting.
“The quality of our measurements has the highest priority,” said Jürgen Schreiber, DWD’s Chief Technical Officer. “We have decided to no longer publish the observational data from the Lingen station, but to use them internally for scientific tests only”.
The DWD spokesman said a second sensor would be used to measure the temperature at another location in parallel. Though after two days it is still too early to make scientific statements, but already there have been noticeable deviations in the temperature measurements. Now the tests are to be carried out with meticulousness.
T-online.de confirms that construction work for a new location has begun and that by next spring there could be temperature data coming from Lingen again.
Share this...FacebookTwitter "
"
Pictures have been coming in to www.surfacestations.org from many places. This one is from Fort Morgan, Colorado’s USHCN climate station of record. Fort Morgan is in the eastern plains of Colorado, about 100 miles northeast of Denver.
In such a place, with all that open space, you’d think it would be an easy matter to place something as important as an official NOAA temperature sensor used to contribute measurements to the national climatic database in some of that open space.
No such luck. In fact, the sensor recording the wide open plains has four air conditioners near it!

But lets not forget, in keeping with current observed trends, that any weather station with air conditioning also needs close-by parking.

It’s not like there’s no other open space to put the sensor in Fort Morgan.

The pictures above, courtesy of the Pielke Research Group shows an electronic Min/Max Temperature Sensor placed near a grain elevator office. Cable length limitations on this sensor have caused hundreds of similar placements in the USHCN network where Stevenson Screens used before could be placed a good distance away from such influences.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea556c7ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
News from Antarctica: how’s the ice?
By Kalte Sonne
(German text translated/edited by P. Gosselin)
The ice in Antarctica, how is it doing? Is it melting, is it growing? In the following we wishto present the latest literature on the subject. There is a lot to report.
Fasten your seat belt, there’s a lot to cover.
Let’s start with the temperature development because along with snowfall, this is the most important control factor for Antarctic inland ice.
At NoTricksZone, Kirye shows ten coastal stations of Antarctica. None have been warming over the past 10 years. An example followws

And here’s the temperature development of the entire Antarctic according to UAH and RSS satellite measurements (from Climate4You, via NoTricksZone):



According to Clem et al. 2018, East Antarctica has cooled over the last 60 years, while West Antarctica has warmed. The authors establish a connection with the SAM ocean cycle, the Southern Annular Mode. Euan Mearns also deals with the temperature development of Antarctica during the last decades.
Increased ice
Based on height and gravity field measurements by satellite and GPS measurements on the ground, Martin-Español et al. 2017 determined an increase in ice mass in the East Antarctic and a reduction in ice mass in the (much smaller) West Antarctic for the interval 2003-2013. NASA researcher Jay Zwally also interprets an increase in the East Antarctic ice mass. However, a paper announced in mid-2018 still seems to be stuck in review…
Will the ice of East Antarctica be dragged along by the melting West Antarctic at some point in the melting vortex? No, this will not happen, Indiana University said in a press release of 2017:
New study validates East Antarctic ice sheet should remain stable even if western ice sheet melts
A new study from Indiana University-Purdue University Indianapolis validates that the central core of the East Antarctic ice sheet should remain stable even if the West Antarctic ice sheet melts. The study’s findings are significant, given that some predict the West Antarctic ice sheet could melt quickly due to global warming.
If the East Antarctic ice sheet, which is 10 times larger than the western ice sheet, melted completely, it would cause sea levels worldwide to rise almost 200 feet, according to Kathy Licht, an associate professor in the Department of Earth Sciences in the School of Science at IUPUI. Licht led a research team into the Transarctic Mountains in search of physical evidence that would verify whether a long-standing idea was still true: The East Antarctic ice sheet is stable.
The East Antarctic ice sheet has long been considered relatively stable because most of the ice sheet was thought to rest on bedrock above sea level, making it less susceptible to changes in climate. However, recent studies show widespread water beneath it and higher melt potential from impinging ocean water. The West Antarctic ice sheet is a marine-based ice sheet that is mostly grounded below sea level, which makes it much more susceptible to changes in sea level and variations in ocean temperature. „Some people have recently found that the East Antarctic ice sheet isn’t as stable as once thought, particularly near some parts of the coast,“ Licht said.
Recent studies have determined that the perimeter of the East Antarctic ice sheet is potentially more sensitive and that the ice may have retreated and advanced much more dynamically than was thought, Licht said. „We believed this was a good time to look to the interior of the ice sheet. We didn’t really know what had happened there,“ Licht said. The research team found the evidence confirming the stability of the East Antarctic ice sheet at an altitude of 6,200 feet, about 400 miles from the South Pole at the edge of what’s called the polar plateau, a flat, high surface of the ice sheet covering much of East Antarctica.
To understand how an ice sheet changes through time, a continuous historical record of those changes is needed, according to Licht. The team found layers of sediment and rocks that built up over time, recording the flow of the ice sheet and reflecting climate change. Finding that record was a challenge because glaciers moving on land tend to wipe out and cover up previous movements of the glacier, Licht said.
The big question the team wanted to answer was how sensitive the East Antarctic sheet might be to climate change. „There are models that predict that the interior of the East Antarctic ice sheet wouldn’t change very much, even if the West Antarctic ice sheet was taken away,“ Licht said. According to these models, even if the ice sheet’s perimeter retreats, its core remains stable. „It turns out that our data supports those models,“ she said. „It’s nice to have that validation.“
The team’s research findings are presented in a paper, “Middle to Late Pleistocene stability of the central East Antarctic Ice Sheet at the head of Law Glacier,” that was published today online in the journal Geology.  The research presented is in collaboration with Mike Kaplan, Gisela Winckler, Joerg Schaefer and Roseanne Schwartz at Lamont-Doherty Earth Observatory in New York.”
A Nature Editorial also dealt with the current growth of the East Antarctic ice in January 2018. Of course, the ice in this region has also been worse at times, so it continues to heat up. However, one would have to go back to the warm Pliocene (5.3-2.6 million years before today):
A history of instability
The East Antarctic ice sheet may be gaining mass in the current, warming climate. The palaeoclimate record shows, however, that it has retreated during previous episodes of prolonged warmth.
The phrase “at a glacial pace” once invoked a sense of slow and unchangeable movement, an almost imperceptible motion. But decades of remote sensing and seafloor observations have shown that glaciers and ice sheets can respond to disturbances much more dynamically than once thought. But as satellites captured the surges and retreat of Greenland’s maritime glaciers in the past decades the Antarctic ice sheets — east and west of the Trans-Antarctic mountains — were at least assumed to be stable. But this, too, turned out to be wrong. First came sediment1 and model2 evidence that the West Antarctic ice sheet collapsed during previous interglacial periods and under Pliocene warmth. Then came erosional data showing that several regions of the East Antarctic ice sheet also retreated and advanced throughout the Pliocene3. An extended record4 of ice-sheet extent from elsewhere on the East Antarctic coast now paints a more complicated picture of the sensitivity of this ice sheet to warming.”
Curiously enough, half a year later, the tide turned when a paper by Shakun et al. 2018, also in Nature, saw no major problems for the Antarctic ice in the Pliocene:
Minimal East Antarctic Ice Sheet retreat onto land during the past eight million years
The East Antarctic Ice Sheet (EAIS) is the largest potential contributor to sea-level rise. However, efforts to predict the future evolution of the EAIS are hindered by uncertainty in how it responded to past warm periods, for example, during the Pliocene epoch (5.3 to 2.6 million years ago), when atmospheric carbon dioxide concentrations were last higher than 400 parts per million. Geological evidence indicates that some marine-based portions of the EAIS and the West Antarctic Ice Sheet retreated during parts of the Pliocene1,2, but it remains unclear whether ice grounded above sea level also experienced retreat. This uncertainty persists because global sea-level estimates for the Pliocene have large uncertainties and cannot be used to rule out substantial terrestrial ice loss3, and also because direct geological evidence bearing on past ice retreat on land is lacking. Here we show that land-based sectors of the EAIS that drain into the Ross Sea have been stable throughout the past eight million years. We base this conclusion on the extremely low concentrations of cosmogenic 10Be and 26Al isotopes found in quartz sand extracted from a land-proximal marine sediment core. This sediment had been eroded from the continent, and its low levels of cosmogenic nuclides indicate that it experienced only minimal exposure to cosmic radiation, suggesting that the sediment source regions were covered in ice. These findings indicate that atmospheric warming during the past eight million years was insufficient to cause widespread or long-lasting meltback of the EAIS margin onto land. We suggest that variations in Antarctic ice volume in response to the range of global temperatures experienced over this period—up to 2–3 degrees Celsius above preindustrial temperatures4, corresponding to future scenarios involving carbon dioxide concentrations of between 400 and 500 parts per million—were instead driven mostly by the retreat of marine ice margins, in agreement with the latest models5,6.”
Also read more at cato.org.
Antarctica stable
Eight million years ago, the earth’s atmosphere had a similar CO2 content as today. Investigations now show that the Antarctic ice sheet had hardly retreated at that time. The ice is apparently more stable than expected. Click here for the press release from the National Science Foundation. You can also read an article in Popular Mechanics.
University of Edinburgh press release from 2017::
Central parts of Antarctica’s ice sheet have been stable for millions of years, from a time when conditions were considerably warmer than now, research suggests.
The study of mountains in West Antarctica will help scientists improve their predictions of how the region might respond to continuing climate change. Its findings could also show how ice loss might contribute to sea level rise.
Although the discovery demonstrates the long-term stability of some parts of Antarctica’s ice sheet, scientists remain concerned that ice at its coastline is vulnerable to rising temperatures. Researchers from the Universities of Edinburgh and Northumbria studied rocks on slopes of the Ellsworth Mountains, whose peaks protrude through the ice sheet. By mapping and analysing surface rocks — including measuring their exposure to cosmic rays — researchers calculated that the mountains have been shaped by an ice sheet over a million-year period, beginning in a climate some 20C warmer than at present.
The last time such climates existed in the mountains of Antarctica was 14 million years ago when vegetation grew in the mountains and beetles thrived. Antarctica’s climate at the time would be similar to that of modern day Patagonia or Greenland. This time marked the start of a period of cooling and the growth of a large ice sheet that extended offshore around the Antarctic continent. Glaciers have subsequently cut deep into the landscape, leaving a high-tide mark — known as a trimline — in the exposed peaks of the Ellsworth range.
The extended ice sheet cooled the oceans and atmosphere, helping form the world of today, researchers say. Their study is among the first to find evidence for this period in West Antarctica. The research, published in Earth and Planetary Science Letters, was done in collaboration with the Scottish Universities Environmental Research Centre. It was funded by the UK Natural Environment Research Council and supported by British Antarctic Survey.
Professor David Sugden, of the University of Edinburgh’s School of GeoSciences, said: „These findings help us understand how the Antarctic Ice Sheet has evolved, and to fine-tune our models and predict its future. The preservation of old rock surfaces is testimony to the stability of at least the central parts of the Antarctic Ice Sheet — but we are still very concerned over other parts of Antarctica amid climate change.“
As the ice in West Antarctica melts, it rises isostatically, which in turn stabilizes the overlying ice, found a research team from Denmark and Colorado.
Again and again there are the climate stories about the Totten Glacier in the East-Arctic Wilkesland. Gwyther et al. 2018 was able to show that the basal melting of the glacier is subject to strong natural fluctuations (press release of the NSIDC here). There is no long-term melting trend.
Melting from volcanoes
Glaciers in the western Ross Sea are also stable (Fountain et al. 2017, press release here). The rapidly melting Pine Island Glacier in West Antarctica has a hot secret that has now been revealed: Beneath the glacier lies a previously unknown volcanic heat source. University of Rhode Island press release from June 2018 (via EurekAlert!):
Researchers discover volcanic heat source under glacier


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Plays critical role in movement, melting
A researcher from the University of Rhode Island’s Graduate School of Oceanography and five other scientists have discovered an active volcanic heat source beneath the Pine Island Glacier in Antarctica. The discovery and other findings, which are critical to understanding the stability of the West Antarctic Ice Sheet, of which the Pine Island Glacier is a part, are published in the paper, „Evidence of an active volcanic heat source beneath the Pine Island Glacier,“ in the latest edition of Nature Communications.
Assistant Professor Brice Loose of Newport, a chemical oceanographer at GSO and the lead author, said the paper is based on research conducted during a major expedition in 2014 to Antarctica led by scientists from the United Kingdom. They worked aboard an icebreaker, the RRS James Clark Ross, from January to March, Antarctica’s summer. „We were looking to better understand the role of the ocean in melting the ice shelf,“ Loose said. „I was sampling the water for five different noble gases, including helium and xenon. I use these noble gases to trace ice melt as well as heat transport. Helium-3, the gas that indicates volcanism, is one of the suite of gases that we obtain from this tracing method. „We weren’t looking for volcanism, we were using these gases to trace other actions,“ he said. „When we first started seeing high concentrations of helium-3, we thought we had a cluster of bad or suspicious data.“
The West Antarctic Ice Sheet lies atop a major volcanic rift system, but there had been no evidence of current magmatic activity, the URI scientist said. The last such activity was 2,200 years ago, Loose said. And while volcanic heat can be traced to dormant volcanoes, what the scientists found at Pine Island was new. In the paper, Loose said that the volcanic rift system makes it difficult to measure heat flow to the West Antarctic Ice Sheet. „You can’t directly measure normal indicators of volcanism — heat and smoke — because the volcanic rift is below many kilometers of ice,“ Loose said
But as the team conducted its research, it found high quantities of an isotope of helium, which comes almost exclusively from mantle, Loose said. „When you find helium-3, it’s like a fingerprint for volcanism. We found that it is relatively abundant in the seawater at the Pine Island shelf. „The volcanic heat sources were found beneath the fastest moving and the fastest melting glacier in Antarctica, the Pine Island Glacier,“ Loose said. „It is losing mass the fastest.“ He said the amount of ice sliding into the ocean is measured in gigatons. A gigaton equals 1 billion metric tons.
However, Loose cautions, this does not imply that volcanism is the major source of mass loss from Pine Island. On the contrary, „there are several decades of research documenting the heat from ocean currents is destabilizing Pine Island Glacier, which in turn appears to be related to a change in the climatological winds around Antarctica,“ Loose said. Instead, this evidence of volcanism is a new factor to consider when monitoring the stability of the ice sheet.
The scientists report in the paper that „helium isotope and noble gas measurements provide geochemical evidence of sub-glacial meltwater production that is subsequently transported to the cavity of the Pine Island Ice Shelf.“ They say that heat energy released by the volcanoes and hydrothermal vents suggests that the heat source beneath Pine Island is about 25 times greater than the bulk of heat flux from an individual dormant volcano.
Professor Karen Heywood, from the University of East Anglia in Norwich, the United Kingdom, and chief scientist for the expedition, said: ‘The discovery of volcanoes beneath the Antarctic ice sheet means that there is an additional source of heat to melt the ice, lubricate its passage toward the sea, and add to the melting from warm ocean waters. It will be important to include this in our efforts to estimate whether the Antarctic ice sheet might become unstable and further increase sea level rise.’
Does that mean that global climate change is not a factor in the stability of the Pine Island Glacier? No, said Loose. ‘Climate change is causing the bulk of glacial melt that we observe, and this newly discovered source of heat is having an as-yet undetermined effect, because we do not know how this heat is distributed beneath the ice sheet.’
He said other studies have shown that melting caused by climate change is reducing the size and weight of the glacier, which reduces the pressure on the mantle, allowing greater heat from the volcanic source to escape and then warm the ocean water. ‘Predicting the rate of sea level rise is going to be a key role for science over the next 100 years, and we are doing that. We are monitoring and modeling these glaciers,’ Loose said.
The scientists conclude by writing: ‘The magnitude and the variations in the rate of the volcanic heat supplied to the Pine Island Glacier, either by internal magma migration, or by an increase in volcanism as a consequence of ice sheet thinning, may impact the future dynamics of the Pine Island Glacier, during the contemporary period of climate-driven glacial retreat.’
In addition to Heywood, Loose worked with Alberto C. Naveira Garabato, of the National Oceanography Centre at the University of Southampton, United Kingdom; Peter Schlosser of Arizona State University’s School of Earth and Space Exploration and the Lamont-Doherty Earth Observatory at Columbia University; William Jenkins of the Woods Hole Oceanographic Institution in Massachusetts; and David Vaughn of the British Antarctic Survey, Cambridge, United Kingdom.”
University of California in Santa Cruz 2015:
Study finds surprisingly high geothermal heating beneath West Antarctic Ice Sheet
UC Santa Cruz team reports first direct measurement of heat flow from deep within the Earth to the bottom of the West Antarctic ice sheet
Read more here.
Article at Spiegel.de 2017:
Researchers discover 91 volcanoes under the ice 
Surprise in Antarctica: hidden under kilometres of ice, researchers have found dozens of previously unknown volcanoes. Eruptions threaten a strong melt – sea levels could rise.”
Read more at Spiegel.de (press release from the University of Edinburgh here).
The West Antarctic Kamb Ice Stream has always puzzled the researchers because here the ice thickened, in contrast to the general melting trend in West Antarctica. What could be the cause? Another volcano, as reported by the University of Washington in 2018: University of Washington 2018:
Volcano under ice sheet suggests thickening of West Antarctic ice is short-term
A region of West Antarctica is behaving differently from most of the continent’s ice: A large patch of ice there is thickening, unlike other parts of West Antarctica that are losing ice. Whether this thickening trend will continue affects the overall amount that melting or collapsing glaciers could raise the level of the world’s oceans.
A study led by the University of Washington has discovered a new clue to this region’s behavior: A volcano under the ice sheet has left an almost 6,000-year record of the glacier’s motion. The track hidden in the middle of the ice sheet suggests that the current thickening is just a short-term feature that may not affect the glacier over the long term. It also suggests that similar clues to the past may be hiding deep inside the ice sheet itself. ‘What’s exciting about this study is that we show how the structure of the ice sheet acts as a powerful record of what has happened in the past,’ said Nicholas Holschuh, a UW postdoctoral researcher in Earth and space sciences. He is first author of the paper published Sept. 4 in The Cryosphere.
The data come from the ice above Mount Resnik, a 1.6-kilometer (mile-high) inactive volcano that currently sits under 300 meters (0.19 miles) of ice. The volcano lies just upstream of the thickening Kamb Ice Stream, part of a dynamic coastal region of ice that drains into Antarctica’s Ross Sea. Studies show Kamb Ice Stream has flowed quickly in the past but stalled more than a century ago, leaving the region’s ice to drain via the four other major ice streams, a switch that glaciologists think happens every few hundred years. Meanwhile the ice inland of Kamb Ice Stream is beginning to bulge, and it is unclear what will happen next. ‘The shutdown of Kamb Ice Stream started long before the satellite era,’ Holschuh said. ‘We need some longer-term indicators for its behavior to understand how important this shutdown is for the future of the region’s ice.’
The paper analyzes two radar surveys of the area’s ice. One was collected in 2002 by co-authors Robert Jacobel and Brian Welch, using the ice-penetrating radar system at St. Olaf College in Minnesota, and the other in 2004 by co-author Howard Conway, a UW research professor of Earth and space sciences. Conway noticed the missing layers and asked his colleagues to investigate. “It wasn’t until we had spent probably six months with this data set that we started to piece together the fact that this thing that we could see within the ice sheet was forming in response to the subglacial volcano,” Holschuh said.
The study shows that the mysterious feature originates at the ice covering Mount Resnik. The authors believe that the volcano’s height pushes the relatively thin ice sheet up so much that it changes the local wind fields, and affects depositing of snow. So as the ice sheet passes over the volcano a section missed out on a few annual layers of snow. “These missing layers are common in East Antarctica, where there is less precipitation and strong winds can strip away the surface snow,” Holschuh said. “But this is really one of the first times we’ve seen these missing layers in West Antarctica. It’s also the first time an unconformity has been used to reconstruct ice sheet motion of the past.”
Over time, the glacial record shows that this feature followed a straight path toward the sea. During the 5,700-year record, the five major coastal ice streams are thought to have sped up and slowed down several times, as water on the base lubricates the glacier’s flow and then periodically gets diverted, stalling one of the ice streams. “Despite the fact that there are all these dramatic changes at the coast, the ice flowing in the interior was not really affected,” Holschuh said.
What the feature does show is that a change occurred a few thousand years ago. Previous UW research shows rapid retreat at the edge of the ice sheet until about 3,400 years ago, part of the recovery from the most recent ice age. The volcano track also shows a thinning of the ice at about this time. “It means that the interior of the ice sheet is responding to the large-scale climate forcing from the last glacial maximum to today,” Holschuh said. “So the long-timescale climatic forcing is very consistent between the interior and the coast, but the shorter-timescale processes are really apparent in the coastal record but aren’t visible in the interior.”
Holschuh cautions that this is only a single data point and needs confirmation from other observations. He is part of an international team of Antarctic scientists looking at combining the hundreds of radar scans of Antarctic and Greenland glaciers that were originally done to measure ice thickness. Those data may also contain unique details of the glacier’s internal structure that can be used to recreate the history of the ice sheet’s motion.
“These persistent tracers of historic ice flow are probably all over the place,” Holschuh said. “The more we can tease apart the stories of past motion told by the structure of the ice sheet, the more realistic we can be in our predictions of how it will respond to future climate change.” The research was funded by the National Science Foundation and NASA. The other co-author is Knut Christianson, a UW assistant professor of Earth and space sciences.
Blown soot apparently has no influence on the Antarctic glaciers in the McMurdo dry valleys, Khan et al. 2018 (press release).
Medley & Thomas 2019 documented an increase in snowfall in the Antarctic, which benefited the ice sheet (NASA press release here). The authors establish a connection with the SAM ocean cycle, the Southern Annular Mode. The University of Colorado in Boulder, however, blames the increase in snowfall on the ozone hole (press release, paper by Lenaerts et al. 2018).
Jenkins et al. 2018 pointed to decadal cycles in the melting of the West Antarctic ice sheet at the edge of the Amundsen Sea. The relationship between melting and ocean temperature is nonlinear:
West Antarctic Ice Sheet retreat in the Amundsen Sea driven by decadal oceanic variability
Mass loss from the Amundsen Sea sector of the West Antarctic Ice Sheet has increased in recent decades, suggestive of sustained ocean forcing or an ongoing, possibly unstable, response to a past climate anomaly. Lengthening satellite records appear to be incompatible with either process, however, revealing both periodic hiatuses in acceleration and intermittent episodes of thinning. Here we use ocean temperature, salinity, dissolved-oxygen and current measurements taken from 2000 to 2016 near the Dotson Ice Shelf to determine temporal changes in net basal melting. A decadal cycle dominates the ocean record, with melt changing by a factor of about four between cool and warm extremes via a nonlinear relationship with ocean temperature. A warm phase that peaked around 2009 coincided with ice-shelf thinning and retreat of the grounding line, which re-advanced during a post-2011 cool phase. These observations demonstrate how discontinuous ice retreat is linked with ocean variability, and that the strength and timing of decadal extremes is more influential than changes in the longer-term mean state. The nonlinear response of melting to temperature change heightens the sensitivity of Amundsen Sea ice shelves to such variability, possibly explaining the vulnerability of the ice sheet in that sector, where subsurface ocean temperatures are relatively high.
And here are even more temporally variable relationships. Wang et al. 2019: reported on temporally variable relationships of the surface ice mass balance in West Antarctica with the SAM cycle and ENSO:
A New 200‐Year Spatial Reconstruction of West Antarctic Surface Mass Balance
High‐spatial resolution surface mass balance (SMB) over the West Antarctic Ice Sheet (WAIS) spanning 1800–2010 is reconstructed by means of ice core records combined with the outputs of the European Centre for Medium‐Range Weather Forecasts “Interim” reanalysis (ERA‐Interim) and the latest polar version of the Regional Atmospheric Climate Model (RACMO2.3p2). The reconstruction reveals a significant negative trend (−1.9 ± 2.2 Gt/year·per decade) in the SMB over the entire WAIS during the nineteenth century, but a statistically significant positive trend of 5.4 ± 2.9 Gt/year·per decade between 1900 and 2010, in contrast to insignificant WAIS SMB changes during the twentieth century reported earlier. At regional scales, the Antarctic Peninsula and western WAIS show opposite SMB trends, with different signs in the nineteenth and twentieth centuries. The annual resolution reconstruction allows us to examine the relationships between SMB and large‐scale atmospheric oscillations. Although SMB over the Antarctic Peninsula and western WAIS correlates significantly with the Southern Annular Mode due to the influence of the Amundsen Sea Low, and El Niño/Southern Oscillation during 1800–2010, the significant correlations are temporally unstable, associated with the phase of Southern Annular Mode, El Niño/Southern Oscillation and the Pacific decadal oscillation. In addition, the two climate modes seem to contribute little to variability in SMB over the whole WAIS on decadal‐centennial time scales. This new reconstruction also serves to identify unreliable precipitation trends in ERA‐Interim and thus has potential for assessing the skill of other reanalyses or climate models to capture precipitation trends and variability.”

Share this...FacebookTwitter "
"
This picture below comes to me via surfacestations.org volunteer Kristen Byrnes, a 15 year old budding scientist that has created a bit of a stir with her critique of Al Gore’s Inconvenient Truth. Her website,”Ponder the Maunder” also has more photos of weather stations.
It is the USHCN Climate Station of Record for Lewiston, Maine, placed at the Union Water Power Company there.

It features an air conditioner unit, a portable barbecue grill, pavement and a nearby building. No close-by parking though as we’ve seen with other stations.
It also features a curious non-standard instrument shelter, of a design I’ve not seen before. The observing height appears to be non-standard, and lower to the ground than usual.

In addition to the close by hard surfaces like concrete pavement, the shelter also is located on an up-slope. That’s a no-no according to NOAA siting specs for a good reason – hot air rises.
Ms. Byrnes found another interesting station in Eastport, Maine. Ms. Byrnes found another interesting station in Eastport, Maine. While it is not part of the USHCN climatic network it is worth looking at because it shows how something simple and obvious that was missed can skew any experiment.
This station is a state operated, NOAA funded special monitoring station with high accuracy, very expensive laboratory grade sensors. The temperature sensor is aspirated, meaning it has a powered fan to draw air in from the outside, and is considered the most accurate way to measure air temperature. The same temperature sensor is used in the US Climate Reference Network (USCRN) specs of which can be seen here and photos here.
The setup also has a portable electronics building to go with it, to house all the data logging and analysis electronics. All that electronics needs to be kept cool, so these building are fitted with an air conditioner.
But the scientists who placed the temperature sensor were apparently so transfixed on the goal, they didn’t notice the air conditioner for the electronics building:

Fortunately, the US Climate Reference Network sites I’ve seen are much better thought out than this station in Eastport Maine.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5298c60',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAn expert psychoanalyst appeared on German television to provide his view on Greta’s high-octane anger, see (German) video below:

At her speech at the UN, Greta’s voice was filled with worry and acute anger. When asked about the source of the intense emotions, German psychiatrist, psychoanalyst and book author Hans-Joachim Maaz commented:
What’s really bad about it, is the marketing, what is being made of her, how she is being, shall we say, used or misused for certain interests. […] I don’t find this is appropriate. I would surely critically ask the parents what they intend by all this, that they are tolerating this, that they are indeed promoting  Greta’s personal problem. I find this ethically problematic.”
Anger has other sources
About the pain and anger in her performance in New York that was accompanied by emotionally charged accusations that her youth and life have been destroyed by the climate situation, Maaz says: “That’s just not real. It’s not the case. She just hasn’t had any such really serious experience that would justify such anger. But she harbors such affects within herself and they certainly have a totally other source.”
The German psychiatrist adds:
That they fundamentally allow her to storm in such a direction, without taking her problems into account, without giving her help – that I find to be highly troublesome.”
Share this...FacebookTwitter "
"Gaze out from the deck of a boat and you will see an ocean that was, in Henry David Thoreau’s phrase, “equally wild and unfathomable always”. There’s a stark contrast in appearance here between the apparently rugged and pristine ocean and our landscapes, so obviously and extensively modified by humans. As well as wild, of course the oceans are vast – they cover some 70% of Earth’s surface, and with an average depth approaching 4km they make up 99% of our planet’s living space.  As for the organisms that inhabit our oceans – well, everyone knows they are phenomenally abundant, geographically widespread, and enormously fertile, right? Together, the vastness of the oceans and the exuberance of marine life makes the concept of marine extinction little more than a far-fetched idea. A decade ago, one of us (Dulvy) decided to take a fresh look at some of these assumptions, reviewing what was then known about marine extinctions, and questioning the easy contrasts often made between marine (widespread, abundant, safe) and non-marine (specialist, rare, threatened) organisms.  Now, the other one of us (Webb) has published a new study in the journal Current Biology that attempts to put on an equal footing extinction risk across both marine and non-marine environments.  We did this by summarising data compiled in the International Union for Conservation of Nature’s Red List of Threatened Species across all major taxonomic groups, separately for marine and non-marine species. This could only have been possible through a decade-long concerted effort by the union to ramp up its assessment of marine organisms.  On the face of it, our results appear to support the view that marine species are at less risk. Simple views of such statistics have previously been used to confirm expectations that extinction risk is lower in the oceans. When we dig deeper into these numbers, however, a rather different picture emerges. Our comparisons can only consider those species which have had their conservation status formally assessed on the red list. Despite 50 years of effort by tens of thousands of volunteers, during which more than 70,000 species have been assessed, we still know the status of just 3% of known marine and 4% of known non-marine species.  Across all these assessed species, extinction rates in non-marine groups are around twice those in marine groups – a much lower figure than previously thought. Digging further still, the pattern changes markedly when we focus only on the best studied groups. This generally means the more charismatic, recognisable animals such as turtles or seabirds. While there are many more species of sea snails or crustaceans out there waiting to be discovered, taxonomists are unlikely to discover many new seabirds. Naturally we expect the most robust estimate of extinction and threat rates for these best-studied groups. And of these animals on average 20-25% of species are threatened with extinction, regardless of whether they live on land or in sea. Marine extinction risk has ramped up rapidly in the past 50 years, to converge upon the level of risk seen on land. There are of course some caveats to consider. Most significantly, very few marine groups meet our criteria for being “well known”, and most of those that do are atypical. Some 90% of the 225,000 or so known sea species are invertebrates, with the diversity of fish more than matched by many less eye-catching groups. But the well-known groups are mainly vertebrates (seabirds, marine mammals, fish, turtles) or otherwise tied to our coastal seas (mangroves, seagrasses, corals). Well-known groups of land animals are similarly biased towards the charismatic and convenient but – and here is a genuine marine-terrestrial contrast – most terrestrial biodiversity is contained within fewer groups to start with. Thus, while flowering plants and insects together cover much of the diversity of life on land, life in the sea is spread more evenly across dozens of major groups. Indeed, a striking finding of this new study is that 64 of the 88 major groups of marine life, together containing almost a third of known marine species, have had no members at all assessed. Clearly the final word on marine extinction rates cannot be written until some at least of these gaps are filled in.  This task falls entirely to the volunteer scientists working in their spare time with little credit, and less funding. Securing this work is vital to ensure that we understand the true research and conservation priorities rather than that provided by the increasingly barren narrative litany of ocean calamities. Despite the lack of overall representation of the marine tree of life in the new study, one group that is included is the archetypal “extinction-proof” marine fish. Thomas Henry Huxley, Darwin’s Bulldog himself, pronounced with authoritative confidence at the 1883 Fisheries Exhibition in London “probably all the great sea fisheries, are inexhaustible; that is to say, that nothing we do seriously affects the number of the fish,” a preconception that still clings stubbornly to 21st century discourse.  The emerging evidence over the past decade shows that abundant/widespread/fertile paradigm of marine life histories is the exception rather than the rule. So are marine species safe from harm? This new work confirms that the precautionary working hypothesis is, “no more than those on land”."
"The rotting remains of a number of tigers, lions and cougars were recently discovered in a raid on a house in Prague. This disturbing find was the culmination of a five-year investigation that revealed an illegal trade in exotic wildlife blooming in the heart of Europe. Czech authorities managed to identify the main figures behind an international crime ring who had been processing and selling wild cat parts as traditional Chinese medicine. Claws, teeth, bones, skin and extracts from their bodies known as “tiger wine” or “broth” were smuggled to Asia or used to supply the domestic demand in tiger products. The slaughtered tigers came from the country’s largest private breeding facility for lions and tigers – where, officially, these protected wildcats are bred for circuses, roadside attractions and petting zoos. This story provides a stark reminder of the cruelty engendered by captive breeding. Even zoos heralded as the beacons of endangered species conservation play a controversial part in this story.  With only 3,900 left in the wild, the tiger family (Panthera tigris) is the only big cat listed as endangered, with two subspecies critically endangered. The captive population, meanwhile, is abundant.  In 2014, the WWF alerted us to the alarming news that there are “more tigers living in American backyards than in the wild”. The organisation called on the US government to introduce a ban on private ownership of big cats. No such federal bill has been passed since, but 21 states ban all dangerous exotic pets, while the rest allow certain species or require permits. Out of 5,000 captive tigers in the US alone, only 350 are held in zoos and other facilities accredited by the Association of Zoos and Aquariums. The estimated number of tigers in the Czech Republic, meanwhile, is 390, only 39 of which are kept in zoos.  A growing number of cities around the world close their gates for circuses that use wild animals. According to Czech law, captive breeding of big cats requires special permits, while the environmental inspectorate records each tiger’s birth, sale or death. Following the discovery of the tiger slaughterhouse in Prague, the European Association of Zoos and Aquariums issued a statement urging authorities to take immediate action in ensuring that all captive tigers serve noncommercial purposes such as research, education and conservation breeding. The idea of protecting endangered species through captive breeding in zoos is relatively new, but has a much longer and darker history.  Exotic animals first entered private collections in Europe as diplomatic gifts. Tigers were particularly highly priced in royal and aristocratic menageries as dangerous predators were seen to embody the political and physical prowess of their owners. Wild cats were also exhibited for popular audiences in circuses and other travelling shows. The intensive traffic in wildlife was largely facilitated by colonial expansion. That is why European port cities, as the centres for colonial commerce, were the first to open public zoos. In the aftermath of decolonisation and the introduction of the Convention on International Trade in Endangered Species in 1973, the lucrative business of capturing and trading exotic animals came to an end. Faced with the termination of a supply of specimens caught in the wild, zoological parks resorted to captive breeding.  They did so, on the one hand to ensure they retained rare species in their collections and, on the other hand, to redirect their mission: from entertainment towards conservation. Devising so-called “Species Survival Plans”, accredited zoos have collaborated since 1981 to breed endangered species and manage all captive individuals of every species as one population to ensure genetic diversity.  But even after this period, research, education and conservation did not always drive captive breeding in zoos. Even non-commercial breeding does not always prioritise animal welfare.  Many zoos, for example, are still devoted to breeding white tigers. Only two years ago the Czech Liberec Zoo celebrated the birth of two white cubs, that were transferred to Pont-Scorff Zoo in France in July this year. This rare variation of the Bengal tiger has distinctive white fur colouring with pale chocolate stripes and mesmerising blue eyes. The extraordinary coating results from a genetic mutation, which as a recessive trait is expressed only if both parents carry the mutation. This inclined the zoos to practice inbreeding, often pairing off siblings in hope for a white-furred offspring. All 250 white tigers in captivity today are related, having a common ancestor captured in 1951 – the wild-caught cub named Mohan that was the pride of Maharaja of Rewa, an Indian royalty who was determined to breed these rare wild cats. After several failed attempts, in 1957 the first white cubs were born in India from the union of Mohan and his daughter Radha.  In 1960, the Smithsonian Institution procured one of the female cubs for $10,000. Today she would be worth eight times more. While the royal ancestry of this exotic feline vividly stimulated the imagination of American zoogoers, her main task at the National Zoo was to produce more offspring of her kind. The demand for these extremely rare animals often justifies pairing off closely related tigers, even though inbred animals are prone to acquiring crippling defects including shortened legs, kidney problems and crossed eyes, as well as psychological issues.  The tigers slaughtered in the Czech Republic were not bred in zoos but in a private facility, yet their story should put captive breeding in general into question.  Today, tigers are bred outside of their natural habitats for a variety of reasons: for zoos, exhibitions, circuses performances or as pets. Tiger cubs are often displayed in petting zoos and subjected to the cruel practice of declawing. Adult tigers are drugged to pose in photos. People still see these extremely dangerous carnivores as proxies for luxury and sexiness. But hopefully attitudes are changing. In 2017, Tinder launched a campaign to encourage its users to stop posting “tiger selfies”. And most recently, due to public pressure, China was forced to reinstate a newly lifted ban on using tiger bone and rhino horn in medicine. Of course we need to pay attention to the conservation of today’s wild tigers threatened by habitat loss due to human activity, poaching, loss of prey and the swelling human-wildlife conflicts. But more attention should be paid to the plight of the enormous captive population of tigers across the world. This article was updated on November 26 to correct the stated number of captive tigers in the US."
"One of the few silver linings of the horrendous bushfires across the country has been the sight of people setting aside their differences to help one another during this time of crisis. “Natural” disasters don’t discriminate based on class, ethnicity, religion or culture: everyone in the line of these fires is equally at risk, and directly affected by the devastating outcome. The only real divide is between people living in dangerously exposed regional and rural areas, and those residing in the relative safety of our cities. Even so, the shocking levels of smoke haze blanketing Sydney, Canberra and Melbourne have meant that city-dwellers cannot avert their gaze from the impact of these massive fire fronts, so devastatingly charged by the impact of climate change on our arid land mass.  So is it too much to hope that when the fires are, at least temporarily, quelled and the smoke subsides, this sense of national unity might extend beyond the immediate crisis? Because to address the urgent threat of climate change and ensure the wellbeing of all Australians, we must stop dividing people along the faultlines of class and culture and rediscover the solidarity that has driven every major social change in our history. Unfortunately, recent comments from those within our political class point to a failure to grasp the nature of this challenge. Since the federal election in May, we have heard repeated calls for the ALP to abandon its “blue collar” base, forget about regional Australia, and throw in its lot with urban small-l liberals of the post-materialist, cosmopolitan class. This way, suggest too many commentators on the fringes of the Labor party, lies an election-winning coalition to return the ALP to federal power. To what end is unclear. First, surely a Labor party that no longer represents working people is no longer worthy of the name. For the ALP to win an election by casting off its obligation to improve the material conditions of the working class and reduce inequality would be the ultimate exercise of seeking power without purpose. More fundamentally, these analyses reveal shallow thinking about the nature of power, and a fundamental misunderstanding of both working class culture and the history of progressive political change. Real progress, both social and economic, is impossible without the working class. Incremental social progress, on issues such as marriage equality and refugee rights, may be championed by educated people of wealth and means, but meaningful action to redistribute power and ensure a fair share of our common wealth for all citizens is never realised through the benevolent deeds of those in positions of social and economic privilege. Throughout history, working people have been at the forefront of progressive change. From the chartists agitating for political reform in the UK in the mid-19th century, to the cotton mill workers in Lancashire refusing to touch slave-grown cotton from the US during Lincoln’s presidency; from Australian unionists fighting for the eight-hour day, to the blockades by the labour movement that were so instrumental in fighting apartheid; from the role of workers in the US civil rights movement in the 1960s to the active involvement of the Australian union movement in the campaign for marriage equality: real social and political change has been driven by the solidarity and mobilisation of working people. Yet today, both here and internationally, the working class communities that powered so many of these achievements are abandoning leftwing political parties and throwing in their lot with rightwing populists and demagogues. And so we hear in response that the left should accept their rejection by the people they are meant to represent, and capitulate to the divisive politics that is effectively undoing two centuries’ of social progress towards a more equal society. Working people, some seem to have decided, are too socially conservative, too selfish and too ignorant to be saved. This is breathtakingly snobbish, woefully ignorant, alarmingly defeatist, shamefully irresponsible – and politically myopic. The left’s pursuit of progressive social policies isn’t the cause of the alienation of working class communities from social democratic politics; rather, it is a loss of trust that the political left still has the will or the capacity to defend the interests of working people against the forces of extreme market capitalism that have fractured their communities and destroyed their livelihoods. The ravages of trickle-down economics have left people scared and insecure. Yet even when the economic project built on deregulation, small government and the relentless pursuit of private profit hit the wall during the GFC, the left was woefully unprepared to step in with an alternative vision to rebuild our common wealth and ensure secure livelihoods for all. A decade later, in the face of repeated defeats, it seems too many people who call themselves social democrats are still seeking short-term political fixes to win government without disrupting the power and privilege of the capital class to which so many of them now belong. The answer to building a progressive coalition to support meaningful action on climate change, address economic inequality and build a better society isn’t to talk down to people and try to outdo the far right in the culture wars by pandering to fear and division; it’s to link social progress to economic progress, and fight for a better society for all Australians. This begins with rebuilding the solidarity of working people, through the bonds of community and social action. It requires us to think beyond the tactical measures required to win an election, and to engage in the hard work of creating a movement for change – one which focuses on solutions to decarbonising our economy that create new, secure jobs for people on the frontlines of ecological and economic change; and the promise of a social contract that allows people the security and certainty to think beyond their immediate material needs and conceive collectively of the conditions required to improve the lives of their neighbours, friends and fellow citizens. This collective action of ordinary people in pursuit of a better society is the means by which social progress has been always been achieved, and it always will be. It’s time to stop navel-gazing and get on with the job. • Emma Dawson is executive director of public policy thinktank Per Capita"
"Straw is cheap, good for the environment and an excellent insulator. So why don’t we see more straw houses? Unless we suddenly stop eating bread or cereal it’ll keep being produced anyway, and the excess straw in the UK alone could build a new city each year.  The UK construction sector must reduce its energy consumption by 50% and its carbon emissions by 80% by 2050, so radical changes are needed to the way we approach building houses. Straw could be a critical part of the transition towards a low-carbon future. The thermal insulation value of a typical straw bale wall meets the requirements of even the most demanding performance specifications. Recent research led by the BRE Centre for Innovative Construction Materials at the University of Bath has shown that straw bale buildings reduce energy bills by 90% compared to conventional housing stock. The manufacture of cement, used in concrete, is responsible alone for up to 8% of all industrially produced greenhouse gas emissions. Using natural materials such as straw, often directly from the field and with little further processing, significantly reduces this impact. Traditionally, the environmental impact of construction materials has been significantly less than the impact of occupation (heating, cooling and so on) over the lifespan of the building. However, in modern energy efficient buildings the proportion attributable to that “embodied” in the fabric of the building is expected to increase to at least 90%. Measures to reduce the impact of the embodied energy and carbon will deliver even more environmentally friendly buildings. Straw is just the dried stalks of plants stripped of their grain. You don’t really “make straw” – it’s a co-product of grain production, an established and essential agricultural process. So using straw doesn’t displace land required for essential food production.  In the UK more than 7m tonnes of straw remains after the production of wheat flour, and up to half this amount is effectively discarded due to its low value – simply chopped up and returned to the soil. As an average three-bedroom house needs 7.2 tonnes of straw, the “leftover” could be used to build more than 500,000 new homes – a city the size of Birmingham could be built each year using discarded straw. Straw is also a low-cost material. But more importantly, as a plant it captures and stores atmospheric carbon dioxide during photosynthesis. By using more and more straw in buildings we are creating a natural carbon storage bank. Though the bible references using straw for bricks – and thatched roofs – have been common for centuries, modern straw construction was developed when mechanical baling machines were first used in late 19th-century Nebraska. Stacked like large bricks, straw bales can be used for modest loadbearing as well as non-loadbearing walls. The oldest surviving straw bale building is around 100 years old. But straw has never caught on as an alternative to bricks, concrete or timber. There are concerns about its poor durability, fire resistance, the way it attracts mice and rats and, as one of the three little pigs found out the hard way, its lack of structural integrity. Straw bales aren’t currently made to the same levels of tolerance and specification as bricks or cement. The fact they’re generally slightly different sizes combined with the need to keep bales dry during construction has meant most builders would not, until recently, consider straw bales a viable solution for anything. Other than perhaps for enthusiastic self-builders. However, the development of prefabricated wall panels using straw bale for insulation has now provided the opportunity to market straw to the mainstream construction industry. Prefabrication, or off-site manufacture, means that wall panels can be made to a very high specification in a factory, protected from variable weather conditions that would otherwise inhibit on-site building with straw.  A prefabricated product can be certified as fit for use by industry bodies, making it much more acceptable to builders, financiers and insurers. It also radically reduces site construction times, with houses able to be erected in ten weeks instead of around 16 weeks for more conventional buildings. It seems the time has arrived for straw bale construction. For the past ten years the University of Bath has been working with a local company, ModCell, to develop prefabricated straw bales. We started out looking at straw as a low-carbon cladding solution and soon moved on to developing panels that could bear heavy loads. Now, we are able to make low-energy prefabricated straw bale houses. The panels have been subjected to fire tests, thermal transmittance tests, accelerated weathering tests, acoustic tests, simulated flooding and impact testing. We’ve even tested the structures in a simulated hurricane force wind, in what has been termed the “big bad wolf” test: the panels and prototype BaleHaus passed with flying colours. These panels have now been granted certification. This in turn means insurers will cover straw houses and home-buyers will be able to obtain mortgages. Hayesfield School in Bath, EcoDepot in York and the School of Architecture at the University of the West of England have all made use of these panels. Certification means the housing market can now use straw too, with LILAC in Leeds completed in 2013 and now a new development in Bristol due for completion later this year, with proposals for larger schemes already in planning. Modern prefabricated straw bale houses are affordable, deliver excellent levels of energy efficiency in use for the home-owner or occupier and provide a genuine sustainable solution by using a cheap and widely available agricultural co-product. Other similar prefabricated systems using straw bale construction have been developed in Australia, Belgium and Canada. Entire communities, towns or even cities built from straw bales. And why not?"
"Arctic sea ice melts each summer, reaching its minimum extent sometime in September, before refreezing through the winter. Over the past 35 years, the September sea ice extent has reduced by about 35% overall and this decline is projected to continue as global temperatures increase. In 2007 and 2012 the summer ice extent was dramatically lower, causing some some media speculation that we would soon see a summer which was “ice-free” (meaning a year with less than 1 million km2 of sea-ice).  Most climate scientists were more cautious. The weather in 2007 and 2012 was warmer than usual and the winds were particularly favourable for melting sea ice. Although human influence on Arctic sea ice has been
detected, there was no evidence that these weather patterns would continue each year.  In contrast, 2013 and 2014 had more sea ice than 2012, causing other  speculation that a recovery was underway. Is this claim warranted? The figure below shows Arctic sea ice extent (the black line) has undergone a long-term decrease, with the dashed line representing a linear trend. But there have also been shorter periods of rapid melt, no change, and apparent increases in extent during this decline – represented below by coloured trend lines for some deliberately chosen eight year periods. The most recent eight-year period, starting from the extreme low of 2007, shows an upward trend. This does not mean that the Arctic sea ice is recovering. As with global temperature, these erratic changes are what we expect to see. Imagine a ball bouncing down a bumpy hill. Gravity will ensure that the ball will move downwards. But if the ball hits a bump at a certain angle it might move horizontally or even upwards for a time, before resuming its inevitable downward trajectory. This bouncing ball is an analogy for the changing Arctic sea ice.  The hill represents the long-term downward trend in Arctic sea ice due to increasing global temperatures and the bumps introduce changes from this smooth trajectory. These erratic bounces could be in either direction, causing an apparent acceleration or temporary reduction in melt rate. By only examining a small part of the trajectory you might conclude that the ball was moving against gravity. A longer term view would see it as a bounce. There is no expectation that sea ice, or any other aspect of the climate, will change smoothly over time. The climate system simply does not work that way. Previous studies have suggested that natural climate variations (or “bounces”) play a key role in how sea ice evolves, and suggested that some of the rapid melt in the early 2000s was a temporary acceleration.  A new study I co-authored with a team of Canadian and American scientists, published in Nature Climate Change, highlights that the recent slower melt is a temporary, but not unexpected, deceleration. The complex climate models used to make projections of future climate also exhibit similar periods of little change and more rapid change in Arctic sea ice. The recent  trends are well within the range of these expectations. We might even see a decade or more with little apparent change in sea ice. The causes of these fluctuations in melt rate are still being explored. One suggestion is that slow variations in Atlantic sea surface temperatures are involved. More observations of the Arctic ocean, atmosphere and sea ice would help answer this question. When will the Arctic be ice-free – or equivalently, when will the ball reach the bottom of the hill? The IPCC concluded it was likely that the Arctic would be reliably ice-free in September by 2050, assuming high future greenhouse gas emissions (where “reliably ice-free” means five consecutive years with less than 1 million km2 of sea ice). We expect the long-term decline in Arctic sea ice to continue as global temperatures rise. There will also be further bounces, both up and down. Individual years will be ice-free sometime in the 2020s, 2030s or 2040s, depending on both future greenhouse gas emissions and these natural fluctuations.  Even when it reaches the bottom of the hill the ball will continue to bounce. Similarly, not every future year will be ice-free in summer. But if global temperatures continue to increase the bounces will become smaller and the ice-free periods will spread from late summer into autumn and early summer.  Commercial Arctic shipping is already increasing to exploit shorter journey times from Europe to Asia, while oil, gas & mineral extraction possibilities are being explored and Arctic tourism is growing. Decisions about such activities need to assess both the risks and opportunities. The important role of natural sea ice fluctuations needs to be considered in such assessments."
"
In the UK Channel 4 produced a new documentary titled:
The Great Global Warming Swindle  This is well worth watching, especially if you’ve ever doubted the veracity of such claims, no matter which side you find yourself on. 
Through interviews with prize-winning climate experts and others, this masterful documentary explains the origins of global warming alarmism; factually addresses claims of man-made global climate change; exposes the motivations of organizations, scientists and activists sounding the alarm; and explains why it’s been extremely difficult, if not downright career killing, for scientists to question global warming orthodoxy publicly.
While presenting hard facts, it is artfully done, making it watchable for the layman and scientist alike.
You can watch the video here. Its about 75 minutes. You can press the Play button and Pause button if you need a break. If the video player below doesn’t work, here is a <a href=”http://www.youtube.com/watch?v=XttV2C6B8pU”direct link
&nbsp


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7a37040',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye 
and Pierre Gosselin
So what’s going on?
NASA and other government agencies keep telling us that the globe is warming and ice becoming more rare, yet when look out the window, things often appear to be going the opposite direction.
Rare cold, snow grip Greece
For example, the Greek Reporter here informed how a “rare spring snow” blanketed large parts of northern Greece. It reported: “Of course, Northern Greece is used to low temperatures and snow, but even for their standards, such an intense snowfall in April is rare.”
Moreover, the widely read Electroverse weather site here reported how southeast Europe had seen its “coldest April morning in a decade”, potentially causing widespread crop damage.
So why are such events happening when they aren’t supposed to be?
Altering: from cooling to warming
Today we look at the NASA data from two stations in Greece: Makedonia and Larissa, shown below:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




These two stations have data going back to 1892 and 1899 respectively.
As we know NASA has been busy over the last years adjusting and homogenizing data from stations around the world, again and again. Data which once stood for years, suddenly got altered – in many cases very substantially. Greece here is another example.
What follows are side-by-side plots of the two stations: V4 unadjusted and V4 adjusted:

The two GHCN V4 unadjusted mean annual temperature data plots of the respective stations clearly show a cooling trend.
So no wonder we are seeing “rare” snow and cold in April.
NASA’s claimed warming is statistically fabricated
But NASA altered the data for the two Greek stations and named the two new data sets “V4 adjusted”. Clearly the adjustments magically produce the warming they like to scream about.
The warming didn’t occur because the air warmed up, but rather because NASA changed the data. The warming is a fake.
Though lots of people, media and politicians are listening and believing it, the weather isn’t. Snow and cold continue to make their appearance, Greece and other places are showing.


		jQuery(document).ready(function(){
			jQuery('#dd_525a8a6412dd739f5bdb3ff44afa9033').on('change', function() {
			  jQuery('#amount_525a8a6412dd739f5bdb3ff44afa9033').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOne highly visible person often seen at Greta Thunberg’s side at public Fridays For Future appearances is German 23-year old climate activist Luisa Neubauer of Hamburg.

Climate activist Luisa Neubauer at the center of infighting between FFF organizers. Image: Andol – own work, CC BY-SA 4.0
The Green Party member was a driving force behind the launch of the Fridays For Future movement in Germany and Europe. However, it appears other FFF organizers have had enough of Neubauer constantly hogging the spotlight amid signs of growing discord and infighting among FFF organizers.
MOPO.de here reports how Neubauer has been “disqualified as permanent speaker” by FFF organizers and “will also not speak at the controversial citizens’ meeting in Berlin in June.”
Moreover, according to MOPO: “Neubauer already was replaced” by 17-year-old schoolgirl Helena Marshall at the Siemens Annual Shareholders’ Meeting on February 5.
Jet-setting Greenie “Longhaul Luisa”
The problem, MOPO writes, is that “Neubauer is too much in the public eye” and that she may not be “suitable as a representative” because it was reported how she had flown often. Neubauer was recently dubbed “Longhaul Luisa” after having posted images of herself on foreign trips in social media.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Over the last years, climate activist Luisa made it around the world by plane several times, via Austria, Switzerland, Italy, Belgium, the Netherlands, Sweden, Poland, England, Scotland, France, Canada, China, Hong Kong, Nepal, Morocco, Namibia, Tanzania, Indonesia, etc.
Once in a talk show, Luisa was asked what she personally would do to protect the climate. She answered: “Fly as little as possible”.
Open discord between Greta and Luisa
Evidence of open discord among the FFF movement organizers also surfaced in a video showing Greta Thunberg expressing her displeasure at comments made by Neubauer during the World Economic Forum earlier this year.

Strife is undeniable as Greta openly shakes her head as Neubauer speaks.
MOPO reports the “first trouble” began “already at the beginning of spring 2019” when co-organizers felt “the distribution of speeches and appearances was too one-sided.”
Greta skips France
Today Greta tweeted she was skipping appearances in the marches in Grenoble and Paris this Friday, citing “family reasons”.


		jQuery(document).ready(function(){
			jQuery('#dd_2eb4fb5288a5586c6e0d291efb22ada4').on('change', function() {
			  jQuery('#amount_2eb4fb5288a5586c6e0d291efb22ada4').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Australia’s current position as “ground zero” for both the impacts of climate change and policy uncertainty presents an opportunity for the country to emerge as a leader in responding to the climate crisis, according to Australian Research Council laureates. In a letter signed by 80 ARC laureate fellows, some of Australia’s top researchers said claims strong action to cut emissions would be economically destructive have no basis and are not “consistent with Australia’s traditional optimism and ingenuity, nor with historical experience”.  “Reducing emissions is a global challenge that requires collective action,” the letter said. “But Australia’s current visibility as ground zero for both climate impacts and climate policy uncertainty presents a unique opportunity for us to emerge as a leader on this challenge.” The ARC laureate fellows are a small group of researchers selected by the ARC as the top researchers across all fields in Australia. The letter, whose signatories include decorated academics in mathematics, science, economics, and language and culture, said the government’s focus on adapting to changed fire patterns “is not enough”. It was written as the country’s unprecedented bushfire season continues, with emergency warnings in place on Tuesday for a fire burning in the Namadgi national park near Canberra. “We welcome government actions to help current victims and improve adaptation to future fires, as well as its acceptance of a role for climate change in the catastrophe,” the letter said. “But this is not enough, because the greenhouse gas amounts driving warming are still rising: the world is only at the beginning of the climate change phenomenon.” The bushfire emergency has brought the Coalition government’s climate policies into sharp focus. The prime minister, Scott Morrison, was criticised for his handling of the crisis through Christmas and early January, and for his failure to meet with former emergency chiefs who warned of the coming catastrophe last year. Morrison said this month that the government’s response to the increasingly visible effects of the climate crisis would be to address “adaptation and resilience” rather than strengthening policies to reduce emissions. But the letter warns that without stronger action to curb emissions, the impacts of further temperature rises could be such that adaptation is not achievable. “This dire outlook demands stronger mitigation of carbon emissions,” it said. “Many argue that actions to achieve this would be economically destructive. This claim has no basis, nor is it consistent with Australia’s traditional optimism and ingenuity, nor with historical experience.” They wrote that achieving net zero emissions was a large but achievable task, and “far less risky and irresponsible” than allowing continued global heating. Australia faced international criticism as one of a handful of countries at the United Nations climate conference in Madrid in December that were responsible for thwarting a deal on the rulebook for the Paris climate agreement. Australia is the only country that plans to use carryover credits from the Kyoto period to meet its targets under the Paris agreement."
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Carbon dioxide regulations promulgated by the EPA are based upon the assumption that they will actually _do_ something about climate change in the U.S., and that the rest of the world, which had been needling the U.S. for decades of inaction, will now follow our virtuous lead.   
  
Neither is going to happen.   
  
This _Report_ is based upon just-released data from the U.S. Energy Information Administration showing that the amount of carbon dioxide emitted from the U.S in the last year was the about the same as was emitted in 1994—nearly two decades years ago. During that time, emissions grew steadily for 14 years, peaking in 2007, and then fell dramatically (Figure 1). The emissions in 2012 were 12% less than those of 2007.   






**Figure 1. U.S. annual carbon dioxide emissions, 1994-2012 (data source: U.S. Energy Information Administration).** Given this non-trivial decline in carbon dioxide emissions, let’s see how the government’s assumptions are holding up.   




**Assumption 1: Reducing U.S. carbon dioxide emissions will do something about global warming-related weather/climate impacts in the U.S.**   
  
The U.S. National Oceanic and Atmospheric Administration (NOAA) compiles the number of “Billion Dollar Weather/Climate Disasters” in the U.S. The higher this number is the more media attention they get and the more global-warming-is-making-the-weather-worse-and-we-must-immediately-act-to-stop-it furor it engenders.   
  
With U.S. emissions on the decline, so too, certainly, are the number of billion dollar weather disasters, right?   
  
Figure 2 shows the annual tally of the number of billion dollar “disasters.” Despite the rapid U.S. emissions decline for the past 5 years, the number of weather disasters in the U.S. is increasing.   






**Figure 2. Number of billion dollar weather/climate disasters in the U.S., 1994-2012 (data compiled by the U.S. National Oceanic and Atmospheric Administration).** There are two primary reasons why this is the case. The first is that the number of people and the amount of stuff that we all have continues to increase (i.e., there is more stuff in harm’s way), and the second, _there is no relationship between U.S. carbon dioxide emissions and extreme weather events in the United States_. U.S. emissions could have been _zero_ and the result would have been even worse—because rebuilding infrastructure in the extremely expensive energy economy that would ensue would cost much, much more.   
  
  
  
**Assumption 2: The rest of the world will follow our lead and reduce emissions.**   
  
As we like to point out, the magnitude of future global warming (and accompanying climate change) rests not on the future carbon dioxide emissions pathway of the U.S., but rather on that taken by the rest of the world. In the typical mid-range emissions scenario employed by the UN’s Intergovernmental Panel on Climate Change (IPCC), the amount of warming projected to occur between 1990 and 2095 is 2.8°C (5.0°F). Of this amount, only about 0.2°C (0.38°F)—about 7%—is expected to result from U.S. emissions. Big deal.   
  
Further, these forecasts are likely way too high. We’ve experienced 0.33°C of warming since 1990, or about 12% of the forecast total, even as 22% of the forecast period is has already passed.   
  
So then why is there so much focus by President Obama on regulations aimed at reducing U.S. carbon dioxide emissions when they will result in no meaningful climatic consequence?   
  
Because our emissions declines are supposed to set an example and other nations of the world will follow suit.   
  
That’s not happening, either.   
  
Figure 3 shows the carbon dioxide emissions from the rest of the world over the same time period as the U.S. emissions in Figure 1. When U.S. emissions increased 14% from 1994-2007, emissions from the rest of the world increased. When U.S. emissions then declined 12% from 2007-2012, emissions from the rest of the world increased even faster.   
  




**Figure 3. Global (less U.S.) annual carbon dioxide emissions, 1994-2012 (data source: U.S. Energy Information Administration updated through 2012 according to Peters et al., 2013).** Why? Because the emissions growth in the rest of the world is primarily being driven by China and other developing countries that are more interested in growing their economies then they are in emulating the U.S.   
  
All which has to make you wonder. As the data show that the premises upon which U.S. greenhouse gas regulations have been promulgated are wrong, why do we persist with such silly notions?   
  
**Reference:**   
  
Peters, G.P., et al., 2013. The challenge to keep global warming below 2°C. _Nature Climate Change_ , **3** , 4-6, doi:10.1038/nclimate1783.  

"
nan
"
Share this...FacebookTwitterTwo new papers use tree ring proxy evidence to suggest modern European temperatures are neither unusual nor higher than they were during the Medieval Warm Period.

Image Source: Esper et al. (2020)
Esper et al. (2020) have produced a new temperature reconstruction for Southern Europe to complement past reconstructions for Northern and Central Europe.
They find “the warmest 30-year period since 730 CE occurred during high Medieval times (876–905 CE=+0.78 °C w.r.t. 1961–1990) and has been slightly warmer than the recent period from 1985–2014 (+0.71 °C)“.
The proxy evidence and instrumental record also show there has been no obvious net warming in Southern Europe since the 1940s.
Past reconstructions for Northern and Central Europe also show no unusual warming has occurred over the last century, with as-warm or warmer temperatures during the 1940s.
Ljungqvist et al., 2020  cite tree ring temperature studies from Scandinavia, Scotland, Continental Europe, and the Pyrenees that also show the 1930s and 1940s were as-warm or warmer than recent decades.

Image Source: Ljungqvist et al., 2020
Share this...FacebookTwitter "
"During my time as a zookeeper I had the privilege of working with both Sumatran and Amur tigers. If they did not both have stripes, you would think they were different species altogether.  The Sumatran tiger is the smallest alive today. At around 100kg, it’s “only” about the weight of a large adult male human. It is suited to the warm and wet forests of the Indonesian island of Sumatra, which is reflected in its smaller size and short, dark rusty orange coat which has many thin black stripes to conceal it in dense vegetation from their prey.  The Amur – or Siberian – tiger is much larger, averaging around 170kg (though there are historic reports of males clocking in at 300kg or more) and is now found mainly one corner of far-eastern Russia. It has a thicker but relatively pale coat, with sparse dark brown stripes, which enables it to survive in freezing and snowy winters.  Tiger experts have long debated what such differences mean scientifically. Should the biggest of the big cats be divided into various subspecies, or are all tigers simply “tigers”? It’s an issue with serious implications for conservation. About 3,500 or so tigers remain in the wild, in just 7% of their former range. And if those tigers are all the same, or if even most of them are the same, then saving individual populations matters slightly less – and tigers can be moved around to assist breeding in the wild.  Traditionally, eight subspecies were considered to exist. They are the two already mentioned, plus the Bengal tiger, found mainly in India, the Indochinese, the South China tiger and then three extinct subspecies: the Bali (extinct in the 1940s) and Javan (80s), both closely related to surviving tigers on nearby Sumatra, and the Caspian tiger from Central Asia which went extinct in the 1970s. As genetic techniques evolved, a 2004 study found there was little genetic diversity among tigers, but enough to support the separation of subspecies. It also suggested that Indochinese tigers living on the Malayan peninsular were different enough to those living further north to warrant a ninth subspecies: the Malayan tiger.  These ideas were contested by a group of researchers in 2015, who argued that the relative lack of variation among the mainland Asian subspecies and large overlaps in their shape, size and ecology meant that all tigers from India to Siberia or Thailand should be considered the same subspecies. The researchers called for just two recognised subspecies: the continental tiger, and the Sunda tiger, found on the various Indonesian islands. However the various subspecies are classified, one of the consistent findings is that tigers follow Bergmann’s rule: a principle in zoology which states that animals within the same overall species will tend to be larger in colder environments and vice versa. The Amur tiger, for instance, benefits from the fact that larger animals are better at retaining heat as they have a smaller surface area relative to their overall mass.  This is where a new study published in the journal Current Biology fits in. Researchers from China and the US looked at the whole genomes of 32 representative tigers and found that there were indeed nine subspecies of tiger – of which six survive today. But their work also demonstrates that the various adaptations to temperature – Amur big and hairy, Sumatran small and sleek – were triggered by significant prehistoric events that changed global and local temperatures.  The findings confirm previous speculation that the low genetic diversity in tigers was caused by a population decline during an ice age 110,000 years ago. Thousands of years later, the earliest split from a single common ancestor species occurred between island and mainland subspecies, with the former developing a smaller body size thanks to natural selection. The super eruption of the Sumatran volcano Toba 75,000 years ago followed by an extreme cooling period was the likely cause. Further splits into more specialised tigers reflect other significant extreme climatic changes.  So why is this important in terms of tiger conservation? As past research has argued, the lack of genetic and morphological differences between mainland tigers could allow them to be managed as single subspecies. Theoretically individuals from any region, wild or captive, could be relocated to repopulate former areas or increase numbers of failing local populations. This could help to increase general tiger numbers and local genetic diversity.  But the recent study suggests that tiger adaptations may be more subtle and intricate than first appeared. If tigers are allowed to hybridise either in captive or wild populations it could drive the more vulnerable subspecies to extinction before we fully understand exactly how they have adapted to their particular area.  There is a downside to considering tigers as separate subspecies and attempting to protect them on this basis, without mixing in tigers from elsewhere. Numbers of each subspecies are very small – there are only around 500 wild Amurs, for instance – and smaller populations are more vulnerable to extinction. This could be caused by the regular threats of habitat loss and poaching or simply due to reduced genetic diversity making a small population vulnerable to disease and other selective pressures. Genetic diversity is key for adaptation and ultimately species survival. As our understanding increases, more informed decisions can be made regarding how best to conserve the tiger. We might not have enough time to solve all the riddles but perhaps this is one step closer to ensuring one of the world’s most iconic animals does not disappear forever."
"

For all those folks worried that a cell phone tower at the Elks Lodge or Hooker Oak Park is going to give rise to a legion of cancer ridden mutants, here’s another study that says “not likely”
March 26, 2007 (Reuters) —
Cell phone use does not appear to be associated with an increased risk of glioma, the most common type of brain tumor, according to a new study.
“Public concern has been expressed about the possible adverse health effects of mobile telephones, mainly related to [brain] tumors,” Dr. Anna Lahkola, a researcher at the Radiation and Nuclear Safety Authority in Helsinki, and colleagues explain in the International Journal of Cancer.
The researchers examined the relationship between mobile phone use and risk of glioma by studying 1,521 glioma patients and 3,301 controls.

The vast majority of both groups reported using cell phones. Overall, 92% of glioma patients and 94% of controls reported using mobile phones.
Overall, there was no evidence of increased glioma risk related to regular mobile phone use.
There were no significant associations observed with duration of use, years since first use, cumulative number of calls or cumulative hours of use.
No increased glioma risk was observed when analog and digital phones were analyzed separately.
There was, however, a trend toward increased risk of glioma in people who used a cell phone for more than 10 years exclusively on one side of the head, which was on the same side as the tumor. The association reached “borderline statistical significance.”
“This may be due either to chance or causal effect or information bias, i.e., overreporting of mobile phone use on the affected side by the cases with brain tumors,” the investigators wrote


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea764b045',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A £30bn British pension fund has threatened to sack investment managers that do not take action on the climate crisis, criticising the sector as “not fit for purpose”. Brunel Pension Partnership, which manages pension money for nine councils in south-west England as well as for the Environment Agency, said it would review the mandates of asset managers that don’t reduce exposure to climate risk by 2022.  The Bristol-based pension fund will demand that companies in which it invests take steps to align their emissions with targets agreed at the 2015 Paris climate summit. Brunel will vote against the reappointment of board members of companies that who are not doing enough, and could also sell its stakes from 2022 onwards. Its new policy reflects the increasing pressure on the role of the asset management sector in abetting the climate crisis. Research by InfluenceMap, a non-profit, found that the portfolios of the world’s 15 largest asset managers were overweight in polluting companies, while the largest US managers opposed the majority of climate-based shareholder votes at the companies in which they invested. Mark Mansley, Brunel’s chief investment officer, said: “Climate change is a rapidly escalating investment issue. We found that the finance sector is part of the problem, when it could and should be part of the solution for addressing climate change.” Brunel’s money is spread between more than 130 investment managers, some of whom control hundreds of billions or even trillions of pounds in assets. Brunel’s managers include Aberdeen Standard, Invesco, Legal & General Investment Management, Royal London Asset Management and Wellington Management. While many asset managers have all but ignored environmental campaigners, the prospect of asset owners such as pension funds withdrawing their lucrative business has prompted some big investors to reassess their climate policies. BlackRock, the world’s largest investment manager with more than $6.9tn (£5.3tn) in assets and another manager of Brunel’s assets, last week bowed to pressure to improve its response to the climate crisis, saying it would divest from companies producing thermal coal and increasingly offer fossil fuel-free products, among other initiatives. Brunel said it had found short-termist thinking and an unwillingness among asset managers to invest in companies pursuing low-carbon technologies, and added that risk models used by investors often relied on flawed assumptions that did not take into account climate risks. Emma Howard Boyd, chair of the Environment Agency, said: “Now is the time for everyone in the finance sector to show leadership in response to the climate emergency.”"
"
I’m surveying climate stations of record around California and documenting their condition as part of a larger project I’m doing. You’ll see more about it here in the near future.
Today I visited Marysville’s Fire Station, just off Hwy 70 at 9th and B Street, where they have the station of record for the city using the MMTS electronic sensor installed by the National Weather Service. The data from this station is part of the USHCN (US Historical Climatological Network) and is used in the computer modeling used to predict climate change.
The Marysville station is located behind the fire department building on a patio and is probably the worst site visited so far. In addition to the sensor being surrounded by asphalt and concrete, its also within 10 feet of buildings, and within 8 feet of a large metal cell tower that could be felt reflecting sunlight/heat. And worst of all, air conditioning units on the cell tower electronics buildings vent warm air within 10 feet of the sensor. Oh and lets not forget the portable BBQ the firefighters use a “couple times a week.” The area has been constantly added to, what was once a grass rear yard was turned to a parking lot, then more buildings added, then a cell tower with one, then two electronics buildings and the air conditioners…no report on how long the firefighters were BBQ’ing back there, when they figured out why I was asking all the questions they clammed up.
I can tell you with certainty, the temperature data from this station is useless. Look at the pictures to see why, and is it any wonder the trend for temperature is upward?
 



Above: Vehicles with hot radiators park within 6 feet of the temperature sensor!

Now compare Marysville to Orland, just 50 miles away, where there’s not been any significant change in the last 100 years at the measuring location. Its obvious that Marysville is measuring UHI (Urban Heat Island) effects.

So the question is, how does bad data like this slip into the NASA GISS model database?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea65b785c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAt COP 25 in Madrid, Princeton physicist and former President Trump advisor Prof. em. William Happer spoke on the “false pretenses of a climate emergency” and called the climate protection movement a “bizarre environmental cult”, “absurd” and “madness”.

Princeton University, Professor Emeritus, Dr. Will Happer Discusses the False Pretenses of a ""#ClimateEmergency""https://t.co/Xl1kn0wWcG#ClimateBrawl
— CO2 Coalition (@CO2Coalition) December 6, 2019

The distinguished professor said, “It’s too bad we are here on false pretenses, wasting our time talking about a non-existent climate emergency.”
“Bizarre environmental cult”
Some 25,000 delegates have flown into Madrid to express their panic over a perceived climate emergency and to pressure governments to take radical actions to profoundly alter human behavior.
Happer added: “I hope sooner or later enough people will recognize the phoniness of this bizarre environmental cult and bring it to an end.”
In his talk, Happer warned leading politicians against viewing combatting CO2 as a religion, and cautioned it could end up badly when millions of people become obsessed with a single delusion and become stark-raving mad at climate. He said there’s been “so much brainwashing that it’s going to be difficult to bring people back to reality.”
The distinguished Princeton professor said the focus needs to be on pollution, and not CO2, and that solar energy and wind energy blight the environment and don’t work very well.
Only very little impact on climate


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On the physics of CO2 trapping heat, Happer presents a CO2 chart and suggests that doubling the CO2 concentration in the atmosphere will have very little impact on climate and that he “can guarantee that no one who knows anything about science can dispute this curve. That’s the truth.”
“Absolute madness”
Taking action based on the curves that show CO2 has little effect, Happer says this is “absolute madness.”
The Princeton physicist also believes the climate models greatly exaggerate the warming and that the trace gas is in fact beneficial to the planet. Today atmospheric concentrations are extremely low compared to previous geological times:

The extra CO2 recently added into the atmosphere has in fact led to a greening of the planet, Happer shows.
“Phony consensus”
Next Happer called the often claimed 97% consensus among scientists “phony” and that science is determined by observation, “not votes”. “Scientific consensus is often wrong,” Happer showed.
Happer summarized that CO2’s impact has been exaggerated “by a factor of 2 to 4”, and that overall a little extra CO2 in the atmosphere is beneficial to vegetation and agriculture.
The whole CO2/climate worry “is absolutely absurd. It’s a cult,” Happer concludes.
Follow CO2 Coalition at Twitter.
Share this...FacebookTwitter "
"
In my previous post, NOAA Throws a roadblock my way I talked about how NOAA/NCDC has thrown a roadblock into the work being done to survey weather stations citing “privacy concerns” of observer’s name being included in station data being used to locate stations.
Alert blog reader Gerald Ingle passed this info on to me.
It appears that NOAA does not follow their own edicts, as they have a web page dedicated to cooperative observer newsletters and awards.

http://www.nws.noaa.gov/om/coop/2002-Awards.htm
On this web page you can find names of the observers, the station name, their PHOTOS in front of their stations, and in some cases their partial life history!
They also have a gallery of images in addition to the newsletters about COOP observers.
For example:

http://www.nws.noaa.gov/om/coop/2002/2002-15.htm

Britt, IA, Cooperative Observers Dianne and Keith Hansons show
off their 10 Year Length of Service Award.
This blows the NOAA/NCDC “privacy concerns” out of the water. They were worried about names appearing with MMS station data, well here we have names, photos, and more on NOAA’s own website.
They can’t have it both ways. Here is the link for the NOAA newsletters page.

http://www.nws.noaa.gov/om/coop/coop_newsletter.htm
Note the link where ANYBODY can sign up their email and get the newsletter chock full of names, stations, and photos of observers

Get on the free newsletter mailing list
It’s not even a confirmation email signup, just type in anybody’s email and it appears to accept it.
No confirmation email was received when I signed up, so apparently having somebody getting spammed isn’t an issue either.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5742f5f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

Last week I was informed that the MBNA credit card I’ve had for years was “acquired” by Bank of America. Ok no big deal, mergers go around all the time as big corporations get even bigger by swallowing other corporations whole.
But I was shocked to discover that my interest rate had soared. It was around 12% previously, but now, thanks to the merger and corporate greed used to pay for that merger, my interest rate was raised to: (drum roll, and please sit down while reading) 24.97%  !!  The friendly note from the “BofA customer satisfaction center: said  “I could of course pay off the balance and avoid the rate change”. Gee, thanks.
WTH? I have excellent credit, no late payments on this card, and I’ve been with BofA since 1994 when Jolene Francis signed me up. I’ve had business loans, home loans, car loans, and my savings, personal, and business checking account with BofA since then, and thanks to the same sort of corporate weasel thinking where the Bank is more important than the customer…one by one, all of these accounts where transferred to other more sensible banks when BofA announced some amazingly stupid new “plan” to improve “customer satisfaction”.
Here’s some insight into the national credit card problem by SF Chronicle columinist  David Lazarus
Now, I’d point out that an interest rate of 25% generally makes it impossible to pay off a loan if the consumer pays the minimum payment listed on the bill. So it became clear to me that BofA was financing their shiny new merger with MBNA. Despite layoffs at MBNA designed to sweeten BofA’s bottom line, they just couldn’t resist sticking consumers with the bill for their merger.
So today marks my end of my 12 year relationship with Bank of America. Hello Discover Card. Hello WaMu.
It amazes me that banks keep pulling these kind of Enronesque stunts and still keep customers. They certainly lost me, and my company business, because they simply got too greedy. The only way consumers can fight back against these sort of practices is to cancel accounts and do balance transfers to more reasonable companies. For example, Discover offered me a balance transfer at a very VERY low interest rate, a bargain compared to 25% from BofA!
From David Lazarus column I found this nugget of wisdom:
“All in all, the world of plastic is an uneven playing field. This is something that should never be far from mind as you spend the next month or so probably running up your biggest credit card bills of the year. “
Corporate mergers never seem to do anybody any good. Debt is acquired with the mergers, and workers get laid off to finance it and the customers get stuck with the bill over the long term. Customer service usually takes a nosedive. Shareholders may earn dividends, and the inner sanctum of corporate weasels that structured the deal usually make out like bandits. But the customer usually suffers at their expense.
I think on the whole, corporate mergers are bad for America, as is excessive credit card debt.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9a63721',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterNow that severe restrictions concerning mobility and social distance have been put in place and led to a shut down of a large part of the economy, climate activists claim that already we are seeing huge environmental benefits, among them: cleaner air.
One person here in Germany even tweeted that he had not seen such clean air since his childhood, and attributed it to scale-down of human activity.
But German wetteronline.de here reports the clean air central Europe has seen recently since the COVID 19 crisis began is not the result of the shutdown, but is mostly due to the current weather pattern over Europe.
Wetteronline.de reports:
The lower volume of traffic and the largely idle economy certainly has an impact on the concentration of dust and dirt in the air. However, the current weather situation is much more important. On the verge of a powerful high over the Baltic, dry and cold air is being carried from Siberia to Central Europe. It is very clear and pure. In addition, there is a gusty easterly wind, which leaves no chance for a so-called inversion. Under such an inversion the concentration of dust, soot and dirt would increase rapidly.”
Wood burning the biggest threat
Moreover, a heated debate has been unleashed by Swiss meteorologist Jörg Kachelmann, who says the biggest threat to clean air in Germany is the now increasing use of wood burning for home heating.

Das ist immer die grösste Lüge. Nichts verbrennt dreckiger als Holz mit so vielen Zusatzsubstanzen.
Siehe den #Thread hierhttps://t.co/gp9ZexphxD


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Und auch auf der Tabelle die Folgen zu sehen im Vergleich.
Jeder Holzofen ist eine Umweltkatastrophe für sich.@davidermes https://t.co/QDgtshMd9g pic.twitter.com/O6XUjboWLq
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) April 5, 2020

40 times dirtier than natural gas
According to the veteran meteorologist, “nothing burns dirtier than wood and its additives” and “every wood stove is an environmental catastrophe”. The following chart shows the fine particle emissions from various heating fuels:

Wood pellets are 40 times worse than natural gas.
With governments moving to restrict fossil heating fuels and encouraging “renewable” wood, more and more Germans are opting for wood heat. Thus climate activists have to expect the air to become dirtier – much dirtier – and not cleaner should restrictions be enacted against the fossil fuel economy.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe onslaught of paleoclimate evidence for warmer-than-now Mid-Holocene climates – when the Earth’s sea levels were meters higher than they are today –  stormed through 2019.
There were 107 scientific papers published this past year indicating today’s warmth isn’t even close to being unusual or unprecedented when compared to the climates of the last centuries to millennia.
As illustrated below, there were also 19 papers affirming today’s sea levels are among the lowest of the last ~8000 years.
This is added to the list of nearly 100 scientific papers published in the last handful of years indicating Mid-Holocene sea levels were multiple meters higher than they are today due to the much more extensive glacier and ice sheet melt occuring during these millennia.

Oliver and Terry, 2019  Thailand, +2.0 to 3.8 m higher than present
“~6000 cal yr B.P. old oysters can be found from between 3.8 ± 0.1 m to 2.5 ± 0.1 m above present day mean sea level. … Dead (fossil) oysters were collected from between 1 and 3 m above the centre of the live oyster band in a more sheltered cleft inside the notch. The oldest sample with an age of 5270–4950 cal yr B.P. was collected at an elevation of 3.01 ± 0.1 m above the apex of the notch. The ages decrease with elevation down to 920–710 cal yr B.P. at 1.03 m. … In all the sites, the 14C age of the dead oysters inside the notches increases with increasing elevation above present day MSL. Clearly, relative sea level was 2 to 3 m higher than present between 6000 and 3000 B.P. and has steadily fallen since.”


Brooke et al., 2019  Queensland (NE Australia), +1-2 m higher than present
“Indicator data for Queensland have been assessed for their accuracy and robustness by Lambeck et al. (2014), who identified a number of coastal and inner shelf island sites in the northeastern region, in which Cowley Beach is located (Fig. 1), where accurately dated in situ fossil coral, coral microatolls and sediment core samples provide robust sea-level records (Chappell, 1983; Chappell et al., 1983; Horton et al., 2007; Yu and Zhao, 2010; Zwartz, 1995; Fig. 3). Here, relative sea level reached a Holocene highstand between 6770 and 5520 yr BP approximately 1–2 m above the present level (Lewis et al., 2013; Fig. 3). Following the highstand, the data record a gradual fall in sea level to the present position (Perry and Smithers, 2011; Lambeck et al., 2014). … Local and regional records for the Holocene at far-field sites may also reflect the influence of climatic variations on sea level, such as shifts in the El Nino Southern Oscillation (ENSO), that can induce minor (<0.5 m) changes in sea level (Duke et al., 2017; Leonard et al., 2018; Sloss et al., 2018) on annual to multi-decadal, rather than millennial, timescales.”

Yamano et al., 2019 SW Japan, +1.1 to 1.2 m higher than present
“Evidence from the core samples and fossil microatolls suggests sea level reached its present position before 5100 cal yr B.P., and a relative sea-level highstand of 1.1–1.2 m above the present sea level occurred from 5100 to 3600 cal yr B.P. This was followed by a gradual fall in relative sea level. The tectonically corrected sea-level curve indicates a stable sea level after 5100 cal yr BP., with a sea-level highstand of up to 0.4 m between 5100 and 3600 cal yr B.P.”
Makwana et al., 2019 Western India, +2 to 3 m higher than present
“The BB trench site is located at an elevation of 2 m above present day msl, where it shows evidences of dominant marine processes at depth of 2 m with a horizon of clay at depth of 3.2 m. In coastal environments, clayey horizons get deposited in calmer and non turbid conditions with depth > 3 m, which explains the clay horizon at BB trench site that would have been deposited with the water level depth of 3.2 m at > 2.5 ka period.”

Loveson and Nigam, 2019 Eastern India, +4 m higher than present 
“The continuous rise in sea level ever since late Pleistocene has reached the present sea level during 6800 years 100 BP and the highest sea level of about ~4m above the present sea level is observed during 6050 BP. Since then, the sea level started fluctuating in lesser magnitudes (between +4.0m to -2.0m), responding to the cycles of global ice melting and climate thereof. … It is also observed that the magnitude of all five high stands in between 7,200 to the recent has a decreasing trend from +4m to 0m. It obviously indicates that the most of the present day coastal plains were once under the sea as evidenced by the presence of many inland leftover paleo delta signatures in the East Coast of India.”

Oliver et al., 2019  South Australia, +2 m higher than present
“Raised beach strata imaged with Ground Penetrating Radar (GPR) at Rivoli Bay suggest a sea-level highstand of +2 m above present ~3500 years ago, steadily falling and reaching the present ~1000 years ago.”
Kylander et al., 2019  Scotland, +9 m higher than present
“At present, the Laphroaig bog is edged by a dune system, but this sand source may have looked very different at the time peat accumulation started 6670 cal. a BP. A primary control on dune building is RSL. Glacial isostatic modelling, supported by radiocarbon-dated sea-level index points, show that the RSL on Islay was about 9 m higher at 6000 cal. a BP, and fell in a linear fashion to 2.2 m higher than present at 2000–1000 cal. a BP (Fig. 7C;Dawsonet al. 1998; Shennan et al. 2006a,b).”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Meeder and Harlem, 2019  Southeast Florida (USA), +1-1.3 m higher than present
“Sea level was at ca 8 m above present during the last interglacial ca 120,000 yr bp inundating the entire platform during deposition of the Miami Limestone strata (Moore, 1982) …  The marls form a leaky seal on the Everglades floor (Figure 14B) slowing water infiltration and storing water, increasing the hydroperiod and providing an environment suitable for peat deposition which started ca 4,500 yr bp (Gleason & Stone, 1994) at elevations between 1 and 1.3 m above present sea level (Wanless et al., 1994). … The historic high‐water stage occurred prior to drainage when the water stage was between 0.6 and 2  m higher than present in the study area (McVoy et al., 2011; Parker, 1975; Parker et al., 1955).”
Cuttler et al., 2019  Western Australia, +1-2 m higher than present
“Ningaloo Reef grew over the last ~8,000 years (Twiggs and Collins, 2010) with rapid reef build up ceasing ~5.8 ka BP when sea level was approximately 1 to 2 m higher than present. During this phase of development, benthic cover was dominated by reef-building corals (Collins et al., 2003; Twiggs and Collins, 2010). After this sea level highstand, reef evolution at Ningaloo was characterised as ‘detrital build-up and aggradational’ as sea level fell to present levels and the reef back-stepped (seaward) to its present location (Twiggs and Collins, 2010).”
Bondevik et al., 2019 Western Norway, +8.2 to +9 m higher than present
“We conclude that the maximum sea level of the Tapes transgression lasted 2000 years from 7600 cal yr BP and extended into the Early Neolithic, to about 5600 cal yr BP (Fig. 13), with an uncertainty of about 100 years. We estimate that the highest spring tide during the Tapes transgression maximum phase was between 8.2 and 9.0 m above the present mean sea level. … To account for additional uncertainties, we suggest that the spring tide sea level at Longva would have been 8.6 ± 0.4 m above present day mean sea level during the Tapes transgression maximum.”

Yamada et al., 2019   Japan, +1 m higher than present
“Post-glacial sea level reached about 1 m higher than today around 6000 years ago and then started to fall (Yokoyama et al., 1996). As such, a sudden appearance and increase of marine and brackish diatoms just below PL-b cannot be explained by eustatic sea-level change.”
Montaggioni et al., 2019 French Polynesia, +0.8 m higher than present
“The foundations of islets (motus), namely conglomerate platforms, started to form with deposition of patchy, rubble spreads over the upper reef-rim surfaces from ca 4,500 yr BP as sea level was about 0.80 m above its present mean level. On these platforms, islets started to accrete not before ca 2,300 yr BP, from isolated depocentres located midway between outer-reef and lagoon margins. At that time, sea level at about +0.60 m above present mean sea level was starting to slowly decrease to its present position.”
Brouwers et al., 2019  Dubai, +1.6 to 2.5 m higher than present
“During Pleistocene glaciations, global sea level was 100–120 m below the present level and resulted in most of the Arabian Gulf occurring as a dry basin (Purser 1973; Gunatilaka 1986) … Since late Pleistocene to early Holocene times, the sea level rose gradually until a maximum sea level stand 1.6– 2.5 m higher than today (Gunatilaka 1986).”
Haryono et al., 2019  Indonesia, +4.5 to 6 m higher than present
“[I]n 5000 BP, sea level increases up to +5 m from the present time; it means it was warmer than the present day. … Sealevel change started in 6,000 BP and rose to reach the highest sea level in 4,500-3,600 BP as +4.5 m above present sea level. Then moderate sea level lasted for 600-700 years until 2,200 BP reached +2.8 m. Low sea level peak occurred in 3,000 BP (+4.5 m above present sea level). Meanwhile, present sea level is lower than sea level peak during the middle period, that reached 2m above mean sea level. … Marine terrace also found in +6 m above present sea level.”

Williams et al., 2019 North Vietnam, +2 to 4 m higher than present
“A freshwater coastal marsh near the mouth of the Cam River in Northern Vietnam stands 2–3 m above mean sea level and is bordered by a coastal barrier that reaches about 6 m above mean sea level. A core from the marsh contains a 14-cm-thick sand and shell layer. The presence of abundant shell fragments suggests inland transport of littoral sediment, and the sand layer is tentatively identified as a washover deposit. The coast of the study area contains a beachrock standing above the modern beach and reaching to ∼4 m above mean sea level. A tentative explanation of this beachrock is that it represents a beach that formed during a mid-Holocene 2–3-m highstand, evidence for which has been reported from Thailand, Malaysia, Singapore, and Vietnam.”
Rivers et al., 2019  Northern Qatar, +1.6 m higher than present
“The Al Ruwais area of northern Qatar has been the site of shallow water carbonate sedimentation since the mid-Holocene. Two distinct depositional packages have been identified. Between ca 7000 and 1400 years ago, when sea-level was up to 1.6 m higher than today, a barrier/back-barrier system was active in an area immediately landward of the modern shoreline. During the same period, a laterally-continuous coral reef flourished in the open waters approximately 3 km to the north. Towards the end of this period sea-level fell to its current position, and the reefal system died, perhaps due to exposure or the influx of detrital sediment. Between 1400 and 800 years ago a new barrier island was established directly on top of the moribund reef, and the old barrier to the south was exposed to the meteoric realm. Over the past ca 800 years the new barrier has retreated landward as much as 1 km to its current position.”

Fachbereich, 2019  Antarctic Peninsula, +14.5 to 16 m higher than present
“Raised beaches along the coasts of Maxwell Bay, located at 7.5 to 4 m amsl (locally termed “6-m-beaches”), interfinger with terminal moraines of the last glacial-readvance (LGR), which occurred between 0.45 and 0.25 ka cal BP (John and Sugden, 1971; Sugden and John, 1973; Clapperton and Sugden, 1988; Yoon et al., 2004; Yoo et al., 2009; Simms et al., 2012). It is therefore likely that these beaches developed during the LGR (John and Sugden, 1971; Sugden and John, 1973; Hall 2010). Recent uplift of KGI was 0.4 mm a-1 during the last decade (Rülke et al., 2015). Average uplift during the entire Holocene, however, is 2.8 to 3 mm a-1 (Bentley et al., 2005; Fretwell et al., 2010). Fall of relative sea level on KGI accelerated during the last 500 years (Bentley et al., 2005, Hall, 2010; Watcham et al., 2011). This was most likely the result of a short-term acceleration in glacio-isostatic rebound after the LGR, with a modeled peak uplift rate of 12.5 mm a-1 between 1700 and 1840 CE (Simms et al., 2012). …  Bentley et al. (2005) show that an initial post-glacial sea-level fall was interrupted by a mid-Holocene highstand at about 14.5 to 16 m amsl from 5.8 to 3.0 ka cal BP. In contrast, data presented by Hall (2010) show a continuous sealevel fall, which becomes accelerated between 1.5 and 0.5 ka cal BP.”
Nirgi et al., 2019  Baltic Sea, +10 m higher than present (rate: +3.5 meters per century)
“Considering the elevations of the pre-Ancylus Lake palaeochannel sediments in the Pärnu site and the highest coastal landforms in the area, the water level rose at least 17.5 m at an average rate of 35 mm per year, which is 5–6 m more than proposed by earlier studies in this area (Rosentau et al., 2011; Veski et al., 2005). Similar fast transgression (40 mm/yr), about 21–22 m, has been documented inthe Blekinge area between 10.8 and 10.3 cal. ka BP (Hansson et al., 2018a). … At about 8.2–7.8 cal. Ka BP, the rising Litorina Sea flooded the palaeochannel in the Pärnu site and floodplain in Reiu at an elevation of 1–2 m b.s.l., around 7.6–7.8 cal. ka BP Rannametsa site at an elevation of 4 m a.s.l. and around 7.6–7.4 cal. ka BP Sindi BOM layer at an elevation of 7 m a.s.l. (Figure 7). The Litorina Sea reached its maximum transgressional RSL ca. 10 m a.s.l. [meters above present sea level] just after 7.6 cal. ka BP, most probably around 7.3 cal. ka BP (Veski et al., 2005), as also determined in Narva-Luga region at the south-eastern coast of Gulf of Finland (Rosentau et al., 2013). Thus, during the transgression, the sea level rose by about 14 m at an average rate of 12 mm per year.”

Rasmussen et al., 2019  Denmark, +3 m higher than present
“Full marine phase (c. 7700–3700 cal. a BP). – The appearance of a high salinity demanding fauna in this phase (several mollusc species, echinoids and Quinqueloculina seminulum) indicates a change to full marine conditions (Figs 4, 11). This marked environmental change coincides with a rapid and significant sea-level rise documented in both the Danish and the Baltic area dated to around 7600 cal. a BP (Fig. 11; Morner 1969; Christensen 1995, 1997; Yu et al. 2007; Lampe et al. 2011; Sander et al. 2015) and probably of global extent related to the so-called ‘global meltwater pulse 3’ documented in Caribbean-Atlantic coral sea-level records c. 7600 cal. a BP (Blanchon & Shaw 1995; Blanchon et al. 2002; Bird et al. 2010; Blanchon 2011a,b). Based on data from a recent study on the island of Samsø in the central Kattegat, Sander et al. (2015) estimated a relative sea-level rise of ~4.5 m between 7600 and 7200 cal. a BP. A high sea level in Aarhus Bay at this stage is supported by an almost complete absence of terrestrial plant macrofossils (Fig. 5) testifying to an increased distance between the core site and the shore. … In the period of greatly increased sedimentation (c. 7700–6300 cal. a BP), the average rate is ~2.8 mm a1 (Fig. 11). The extensive coastal erosion during this sea-level highstand period is manifested in today’s landscape in the form of numerous fossil coastal cliffs situated above present-day sea level that formed during the Mid-Holocene when the relative sea level was ~3 m higher than present along the coasts of the Aarhus Bay area (Mertz 1924). … In a study of the island of Anholt in the central part of the Kattegat, the drop in absolute sea level was estimated to 2.6 m over a 700-year period between 4300 and 3600 cal. a BP (with most of the sea-level fall taking place between 4250 and 3740 cal. aBP; Clemmensenet al. 2012).”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe Corona crisis has reduced car traffic, yet air quality has not improved. This suggests that the automobile’s role in air pollution has been vastly exaggerated. 

Image: NASA (public domain)
There are fewer cars on the road in major German cities due cities to the massive COVID-19 restrictions and “green zones”, “and yet it apparently does not look as if this will significantly improve air quality,” reports the online German Nordkurier here.
“Despite existing driving bans in large cities and the corona protection measures, nitrogen oxide pollution remains the same and is even increasing in some cases, according to the FDP in Mecklenburg-Western Pomerania.”
Environmentalists and climate activists like to claim that modern cars and industry have been polluting the air with dangerous particulate matter or nitrogen dioxide, but since restrictions were put in place 3 weeks ago, no improvement has been detected thus far.
“Less of an impact than previously assumed”
“The only conclusion that can be drawn from this is that air pollution from internal combustion engines has less of an impact than previously assumed,” said René Domke, regional chairman of the Free Democrat Party (FDP), in the north German city of Schwerin.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“He demanded that the future of internal combustion engines must be managed objectively again after these figures become known,” report Nordkurier.
Skeptics claim that there’s been way too much hysteria and baseless activism surrounding automobile traffic and the pollution they allegedly cause.
“Correlation broken, causality clearly refuted. Any further discussion about driving bans in view of these undeniable facts is completely unnecessary,” said Mr. Domke.
“Renewable” wood-burning the culprit
The real air quality problem now in Germany, critics say, is caused by the ever-increasing use of wood-burning for home heating, especially in the wintertime.
Often viewed  as a renewable source of heating energy, wood-burning increasingly has been shown to be a major cause of pollution in German cities. The leftist Guardian here reported in 2018, for example, that wood-burning has in fact been “suffocating cities” in the UK.
Since traffic has been reduced over the past weeks, air quality has  not improved, indicating that pollution from automobiles has long been over-hyped.
According to Domke, “The downright hysteria which the Deutsche Umwelthilfe and other NGOs have created  combustion engines, which at times dominated everyday political life, was and is completely unfounded.”
“In order to ensure mobility in the mainly rural Mecklenburg-Western Pomerania, we urgently need individual transport,” said Domke.


		jQuery(document).ready(function(){
			jQuery('#dd_3c88cf748d15ad302ac45b5484cb6404').on('change', function() {
			  jQuery('#amount_3c88cf748d15ad302ac45b5484cb6404').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn an interview with Punkt.Preradovic, finance Prof. Dr. Stefan Homburg of the Leibniz University of Hanover said Germany’s lockdown has “amounted to nothing”, has had no effect on the spread of the corona virus and that the spread had already slowed down below a reproduction number of 1.0 before the lockdown.
Citing data from Robert Koch Institute (RKI) 
In the interview, the prominent professor, once an adviser to former chancellor Gerhard Schröder, cited a chart from the Robert Koch Institute (RKI) that was issued on April 15th:

As the RKI chart shows, in early March the reproduction number had risen rapidly before reaching a peak on about March 10. By March 21st, the reproduction number dropped below 1.0.
“Ineffective”, “completely unnecessary”
It wasn’t until March 23 that the German government decreed a lockdown. As the chart shows, since the lockdown was enacted, the reproduction number did not change at all. It’s had no effect.
“It is not the case that the reproduction number went down after the lockdown”, Professor Homburg says. “There are two points we can draw from this: First, the lockdown was not necessary because the number was below 1, and secondly, the lockdown was not effective because the number didn’t drop afterwards.”
“Enormous economic damage”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Homburg agrees that the lockdown led to “enormous economic damage” and was “completely unnecessary”. In view of the data, Homburg does not know why the lockdown continues even today. Currently the reproduction rate stands at 0.7.
Homburg tells Preradovic that the politicians issued the lockdown in panic, came too late and thus so served no purpose. “It was not only unbelievably damaging for the economy, but also for other human factors. It’s about suicides and delayed operations.”
Panic fanned by absurd numbers
Citing the RKO numbers, Homburg also says: “There is not going to be any terrible epidemic. All the panic was fanned by the Robert Koch Institute, who said on March 20th that in the best case we will see 300,000 dead, and maybe 1.5 million dead.” Today the number is well under 5000.
One single alarmist paper
Homburg explains that the origin of the alarming death projections were adopted by the RKI from one single alarmist paper and that “it’s unbelievable that the government allowed itself to be so misled.”
Financial ruin for no reason at all
On the overall situation, Homburg comments: “It’s completely unimaginable. Huge damage is being caused. People and businesses are being financially ruined without any reason at all.”
On why it’s happening, Homburg says that German politicians, such as Angela Merkel, have gotten full cover from the media, and today enjoy high approval ratings because of the appearance of being competent crisis managers. And as long as the polls remain so, Homburg says, there’s little incentive for politicians to relax the lockdown.
Trump may wish to take a look at what’s happening in Germany, and move fast to end the destructive lockdown in USA. 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAt his rally in Hershey, Pennsylvania, President Donald Trump was at the top of his game, doing what he does best: trolling his floundering opponents. The US president is ripping his Democrat rivals like a lion mauling hyenas.
At his Hershey speech, 31:00 mark video below, Trump brings up the climate and energy issue, ridiculing wind energy and alarmist climate science just as COP25 takes place in Madrid amid declarations of a state of emergency in Europe and pledges to go carbon neutral.

“You’d have windmills all over the place if you had Crooked Hillary. They’d be knocking off birds left and right,” said Trump before a packed crowd.
President sees through global warming sham
“Darling, I wanna watch television tonight, and there’s no damn wind. What do I do?” Trump mocked wind energy before a laughing audience, who applauded enthusiastically. Trump even imitated the noisy, unreliable contraptions that litter the landscape.
“I wanna watch the election results. Darling, there’s no wind, the damn wind just isn’t blowing like it used to because of global warming, I think,” Trump mocked. “I think it’s global warming. Global warming – no more wind. No more life!”
The President then trolled the climate alarmists, sarcastically warning: “The oceans are gonna rise an eighth of an inch within the next 250 years,” said Trump, feigning breathlessness. “We’re gonna be wiped out!”
“Clean air, clean water”
The President then conveyed to the audience that the focus needs to be on clean air and clean water, and without shutting down industries and causing job losses.
Share this...FacebookTwitter "
"People in Sydney woke on the morning of Thursday November 22 to see a bright orange sky as the sun was blotted out by a dust storm. Visibility and air quality were very poor and people with heart and lung conditions were advised to stay indoors. Ambulance call-outs spiked and extra paramedics were called upon to deal with the crisis. The dust cloud originated from north-western New South Wales, where strong winds ahead of a cold front met with dry, dusty soil. This lofted tiny dust particles into the atmosphere, which were carried hundreds of kilometers towards Sydney, and then onwards over the Pacific Ocean. Any parts of the world which have an ample supply of soil dust particles, dry conditions, and strong winds are susceptible to dust storms. The “dust belt” –- a train of dust hotspots stretching from the Sahara desert across the Middle East to Central Asia, dominates global dust activity.  Once dust particles are lifted into the atmosphere, they are transported thousands of kilometres by the prevailing winds due to their small size –- around 1 to 10 microns – smaller than the width of a human hair. Over the Sahara during summertime, intense solar heating and convection allow dust to reach altitudes of up to 5 km, from where the dust particles can take up to ten days to sediment out of the atmosphere and travel as far as the Americas, degrading Caribbean air quality.  Although Australia has its fair share of dust hotspots, they are more abundant across the global dust belt and atmospheric dust loadings are much higher in these regions. However, many of the world’s largest dust hotspots are in sparsely inhabited regions such as the Sahara, so some of the worst impacts of dust storms are less frequently experienced by humans. So what can we expect from the future in terms of dust storms? Predicting their frequency, intensity and transport requires forecasts of surface wind speeds, surface conditions such as land surface type and soil moisture, and therefore also precipitation. Accurately simulating these properties over remote, sparsely observed regions such as the Sahara even in the present day is a challenge to models due to the complex nature of the interaction between these parts of the climate system. Climate models are a valuable tool for forecasting dust storms and how they will affect us in the future. It might be expected that, in a warmer future with more drought conditions, we would experience more dust storms. However, the environmental conditions that drive dust storms must be considered in combination.  For example, one study actually predicts a decrease in African dust emissions throughout the 21st century due to a slowdown of tropical circulation and therefore surface winds, which drive the dust uplift. This scenario would result in improved air quality in West Africa.  However elsewhere in North America, dust activity is predicted to increase by the end of the century over the Southern Great Plains largely due to reduced precipitation, enhanced land surface bareness and increased surface wind speed, while the opposite trend can be expected over the northern Great Plains. Globally, the trend in dust activity projected across the next century by climate models is highly variable regionally and seasonally. For Australia, climate models predict more dust activity during the southern hemisphere autumn, driven by increases in surface wind speeds. Our ability to forecast dust both in the short and long term requires an accurate understanding of the processes which drive dust uplift. Climate models need to be able to simulate surface wind speeds, precipitation, soil bareness and soil moisture, as well as the processes which drive transport in the atmosphere and allow dust particles to be carried over long distances.  Dust storms highlight how vulnerable societies are to these extreme weather events and how difficult predicting them is under a changing climate. They’re affected by soil conditions on a microscopic scale and large-scale atmospheric circulation, and so dust storms are uniquely challenging to predict and we urgently need to find out more about them if we are to adequately prepare ourselves for the future."
"It’s well known that carbon in the atmosphere is causing global warming. What is less well known, outside of scientific circles at least, is the role oceans have to play in this. Our seas contain 60 times more carbon than the atmosphere, and they can release it at sufficiently rapid rates to cause dramatic changes in the climate. In fact, as we describe in research published in Nature, CO2 released by the oceans brought about the end of the last ice age. More than 50 million cubic kilometres of ice once covered North America and Scandinavia. It melted away between approximately 19,000 and 10,000 years ago, releasing enough water to raise the sea level by about 130 metres. This came after CO2 concentrations increased by approximately 50%, from 180 to 280 parts per million between the last ice age and the current interglacial period. To explain such a pronounced increase, we have to look at the ocean. Scientists have thought for a long time that the southern sectors of the Atlantic, Indian and Pacific Oceans, a region known as the Southern Ocean, may be key to explaining the increase in atmospheric CO2.  Large volumes of deep water loaded with carbon come to the surface in this area. However, the low concentration of certain nutrients (for example iron) in surface waters limits the metabolism of planktonic organisms, which cannot fully consume all the carbon brought to the surface ocean, resulting in CO2 being “outgassed” to the atmosphere. We wanted to assess if the ocean contributed to the atmospheric CO2 increase during the last deglaciation, so it made sense to look at areas that are important today for the ocean-atmosphere exchange of carbon: the Atlantic Sector of the Southern Ocean and the Eastern Equatorial Pacific, another area where deep, cold water rises to the surface. But how can we then go back in time and check if these areas were a source of CO2 in the atmosphere? The answer is buried a few thousand meters below the surface of the oceans.  Research vessels such as the Joides Resolution are capable of drilling the sea floor to recover long sequences of sediments in which the history of the oceans is recorded. The sediments contain, among other things, fossils of tiny organisms that once lived in the upper ocean, called foraminifera. These creatures build chalky shells, and the waters they live in influence their chemical composition. After death, the shells sink to the bottom of the oceans, where they accumulate. We analysed the sediment cores and looked for the isotopic composition of the element boron present in shells that lived during particular times of interest. Boron tells us pH levels of the waters, which in turn tells us about carbon levels: a high concentration of CO2 in the waters will make them more acidic (lower pH), and vice versa. We found a link. When the glaciers of the last ice age were melting, and the atmospheric CO2 was increasing, the surface waters of the Southern Ocean and the Eastern Equatorial Pacific were also more acidic. This signalled an increased concentration of CO2 – much higher than those in the atmosphere.  This is the key finding of our research: the ocean was a source of CO2 to the atmosphere during key intervals of the last deglaciation, which explains the large increase in CO2 concentrations. It’s the next obvious question. Previous research has found that the last ice age saw much less carbon exchanged between ocean and atmosphere than we see today, mostly because the Southern Ocean was intensely stratified at the time and deep waters rarely made it to the surface. Nutrients and CO2 were accumulating in the deep Southern Ocean, due to the decay of the organic matter that was being produced in the surface ocean and transported to the abyss.  During the deglaciation, the effective communication between deep and upper ocean was re-established, and this carbon “reservoir” was leaked to the atmosphere. Since the beginning of the industrial revolution the oceans have absorbed an estimated 155 billion tonnes of carbon, about 30% of the total human emissions. The present atmospheric CO2 concentrations, approximately 400 parts per million, have not been seen on Earth since the Pliocene, around 3 million years ago, and the rate of increase is unprecedented in the period of on-off glaciers we have had since. Humanity is performing a large scale experiment with the Earth, and the consequences are already being seen in the form of increased atmospheric and oceanic temperatures, raising sea levels and ocean acidification, to name a few. How the oceanic uptake of CO2 is going to operate in the future remains unknown, but studies like ours advance our understanding of how the ocean works to store and release carbon on timescales of millennia and that therefore are way beyond the reach of the instrumental record."
nan
"
Share this...FacebookTwitterMarch mean temperatures over Northern Europe showed an overall cooling trend and so with it a later start to spring, the data from the Japanese Meteorological Agency (JMA) show. 
By Kirye
and Pierre Gosselin
Europe saw a very mild winter – one of the mildest on record – and so people living here believe it’s warming and that every year spring is arriving earlier.
Yet mean the temperature data from the Japan Meteorological Agency (JMA) going back 2 decades and more tell us the opposite is in fact the case in northern Europe and elsewhere: March has been cooling.
First we look at the mean March temperatures at 14 stations across the United Kingdom.

Data Source: JMA
Most stations plotted above show a cooling trend for the month of March. Obviously CO2 is not the driving factor, rather likely it has to do with the North Atlantic oceanic cycles.
The story is the same across the Netherlands when we look at data going back to the time the Kyoto Protocol was adopted, in 1997.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: JMA.
The northern European country of Finland shows no climate change, at least in march, over the past 30 years:

Data source: JMA. 
Moving to the Scandinavian country of Sweden, the home of teen activist Greta Thunberg, we see a trend that is more in line with United Kingdom, possibly being influenced by the North Atlantic cycles as well:

Data source: JMA. 
The six Swedish stations examined show cooling in March over the past 30 years.
Why everyone is still speaking about warming even though the planet is showing more a mixed bag remains a mystery. It probably has a lot to do with the global warming obsessed media.
Summary: Northern Europe early Spring (March) is not getting warmer, and is in fact cooling modestly, when we look at the untampered data from the JMA.
Plotted in this post were the stations for which the JMA had complete or almost complete data sets.


		jQuery(document).ready(function(){
			jQuery('#dd_1a2510940116542717cdb96031995e99').on('change', function() {
			  jQuery('#amount_1a2510940116542717cdb96031995e99').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"In part three of the BBC’s new nature series Dynasties, the protagonists, Charm and Sienna, show us how hard it is to be a successful lioness in a land filled with enemies.  Under constant threat of marauding hyenas and cub-killing male lions, the two mothers have to fight for their lives to ensure their offspring have a chance of making it to adulthood. But the episode also shows us that the biggest enemy of lions isn’t other wild predators – it’s humans. Down from as many as 200,000 lions a century ago, some experts believe that we could now have as few as 20,000 individuals remaining in the wild – and that number is likely to be falling by the day. Worryingly, the general public are mostly unaware of their precarious conservation status. We have done a bad job of showing the perilous state of these big cats. Lions face attack by humans on many fronts. Panthera, a wild cat conservation organisation, believes the most serious causes for their decline include habitat loss, humans killing them to protect their livestock, wild prey depletion, accidental snaring, poorly managed trophy hunting and the illegal wildlife trade. Since their threats are so varied, there is no single solution for protecting lions and overcoming these threats will be no mean feat. It will require locally-tailored solutions that fit each specific context. For instance, for lions that reside alongside people in areas outside national parks, research has shown that it is absolutely vital to reduce the perceived costs of lions to local people, like livestock depredation, while increasing their benefits, such as income from photographic tourism or trophy hunting. For lions inside protected areas, some experts argue that we must fence lions in to stop them causing problems with people. However, this has earned criticism from others, who believe that fences incur significant ecological and economic costs by disrupting the migration of herbivores. The issue over “to fence or not to fence” has turned into a bit of cat fight and shows the political nuances and ecological complexities of conserving such a charismatic species. In a bold attempt to reunite conservationists, Pride, the Lion Conservation Alliance, has brought together five lion NGOs to pool their efforts and share funding. It may come as no surprise that, like the species they’re fighting to conserve, they have realised the benefits of coming together and working as a team rather than competing.  Focusing on lion populations in Kenya, Mozambique, Tanzania and Zambia, their community conservation efforts empower locals to be stewards of wildlife. By turning lion poachers into guardians, their initiatives have reduced lion killing by up to 99% in some of the areas in which they work.  By building on the cultural significance of lion hunts, young warriors that would usually show their bravery by killing lions are now employed to track lions and monitor their activities. They also inform their community if lions are approaching so that farmers can guard their livestock. While TV shows such as Dynasties are helping to raise the profile of this threatened carnivore, what the lion needs now more than anything is funding. Conserving lions is an expensive business: one recent paper showed that to effectively manage the protected areas where lions currently reside would require a whopping US$0.9 billion to US$2.1 billion in additional income per year – on top of the money that is already raised.  Where this cash comes from remains a bit of a mystery. We have to go beyond financing conservation from the meagre income of photographic tourism in national parks. Solutions could involve more corporate partnerships and financially linking lion lovers in the West to Africans living with lions.  An idea from Sir David Attenborough himself argues that companies that use lions in their marketing should pay for lion conservation. What is abundantly clear is that if we want lions to have a future, we must start stumping up the cash for their conservation. Many commentators have suggested BBC’s Dynasties takes on the gripping, conflict-ridden format of storytelling that Game of Thrones perfected. If this is the case, humans would surely play the vicious and selfish King Joffrey. It is us, after all, who terrorise lions the most. But it is us, too, who have the power to guarantee their survival."
"
About two weeks ago I published this story about the loony idea that was proposed by some researcher in Europe about “cell phone radiation may be killing bees”. I pointed out that it was garbage then, as it is now. Here’s a portion of the original post I made:


There’s an article on UK’s The Independent website about a most unusual scientific theory: “Cell Phones kill bees.”

Well today in the LA Times,  it seems that UC San Francisco researchers have uncovered what they believe to be the real cause, and its not loony ideas like cell phones. Its fungus.
From the article:
A fungus that caused widespread loss of bee colonies in Europe and Asia may be playing a crucial role in the mysterious phenomenon known as Colony Collapse Disorder that is wiping out bees across the United States, UC San Francisco researchers said Wednesday.
Researchers have been struggling for months to explain the disorder, and the new findings provide the first solid evidence pointing to a potential cause.
Other researchers said Wednesday that they too had found the fungus, a single-celled parasite called Nosema ceranae, in affected hives from around the country — as well as in some hives where bees had survived. Those researchers have also found two other fungi and half a dozen viruses in the dead bees. 
The researchers caution that the results are preliminary, and data sampling represents just a fraction of hives, but they are encouraged by the findings. Hopefully they’ll be able to come up with a solution.
Yet it appears that the “Cell Phones kill bees” lunacy has caught on, since there’s a comment today in the ER’s “Tell it to the ER” that furthers that nutball idea. What a public disservice that column is.
Thanks to Lon Glazner for the tip.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6c06c18',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Irregardless of our political situation at home, our troops overseas and their families, always deserve the best support we can provide. For the past two years, this event gave comfort to many local families whose loved ones are serving overseas in the cause of freedom. Thanks to a great outpouring of community support, it was hugely successful. This yearâs event promises to be even better.
While the sacrifice of the men and women in our nationâs armed forces is clear, what many in our community donât know is that families of our servicemen and service women suffer even beyond the separation and the worry. Many families lose their breadwinner, and often take a large pay cut in service to our country. Sometimes it takes months for military pay to even be received as training, locations, and deployments change, leaving families to subsist on savings, borrowings, and kindness. Some families find themselves behind in rent, or leaving important bills unpaid while their loved ones selflessly serve our country.
Proceeds of this event will replenish an assistance fund used to help local military families in their time of need.
It also will provide entertainment, moral support, and inspiration. Most importantly it will provide cheer and personal contact via an Internet Video Conferencing System that will allow our troops stationed overseas to see, hear, and interact with their loved ones left behind.
On behalf of our local military families, and the members of the Chico National Guard, I ask for your support.
You can support our troops 3 ways:
1. Buy tickets to the event
2. Make a donation of goods and services for the silent auction
3. Make a cash donation.
Details are below.  Thank you for your consideration. Anthony

Support Our Troops
A Red White and Blue Christmasâ¦The 3rd Annual Northern California Holiday Fundraiser to assist the Chico National Guard Families
â¢ Emceed by Bruce Sessions, KPAY Radio
â¢ Tri Tip dinner with the Soldiers families, catered by Larry Juanarena (Pat and Larryâs Steak House)
â¢ Fundraising Silent Auction
â¢ Guest Speakers – Entertainment, and more.
When and Where:
Chico Elks Lodge, 1705 Manzanita Ave. Chico Tuesday December 19th 2006 6PM -11PM
Tickets are $30 per person
Tickets are available at:
â¢ California Water Service 222 Whitman Ave. Chico (Next to Costco)
â¢ Wittmeier Auto Center, Forest Ave next to Wal-Mart in Chico
â¢ Diamond W. Western Wear 181 E. 2nd, Street, Chico
â¢ Coldwell Banker, 7030 Skyway, Paradise
Donations can be made out to âChico National Guard Family Assistanceâ? and dropped off at California Water Service Company 222 Whitman Ave Chico or at Tri Counties Bank, All Chico Locations.
If you have a donation for the silent auction, they’ll be happy to come by and pick it up. Email Sgt. Douglas Shaw at:
Chicotroops@yahoo.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea98de6e2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn recent decades there have been “notable cooling trends” throughout many regions of the globe according to several new studies.
A year ago NoTricksZone (NTZ) announced Greenland Has Been Cooling In Recent Years – 26 Of Its 47 Largest Glaciers Now Stable Or Gaining Ice.
Six months ago NTZ cited several scientific papers indicating The Region From 50-70°S Has Cooled Since The 1980s As North Atlantic SSTs Have Cooled 1°C Since 2004.
Three months ago we reported A Massive Cooling Of 2°C In 8 Years (2008-2016) Has Jolted Large Regions Of The North Atlantic.
A few days ago we shared a New Study Finds The Larsen Ice Shelf (Antarctic Peninsula) Has Cooled More Than 2°C Since 1991.
Now we shine the light on 3 more studies that assess “Eurasia, North America, Africa, Australia, South America, and Greenland experienced notable cooling trends” from 2002 to 2013 (Xu et al., 2020), and both West and East Antarctica have been rapidly cooling since the mid-2000s (Hrbáček and Uxa, 2020 and Fatras et al., 2020).
At some point the question may need to be asked: Just how global is recent “global warming”?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Xu et al., 2020
“Concurrent with the slowdown of global warming during 2002–2013, the wintertime land surface air temperatures over Eurasia, North America, Africa, Australia, South America, and Greenland experienced notable cooling trends. … The slowdown concurs with a negative phase of the Pacific Decadal Oscillation (PDO), indicating that PDO plays an important role in modulating the global warming signal. Not all ensemble members capture the cooling trends over the continents, suggesting additional contribution from internal atmospheric variability.”

Image Source: Xu et al., 2020
Hrbáček and Uxa, 2020
“A significant air temperature decrease started around 2000 along most of the Western AP. The cooling triggered by natural variability of cyclonic activity and increasing sea‐ice concentrations near coastlines caused MAAT trends of −0.16 to 0.05°C y−1 in the period 2006–2015. In contrast, the MAAT on JRI was increasing at a non‐significant rate of 0.10°C y−1, which corresponds to observations from other sites of the north‐eastern AP where positive, but non‐significant, trends between 0.02 and 0.08°C y−1 have been reported.10 Unlike MAAT , there was a non‐significant negative trend of −0.05°C y−1 for MAGT 5. Interestingly, the MSAT and MSGT 5 trends were positive only in autumn (MAM), at 0.30 and 0.13°C y−1, respectively, while they were negative in the other three seasons. Yet, the north‐eastern AP region exhibited a MAAT more than 1°C lower in the period 2006–2015 compared to 1996–2005, and autumn (MAM) air temperature was even about 1.5°C lower.”

Image Source: Hrbáček and Uxa, 2020
Fatras et al., 2020
“The mean annual Sea Surface Temperature (SST) variations from ECMWF ERA interim database are displayed on Fig. 4b. They present no particular trend for the 1979–2018 period, with variations contained between -1°C and +0.15°C. Nevertheless, the mean temperature between 1979 and 2000 is -0.45°C and decreases to -0.64°C during the 2000–2015 period.”

Image Source: Fatras et al., 2020
Share this...FacebookTwitter "
"This article is part of The Conversation’s worldwide series on the Future of Nuclear. You can read the rest of the series here. Nuclear power has had a makeover. What was once seen as a futuristic source of limitless energy has been reframed as a response to global warming, an ideal solution for countries looking for a continuous source of low-carbon power. But who are these countries? At the moment 31 different nations operate nuclear power plants (see page 14 here, with a total of 388 reactors, and before Fukushima, most planned nuclear power plant projects were in Asia and Eastern Europe, extending a trend from earlier years. Industry lobbyists the World Nuclear Association suggests that nuclear power capacity worldwide is increasing steadily, with more than 60 reactors under construction in 13 countries. They say that eight countries are either planning to build for the first time (Belarus and United Arab Emirates), have signed contracts (Lithuania and Turkey), or have some plans to build (Bangladesh, Jordan, Poland, and Vietnam). In contrast, the more independent World Nuclear Industry Status Report describes a declining trend, with annual nuclear electricity generation reaching a maximum of 266 GW in 2006 and dropping to 235 GW in 2013 – with 50 fewer operating reactors than the peak in 2002, and total installed capacity comparable to levels last seen two decades ago. This decline is also confirmed in BP’s recent Energy Outlook. In terms of new build, 67 reactors are under construction worldwide with a total capacity of 64 GW. For the nuclear industry this at first sounds promising, but then “under construction” doesn’t necessarily mean it will be finished any time soon – work first began on one reactor opened in Argentina last year back in 1981. It’s true to say that the risk to people, the environment and to the future of nuclear energy from another major incident is still very real, and reactor accidents from “beyond design-base” cascading events, such as the Fukushima disaster and all other major nuclear accidents, are the single largest financial risk – far outweighing the combined effect of market, credit, construction and operational risks. The thing is, in trying to “design out” these accidents, reactors have become much more expensive, complex, and hence, difficult to build on time and on budget.   Of the 67 currently being built, eight reactors have been under construction for more than 20 years, another for 12 years; and at least 49 have significant delays. For the remaining 18 reactor units, either construction began within the past five years or the reactors haven’t reached projected start-up dates, with construction projects in Finland and France very many years behind schedule. Average construction times are increasing: The Russian state nuclear corporation Rosatom is building in Russia, China, and Belarus, and claims more than 20 export reactor orders in Iran, Turkey, Vietnam, Bangladesh, Jordan, Hungary, Finland, Egypt, India and South Africa. But there are questions about whether it’s got the finances and supply chain resources to carry out more than a small fraction of these – most depend on Russian finance, hit hard by the recent downturn, and Rosatom is already facing delays in its homeland due to lack of resources. Meanwhile, the World Nuclear Status Report shows that China has 28 reactors under construction – 42% of the world’s total new-build - with 21 reactors (17 GW) in operation, which in 2013 provided 2.1% of the country’s electricity. If all their reactors under construction come online before 2020, this would bring the total to 49 reactors. To put this into perspective, in 2013 alone, China installed 12 GW of solar, a threefold increase over 2012. And recent events have challenged China’s plans for nuclear. There’s been the usual construction delays, cost increases, doubts over the siting of reactors in provinces inland, and questions over safety and regulatory oversight  – and, remarkably, just last month, significant faults were found in the reactor pressure vessels already installed in the Areva EPRs at Taishan 1 and 2. While nuclear carries very real technical and regulatory risk – construction cost represents a key challenge. New builds will only go ahead after government guarantees public subsidies, including long-term power purchase agreements. This is because the private sector can’t afford to build new nuclear plants themselves. The reality is that nuclear new builds are high-value, high-risk projects with a marked tendency for significant delay and delay claims, cost growth and investor risk. For example, in Finland, their nuclear corporation TVO is pressing a €2.7 billion compensation claim for delays to the Olkiluoto EPR nuclear power plant. Perhaps amusingly, the French nuclear corporation Areva is in turn demanding €3.5 billion from TVO. The project’s turn-key price was €3 billion in 2005 and the current estimated price stands at €8.5 billion, with a construction time of 13 years and rising. And just recently, TVO has dashed Areva hopes of building any more EPRs in Finland. So the general post-Fukushima situation in the EU implies there will be limited construction in the coming decade. Although new builds are still planned in Finland, France and the UK - Italy and Switzerland have cancelled plans for new reactors, Belgium has confirmed a nuclear phase-out, and eight EU countries have signed a declaration that nuclear power is incompatible with the concept of sustainable development. At the heart of the nuclear question are differing views on value for money, foresight and responsibility. Huge long-term investments are needed and it’s clear there are critical social, environmental and economic decisions to be made. Germany, Europe’s dominant electricity user, has made its choice. Its decision to phase out nuclear power by 2022 and to instead invest in renewables, efficiency measures, grid infrastructure and energy storage, will prove significant for both European and international energy policy."
"“Snowmageddon” was predicted – three feet of snow, blizzards whipped up by high winds, a freeze of the whole transport system. What New York got was “snowperbole”.  Yes, it snowed, but not as badly as predicted, and many people have been left wondering why the city was effectively shut down for what was, in New York terms, a light dusting. So what happened? The blame game began immediately. Some meteorologists have already put their hands up. “My deepest apologies to many key decision makers and so many members of the general public” said the National Weather Service’s Gary Szatkowski, on Twitter. The politicians have defended their actions, with New York mayor Bill de Blasio saying the city shutdown was the sensible choice, given the potential for damage and loss of life. He said “we made the decision, better safe than sorry.” Is this a “forecast bust” or a case of expectations exceeding capabilities? Clearly people are not satisfied with the system. But is there anything that can be done to reduce such problems in the future? The answer, of course, is complicated. If you want to study how society could better respond to extreme weather, you need the help of political scientists, behavioural psychologists, media analysts, communication designers, town planners, engineers – and perhaps even some meteorologists. We will need time and in-depth research to assess precisely what happened in this event, but, on the face of it, the National Weather Service forecast was actually pretty good. Yes, they were predicting a chance of very high snowfall, which failed to materialise so dramatically over New York City. But Long Island and other parts of the north-east were badly hit, as predicted. Snow depth is one of the hardest aspects of weather to forecast, and the old saying of “too cold to snow” has some truth behind it. There can be a very fine line between conditions warm and moist enough to bring lots of snow, those which bring only some, and those which mean rain.  It’s also very hard to see in advance just how extreme the level of precipitation might be. We can very often say that the conditions will be right to cause a lot of rain or snow, but knowing whether a storm system will bring three inches of rain or five – or a foot of snow or three – is much more difficult. But this critical in terms of planned response. Five inches of rain might cause dramatic flooding, whereas two inches is just a normal shower. In New York, three feet of snow might cause snowmageddon. Less than a foot? Well, that’s just snow. Weather forecasting has made enormous advances. A forecast of the quality and accuracy provided by the National Weather Service this week would have been unthinkable 20 years ago. But as we have advanced we have realised more and more that we can only think of forecasts in terms of probabilities. The essence of extreme events is that, by definition, they are rare at a given location; conditions have to combine in just the right way to give us the worst case scenario. Otherwise, we just get “severe but typical”, for which we’re usually quite well prepared. Advanced predictions of the most extreme winds or rain will always see significant uncertainty. The paradox of extreme events is that it is impossible to judge the accuracy of a forecast from a one off; we need enormous amounts of computer power not just to make the forecast, but to evaluate and improve the reliability of the forecast by studying past events. Some have called for New York to be given a special high-powered supercomputer to help make more accurate predictions in future.  More processing power might well have helped in this case, but computers are not the answer to everything. Improved observations are just as important – larger numbers of measurements, provided with greater accuracy, from weather stations, radars, satellites, aircraft and elsewhere. But we also need a greater understanding of how best to use these observations and, perhaps first and foremost, a deeper understanding of the weather processes themselves. This is the “science behind the weather” that keeps me busy. It is important to realise that there is still lots about how weather works that we just don’t understand. Even where we do have a good understanding, we need to simplify things so that our computers are capable of processing all the information we have in time to make a useful forecast.  Some computer programs are so complex they take days or even weeks to model a single cloud. That’s not much use to forecasters who have to make a prediction now, about tomorrow’s weather, for a whole city, state or country. Extreme weather warnings are crucial and can save lives, but it’s important that neither scientists nor politicians promise things they can’t deliver. In a way, forecasters are victims of their own success, by raising people’s expectations of their abilities.  Any ambitious forecast system will have false alarms. Imagine, for example, an extreme storm that might occur only once in a hundred years – about 0.0027% chance on any given day or 0.01% if we restrict ourselves to winter months. Forecasting a 10% probability of this storm would be an incredibly impressive achievement. However nine times out of ten such a forecast will be perceived as “wrong” (though it may be, on its own terms, a perfect forecast).  While false alarms might be expensive, and lead to the danger that people don’t pay heed to the next one (“crying wolf”), it’s better to be safe than sorry. Response must be weighed against both the severity of the event and its likelihood. Authorities must learn how to make the most objective and rational decisions (which they may well have done in this case) while the public need to understand that “we acted on the best information available” does not mean “we got it wrong”."
"
I’ve been involved in meteorology in one way or another since 1976, and while I knew of the vast number of COOP stations around the USA, I never knew that a good number of them are at sewage treatment plants until I started my surfacestations.org project. It seems to me, that given the physical makeup of these facilities, they are one of the worst possible environments to measure air temperature. But like many historical stations, they weren’t chosen with the environment in mind, but rather if there was a human being present 7 days a week whom could take the high/low temps and rainfall and write it down on an NCDC B44 form.
This week I visited a few stations in southern California, and Santa Barbara is one of those USHCN stations that is also a sewage treatment plant. Conicidentally, a few other USHCN stations that are also WWTP’s were posted by www.surfacestations.org volunteers. So I thought I’d give you the grand tour.

Above: aerial view of Santa Barbara WWTP and USHCN climate station of record

Above: Placement of Santa Barbara’s MMTS Temperature Sensor – looking NW

Here’s one from Tifton, GA taken by Joel McDade:

more pictures here
Cheraw, SC taken by L. Nettles:

more pictures here
Albany, GA from Joel McDade:

more pictures here
Zumbota, MN from Don Kostuch

more pictures here
And let’s not forget Urbana, OH, by Steve Tiemeir

more pictures here
There’s lots more, but you get the idea.
surfacestations.org volunteer Don Kostuch wrote this to me about WWTP’s recently:
“I spoke with the curator in New Hampton IA. He gave me these figures for his plant last January:
780,000 gal/day
Incoming temp 55F
Outgoing temp 43F
I calculate this heat loss is about 3 million btu/hr.
The population is about 3500 so each person releases about 1000 btu/hr at the plant on a cold day.
The effect on the sensor depends on the placement, temperature, wind, location of the tanks, etc. which I have not attempted to analyze, but it seems to be worth some careful attention.
The worst example I saw was in Winnebago, MN where the  sensor is above and in the middle of four large tanks all huddled together in about a 100
ft square. The population there is about 1500 so the heat released would be about 1.5 million btu/hr in an area of about 10000 sq.ft.”
And, as population grows in a city so would waste water volume. So it stands to reason the a temperature sensor at a WWTP would be directly sensing waste heat produced by population growth, and the amount of waste heat would grow proportionately with population.
Perhaps we should call the WWTP effect “P-UHI”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4fe67e5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In an historic event next week, Pope Francis will make his first visit to the United States. It is expected to generate as much political interest as it will religious concerns. On Thursday, he will address a joint session of Congress, and on Friday he will speak to the United Nations General Assembly. He is widely expected to focus on climate change, a topic on which he shares much political ground with President Obama.



This will not be the first time Pope Francis has ventured into the global warming debate. The June 2015 release of his encyclical “Laudato si” marked his initial foray into the discussion. Therein, Pope Francis echoed President Obama’s tune, claiming there exists “solid scientific consensus” that human activities are causing a “disturbing warming” of the climate, which left unchecked will result in a type of planetary Armageddon manifested by escalating temperatures, melting polar ice caps, rising seas, more frequent and more severe weather, ecosystem degradation, and plant and animal extinctions, all of which he claimed will severely affect humanity.



Given that this was the pope’s stated position on global warming a mere three months ago, look for a familiar refrain to accompany his remarks in Washington and New York next week. He will likely repeat a challenge first issued in his June encyclical, which called for humanity to “recognize the need for changes of lifestyle, production and consumption, in order to combat this warming,” which he believes is “aggravated by a model of development based on the intensive use of fossil fuels.”





Contrary to what Pope Francis says, fossil fuels are good for the poor and the Earth.



 **The Consensus Isn’t**



But are the pope’s concerns over potential global warming based upon the best available science? Or are they significantly overinflated? Is the biosphere rapidly spiraling downward toward planetary Armageddon? Or is it marching forward toward biospheric rejuvenation? Is limiting fossil fuel use a policy prescription panacea? Or is it a recipe for social and economic disorder and regress?



With respect to the science, those who promulgate a fear of planetary Armageddon often conveniently fail to disclose that literally _thousands_ of scientific studies have produced findings that run counter to their view of Earth’s climatic future. As just one example, and a damning one at that, _all_ of the computer models upon which this vision is based failed to predict the current plateau in global temperature that has continued for nearly two decades now. That the Earth has not warmed significantly during this period, despite an 8 percent increase in atmospheric CO2, is a major indictment of the models’ credibility in predicting future climate, as well as the assertion that debate on this topic is “settled.”



Numerous other problems with the apocalyptic vision of our future climate have been filling the pages of peer‐​reviewed science journals for many years now, evidenced most forcefully by the work of the Nongovernmental International Panel on Climate Change, which has highlighted the results of thousands of scientific studies challenging the alarmist and model‐​based vision of the planet’s future. This large and well‐​substantiated alternative viewpoint contends that rising atmospheric CO2 emissions will have a much smaller, if not _negligible_ , impact on future climate, while generating several biospheric _benefits_.



 **Global Warming Could Be Good**



Concerning such benefits, it is a well‐​established _fact_ that atmospheric CO2 is the major building block of nearly all life, as it is used by plants in the process of photosynthesis to construct their tissues and grow. As numerous scientific studies have conclusively demonstrated, the more CO2 there is in the air, the better plants grow. They produce greater amounts of biomass, become more efficient in using water, and are better able to cope with environmental stresses such as pollution, drought, salinity, and high temperatures.



The implications of these benefits to society are enormous. One study, for example, calculated that over the 50‐​year period of 1961 to 2010, the direct monetary benefits atmospheric CO2 enrichment conferred on global crop production amounted to a staggering $3.2 trillion. Projecting this positive externality forward in time reveals it will likely bestow an additional $9.8 trillion in crop production benefits between now and 2050.



By ignoring these realities, policy prescriptions calling for a reduction in fossil fuel use are found—on this basis alone—to be ill‐​advised. Yet there are still other important reasons to reject them.



 **The World Needs More Energy, Not Less**



We live in a time when approximately half the global population experiences some sort of limitation in accessing the energy they need for the most basic of human needs, including the production of clean water, warmth, and light. One‐​third of those thus impacted are children. An even greater portion finds its ranks among the poor. How can a society turn its back on these individuals and deny them the right to increase their energy and fossil fuel use so that they can increase their living standards? It is reprehensible to even consider such an action and it is certainly morally wrong to do so. The world needs _more_ energy, not less.



Taxing or regulating CO2 emissions is an unnecessary and detrimental policy option that should be shunned. Why would any government or religious institution advocate to increase regulations and raise energy prices based on flawed computer projections of climate change that will never come to pass? Why would any government or religious institution advance policy that seeks to destroy jobs, rather than to promote them? Why would any government or religious institution want to deny increasing energy access to those in the world who are in most need of it? And why would national governments or religious institutions actually “bite the hand that feeds them”?



It is high time for our world leaders to recognize and embrace the truth. Contrary to misguided assertions, political correctness, and even government or religious edicts, carbon dioxide is _not_ a _pollutant_. Its increasing concentration only minimally affects our climate, while offering great benefits to the biosphere. Efforts to regulate and reduce CO2 emissions will economically burden society and yield little to no measurable impact on Earth’s climate.
"
"Hydraulic fracturing, or fracking, for shale gas has been a major concern for people in the UK since the first site to open, at Preese Hall, Lancashire, caused two earthquakes in 2011 that were strong enough for humans to feel. With local magnitudes (ML) of 2.3 and 1.5, these earthquakes were subsequently linked to the reactivation of a previously unknown natural geological fault.  Fracking was temporarily banned but after a seven-year hiatus it resumed in October 2018 at a new site at Preston New Road in Lancashire. Three days later the British Geological Survey began to detect earthquakes related to the site on their local seismic network.    Fracking works by fracturing underground shale rock, which creates pathways along which trapped gas in the shale can be extracted. Because earthquakes occur when rock breaks, fracking is, by design, intended to bring about the very process which results in earthquakes. However, the depth of fracking operations, about 2km underground at the Preston New Road site, mean that they are too small to be felt by humans. On the other hand, larger earthquakes with magnitudes of ML 0.5 or greater probably indicate that natural pre-existing geological faults have been reactivated, releasing stress already stored in the shale. The result can be earthquakes large enough to be felt by humans.  To reduce the likelihood of these felt earthquakes, the UK uses a regulatory traffic light system, which requires injection to be temporarily suspended if an earthquake with ML 0.5 or larger occurs. So far, six of the 36 earthquakes induced at Preston New Road have triggered this system, with operations resuming after an 18-hour monitoring period and well integrity check. The well has now produced its first natural gas.  Only the largest earthquake in the sequence, an ML 1.1 event on October 29, was felt by people, but it cannot be ruled out that more will occur during the operation.  The shale formations that are currently targeted by fracking in England are highly (naturally) faulted, and induced earthquakes used to be relatively common as a result of coal mining. The new challenge, however, is working out how stressed these faults are and how common human-felt earthquakes might be. Good seismic monitoring of initial fracking operations will provide important data to help answer this.  There also needs to be consideration given to the current traffic light system. At what magnitude above ML 0.5 should fracking sites be required to close permanently? Alternatively, should the current stop-start cycle at Preston New Road be allowed to repeat indefinitely providing the well maintains integrity and earthquakes are not damaging or causing nuisance to local residents?  Furthermore, is a traffic light system based on magnitude really suitable? Magnitude characterises the size of an earthquake but is not a measure of ground shaking, which is the effect felt by humans at the surface. An ML 0.8 earthquake on October 26 produced far less ground shaking than a lorry typically produces as it drives past. The ground shaking was also well below the regulatory limit for quarrying operations. A traffic light system based on the intensity of ground shaking may therefore be more appropriate for fracking. At the end of the day it is those who may be affected by the earthquakes whose opinion should carry the most weight. It remains to be seen if local residents are willing to tolerate induced earthquakes in exchange for local economic benefits. Co-operation of stakeholders (including local residents) and research by independent organisations such as the Researching Fracking (ReFINE) consortium and the British Geological Survey will be required to tackle these questions. The answers will play a key role in determining whether UK shale gas has a future – and how it may contribute to a sustainable, affordable, and secure energy future for the nation."
"Three bushfire survivors have joined environment group Friends of the Earth in a claim against ANZ, accusing it of financing the climate crisis by funding fossil fuel projects. The case, lodged under international guidelines agreed by members of the Organisation for Economic Co-operation and Development (OECD), demands the bank disclose its greenhouse gas emissions, including “scope three” emissions resulting from its business lending and investment portfolio, and set ambitious targets that align with the Paris climate agreement. The claim was inspired by a successful complaint against ING bank in the Netherlands by Friends of the Earth, Oxfam and Greenpeace. Mediation following that complaint led to ING committing to measure and publish its indirect emissions, reduce its thermal coal exposure to near zero by 2025 and make its portfolio consistent with the Paris goal of keeping global heating well below 2C above pre-industrial levels. The complainants in the ANZ claim include Jack Egan, who was approached by Friends of the Earth to join the action after his home near Batemans Bay, on the New South Wales coast, was destroyed on New Year’s Eve. Egan said there was a clear link between ANZ and other institution’s ongoing support of fossil fuels and the extreme hot and dry conditions that exacerbated the fire that left him homeless. “We are not seeking damages or compensation from ANZ, I just want them to stop fuelling dangerous climate change,” he said. Friends of the Earth announced the action at a protest outside ANZ headquarters in Melbourne’s Docklands. It lodged the claim with the federal government’s OECD national contact point, a section of the federal treasury responsible for hearing complaints of corporate wrongdoing under the OECD guidelines for multinational enterprises. The national contact point’s initial role is to attempt to broker a mediation between the parties. If agreement cannot be reached, it can make recommendations, but cannot force parties to take action. ANZ declined to comment on Thursday. The environment group alleges ANZ’s breaches of the OECD guidelines include misleading consumers by claiming to support the Paris agreement targets while continuing to invest in projects that undermine the meeting of those targets. Emila Nazari, a legal officer with Friends of the Earth, said the bank had increased investment in coal 34% over the past two years as it lent $8.8bn to the fossil fuel sector. She said the bank was Australia’s largest financier of fossil fuel industries, and continued to invest billions of dollars in “climate wrecking projects” while bushfires raged across Australia. “It is illegal for someone to light a bushfire, and we believe it is illegal for companies to finance the burning of our common home. This case is one of many to come against climate criminals,” Nazari said. The other names attached to the action are Joanna Dodds, a Bega Valley Shire councillor and member of the group Bushfire Survivors for Climate Action, and Patrick Simons, a Friends of the Earth renewable energy campaigner whose family lost their home in NSW."
nan
"Often when the topic of glaciers and climate change is discussed, focus shifts to those in Greenland and Antarctica. But there are glaciers elsewhere too, such as in the Himalayas, which play a vital part in supplying water to people who live downstream. Now, our research has found that these glaciers may react more sensitively to predicted future climate change than previously thought, which could lead to them melting at a faster rate. In 2017 and 2018, our EverDrill research team travelled to Nepal, to measure ice temperatures (using a converted pressure washer) on the Khumbu Glacier. Khumbu, whose ice is sourced in the Western Cwm of Mount Everest, flows down the flanks of the mountain from around 7,000 to 4,900 metres above sea level. Along with many other glaciers in the Everest region and across High Mountain Asia, the meltwater from Khumbu contributes to the water resources of huge populations in the mountain foothills.  We spent two field seasons (six to eight weeks each) drilling boreholes using a jet of hot, pressurised water to incise into the ice. In total we drilled 27 boreholes, ranging between 1-192m deep, across five sites. These boreholes are the first, deepest, and most spatially extensive achieved to date in the Himalaya using hot-water drilling. Once the boreholes were drilled, we measured ice temperatures by installing strings of pre-built thermistor sensors linked to data-loggers located at the surface. We left them for six months and collected the data on a return trip in November 2017.  The main finding from this research was that the ice was warmer than we expected, with the coldest ice measuring –3.3°C. As the ice is formed on the flanks of Mount Everest, where the mean annual air temperature is –13°C at 7,000 metres elevation, we might have expected the ice to be at this temperature. Our borehole data reveal that this was not the case. Not only was the ice not this cold in our boreholes, but ice temperatures also increased towards the glacier terminus. Why does it matter that we found warmer ice temperatures than we expected? Glaciologists acknowledge two thermal types of ice: “Cold” ice and “warm” ice. Cold ice is below the melting-point temperature, so when additional heat is applied (from the sun or warmer air temperatures), the ice simply becomes less cold (going from –15°C to –14°C, for example). Warm ice, alternatively, is at the melting-point temperature, so any heat input melts the ice to become water. In short, this means that Khumbu will respond more sensitively to any future additional heat inputs, such as warming air temperatures. It has been shown that air temperature increases are amplified at high elevations. For example, a global temperature rise of +1.5°C has been predicted to result in a +2.1°C warming across High Mountain Asia and a significant loss of ice mass across the region. As these glaciers melt and recede, their contribution to water resources will initially increase. However, as the volume of ice mass remaining decreases, this contribution will steadily decline. While the timescale over which this might occur is unknown, predictions suggest that “peak water” may be reached as soon as the middle of this century. Our finding of warm ice within Khumbu supports the predicted sensitivity of such glaciers to warming air temperatures. But it’s not all bad news for the region’s glaciers. First, we don’t know if the temperatures we measured on Khumbu are representative of all glaciers in the area. More measurements are needed on other glaciers to determine this. Second, Khumbu and many other Himalayan glaciers are debris-covered glaciers, which contain a surface layer of rocks and boulders that typically increases in thickness towards the terminus, up to several metres depth. This debris layer complicates the amount of ice surface melt that is produced: where the layer is thick, it acts like a blanket and insulates the glacier from warmer air temperatures, reducing melt rates. However, this insulation of the lower glacier also results in the location of maximum melt shifting further up the glacier, to where the debris cover is thinner. In this area, the glacier melts by surface lowering, which in an extreme future scenario could lead to the detachment of the lower glacier, forming a new terminus at this higher elevation. While the lower, detached ice would become stagnant, it would still be protected from instant melting by its debris blanket. The new terminus above this would not, and melt rates could increase. Yet, our deepest borehole in this area of surface lowering was 192 metres, and did not reach the bed so the ice thicknesses remains unknown – but it could be that there is plenty of ice left in Khumbu to stop this happening. These ice temperatures will now be fed into an ice flow model to better predict how the Khumbu Glacier will respond to climate warming and contribute to river discharges in the future. Meanwhile, we are still collecting temperature data to analyse and better understand how the highest glaciers in the world will be affected by climate change."
"
I’m sitting in a presentation by William R. Cotton, of Colorado State University where he’s talking about the effect of Urban Heat Islands (UHI) on precipitation. He’s making a convincing pitch showing how the UHI factors into downwind delayed convection initiated by the city UHI along with a significant contribution of aerosols and ice nuclei that seed the precipitation. He’s been able to demonstrate that in St. Louis, downwind from the city (typically NE to SE based on prevailing winds) there are increased precipitation from thunderstorms by as much as 160% during the life cycle of the storm.
Yesterday, I saw a very similar study done by Indiana State Climatologist, Dev Nyogi, where he studied Indianapolis, IN and came to similar conclusions. The midwestern cities make good case studies because they are singular islands of urbanization (as opposed to sprawling cities like Los Angeles and Chicago) that essentially become point heat sources at the mesoscale level.
The summary is this: Urban and-use has the biggest control on locations and amounts of precipitation and that condensation nuclei added by the city also have a significant effect. Heat and particles contributed by the city can make bigger, more precipitating thunderstorms.
Of course studies by Parker tells us there is no significant UHI effect, so this presents yet another challenge to what is looking ore and more like a flawed study by Parker.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4780e8a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterRoland Tichy’s site here reports that although car traffic in the German city of Stuttgart has decreased significantly due to COVID-19, the drop in NO2 content has only been “slight”.
 So it cannot be the evil diesel engine cars that are “choking” our cities.

Image: NASA here (public domain) 
“The Corona crisis is bringing it to light: car traffic has decreased significantly, but the air quality in city centers has hardly changed,” Tichy comments.
Environmental claim exposed as false
Recall that a major reduction of diesel engines was supposed to improve air quality. After all, experts at the Baden-Württemberg State Institute for the Environment (LUBW), for example, have attributed a large 80 percent share of air pollutants especially to diesel vehicles – so they have to be banned soon.
Yet Tichy notes: “If this is true, the ‘shutdown’ would have to have a drastic effect. But it does not.”
It turns out that the involuntary “corona experiment” with its widespread stop of car traffic has exposed bare the false claim made by environmental activists: Diesel cars are responsible for polluting the air of German cities.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Diesel car bans “pointless”
According to Tichy, the “Corona Experiment” exposes just how pointless driving bans issued by the green transport ministers can be. “They obviously have no effect on the NO2 concentrations in the air.”
“The measured values, for example, at the Am Neckartor station in the Stuttgart city center were already below the limit value of 40 µg/m3 in February and March, ” writes Tichy. “At that time, traffic was still flowing and ‘shutdown’ had not yet been announced.”
Other larger factors
Tichy adds that engineer and measurement expert Martin Schraag accuses proponents of car bans of data “manipulation” and reminds that today’s “newer vehicles and those retrofitted with software updates hardly emit any exhaust gases. This should also have been reflected in the results.”
Schraag notes that NO2 values fluctuate strongly and depend heavily on Stuttgart’s weather conditions and wintertime  heating can be the cause.
“The weather, if you look at the data, has a decisive influence,” Tichy comments. “The experts of the LUBW environment office obviously did not care about these influences and officially know nothing about them. They still assume that traffic accounts for 80 percent of air pollutants.”
At a loss for words
Now they are facing difficulties in explaining the situation. As Schraag suspects, the 80 percent figure cannot be correct if there are significantly fewer cars on the road and yet the values have not changed.
The Bavarian State Office for the Environment also confirmed that the air pollutants in the city of Würzburg – hometown of former NBA superstar Dirk Nowitzski – had hardly changed either although traffic has decreased significantly.


		jQuery(document).ready(function(){
			jQuery('#dd_b0ff816632eacdd59517875595db342e').on('change', function() {
			  jQuery('#amount_b0ff816632eacdd59517875595db342e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterFour reconstructions from the central and western High Arctic reveal July temperatures were about 1-2°C warmer than today during most of the 1st millennium and Medieval period (Tamo and Gajewski, 2019).
A few years ago, a chironomid reconstruction of Boothia Peninsula in the Canadian Arctic (Fortin and Gajewski, 2016) revealed not only were today’s temperatures the coldest of the last 7000 years, but the last 150 years “do not indicate a warming during this time.”

Image Source: Fortin and Gajewski, 2016
The Canadian Arctic’s Baffin Island had 5°C warmer summer temperatures between 10,000 and 8000 years ago (Ilyashuk et al., 2011). Somehow the polar bears managed to survive this sea-ice-free period.

Image Source: Ilyashuk et al., 2011
Earlier this year, another Canadian Arctic reconstruction (Bajolle et al., 2019) was published indicating temperatures were about 2-4.5°C warmer than today throughout the last 8500 years. Only 3 records mark temperatures colder than they are now.

Image Source: Bajolle et al., 2019
Another new reconstruction of Arctic Canada temperatures cites 4 records indicating the Medieval Climate Anomoly (MCA) was about 1-2°C warmer than today (Tamo and Gajewski, 2019).

Image Source: Tamo and Gajewski, 2019
Share this...FacebookTwitter "
"

The first Earth Day, in 1970, was celebrated after a wave of environmentalism swept the nation. Many give credit to Rachel Carson’s 1962 book, _Silent Spring_ , which popularized the notion of large‐​scale chemical pollution, for igniting the movement.



But she was really feeding off of a concept developed a few years earlier. The “precautionary principle” was conceptualized when the National Academy of Sciences proposed a radical change in the risk assessment of exposure to radiation and carcinogens. It recommended changing the regulatory paradigm from a “threshold dose” model to a linear one.



The threshold paradigm was what one might call common sense. It held that humans could tolerate small doses of things that, in larger doses, could be harmful.



Sunlight is a perfect example. Low doses are actually required for survival, as ultraviolet radiation — the same general type that causes sunburn — catalyzes the formation of Vitamin D. But, as is obvious to anyone who lives in a sun‐​drenched area, excessive exposure can lead to death in the near term (from dehydration) or the longer term (from skin cancer).



The “linear model” assumes that just a single molecule of a carcinogen or a single ionization from an X‐​ray can induce cancer. The enthusiasm spawned by Earth Day soon gave us brand new regulatory agencies such as the Environmental Protection Agency and the Occupational Safety and Health Administration. The EPA routinely applies the linear model to carcinogens.





Environmental regulations based on the “linear model” are having a negative impact, not only on societal costs, but on our health as well.



The linear model is a case study in the unintended consequences of the desire to do good. In this case, an ideologically driven scientist, Nobel Prize laureate Herman Muller, whose research formed the basis for EPA’s model, led the charge. A very powerful figure in health physics, he is now known to have marginalized and obstructed the publication of any research that provided evidence counter to the linear model.



If that sounds like the way senior climate scientists were found to behave in the famous 2009 “Climate‐​gate” emails, it should.



The regulatory agencies fell in line, as did a compliant scientific community and a media that was afraid to dig deeper. Every country followed the U.S.’ lead.



The linear model is rigid, absolute and wrong. We now know that there are so many flaws or holes in the linear dose response model that it looks more like Swiss cheese. The resulting environmental regulations are having a negative impact, not only on societal costs, but on our health as well.



Over the past several decades, considerable research has revealed a plethora of life‐​saving adaptive processes that can be used to enhance the quality of life and to extend life. Our cells are flexible, adaptive and can actually be strengthened via low‐​level exposure to a large number of compounds that the EPA would like to regulate down to the last molecule.



Instead of preventing harm, the precautionary principle actually causes harm. The entire therapeutic model is built around the notion that certain compounds that are highly toxic in large doses can be life‐​enhancing and life‐​extending in low ones.



How can the regulatory community accept the linear model when so many of its senior practitioners are living lives that prove the opposite? Many of these aging regulators are taking ACE (angiotensin converting enzyme) inhibitors to control blood pressure. The original ACE inhibitor, Captopril, is the active substance in the venom of the Brazilian viper. A lot will kill you very quickly. A little could extend your life for decades.



We need a new Earth Day. It should be dedicated to righting the past deceptions and correcting the ongoing errors in environmental regulation. It should be one that acknowledges our adaptive responses to what, in high doses, can cause cancer, but, in low doses, can improve our well‐​being.
"
"

Multiple news sites are reporting that 
levels of the second most important greenhouse gas, methane, have stabilized.
From Scientific American: ""During the two decades of measurements, methane
underwent double-digit growth as a constituent of our atmosphere, rising from
1,520 parts per billion by volume (ppbv) in 1978 to 1,767 ppbv in 1998. But the
most recent measurements have revealed that methane levels are barely rising
anymore — and it is unclear why.""
From NewScientist: ""Although tis is good news,  it does not mean that methane levels will not rise again, and that carbon dioxide remains the 800-pound gorilla of climate change.""
Actually, NewScientist is wrong. CO2 is not the biggest ""gorilla"" of
greenhouse gas on planet earth. It’s water vapor. Our earth would be much colder without water vapor in the atmosphere…it would be much like Mars.
So many of the climate models focus solely on CO2, but they leave out water vapor in the equations, or assume its ""static"".
CO2 is far from being the most potent greenhouse gas. Chloroflourocarbons
(CFC’s) commonly used as refrigerants as far worse at trapping infra-red in our
atmosphere.
Of naturally created GHG’s, Methane is 23 times more effective at warming the
atmosphere than CO2. Nitrous Oxide is even worse at 296. So far no emergency
legislation has been authored to eliminate the effect of cows or dental
surgeons. The Kyoto treaty does not address these other gases either.
Global Warming Potentials 
(100 Year Time Horizon) 
GAS GWP 
========================
Carbon dioxide (CO2) 1 
Methane (CH4) 23 
Nitrous oxide (N2O) 296 
Hydrofluorocarbons 
HFC-23 12,000 
HFC-125 3,400 
HFC-134a 1,300 
HFC-143a 4,300 
HFC-152a 120 
HFC-227ea 3,500 
HFC-43-10mee 1,500 
Fully Fluorinated Gases 
SF6 22,200 
CF4 5,700 
C2F6 11,900 
C4F10 8,600 
C6F14 9,000 
The concept of the global warming potential (GWP) was developed to compare the
ability of each greenhouse gas to trap heat in the atmosphere relative to
another gas. In this case, CO2 is the reference gas. Methane, for example, has a
GWP of 23 over a 100-year period. This means that on a kilogram for kilogram
basis, methane is 23 times more potent than CO2 over a 100-year period.
The interesting thing here is that this stabilization of methane levels in
our atmosphere happened all by itself, and the scientists are clearly baffled as
to an explanation. As I’ve always said, the earth’s atmosphere is such a complex
system, that pinning its change on just one thing is not good science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9ceefb1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"The outer layer of the Earth, the solid crust we walk on, is made up of broken pieces, much like the shell of a broken egg. These pieces, the tectontic plates, move around the planet at speeds of a few centimetres per year. Every so often they come together and combine into a supercontinent, which remains for a few hundred million years before breaking up. The plates then disperse or scatter and move away from each other, until they eventually – after another 400-600 million years – come back together again.  The last supercontinent, Pangea, formed around 310 million years ago, and started breaking up around 180 million years ago. It has been suggested that the next supercontinent will form in 200-250 million years, so we are currently about halfway through the scattered phase of the current supercontinent cycle. The question is: how will the next supercontinent form, and why? There are four fundamental scenarios for the formation of the next supercontinent: Novopangea, Pangea Ultima, Aurica and Amasia. How each forms depends on different scenarios but ultimately are linked to how Pangea separated, and how the world’s continents are still moving today.  The breakup of Pangea led to the formation of the Atlantic ocean, which is still opening and getting wider today. Consequently, the Pacific ocean is closing and getting narrower. The Pacific is home to a ring of subduction zones along its edges (the “ring of fire”), where ocean floor is brought down, or subducted, under continental plates and into the Earth’s interior. There, the old ocean floor is recycled and can go into volcanic plumes. The Atlantic, by contrast, has a large ocean ridge producing new ocean plate, but is only home to two subduction zones: the Lesser Antilles Arc in the Caribbean and the Scotia Arc between South America and Antarctica.  If we assume that present day conditions persist, so that the Atlantic continues to open and the Pacific keeps closing, we have a scenario where the next supercontinent forms in the antipodes of Pangea. The Americas would collide with the northward drifting Antarctica, and then into the already collided Africa-Eurasia. The supercontinent that would then form has been named Novopangea, or Novopangaea.  The Atlantic opening may, however, slow down and actually start closing in the future. The two small arcs of subduction in the Atlantic could potentially spread all along the east coasts of the Americas, leading to a reforming of Pangea as the Americas, Europe and Africa are brought back together into a supercontinent called Pangea Ultima. This new supercontinent would be surrounded by a super Pacific Ocean. However, if the Atlantic was to develop new subduction zones – something that may already be happening – both the Pacific and Atlantic oceans may be fated to close. This means that a a new ocean basin would have to form to replace them.  In this scenario the Pan-Asian rift currently cutting through Asia from west of India up to the Arctic opens to form the new ocean. The result is the formation of the supercontinent Aurica. Because of Australia’s current northwards drift it would be at the centre of the new continent as East Asia and the Americas close the Pacific from either side. The European and African plates would then rejoin the Americas as the Atlantic closes. The fourth scenario predicts a completely different fate for future Earth. Several of the tectonic plates are currently moving north, including both Africa and Australia. This drift is believed to be driven by anomalies left by Pangea, deep in the Earth’s interior, in the part called the mantle. Because of this northern drift, one can envisage a scenario where the continents, except Antarctica, keep drifting north. This means that they would eventually gather around the North Pole in a supercontinent called Amasia. In this scenario, both the Atlantic and the Pacific would mostly remain open. Of these four scenarios we believe that Novopangea is the most likely. It is a logical progression of present day continental plate drift directions, while the other three assume that another process comes into play. There would need to be new Atlantic subduction zones for Aurica, the reversal of the Atlantic opening for Pangea Ultima, or anomalies in the Earth’s interior left by Pangea for Amasia.  Investigating the Earth’s tectonic future forces us to push the boundaries of our knowledge, and to think about the processes that shape our planet over long time scales. It also leads us to think about the Earth system as a whole, and raises a series of other questions – what will the climate of the next supercontinent be? How will the ocean circulation adjust? How will life evolve and adapt? These are the kind of questions that push the boundaries of science further because they push the boundaries of our imagination."
"The stunning colours of coral attract many divers to the world’s reefs but, for us coral scientists, one mystery has always remained. Swimming over a reef, you can frequently spot brightly coloured coral sat next to differently coloured or colourless individuals of the same species. Why such variation in the same environment? We now have the answer. Our research at the Coral Reef Laboratory at the University of Southampton lets coral colours appear in a new light: as sunscreening pigments that help explain how corals adapt to environmental stress. Our findings are published in the journal Molecular Ecology. The underlying ecological concept may not be restricted to pigments in corals but might help to explain how species respond to changes in environmental conditions. Answers to these questions are urgently required in times of global change and alarming species extinction rates. The brownish appearances of many corals under daylight is due to photosynthetic pigments from the microscopically small plants that live in symbiotic partnership within them. But most of the green, red and purple-blue hues are caused by a family of Nobel prize-winning protein pigments. Some coral pigments glow green or red under ultra-violet or blue light, a physical phenomenon called fluorescence. During this process, light of a distinct colour is taken up by certain dye particles and re-emitted with a different, more red-shifted colour. The same process is responsible for the neon colours of marker pens or high-visibility clothing. In shallow waters the pink, purple and blue coral colours are most striking. Beyond depths of around seven metres these colours tend to become dull since their brilliance depends on the reflection of red light. Down there, where blue light dominates, the green and red fluorescence of some corals makes them stand out from the bluish-grey background.  Fluorescence can be best observed with the help of blue light torches and special filter masks under low light conditions. Using this equipment, the glowing corals make a night dive a psychedelic adventure. Some corals increase the production of colourful protein pigments when they are exposed to more intense sunlight. Humans get a sun tan – corals become more colourful. We found the pink and purple proteins act as sunscreens for the corals by removing substantial light components that might otherwise become harmful to the algae hosted in their tissue. Corals rely on these light-dependent miniature plants, the so-called zooxanthellae, since they provide a substantial amount of food.  We have also explained why some corals accumulate exceptionally high amounts of  colourful pigments in growing areas such as branch tips or near wounds. These areas contain essentially no symbiotic algae, so much of the light is reflected by the white coral skeleton instead of being used by the algae.  The resulting increased light intensities in the new parts of the coral represent a potential danger for the algal cells that need to colonise these areas. Hence, it seems the corals use a clever trick to help their symbionts. The higher light intensity switches on the genes that are responsible for the production of the sun-screening pigments.  Our results suggest that this shading effect could help the algae to enter the new tissue and establish the necessary symbiotic association. Once the population of symbiotic algae is fully established, the light levels in the tissue decrease as the algae use most of the incident light for photosynthesis. As a consequence, the genes of the chromoproteins are switched off again, allowing the coral to save the energy required for their production. Increased growth is associated with wound healing and neutralising potentially dangerous organisms by overgrowing them. The growth-related increase in pigmentation can also explain the bright colours of corals in areas where the animals have been damaged or are struggling with other organisms settling on their surface or in their skeleton. Despite these recent advances in understanding the functions of coral pigments, we still didn’t know why corals of the same species could display such different colours, even when sat next to each other. It raises challenging questions. If the production of the pigments is triggered by the light intensity, then why don’t all individuals have the same colours when they are exposed to the same light environment? And if these pigments help survival by acting as a sunscreen, then why aren’t corals in shallow waters always colourful?  Our most recent publication  explains the genetic framework that results in the dramatic differences in coral individuals. We found that instead of using a single gene to control the production of sunscreening pigments, corals use multiple copies of the same gene.  These genes do indeed respond to light, but not all of them, thus it is the number of these active genes that is important – and this varies between individual corals of the same species. Depending on how many genes are active, the individual coral will become more or less colourful, even despite being exposed to the same light conditions.  However, the enhanced protection offered by the sunscreening pigments costs the corals a lot of energy that might be diverted away from growth or reproduction. Therefore, being brightly coloured might not be a good investment for corals settling in more shady parts of the reefs. This genetic variation ensures some individuals within a coral population are well protected and are likely to survive better in stressful environments. Others are less protected but can instead invest their energy in processes that could help them to succeed in habitats with less light stress. These are probably the driving forces that keep multiple colour variants in the game for survival.  The resulting colour polymorphism makes it easier for coral species to inhabit more ecological niches in a reef. Humans can support the efforts of the corals by sheltering them from other forms of stress that they might not be able to deal with by themselves: heated waters, pollution, nutrient enrichment, sedimentation, overfishing, to name only some."
"

For those of us that hate having Winnie the pig presented to us as part of our local weather report, I’d like to offer this solution that cuts all of us annoying weather middlemen and weather forecasting pigs right out of the picture and give you total control over your weather report.
Its called the ViziFrame – now you can program your own local weather channel at home, or at the office, or at the marina, or the golf course, your school, a truck stop, gas station, or wherever there may be an interest in weather to make a go/no go decision. You can view it on your own terms, and unlike the Weather Channel, you don’t have to wait a half hour to get the info you need.
And the graphics, look at good as anything on TV. For those of you with a profit in mind, it can have advertising and other information too. Its way cool, inexpensive, and trouble free. It works with any TV, big screen, or computer monitor. It updates its information via WiFi or a regular Internet cabled connection to a home DSL/cable router or T1 router.
The Chico Chamber of Commerce is going to put a bunch of them (the premium model that also does video and audio clips with touch screen interactivity) around town at hotels, restaurants, city hall and other public places that cater to visitors. Local artist Gregg Payne worked out a cool design for the front fascia that looks like the Hooker Oak tree…a concept view is below along with the current weather page and forecast…which are live content links soon to be on the Chamber of Commerce web page. Kris Koenig and Anita Berkow of Interstellar Studios are doing the interactive kiosk presentation for it. Look for these around town soon!



Note the current conditions page – it has solar irradiance on it – I figured if we were going to become a solar powered city, getting a real-time indicator that people can use to calculate solar panel efficiency would be a good first step, so I invested in the equipment to do that. I’ll have an entire blog entry on this service later.
The graphics are made in my rendering system as weather data arrives at my office here in Chico, there’s actually about a hundred plus graphics that are available.
But you can get a weather channel for your home or business too. See www.viziframe.com I’ve sold several of these already and people at home just connect them up to a spare video port on their big screen TV, and when they want weather, just switch to it. No waiting, no pigs, no hassle.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7156f18',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterA new study finds rising CO2 concentrations (and warming) have driven the rapid increase in Earth’s photosynthesis processes, or greening. CO2-induced planetary greening leads to an enormous expansion of Earth’s carbon sink. By 2100 this greening-sink effect will offset 17 years of equivalent human CO2 emissions. This easily supersedes the effect of Paris Agreement CO2 mitigation policies.
In a break from the deflating global news of viral infections and rising death rates, a groundbreaking new study (Haverd et al., 2020) affirms the “beneficial role of the land carbon sink in modulating future excess anthropogneic CO2 consistent with the target of the Paris Agreement” via the fertilization effect of rising CO2.
There has been a 30% rise in global greening since 1900. CO2 fertilization is the “dominant driver” of these greening trends, with an additional positive contribution from climate warming.
When CO2 levels double (to 560 ppm), this CO2-fertilization-greening effect is expected to increase to 47%.
Growth in the land’s carbon sink – absorbing excess CO2 emissions – will reach 174 PgC by the end of the century.
This is the equivalent of eliminating 17 full years of human CO2 emissions.

Image Source: Haverd et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_fba188469f13af433635491d7989d885').on('change', function() {
			  jQuery('#amount_fba188469f13af433635491d7989d885').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Antarctica’s glaciers have been making headlines during the past year, and not in a good way. Whether it’s a massive ice shelf facing imminent risk of collapse, glaciers in the West Antarctic past the point of no return, or new threats to East Antarctic ice, it’s all been rather gloomy. And now I’m afraid there’s more bad news: a new study published in the journal Science, led by a team of my colleagues and I from the University of Bristol, has observed a sudden increase of ice loss in a previously stable part of Antarctica. The region in question is the southernmost half of the Antarctic Peninsula, a section of the mainland which extends 1300km into the Southern Ocean. Its northern half is the continent’s mildest region and the climate effects there are clear. We already knew for instance that the glaciers of the Northern Antarctic Peninsula were in trouble following the disintegration of some of its ice shelves, most famously Larsen A and B.  Further to the west, the massive glaciers feeding into the Amundsen Sea have been shedding ice into the ocean at an alarming rate for decades. Out of the blue, the Southern Peninsula filled up the gap between these two regions and became Antarctica’s second largest contributor to sea level rise. Using satellite elevation measurements, we found the Southern Antarctic Peninsula showed no signs of change up to 2009.  Around that year, multiple glaciers along a vast 750km coastline suddenly started to shed ice into the ocean at a nearly constant rate of 60 cubic km, or about 55 trillion litres of water, each year – enough water to fill 350,000 Empire State Buildings over the past five years. Some of the glaciers are currently thinning by as much as 4 metres each year. The ice loss in the region is so large that it causes small changes in the Earth’s gravity field, which can be detected by another satellite mission, the Gravity Recovery and Climate Experiment (GRACE). The answer is both yes and no. Data from an Antarctic climate model shows that the sudden change cannot be explained by changes in snowfall or air temperature.  Instead, we attribute the rapid ice loss to warming oceans. Many of the glaciers in the region feed into ice shelves that float on the surface of the ocean.  They act as a buttress to the ice resting on bedrock inland, slowing down the flow of the glaciers into the ocean.  The westerly winds that encircle Antarctica have become more vigorous in recent decades, in response to climate warming and ozone depletion.  The stronger winds push warm waters from the Southern Ocean poleward, where they eat away at the glaciers and floating ice shelves from below. Ice shelves in the region have lost almost one-fifth of their thickness in the last two decades, thereby reducing the resisting force on the glaciers. A key concern is that much of the ice of the Southern Antarctic Peninsula is grounded on bedrock below sea level, which gets deeper inland.  This means that even if the glaciers retreat, the warm water will chase them inland and melt them even more. The region’s melting glaciers are currently adding about 0.16 millimetres to global sea levels per year, which won’t immediately make you run for the hills. But it’s yet another source of sea level rise, about 5% of the global total increase. What might be a bigger source of concern is that the changes occurred so suddenly and in an area that was behaving quietly until now. The fact that so many glaciers in such a large region suddenly started to lose ice came as a surprise. It shows a very fast response of the ice sheet: in just a few years everything changed. The Southern Antarctic Peninsula contains enough ice to add 35 cm to sea level, but that won’t happen any time soon. It’s too early to tell how much longer the ice loss will continue and how much it will contribute to future sea level rise. For this, a detailed knowledge of the geometry of the local ice shelves, the ocean floor topography, ice sheet thickness and glacier flow speeds are crucial.  But the ice on Antarctica is like a sleeping giant. Even if we would stop emitting greenhouse gases as of today, or the inflow of warm water would stop, this inert system would take a long time to find an equilibrium again."
"
Share this...FacebookTwitterScientists find three Arctic (Svalbard) lakes were all ~7°C warmer than they are now about 10,000 years ago – when CO2 concentrations were only 260 ppm.
According to a just-published Geophysical Research Letters paper (van der Bilt et al., 2019), not only were surface temperatures 7°C warmer than today in High Arctic Svalbard due to the accompanying “high radiative forcing” during the Early Holocene, but sea ice limits were well north of the study area back then too.
The authors point out that model “simulations neither reproduce this reconstructed pattern nor its magnitude.” This is presumably because the model simulations are predicated on the assumption CO2 concentrations are a primary climate driver.

Image Source: van der Bilt et al., 2019
This paper is yet another in a “growing body of recent work” that uses the prevalence of warmth-demanding (thermophilious) species present in Arctic locations to reconstruct regional temperatures based on a requisite warmth limit for the species’ survival.
Earlier this year Leopold et al., 2019 assessed temperatures were 5-8°C warmer than today in Arctic Svalbard 8 to 10 thousand years ago due to the presence of Mytilus spp, a warmth-loving mussel.

Image Source: Leopold et al., 2019
Share this...FacebookTwitter "
"
This post is an outgrowth of comments I made on Commission Impossible on the new proposed strengthened tree ordinance.
I like trees, and I recently planted four, but at the same time I’ve had to remove a couple of trees from my home and business property. In the latter case, the City did the work because they agreed with me that the tree was unsafe and posed a public hazard.
This tree ordinance thing is taking on overtones of the abortion battle, except that the roles seem to be reversed, with the “right to life” being on the left. Lately, it seems that meadowfoam, garter snake habitat (see Sundays letters to the editor) and beetle habitat Elderberry bushes are more important than the rights and lives of people.
Case in point – how many people have died at the Highways 70/149/99 interchanges in the 10+ years that environmentalists have placed roadblocks in front of that project? I remember one little boy, about two years ago, who died when the car he and his mom were riding in was broadsided by a car on 70 as she turned onto 149. If the road had been improved on schedule, that never would have happened.
Was that worth 10 years of delay to protect some Meadowfoam and beavers? I think not. Meadowfoam is being grown in quantities at reserves near Vina and commercially in Oregon, and the Limnanthes Flococcus Californica aka Butte County Meadowfoam can just as easily be grown with it. Beavers relocate with ease too. Anybody who tells you otherwise is just pushing an agenda.
Now we have the City saying there’s a delay in authorizing a bid to fix drainage problems for a man made stormwater retention basin near south Chico street Paseo Campaneros that becomes a West Nile hotspot. Two people have died on that street from West Nile in the past year…yet the “garter snake habitat” aka man-made retention basin gets hands-off priority according to what the city said recently.
It’s lunacy and its morally wrong. Public health, be it an accident prone intersection or a festering man-made mosquito pool should always trump protecting bugs, plants, snakes, and the occasional beaver. If you think these things are more important than the health of the community, then you have your social priorities reversed.
Environmentalists digging in their heels on this only hurts their cause, because it makes them look unreasonable, and maybe they are. But most people I know, on either side of the political spectrum actually want to protect the environment, me included, but they want to protect their children and grandparents more.
Making them choose through obstruction is a no-win polarizating situation. We CAN have it both ways.
In the case of trees, the folks pushing this strengthened law act as if the tree, once cut down, could never be replaced. We’re not talking giant Redwoods here…more like Dogwoods and Pines, available at Home Depot.
Compromise folks…compromise.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6f63713',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWinter has not even officially arrived, but already large areas of the northern hemisphere are seeing “historic snowfalls”, frigid temperatures and even avalanche alarms.
The Northern Hemisphere has certainly caught a major cold, one certainly not caused by the human CO2 virus. Instead of fever, parts of the northern hemisphere are in hypothermia!
Alarmists, media desperate
Though global warming scientists will never admit it, they are really surprised and stunned. All that is left for them is to make up some cockamamie warming-causes-cold explanations and hope there are enough severely stupid among the media and masses to believe it.
“United States — Rewrite the Record Books”
Beginning in North America, “sub-zero temperatures are now blasting” millions of Americans following “the three historic snowstorms which buried parts of the U.S. last month,” reports weather site electroverse.net here.
Electroverse writes that “lows throughout the week will be more like January temperatures” with readings below zero for many U.S. states and “temps down into the teens are even forecast as far south as Texas.” Yesterday, 97 records toppled.
“It’s a big deal,” Electroverse writes in its headline.
Solar activity suspected
It’s not the sort of thing we are supposed to be expecting from a “warming planet”.  Some climate experts blame natural factors, like solar activity, for the cold, and that these warnings have long been known since the sun has entered a new period of calm.
Freeze watches and warnings also extend as far south as Florida. And it’s only early November. And don’t expect to see many FFF activists to show up at rallies protesting hot weather any time soon.
Polar Bear Science site here also reports that the Hudson Bay in Canada has started freezing up earlier than normal three years in a row!
Europe starting to get clobbered by snow, 2m in Alps
Meanwhile cold has also spread across Europe, though not quite as brutal as what we’ve been seeing across North America.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In central Europe, the Austrian online heute here reports that “huge amounts of snow” are on the way for the Alps. German site Wetteronline.de reports here of “new, severe snowfalls in the Alps” with “up to two meters of fresh snow are possible in places up to the weekend” in Switzerland, Austria and Northern Italy, “This is good news for winter sports enthusiasts – but the danger of avalanches is increasing.”
Biggest November snowstorm in 40 years
Even global warming child activist Greta Thunberg’s Sweden is getting hard hit by extreme cold and snow. Electroverse reports the Nordic country is suffering “its biggest November snow storm in 40 years.”
On November 10th, Mika tweeted that temps in northern Sweden fell 10 -34.5°C.

Today is the coldest morning so far during the ongoing winter season:
-34.5°C in Sweden, -31.1°C in Norway and -30.6°C in Finland (not shown on the map). pic.twitter.com/PoH2Ddnde4
— Mika Rantanen (@mikarantane) November 10, 2019

Most snow in 60 years
The German Ruhrkultur site reports how also Finland just saw “the coldest autumn temperature and the highest snow depth in at least 60 years” and that ” the temperature in Enontekiö, a municipality in Finnish Lapland, dropped to 28.2°C on Tuesday 5 November.”
Deepening cold across Siberia as well
“On November 11 in Yakutia, the daily temperature never rose above −30°C (-22F),” reported SOTT site here. “Some parts of Siberia were even colder: In Evenkia and the northern regions of the Krasnoyarsk Territory, the temperature dropped to −41 … −44°C.”

SOTT comments (sarcastically): “I wonder how much ice will melt at −44°C (-47F).
With all the early winter weather, it’s ridiculous to claim the globe is burning up. So it’s no wonder the alarmists have taken their climate ambulance to the far side of the globe, NSW Australia, and kept their narrow focus on brush fires.
Hat-tip: Yota at Twitter
Share this...FacebookTwitter "
"It has been almost 10 years since BP was mired in the largest marine oil spill in history, following the explosion at the Deepwater Horizon rig in the Gulf of Mexico. In 2010 Bob Dudley was the man tasked with steering the group back from the brink of collapse as compensation payouts threatened to suffocate the business. A decade on, Dudley’s successor will now have to navigate an environmental crisis on a global scale.  Bernard Looney becomes BP’s new chief executive next month at a time of mounting public pressure to prevent a climate breakdown by phasing out fossil fuels. His career success has been built on the millions of barrels of oil that BP produces every day, but his tenure at the helm will be defined by his response to the climate crisis. The dapper, 49-year-old Irishman has been hailed as a charismatic moderniser of the oil industry for the millennial generation. This week he launched an Instagram account to encourage a “candid” conversation about the energy transition. Many believe this unconventional oil man may be BP’s best hope of surviving the global shift from fossil fuels. Sources within the company say Looney was the natural frontrunner in the race to take over from Dudley. “It was no surprise to me whatsoever,” says one. “This is a business which needs to continue to deliver on its promises to shareholders, but must also evolve. The new leader will need to do both, and for many Bernard was the natural choice.” So, how “green” is BP’s new chief executive? “It’s a legitimate question,” says Mark Lewis, head of sustainability at the asset management arm of French investment bank BNP Paribas. “The burden of truth will rest firmly on his shoulders.” He adds: “In Europe’s financial centres the sense is that BP may be the European major with the most work to do in terms of improving their reputation and performance on climate change. I think Mr Looney will find that this is the first and most important question that he will need to give reassurance on.” Looney is a BP “lifer”. He was marked as a rising star soon after joining as an engineering graduate from University College Dublin, and primed for senior leadership. A series of plum apprentice roles beside BP bosses followed, including stints working directly for John Browne and Tony Hayward. When Dudley took the reins in 2010 he promoted Looney to BP’s senior management team. Looney later became the global boss of “upstream” oil and gas production, where he was in charge of reigniting the company’s fossil fuel growth. Under his leadership BP produced 2.6 million barrels of oil and gas every day, and secured a series of multibillion-dollar investments that will pour millions more barrels into the global market in the years ahead. Mark van Baal, from the Dutch shareholder activist group Follow This, warns that a career steeped in oil reserves may make it difficult for Looney to “imagine a future beyond oil and gas” or see renewables “as a business opportunity not a chore”. However, Nick Boyle, the chief executive of BP’s solar joint venture Lightsource BP, says he has “absolutely no doubt” about BP’s commitment to developing renewable energy. Boyle teamed up with BP two years ago and has met Looney twice in recent months. “He was genuinely interested and asking intelligent questions about challenges and what we thought the future would be for solar. We know from any of the energy projections available that solar is going to play an ever increasing part in the energy mix, and part of BP,” he says. Still, the oil bosses who have led the most meaningful climate action have tended to be new arrivals to the industry. The Danish energy giant Orsted has traded oil rigs for offshore windfarms under the leadership of Henrik Poulsen, a former telecoms boss who was once an executive at Lego. Spain’s Repsol set out industry-leading plans to emerge as a carbon- neutral company by 2050, mapped by its chief executive, Josu Jon Imaz, after a career in politics and scientific academia. Looney intends to be the exception. In an industry speech shortly before he was named as BP’s new boss, he told delegates: “We need to listen, hard, to society’s concerns. They are real concerns – the world is not on a sustainable path – and they are our concerns. Shouting louder about the good that we do is not a winning strategy. We need to demonstrate that we are part of the solution – that we get it.” He is reportedly poised to unveil the biggest strategic overhaul of the company’s century-long history. According to Reuters, the plan is to broaden BP’s carbon targets beyond the emissions it produces within its operations to include the emissions from the product it sells to customers. Environmental campaigners believe this would be a crucial step, an essential means of pushing oil companies into providing cleaner sources of energy. “Maybe it’s the Irish thing,” Boyle adds. “But he instils a confidence and a positive, can-do mentality which is infectious. It’s a big company, but he’s always been well-known as a charismatic, lead-from-the-front, roll-your-sleeves-up type of guy. The big plus with Bernard is that he gets it.” BP’s chances of surviving beyond its next environmental crisis will depend on that. Activist investors say oil firms must: Cut their own emissionsOil companies can do a lot to reduce CO2 emitted during production – by, for example, cutting methane leaks from rigs and stopping “flaring” – the burning of “waste” gas from oilfields. Take responsibility for the carbon footprint of the energy they sellOutgoing boss Bob Dudley stated that BP “cannot control” how people choose to drive or heat their homes, but setting an emissions target for the energy they produce should help a shift towards clean sources. Set a clear strategy for hitting the Paris climate goalsTo achieve net zero emissions by 2050, oil companies also need to invest in measures such as tree planting and carbon capture. Link executive pay to climate actionThis would incentivise leaders and guard against “greenwash” promises."
"Not heard of the “Extinction Rebellion” before? Then you heard it here first. Because soon, everyone is going to have heard of it. The Extinction Rebellion is a non-violent direct action movement challenging inaction over dangerous climate change and the mass extinction of species which, ultimately, threatens our own species. Saturday November 17 2018 is “Rebellion Day” – when people opposed to what they see as a government of “climate criminals” aim to gather together enough protesters to close down parts of the capital – by shutting down fossil-powered road traffic at key pinch-points in London. I’m a Reader in Philosophy at the University of East Anglia and I have thrown myself headfirst into this movement. Our long-term aim is to create a situation where the government can no longer ignore the determination of an increasingly large number of people to shift the world from what appears to be a direct course towards climate calamity. Who knows, the government could even end up having to negotiate with the rebels. As someone who is both a veteran of non-violent direct actions over the years and an academic seeking to make sense of these campaigns, I’ve been thinking quite a lot about what’s old and what’s new about the Extinction Rebellion. Here are my conclusions so far. The Extinction Rebellion is rooted in longstanding traditions exemplified by the radical nuclear disarmament movement. The founders of the Extinction Rebellion have thought carefully about past precedents, and about what works and what doesn’t. They’ve noted for instance that you don’t necessarily need active involvement from more than a tiny percentage of the population to win radical change, provided that you have a righteous cause that can elicit tacit backing from a much larger percentage. The Extinction Rebellion is also quite different from its predecessors. True, the disarmament movement was about our very existence, but nuclear devastation was – and still is – only a risk. Extinction Rebellion’s aim is to prevent a devastation of our world that will come – and quite soon, unless we manage to do something unprecedented that will radically change our direction. Climate activists often compare their struggle to victories from the past. But in my view comparisons which are often made – to Indian independence, the civil rights movement or the campaign for universal suffrage, for example – are over-optimistic, even fatuous. These historical movements were most often about oppressed classes of people rising up and empowering themselves, gaining access to what the privileged already had.  The Extinction Rebellion challenges oligarchy and neoliberal capitalism for their rank excess and the political class for its deep lack of seriousness. But the changes that will be needed to arrest the collapse of our climate and biodiversity are now so huge that this movement is concerned with changing our whole way of life. Changing our diet significantly. Changing our transport systems drastically. Changing the way our economies work to radically relocalise them. The list goes on. This runs up against powerful vested interests – but also places considerable demands upon ordinary citizens, especially in “developed” countries such as the UK. It is therefore a much harder ask. This means that the chances of the Extinction Rebellion succeeding are relatively slim. But this doesn’t prove it’s a mistaken enterprise – on the contrary, it looks like our last chance. This all leads into why I sat in the road blocking the entrance to Parliament Square on October 31, when the Extinction Rebellion was launched – and why I will be “manning the barricades” again on November 17. As a Quaker, I cherish the opening words of the famous Shaker hymn: Tis the gift to be simple. What does it mean to live simply at this moment in history? It means to do everything necessary so that others – most importantly our children (and their children) – can simply live. It isn’t enough to live a life of voluntary simplicity. One needs also to take peaceful direct action to seek to stop the mega-machine of growth-obsessed corporate capitalism that is destroying our common future. That’s why it seems plain to me that we need peaceful rebellion now, so that we and countless other species don’t face devastation or indeed extinction.  The next line of that Shaker hymn goes: “Tis the gift to be free.” In our times, to be free means to not be bound by laws that are consigning our children to purgatory or worse. If one cares properly for one’s children, that must entail caring for their children, too. You don’t really care for your children if you damn their children. And that logic multiplies into the future indefinitely – we aren’t caring adequately for any generation if the generation to follow it is doomed. As mammals whose primary calling is to care for our kids, it is therefore logical that an outright existential threat to their future, and to that of their children, must be resisted and rebelled against, no matter what the pitifully inadequate laws of our land say. I’ve felt called upon to engage in conscientious civil disobedience before, at Faslane and Aldermaston against nuclear weapons and with EarthFirst in defence of the redwood forests threatened with destruction in the Pacific Northwest of the USA.  But the Extinction Rebellion seems to me the most compelling cause of them all. Unless we manage to do the near impossible, then after a period of a few decades at most there won’t be any other causes to engage with. It really now is as stark and as dark as that. If you too feel the call, then I think you now know what to do. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

The picture above is of the official USHCN climate station of record in Quitman, GA and comes to me via www.surfacestations.org volunteer Joel McDade.
It is located at a residence, the observer has consented to having this NOAA weather equipment at his home.
Besides the usual problematic close-by parking of vehicles that we’ve seen before, and buildings less than 100 feet from the temperature sensor, we have a new issue to contend with: inoperable vehicles and abandoned appliances near the temperature sensor. Such big chunks of metal have thermal retention, which means that heat is retained past sunset and re-radiated near the sensor. This may bias overnight lows.
I thought the old washing machine was a nice touch though. It illustrates how little quality control of the temperature measuring environment is being done with the US Historical Climatological Network.
Additional pictures of the site are available at the surfacestations.org online database.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5b237d1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Wind turbines are, it appears, everywhere. Even if you can’t see some on the horizon on your way into work every day, it is hard to miss the continual news coverage of new developments. Clearly, efforts to move away from fossil fuels are – at least in part – working, and from the perspective of combating climate change, this must surely be a good thing. However, much of the news coverage of turbines highlights negatives such as a perceived degradation of the landscape, or their impacts on wildlife. There is good cause for concern in this regard, particularly with respect to wildlife.  Reams of published scientific papers show that birds and bats can be killed (sometimes in relatively large numbers) by colliding with the spinning blades. Clearly, where turbines are poorly placed or where rare or vulnerable species are affected, this is a problem. Images of dead birds of prey or rare vagrant birds under wind turbines are easily turned into emotive and sensational news stories, and are terrible PR for the wind industry. At first glance, a new study published in Nature Ecology & Evolution appears to add to the evidence of widespread lethal effects. The authors, a team based at the Indian Institute of Science and led by Maria Thaker, make the attention-grabbing observation that the effects of wind turbines are “akin to adding an apex predator to natural communities”. Apex predators are animals right at the top of their food chains, like killer whales or tigers. Adding an “apex predator” surely means that these man-made metal mincing-machines kill off birds and bats in large numbers, right?  While it certainly is true that collision with turbines can cause direct mortality, what the new research actually shows is far more interesting and complex than this. The result of many years of data collection, the work shows that through the (direct or indirect) effects on birds of prey, wind turbines can have broader and more subtle effects on the wider ecosystem – including on some unexpected species. Specifically, they looked at a lizard that lives only in the Western Ghats mountains of India, the superb fan-throated lizard (Sarada superba). They found that the number of lizards was considerably higher in areas with turbines installed compared to otherwise similar areas without turbines.  In areas with turbines, they also found fewer birds of prey such as buzzards and kites (the lizards’ main predators), and a lower frequency of attacks on lizards. Putting the two together, the authors propose that the presence of turbines is associated with lower predator activity, and thus higher prey numbers. In some ways, it may indeed seem as if wind turbines act similarly to the introduction of an “apex predator” into the food chain: by reducing the number and activity of intermediate predators such as birds of prey, predation pressure on smaller animals may be reduced. However, it is of course important to stress that almost all biological predators are, in the end, limited by the availability of prey. By contrast, turbines are not limited in this way and will continue to be present regardless of whether their “prey” goes locally extinct.  Perhaps more important is the non-lethal effects on the wider ecosystem that is highlighted by Thaker and colleagues. By reducing predation or reducing the activity of predators, the physiology, behaviour and population density of prey populations can be changed in unpredictable and subtle ways. What this work really highlights is that wind farms can have effects that are indirect or not immediately visible. This poses a huge challenge for impact assessments and survey work. Perhaps understandably, many assessments of wind turbines focus on vulnerable species, which tend to be the birds and bats most at risk of direct collision. Similarly, where carried out, post-construction monitoring often focuses on collision casualties. These are things that can be directly measured or counted, over clearly defined periods.  By contrast, assessing the longer-term effects on indirectly impacted species such as lizards takes a lot more time and money. Unfortunately, this luxury is typically not available for commercial impact assessment studies. What is more, it is likely to be significantly harder to convince regulators that impact assessments should be broadened to consider wider ecosystems, particularly where these cannot be immediately seen or counted as dead animals. At least in the UK, there is some evidence that consideration of such “synergistic” effects of developments is now increasingly expected in impact assessments. While this is encouraging, more work along the lines of study by Thaker and colleagues is needed to build up a better understanding of the extent and importance of similar effects in different areas. From a pragmatic perspective, it may well be that in the future impact assessments will move away from focusing on direct collision mortality and towards more targeted and in-depth assessments of “downstream” ecological impacts."
"
Share this...FacebookTwitterClouds regulate Earth’s climate. New studies suggest uncertainty in clouds’ surface radiative effects reach 17.4 W/m² per year (±8.7 W/m²/year). Total CO2 climate forcing is said to be just 0.02 W/m² per year. The difference in these magnitudes preclude detection of a CO2 signal in climate forcing.
According to Feldman et al. (2015), a 2 ppm increase in CO2 per year (22 ppm over the 11 years from 2000-2010) results in a surface radiative forcing influence of 0.02 W/m², or 0.2 W/m² per decade. This is said to be just “ten per cent of the trend in downwelling longwave radiation” when clouds and water vapor are considered.
In contrast, the influence of clouds in total longwave forcing is substantially larger, with radiative forcing trends reaching ±4 W/m² per decade.
From 1978 to 2010, the total longwave anomalies reached amplitudes of about ±2 W/m² per year (Loeb et al., 2012, below image). These anomalies dwarf the 0.02 W/m² per year radiative influence from CO2, and thus factors other than CO2 must be driving the variability.
Further, the overall trend in longwave or greenhouse effect forcing appears to have been flat during this 30-year period. CO2’s 0.2 W/m² per decade contribution may have had no net impact on the trend.

Image Source: Loeb et al., 2012
Even if a CO2 forcing signal was detectable in an overall trend over the last few decades, it would be lost amid the uncertainty and noise of the radiative effects of clouds.
According to L’Ecuyer et al., 2019, the global annual net cloud radiative effect (CRE) at the Earth’s surface is estimated to be -24.8 ±8.7 W/m². In other words, when cloud cover increases, surface temperatures cool because the net shortwave effects of cloud (-51 W/m²) exceed the net longwave effects of cloud (+26.3 W/m²). The uncertainty value associated with this overall surface forcing estimate, ±8.7 W/m², has a range of 17.4 W/m².
If CO2’s net radiative effect in surface forcing is 0.02 W/m² per year and the uncertainty the radiative effects of clouds is ±8.7 W/m² per year, this means that uncertainty is 870 larger than the CO2 influence.
A CO2 forcing signal is therefore not detectable in the Earth’s energy balance.

Image Source: L’Ecuyer et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_717fa080377e4740ab19829655e23af7').on('change', function() {
			  jQuery('#amount_717fa080377e4740ab19829655e23af7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"It’s bee season and now’s the time to go outside and observe these popular insects. Bees hold a relatively special place in people’s affections – we have them to thank for honey, of course, and they’re also essential pollinators of many food crops and wild plants. But most bees aren’t the snazzy hive-dwelling orange and black characters we know so well. In fact, there are around 20,000 species of bee globally and just seven of these are honeybees, and the vast majority of honeybee colonies belong to only one species: Apis mellifera.  So what about the rest? Well around 250 species are bumblebees, these big fluffy garden favourites are closely related to honeybees and also live socially with a queen and workers. But this still leaves us with around 19,700 bee species to account for. Indeed there are so many of them that the vast majority don’t even have a common English name and we have to use the scientific name.  Most of these remaining species are solitary; unlike social bees they do not have queens or workers, all females are reproductive and live independently. These solitary bees are important pollinators and have evolved all sorts of interesting characteristics more suited to a life flying solo. Lots of wild bees nest below ground. Some, like the tawny mining bee, Andrena fulva, often leave very noticeable nest entrances in lawns and parks, while other species such as Lasioglossum malachurum can be found nesting in the compacted ground of footpaths – keep a look out for tiny circular holes the next time you’re walking on a footpath, occasionally you can see the little face of the occupant. Some species of wild bees nest in vegetation, such as Ceratina cyanea which sets up in the hollow stems of plants like brambles. Leaf-cutter bees like Megachile centuncularis cut out neat circular sections from leaves and use those to line the cells of their nests which are usually in timber or wall cavities. One of the more unusual nesting strategies is that of Osmia bicolor, the females of this species nest in abandoned snail shells. They even go as far as to cover the shells with vegetation to hide them from view of predators and parasites.  A close relative of this species is O. bicornis aka the red mason bee, this is a very familiar garden visitor across Europe and Asia due to its habit of nesting in the brickwork of our homes. If you look closely at the females you will notice two small horns on the front of its face (this is the inspiration for the scientific name “bicornis”), they use these horns like trowels to position mud they have collected to seal their nests. Adult bees usually only feed on nectar, they have finished growing so they just need sufficient energy and there’s no better energy drink for them than sugary nectar. Their larvae however need protein to grow and develop, so female solitary bees collect pollen from flowers and place a big ball of it in each nest cell before laying an egg and sealing the cell. All bees are generalists in terms of the flowers they take nectar from, but some species will only take pollen from a small number of plant species. Macropis europaea is an unusual bee. It is monolectic, which means it only collects pollen from one species of plant: yellow loosestrife. It doesn’t just feed its larvae on the pollen from this plant, it also collects fatty floral oils and mixes them with the pollen. This bee has very wide hind legs, presumably for transporting these floral oils. Chelostoma campanularum is another food specialist, this tiny wasp-like bee will only take pollen from a few plant species and again the scientific name is relevant as these plants belong to the genus Campanula (bellflowers). The males of this bee sleep in the flowers, so if you live in Europe and have any bellflowers in your garden keep an eye-out in summer especially early in the morning when the bees are still sleeping. There is one family of wild bees often categorised as solitary bees, which contains some species that are not solitary. I like to call this confusing group the “social solitaries”, though in some parts of the world they are known as “sweat bees” as some species have developed a habit of licking sweat from people’s skin. Some of these species appear to always be social, while others display both behaviours. Halictus rubicundus is a good example of this, as it is usually social in warmer areas and solitary in cooler ones. Some of the smallest bees in the world are in this group and they are also often among the most beautiful. Lasioglossum morio is a very small social species (5-6mm long) which has a metallic shiny green appearance. Unfortunately they are easy to overlook as they are so small and appear dark and colourless from afar. This is just a taster of the huge variety of shapes, colours, sizes and behaviours of bees. If you want to learn more then try your local bee or hymenoptera recording society, like BWARS (Bees, Wasps and Ants Recording Society), they have pictures and information on all 240+ bee species native to the UK. I have lots of favourite species, but there is one that I feel a particular affection for: Xylocopa violacea, the violet carpenter bee. This massive solitary bee has very beautiful iridescent purple wings; it’s currently native across continental Europe and Asia but a few individuals find their way to the UK most years. Unfortunately I’ve never seen it in the wild, so I’m hoping it will soon colonise the UK."
"Every year massive amounts of valuable resources are deemed “waste” and consigned to landfill. Take the UK – around 540 million tonnes of products and materials enter the country annually, but only 117 million tonnes are recycled.  However if we truly thought that waste equals value, and ensured that resources are kept in the economy for longer and thus reducing the use of raw materials, we could make serious steps towards diverting all this waste from landfill. This is what’s known as the circular economy. For the circular economy to thrive, we must be serious about reducing waste. And yet we’re still locked into an “iron cage of consumerism” – shifting to a future in which we’re more efficient with materials will require significant changes in the way we consume things and in the way businesses are organised.  To make this shift possible, we must recognise that a lot of customers simply don’t consider sustainability the main selling point of a product. We must be realistic, and businesses have to target what consumers really want. Recycling has thus far been the main focus of waste production, but this is far from ideal as it takes a great deal of energy to reprocess and remanufacture materials, as well as transport them. We need to focus on making new stuff last longer. In short, we need to do away with the “consumer culture”. To do this, businesses need to think differently about the ways in which products are provided to consumers. Providing better information to consumers about durability of products would be a good start. Giving accurate lifetime information on product labels could keep them in use for a longer time.  The average consumer expects a washing machine to last six years before it needs to be replaced, and Which? magazine cites quality and reliability as strong buying criteria.  So why don’t people actually keep their washing machines for that long? It’s because they’re confused by the labelling. Consumers tend to assess the expected lifetime of large household appliances through the manufacturer’s warranty. However most just give between one or two years warranty, making it difficult for the consumer to really assess how long it would last.  Rectifying this would mean businesses and their brands become more trusted, enabling them to build better and more prolonged relationships with consumers. When people aren’t switching brands all the time, products will be considered less disposable. Leasing isn’t a new concept, but it could pay dividends for the environment – it’s one of the business models which could reduce the use of materials in the longer term. Why can’t household appliances like washing machines be rented rather than bought? They could be rented from manufacturers. As manufacturers would be responsible for repair and replacement rather than consumers, this should help to remove any commercial benefit to be gained from making products short-lived. More leasing would thus significantly cut the use of materials. Appliances could be re-used from one consumer to another while all repairs would be undertaken by the manufacturer, rather than machines being scrapped when they break down. As well as providing added revenues for manufacturers that would spread over time, this would trigger a change in the design of washing machines which would enhance the focus on longevity. It is true that consumers already have well-established buying habits, and that firms would encounter significant costs when setting up new business models. But the benefits it seems could out-weigh the immediate barriers.  There would be major cost savings on manufacturing materials, something which may become very important in the future. The more scarce materials become, the more expensive they will be – a real pressure point for the industry of tomorrow. Re-use and repair services and leasing models will inevitably start to become more appealing to businesses which are looking at ways to turn a profit in the face of increasing costs. In the past few years, the sharing economy or collaborative consumption has demonstrated new ways of distributing goods and services to consumers. As goods are shared instead of owned, the sharing economy represents an alternative to our consumerist society. Take for example Bla Bla Car, where people can share a ride, or Airbnb where travellers can rent a spare room in someones home, as an alternative accommodation. This peer-to-peer business models had prove to be successful. However, there still some barriers as they depend on trust between people. Similar business models could be applied to other products such as large household appliances. A laundrette is the perfect example. In Scandinavian countries, famed for their sustainability, it is very common to find laundrettes in private apartment blocks. However, in the UK and many other countries laundrettes are often perceived as unhygenic and people can be snobby towards them. Such attitudes aren’t helpful. We should reinvent the laundrette as a key part of the shift towards a more sustainable, sharing economy."
"There has been a lot of debate recently on the extent of the global fishing footprint. A recent paper claimed that fishing affects 55% of the world’s oceans. Given that many people in the developing world rely on fish as their main source of protein, and the increasing preference for luxury fish products in countries such as China, such statistics might seem plausible. To calculate the 55% figure, the researchers relied on the automatic identification system (AIS). Primarily intended for safety purposes, AIS combines radio and satellite monitoring with other electronic data such as speed, heading and destination port, to track, monitor and even predict vessel activity. All vessels over a certain size must have an AIS transceiver, so this widespread monitoring produced the huge amounts of data that allowed the researchers to estimate the global fishing footprint. However, when determining what proportion of the ocean is being fished, the scale at which the fishing activity is mapped makes a significant difference to the accuracy of the overall result. Using higher resolution data, with grid squares of between one and 3km², rather than 1,000km² for example, produces a footprint which differs by a factor of more than ten. We need to manage the impact of human activity on ecosystems, but doing so demands a more accurate understanding of fishing. In a similar way to other food production systems, fisheries can have a wide range of effects on the ecosystems and species that they interact with. Wild capture fisheries are also vulnerable to over-fishing unless they are well managed and regulated. For this management to be effective, we need to know how much fishing is taking place, where it is happening, and the exact type of fishing that it is.   Such a high level of accuracy is especially important when looking at the impact of bottom trawling. Many forms of fishing have a wide impact on the ecosystem. But no fishing activity exemplifies this better than bottom trawling, which sees large, heavy nets being towed across the seabed. It is associated with the removal of species from the seafloor and temporary or longer-lasting modification of habitats. This has become a particularly emotive issue with campaigns seeking to outlaw its use.   However, bottom trawl fishing is a key source of food, accounting for 25% of global landings. With such high demands, we can’t simply stop the practice dead in its tracks. The impacts can be managed, but first we need to understand where bottom trawling occurs and how often, so it can be done in a sustainable way.  For our newly published research, we looked into the true extent of bottom trawling around the world. We used high resolution vessel monitoring data to reconstruct fishing footprints for 24 regions of the sea of less than 9km² each. We found that, on average, only 14% of this area was affected by bottom trawling. There were, however, major regional differences. For example, trawling affects less than 10% of the Australian and New Zealand seabed, compared to over 50% in some European seas.  Funded in part by the Marine Stewardship Council, this research will facilitate the implementation of sustainable fishing practices around the world. It also demonstrates that when fisheries are well-managed and sustainably fished, the associated impacts on the seabed are reduced, compared to other less well managed fisheries. In other words, if you manage the fishing of the target species appropriately you’ll probably also succeed in reducing other effects of fishing activity.  Some habitats are also highly resilient to the effects of trawling, whereas others are more vulnerable and take decades to recover. Recent advances in our understanding of the effects of bottom trawls mean that it is now possible to predict their impact and to suggest pragmatic ways to reduce those impacts through different management options, such as by directing fishing activity away from sensitive areas of the seabed to more resilient areas. These new advances in understanding pave the way for a truly ecosystem-based approach to fisheries management, in which we can manage target species and the wider effects of fishing on the seabed. This is the approach taken by the Marine Stewardship Council fisheries standard, which integrates the full range of ecosystem effects of fishing.  So, what challenges remain? While AIS data is publicly available, it covers only a proportion of the world’s fishing fleet. Vessel monitoring systems (VMS) are used for a larger selection of the world’s fishing fleets, but can be shrouded in confidentiality issues (the data is usually considered the property of fishermen). In addition, small-scale vessels, which in many places account for the largest proportion of fishing boats, are not monitored at present.  However, much data does still exist that can guide improvements in the management of fishing impact. It is critical that we interpret this data properly, using appropriate resolution mapping, to avoid inaccurate representations of the size and extent of fishing’s footprint on the oceans."
"

Sky of Jerusalem at 7BC-11-12 at about 7:30PM local time – Click to View Larger image
With all the hullabaloo about politically correct “Happy Holidays” greetings, as done up in electric lights on top of the Sierra Nevada Brewery, I thought the Christmas Star would be an appropriate topic.

About 2,006 years ago, according to a widely accepted historical and biblical accounts, a star rose in the east and guided three eminent thinkers, known as the Magi,  to the scene of an event that was to change the face of the world.
Since that time, astronomers and theologians have been baffled as to the precise nature of the star which, as told in the Gospel of St Matthew, led the Magi to the stable in Bethlehem where Christ was born.
Was it a miracle, a divine intervention to herald the birth of Christ? Was there a star at all, or was it simply added to the Bible to fulfil an Old Testament prophecy? Or was there some actual astronomical event that gave rise to the story of the Star of Bethlehem?
These questions have intrigued scores of scientists, writers, and artists ever since.
Evidence drawn from modern Biblical scholarship, recent findings in space and ancient Chinese history to suggest that evidence of the star’s existence could be at hand.
A British astronomer, Mark Kidger suggests that the Nativity may well have taken place at some time in March or April rather than in December.
Christ’s birth is said to have taken place while shepherds were watching their flocks at night, he notes, something that takes place at lambing-time in the spring rather than in the depths of winter. If the local inns were full, as the Gospel of Matthew insists, this would be because of the Jewish Passover, which also occurs in the spring.
Kidger concludes that Christ was born some time around March in 5 BC, taking account of the generally accepted fact that the inventor of the Christian calendar, the 6th century monk Dionysius Exiguus, was five years out in his calculations.
Ther have been several theories, including  the “star” could have been an unusual sighting of Venus, or perhaps Halley’s Comet, a supernova, or a meteor shower.
More plausible is the popular theory that what the Magi saw was a planetary conjunction, which occurs when two planets pass very close to each other in the sky, often producing a very striking configuration.
As shown in the picture above, generated by a computer program known as Starry Night, one such conjunction took place in 7 BC when Jupiter and Saturn came close to each other three times in seven months and were then joined by Mars, an event known to have been observed in Babylonia, well to the east of Bethlehem.
A more recent idea is that the Star of Bethlehem may have been an occultation of Jupiter by the moon that occurred in 6 BC, the re-emergence of the royal planet from behind the moon’s disc suggesting a royal birth.
However Kidger points out that the event would have taken place so low in the twilight sky of the region it would have been impossible to observe directly.
For his “best guess” at solving the Star of Bethlehem riddle Kidger looks to an ancient Chinese chronicle called the Ch’ien-han-shu which states that an object, probably a nova, or new star, was observed in March in 5 BC and remained visible for 70 days.
The object would have appeared in the east and remained in the sky long enough to have guided the Magi — Babylonian astrologers, according to some scholars — across the desert to Bethlehem.
“It’s hard to believe the Star of Bethlehem could have been anything else,” Kidger says of the nova, citing the coincidence in date, the duration of visibility and its position in the sky. And proof of its identity may soon be possible by looking for its telltale remains when the successor to the Hubble telescope goes online in 2011.
When a star goes nova, or supernova, if it has any planets, those planets usually become toast in the process. It may be that our birth of Christianity was heralded in by the destruction of another planet, possibly an entire civilization. As they say, God does work in mysterious ways.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9509c38',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

This morning, I took my children out front, and we placed three flags in our front yard. Each child got one little flag on a wooden stick to plant in the front garden, while mommy and daddy got the big flag to hang from the porch.
After a little discussion on why we did this on Memorial Day,” to remember those who keep us free”, my son William remarked, “ok…can we wash the car now?” (that was our next project).
Well maybe it’s a little early at nearly 4, to install some patriotism. But later when William and I drove to the hardware store together he said “Daddy, how come those houses don’t have flags? We have flags”. It was then I realized we were the only house on our entire street displaying flags today.
Good question son, good question.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea63cd550',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This photo comes come to me from NOAA’s Weather Service Forecast Office in Monterey.
This is the official USHCN climate station of record for Livermore, CA. USHCN # 44997 The temperature sensor is located in a backyard of a residence within six feet of the swimming pool.

Here is the temperature trend from NASA GISS:

The question is: can an unbiased and accurate reading of temperature be obtained in somebody’s backyard next to their pool? With NOAA siting requirements saying a minimum of 100 feet from buildings, I would assume this would apply to pools too.
I couldn’t make this up if I tried.
You can see the picture without the annotations on the NWS website with this direct link:
http://www.wrh.noaa.gov/images/mtr/cpm/4997.jpg


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5653ce6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
You know your presentation was successful when:
1) Nobody threw rotten fruit
2) People came up to me afterwards and said “I have photos I can get to you”
3) A high level official at NCDC requests a copy of my presentation “as soon as you can get it to me”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea43650d4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This mornig s session is all about drafting a set of suggestions to forward to other key members of the climate research community using the group knowledge gained from this conference. I have submitted my suggestion, and it has been accepted for inclusion in the publication. It reads:
 It has become clear that many surface weather stations, possibly a
significant number, may have undocumented biases that may or may not
be correctable using data analysis and data adjustment techniques.
After completion of weather station surveys for USHCN and other
networks, Why not identify the known good stations that have long term
records, few station moves, and no obvious microsite biases and
separate their data into a subset. Study the data and trends the known
good station subsets produce separately to see what can be learned.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea427e598',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"David Cameron might have been forced into accepting several restrictions on fracking to avoid a Commons defeat, but one thing that has never appeared in doubt is the Conservative party leadership’s commitment to the practice.   A year ago, the prime minister famously declared: “We’re going all out for shale. It will mean more jobs and opportunities for people, and economic security for our country.” Chancellor George Osborne also wrote a detailed letter to ministers asking them to make policy implementation a “personal priority”. The UK government has said that fracking has three main benefits: energy security, decarbonisation and economic growth. The government has shown clear support for test drilling to assess the economic feasibility of fracking. It has reinforced this support with a range of policies such as tax breaks; local compensation rules; pro-fracking planning guidance; and a sovereign wealth fund from shale petroleum revenues that would be invested in north England.  Yet the government has not gone all out for shale. It is part of a loose coalition of organisations which, “on average”, is tentatively pro-fracking. The coalition includes UK government departments and agencies, the three main UK political parties, would-be frackers such as Shell and Cuadrilla, industry groups such as Oil & Gas UK, and groups generating and sharing research such as the Royal Academy of Engineering and the British Geological Society.  The common element to this coalition is a wish to approve test drills, to get a better sense of the economic potential of shale gas, and support extensive regulation. Only some members of this coalition favour the “all out” strategy. The anti-fracking coalition that it competes against is smaller but has a less equivocal political position. It includes the likes of the Green Party, the Campaign to Protect Rural England, Frack off, Friends of the Earth and WWF UK. Neither has the UK government sought to take full control of shale petroleum policy. It has overall responsibility for energy and retains ownership of mineral and gas resources, but has not centralised many aspects of fracking policy.  These policy aspects are shared between devolved governments, who are responsible for developing national planning guidelines (Scotland will also soon receive powers on licensing); local authorities charged with granting planning permission for individual drilling sites; and public bodies responsible for ensuring environmental protection and health and safety. It also shares responsibility for environmental policy with the European Union.  The UK has taken responsibility for strategic issues related to energy security, finding evidence to address questions about the economic viability and environmental uncertainty around fracking, the tax and incentives regime, and the UK-wide system granting energy companies the right to operate to extract minerals. But crucially it has not sought to control the decision to approve drill sites in local areas (a Lancashire County Council decision looks likely to approve two controversial sites, but it is the council that retains the control and not the government).  This is reflected in its rather convoluted “regulatory roadmap” for private companies. It involves at least 15 steps and interaction with government and numerous public bodies, culminating in the need to satisfy local authorities that the energy companies should drill in their area. (By the way, public bodies, such as environment agencies, also implement a complex mix of EU, UK and devolved regulations.) The result is that we don’t quite know what will happen, particularly since devolved and local governments are much more hesitant to approve actual development in their areas. The UK government may be ostensibly “all out for shale”, but this is not reflected in its decision – not least the latest climbdown in the Commons, though that can at least be put down to realpolitik.  On this basis, we might expect one of three things (assuming of course that energy prices return to levels at which shale gas is economic). First, as events proceed and local areas begin to make decisions on individual sites, the anti-fracking coalition may swell to reflect a growth in opposition or the decision of local authorities to reject planning applications. This is particularly likely if tremors or earthquakes happen again close to test-drilling sites.  Second, the majority coalition may swell, but change further to reflect a greater degree of hesitant pro-fracking attitudes that are not sufficient to enable commercial fracking to go ahead. Or third, the majority coalition might become more in favour of fracking, perhaps following test drills and the gathering of evidence that suggests that regulations are sufficient and the commercial potential of shale gas is more certain. Obviously energy prices will have a vital bearing on how this proceeds. The latter outcome is by no means certain whatever David Cameron and his allies are saying in public."
"
Share this...FacebookTwitter
Image cropped from Met Office here. 
By Die kalte Sonne
(German text edited by P. Gosselin)
On June 3, 2020, npj Climate and Atmospheric Science published a study by Athanasiadis et al. 2020, in which the authors investigated the question of whether changes in the frequency of blocked weather situations in the North Atlantic and Central European region are predictable.
“Quite some nonsense”
Previously, scientists inclined towards climate alarmism had told us that CO2 would lead to more and more blocked weather situations. Quite some nonsense as it now turns out, because the blockings are more likely to be due to the 60-year AMO ocean cycle, which in turn affects the NAO. These are exciting results.
Here’s the abstract:
Decadal predictability of North Atlantic blocking and the NAO
Can multi-annual variations in the frequency of North Atlantic atmospheric blocking and mid-latitude circulation regimes be skilfully predicted? Recent advances in seasonal forecasting have shown that mid-latitude climate variability does exhibit significant predictability. However, atmospheric predictability has generally been found to be quite limited on multi-annual timescales. New decadal prediction experiments from NCAR are found to exhibit remarkable skill in reproducing the observed multi-annual variations of wintertime blocking frequency over the North Atlantic and of the North Atlantic Oscillation (NAO) itself. This is partly due to the large ensemble size that allows the predictable component of the atmospheric variability to emerge from the background chaotic component. The predictable atmospheric anomalies represent a forced response to oceanic low-frequency variability that strongly resembles the Atlantic Multi-decadal Variability (AMV), correctly reproduced in the decadal hindcasts thanks to realistic ocean initialization and ocean dynamics. The occurrence of blocking in certain areas of the Euro-Atlantic domain determines the concurrent circulation regime and the phase of known teleconnections, such as the NAO, consequently affecting the stormtrack and the frequency and intensity of extreme weather events. Therefore, skilfully predicting the decadal fluctuations of blocking frequency and the NAO may be used in statistical predictions of near-term climate anomalies, and it provides a strong indication that impactful climate anomalies may also be predictable with improved dynamical models.”

Share this...FacebookTwitter "
"

UPDATE: The national website www.junkscience.com has referenced this blog entry.
From the waaaayyyy over the top department:
The Weather Channel’s climatologist, Dr. Heidi Cullen who hosts the program “The Climate Code”, is advocating that broadcast meteorologists be denied certification (or re-certification) if they express skepticism about predictions of manmade global warming. She posted this revelation in the blog she runs on the Weather Channel website and you can read it here: http://climate.weather.com/blog/9_11396.html
She writes: “If a meteorologist has an AMS Seal of Approval, which is used to confer legitimacy to TV meteorologists, then meteorologists have a responsibility to truly educate themselves on the science of global warming.” “Meteorologists are among the few people trained in the sciences who are permitted regular access to our living rooms. And in that sense, they owe it to their audience to distinguish between solid, peer-reviewed science and junk political controversy.” “If a meteorologist can’t speak to the fundamental science of climate change, then maybe the AMS shouldn’t give them a Seal of Approval.”
Them’s scientific fightin’ words lady.
So, apparently any free speech, scientific debate, and public dialog that doesn’t agree with the peer reviewed popular scientific opinion is grounds for denying an AMS Broadcast certification?
This reminds me of Galileo and his fight with the Roman Catholic Church in 1632. Galileo wanted to publish a book Dialogue Concerning the Two Chief World Systems which totally revised the earth centric view of the universe favored by scientists, scholars, and clergy of the time and built on the work of the earlier astronomer Copernicus. Galilieo was tried and imprisoned for daring to speak out against the “consensus” of the time for what he saw as a scientific truth.
I think we would all do well to follow this maxim: “People who live in greenhouses shouldn’t throw stones”.

Open scientific debate is essential to the scientific process, to call for castigating and silencing TV weathercasters who see other evidence is not only against American free speech values, it’s unprofressional for a scientist. Freedom of speech is the concept of the inherent human right to voice one’s opinion publicly without fear of censorship or punishment.
I support Cullen’s freedom of speech to make the claim that Global Warming is entirely an affliction caused by humanity, but I don’t support her call for decertifying of proponents of alternate theory
Despite receiving over 1000 blog comments by the public, most of them harshly critical of Cullen’s call for suppressing the voices of manmade global warming skeptics Cullen has refused to retract her call for AMS decertification of broadcasters who may also be global warming skeptics but instead blamed the whole mess on “spin.‿ Here is her latest post on the controversy. No mention of the word “sorry” or mea culpa in that post.
The Weather Channel has yet to officially comment on the matter. They are most likely being very careful as they are now in the middle of a scientific and political firestorm.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea8d3e6bf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the third of a century since its founding, the Cato Institute’s scholars have issued a wealth of predictions about the likely effects of government policies and programs. While sometimes ignored or belittled, these predictions have often proved prescient.



Most famous was Joe Stilwell’s Policy Analysis published in 1982. In “The Savings & Loan Industry: Averting Collapse,” Stilwell warned that, “regardless of changes in the economic climate, numerous S&Ls will be unable to meet their financial obligations.” Few in government listened then. Through the remainder of the decade, Americans would have been better off if they had, before the taxpayers had to come up with a $500 billion rescue plan.



In 1982, Cato founder and president Edward H. Crane wrote about his recent visit to the Soviet Union. “It is a society that appears to be crumbling from within,” Crane wrote. He added, “If we can avoid confrontation with the Soviets over the next 20 years, their system should collapse of its own bureaucratic weight.” Such a prediction sounded crazy at the time. And indeed Crane’s estimate was off target.



The Soviet Union vanished, in not 20 years, but 9.



Stanley Kober, a research fellow in foreign policy studies at the Cato Institute, warned in a 1996 paper that “the terrorist attacks in Saudi Arabia, Israel, and other countries suggest that the trend in the Middle East is not nearly as hopeful as it appeared just a few years ago,” and he identified Osama bin Laden as a particular terrorist threat to the United States.



In a study he published in February 2001, Daniel Griswold wrote, “A domestic recession would reduce the trade deficit, as it has in the past, but at great cost to U.S. workers and their families.” A month later, the U.S. economy slipped into recession and the trade deficit declined in 2001 compared to 2000, after having risen in each of the previous five years. Then came the Great Recession, beginning in 2008. The trade deficit in 2009 was $300 billion smaller than in the pre‐​recession year of 2007.



In few areas have Cato scholars been more consistently correct and more consistently outside the mainstream consensus than the Iraq war. In 1999, Ted Galen Carpenter argued that “removing a thug like Saddam … is extremely ill‐​advised. It will make Washington responsible for Iraq’s political future and entangle the United States in an endless nation‐​building mission beset by intractable problems.” William Niskanen wrote in the _Chicago Sun‐​Times_ in December 2001, “Another war in Iraq may serve bin Laden’s objective of unifying radical Muslims around the world in a jihad against the United States.” In 2002, Doug Bandow warned that, “If Iraq’s forces don’t quickly crumble, the U.S. might find itself involved in urban conflict that will be costly in human and political terms.” And in March 2003, Christopher Preble argued America’s experiences with nation‐​building in Germany and Japan advise against attempting the same with Iraq. “If these ‘success’ stories reflect the model for post‐​war Iraq,” Preble wrote, “we should expect the U.S. to remain in this troubled region for many years.” Returning to domestic affairs, in March 2007, Jim Harper said in congressional testimony: “Mr. Chairman, the REAL ID Act is a dead letter. All that remains is for Congress to declare it so.” More than three years later, REAL ID, an attempt by the federal government to establish a national personal identification system, has gone nowhere, and two major implementation deadlines have passed.



In February 2009, when President Obama’s approval rating was in the mid‐​60s and most political opinion makers thought he was on the cusp of radically remaking America, Gene Healy published his first weekly column in the _D.C. Examiner_. Healy wrote, “When he fails to fully heal our financial troubles, fix health care, teach our children well, provide balm for our itchy souls, and so forth, his hopeaddled rhetoric will seem all the more grating, and the public will increasingly come to see him as the source of all American woes.” By July 2010, according to Gallup, President Obama’s approval rating had fallen to 44 percent, the lowest of his presidency, and his party was fearing considerable losses in the upcoming congressional elections.



As Healy predicted, President Obama did fail to fix health care. Instead, he ushered through Congress the ill‐​considered legislation known as ObamaCare.



Michael Cannon predicted in September 2009, six months before the bill’s passage, that ObamaCare’s individual mandate would force as many as half of all Americans with private insurance to switch to a more expensive plan. At the time, the administration insisted this was fantasy. In June, it all but admitted Cannon was right, prompting the _New York Times_ to write that “the rules appear to fall short of the sweeping commitments President Obama made while trying to reassure the public in the fight over health legislation.” Even earlier was Michael Tanner’s 2006 paper, “Individual Mandates for Health Insurance: Slippery Slope to National Health Care.” Later that same year, Massachusetts enacted health care legislation that included an individual mandate. The results have followed Tanner’s script exactly. RomneyCare’s individual mandate took effect in 2006, along with health insurance exchanges. Subsequently, 16 mandates have been added to the original list of benefits that health insurers must provide in the Bay State. Massachusetts now has the most rapidly increasing premiums in the nation. The most recent attempt to control costs, as Tanner predicted, was to simply prohibit insurers from increasing premium rates, leading insurance companies to predict that they will suffer from hundreds of millions of dollars in losses this year. In addition, wait times have increased to see both primary‐​care physicians and specialists, just as Tanner’s paper said they would.



The fact that policymakers failed to take Cato scholars’ warnings of the last 30 years to heart, makes it only more crucial that they do so in the next 30.
"
"



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea89e324b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
It appears that in the quest to save our planet from dangerous chemicals, people will blindly sign anything.  Read more about this dangerous chemical here: http://www.dhmo.org/ It’s “an odorless, tasteless chemical” that can be deadly if accidentally inhaled.

This chemical is so dangerous that a local city council in Aliso Viejo, CA put it on the agenda to ban foam containers made with it. I expect our liberal city council may soon address this danger like we’ve already done for nuclear weapons in the city limits.
Penn and Teller provide an entertaining look into mindless activism.

with a hat tip to Mark…


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea66a15b8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterOf good trees and bad trees: an unimaginable story
By Die kalte Sonne
(Text translated by P Gosselin)
We have already reported about the very different views on trees in this blog. Perhaps this phenomenon has something to do with the fact that the words environmental protection and nature conservation are slowly but surely disappearing from our language and being displaced by climate protection. Everything has to subordinate itself to this, also environmental and nature protection. Sometimes this has has had disastrous consequences.
The value of trees is in the eye of the observer or his agenda
Trees are extremely valuable carbon stores. They are true CO2 sinks. It is estimated that a large tree removes and stores about 12.5 kg of CO2 per year from the atmosphere. Actually, one would have to think, we should not only reforest massively, as Professor Werner Sinn suggested in his lecture “How we save the climate and how not“, but also preserve existing tree populations.
Of course, trees are protected, sometimes with drastic means such as in the Hambach Forest. There, however, not for CO2 storage reasons but because the activists want to prevent lignite mining. Such actions are spectacular and get through the media. So this is about good trees.
Much less attention is paid to protests by residents of Grünheide in Brandenburg, who are mobilizing against the deforestation of an area the size of 420 football pitches, which are to make way for Tesla’s new megafactory. Here too, nature is losing carbon stores, and no activist is really itching because they are bad trees. Or were there demos of Fridays For Future (FFF) or Extinction Rebellion in Grünheide?
Weird swaps in Scotland
Just as little interest in Scotland. There it has now been discovered that almost 14 million trees have had to be felled since 2000 to build wind turbines (WTGs). According to the above calculation, Scotland has thus “given up” 175,000 tonnes of CO2 reduction per year in order to save the climate. Even planting 100,000 trees, as in Scotland, is of little use, as they only replace the lost capacity to a very limited extent. Trees simply need time until they are stately and can absorb the above-mentioned amount of CO2 annually.
Foundations and access
The areas for the foundations are still the least of the evils, although in Schleswig Holstein alone, a sealed area of 3 million square meters was assumed in 2018. Approximately 1300 cubic meters of concrete and 180 tons of steel disappear in such foundations.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Image: By Mussklprozz (Own work) [CC-BY-SA-3.0 ( http://creativecommons.org/licenses/by-sa/3.0 )],via Wikimedia Commons
It is not even clear how such colossuses are to be removed from the forest later on, or how they can be removed at all. Anyone who has ever spent a holiday in France on the Atlantic Ocean knows that concrete remains, i.e. bunkers, of the 3rd Reich bunkers stubbornly refuse to decay on the coasts. Ina 1000 years probably only the bunkers of the Nazis or the foundations of wind turbines will remain.
Access to wind turbines is much more serious than the foundations, which only cover a relatively small area. Wind turbines are getting ever taller and the rotors ever bigger. The radius that the special transport vehicles now is so large that a massive quantities of trees have to make way for access roads. And since the wind turbines only have a limited lifetime, the access roads have to remain, because at some point they will have to be dismantled or maintained. The forest at this point is lost and chopped up.
German conservative CDU now poised to play along
The CDU Lower Saxony is now poised to go along with a proposal that more wind turbines in forests should be approved. Whether an impact assessment has been made here? Especially in forests, the population of birds of prey is high and one can only guess what will happen if huge rotors rotate over birds or their breeding grounds and habitat. These rotors are, as studies show, a considerable hazard for birds of prey.
Indeed the wind power lobby is trying to invalidate such studies, for example by pointing out the large number of songbirds and garden birds that are killed annually by windows, cars or cats. But if you use your common sense, here you see whataboutism in its purest form. Birds of prey very rarely die from windows, cars or cats and songbirds and garden birds rarely die from wind turbines. At the latest, when the census of seabirds in the Irish Sea – an area with a lot of wind turbines – shows that the population is declining massively, the windscreen/cat/car argument falls apart.
The same outcome, but completely different reactions
But it gets really crazy when we look at the situation in places like the Reinhardts Forest in the state of Hesse. This forest is very valuable, because it still has a virgin forest character in parts. Nevertheless, wind turbines are to be built there with all the consequences described above. Residents’ protests are being dismissed as an obstacle to technology and energy production transformation.
Yet, at the same time, the people go into collective outrage when the Amazon becomes smaller through slash-and-burn clearing. In both cases forests, biotopes and very same carbon reservoirs are lost, but with completely contrary reactions. Good and bad trees.
Used to be tranquility above the tree tops
But forests are much more. Many people pursue various activities there. A climate activist from Berlin Kreuzberg or Hamburg Ottensen may find this hard to imagine, but there are actually people who visit the forests enjoy tranquility or the sounds of nature. If the plans of wind power advocates are anything to go by, then in many forests this will soon be lost forever.
Share this...FacebookTwitter "
"
This is the USHCN climate station of record for Bainbridge Georgia. It comes to me by way of surfacestations.org survey volunteer Joel McDade. Joel wins the award for finding the USHCN station closest to an air conditioner, at 8.9 feet. That honor was previously held by Oregon State Climatologist George Taylor at just over 10 feet in his picture of Forest Grove Oregon.

In addition to the air conditioner, this USHCN climate monitoring station sports several other features:
– A building just 14.3 feet away
– Convenient close-by radiator forward parking for your vehicle within feet of the MMTS sensor
– An asphalt road within 10-15 feet of the sensor
– A mature shade tree that changes shade patterns with the season
– A station move of about 150 feet closer to the building to accommodate the new MMTS sensor cable length
The station is operated by the International Paper Company. The plot of temperature below illustrates some data gaps and jumps that may be related to station moves.

Full details on this site are at the surfacestations.org online image database


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5e3246c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter The radiative impact of CO2 on the ocean’s thermal skin layer cannot penetrate deeper than 0.01 mm. This effectively eliminates the potential for CO2 to be a driver of global warming.
According to mainstream anthropogenic global warming (AGW) science, 93% of global warming is manifested in the 0-2000 m oceans. Just 1% of global warming is manifested as a change in atmospheric temperature.

Image Source: IPCC (ipcc.fandom.com)
Consequently, for anthropogenic CO2 emissions to be a driver of global warming, CO2 concentration changes must drive changes in the Earth’s ocean heat content.
Oceanographers Wong and Minnett (2018) point out that total CO2 forcing can only radiatively exert an impact on the top 0.01 mm of the ocean’s thermal skin layer. (Human hair is about 0.06 mm thick.)

Image Source: Wong and Minnett, 2018
Problematically, the amount of solar radiation absorbed in the upper 0.01 mm layer of the ocean is just 4.9 W/m².
Thus, CO2 concentration changes may, at most, affect 0.049% of the global oceans’ thermal skin layer.
This is the total extent of the radiative impact for CO2 in global ocean heat content changes.
CO2 may therefore be ruled out as a driver of global warming.
Share this...FacebookTwitter "
nan
"

 _Global Science Report is a feature from the Center for the Study of Science, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Last fall, the press pounced on the results of a new study that found that global climate change was leading to an increasing frequency of heat waves and thus resulting in greater heat-related mortality. Finally a scientific study showing that global warming is killing us after all! See all you climate change optimists have been wrong all along, human-caused global warming is a threat to our health and welfare.   
  
Not so fast.   
  
Upon closer inspection, it turns out that the authors of that study—which examined heat-related mortality in Stockholm, Sweden—failed to include the impacts of adaptation in their analysis as well as the possibility that some of the temperature rise which has taken place in Stockholm is not from “global” climate change but rather local and regional processes not at all related to human greenhouse gas emissions.   
  
What the researchers Daniel Oustin Åström and his colleagues left out of their original analysis, we (Chip Knappenberger, Pat Michaels, and Anthony Watts) factored in. And when we did so, we arrived at the distinct possibility that global warming actually led to a reduction in the rate of heat-related mortality in Stockholm.   
  
Our findings have just been published in the scientific journal Nature Climate Change as a Comment on the original Oustin Åström paper (which was published in the same journal).   
  
We were immediately skeptical because the original Oustin Åström results run contrary to a solid body of scientific evidence (including our own) that shows that heat-related mortality and the population’s sensitivity to heat waves was been declining in major cities across America and Europe as people take adaptive measures to protect themselves from the rising heat.



Contrarily, Oudin Åström reported that as a result of an increase in the number of heat waves occurring in Stockholm, more people died from extreme heat during the latter portion of the 20th century than would have had the climate of Stockholm been similar to what it was in the early part of the 20th century—a time during which fewer heat waves were recorded. The implication was that global warming from increasing human greenhouse gas emissions was killing people from increased heat.   
  
But the variability in the climate of Stockholm is a product of much more than human greenhouse gas emissions. Variations in the natural patterns of regional-scale atmospheric circulation, such as the Atlantic Multidecadal Oscillation (AMO), as well as local impacts associated with urbanization and environmental changes in the direct vicinity of the thermometer are reflected in the city’s temperature history, and the original Oudin Åström et al. publication did not take this into account. This effect is potentially significant as Stockholm is one of Europe’s fastest growing cities.   
  
But regardless of the cause, rising temperatures spur adaptation. Expanded use of air conditioning, biophysical changes, behavior modification, and community awareness programs are all examples of actions which take place to make us better protected from the dangers associated with heat waves. Additionally, better medical practices, building practices, etc. have further reduced heat-related stress and mortality over the years.   
  
The net result is that as result of the combination of all the adaptive measures that have taken place over the course of the 20th century in Stockholm, on average people currently die in heat waves at a rate four times less than they did during the beginning of the 20th century. The effect of adaptation overwhelms the effect of an increase in the number of heat waves.   
  
In fact, it is not a stretch to say that much of the adaptation has likely occurred because of an increased frequency of heat waves. As heat waves become more common, the better adapted to them the population becomes.   
  
Our analysis highlights one of the often overlooked intricacies of the human response to climate change—the fact that the response to climate change can actually improve public health and welfare.   
  
Which, by the way, is a completely different view than the one taken by the current Administration.   
  
References:   
  
Knappenberger, P., Michaels, P., and A. Watts, 2014. Adaptation to extreme heat in Stockholm County, Sweden. Nature Climate Change, 4, 302-303.   
  
Oudin Åström, D., Forsberg, B., Ebi, K. L. & Rocklöv, J., 2013. Attributing mortality from extreme temperatures to climate change in Stockholm, Sweden. Nature Climate Change, 3, 1050–1054.


"
"

Our comment primarily concerns the Department of Energy’s (DOE) use of the social cost of carbon (SCC) in the cost/​benefit analysis of the Energy Conservation Program: Energy Conservation Standards for Small, Large, and Very Large Air‐​Cooled Commercial Package Air Conditioning and Heating Equipment proposed rulemaking. The DOE’s determination of the SCC is discordant with the best scientific literature on the equilibrium climate sensitivity and the fertilization effect of carbon dioxide — two critically important parameters for establishing the net externality of carbon dioxide emissions. It is also at odds with existing Office of Management and Budget (OMB) guidelines for preparing regulatory analyses. It is based upon the output of Integrated Assessment Models (IAMs) which have little utility because of their great uncertainties. They provide no reliable guidance as to the sign, much less the magnitude of the social cost of carbon. Additionally, as run by the Interagency Working Group (IWG) (whose results were incorporated by the DOE in this action), the IAMs produce illogical results that indicate a misleading disconnection between climate changes and the SCC value. Further, we show that the sea level rise projections (and thus SCC) of at least one of the IAMs (DICE 2010) is not supported by the mainstream climate science.
"
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




Climate change is a moral, non‐​partisan and pragmatic issue which can be addressed by solutions with multiple co‐​benefits. We urge legislators to join global business, faith, scientific, health and military leaders in acknowledging that climate disruptions are real, happening now, and requiring our nation’s leaders to act.



It is interesting that they juxtapose a “moral issue” with calls for “policies to reduce national and global greenhouse gas emissions.” Interesting, we say, because there is a soon‐​to‐​be released and incredibly compelling book written by the Center for Industrial Progress’s Alex Epstein titled _The Moral Case for Fossil Fuels_. Its main premise is that both the short‐ and long‐​term benefits of using fossil fuels greatly outweigh the risks of any climate change that may occur as the result of the accompanying carbon dioxide emissions. Epstein argues that the “moral” thing to do is to continue (and expand) the use fossil fuels:   




If we look at the _big picture_ of fossil fuels compared with the alternatives, the overall impact of using fossil fuels is to make the world a far better place. We are morally obligated to use more fossil fuels for the sake of our economy and our environment.



The primary case against expansion of current fossil fuel use involves the risk from anthropogenic climate change. However, here, the threats are overstated—especially by organizations (like many of those behind The People’s Climate March) that favor centralized government control of energy production (and most everything else).   
  
  
The sea level rise concerns that are to be described in the Hill briefing will undoubtedly fall into the “overstated” category. According to the briefing’s flier:   




“The U.S. National Climate Assessment projected that sea levels will rise 1 to 4 feet by 2100, affecting 39 percent of the U.S. population and impacting the very futures of many coastal communities and small island nations.”



We imagine that the focus will be on the high end of the 1 to 4 foot range (and beyond), even as a plethora of new science argues for an outcome nearer to the low end.   
  
  
The current decadal rate of sea level rise is about 3 mm (.12 in) per year, which would result in about a foot of sea level rise during the 21st century. There is a lot of recent research that concludes that a large increase in this rate of rise as a result of the melting of Greenland’s and/​or Antarctica’s glaciers is unlikely.   
  
  
The statistical models most responsible for the high‐​end sea level rise projections used have been shown to be questionable and thus unreliable. And finally, and perhaps most importantly, the future projection of temperature rise made by climate models (upon which the sea level rise projections are based) have been shown by a growing body of scientific research to be overestimated by about 40 percent.   
  
  
Taken together, the latest science argues that the case for rapid and disruptive sea level rise is flimsy at best.   
  
  
Undoubtedly, sea levels will continue to rise into the future, in part, from the earth’s temperature increase as a result of human carbon dioxide emissions resulting from our use of fossil fuels. Appropriate adaptations will be necessary. However, signs point to a rather modest rise in sea levels accompanying a rather modest rise in temperature—a pace at which our adaptive response can keep up.   
  
  
So long as this is remains case, the continued use of fossil fuels to power the developed world and the expanded use to help provide safe, reliable, and cheap electricity to the more than 1 billion people in the underdeveloped world that currently live without any (or very minimal) access to it is a no‐​brainer. That’s where the moral imperative should lie.
"
"

Yes, Virginia, there is a sales tax. With Republicans no longer in control of the House, Forbes says this may be the last Christmas you’ll be able to dodge sales tax by buying that $350 iPod or $1,200 laptop online.
The Internet was just coming into its infancy in 1994 when Republicans took control of the House and Senate. Republicans have been steadfast in their resistance to taxing the Internet, but they may no longer be able to prevent it.
From the article:
“With Democrats in charge… ‘The stars are lined up better,’ says Harley Duncan, executive director of the Federation of Tax Administrators, which represents state tax officials… [But] this is hardly a done deal. The 4,700-member Direct Marketing Association is fighting any new authority for the states.”
It remains to be seen if the Internet will become the next tax revenue source. One thing’s for sure, Al Gore won’t come out and say he invented Internet taxation.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea9986e9c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Global Warming is a hot topic here on Earth, but it may be the issue will be settled not here on Earth, but on Mars.  A study of the ice caps on Mars suggests it is also experiencing a warming trend. A story about new data from NASA cites a six year study by researchers at Duke University showing that Mars may also be seeing a Global Warming trend and that both the Earth and Mars are seeing changes related to solar output.


You can click on this link to get a time-lapse image of the Mars ice cap changes between 1999 and 2001. It may take awhile to load as its 1.6 megabytes in size. Those with very high speed connections can get an even larger and more detailed time lapse here which is 6.2 megabytes in size
Planetary scientists have been watching melting of deep, wide pits in the southern Martian ice caps. The melting is substantial. According to Michael Malin, principal investigator for the Mars Orbiter Camera, the polar ice cap is shrinking at “a prodigious rate.”
Now where have we heard that before? Oh, seems Al Gore said in his movie An Inconvenient Truth that our own ice caps are melting and that we’ll see a 20 foot rise in sea level as a result. Here’s a transcript of the movie if you are interested.
The scientists believe this observed melting on Mars means that there is a layer of dry ice that is evaporating off of a thicker layer of water ice. The yearly increases in evaporation may be caused by a global warming trend happening on Mars.
The most recent images in the NASA sponsored study show changes from 1999 to 2005, suggesting the climate on Mars is presently warmer, and perhaps getting warmer still, than it was several years or decades ago.
Another recent NASA announcement, that recent water flows have also been discovered on Mars, also lends credence to the idea that Mars is getting warmer. One of the mechanisms suggested is that liquid water is subsurface, and that patches of dry ice acting as “plugs” are melting, releasing the water which moves enough surface material around before it freezes again to show up on photographic comparisons.
If both Mars and Earth are experiencing global warming, then maybe there is a larger phenomenon going on in the Solar System that is causing their global climates to change, like changes in the Sun. There’s correlating evidence showing sunspot trends match temperature trends on Earth. Mars may have a similar linkage.
But perhaps there will be those arguing it’s because we’ve landed two tiny SUV’s (Space Utility Vehicles) named Spirit and Opportunity on Mars surface.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea962d53c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterLast Thursday evening in Münster, Germany, amid an atmosphere of loudly protesting students and Extinction Rebellion activists outside shouting obscenities and beating drums, prominent SPD social democrat and climate science critic Prof. Fritz Vahrenholt spoke on why Germany was headed down the wrong path with its now flailing transition to green energies, dubbed “Energiewende“.

Prof. Fritz Vahrenholt. Image: GWPF. 
Vahrenholt called the Energiewende: “An impending disaster.”
According to the Westfälische Nachricten here, “Scientists for Future activists handed out leaflets to emphasize that in their opinion the climate models of the IPCC (‘Intergovernmental Panel on Climate Change’) accurately depicted climate warming and that only trace gas CO2 was responsible for it.”
Hat-tip: Die kalte Sonne


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But Vahrenholt, former environment senator of Hamburg, refuted the claims and showed why he thought CO2 is only half responsible for climate change today and that the rest was due to natural factors like sun and clouds.
In his 45-minute presentation, Vahrenholt showed those in attendance how Germany’s foray into green energies was doomed to fail. As leaders in Germany continue to insist wind energy is able to supply the country’s energy needs, Vahrenholt – an environmentalist and one of the founders of Germany’s modern environmental movement – pointed out the major technical obstacle: the inability to store wind energy for periods of low wind.
“Not even in the grid, like one well-known Green politician claimed,” said Vahrenholt, taking a shot at Green party leader Annalena Baerbock, who once famously claimed the power grid could store energy.
German electricity prices among world’s highest
Vahrenholt also reminded that the Energiewende has made Germany’s electricity prices among the highest in the world and that it would hit the poor especially hard. “I never understood the SPD here,” said Vahrenholt, criticizing his own party. The retired professor said it would take 90,000 wind turbines to supply Germany with electricity, a number that would lead to the country having a turbine every 2 kilometers.
The Westfälische Nachrichten sums up on whether the Energiewende is going to work:
At the end of the complex, 45-minute presentation, the majority in the hall were probably convinced: it can’t. The facts and figures presented by the environmentalist were too overwhelming.”
Share this...FacebookTwitter "
"The publication of a hefty two-volume report on geoengineering by the US National Research Council represents a marked shift in the global debate over how to respond to global warming. To date, the debate has been about mitigation, with the need for some adaption because of the failure to reduce emissions adequately. The new report, backed by the prestige of the National Academy of Sciences of which the NRC is the working arm, now argues that we should develop a “portfolio of activities” including mitigation, adaptation and climate engineering. In other words, rather than presenting climate engineering, and especially solar radiation management (rebranded “albedo modification”), as an extreme response to be avoided if at all possible, the report normalises climate engineering as one approach among others. To be sure, the committee writing the report points to the serious risks likely in albedo modification, but it recommends the US set in train what would be a major research program into various forms of geoengineering, including field experiments in a technique to cool the planet by spraying sulphate aerosols into the upper atmosphere. And it endorses the deployment of various carbon dioxide removal methods as relatively benign ways to counter human emissions, arguing that the decision on mitigation versus carbon dioxide removal is largely a question of cost. This approach is riddled with political dangers. By mainstreaming geoengineering as a response to global warming the committee has left behind the argument put by Dutch Nobel laureate Paul Crutzen, in his famous 2006 article that opened the floodgates for geoengineering research, that desperate times will require desperate measures. With no talk of “climate emergencies” in the report, we look in vain for any clear rationale for the possible deployment of albedo modification. The “buying time” argument – according to which we can temporarily increase the Earth’s albedo (surface reflectivity) while the world decides to put CO2 controls in place – has fallen out of favour because any warming suppressed by a solar shield will just come back to bite us once the shield is removed. So there is a contradiction buried in the report: it recommends the initiation of a federal research program into albedo modification but does not give a plausible analysis of the circumstances in which the solar shield might be deployed. The  recommendation that “Albedo modification at scales sufficient to alter climate should not be deployed at this time” (my emphasis) is hardly reassuring. In the absence of a rationale, the report reverts to the standard scientists’ trope: we need more information. Deploying a fleet of planes to coat the Earth with a layer of sulfate particles “should only be contemplated” when we have enough data to know what effect it would have, and for this we need a lot of research. But who should do it? Who should oversee it? Who should own the results? Who would deploy the technologies? How can we ensure research is not misused? These questions, which ought to come before a decision is made to proceed with research, are either not considered or are shunted off to some vague “governance” space. Research does not take place in a social vacuum. When scientists propose to investigate technologies that would allow someone to take control of the Earth’s climate, and the research is proposed only because powerful interests have prevented a much better solution, then the research is intensely and inevitably political. So we should not let the genie out of the bottle unless we are pretty sure we can put it back. And that means no research before governance. The committee stresses its desire for public engagement but then undoes it by seeming to endorse a proposal for an “allowed zone” in which scientists alone would decide which experiments could take place. In this zone, experiments “should not be subject to any formal … vetting and approval”, so the report’s fine words about civil society engagement begin to ring hollow. An essential mistake of the report is the unwillingness to recognise (even though it has been pointed out repeatedly) that field experiments that do not change the physical environment can radically change the social and political environment. To maintain the physical-social separation the report must play down or dismiss the problem of “moral hazard”, that is, the likelihood that a substantial research program, let alone any deployment, would almost certainly reduce the political incentives to rein in carbon emissions. The committee’s answer is, as always: we need more information to make good decisions. Of course, this does not answer the concern at all but merely asserts that more information will always trump the flaws of politicians – as if the information deficit model has proven itself so effective in the past. The committee has a touching faith in the power of reason and holds it up as a kind of crucifix, declaring that “it considers it to be irrational and irresponsible to implement sustained albedo modification without also pursuing emissions mitigation, carbon removal, or both.” And yet this report has been written precisely because we live in an irrational and irresponsible world. And one has to ask how rational and responsible it is to include solar radiation management in a “portfolio of responses” to global warming, as this report does. The mandatory declaration that albedo modification “does not constitute a licence for unbounded CO2 emissions” becomes a kind of incantation to ward off the irrationalities of the actual world. One strategy for creating a rational world where climate engineering would never be misused is canvassed in the report. Social anxieties over deployment of climate engineering could be mitigated by “further research”. Negative perceptions of programs to modify the Earth’s albedo should be “extensively studied” so that they can be countered.  Sadly, the social world does not behave like the Earth system. It cannot be reduced to theorems and principles to be uncovered by further research. If we knew how to fix society through scientific study we would not be in such a mess that we are now considering an idea that Ray Pierrehumbert, climate science professor and a rogue member of the committee, describes as “wildly, utterly, howlingly barking mad”."
"
Share this...FacebookTwitterA new study (Stallinga, 2020) assesses the climate sensitivity to rising CO2 concentrations is just 0.0014°C per ppm. 
Dr. Peter Stallinga has published a comprehensive analysis of the Earth’s greenhouse effect. He finds an inconsequential role for CO2.
Doubling CO2 from 350 to 700 ppm yields a warming of less than 0.5°C (500 mK).
Feedbacks to warming are likely negative, as adding CO2 may only serve to speed up natural return-to-equilibrium processes.
As for absorption-reemission perturbation from CO2, “there is nothing CO2 would add to the current heat balance in the atmosphere.”

Image Source: Stallinga, 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A portion of Dr. Stallinga’s paper worth highlighting – which he mentions only in passing – refers to the early history of the Earth’s greenhouse effect paradigm.
K. Ångström receives little attention as a pioneer of the conceptualization that warming and cooling resul from radiative imbalances within a planetary greenhouse effect.
About 120 years ago, Ångström (1900) contradicted the oft-cited Arrhenius (1896) – the atmospheric physicist referred to by proponents of anthropogenic global warming.
Ångström suggested Earth’s greenhouse effect is already saturated in its current (1900) state, and therefore increasing CO2 will have “no effect whatsoever” on climate (Stallinga, 2020).
Ångström’s conclusions were largely ignored.

Image Source: Arrhenius, 1896 and Stallinga, 2020
Share this...FacebookTwitter "
"The opah, or moonfish, a large colourful fish living across the world’s oceans, has been found to have a warm heart and maintain a high body temperature, according to a report in the journal Science. It’s a zoological curiosity and a remarkable evolutionary development for fish. In the cold darkness of the deep sea there is a clear advantage to being warm-blooded and able to move faster than all the other creatures in order to hunt them down or to avoid being eaten. Mammals such as seals or whales exploit this to great effect. They take a big breath and dive down, insulated from the cold by a thick layer of blubber, to snatch live food such as squids, fish and shrimps from the depths. Until now it was thought that fish couldn’t keep warm in this way because instead of breathing air they extract oxygen directly from the water through their gills. The advantage of this is obvious: fish can stay underwater indefinitely. However, although their blood may be warmed by muscle activity on every circuit of the body as it comes gushing out of the heart it goes directly into the gills and is instantly cooled to ocean temperature.   The gills are intricate oxygen exchangers. A tiny membrane one thousandth of a millimetre thick is all that separates the blood and the sea, which ensures instant transfer of oxygen into the red blood cells. Heat flows faster than oxygen, so no matter how much heat the fish might be generating, its blood is automatically chilled with every heart beat. The opah (Lampris guttatus) has evolved a unique solution to this problem. A team from the NOAA SouthWest Fisheries Science Center in San Diego, led by Nicholas Wegner, analysed fish collected off the coast of central and southern California and discovered they had a special insulated network of blood vessels between the heart and the gills. These vessels act as a heat exchanger in which warm blood from the heart reheats oxygenated blood leaving the gills before it goes to the body. In this way heat is retained and not dissipated into the ocean.   This enables the opah to maintain a body temperature 5°C higher than the surrounding water and to dive 500 metres below the surface without cooling down. An insulating layer of fat in the skin keeps the heart, brain, muscles and vital organs warm. This discovery is surprising since the opah is large and conspicuous; indeed, it’s already a favourite in fish markets and restaurants. Wegner and his colleagues deserve great credit for recognising and describing in detail the specialised gill heat exchangers that have been hidden right under the noses of fishermen and chefs for centuries.   The opah is shaped like a flattened disc with bright red fins. It grows up to two metres long and can weigh up to 80 kilograms. It’s a solitary fish, never caught in large numbers and is found in all oceans except polar seas. It swims by continuously flapping its pectoral fins in a similar way to the wings of a bird – and it is the energy from these muscles that provides most of the heat.  It has long been known that certain high-performance fishes such as sharks, tuna and swordfish can warm some muscles, the brain or their eyes using a dense web of warm and cold heat exchanging blood vessels around the area in question. However their blood is still cooled to ocean temperature each time it passes the gills, as in all other fishes. With its heart and all its vital organs working at an elevated temperature, the opah is the first fish that can be regarded as truly warm-blooded.  It is intriguing to speculate whether this is a new evolutionary trend for fish that in future might emulate the warm-bloodedness of birds and mammals. For most fishes living in tropical seas this adaptation is not necessary; the warm water temperature is ideal for life. But for the opah, which wants to stay down deeper for longer in order to hunt squid in cold waters, the warm-blood adaptation helps it outcompete partially heated rivals like the Albacore tuna. The mechanism can only work for large-bodied fish with space for insulation, meaning heat loss to the surroundings can be controlled. Even with specialised heat-retaining gills like the opah has, a small fish the size of a mouse would quickly cool down, the heat absorbing capacity of water is too great for any small animal to retain body warmth.   Even the opah is not able to compete with warm-blooded diving foragers such as penguins and seals, or whales in the polar seas. The fish is a zoological oddity belonging to a group that appeared in the last 100m years at the same time as mammals and birds evolved. We cannot know if the fossil species were warm-blooded and if we search further we may find other species with similar adaptations."
"

In his May encyclical, Pope Francis captured reader interest with an appeal to the deep sense of awe stemming from our attempts to comprehend complex natural processes of geology, biology, climatology, and others that comprise our ecosystem.



Unfortunately, interwoven with the science are familiar Malthusian ideological themes: excessive consumption by wealthy nations is responsible for climate change and the world’s poor are negatively and disproportionately affected’.





It is now within our power to export energy to poor, friendly nations and trading partners to help them keep their lights on, warm their homes, refrigerate their food and power their industry.



Not to worry though, because the encyclical follows up with matching ideological solutions. It implies that wealthy Western nations need to: quickly restrict and eventually eliminate all fossil fuel consumption, rush to develop renewable energy resources to power the Earth and clean up the environment, enable third world nations to eventually begin using renewables in their own grid, and, of course, foot the bill for all of it.



Part of the encyclical’s theme actually mirrors EPA’s recently published Clean Power Plan (CPP). Both documents attempt to foist expensive, unreliable, and unworkable “renewable” solar and wind power upon situations where it will not deliver for either wealthy nations or third world countries.



Pope Francis should realize we are truly blessed with our abundant energy resources. Just this year, the United States became the world’s top producer of oil and gas, and there is no end in sight to the vast energy supplies contained within our country and in other nations. Oil and natural gas production, particularly from the enormous U.S. shale‐​bearing basins, are at record‐​breaking levels and prices are continuing to drop — an enormous help to the poor, who spend an inordinate amount of their earnings on energy.



Now, for the first time in many years, we are taking more control of our energy and environmental future, and that of our allies. Supplies of oil and gas appear to be abundant worldwide. Thankfully, using cleaner natural gas in place of coal for power generation has cut carbon emissions by over 50 per cent, and that number is continuing to fall.



It follows that other countries are slowly gaining the extensive knowledge required to produce from shale, and some undeveloped countries can now see themselves participating in the “shale revolution” and producing sufficient energy to build their economies and address their national dreams and desires, just as we have done in the U.S. and in other Western nations.



As the Pope surely knows, the scriptures (Psalm 104:24 for example) teach that mankind is endowed with the wealth from the earth — from the rocks, surface waters, lifeforms, oceans, and atmosphere — and that we derive various forms and quantities of energy from each of these environments to sustain our lives and our progeny. It would appear that the papal encyclical’s discussion on man’s use of energy resources contains a major contradiction that must be addressed.
"
"India’s heatwave again highlights just how seriously extreme weather conditions threaten our ability to put sufficient nutritious food on all our plates.   Headlines have focused on the human deaths – at least 2,500 at last count – but a heatwave can hit farming too. There are reports of scorched crops and livestock struggling to survive in temperatures of 40C or more. More than 17 million chickens have died so far, leading to rapid price increases.   What is not yet so clear is the effect of the current extreme conditions on crop yields and food supply later in the year. Crop scientists the world over are grappling with these questions – but can their work can really protect food security in the face of extreme climate challenges? Extreme weather like that India is experiencing at the moment presents crops not just with extreme high temperatures, but also with drought. Of those linked challenges, drought is the easier of the two to deal with, at least at first sight. Farmers have been watering crops for millennia, but the scale of modern food production means agriculture is already a major user of freshwater resources in many parts of the world, competing with the demands of other industries and, of course, the need for adequate safe drinking water. Irrigation water has to come from somewhere. That might be from lakes and rivers, perhaps diverted into reservoirs, or from groundwater. However all these sources are already under intense pressure, both in India and elsewhere – California is another very topical example.   Taking too much water from those sources has knock-on effects on rivers and wetlands.  Over-extraction of groundwater can also allow sea water to seep in to groundwater, reducing its quality for both people and crops. So, while irrigation has a part to play, we have to recognise that water is a limited resource  in much of the world, and that we need to use it as efficiently as we possibly can.  Poorly managed irrigation, especially using poor quality water, also contributes to the build-up of salts in agricultural soils. Ironically, trying to deal with the heat and drought can introduce a third threat to food production, saline soils.  So, beyond irrigation, what options are there? Producing crops that can grow and yield in the face of high temperature, drought and salty soils is a top priority for crop scientists around the world.   Some employ modern genetic modification techniques to produce new crop varieties that can cope with these harsh growing conditions. There are teams around the world manipulating specific crop traits to do this, but the problem lies in those three words: “specific crop traits”.  GM has given us the means to modify single genes. Whatever your views on GM crops, there is no doubt that changing a single gene can produce crops that are resistant to a herbicide or attack by a particular pest. But heat, drought and salinity damage crops through a range of mechanisms. You may be able to change a gene to reduce one type of damage but that’s just one piece in a complex puzzle.  You could look to combine different traits. Improving root growth for instance can allow a crop to access water deeper in the soil. Leaves might be modified to reduce water loss or to reflect more light to reduce the heat load.  Or plant chemistry might be changed, for example to help the plant deal with cellular damage caused by extreme heat, drought or salinity.   Combining these and other possible modifications might ultimately produce much more robust crops. For example the International Rice Research Institute (IRRI) is using these approaches to develop drought resistant rice, but that remains a huge technical challenge and is certainly no “quick fix”.   This “bottom up” combination of specific characteristics actually has many parallels with traditional breeding – crossing existing crop varieties to combine good characteristics and then screening thousands of offspring under challenging growing conditions to identify those that have improved performance. Again, that’s possible, and is being done now to produce new heat resistant maize varieties for Asian farmers, but it certainly isn’t a quick or easy process, even with modern methods that can accelerate traditional breeding. Crop scientists are also looking to alternatives to these genetic methods.  For example, we now understand how roots perceive drying soil, and then communicate that to the shoot to induce water-saving changes in the leaves.  That understanding has led to new water-efficient irrigation that delivers “more crop per drop”, an approach that can be deployed at very low cost even by small farmers.   Understanding how plants regulate their response to heat stress and salinity also provides routes by which we might intervene to boost these responses – a little like boosting our immune system to help prevent disease rather than cure it. There is a great deal of promising research here, and hints that a new generation of anti-stress agrochemicals may not be too far away, many based on plant’s own natural defence systems. Increasing crop losses are one symptom of the weather extremes which are becoming ever more commonplace.  Advances in crops and agricultural methods help reduce the severity of that symptom, but we have to be realistic about the limits.  Even the best application of crop science can’t make the world’s staple crops able to cope with every extreme of weather.  Simply dealing with the symptom is useful, but it can’t be a substitute for a cure to the underlying cause. While we can’t simply link the current heatwave in India with climate change, the message surely isn’t all that hard to read.  Let’s hope that’s a message which is heard in Paris this Autumn."
"

Tuesday on page 7A of the Enterprise Record there was a full page ad for the Oriental Buffet at the corner of East Avenue and Esplande that touted a copy of the most recent Butte County Health Department inspection with the words in bold “Compliance Achieved” on the newspaper ad.
You may remember the previous restaurant left an indelible mark in the minds of many Chicoans when it was closed down over a year ago due to massive health violations. Here is the ER Article.
Everybody deserves a chance to succeed, but I have to wonder about the wisdom of opening a door like this by putting your health report in a newspaper ad because it invites people to take a further look. It was the topic of my morning discussion group on Tuesday, so I decided to look for myself.
You can see Food Facility Inspection Reports for the Chico Area online here
And the inspection reports starting 5/07/07 for the Oriental Buffet are here:
( you’ll need Adobe Acrobat PDF reader to view these – its free here )

Oriental Buffet, 2539 Esplanade, Chico 05/07/07 Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/08/07 Re-Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/09/07 Re-Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/11/07 Re-Inspection 
On the first inspection on 5/07/07 there were 7 major violations and 14 minor ones, for a total of 21 violations. The inspector made 22 notations on the issues filling two pages. The next day on 5/08/07 they were down to 4 major violations and no minor ones. On 5/09/07 they were down to 3 major violations. On 5/11/07 they finally achieved “compliance”. The restaurant has been open since April 8th.
But I have to wonder, compliance for how long? You have to wonder that when a restaurant runs a full page ad touting “compliance” given the visually dramatic stigma the building has attached to it maybe the owners don’t fully understand what they are up against. Like I said, everybody deserves a chance to succeed, but perhaps a different theme would be the way to do it in this case.
To be fair though, I’ll point out that the inspection reports show that Egg Roll King on Palmetto needed 4 attempts to reach compliance this year , as did Gen Kai on Pillsbury, and Big Al’s needed 4 last year and so did Rice Bowl this year, and so did Sin of Cortez. Thai House on Broadway needed 5 inspections this year.
The all time high was Happy Garden on Cohasset with six consecutive inspections required last year before compliance was acheieved.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea61e5f6c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Wildfire is an integral part of the Earth system and has been for over 400m years. It is also an important and natural part of many of the world’s ecosystems. Indeed, some ecosystems, such as savannas, would not exist without fire – although others, such as the rainforests, cannot survive with wildfires and so work to maintain a damp climate. We have evidence of controlled fire from more than 400,000 years ago. But ascertaining the onset of our ability to kindle fire is more difficult. We certainly know it existed 40,000 years ago, but potentially as far back as 400,000 years ago. Over several thousand years, human populations have used fire to alter landscapes, hunt for food, and for comfort and to prepare food in the home. We have learned some control over fire and also how to respect it. But over the past hundred years or so, there has been a major transition in how we view fire –- what researcher Stephen Pyne has termed the “pyric transition”. This change, coupled with climate change, is behind the current wildfire crisis engulfing mansions in Hollywood and threatening lives in major towns and cities, not only in California but across the world. Although this problem has been widely [discussed in the scientific literature], many lessons remain to be learned by developers and residents living where wildfires are an environmental reality. As towns and cities have become larger, fire has been excluded – and if it breaks out at all it is carefully controlled before it can spread. As a result, urban populations have lost their understanding of fire and fire has been demonised. This demonisation has become worse as urban dwellers blur the boundary between human habitation and wilderness, expanding into remote areas and, in many cases, into flammable vegetation. We now face a situation in which all fires get rapidly extinguished when to let them burn may sometimes be the best option. This is because extinguishing fires in flammable landscapes can produce a build up of fuel, meaning future fires may be larger and more intense.  We need to learn about the biology of fire responses, which is key to recognising fire-dependent versus fire-sensitive plant species and ecosystems which exclude or cultivate fire. This is central to devising appropriate fire management systems and to deciding where we build. The twin problems of flammable non-native invasive plants and climate change are posing a bigger challenge to fire policy across the world. Flammable plants are spreading into vegetation which is unable to recover from wildfires, such as in the Saguaro cactus (Carnegiea gigantea) areas of the southern US. As a result, this ecosystem may cease to exist within only 20-30 years. In many cases –- even in areas that burn regularly –- non-native plants can cause significant problems, such as with cheatgrass (Bromus tectorum) in North America and Eucalyptus plantations in Portugal. Many of the non-native grasses were originally introduced into new areas as feed for cattle but have spread uncontrollably before it was appreciated that they were particularly flammable and could alter fire regimes. The relationship of fire with the evolution of plants and animals is a long one and many have adapted to a fiery landscape.
It has even been suggested that some plants encourage fire, by being more flammable but also by taking advantage of fire when it occurs by rapidly expanding in to the vacant space. Where wildfire is a normal part of the landscape, such as in the western US, earlier spring snow melt and a longer dry – and hence, fire – season thanks to climate change have combined with the spread of invasive grasses and people building in this flammable landscape to produce more frequent and larger fires. Fire cannot be excluded from these landscapes and we must learn how to live in fiery environments. We need to plan more carefully, not only regarding building materials, firebreaks and transport infrastructure, but also in how to maintain safe habitation in flammable landscapes. Attempts are being made to create fire-wise communities, but clearly we have some way to go and time is not on our side. Even in the UK, we face considerable challenges with the changing climate. Surrey is the most wooded county in England with a large population and built environment. A small heathland fire could easily turn in to a much more significant crown fire, threatening people and property. In such places, we need to plan for fire even if fire is not yet an everyday occurrence. Above all, we need to rethink fire . It has always been with us and will be on Earth when we are all long gone. We need to learn lessons from more than 400m years of fire on Earth if we are going to cope with fire in the future. We may kid ourselves that we can control fire, but wildfire is an ancient force that will easily escape our attempts to tame it."
nan
"

Beginning with the Carter “hit list” and continuing with the fiscal conservatism of the Reagan administration, westerners have been obliged to realize that the days of concrete and steel solutions to water problems are gone. In stressing the West’s need to adjust to the new realities of water, Gov. Richard D. Lamm of Colorado described the change that has taken place as follows:



When I was elected governor in 1974, the West had a well‐​established water system. … Bureau [of Reclamation] officials and local irrigation districts selected reservoir sites and determined water availability. With members of the western congressional delegation, they obtained project authorization and funding. Governors supported proposals, appearing before congressional committees to request new projects, and we participated in dam‐​completion ceremonies.



In 1986, the picture is quite different. The boom in western resources development has fizzled, though tourism remains an economic mainstay.… Congress, including members of the western delegation, has to worry about how to cut spending, not which [water] projects to fund.… Farmers are trying to stay in business and are recognizing that their water is often worth more than their crops. Policymakers recognize that the natural environment must be protected because it is a major economic asset in the region.[1]



The current political, social, and economic climate is ushering in a whole new era in western water. In the face of efforts to curtail runaway government spending and protect the environment, water institutions must foster the conservation and efficient allocation of existing supplies. They must also take water’s growing recreational and environmental value into account. The crucial question is, can the current water institutions meet today’s requirements?
"
"
Share this...FacebookTwitterIt’s been acknowledged by mainstream scientists for years now that at certain locations on planet Earth, rising carbon dioxide levels cause cooling. It’s now been determined that rising CO2 also causes “negligible” cooling (or warming) depending on the season.
A few years ago a seminal paper (Schmithüsen et al., 2015) was published in Geophysical Research Letters that indicated raising the concentration of CO2 causes a negative greenhouse effect, or cooling, in central Antarctica.
The forcing from the CO2 greenhouse effect ranges from -2.9 W/m² to +1 W/m², and the forcing for the Arctic (central Greenland) is said to be “comparably weak”.

Image Source: Schmithüsen et al., 2015
Now scientists have found that CO2 – to the extent that it has a “negligible” influence on temperature – causes the climate to cool from winter to summer and to warm from summer to winter.

Image Source: Lightfoot and Mamer, 2018
For the most part, CO2 varies due to temperature and water vapour level changes. The variance can range from 403 ppm during the drier winter to 377 ppm during the summer.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Lightfoot and Mamer, 2018
Similar seasonal CO2 variability can be found in pristine cave environments.
A paper published earlier this year (Al-Manmi et al., 2019) also finds CO2 rises to 756 ppm in winter but drops to 484 ppm in summer.
So observations indicate higher CO2 concentrations are linked to cooler temperatures, not warmer temperatures.

Image Source: Al-Manmi et al., 2019
Nowhere do these observations support the paradigm that says real-world temperature (and water vapour) changes are driven by variations in atmospheric CO2 concentrations.
If anything, it’s the other way around.
Share this...FacebookTwitter "
"

You may have heard or seen that I donated the equipment and continue to provide the high bandwidth server to carry the City Council Meetings, School Board, Planning Commission and other public meetings via live Internet Webcast as a public service.
Councilman Larry Wahl and I worked together on this project to make it become a reality, and I was pleased to announce its operation in September of 2005. It was a fun and useful project since many people can’t get cable channel 11 to see public meetings.
Well tonight as I blog this over a glass of wine, there’s some new personal satisfaction in that I’ve sucessfully completed a major test that will make this medium even more valuable for the citizens of our fair city.

It all started last week when Kris Koenig of the Chico Observatory asked me if I could run a live webcast to cover this weeks Northern California History Museum Cosmic Hike lecture series done in conjunction with the Chico Community Observatory. This weeks topic was about the Sun and Global warming, so naturally I said “absolutely”.
Now if you have ever said yes to something before fully understanding what you just committed to you’ll realize this is why parents tell their kids “don’t volunteer” when they go into the Army.
I figured, “hey no sweat”. I’ve done it before and I can do it again. Well, in the world of Internet connectivity, thats a whole different animal. I should have known better because last week I was knee deep in another computer problem when somebody suggested the Occam’s Razor solution, (the simplest answer is the most likely solution) to which I replied: “Occam never owned a computer!”.
Enter the CARD Center on Vallombrosa. They have a broadband network, should be easy to connect up my streaming video software and away we go…instant live broadcast right? Wrong.  Like many organizations, they have a firewall. A big one, and…it requires login to even do web browsing. No connect and go here.
My previous setup for the City Council Chambers used a fixed public IP address…the simplest most direct way to connect. But its also dangerous, as its like setting up a lemonade stand on Highway 99 and 149 interchange. You are likely to get run over just sitting there unless you know what you are doing. In my case it was designing an “invisible” server to connect the Cable 11 video feed to the Internet. Firewalls are designed to protect the foolish from the “raw” Internet and its vagaries of hackers, viruses, spyware, and trojans… but they also make life miserable when you want to do something other than simple web browsing and email.

So anyway, to make a long story short after four days of email, support calls, testing, alternate testing, testing again, reporting results, trying new things, etc I still didn’t have a working solution for Thursday night’s CARD center event. But I was getting close. I’d solved one Microsoft induced problem, that of a network card driver that didn’t like certain types of network traffic, but my network engineer and I were still butting our heads up against the CARD firewall problem.
Today, with help from a programmer, we tried a new setup and voila’ …all was right with the world again. We got it working at the CARD Center.
Not only can I now stream video from wired connections like the CARD center, but now I can stream video from almost ANYPLACE that has wired or wireless “WiFi”connectivity.

The little picture above of color bars may not say much to you, but to me it speaks volumes, because it was captured at the Market Cafe restaurant that has a firewall, PLUS a Wireless Encrytion Protocol. They use a WEP key that they give customers to logon. AND its traveling all the way to Arlington Texas where I have a rented high bandwdith server and back to the laptop on the bar again. it’s the worst case scenario. I brought in my laptop and set it up on the bar with my portable NTSC Test signal generator and a USB video capture device. And by golly…it works! I’m blogging this entry from the bar too.
So what does this mean?
Well it means that I can now broadcast ANY live event in Chico or wherever, as long as there is some kind or wired or wireless Internet connection. For example, I could broadcast concerts in the new City Plaza, I could broadcast from Laxon auditorium, I could broadcast from Starbucks, Bidwell Perk, Moxies, local schools, courtrooms, backyard BBQ’s, concerts, …you name it.
But wait…there’s more. Not only that, but now I have the ability to simultaneously record the live webcast and make it available for playback later. Did you miss last night’s City Council meeting where somebody suggested pushing conservative counselors out of Enloe’s Flightcare helicopter because “they are going to lose the next election anyway”…no problem, log on and play it back. Just joking, that never happened though something like it once did at a planning commission meeting.
The Internet world just got a whole lot bigger, look for fun stuff to come.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea891ad7d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The UK aviation industry has pledged to cut its net carbon emissions to zero by 2050 – despite still planning for 70% more flights over the next three decades. Members of the Sustainable Aviation coalition, which includes most major airlines and airports, as well as aerospace manufacturers, will sign a commitment to reach net zero by mid-century. More than a third of the proposed net reduction will be achieved through offsetting.  A “decarbonisation road map” will be published outlining how aviation can cut its carbon footprint – replacing a previous road map that only committed the industry to halving emissions over the next three decades. The plan sets out potential reductions coming from smarter flight operations, and new aircraft and engine technology – including some yet to be invented. Modernising airspace and developing sustainable aviation fuels will also contribute to reducing pollution. About 25.8 million tonnes of CO2, out of 71.1 million tonnes set to be created annually by the UK sector, will need to be addressed through what Sustainable Aviation calls “market-based measures”, or offsetting. The coalition forecasts that sustainable jet fuels, which are yet to be employed commercially, could meet almost a third of UK’s aviation fuel demand by 2050. The transport secretary, Grant Shapps, described the commitment as a huge step forward in creating a greener future. He added: “Aviation has a crucial role to play in reducing carbon emissions, and with the help of new technologies, renewable fuels and our continued international cooperation … we’ll be able to strike that balance.” Neil Robinson, the chair of Sustainable Aviation, said: “Climate change is a clear and pressing issue for people, businesses and governments across the world. We know aviation emissions will increase if decisive action is not taken, and that’s why UK aviation today commits to achieving net zero carbon emissions by 2050, through an international approach, working with governments around the world and through the UN.” However, Greenpeace dismissed the move as “greenwash”. John Sauven, its UK executive director, said: “This whole strategy is a flight of fancy. Carbon offsetting is simply an excuse to carry on with business as usual while shifting the responsibility to cut emissions to someone else, somewhere else, and some other time. It’s greenwash pure and simple and ministers should be wary of lending it any credibility.” British Airways’ owner IAG has already committed to net zero carbon emissions by 2050, while easyJet has gone further by already offsetting all flights."
"

China’s born‐​again planners, led by President Jiang Zemin, are addicted to the idea that social and economic order areimpossible without the firm hand of the state. The Ninth Five‐​Year Plan (1996–2000) reconfirms the Chinese CommunistParty’s (CCP’s) faith in socialism and distrust of capitalism. The plan embraces the illusion that it is possible to revitalizestate‐​owned enterprises by a change in management rather than ownership. 



To think that China’s ruling elite can control an economy of 1.2 billion people is a fatal conceit. The failure of central planningand of communism throughout the world is testimony to the ill‐​fated desire to engage in social engineering. Indeed, recentstudies have shown that those countries that have fostered economic freedom have experienced greater wealth creation thanhave those that have failed to protect economic liberties. And as people have acquired greater economic freedom, they havedemanded other significant rights, including the right to participate in political life. That demand is exactly what worries China’shard-liners. 



Institutional innovation in China since 1978 has produced the fastest economic growth in the world and enlarged opportunitiesfor many people, but it has not eliminated the main obstacle to China’s future prosperity — the CCP. The party’s monopoly ofpower has been eroded by the market‐​based reforms of China’s paramount leader Deng Xiaoping, but with the Deng eraending, China is at a crossroads. The nation must decide whether to deepen reform and risk pressures for political change orslow reform and risk alienating China’s newly emerging middle class and fomenting social unrest. The dilemma is complicatedby the advent of democracy in Taiwan and the return of Hong Kong in 1997. 



**China at a Crossroads**



In less than two decades China has become one of the top 10 trading nations in the world. Over the past decade nearly $90billion in direct foreign investment has flowed into China’s emerging markets. If the Chinese economy continues to grow at anaverage annual rate of 9 percent, it could soon become the world’s largest. 



The challenge for China is to develop the hard institutional infrastructure of a market economy. The centerpiece of thatinfrastructure is a rule of law that protects property rights and limits the power of government. Therein lies the difficulty, for theCCP is unlikely to give up its power and let freedom reign. Nevertheless, there are internal and external forces at work thatmay push China in the direction of greater liberalization and democratization. Internally, China’s reformers have created aneconomic space that allows individuals the freedom to improve their living standards outside the state sector. Externally,China’s “open‐​door” policy has allowed foreign competition and know‐​how to help the nonstate sector grow. In the process,new business practices have evolved along with legal norms associated with a market‐​liberal order. 



The Ninth Five‐​Year Plan may slow China’s march toward the market, but it won’t stop it. The forces for change are toostrong. And the further the market advances, the more costly it will become for the CCP to try to reverse it. Yet it is clear fromChina’s saber rattling on the eve of Taiwan’s ascension to democratic rule that China’s authoritarian rulers are willing to incurconsiderable economic losses to protect their positions of power and privilege. That willingness is further evidenced by China’svow to crush any democratic movement in Hong Kong after 1997. 



The dilemma facing Western leaders is whether to contain China and risk military confrontation or to peacefully engage Chinawith the knowledge that the best way to foster human rights is by opening markets and cultivating exchanges — not by using theblunt instrument of economic sanctions or the rhetoric of China bashing. 



In evaluating their options, Western leaders should heed the lesson of Taiwan — a police state that turned into a free‐​marketdemocracy because economic liberalization led to political liberalization. Taiwan’s leaders were willing to experiment withinstitutional change and had the courage to let the people speak — in the market and in the polity. 



China, too, has created a new economic space but has resisted political change. Even so, there is reason to believe China mayfollow Taiwan’s “quiet revolution.” According to Lee Teng‐​hui, Taiwan’s first democratically elected president, “Vigorouseconomic development leads to independent thinking. People hope to be able to fully satisfy their free will and see their rightsfully protected. And then demand ensues for political reform.” Thus, he predicts, “The model of our quiet revolution willeventually take hold on the Chinese mainland.” 



**Deng’s Experiment**



The key to China’s progress has been its willingness to allow institutional change on a trial‐​and‐​error basis and to promotesuccess. Like Taiwan, China has reduced the relative size of the state sector by cultivating the nonstate sector, not byprivatizing large state enterprises. State‐​owned firms now account for only 25 percent of China’s total output (includingagriculture and services), and their share of industrial output has fallen from 80 percent in 1978 to 40 percent today. 



On the heels of the failure of central planning, Deng had no grand vision for institutional change, but he was willing toexperiment. His guiding principle was, “Once we are sure that something should be done, we should dare to experiment andbreak a new path.” 



Deng began to break a new path in 1978, when he launched his agricultural reform. Communal ownership of land wasabolished and a system of contractual relations was introduced through the “household responsibility system.” Rural familieswere allowed to hold long‐​term leases and acquired the right to use the land at their disposal. They could sell their crops in theopen market, provided they first satisfied the state quota. Under the new incentive structure, farmers increased production andbegan to invest their profits in town and village enterprises (TVEs), which are beyond the reach of state ministries. 



Although TVEs are legally owned by local governments, individual households are allowed to share profits, hold(nontransferable) shares, and receive dividends. Wages are tied to profits, and the managers of TVEs face hard budgetconstraints, unlike the politically motivated managers of state‐​owned enterprises (SOEs). As a result, TVEs have mushroomedwhile SOEs continue to whither on the vine of state subsidies. 



In addition to creating new ownership arrangements, Deng’s reforms decontrolled prices, opened China to the outside worldthrough trade liberalization and the establishment of Special Economic Zones, devolved power from the central government tolocal governments, and instituted a system of fiscal contracts that limited Beijing’s share of tax revenue and provided localofficials with an incentive to promote markets — a system Yingyi Qian and Barry Weingast have called “market‐​preservingfederalism.” Those institutional changes resulted in a parallel economic structure to compete with the SOEs, reduced thecentral government’s share of tax revenue from 60 percent in 1978 to 40 percent in 1993, and helped weaken the centralgovernment’s grip on everyday life. 



Those reforms, however, have failed to create a genuine market system founded on the principles of private ownership andfreedom of contract. The goal of China’s born‐​again planners is not market liberalism but market socialism. The resultant lackof clear rules at the enterprise level and attempts to plan the market are, in the absence of a constitution that protects propertyand contracts, reflections of what F. A. Hayek aptly called the “fatal conceit” of socialism. 



**Revitalizing Civil Society**



The quiet revolution that has been taking place in China’s economy since 1978 is combining with the information revolution tostrengthen the fabric of civil society and weaken the CCP. As China has expanded the freedom to earn a living outside thestate sector, individuals have gained greater control over their lives. In its 1994 report on human rights, the U.S. Department ofState noted the connection between economic rights and human rights: “A decade of rapid economic growth, spurred bymarket incentives and foreign investment, has reduced party and government control over the economy and permitted everlarger numbers of Chinese to have more control of their lives and livelihood.” 



People are learning how markets work by participating in the growing nonstate sector and by engaging in foreign trade. As themarket has replaced Marx, newly acquired ideas and wealth have given rise to a spirit of independence and to a rebirth of civilsociety, especially in China’s southern coastal provinces. Commenting on China’s cultural transformation, Jianying Zha writes inher book _China Pop_ , 



The economic reforms have created new opportunities, new dreams, and to some extent, a new atmosphereand new mindsets. The old control system has weakened in many areas, especially in the spheres of economyand lifestyle. There is a growing sense of increased space for personal freedom [so long as people stay out ofpolitics].



Anyone who has visited China and seen the vibrancy of the market, the dynamism of the people, and the rapid growth ofurban areas will concur with Zha’s cautious optimism. 



New towns and cities are evolving naturally as people flee the countryside for improved living conditions and the chance tostrike it rich in the nonstate sector. Villages that were once small fishing centers along the southern coast are now booming withthe flow of trade and people. The new urban centers, such as Shishi in the province of Fujian, are characterized by the market,not the plan. Their model of development, writes Kathy Chen of the _Wall Street Journal_ , is “small government, big society[ _xiao zhenfu, da shehui_ ] — which advocates less involvement by cash‐​strapped governments and more by society.” 



Ambitious young people want to become capitalists, not communists. A recent survey found that young people ranked beingan entrepreneur first among 16 job choices and employment with the national government eighth. Freer labor markets have ledto a growing demand among college students for business courses, and universities are responding. The CCP has lost much ofits credibility and is no longer the major route to success. 



The freedom to trade is an important human right in China. As trade expands, there will be a growing middle class with a largestake in China’s future. Moreover, China’s high savings rate gives all those who sacrifice current consumption and invest theirearnings in the nonstate sector a strong incentive to further depoliticize economic life. The formation of economic and civilsociety will lead to a natural call for greater participation in political life. Yet as long as the CCP stands in the way of thespontaneous market order, controls the flow of information, and prevents free association, the future of China’s civil societywill be in jeopardy. 



**Institutional Change and Democratization**



If democratization is to proceed in China, the government must continue to allow experimentation and new forms ofownership. Yuan Mu recently articulated the key role of ownership reform in the Beijing press: “We should discover the bestmodel for ownership by the whole people [notice the bias against privatization], so that they will genuinely become the mainbody of market competition and operate with vigor and vitality in accordance with the rules of the market economy.” 



Those rules will evolve as individuals grope for ways to lower the costs of exchange and expand markets. In _China Pop_ , Zhaquotes Liu Ge, a lawyer trained in both China and the United States, as saying, 



Gradually, there will be more laws and rules; the market will be more mature, more compatible withinternational standards, the competition more fair and open. Then, China will have been structurallytransformed! Political change will come after that.



According to Zha, “A lot of the educated urban Chinese … echo this way of thinking.” There is reason to believe, therefore,that institutional change in China will bring about what Princeton University professor Pei Minxin has called “creepingdemocratization.” 



Pei points to the upward mobility of ordinary people, occasioned by the deepening of market reform, and to the positiveimpact of China’s “open‐​door” policy on political norms. In his view, public opinion and knowledge of Western liberaltraditions, such as the rule of law, “have set implicit limits on the state’s use of power” and have promoted the democratizationof the legal system. People are starting to use the court system to contest government actions that affect their lives, liberty, andproperty. There has been a sharp rise in the number of civil lawsuits against the state, and individuals are beginning towin — perhaps as many as 20 percent of — their cases, according to official sources. 



The opening of the legal system is important because it paves the way for the transition from “rule by law” to “rule of law.“Marcus Brauchli of the _Wall Street Journal_ writes, 



The state’s steel‐​clad monopoly on the legal process, which makes the courts just another arm of government,is corroding. China’s economic liberalization … has spawned a parallel legal reform that raises the prospect ofrule of, not merely by, law.



Nevertheless, Brauchli recognizes that “legal ambiguity” remains “a ruthless weapon” for harassing the population. Until thatfacet of China’s institutional structure changes, no one’s rights will be secure. 



**China’s Future**



The challenge for China is to get out of the way of the market and let it grow naturally along with civil society. Doing so,however, requires an understanding of the institutional infrastructure that makes the market system tick and an appreciation ofthe spontaneous order that emerges when private property and freedom of contract are protected by a rule of law.Democracy is neither necessary nor sufficient for a market system — as the experience of Hong Kong has illustrated. What isnecessary is a stable legal framework that protects life, liberty, and property. If China is to prosper in the global economy, thenation will have to adopt common‐​law practices and abide by international commercial codes and customs. Old habits arehard to break, but the forces for change are strong, and there is reason to believe that China will “creep along in the rightdirection.” 



China has been willing to experiment, but it has not yet provided the climate of freedom necessary for growing market‐​liberalinstitutions. In fact, there is an effort to give the central government greater power by ending the system of fiscal federalism.Putting more money into the pockets of Beijing bureaucrats by recentralizing the tax system, however, is not the answer toChina’s problems. Nor will improving the management of SOEs do anything to solve the problems of loss‐​ridden enterprisesthat have no real owners. 



Real stability will come to China only when its leaders abandon their fatal conceit and realize that it is impossible to plan themarket or society. Although the leadership is willing to tolerate gradual reform to keep the economy strong, there is noindication that they will tolerate political reform. The crackdown on dissidents, especially the arrest of Henry Wu and WeiJingsheng, is a clear signal to Hong Kong and the rest of the world that democratic rule is unacceptable. The West should notconfuse economic liberalization with a desire for democratization. 



Foreign pressure is unlikely to foster positive political change in China. Indeed, such pressure is likely to be counterproductive.Beijing’s frosty attitude toward the United States and our confrontational approach to China will do little to promote stability inEast Asia or to advance human rights in China. Economic sanctions and partial removal of most‐​favored‐​nation trade status forChina would surely damage China, but they might damage the wrong people. Sanctions or higher tariffs could inflict harm onthose who are fleeing the state sector for greater opportunities and freedom in the market sector, and protectionist measuresclearly would harm U.S. consumers and Americans who do business in China. 



To depoliticize economic life, China needs constitutional change and new thinking. As Chinese scholar Jixuan Hu writes, “Bysetting up a minimum group of constraints and letting human creativity work freely, we can create a better society withouthaving to design it in detail. That is not a new idea, it is the idea of law, the idea of a constitution.” Ultimately it is up to theChinese people to shape their own institutions and to secure their fundamental rights, including the right to self‐​government. 



The United States, as the world’s leading constitutional democracy, should spread its ethos of liberty by keeping its marketsopen and extolling the principles that made it great. It should not play the dangerous game of pitting human rights activistsagainst free traders. China should be admitted to the World Trade Organization as soon as possible and be givenmost‐​favored‐​nation status unconditionally. 



It may take another generation for China’s quiet revolution to succeed, but with patience and foresight China may yet joinTaiwan in a mutually beneficial alliance based on free markets and free people. 



_James A. Dorn is vice president for academic affairs at the Cato Institute. He has lectured at Fudan University in Shanghai and is coeditor of_ Economic Reform in China: Problems and Prospects _._
"
"In the early hours of 11 October 2016, I closed a safety valve on an oil pipeline in Skagit county, Washington. I was acting as part of the “Valve Turners” direct action against climate change. Five of us, in locations across four states, succeeded in shutting down all five pipelines carrying Canadian tar sands oil into the US for a day. We were careful, transparent, civil and nonviolent. We put a premium on minimizing damage to pipeline property, and carefully considered ways to minimize any violations of the law. We called the pipeline companies beforehand, and waited around afterwards for the police to arrest us (nearly an hour in two cases).  Our motley crew of mostly retirees included a former IT manager, a retired tribal government attorney, a psychologist, a poet and, in my case, a climate activist and part-time handyman. None of us had ever been charged with a major crime. We were moved to action because the world is marching toward climate cataclysm, with almost nothing being done to change that. We acted out of distress for our children and grandchildren. We acted on behalf of the poorest peoples of the world, who have contributed almost nothing to the climate problem yet will suffer the most from its effects. We acted for all the wild things and wild places which have no voice. So it was stunning, and chilling, to learn that our protest was listed as an act of “domestic terrorism” by the US Department of Homeland Security, as the Guardian recently reported. In an intelligence report, the DHS catalogued 34 deaths and numerous cases of violence in recent years. Those included acts of terror by Dylann Roof, who “used a Glock 41 pistol to conduct a shooting at a bible study at Emanuel African Methodist Episcopal Church in Charleston, SC”, killing nine; Robert Dear, who “used an AK-47 to conduct a shooting at the Colorado Springs, CO Planned Parenthood”, killing three; and Micah Johnson, who killed five police officers in a shooting spree in Dallas. Tucked between those murderous rampages, the DHS reports that “suspected environmental rights extremists coordinated the shutdown of five pipelines in Minnesota (2), Washington, North Dakota and Montana”. That’s me and my friends, trying to do something before it’s too late. For my part in that action, I will be tried for a third time next month. My first trial, in January 2017, ended in a hung jury. At a second trial, in June, 2017, the jury hung one count of sabotage and voted to convict on the charge of burglary. Last April, an appeals court overturned that conviction because I was not permitted to argue that my action in shutting down the pipeline was justified by the greater need of addressing the climate crisis. Next month, a jury will consider that question. They will weigh the testimony of climate experts, and listen to my own explanation of why this kind of action, at this time, is necessary. There is no doubt of the straits we’re in. Each day brings more devastating ecological news, and millions of people are displaced by the extreme weather events triggered by our changing climate. Yet the US government ignores the increasingly frantic voices of the world’s climate scientists and drags us further down the path of no return. That is the real environmental extremism, and that is the extremism we ought to be fighting. Our government is directly complicit in this crisis. By subsidizing fossil fuels and leasing public lands to the carbon industry, the US is in large part responsible for the current state of our planet. All the while, our government has been working overtime to quash any prospect of addressing climate change. Last week, responding to repeated motions from our government, a federal court threw out the lawsuit Juliana v United States. The suit was brought by 21 youth plaintiffs, who charged that the US government has “sanctioned, permitted and authorized a fossil fuel system” that violated their rights to a stable climate. There is next to no possibility that the immediate steps required to stave off widespread catastrophic climate change – including ending the burning of tar sands oil and coal – will be undertaken by the Trump administration, our divided Congress or by the voluntary action of the fossil fuel industry. It has become clear: we cannot wait for our government to save us when they have created the problem in the first place. This article was amended on 31 January 2020. An earlier version misstated the dates of the first two trials. They occurred in 2017, not 2015. This has been corrected"
"Carbon dioxide is the “face” of the greenhouse gases, but nitrous oxide (N2O) merits its own spotlight. The same “laughing gas” once used by dentists as an anaesthetic and used today by people looking for a quick, giggly high, turns out to be pretty bad for the environment. Nitrous oxide (a molecule made of two nitrogen atoms and an oxygen atom) is over 300 times more potent as a greenhouse gas than CO2 and accounts for 6.3% of all UK greenhouse gas emissions. If nations are to meet their climate change targets, they need to pay attention to N2O. While the gas is best known for its recreational uses, most of it is actually generated through farming, where microbes in the soil combine oxygen (from the air) and nitrogen (added to farmland) to create new compounds. This results in the leaking of N2O gas from the soil. As more nitrogen is added to the soil more N2O is emitted, so the best way to manage emissions is to control the nitrogen added via synthetic fertilisers, manures and slurries. This century, the world faces a challenge to supply enough nitrogen to maximise crop yields while reducing the release of excess nitrogen into the surrounding environment as pollution. It’s an issue I looked at in a recent report for the Parliamentary Office of Science & Technology.  Nitrogen is an essential element for life, but it is mostly present as an unreactive gas, dinitrogen (N2), which only a few organisms can use directly. Agriculture was revolutionised in the early 20th century when the large-scale industrial synthesis of nitrogen fertiliser became possible. Food production increased and population growth followed; but huge amounts of nitrogen have subsequently been added to soils and the increase of N2O emissions is the inevitable result. There are some scientific developments which could help to reduce nitrogen emissions while maintaining crop yields and so global food production levels. A few are listed here.  Instead of applying fertilisers equally across a field, precision farming allows farmers to fine-tune the location and amount of fertiliser spread by machines.  This is based on soil and plant condition measurements and associated software-generated maps – optimising the yield and reducing fertiliser waste (pollution) and cost. In 2012, 20% of English farms used soil mapping to optimise fertiliser applications.  Plants could be bred to enable a reduction in nitrogen fertiliser. Most commercial plant breeding focuses on maximising crop yields under optimal plant growth conditions, which include a requirement for high levels of nitrogen (usually delivered via fertilisers). Some researchers have argued for programmes which focus on breeding plants that perform better under lower nitrogen conditions.  The final option is further away from realisation: the crops’ genetics can be altered to reduce the need for nitrogen fertilisers. Some plants such as legumes (e.g. clover and beans) work with bacteria to convert unreactive N2 from the air into a form that is available to the plant. Scientists at the John Innes Centre in Norwich have recently begun research that aims to transfer this capability into cereal crops. These research efforts are part of an international focus to sustainably intensify agricultural production: increasing yields without adversely affecting the environment or cultivating more land. Nitrous oxide is critical to the debate on climate change, which means that farming is too."
"The average UK commuter spends about 1.5 hours a day at the wheel. While not great for stress levels in general, there are other ways that the daily churn through traffic can negatively affect health. Research by my team at the University of Surrey has shown how drivers and pedestrians are being exposed to very high levels of air pollutants at traffic lights. The World Health Organisation links air pollution to seven million premature deaths every year. It’s well known that road vehicles in particular emit polluting nanoparticles which contribute to respiratory and heart diseases. Despite efforts to encourage a reduction, car usage has remained fairly constant in recent decades. Our team monitored drivers’ exposure to air pollutants at various points of a journey and found traffic intersections were high pollution hot-spots due to the frequent changes in driving conditions. With drivers decelerating and stopping at lights, then revving up to move quickly when lights go green, peak particle concentration was found to be 29 times higher than that during free-flowing traffic conditions. Also of course, while travelling by road we are generally pretty close to the air pollution source, which is the tailpipe of preceding road vehicle.  Though drivers spend just 2% of their journey time passing through intersections managed by traffic lights, this short duration contributes to about 25% of total exposure to these harmful particles. It’s not always possible to change your route to avoid these intersections, but drivers should be aware of the increased risks at busy lights and at least try to avoid regularly taking routes that force them to sit in traffic inhaling potentially harmful fumes. Where this is unavoidable the best way to limit exposure is to keep vehicle windows shut, fans off and try to increase the distance between the cars in front where possible. Pedestrians regularly crossing such routes should consider whether there might be other paths less dependent on traffic light crossings. But there is more to it than asking drivers to take circuitous routes. Local transport agencies could also help by synchronising traffic signals to reduce waiting time and consider alternative traffic management systems such as flyovers. They could also consider the appropriate placement of traffic lights. The use of these systems in built up residential areas, near schools or hospitals may serve to manage traffic flow but at the cost of trapping higher concentrations of harmful pollutants in exactly the areas where residents, and vulnerable members of society will most regularly commute or walk. I have written before about the use of low-cost sensing to capture air pollution hotspots in urban areas. The kind of data such projects could deliver feed directly into research such as this. The more we understand about where pollution hot spots are, the more direct action we can take in our own lives and the more we can push for greener, cleaner planning. The UK’s Environmental Audit Committee recently described air pollution as a “public health crisis”. These considerations are not just a “nice to have”, they have a direct effect on our health and wellbeing."
"Shell’s recent AGM was tumultuous. Shareholders voted overwhelmingly for the company to report on whether its activities were compatible with promised government action on climate change. The firm’s board reportedly faced a sometimes-hostile barrage of questions about its approach to the environment. The key question shareholders are asking is this: what if the majority of Shell’s proven fossil fuel reserves must stay in the ground in order to avoid a dangerous global temperature increase of more than 2°C? Shell’s proved reserves are the company’s biggest asset against which it borrows money from banks and attracts investments from shareholders. Most of the oil and gas majors are struggling to find enough new reserves to keep growing in the future. This is why Shell and all other major players in the industry have to go to more extreme lengths to find the fossil fuels that keep our lights on, cars on the road and their profits growing. Controversial and environmentally very suspect investments into Arctic oil drilling, US shale gas and Canadian tar sands have already tarnished the environmental credentials of Shell. But Shell needs to find more oil and gas to keep its asset base growing and its profit potential intact. So it agreed to buy UK-based oil and gas exploration group BG Group for a staggering £47bn. To quote recent analysis, this  “gives Shell a presence in the productive zone off the coast of Brazil, and will ensure that Shell’s own production is maintained over the medium term, taking away the requirement to make large discoveries to offset natural depletion”.  But now an entirely new threat hangs over Shell’s future viability as a leading fossil fuel company. A high-profile campaign has argued that most of the proven reserves by oil and gas majors are “stranded assets” – something Shell has denied in the past. This would render Shell’s acquisition of BG Group and its investments in the Arctic wasted capital. The idea that fossil fuel firms’ reserves are overvalued because of looming climate legislation is often referred to as the “carbon bubble”. Shell’s chief executive, Ben van Beurden, was forced to admit at the recent AGM that this argument “sounds quite convincing and quite strong”. But he is also convinced that fossil fuels are here to stay for a long time to come and that decarbonisation will take decades. Can he really have it both ways? The campaign argues that we will never be able to burn any of the new fossil fuels we are discovering in the world from now on. At least not if we want to have a realistic chance of staying beneath a 2°C temperature rise above pre-industrial levels, which politicians agreed at the Copenhagen UN climate change summit in 2009 to be “the long-term goal”. It is that long-term goal that may stand in opposition to the long-term survival of Shell as a fossil fuel company. If the world will be prevented from burning oil and gas at increasing rates and emitting many more million tonnes of carbon gases than the biosphere can absorb, Shell’s current business model appears broken. Indeed, Norway’s sovereign wealth fund and other Shell shareholders have called on the company to reveal exactly how climate change could affect the company’s future, a motion that was carried at the recent AGM. In its latest New Lens Scenarios report, Shell admits that the world is on course for a temperature rise of well above 2°C. It says this will even be the case if coal is rapidly displaced by gas (which the company is banking on with its BG Group and shale gas investments) and there is an accelerated deployment of carbon capture and storage (CCS) technologies, which, however, has developed at a snail’s pace and requires large subsidies, making it non-competitive.  The wording of the report has led to claims by The Guardian – which has run its own high-profile “Keep it in the Ground” campaign – that Shell has given up on the 2°C target and is in fact planning for a much higher threshold of 4°C or even 6°C. I think this is overstated. But what is clear from the paper’s most recent interview with van Beurden is that Shell is banking on a very slow pace of climate change negotiations; it doesn’t feel under huge pressure to change its current business model until after 2015. Not a good omen for the forthcoming UN climate change summit in Paris.  At least since 2009, the world’s governments have failed to come up with credible plans for radically reducing carbon emissions. The worldwide scientific community agrees this needs to happen if we want to avoid dangerous effects of climate change. There has been far too much focus on listening to industry lobbyists protecting corporate interests, their current and future assets as well as millions of associate jobs in the oil and gas industry. Instead we need a clear-headed admittance that the business model of major fossil fuel companies needs to radically change if we don’t just want to adapt to climate change but have a last-ditch attempt of mitigating it. Shell’s CEO van Beurden seems to agree with this, but doesn’t see this radical change coming until post-2050. Most experts agree, however, that we cannot leave it until then; it’s too risky. Global, powerful corporations, such as Shell, are often targeted by environmental activists in their attempt to hold them to account. Rightly so. But they are simply money making machines. Investing into fossil fuels is still very profitable, and shareholders demand high returns on their investments. Only politicians can change the logic of energy investments, and they will have to do exactly that when they will sit around the negotiating table in Paris in December this year. If we want to have a chance to keep within a 2°C temperature rise, politicians have to realise that current investment practices in the oil and gas industry are not sustainable and even, in the words of Al Gore, “absurd”."
"

Have you ever worked for an employer that was cheap to the point of making you want to find another job? So miserly and humiliating at Christmas that you had visions of fun revenge instead of sugerplums dancing in your head?
Slate Magazine decided to have a contest for the worst corporate scrooge, and over 200 entries were submitted.

Some of the stories are pretty humorous, others are just downright depressing. You can read it here.
I have a couple of stories of my own. Both were TV stations I worked for. WLFI-TV in Lafayete Indiana where I got my start was owned by The Shively Brothers, and Harold, the younger brother, was the General Manager. They were newspapermen and this was their first TV venture. Being that, they were both clueless on how to run a TV station or treat employees that had been in the business awhile.
One Christmas ‘bonus’ was quit memorable. Each employee received a brass key chain with a giant brass letter signifying their last name. Seeing how mine was one of the largest letters, a “W”, and it was about 5 inches long making it impossible to even fit into my pocket. One employee suggested we all get together, spell out an appropriate message (you guess what it was) and present it to the our clueless GM.
Another one happened right here at KHSL-TV in Chico, soon after the corporate weasels from Catamount Broadcasting bought the station, and turned it from a pleasant family run business, to a place that was all about the bottom line.
Mickey McClung, the former owner, and a kind woman, always made sure each employee had extra money or a gift card to buy Christmas dinner for the family, and we could always count on that, and sometimes more in good years. The next owner, Howard Brown, kept the tradition and was even more generous.
After Catamount took over, we wondered what would become of that tradition. We soon found out.
The next Christmas we were presented with a $25 gift certificate to Holiday Markets. They were an advertiser, and the gift certificates were “trade outs” for airtime, so they cost the corporation nothing.
And…surprise, there are no Holiday Markets in Chico, the company had closed its Chico store that year. They had stores in Redding and Paradise, but with the price of gas, it was hardly worth the trip.
What I learned from these experiences was to treat my own employees well. And hand out real, significant cash bonuses at Christmas instead of something that will make them think less of you. Employees make the company, not the other way around.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea932335f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Kirye
and P. Gosselin
Of course all we hear from the media nowadays is that weather extremes have been getting worse (over the past decades) and the planet is warming rapidly. But when we look at the untampered data, we see that many places have been cooling.
Today I present to you some examples, now that untampered November data for 2019 have become available at the JMA.
Iceland
First we look at November mean temperatures of three stations Iceland, isolated in the middle of the North Atlantic. The source of the data is the Japan Meteorological Agency (JMA):

Three Icelandic stations with sufficiently available data from the JMA show a cooling November trend since the early part of the century. Data JMA.
In Iceland, the start of winter in fact has been getting colder, and not warmer like AGW theory would suggest. So there’s been lots of deception coming from activists.
Netherlands
The data tell a similar November story at the North Sea country of Netherlands: The start of winter is not getting delayed in the sense of warming. Quite to the contrary, it’s been cooling in November:

No real warming to be seen in November from the five examined stations in the Netherlands. Data: JMA.
Ireland
Over at the North Atlantic island of Ireland, we find 6 stations with sufficient data from the JMA to allow the plotting of November trends. Here’s the result of the six stations observed:

Warming? Ireland is hardly behaving like the alarmists and Greta claim. Early winter in Ireland has been cooling! Data: JMA. 
Norway (Greta’s neighbor)
Greta Thunberg’s western neighbor, Norway, has also been defying claims made the teenage Swedish activist, and, for the most part, has been seeing cooling in November for two decades:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





November in Greta Thunberg’s neighbor, Norway, has been cooling for some two decades now. Data: JMA.
Seven of 11 stations in Norway show November temperatures have had a cooling trend since 1999 (20 years)!
This year TROMSO/LANGNES and VARDO saw their coldest November in the last 17 yrs, BODO VI, ORLAND III, BERGEN/FLORIDA and OSLO/GARDERMOEN were the coldest in 9 years.
Sweden
Next we move to Greta’s Sweden, whose future she says we are stealing. But looking at her home country as a whole, 3 of 6 stations in Sweden show November temperatures have had a cooling trend since 1999:

No significant Swedish warming during Greta’s lifetime. Data: JMA.
Powerful ocean cycles, not trace gas CO2
So how can these northern European countries examined above be cooling in times of “rapid global warming”? Why is winter coming earlier, and not later? Isn’t CO2 greenhouse gas supposed to be trapping heat there and making things warmer?
Obviously it isn’t, and so there has to be other natural explanations.
The explanation for this of course has long been suspected, but has been kept hidden from the public. The major reason for the cooling of November over the recent years very likely has something to do with the North Atlantic sea surface temperature (SST).
As the following chart shows, SSTs are cyclic, and they have been trending down for the past 15 years.
 

Chart: NOAA, via Climate4you.
So it’s no surprise that November has been cooling over northern, coastal Europe since the start of the century. Claiming that CO2 drives climate is a sham.
Share this...FacebookTwitter "
"
The picture below is of the USHCN climate station of record for Newport Beach, CA When I first visited this site I did a double take. Then started searching for the “real” temperature sensor.


I couldn’t believe that NOAA allowed them to use consumer grade equipment. I was sure I just hadn’t located the MMTS sensor. It wasn’t until I looked up the MMS metadata entry for equipment for NB and saw “miscellaneous” listed for rain and temperature sensors, that I began to get concerned.

I then went back a second time to be sure I hadn’t missed the station, after checking lat/lon on my GPS…because I just didn’t think it possible NOAA would allow a consumer grade sensor in the USHCN dataset. Then I found somebody in the harbor patrol office to ask, and he confirmed that was the station they use to send readings to NOAA.
I was reminded of that famous quote from the movie “Treasure of the Sierra Madre” lampooned in the movie Blazing Saddles; “We don’t need no stinking badges!”. Except, what was playing in my mind then was “We don’t need no stinking homogeneity!”
Note to NOAA: standards exist for a reason.
Apparently the observer wanted wind too, (the wind sensors are on top of the tower, not shown in these pictures)and while I can appreciate that being located at the harbor patrol office, NOAA could have supplied standard equipment in addition to the shiny new consumer grade Davis station. In fact a standard rain gauge and MMTS did exist, but was removed in 1998 in favor of “miscellaneous” equipment.
Now don’t get me wrong, Davis makes a great weather station, but we can’t just replace sensors with other types willy-nilly and have a homogeneously rigorous data set.
But there are other issues too, such as the rooftop proximity, the diesel generator, and the parking lot it sets in the middle of. More pictures available on surfacestations.org


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5193595',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
German Coal Power Plants To Be Converted: To Burn Trees

Millions of trees to be shipped from around the world to Europe to be burned as “green coal”. Image cropped from “Planet of the Humans”
By Die kalte Sonne
(Translated/edited by P. Gosselin)
On May 2, 2020, we reported on the movie Burned. In the USA, the focus is on biomass.
However, they do not ferment fast-growing plants into gas as is the case in Europe, rather they cut down trees and burn them in power plants – often together with other things like car tires or soaked railway ties.
The issue is controversial because it is about pure ideology. Climate organisations such as 350.org, which in the USA is like Fridays For Future (FFF) in Europe, have given their blessing to this type of power generation.
The film Planet of the Humans by Michael Moore also denounces this.
Converting CO2 sinks instantly into atmospheric CO2
And so the USA is losing valuable carbon sinks and biotopes, destroying its environment and lying to itself about sustainability and the climate. A tree that takes 50 – 100 years to become big and stately, but then is burned up in a few minutes, can never have a favorable climate balance, no matter how you calculate it. Trees are the new coal, it seems.
But anyone who thinks that this is only done in the USA, where huge forests and thus carbon sinks are destroyed, is mistaken.
“Madness”: German coal plant to be converted to burn trees
The online daily Weserkurier reports on a coal-fired power station in Wilhelmshaven (North Germany) that is to be converted to burn wood. This made Germany’s most famous forester, Peter Wohlleben (book “The Secret Life of Trees“) flash with anger on Twitter.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Der Wahnsinn geht weiter: Obwohl hunderte Wissenschaftler vor der Holzverbrennung als Klimakiller warnen, setzen Politik und Wirtschaft in Deutschland auf Waldzerstörung und wollen Kohlekraftwerk umrüsten. https://t.co/gHJvNGi4YR @GrueneBundestag @SvenjaSchulze68 @spdbt
— Peter Wohlleben (@PeterWohlleben) June 13, 2020

 
Wohlleben’s tweet in English:
The madness continues: although hundreds of scientists are warning against burning wood as a climate killer, politics and industry in Germany are backing forest destruction and want to convert coal-fired power plants.”
What Wohlleben means by madness could be the statements of Social Democrat Party member of parliament Siemtje Möller. Her slogan on her own website: “Think about the climate too!”
“Green coal”
Siemtje Möller is already thinking ahead. After all, the Wilhelmshaven site could eventually also produce hydrogen with the green coal. The stimulus for the technology, worth billions of euros, which has just been ratified, should also come to Wilhelmshaven.
“I’d like a fair share here,” says the Siemtje Möller about the budget. In general, she sees the hydrogen initiative, the coal phase-out law and the structural transformation law as “a huge opportunity for the Northwest to enter the future”.
She calls trees “green coal” in all seriousness and then wants to use the energy from burnt trees to produce hydrogen. Does the federal hydrogen initiative mean something like that? Probably not. Destroying carbon sinks cannot possibly be a huge opportunity for the future.
Why does Ms. Möller take her own slogan so little seriously?

Share this...FacebookTwitter "
nan
"About one in nine people globally  still suffer from hunger, with the majority living in Africa and Asia. The world’s forests have great potential to improve their nutrition and ensure their livelihoods. In fact, forests could be essential to global food security, particularly when considering the importance of diverse, nutritionally-balanced diets. Forests are key to protecting biodiversity, and for mitigating the effects of climate change. This is well known. However their contribution to alleviating hunger and improving nutrition has been somewhat neglected. A recent study by the Global Forest Expert Panel on Forests and Food Security, which I chaired, shows how forests and trees can complement agricultural production and give an economic boost to some of the world’s most vulnerable regions.  Wild meat, fish, and insects are also important forest food sources. Insects are an especially cheap, abundant source of protein and fat. Caterpillars are great for vitamins and minerals. Particularly in South-East Asia, many forests and agroforests (tree-based farms) are managed by local communities specifically to enhance edible insect supply, such as the management of sago palms in Papua New Guinea and eastern Indonesia to support grub production.  Forests are also essential for firewood and charcoal. In developing countries, 2.4 billion people still use wood-fuel for cooking and heating. In India and Nepal, even better-off rural households depend on it. The volatile and often high prices for other energy sources suggest this situation is unlikely to change for some time. Access to cooking fuel provides people with more flexibility in what they can eat, including more nutritious foods that require more energy to cook. Trees offer a multitude of ecological services. For instance, they support bees and other pollinators, which are essential for crop production including on farmland. They also provide animal fodder that enables communities to produce meat and milk, and protect streams and watersheds as habitat for fish. Close to one out of six people directly depend on forests for their food and income, and it is important to recognise the rights of local people to these livelihood options. In the Sahel, for example, trees can contribute as much as 80% to household incomes, especially through shea nut production.  Novel initiatives are attempting to develop new tree commodities to supply the poor with sustainable incomes. For example, poor producers in Tanzania are engaged in a global effort to produce the seeds of the Allanblackia crop, which yield an edible oil. A private–public partnership known as Novella Africa is developing a sustainable Allanblackia oil business that could be worth hundreds of millions of dollars annually for local farmers. Although forests are not a panacea for global hunger, they play a vital role in complementing crops produced on farms. This is especially important when the staple food supply is impaired by droughts, volatile prices, armed conflicts, or other crises.  While large-scale crop production remains important, it is highly vulnerable to extreme weather events, which may occur more frequently under climate change. Tree-based farming can adapt far better to such calamities. During periods of food shortage triggered by such events, forest foods can provide a vital safety net, especially for the poorest households. This forest-farm link also means that the loss and degradation of forests exacerbate the problem of food insecurity. Losing forests jeopardises “ecological services” such as a clean water supply, crucial for crop and livestock production. Managing landscapes on a multi-functional basis that combines food production, the maintenance of ecosystem services and other land use services should be at the forefront of efforts to achieve global food security. In the lead up to the UN’s finalisation of the Sustainable Development Goals later this year, the contribution of forests and tree-based systems to the “Zero Hunger Challenge” needs to be emphasised. They can be managed to provide better and more nutritionally-balanced diets, greater control over food inputs – particularly during lean seasons and periods of vulnerability (especially for marginalised groups) – and deliver ecosystem services for crop production. It will be a critical element of the responses to global hunger."
"
Share this...FacebookTwitterOn May 26, the World Meteorological Organization (WMO) issued a press release warning of “another record-breaking heat season” for the northern hemisphere this summer, along with the potential of the COVID-19 pandemic amplifying the health risks of the hot weather.
Media outlets picked up the WMO warnings and spread panic stories of mayhem and climate breakdown among the public.
But veteran Swiss meteorologist Jörg Kachelmann, citing models from the ECMWF, doesn’t see any evidence of another “record breaking summer”. He tweeted:

Dear @WMO I've wondered since you wrote thishttps://t.co/2JEahOzwIi
What forecast data led you to the conclusion that we could expect the hottest summer for the Northern hemisphere ever in this year. Wherever I look, I can't see any evidence.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Can you help me? Thanks. pic.twitter.com/zMrlZRtCTC
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) June 12, 2020

So far Kachelmann has yet to receive an explanation from the WMO.
Like the ECMWF model for the next 45 days shows, the northern hemisphere has extensive cool patches, and so no signs of a “record breaking northern hemisphere summer this year.
Whether it’s the World Health organization (WHO or the WMO, global institutions set up to guide policy are doing a lousy job and are in need of extensive reform, as some leaders have already called for.
Share this...FacebookTwitter "
"As we enter 2020 and survey the state of the world, two competing storylines seem to dominate the present. The first is ruthless, relentless inequality. We see this not only in the imbalance of our economy, but also in our politics and government, in education and expression, and in the ways that our social structures and cultural practices dismiss and disregard people because of their gender, race, ethnicity, ability, and more.  Then there is the corollary: People standing up, speaking out, and fighting back. Young, inspirational figures like Kynan Tegar, Greta Thunberg, and Artemisa Xakriabá are spearheading movements to address the global climate crisis, because their future is at risk. And in places like Hong Kong, India, Lebanon, Mexico, Sudan and many others around the globe, ordinary citizens are demanding better treatment, more transparency, greater equality, and human dignity. They demonstrate, time and again, their willingness to sacrifice everything – their time, their efforts, even their safety – in service of a better future for us all. Unfortunately, the truth is, the courageous individuals leading the change – the people driving the conversation and making the largest sacrifices – too often are the very people most threatened by inequality. And so, I find myself asking: Why is it that those with the least tend to sacrifice the most? How can it be that the most comfortable among us contribute, in relative terms, the least? What crisis needs to befall us in order to act? To be sure, many with power and privilege already give with extraordinary generosity. As the president of the Ford Foundation, an organization originally endowed by a family of great means, I have seen firsthand how the generosity of a few can affect the lives of many. But this kind of generosity is too often superseded by the pervasive inequality of sacrifice – which we see both day to day and in the data. For example, a survey from The Chronicle of Philanthropy found that, during the Great Recession, American households that earned more than $200,000 reduced their giving by more than 4%, while households that earned less than $100,000 did just the opposite. In fact, as they saw the need increase, they increased their giving, in turn – by more than 4%. And this despite the fact that tax codes in the US, Europe, and other parts of the world have built-in incentives that reward the wealthy for their charitable donations. This all-too-common discomfort with sacrifice is not about good versus bad. The disconnect – and disconnection – follows from isolation and insulation. Separation breeds selfishness, despite the best of intentions. In the United States, for instance, segregation was outlawed over 65 years ago. Yet, a recent Civil Rights Project report found that America’s schools remain both separate and unequal. We see similar trends of geographical divide in cities across America and Europe, where a lack of economic opportunity has led to the disintegration of our social fabric. Restoring unity and trust will take more than generosity and good will. Charity, while wonderful, ameliorates the symptoms of inequality, but it does not address its root causes. And we must address the root causes. Keep in mind that large swaths of people around the world are growing increasingly impatient with the status quo, and we with power will feel their ire if we ignore their righteous demands for a fair and just society. So those of us with privilege need to find ways to do more justice. That means not only helping those who are often excluded, but also undoing the systems and structures that create inequities and imbalances in the first place. That will sometimes also mean working to transfer our own power, and giving up some of the privileges we currently enjoy. Toward the end of last year, when our foundation hosted a conference on the future of philanthropy, I was struck by something Luis Miranda Jr, philanthropist and father of the entertainer Lin-Manuel Miranda, said about the philosophy with which he was raised. “If it doesn’t hurt,” Miranda argued, “if you’re not not-doing something else, then you’re not giving enough.” This conception of sacrifice is profound: only when it is uncomfortable or even painful to give do you know that you are giving for reasons beyond your own benefit. And it is worth remembering that, despite this prick of pain, sacrifice has a value beyond what is given up. When you make this kind of sacrifice, you do not give up something for nothing; you give up some of your privilege for something far more valuable: justice. If we want to address the inequality of sacrifice, therefore, those of us with the most ought to recommit ourselves to doing the most. Perhaps that means paying a higher tax rate to finance public goods or wielding our influence to promote inclusive public policies. Or perhaps it means devoting more time, money, and effort toward rebuilding the broken systems that perpetuate inequality. We all must ask ourselves what system, status, or status quo we are willing to part with to benefit others. Only then – only when we have paired our generous impulses with acts of justice – will we diminish the inequality of sacrifice, and build a better, more equitable world. Darren Walker is president of the Ford Foundation"
"

There’s an article in The Oil Drum that focuses on electricity production; or rather how or what we will need to do to keep pace with people’s demands while balancing that with environmental and economic impact. It is lengthy but well-reasoned and good reading.
From the article: “One of the biggest threats the USA faces today is a serious shortage of energy. Vulnerabilities in our system have been made glaringly obvious several times; since the 1970’s the USA has had social and economic upheaval due to the actions of foreign oil producers, and two hurricanes in 2005 showed just how fragile our remaining domestic supplies of oil and natural gas are.”
The president recently reiterated a commitment to reducing our national oil consumption, and I hope that gets implemented as its really a good idea. Hybrids and electric vehicles are looking better and better. Chances are my next car will be one of these.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea86f23c6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

If you live near the ocean, chances are high that your home is built over sandy soil. For example many places in San Francisco are built on sandy soil or fill. Many homes built on this type of soil were badly damaged during the 1989 Loma Prieta earthquake.
When an earthquake strikes, deep and sandy soils can turn to liquid by a process known as liquefaction, with disastrous consequences for the buildings above. In an odd application of biotech, researchers at UC Davis have found a way to use bacteria to steady buildings against earthquakes by turning these sandy soils into rocks.
“Starting from a sand pile, you turn it back into sandstone,” the chief researcher explained. It is already possible to inject chemicals into the ground to reinforce it, but this technique can have toxic effects on soil and water. In contrast, the use of common bacteria to “cement” sands has no harmful effects on the environment. The new process, so far tested only at a laboratory scale, takes advantage of a natural soil bacterium, Bacillus pasteurii. The microbe causes calcite (calcium carbonate) to be deposited around sand grains, cementing them together.
So far this method is limited to labs and the researchers are working on scaling their technique.
Below: Before and After electron micrographs of microbiollogically-induced calcite  precipitation in which B. pasteurii cells are embedded.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7f4c617',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterScientists (Syring et al., 2020) find almost sea ice-free conditions pervaded a much warmer northern Greenland region during the Early Holocene.  Arctic sea ice extent has “continuously” grown for ~4800 years, with modern conditions a bit lower than the peak of the last few centuries.

Image Source: Syring et al., 2020
In a new paper (Syring et al., 2020), scientists rely on biomarker evidence – (a) the presence of warmth-demanding species Armeria scabra and Mytilus edulis, and (b) IP25, a proxy for the presence or absence of sea ice – to suggest not only were there much warmer (4 to 5°C) northern Greenland temperatures 10,000 to 8500 years ago, but effectively sea ice-free conditions pervaded the region during this time.
The sea ice in the region has been growing “continuously” for the last 4800 years, reaching its peak during the last millennium.
The authors also find decadal- and centennial-scale periodicities in solar activity have coincided with variability in Arctic sea ice (IP25) throughout the Holocene.

Image Source: Syring et al., 2020
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWe grieve the loss of Dr. S. Fred Singer
By Michael Limburg, EIKE
(Translated, edited by P. Gosselin)

Dr. Siegfried Frederick Singer

Yesterday, our mentor and good friend, the outstanding scientist S. Fred Singer passed away – peacefully and quietly at the age of 95. Without the constant encouragement we received from this outstanding scientist from the very beginning, the founding of EIKE and our commitment to the dissemination of the scientific facts on climate change would not have been possible here in Germany.


Dr. Fred Singer as the keynote speaker at the first European Climate and Energy conference on May 30, 2007.
Distinguished career spanning 7 decades
Dr. Singer was the keynote speaker at our very first Climate Change Conference in Berlin in 2007 at the premises of the Institute for Entrepreneurial Freedom (IUF) on May 30, 2007, immediately after our founding. And he remained loyal to us in all subsequent years, even though in recent years his physical condition made the long journeys from his home in Virginia increasingly difficult.
But his unrestrained desire not to let science degenerate into a water-boy for politics, which was particularly evident in the increasing appropriation of environmental science by politics, allowed him to marshal all the strength his body could muster.
Fortunately for us all, he was able to do so for almost one and half decades. No one would have been more predestined than him to see exactly this monopolization, because he came directly from science and always worked there in outstanding positions. A short and partial look at his extraordinary curriculum vitae shows.
Father of U.S. weather satellites
His scientific work has also been published over 200 times in leading scientific journals. In 1954, President Eisenhower even awarded him a special prize from the White House for his work.
Without any exaggeration it can be said that S.Fred Singer can be called the father of the US weather satellites. Atmospheric physics was his domain.
Politicization of science “highly dangerous for democracy”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Because he saw that the emerging environmental movement was striving for a symbiosis with politics in particular, which was highly dangerous for democracy, he founded the Science and Environmental Policy Project (SEPP) in 1990 and the Nongovernmental International Panel on Climate Change (NIPCC) in Vienna in 2008. Both institutions were active in the collection and dissemination of scientific facts, against the increasing ideologization of the environmental idea – and the emerging panic-mongering about supposedly man-made climate change.
Over 200 scientific publications, wealth of books
A wealth of scientific books (Climate Change Reconsidered, or Unstoppable Global Warming, Every 1,500 Years, together with Dennis Avery) and many works written during this fruitful period, many of them with the support of Heartland and CFACT, bear eloquent witness to this.

Fred Singer at the 5th Climate and Energy Conference in Munich in 2012.
Escaped Nazi Germany, “unspeakable cruelty”
Our friend, my good friend S. Fred Singer, was also a living example of the unspeakable division and cruelty our continent saw in the last century. Born in Vienna in 1924 as the child of a Jewish family, he left his home country at the early age of 14 after the annexation of Austria by Nazi Germany in 1938, fled first to the Netherlands, where he was apprenticed to an optician, and from there emigrated further via England to the USA.
After serving in the U.S. Navy, he studied physics at Princeton and received his doctorate in 1947. Later he also studied electrical engineering at Ohio State University, where he graduated with a diploma. In addition to English, Fred spoke German, Swedish and Dutch.
Defamed by environmental activists who spread lies
However, neither his resume nor his extraordinary scientific merits kept the growing opposition from the green-left camp from attacking and muzzling him using unspeakable defamation and lies instead of scientific debate. The German WIKIPEDIA issue (here) offers readers an example of this. Among other things, the lie is repeated repeatedly that Singer would have let himself be bought by the tobacco lobby because he – himself a lifelong non-smoker and chairman of a non-smoker’s association – had truthfully stated that the carcinogenic effect of passive smoking could not have been scientifically proven.
My last e-mail contact with him is dated October 8, 2019, when we, the board of directors of EIKE, congratulated him on his 95th birthday. We didn’t get an answer to that, his mind was still alert, as we know, but his body refused  to go on.
Farewell, good old friend, rest in peace. You have done so much for this society. I am very proud to have had you as my friend.
Michael Limburg
European Institute for Climate and Energy
===========================
Also this blog, NoTricksZone, was in large part inspired by Dr. Singer, his occasional emails to me and particularly his book: Unstoppable Global Warming Every 1500 Years. And I know other skeptics here who also say they were inspired by him.
I recall wishing hm a happy 90th birthday with this blog post. He’s going to be missed.
Share this...FacebookTwitter "
"Fifty-one per cent of voters believe the prime minister, Scott Morrison, should have sacked the Nationals minister Bridget McKenzie over her handling of the $100m sports rorts affair, according to the latest Guardian Essential poll. The latest survey of 1,080 respondents, taken in the wake of revelations that McKenzie used a sports grants program to favour groups in Coalition-held or targeted seats, also shows voter support galvanising for an independent federal anti-corruption body, with 80% backing the proposal. When asked about the conduct of McKenzie in allocating grant funding to marginal seats to favour the Coalition in the election, 51% said they believed the prime minister should have stood McKenzie down from cabinet over the issue, while just 15% endorsed Morrison’s decision to support his colleague. About a third (34%) of participants said they had not been following the issue. Coalition voters were the least likely to say that Morrison should have stood the minister down, with 41% of voters saying she should go, 27% supporting the prime minister and the remainder unsure. The result comes after an explosive ABC report on Tuesday revealed that Sport Australia raised concerns that the Morrison government’s administration was compromising its independence in the weeks leading up to the election, and as the merit assessment scores of more than 2,000 projects were made public. The fresh revelations will heap further pressure on Morrison to move against the agriculture minister, whose conduct is being examined by the head of the prime minister’s department, Phil Gaetjens. The Essential survey, taken from 20 to 27 January, shows that support for McKenzie’s removal is higher than voter support in December last year for the removal of Angus Taylor from cabinet. At that time 35% of voters supported Taylor’s removal after police initiated an investigation into the doctored documents scandal. Voters were also asked in the same poll if they supported the establishment of an independent federal corruption body to monitor the behaviour of our politicians and public servants, with 75% supporting the idea. This has since grown to 80%. The number of respondents “strongly supporting” a new federal anti-corruption body has also risen, going from 42% in December to 49%, with support highest among Labor voters at 86%. Across all voting groups, only 7% opposed the idea, while 13% reported they were unsure. The survey also tested voter sentiment about the government’s climate change policies, asking about a range of hypothetical policy responses that could underpin the government’s pledge to “evolve” its response to the climate crisis. The most popular response was for the accelerated development of new industries and jobs powered by renewable energy, with 81% of respondents supporting the measure, including 75% of Liberal and National voters. Almost three-quarters (71%) of those surveyed supported a zero-carbon pollution target to be set for 2050, but support was high among Labor and Green voters (81% and 89% respectively) and low among Coalition voters at just 56%. When asked if they supported the prevention of new coalmines opening in Australia, the view of respondents split dramatically, with 48% of Coalition voters opposed to the idea, and 84% of Greens voters and 70% of Labor voters supportive. Across all potential policies, support was higher among Labor and Greens voters, and lower among Coalition supporters. The survey on climate policies comes as Morrison faces mounting pressure to explain how he intends to “evolve” the government’s climate change policies after he indicated the government would do more in the wake of the bushfire crisis. The prime minister has conceded the severity of the fire season is partly caused by climate change, but has sought to shift the focus to “adaptation and resilience” and a “practical” response, rather than on emissions reduction measures. In a speech at the National Press Club on Wednesday, Morrison will focus on the government’s “practical” response to the bushfire crisis, flagging a change to how the federal government can respond in times of national emergency. Morrison will say that while it is appropriate for states to take a leading role, “where, when and how the resources and capabilities of the federal government should be engaged is less clear”. “To date, the role of the commonwealth in responding to natural disasters has been limited to responding to requests for assistance from state governments. They judge the time and form of support needed,” draft excerpts of Morrison’s speech say. “The scale of the bushfires this season – not least their simultaneous reach across state borders – has unequivocally demonstrated the limits of those arrangements.” He will flag a review of the constitutional and legal framework to allow the commonwealth to declare a national state of emergency, the legal interface with the states and territories on responding to national emergencies, and an enhanced national accountability regime for natural disaster risk management, resilience and preparedness. The Essential poll, released on Wednesday, also canvassed support for a new national day to recognise Indigenous Australians either to replace or to sit alongside Australia Day. The survey found that compared to last year, support has decreased 2 percentage points with an overall 50% of respondents supporting the concept. Of those surveyed, 32% supported including a separate national day with a further 18% supporting a replacement of Australia Day. Support for a separate day was highest among Greens (73%) and Labor (58%) voters, and among those aged 18-34 (65%). Young people in this cohort were also far less likely to celebrate Australia Day compared to last year, down from 45% in 2019 to 32%. The poll’s margin of error is plus or minus 3%."
"Lots of people find elections dull, but there’s nothing boring about the political manoeuvres that take place in the animal kingdom. In the natural world, jockeying for advantage, whether this is conscious or merely mechanical, can be a matter of life or death. Chimpanzees, our closest relatives, are highly political. They’re smart enough to realise that in the natural world brute strength will only get you so far – getting to the top of a social group and remaining there requires political guile.   It’s all about making friends and influencing others.  Chimps make friends by grooming each other and forming alliances; this behaviour is especially prominent in males wishing to be group leader. In times of dispute they call upon their friends for assistance or when they sense a coup may be successful. And the ruling group either reaffirms its position or a new group grabs control – but having the weight of numbers is normally critical to success. Back in the 1980s, the leading Dutch primatologist Frans de Waal spent six years researching the world’s largest captive colony for his classic book Chimpanzee Politics. He soon realised that, in addition to forming cliques, chimp politics still involves some degree of aggression. Humans in modern societies have largely replaced antagonistic takeovers with voting.  Chimps do not, however, live in a democratic society. For them, the social structure of the ruling party is usually one based on male hierarchy, where dominant individuals have best access to the resources available – usually food and females.  In many primate species, the ruling party members are relatives.  Alternatively, power alliances between individuals are based on reciprocal altruism – more colloquially known as “you scratch my back and I’ll scratch yours.”  Does some of this sound familiar? However social species don’t always need a ruling party or a charismatic leader and, even among the social primates, we have examples of leaderless organisation.   Muriquis, (or woolly spider monkeys) from south-eastern Brazil live in large social groups and yet there are no leaders.  Males do not boss other males or females and there is no dominance hierarchy – a truly egalitarian society. They are very peaceful primates. Males will even wait in line for their opportunity to mate with a receptive female.   Relative to many primate species, muriquis don’t spend much time grooming each other or socialising, which may suggest a poorly developed political system. But this would be an over-simplification, as individuals socialise through group hugs – and social network analyses confirm the important roles of certain individuals. For example, the longer a male hangs out with his mother, the more “introductions” to unrelated females he will get and more offspring he may sire. While we might expect our fellow primates to display some degree of recognisable “politics”, it is more surprising to discover that this behaviour extends right through the animal kingdom – and beyond. Bacteria are simple microscopic organisms and yet even at this lowly form of existence, individuals try and influence others into following their action. They of course do not have a leader but instead live in a decentralised system where decisions are made through a system of stimulae and response related to population density, known as quorum sensing.  Bacteria vote by releasing signalling chemicals and they are able to count the number of chemicals released (votes made). For example, pathogenic bacteria must vote on when to become virulent (exploit their host); that is, to jointly release virulence factors (chemicals). By acting together they overwhelm the host’s immune system and can therefore colonise it.  More famous decentralised “political” systems exist in social insects such as bees and ants. Bees sometimes need to find a new nest site and they too use quorum sensing to decide on the location. Individuals will go and search for potential sites and upon returning to the nest they will do their famous waggle dance indicating the location of the site they have found.   However if the site the bee has found is of poor quality it will quickly stop dancing. Those individuals who are the most persistent – who shout loudest and longest from their soap boxes – will gain the greatest number of followers to their proposed new nest site. These followers, having returned from the potential new nest site, become political activists and add their dance/vote to the cause until there is a landslide victory for one particular new nest site. What these examples show us is that politics is everywhere: from the bacteria in our bodies to the animals in the world around us. Like it or not there is no escaping politics: roll on May 8."
"

In politics, timing is everything, Howard Dean being a wonderful example. So is Al Gore, who chose to give a completely paranoid speech about global warming in New York two weeks ago, on a day when the temperature was 22 below normal. In a remarkably Dean‐​like rant to the Democratic organization MoveOn, he said that the reason Americans reject his vision of climate‐​Armageddon has more to do with what he called “a massive and well‐​organized campaign of disinformation” on the part of me and my few friends, than it does with the thermometer. 



When it comes to disinformation about climate change, Al’s got competition in the principal beneficiary of Howard Dean’s rhetorical largesse, John Kerry, who looks to me like a cinch for the Democratic nomination. On May 17, 2000, Kerry said:





I’m offering a night of free beer to the first journalist who can come up with a picture of John Kerry wearing a coat in November (and expect to have to pay off within one minute of this column’s publication). But what about that whopper about northern New Hampshire’s ponds?



One lesson in climate hype that Gore never learned (and which may have cost him the presidency) is that people can look up facts pretty quickly now. Gore lost normally Democratic West Virginia because of his hype on global warming and his resultant vitriol against the coal industry. Miners, who he would have put on unemployment, stayed home or voted for Bush. Now Gore’s venting about planetary heating in howling blizzards.



So should Kerry beware. There’s lots of data on the Internet, including a study by the U.S. Geological Survey of “ice‐​out” dates on lakes in northern New Hampshire. That’s the day of the year when you can no longer play hockey.



John Kerry is 60 years old, so it’s safe to say he was playing hockey in northern New Hampshire, his home, from the ages of 7 to 17, or 1950 through 1959, near First Connecticut Lake. The average date of ice‐​out for that period was May 1. From 1991–2000, when, according to Kerry, “you are lucky if the ponds freeze,” the average ice‐​out date is later, on May 5. 



A year later, on May 1, 2001, Kerry said, “This summer the North Pole was water for the first time in recorded history,” a story that was originally carried by the _New York Times_ in September 2000. It was retracted three weeks later as a barrage of scientists protested that open water is common at or near the pole at the end of summer. Further, it’s common knowledge in the scientific community that there has been no net change in Arctic temperatures in the last 70 years.



He went on: “In 1995, after a period of unusual warming, a 48 by 22 mile chunk of the Larsen Ice Shelf in Antarctica collapsed.” Disregarding that ice shelves don’t “collapse,” the fact, as accessible as the nearest _Nature_ magazine, is that Antarctica shows a slight cooling trend in recent decades.



Voters need to stay tuned to Kerry on global warming for the Arizona primary on Feb. 3. John McCain, who will do anything to defeat George Bush, has been on a merciless campaign of badgering the president about climate change, including shepherding the first Senate vote to restrict energy use because of global warming, which only failed by eight votes last fall. You can bet Kerry is going to feed off of McCain’s Arizona popularity. He may even entreat him into the Veep slot, claiming to be the ultra‐​centrist and spelling sure defeat for President Bush. 



Anyway, now that he’s the front‐​runner, he’s going to have to watch what he says. Or what he wears. Again, free beer for that picture of him wearing a coat in November.



If Kerry doesn’t check his facts better, he’ll soon be sharing the platform with Al and Howard, trapped in the living hell of the formerly relevant. 
"
"The UK government is being sued for approving a large new gas-fired power plant, overruling the climate change objections of its own planning authority. The plant, being developed by Drax in north Yorkshire, would become the biggest gas power station in Europe and could produce 75% of the UK’s power sector emissions when fully operational, according to the environmental lawyers ClientEarth, who have brought the judicial review.  The planning inspectorate recommended to ministers that the 3.6GW gas plant was to be refused permission because it “would undermine the government’s commitment, as set out in the Climate Change Act 2008, to cut greenhouse emissions” by having “significant adverse effects”. It was the first big project rejected because of the climate crisis. However, Andrea Leadsom, secretary of state for business, energy and industrial strategy, rejected the advice and gave the go-ahead in October. Now ClientEarth has been given permission by the high court to sue ministers, with the case expected to be heard in about two months. The environmental lawyers have previously inflicted three defeats on ministers over their failure to tackle air pollution. “With scientists ringing the alarm bells for decades, we shouldn’t need to take the government to court over its decision,” said Sam Hunter Jones, a lawyer at ClientEarth. “[Leadsom’s] decision is at odds with the government’s own climate change plans. As the planning inspectorate found, if this plant goes ahead the public risks a carbon budget blowout, or a huge stranded asset that would require propping up by the taxpayer, or a combination of the two.” A Drax spokeswoman said the company’s ambition was to be removing, not adding carbon to the atmosphere, by 2030. It would do this by burning wood or plants and then capturing and storing the emissions. She said Drax’s carbon negative ambition could be achieved alongside “new, high efficiency gas power capacity as part of our portfolio” and provide electricity when the wind was not blowing or the sun shining. The UK government’s actions to tackle the climate emergency are under particular scrutiny this year as it will host a vital UN summit in Glasgow in November. The world’s nations must dramatically increase their pledges to cut carbon emissions at the summit to avoid a disastrous 3-4C rise in global temperatures. The government is to bring its environment bill before parliament on Thursday, which it said underlined its commitment to tackling the climate crisis. The Guardian revealed last week that more than 90% of the £2bn in energy deals struck at a UK-Africa investment summit were for fossil fuels. In its planning application, Drax said its proposal for four new gas turbines was warranted to replace its existing two coal-fired units ahead of the government’s proposed phase-out of coal in 2025. It said the new gas plant would be “capable” of having carbon capture technology fitted in the future. In overruling the planning inspectorate, Leadsom argued that the plant’s high carbon emissions were not a reason to block approval under the existing rules. “While the significant adverse impact of the proposed development on the amount of greenhouse gases emitted to atmosphere is acknowledged, the policy set out in the relevant National Policy Statements makes clear that this is not a matter that should displace the presumption in favour of granting consent.” ClientEarth says the government’s latest forecasts estimate the UK will need 6GW of new gas generation up to 2035. The UK has already approved more than 15GW of large-scale gas plants, it said, so approving Drax’s project would take this to three times the government’s estimates. The environmental lawyers argued the combination of the project’s large scale, level of carbon emissions and long operating life made it a significant threat to the UK’s carbon targets. The planning inspectorate also concluded that wind and solar power would cut payers’ bills, while the proposed gas plant would not. “Both [Drax] and [National Grid] confirmed that it is the production of renewable plants that will deliver cheaper energy.”"
"

Have you ever wondered why the vast majority of plants and trees have green
leaves and not some other color?
It’s always been a bit of a mystery why plants absorb red and blue light, but
reflect green, allow us to see the leaves as green. It seems inefficient of
nature when the sun emits the peak energy of its visible spectrum in the
yellow-green areas. A new theory offers one possible answer: that the first
chlorophyll-utilizing microbes evolved to

exploit the red-and-blue light that older green-absorbing microbes didn’t use,
eventually out-competing them through

greater efficiency and the rise of oxygen.
If that were the case, plant life long ago may have had purple leaves to
catch both the red and blue portions of the spectrum. For those whom don’t know
this, RED + BLUE =
MAGENTA (purple)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6e7304d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

OK, _The New York Times per se_ has not weighed in with harsh criticism, but Prof. Hal Varian of U. Cal. Berkeley, a contributor for the NYT’s excellent “Economic Perspectives” column, weighs in today with a nice summary of the problematic assumptions made by Sir Nicholas Stern in the oft‐​quoted Stern Review on the Economics of Climate Change. For those who don’t recall, Stern argued that it makes sense to spend 1 percent of the world’s GDP to reduce greenhouse gas emissions because the costs associated with those emissions might total anywhere between 5–20% of global GDP some time down the road.



Regular readers here will notice that Prof. Varian’s arguments closely mirror those I made earlier on this page (for the curious, here and here, with a minor correction to the latter here).



So it’s not just me folks .…. 
"
"

A “computer glitch” when the reporter sent my story to copy editing added an extra “o” to the word “Outlook” in the title, sending my entry into “etherland”. 
You can view the entire Outlook Special online at:
http://www2.chicoer.com/specialSections/Outlook_2007/Outlook_2007.pdf (takes awhile to download, my article on Page45, which they added afterwards)
Or you can read it below. If you have been thinking about putting solar on your home, here is a guide. Enjoy.
ER Sustainability Outlook 02/27/07
Sustainability is a trend that is growing not only here, but also throughout the world.
It is an attempt to provide the best outcomes for the human and natural environments both now and into the future.  Essentially you could think of it as balanced use of the planet, where the use doesn’t outstrip regeneration.
Locally there have been a number of movements towards this goal, particularly with solar power. Butte County is particularly well suited for solar power. Climate records show that we have 219 sunny days and 57 partly cloudy days per year on average, which makes solar power viable. It wasn’t always that way, and it’s only now that solar power is becoming economically viable due to increased electricity costs, increased solar cell efficiency, and state rebate programs to help home and business owners kick-start the process.

There’s three reasons to do solar power on your home or business:
1 – You want the economic benefit of reduced power costs.
2 – You want to do something environmentally sound.
3 – You have no other power options available, such as a summer cabin.
Most often it’s the first two, but there are some limits you should be aware of related to economics. Solar power can be an expensive proposition to install, even with rebates. Thus unless you have money to burn you have to plan it carefully to ensure that you get payback on your investment. You also need an unobstructed view of the southern sky.
I myself have placed two solar power systems into use, one on my home, and another for Chico Unified School District on Little Chico Creek School, which is the largest solar power system for a school north of Sacramento.
In both cases, there were high power uses going on, which made the economics easy. My home had a deep well, a pool, and an upper and lower A/C unit, making my power bills hit as much as $500 per month in the summer! I’m studying a design for a third solar power system on my new home, purchased just last year, but its more energy efficient, making the planning task more detailed.
Typically, you’ll need to have a power bill of at least $150-200 per month or more to make solar viable for your home as a retrofit. However, if you are building a new home, planning solar into the building process is less expensive.
Some forward thinking developers are now offering turnkey solar built into new homes, such as is being done in Fresno. So far, I haven’t seen Chico developers offer such an option, but I think the time is right for our Building Industry Association, Chamber of Commerce, and City Government to work together to make such an offering practical.
The way solar power works for homes and businesses is by a reverse metering scheme based on Time Of Use (TOU). During peak power need times of noon to 6PM on weekdays, electricity is far more valuable than during off-peak times. PG&E will credit any power generated during those peak times as much as 4 times the value of electricity used during off-peak times.
It’s sort of like the stock market, sell high and buy low.
To achieve this, your home or business has to outfitted with a TOU Meter, so that PG&E can track when you use power. Then when you connect a solar power system to that, it will log when you are generating power during midday peak times, and when you are drawing power during off-peak times. The trick is to generate exactly enough power to result in a net-zero energy use, because PG&E does not pay you back for any excess power generated.
A solar power system generates DC voltage from the solar panels, and when they are working at peak you can expect a 15% solar to electricity power conversion efficiency. The DC power from the solar cells must then be converted and phased to match the 60 cycle AC power grid. This is done with DC to AC inverters, usually mounted near your mains breaker box. There’s about a 10% conversion loss in that process.
If you are planning to do solar, there are a few things you should know:
· Pick a reliable contractor experienced with the process, particularly with the California Energy Commission rebate process, because a mistake there can cost you a lot of time and money.
· Be prepared to spend money or to seek financing. There are low cost state-sponsored finance programs available.
· Be patient. The process takes time, often more than you think, especially in a retrofit. There are applications, permits, tests, and government interactions involved.
· Solar will immediately add to the resale value of your home, that value never decreases. So when you get a state rebate, say for $10,000 towards the purchase, you get to keep that as equity.
· Financing should be balanced in such a way so that it is equal to or less than your average existing electricity bill, so that you are paying yourself back. When the system is paid off, then you’ll have zero payments for energy.
· You’ll be switched to a yearly billing system rather than a monthly. If your solar system doesn’t produce enough electricity to cover all your use, at the end of the year you’ll have what’s called a “true-up” bill, which could be large, but averaged over the year will be much smaller. Be sure to plan for that.
· Right now, solar isn’t for everyone as its still a rather expensive and complicated process to install as a retrofit. However, as solar panel efficiencies increase, and more companies get online producing solar cells, the costs will come down, as happens with any new technology.
· There are state and federal tax credits for any solar installation which when figured in with rebates, can make the project quite attractive, and in some cases, a very low cost.
Given that energy demands are only going to go up, and prices will naturally follow that demand, if you have high electric bills or have a business that could benefit, solar power is certainly worth looking into.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7d4c3e0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Well I just finished Day1 at the conference at UCAR (University Corporation for Atmospheric Research) put together by Dr. Roger Pielke, and sponsored by the National Science Foundation titled: Detecting the Atmospheric Response to the Changing Face of the Earth: A Focus on Human-Caused Regional Climate Forcings, Land-Cover/Land-Use Change, and Data Monitoring.
The day started off bright and early with the shuttle to the NCAR headquarters, shown above. It’s a unique place, at over 6000 feet up right next to the “flatirons“. Once there, we learned that the conference had been moved to downtown Boulder (somebody forgot to tell the shuttle driver). So we had to wait for the shuttle to return. A new one arrived, and we piled in. Then we sat there and waited because others were coming. As we waited in the sun, someone remarked, “It’s getting hot in the van, open your window” to which I remarked “well, with all these windows, it’s a simple greenhouse experiment”. That brought a chuckle, then “no, really, open he window”. So 10 minutes later, we were on our way in a van that holds 12, we had 7.
The driver informed us he had two stops to make to pickup additional people. We added three at the first stop, and at the second stop, at the invitation of the driver (I don’t mind if you don’t ) we added 6 more people, for a total of 16, all crammed into a van that holds 12.  After that exercise I quipped: “well in addition to our earlier greenhouse experiment, now we are adding population growth in an urban setting” which drew a big laugh – inside joke for climate science, you had to be there.
At the conference we had a busy day, lots of papers on land use changes, urbanization studies, rainfall studies, and one statistical study which really caught my eye because I had lunch with the presenter and he gave me the real inside scoop on the “adjustments” process used to turn raw temperature data into “usable” data. More on that later.
I felt a bit out of place at first, because I’d been away from the scientific community for awhile, and this was the first presentation of this type (mine comes tomorrow) in about 25 years. So I was a bit nervous. That soon faded, as people whom I’ve never met saw my name tag, came up and introduced themselves, and said things like “I’ve been following your work, I’m really looking forward to seeing what you’ve found” “after what I’ve seen on your website, I’m beginning to think the surface temperature record is hopeless, and we should focus elsewhere”. So I started feeling a bit more confident. I didn’t see anybody packing rotten tomatoes, and everyone was very nice today, so I’m hoping for the best tomorrow.
Of course Roger Pelke Sr. was a most gracious host, as was his assistant, Dallas, and it was a comfortable and easy day thanks to their efforts.
Later I’ll have a short summary of some of the papers presented today.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4c6bd7e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe current furor about an alleged connection between climate change, CO2 emissions, and Australian fires finds no support in the scientific literature. According to scientists, rising CO2 concentrations reduce fire ignition and burned area. Further, both global-scale and Australian fires were far more pervasive during the colder Little Ice Age.
Here’s what the scientific literature has to say about fires and their connection to climate and CO2 concentrations.

1. “Elevated CO2 and warmer climate promote global total tree cover” and higher CO2 “leads to reduced fire ignition and burned area” (Chen et al., 2019).
Image Source: Chen et al., 2019

2. Globally, fires were much more common during the colder Little Ice Age (Yang et al., 2007; Ward et al., 2018; Doerr and Santin, 2016), or before 1900. Fire frequencies have been rapidly declining as CO2 emissions began abruptly rising in the 1940s.

Image Source: Yang et al., 2007

Image Source: Ward et al., 2018

Image Source: Doerr and Santin, 2016

3. There has been a continued decline in global fire since the 21st century began (Earl and Simmonds, 2018).

Image Source: Earl and Simmonds, 2018



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




4. Australia’s mainland experienced far more pervasive fire during the 1800s to early 1900s, or the Little Ice Age. There has been an abrupt decline in fire activity for the entire Australasian region in the last 50 years (Mooney et al., 2011).
Image Source: Mooney et al., 2011

5. The “assumed positive relationship between drier climates and biomass burning” is not supported by wetter Little Ice Age climates coeval with more burned area in Australia (Tibby et al., 2018).

Image Source: Tibby et al., 2018

6. Australia, like the globe, has neither become wetter or drier over the last 3 decades. There is “no evidence” global precipitation patterns have been been altered by global or regional temperature changes (Nguyen et al., 2018).

Image Source: Nguyen et al., 2018

Image Source: Nguyen et al., 2018

To summarize, the scientific literature does not lend support to claims fires in Australia are connected to warming, rising CO2 emissions, dry climates, or wet climates.
If there were a potential climate linkage, it would be that enhanced fire activity arises in cooler climates.
In other words, there is no apparent link to anthropogenic global warming that can be supported by evidence found in the scientific literature.
Share this...FacebookTwitter "
"The African continent is a “blind spot” for coverage of the humanitarian crises that are being fuelled by the climate emergency, according to a new analysis [pdf]. Madagascar’s chronic food crisis, where 2.6 million people were affected by drought in 2019, came top of the list of 10 of the most under-reported crises last year, Care International’s annual survey found.  Others included Zambia, a country on the frontline of the climate emergency, with 2.3 million struggling to eat due to drought, and Kenya, which received only 20% of expected rainfall in 2019, and where 1.1 million people were hungry amid both floods and drought. Last year, climate activism led by Swedish teenager Greta Thunberg dominated headlines in the northern hemisphere, but the suffering of millions of people in food poverty caused by global heating in the south was not being covered, according to the research. Nine of the 10 countries in which at least one million people were affected by natural or man-made disasters to receive the least media attention were in Africa, where temperatures are rising at twice the global average, according to the Intergovernmental Panel on Climate Change. “We’re seeing increasing linkages between the effects of man-made climate change and the longevity and complexity of humanitarian crises,” said Sally Austin, international head of emergency operations at Care. “From Madagascar to Lake Chad to North Korea, the majority of crises ranked in our report are partly a consequence of declining natural resources, increasing extreme weather events and global warming more broadly.” “What the report does is to highlight those 10 countries which received the least amount of media coverage. Is this because people aren’t interested in reading about it? Should we be thinking: ‘Is this good enough?’” North Korea and Eritrea, both highly secretive states where press freedom is limited and reporting is restricted, were also on the list. “The increased public attention for the global climate crisis is encouraging, but we must ensure that the conversation is not limited to the global north and much-needed transformations there,” Austin said. The countries with most media coverage of humanitarian crisis were Syria and Yemen and the Democratic Republic of Congo, all countries with ongoing conflict. For its fourth annual survey, Care used the Meltwater group to monitor and analyse 2.4 million online sources, in English, French, German, Spanish and Arabic. A list of 40 humanitarian crises in which a million people were affected was monitored from January 2019 until 15 November. The other countries included the Central African Republic, which was ranked second after Madagascar, due to ongoing conflict; Burundi, where instability is causing displacement and 1.7 million people are hungry; and Burkina Faso, where a quarter of the population, 5.2 million, are affected by escalation of violence. Also among the areas listed were Ethiopia, one of the world’s most drought-prone countries, where 7.9 million people are suffering a cycle of disaster, hunger and displacement, and the Lake Chad basin, where 10 million people are in need due to conflict, displacement and hunger, partly due to the lake’s shrinking.  The report found a correlation between media coverage and funding received: three of the 10 most under-reported crises in the report are also on the UN’s 2019 list of most underfunded emergencies."
"

Mila Zinkova of San Francisco who took this picture of the setting sun on Dec. 29, 2006
You have probably heard something about green flashes, but may not have seen one. If so, you’ll be happy to find that a number of pictures of green flashes are available on the Web like the one above. The one pictured above is special because its a TRIPLE green flash which is exceedingly rare.  Its explanation lies in refraction of light (as in a prism) in the atmosphere and is enhanced by layered atmospheric inversions and possibly fog.
There was a time when green flashes were thought to be fables. Jules Verne, of all people, fixed them as real in his 1882 novel “Le Rayon Vert” (The Green Ray). He described “a green which no artist could ever obtain on his palette, a green of which neither the varied tints of vegetation nor the shades of the most limpid sea could ever produce the like! If there is a green in Paradise, it cannot be but of this shade, which most surely is the true green of Hope.”
Green flashes are real (not illusory) phenomena seen at sunrise and sunset, when some part of the Sun suddenly changes color (at sunset, from red or orange to green or blue). The word “flash” refers to the sudden appearance and brief duration of this green color, which usually lasts only a second or two.
For an explanation along with some great pictures of the atmospheric optics involved in green flashes and other sorts of colorful atmospheric phenonmena, I recommend this website in the UK: http://www.atoptics.co.uk/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea82e805f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Who can forget the cartoon scene shown above where Wile E. Coyote has a little trouble controlling some tornados he seeds in yet another futile attempt to capture the elusive Road Runner?
About ten years ago I thought of creating a computer game where you could create and steer your own tornado, but then I quickly thought to myself “I’d instantly be laughed and ridiculed out of the TV weather business for making such a socially distasteful product” so the thought passed just as quickly as an F0 twister.
Today I’m wandering COMPUSA in LA and what do I see? A game called Tornado Jockey.
From the game’s website description: “Target different objects like: vehicles, baseball stadiums and drug stores. Steer your storm away from forces like ‘Ray Gun Trucks’ and ‘Radar Bombers’ that aim to kill your tornado. You’re at the helm of mother nature’s energy, so pick your path and have a little fun!”
Then they go on to add: * Percentage of proceeds are donated to the American Red Cross
Yeah…. surely that makes it far more… umm…socially correct?
Here’s a screenshot of what the game looks like in operation:


Some of the game features:
– In-game objects: dairy farms, gas stations, amusement parks, & more (what, no mobile homes?)
– Enemies like: ‘F-Killer rockets’, ‘Storm Chasers’, & ‘Ray Cannons’
– Educational facts for: funnels, tornado types, supercells, etc.
– Dynamic game-play, original settings, outstanding special effects!
There’s nothing like seeing “educational facts” while ripping apart an amusment park full of kids.
But the icing on the cake has to be the game summary, where a blonde “weather babe” does a live TV report and tallies up your damage score in dollars.

Please excuse me while I go practice some power hurling into a wastebasket.   Blllluurrrrch!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea85d9209',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterStefan Rahmstorf on the IPCC modelling breakdown: Reason to breathe a sigh of relief, new climate models are far too sensitive.
By Die kalte Sonne
(Translated by P. Gosselin)
DER SPIEGEL provides a regular platform for the controversial climate scientist Stefan Rahmstorf. On 12 May 2020 he was allowed to:
Stronger temperature rise: Why the climate models are running hot
A guest article by Stefan Rahmstorf
New calculations have alarmed the scientific community – they suggest the earth could be more sensitive to greenhouse gases. Will global warming be stronger than previously thought?
Here the quick reader will suspect one of the usual Rahmstorf climate alarm pieces. And this is exactly how the beginning of the article reads. However, it deals with a tricky topic that will certainly hit the Potsdam scientists quite hard to the stomach.
Huge mishap
In the course of the preparation of the 6th Climate Status Report, the IPCC has again run a large number of climate models. This time, however, a huge mishap has occurred. Several of the models have delivered far too much warming, which is not compatible with the measured data of the last decades. This fundamentally casts the models into question. They suggest that the warming effect of CO2 is far too high. A scandal that should actually cast everything into question.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Rahmstorf plays it dumb at the beginning of the article, luring his readers into the alarm trap. Will everything get much worse than expected? This is the typical Rahmstorf narrative.
“Models are crap”
But if you can make it to the end of the article, you will be surprised. Rahmstorf actually admits quietly that the models are crap, running way too hot.
In reality it’s all not so bad. Rahmstorf writes literally in his article:
The comparative study by researchers from the University of Exeter now shows that in particular the warming since 1975 – i.e. most of the modern global warming – is clearly too strong in the sensitive models. More recent analyses by ETH Zurich, for which more models have already been evaluated, confirm this conclusion. This is a reason to breathe a sigh of relief: there is currently some evidence that these models are not better than the old ones, but are simply too sensitive.“
Did SPIEGEL force its guest author to write this article? Was this a prerequisite for him to continue writing there? A balanced presentation with a fair evaluation of all opinions represented in science has never really been Rahmstorf’s strength.
Obvious failure
Or was it a flight to the front because the modelling failure was all too obvious and Rahmstorf feared complete professional isolation? It’s hard to say.

Stefan Rahmstorf must have struggled for several months before deigning to admit this mishap. This certainly could not have been easy for him.
By the way, here in the blog we have already reported on the topic several times: “The sun in February 2020, science against doom and gloom” and “The sun in November 2019 , when models exaggerate” and “The sun in December 2019, advances in climate science“.


		jQuery(document).ready(function(){
			jQuery('#dd_a5e15193542096fb84730e25e385a8c3').on('change', function() {
			  jQuery('#amount_a5e15193542096fb84730e25e385a8c3').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"

This picture comes to me via www.surfacestations.org courtesy of Dr. Roger Pielke Sr. of the University of Colorado.
It is the US Historical Climatological Network (USHCN) Station of Record for Hopkinsville, KY. The NOAA provided Max/Min Temperature Sensor is located at the observers home. The nearby air conditioner is just 10 feet from the temperature sensor. Then there’s the chimney. The contribution of the portable BBQ grill to the temperature record is unknown.
The MMTS temperature sensor wasn’t always mounted on the tower next to the house, it used to be in the yard, but the observer made some “improvements” over time. Note that published NOAA/NWS siting standards require a 100 foot distance from buildings.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5a35352',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

_The_ _Washington Post_ writes about how President Obama became obsessed with grabbing our complex energy systems by the scruff of the neck and shaking them into something more appealing to Ivy League planners. I was struck by this vignette: 



But even before the late‐​night session in July, Obama had begun to educate himself about energy and climate and to use those issues to define himself as a politician, say people who have advised him. He read a three‐​part New Yorker series on climate change, for instance, and mentioned it in three speeches.



It’s great that he read a three‐​part series in the New Yorker. But has the president ever actually read anything by a climate change skeptic? Actually, a better term would be “a climate change moderate.” Leading “skeptic” Patrick J. Michaels, for instance, of Cato and the University of Virginia, isn’t skeptical about the reality of global warming. His summary article in the Cato Handbook for Policymakers begins: 



Global warming is indeed real, and human activity has been a contributor since 1975.



But he also notes that climate change is complex, and its policy implications are at best unclear. “Although there are many different legislative proposals for substantial reductions in carbon dioxide emissions, there is no operational or tested suite of technologies that can accomplish the goals of such legislation.” The flawed computer models on which activists rely cannot reliably predict the future course of world temperatures. The apocalyptic visions that dominate the media are not based on sound science. The best guess is that over the next century there will be very slight warming, without serious implications for our environment our society. Michaels’s closing appeal to members of Congress would also apply to President Obama and his advisers: 



Members of Congress need to ask difficult questions about global warming.   
  
  
Does the most recent science and climate data argue for precipitous action? (No.) Is there a suite of technologies that can dramatically cut emissions by, say, 2050? (No.) Would such actions take away capital, in a futile attempt to stop warming, that would best be invested in the future? (Yes.) Finally, do we not have the responsibility to communicate this information to our citizens, despite disconnections between perceptions of climate change and climate reality? The answer is surely yes. If not the U.S. Congress, then whom? If not now, when? After we have committed to expensive policies _that do not work_ in response to a misperception of global warming?



Please, President Obama — in addition to the lyrical magazine articles on the apocalyptic vision that you read, please read at least one article by a moderate and widely published climatologist before rushing into disastrously expensive policies.
"
"Australia’s bushfire crisis is expected to contribute up to 2% of what scientists forecast will be one of the largest annual increases in atmospheric carbon dioxide on record. The atmospheric concentration of the heat-trapping gas is projected to peak at more than 417 parts per million in May, and average about 414.2 parts per million for the year, according to the forecast by the British Met Office. It is a 2.74ppm increase above the 2019 average.  Science agencies have associated concentrations of more than 450ppm with average temperature rise of 2C above pre-industrial levels, a point at which some catastrophic effects of global heating may become irreversible. In its statement explaining the forecast, the Met Office Hadley Centre highlighted the role of the Australian bushfires in contributing to the unusually large forecast annual rise. Richard Betts, a professor with the Met Office and the University of Exeter, said human-induced climate change and local weather patterns had contributed to the hot, dry weather that played a key role in the severity of the Australian bushfires, which in turn increased emissions. Fires have affected more than 12m hectares of country this season, including more than 5m hectares in New South Wales. Betts said fossil fuel burning and deforestation had driven a year-on-year increase in carbon dioxide levels since 1958, when readings began at the Mauna Loa Observatory in Hawaii, but the rate of the increase fluctuated depending on how much was absorbed by forests and other ecosystems. The response of ecosystem carbon sinks was expected to be weaker than normal for a second year running as warmer and drier conditions limited the ability of plants to grow and absorb carbon dioxide, increasing the risk of wildfires that released more emissions. “While human-caused emissions cause the carbon dioxide rise in concentration, impacts of weather patterns on global ecosystems are predicted to increase the rise by 10% this year,” the statement says. It says emissions from the Australian fires contribute “up to one-fifth of this increase”, or up to 2% of the total rise. Emissions from bushfires are usually considered to be climate neutral in carbon accounting, based on the assumption that forest regrowth absorbs a similar amount of carbon dioxide as was released. But scientists increasingly warn this is likely to be optimistic as many burned areas never recovered to their pre-fire state. Australia contributes about 1.3% of annual global emissions under UN greenhouse accounting rules. Bill Hare, the chief executive of Climate Analytics, said the wildfire emissions would not be counted in Australia’s emissions due to the way the country constructed its greenhouse gas reporting. “Along with other accounting tricks, such as Kyoto carry over, it is a clear global signal of the effect of Australia avoiding its responsibility to take real action,” he said. Ken Thompson, a former NSW deputy commissioner for fire and rescue, said it should not be a surprise that Australia’s fires were making a mounting contribution to the climate crisis. “Our emissions from bushfires keep going up as a result of our failure to address climate change, creating the conditions for even worse fire seasons to come,” he said. Met Office research suggests a 2C rise in global temperatures is likely to lead to between 20 and 30 extra days of “very high” fire risk in south-western and eastern Australia each year. It found the area at risk of “extreme” fire danger conditions could increase by about 20m hectares, roughly the size of Victoria."
"The Atlantic Ocean’s surface temperature swings between warm and cold phases every few decades. Like its higher-frequency Pacific relative El Nino, this so-called “Atlantic Multidecadal Oscillation” can alter weather patterns throughout the world. The warmer spell we’ve seen since the late 1990s has generally meant warmer conditions in Ireland and Britain, more North Atlantic hurricanes, and worse droughts in the US Midwest. However a colder phase in the Atlantic could bring drought and consequent famine to the developing countries of Africa’s Sahel region. In the UK it would offer a brief respite from the rise of global temperatures, while less rainfall would mean more frequent summer barbeques. A cold Atlantic also means fewer hurricanes hitting the southern US.  The good news is our latest research, published in the journal Nature, gives us a much better understanding of these Atlantic oscillations. We now know that accelerations in sea-level rise in cities like New York and Boston on the north-east coast of the US are linked to a cold spell in the Atlantic.  The bad news, at least if you’re an African farmer or have a coastal property in New England? We’re about to go into a cold phase. Once we adjust for overall global warming trends, an oscillation in Atlantic sea-surface temperatures emerges. The ocean went through a warm period in the 1930s/1940s and again in the 1990s/2000s. However the 1970s/1980s were much cooler and there are hints of a transition to a relatively cold period at the moment. This is known as the Atlantic Multidecadal Oscillation (AMO), and the transition between its positive and negative phases can be very rapid. For example, Atlantic temperatures declined by 0.1ºC per decade from the 1940s to the 1970s. By comparison, global surface warming is estimated at 0.5ºC per century – a rate twice as slow. In many parts of the world, the AMO has been linked with decade-long temperature and rainfall trends. Certainly – and perhaps obviously – the mean temperature of islands downwind of the Atlantic such as Britain and Ireland show almost exactly the same temperature fluctuations as the AMO. Atlantic oscillations are associated with the frequency of hurricanes and droughts. When the AMO is in the warm phase, there are more hurricanes in the Atlantic and droughts in the US Midwest tend to be more frequent and prolonged. In the Pacific Northwest, a positive AMO leads to more rainfall. A negative AMO (cooler ocean) is associated with reduced rainfall in the vulnerable Sahel region of Africa. The prolonged negative AMO was associated with the infamous Ethiopian famine in the mid-1980s. In the UK it tends to mean reduced summer rainfall – the mythical “barbeque summer”. Because the AMO influences a range of climate conditions in different parts of the world, it is important that the mechanisms driving it are properly understood. Scientists have widely hypothesised that ocean circulation, and in particular the Atlantic meridional overturning circulation which sends warm surface water northward (the Gulf Stream) and deeper cold water southward, drives the phases of the AMO by moving heat around. However, we do not have direct observations of ocean circulation of sufficient duration to support this theory, which has lead some to question whether the AMO is actually controlled by the ocean. We have measurements of the strength of the Gulf Stream flow in the Straits of Florida since 1982 and the flow across the Greenland-Scotland ridge since the mid-1990s. Since 2004, we also have continuous, full-depth, basin-wide measurements of the Atlantic overturning circulation with the RAPID monitoring project at 26ºN. However, none of these records are long enough to directly link ocean circulation with the decadal climate variations such as the AMO. Sea-level measurements from tide gauges on the other hand extend back more than 100 years in places. Many studies, dating back to 1938, have used this data to study variations in ocean circulation. In our latest research we were able to show that differences in sea level along the US east coast provide a measure of the strength of ocean circulation. Sea-level fluctuations from Florida to Boston can be divided by Cape Hatteras, where the Gulf Stream leaves the coast to flow eastward. The difference (south minus north) is representative of ocean circulation, and more circulation means more heat is transported. By comparing our sea-level index against the AMO index we were able to provide, for the first time, observational evidence of the widely hypothesised link between ocean circulation and the AMO. Our results show that ocean circulation responds to the first mode of Atlantic atmospheric forcing, the North Atlantic Oscillation, through circulation changes between the subtropical and subpolar gyres – the intergyre region. This a major influence on the wind patterns and the heat transferred between the atmosphere and ocean. The observations that we do have of the Atlantic overturning circulation over the past ten years show that it is declining. As a result, we expect the AMO is moving to a negative (colder surfer waters) phase. This is consistent with observations of temperature in the North Atlantic."
"The amount of renewable, low-carbon, energy the UK produces is increasing, but it is very different to traditional types of power. It can’t just be turned on when wanted. As a result, the capacity market scheme – essentially a programme of subsidies – was set up to help provide backup power when the supply of renewable energy the UK produces is outpaced by demand.  Until recently this was the flagship mechanism for helping the UK meet its climate change reduction targets while maintaining a secure electricity supply. However, on November 15 the European Court of Justice ruled that the European Commission had failed to properly investigate the scheme, rendering it unlawful. Despite its huge implications, this news was buried beneath blanket coverage of prime minister Theresa May’s Brexit deal, which came out at the same time.  Participants in the capacity market scheme – predominately fossil fuel-based power suppliers – were paid per megawatt (MW) for the capacity they offered. In order to offset the risk of investing in less predictable supply, fixed monthly payments were made to these suppliers. The first contracts were agreed at the end of 2014 and their requirement to start providing capacity started last winter.  Funding of up to £2.6 billion a year was set aside in 2012 by the UK government, as it was deemed the most appropriate mechanism for helping meet demand in an increasingly supply-led system.  However, in 2014 Tempus Energy raised objections about the scheme to the European Court of Justice. Its stance was that it favoured the use of fossil fuels rather than considering other mechanisms, such as flexible price incentives for industry and domestic users, which involve reducing costs when there is an abundance of wind and solar, and increasing the cost when there is not. Smart metering is one way in which this can be done on a domestic scale. This can also happen on an industrial level with incentives to increase or reduce demand based on supply levels, for example, Demand Turn Up or Firm Frequency Response. In essence Tempus argued that the scheme made it harder for it and other demand management companies to compete with the well established fossil fuel-based power generators. Under EU state aid rules, member states are obliged to consider alternative ways of meeting market demand for power before subsidising polluting generators. They also require any capacity boosting measures to be designed in a way that provides “adequate incentives” for operators of new, cleaner technologies.  Tempus data shows that the capacity market had predominantly supported fossil fuel providers. Therefore, the UK had effectively been state funding fossil fuel power generators to be on standby to produce power when, and if, there was a shortfall in the electricity being produced. The concern is that by using the scheme to predominantly fund fossil fuel provision, Britain will continue to rely on such technologies into the future, reducing the ability to cut climate change targets and uphold agreements.  So on November 15, the ECJ ruled in favour of Tempus and stated that the European Commission was wrong to clear the scheme for state aid approval in 2014 without looking into it in more detail. As a result the market has been suspended indefinitely.  The decision means the UK government cannot now issue capacity market payments to energy firms. In addition, it cannot hold any further auctions, including the upcoming auctions scheduled to run in January and February 2019 to secure additional power capacity for upcoming winters.    Britain will, most probably, be able to keep the lights on, but it might come at a higher financial cost in the short term. Energy policy in the UK is incredibly complex. There are numerous incentives, taxes and subsidies all with the aim of reducing cost, reducing carbon and meeting green house reduction targets and producing a sustainable supply.  A review of the cost of energy in 2017, by the Department for Business, Energy & Industrial Strategy, outlined the UK’s energy policy to be lacking in terms of consistency and ability to meet the needs of the country. It stated that, “energy policy, regulation and market design are not fit for the purposes of the emerging low carbon energy market”.  It’s hard to move from a dispatchable system to a more supply-led system. All of the UK’s costing models and energy policies are essentially based on the ability to burn something to provide power, and the economics have developed from the traditional supply-and-demand approach. What Britain now needs to do is move from traditional demand based policies to those which include the whole energy system. The capacity market attempted to do part of this by favouring fossil fuels as a mechanism to cope with short term shortfalls in supply. The UK needs to be doing better than this. It must adopt a truly systems-based approach to dealing with its increasing renewable supply.  This ruling, while problematic in the short term, is a long term opportunity to develop a more dynamic and flexible energy system. The UK must invest in demand management programmes rather than only investing in generating assets. Britain needs to embrace a truly low carbon system with active demand management systems alongside renewable technology – enabling it to phase out its reliance on fossil fuels."
"The Prince of Wales has urged business and political leaders at Davos to embrace a radical reshaping of economies and markets in order to tackle the climate crisis. In a special address at the World Economic Forum on Wednesday, Charles outlined a 10-point plan to help the global economy become more sustainable including the imposition of green taxes and investing in environmentally friendly technologies. He argued that taxes, regulations and policies could all be changed, as part of a drive to reverse environmentally damaging subsidies such as financial assistance for the fossil fuel industry. He also cited the “polluter pays” principle that requires those who create environmental damage to pay for the clean-up. “In order to secure our future and prosper, we need to evolve our economic model,” the Prince told an audience of politicians and business and civic leaders at Davos. “Do we want to go down in history as the people who did nothing to bring the world back from the brink in time to restore the balance, when we could have done?” Charles, who is supported financially by the £1bn Duchy of Cornwall, made a direct challenge to some of the world’s wealthiest people gathered at the Swiss ski resort to think about more than money. “What good is all the extra wealth in the world gained from business as usual if you can do nothing with it except watch it burn in catastrophic conditions?” he said. Charles, who also met the climate activist Greta Thunberg at the summit, urged the private sector to use its ingenuity and practical skills to help lead the world out of a climate calamity. “The only limit is our willingness to act and the time to act is now.” Davos is a Swiss ski resort now more famous for hosting the annual four-day conference for the World Economic Forum. For participants it is a festival of networking. Getting an invitation is a sign you have made it – and the elaborate system of badges reveals your place in the Davos hierarchy. The meeting is sponsored by a huge number of international banks and corporations. For critics, “Davos man” is shorthand for the globe-trotting elite, disconnected from their home countries after spending too much time in the club-class lounge. Others just wonder if it is all a big waste of time.  The 2020 meeting is being advertised as focusing on seven themes: Fairer economies, better business, healthy futures, future of work, tech for good, beyond geopolitics and how to save the planet. Young climate activists and school strikers from around the world will be present at the event to put pressure on world leaders over that last theme.  The prince was speaking after launching an initiative, the Sustainable Markets Initiative and Council. His 10 proposals included the rapid decarbonisation of businesses, driving investment in new environmentally friendly technologies and helping consumers to make sustainable choices. Climate change, biodiversity loss and global warming are the greatest threats humanity has ever faced, he warned, adding that capital needs to be properly deployed in order to tackle these threats. The prince said global consumers could make markets sustainable, but could not make sustainable choices if these options were not clearly laid before them. Charles also warned that being socially and environmentally conscious cannot be an option for wealthier people only. Markets needed to change, so that prices actually reflected the environmental as well as economic costs. “If all the true costs are taken into account, being socially and environmentally responsible should be the least expensive option because it leaves the smallest footprint behind,” he argued, implicitly calling for subsidies and tax changes. Charles’s intervention comes nearly 30 years after he last spoke at the World Economic Forum, when he had already begun campaigning on environmental issues and corporate responsibility. He joked on Wednesday that it had been “an uphill struggle” trying to generate support but he believed the world was now at a turning point. The Prince of Wales arrived in an electric car – rather than the helicopter option favoured by some, such as Donald Trump. However, he reportedly travelled to Switzerland by private jet, a reminder of the gap between the goals outlined at the forum this week and the global elite’s behaviour. Environmental issues and the climate emergency have taken centre stage at Davos this year, with a stream of business leaders and politicians expressing concerns and signing up to initiatives to promote sustainability and climate action. But the meeting also exposed the split between the US and Europe on the issue, with Trump attacking “prophets of doom” in a speech heavily criticised by environmentalists. 1) Put nature and the protection of nature’s capital at the heart of operations. 2) Create responsible pathways to decarbonise to reach net zero, and for governments and businesses to set a clear plan for how they will decarbonise. 3) Reimagine industries through the lens of sustainable markets. 4) Identify game-changing technologies that can speed up the creation of a sustainable economy and eliminate barriers to change. 5) Remove subsidies that prevent the economy becoming more sustainable, and set taxes, policies and regulations in a way that catalyses sustainable markets. 6) Invest in science, technology, engineering and maths skills, and in research and development, to help bring emerging technologies to market. 7) Invest in nature as an economic driver of growth. 8) Agree unified metrics for measuring environmental, social and governance standards, to provide transparency to company’s supply chains. 9) Make it easier for consumers to see which products are ethical and sustainable. 10) Realign investing so it can support sustainability. This would direct trillions of pounds in pension funds, sovereign wealth funds into environmentally responsible projects that offer long-term value and rate of return."
"Some 250 years ago James Watt, then scientific instrument maker to the University of Glasgow, devised the separate steam condenser while ambling through Glasgow Green one spring morning in May 1765. If any single moment can be traced to the rise of fossil fuel consumption then it’s surely this. Watt’s quite brilliant insight was to improve the efficiency of steam power three-fold, arguably pushing Britain 60 years into the future. But not only could Watt’s new machine replace human labour in production, it could efficiently pump water from mines, thereby accelerating the extraction and consumption of coal. What was still mostly a boutique fuel for 18th-century industry and heating would become the fuel of choice to power industrial Britain in the late 19th and early 20th centuries. Now, due to the 21st-century climate risks associated with long-term fossil fuel consumption, the divestment movement is urging institutions to pull their investments from the fossil fuel industry. But those most vocal in calling for a blanket divestment from all fossil fuels often ignore the nuances of prior energy transitions, underestimate the scale of the challenge and some even oppose the growth of proven low-carbon technologies such as nuclear power. If climate is the issue and carbon is the problem, then it’s perhaps surprising that our energy economy has actually been decarbonising in relative terms for centuries. Not all fuels are alike, and historical energy transitions from wood to coal and then through oil, methane and nuclear have led to a steady fall in the amount of carbon emitted per unit of energy delivered.  The underlying connection between these prior energy transitions has been a steady increase in the energy density of fuels, for example from carbon-rich coal to hydrogen-rich methane, which packs a greater energy punch weight-for-weight. Of course accumulated emissions have grown steadily during this period, but that’s due to the tight link between economic growth and energy consumption during rapid industrialisation. A pragmatic energy policy would therefore build on an acceleration of these historical long-waves of improving energy density. Such a programme would feature a large-scale shift from carbon-rich coal to hydrogen-rich methane and carbon-free nuclear, both of which can provide power on demand. Although renewables will no doubt play a significant role in the future, they are diffuse and so have a poor power density and work only intermittently, bucking the long-term historical trend. While nuclear can often directly replace coal, wind and solar still require fossil-fuelled plants to ensure electricity keeps flowing on still, cloudy days.  Industrial-scale batteries or other energy storage systems can be a partial solution, but they’re an additional capital cost to deal with intermittency and the scale of storage required is immense. But changes can also come from within the fossil-fuel industry itself. Only the oil and gas industry has both the capital and the engineering expertise to make coal cleaner through carbon capture and storage or partly replace carbon-rich coal with hydrogen-rich methane. Indeed moving from coal to methane as a fuel of choice is one of the easiest and most cost-effective energy transitions we could make in the near-term. This can clearly be seen through the significant decline in US emissions in recent years, which is largely due to cheap shale gas displacing coal. The very real success of the US shale gas industry in both economic and emission terms exposes a problem for the uncompromising position on all fossil fuels taken by the divestment movement. In terms of actual real-world divestment actions, it’s worth noting that Norway’s vast sovereign wealth fund has adjusted the spread of its energy portfolio, entirely divesting from coal but making careful reinvestments in oil and gas. Similarly, Shell’s recent acquisition of natural gas company BG Group underlines the push to replace coal with gas, following the long historical trend from carbon-rich to hydrogen-rich fuels.  The divestment movement also needs to deal head-on with its often highly contradictory views on nuclear energy. Veteran climate scientist Jim Hansen states that believing in a rapid transition away from fossil fuels using renewables alone is akin to “believing in the Easter Bunny”. He also slates many of the leading environmental groups as being one of the greatest obstacles to emission reductions due to their uncompromising anti-nuclear stance.  Simultaneously campaigning for divestment from all fossil fuels and an end to low-carbon nuclear energy reveals that, for some at least, the divest campaign is not simply about climate, it’s about a much broader agenda. If we’re serious about reducing carbon while delivering modern energy services for all, we need to focus on innovation. We should remember that when James Watt’s efficient new steam engine began to displace Thomas Newcomen’s earlier machine, it didn’t require top-down political targets or state incentives to encourage the take-up of the new technology. Watt’s idea succeeded simply because it required only one third as much coal to deliver the same quantity of mechanical work.  If we can develop cheaper, cleaner alternatives to fossil fuels over the coming decades which are truly scalable, then the subsequent fall in energy prices will reach our fellow citizens in the developing nations living with the impacts of permanent energy austerity and as a side effect the climate problem will ultimately solve itself. Finally, as we look to the future from the 250th anniversary of James Watt’s invention, we quickly need to rediscover his enlightenment-era inventiveness and reclaim the notion of a progressive, human-centred future. In the long twilight of the wood-burning era before James Watt we worked for energy, but now energy works for us, freeing us to pursue all that is good in our modern civilisation.  We also need to be clear that in the long-term, the deeply intertwined issues of energy and climate won’t be solved by climate science; they will be solved by engineering science, while global energy poverty will be solved by deflating, not inflating energy prices.  James Watt’s initial is stamped on every light bulb, measuring the power it delivers, but also reminding us of the intellectual light and sheer progress he brought to the world. Rather than obsess over the symbolism of divestment, this is what we need to invest in."
"Curious Kids is a series for children of all ages, where The Conversation asks experts to answer questions from kids. All questions are welcome: find out how to enter at the bottom of this article.  Why do leaves change colour in autumn? – Isaac, age eight, Guildford, UK Hi Isaac, this is a really interesting question and something that lots of people wonder about when the seasons change. In the autumn, lots of plants (especially trees) throw away their leaves.  These are great for jumping in, but why do some plants do this? It seems like a waste. But actually, by dropping their leaves they are saving their nutrients for the next summer.  For plants to grow, they need sunlight, nutrients and water. The nutrients and water come from the soil. The sunlight is captured by the leaves.  To capture the sunlight, the leaves use a chemical called chlorophyll, which is what makes leaves green. Chlorophyll turns sunlight into food, which the trees need to grow, through a process called photosynthesis.  In summer, plants do lots of photosynthesis, because they get lots of light and because it is warm. The food they make is sugar, which they use to grow new leaves, flowers and seeds. In winter, things are less comfortable. The days get shorter, it gets colder and there is less sunshine. For you, this is not a problem – if you are cold, you can put a coat on. But plants can’t do this. And when it gets really cold, and freezes, their leaves can be damaged. If you want to see what freezing does to different leaves, there is an experiment you can try at home. Take some different leaves and put them in your freezer, or the ice box in your fridge. Leave them for a day to get really cold, then take them out again. Put them on a plate so you don’t make a mess, then just wait for them to warm up (this will take a while).   Some leaves are really tough and don’t mind being frozen. If you take a holly leaf, it will look just the same after you freeze it as it did before. But if you try this with a soft leaf, such as lettuce, you will see something different.  For plants with leaves that don’t like to be frozen, winter is a bad time. Their leaves are all going to be destroyed in the cold weather. If this happens, they will also lose a lot of good things which are in the leaf, especially the nutrients they get from the soil.  They use the nutrients to make chlorophyll and they don’t want to lose them when the leaves freeze. So instead, they break down the chlorophyll to get the nutrients out and store them in their roots, which are protected from the cold. As the plants break down the chlorophyll, the green colour disappears from their leaves. What is left behind is other chemicals which you normally cannot see. The most important of these are called carotenoids, which are what makes carrots orange.  Depending on which chemicals are found in the leaf, they can turn different shades of yellow or orange or even red. These chemicals do not have any nutrients in them, so the plant does not bother to break them down, it leaves them in the leaves.   Once all the chlorophyll is taken out, the leaf dies. As it dries out, the leaf starts to look brown and becomes crispy. At this stage, it falls off the tree. In the spring, as the days get longer and the weather gets warmer, the tree uses the nutrients and food that it has stored in its roots to make new leaves, ready for the summer when it can do lots of photosynthesis again. Hello, curious kids! Have you got a question you’d like an expert to answer? Ask an adult to send your question to us. You can: * Email your question to curiouskids@theconversation.com 

* Tell us on Twitter by tagging @ConversationUK with the hashtag #curiouskids, or

* Message us on Facebook. Please tell us your name, age and which town or city you live in. You can send an audio recording of your question too, if you want. Send as many questions as you like! We won’t be able to answer every question, but we will do our best. More Curious Kids articles, written by academic experts: Why do flies vomit on their food? – Lili, age ten, Adelaide, Australia If you have lots of the thing you’re allergic to, does your body get used to it? – Karen and Dawn, Manchester, UK Why do we need food? – Milo, age five, Cowes, Australia"
"The ancient city of Petra is famous for its spectacular ravines which have been the backdrop to Hollywood movies and countless tourist brochures. However, nearly 4,000 visitors  to the Jordanian ruins narrowly avoided being swept away recently when intense rainstorms turned the dry channels into raging torrents.  Further north, near the capital Amman, 13 people died in similar flash floods later the same day as rainwater inundated farmland along the dry ephemeral,  river channels – referred to as wadis – sweeping away roads and bridges. These recent events follow an even worse loss of life in October when more than 20 Jordanian schoolchildren were killed by flash floods in a wadi near the Dead Sea, leading to governmental resignations and widespread outcry. This is not a problem limited to Jordan. In the last year alone, flash floods have killed people in the US, Italy, Israel and elsewhere. The frequency, spread and scale of these tragedies begs the question: why do so many people, in so many places, seem so unprepared? We do not lack an understanding of flash flooding in arid or semi-arid regions. Intermittent, high volume flows are common in deserts, where infrequent but intense rain falls onto dry, steep hillsides. These drain down dry channel networks, eroding, transporting and depositing large quantities of sediment, ranging in size from microscopic silt particles to enormous boulders. Flash floods are the norm, not the exception in drylands.  The latest events in Jordan centre on a 2,000km² area, referred to as the Wala watershed, just south of Amman. The region has been impacted by the arrival of refugees from the Syrian civil war and earlier migration from Iraq, just to the east, in the 1990s and 2000s.  Wadi Wala was also dammed in 2002 with the aim of pumping stored water into underground aquifers to secure water supplies for the growing populations in the capital, and expanding agriculture and industry. But the drainage network transports so much sediment that the 9m cubic metre reservoir is rapidly filling up. Work is already under way to raise the dam to increase the reservoir capacity.  Agricultural intensification, uncontrolled development and population growth are blamed by some for the recent human costs of the floods. Others have claimed that increasingly intense rainstorms are to blame. Indeed, both factors have significant implications, not just for flood hazards, but also for the management of Jordan’s scarce and increasingly stressed water resources. We argued in a 2016 paper that detailed hydrological modelling of catchments, despite the lack of high quality data in Jordan, is the only way to untangle the complex interactions between human development – urbanisation, agricultural intensification – and increasing extremes of rainfall under climate change. Our latest modelling suggests that intensifying agriculture is playing a role in modifying the hydrology of the area. However, a far more significant effect 
on the volume of water discharged, the ferocity of floods, and the amount of sediment transported is the management and coordination, or lack thereof, of water throughout the region.  A UN FAO report endorsed several small measures, from soil drilling and earth bunds to retain water at the field scale, up to check dams and small scale flood storage distributed along smaller tributaries, to slow the flow and prevent surges accumulating downstream. However, modelling and modest infrastructure projects alone will not solve the problem. Communication and education of public and authorities, informed by the outcomes of said modelling is the real way to achieve the level of cooperation required to reduced the scale of these floods.   Unfortunately, flash flooding – going from long periods of no water at all to sudden, brief, extreme hazard – plays to the same negative psychological conditioning as other intermittent environmental risks. Local peoples are reluctant to invest in protection against a threat which is rarely manifest.  Studies from around the globe highlight the importance of cultural memory and language in determining our resilience and preparedness in respect of flooding. In Chile, subtle changes in the sense of words used locally to describe intermittent or ephemeral channels are seen to correlate with variations in the engineering of nearby infrastructure and the organisation of development. In Sheffield, UK, recent work has identified the importance of cultural memory in determining flood resilience among businesses and districts affected by major floods in 2007. In Jordan, many commentators have cynically noted the contrast in fatalities between the international tourists at Petra, protected by a hi-tech flood warning system, and the rural local population in Wala. Away from the tourist hotspots, what is needed is not sirens and evacuation plans but education and integrated watershed management. Deaths in flash floods and dwindling water resources are in fact both symptoms of a fundamental disconnect between human development and natural systems, in some of the most stressed and precarious environments on Earth."
"
Share this...FacebookTwitterIn her latest panic attack, teenage Swedish climate activist Greta Thunberg – citing the Guardian –  once again appeared to be proclaiming the end of the world was a step closer when she tweeted Antarctica has set a new record high temperature:

20,7°C on Seymour Island off Antarctica… https://t.co/OiIdlQIl6A
— Greta Thunberg (@GretaThunberg) February 13, 2020

Two new warm records
According to the Guardian, “The 20.75C logged by Brazilian scientists at Seymour Island on 9 February was almost a full degree higher than the previous record of 19.8C, taken on Signy Island in January 1982.”
That reading, the Guardian reports, follows the February 6 record of 18.3C recorded at the Argentinian research station, Esperanza measured.
As is the case with most alarmists, every warm single datapoint anomaly gets uncritically accepted with open arms as solid evidence of man-made global warming while cold trends get dismissed or downgraded as “natural variability”.
Seymour Island has been cooling for over a quarter century
So we have two recent warm records set at and near the Antarctic peninsula over the past week or so and that means the region there is heating up, alarmists like Greta and the Guardian want us to believe. But what are the real TRENDS there? Do the 2 recent warm records mean the region is heating up.
Looking at official data from NASA, it turns out that warming isn’t true. And because climate is always changing, the temperature in the region in question has also not remained completely steady. The only possibility left? COOLING.
Seymour Island, also known as Marambio Island is an island in the chain of 16 major islands around the tip of the Graham Land on the Antarctic Peninsula. What follows is a plot of the mean annual temperature measured at Seymour Island – based on NASA data – going back to the time all the global warming predictions began in earnest:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Contrary to that implied by The Guardian and Greta, the island has in fact cooled a bit during the period, despite the warm spike of 2016.
13 of 13 Antarctic Peninsula/island stations cooling
Next we look at the Antarctic Peninsula, which global warming alarmists also like to have us believe is teetering on the brink of meltdown. Not long ago Japanese climate blogger Kirye posted a chart showing the mean annual temperatures of 13 stations located there – going back two decades.
For alarmists, the results turn out to be terribly inconvenient. The following map shows the location of the stations:

The following chart shows the plots of the mean annual temperature of the 13 stations, using NASA Version 4 unadjusted data:

13 of 13 Antarctic Peninsula and nearby island stations show cooling over the past 21 years. There hasn’t been any warming there so far this century. Data source: NASA GISS, Version 4 unadjusted. 
Natural ocean cycles
Buried near the end of the Guardian article is mention of the real reason behind Antarctic temperature trends:
Scientists on the Brazilian Antarctic programme say this appears to be influenced by shifts in ocean currents and El Niño events: “We have climatic changes in the atmosphere, which is closely related to changes in permafrost and the ocean. The whole thing is very interrelated.”
Indeed it is. Very likely in ways the climate alarmists prefer not to mention.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Kirye
and P. Gosselin
Yes, climate change is real.
But what they don’t tell us is that in many places that change has gone in the opposite direction of what alarmists like to have us think.
Moreover, that change is obviously driven far more by natural causes, such as solar and oceanic cycles, and has very little to do with man-made CO2.
Today we look at the untampered temperature datasets of the Japan Meteorological Agency (JMA) that go back to 1988 and which are mostly complete.
Here’s the plot of the 6 stations with adequate data:

Data: JMA
Five of the 6 stations show cooling or no upward trend. Earlier predictions of rapid warming are proving to be false.
Next is a plot of 8 stations using data from NASA, which show notable cooling trend over the past 25 years:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Deep blue Vermont says no to Big Wind
When it comes to wind energy, we’re seeing strong signals against it coming from Vermont, a state that is as politically blue, and thus pro-green, as any state could possibly be.
One would think that the Green Mountain State would welcome “clean”, renewable wind energy and happily make its contribution to rescuing the climate and environment. Vermont, after all, is home to Bernie Sanders and Bill McKibben.
Ironically, Vermont’s strong environmental streak is backfiring on industrial wind. Vermont citizens are realizing industrial wind parks are not green after all, and aren’t worth defacing the rural landscape. Vermonters now more than ever want them the hell out.
For example, Windpower Engineering Development site here reports how Vermont’s political environment now “is hostile to wind energy”.
“In 2012, there were over a dozen wind projects in development. Now there are none. This is truly a sad state of affairs for Vermont,” stated David Blittersdorf, CEO and founder of AllEarth Renewables.
Large wind facilities banned
Meanwhile voters in the tiny Vermont village of Grafton have endorsed a new town plan that prohibits industrial and commercial wind, reports the Brattleboro Reformer here. “On a 95-66 vote during an all-day ballot Monday, residents approved the new town plan, which bans any large wind facility, and includes other planning updates.”
Also neighboring Windham has said “no” to industrial wind parks by developer Iberdrola, which has since “dropped its plans to build what would have been the largest wind project in the state of Vermont,” writes the Brattleboro Reformer.
Good to see Vermonters are finally waking up to the green energy madness and landscape blight and zero benefit it all leads to.
Share this...FacebookTwitter "
"The world’s tropical oceans are suffering turbulent times. Dire predictions of yet more disastrous coral bleaching episodes have been released, placing the very future of wonders like the Great Barrier Reef in danger. Without global action to prevent runaway climate change, we as individuals are largely powerless to stop such loss. With a heavy heart we are now at a marine conservation crossroads. All paths look precarious at best. Do we continue to chase the possibility of developing a means to save and restore our coral reefs as the global climate becomes more and more inhospitable? Or do we follow a new path? One that takes stock of what can be saved, and develops viable solutions that maximise the potential of the oceans.  In Australia, the government has granted AUS$444m to the Great Barrier Reef foundation to find an innovative solution to the problems faced by its namesake coral reef. Proposals for reef shading, spreading of lime (to reduce seawater acidity), and “coral IVF” are all being investigated. In addition, major donor funding totalling US$86m has been given to the 50 Reefs programme aimed at saving some of the world’s most resilient coral reefs from climate decimation. But these aren’t solutions to the problem, they are merely sticking plasters. Despite lots of diverse work being carried out to save them it’s looking increasingly likely that many coral reefs are going to be lost  and we need to start addressing the problems that this will cause. The loss of the world’s coral reefs will result in hundreds of millions of people needing alternative supplies of food as these fisheries habitats rapidly decline in productivity. We are already seeing movement away from coral reef fisheries. In eastern Indonesia, research has found fishers are increasingly dependent on alternative habitats such as seagrass meadows and mangrove forests, instead of degraded and damaged coral reef habitats. By moving away from coral reefs, this highly intense fishing is being focused onto smaller areas. This decimates fish stocks even further, destroying the ecological balance of the tropical seascape. Much commendable conservation and research work is being done to make coral reefs more resilient to climate change. But their global functioning in support of fisheries is collapsing now, and we need current, real world solutions to this as well as blue sky innovation. In our newly published article we argue that there is an urgent need to take heed of the warnings of a future widespread decline in the productivity of coral reef fisheries, and broaden the focus of tropical marine conservation.  Burying our heads in the sand as fisheries move and their negative impact is concentrated elsewhere can no longer be an option for marine conservation. The world needs to look at how conservation efforts can maximise fisheries resources and sustainability throughout the tropical marine seascape. This is not about moving away from coral reef conservation, it’s about taking a much more holistic view of tropical marine conservation. We need to now think about how we can ensure people can continue to sustain themselves into the future. All too often tropical conservation ignores anything that isn’t coral. A rapidly changing climate means that such tunnel vision conservation is no longer viable. Looking to other components of this seascape, practicable conservation opportunities do exist to develop sustainable ways to respond to increased resource use, such as fishing gear management. The problems faced by seagrasses, for example, are immediately related to catchment management and coastal development rather than global climate change. These and other drivers are largely manageable, and threats can be reduced. Yet they are largely off the marine conservation radar. Although habitats like seagrass meadows are sensitive to a changing climate too, predicted scenarios suggest a much brighter future for them than for coral reef systems. We have global evidence of the value of seagrass ecosystems in supporting fisheries production and helping to sequester carbon dioxide from our atmosphere. Targeted action now could restore and protect them in to the future. Coral reefs rightly have had a lot of attention but we believe that now is the time for global conservation efforts to look beyond the reefs and focus on other vital areas of ocean conservation too."
"Hundreds of Amazon employees defied corporate policy to publicly criticize the company for failing to meet its “moral responsibility” in the climate crisis. More than 340 tech workers at Amazon used the hashtag #AMZNSpeakOut in public statements that condemn the company for not taking sufficient action on the climate crisis.  The action comes in direct opposition to an Amazon policy barring employees from speaking about the company’s business without prior approval from management. That policy was introduced after employees vowed to participate in the global climate strikes of September 2019. “Every person who shared a statement had to decide for themselves that whatever the consequences, they needed to stand up for what they felt was right,” Victoria Liang, a software development engineer at Amazon who joined the public action, said. “The climate crisis is just that urgent. We just couldn’t be silenced by these policies on issues of such moral weight.” Employees at Amazon have increasingly criticized the company in recent years for its contracts with large oil and gas firms. In spring 2019, more than 8,700 employees signed an open letter to the CEO, Jeff Bezos, urging him to take bolder action on climate change. The presidential candidates Bernie Sanders and Elizabeth Warren have also offered support of employees for speaking out. The employee activism is part of a broader trend in the tech industry of employee walkouts and protests against corporate policies. Google workers staged internal protests over sexual harassment policies in 2018 that continued into 2019 and gig workers at Instacart and Uber have organized strikes to fight for better pay and benefits. In June 2019, workers at the online furnishings retailer Wayfair walked off the job to oppose the company’s contracts with detention centers for immigrants. An Amazon spokeswoman said the company was aware of the employee actions. She added that the external communication policy had been updated in spring 2019 but was not directed at any one group of employees. Amazon has pledged to reach net-zero carbon by 2040 and 100% renewable energy by 2030. “While all employees are welcome to engage constructively with any of the many teams inside Amazon that work on sustainability and other topics, we do enforce our external communications policy and will not allow employees to publicly disparage or misrepresent the company or the hard work of their colleagues who are developing solutions to these hard problems,” she said. In January, at least three employees said they were threatened with termination for speaking publicly about environmental issues, stoking further protests against the new policy. “I’m proud to work at Amazon, but policies that silence employees who are challenging us to do better runs counter to our own leadership principles,” said Nolan Woodle, an associate contracts manager at Amazon. “When there is an issue of such importance, we have to be able to talk about it. Silencing employees is simply not the right approach.” Meanwhile, other companies appear to be responding to employee unrest. In January 2020, Microsoft announced it would be “carbon negative” by the end of the decade after a number of global actions orchestrated by employees. Amazon’s big tech counterpart Google also appears to be making public responses to the organizing. On the job platform LinkedIn on Monday, someone listing himself as a hiring manager at Google left a post that appeared to attempt to recruit defectors from Amazon to the company. “I recognize your courage and bravery to speak out about climate change,” he wrote, linking to a post on how Google is “working to battle climate change”. A Google spokeswoman said the company was not aware of the post and that it was made by a recruiter acting on his own initiative. The company did not respond to accusations that Google, too, has fired employees over organizing."
"Just because a country meets its emissions-reduction targets doesn’t mean it isn’t responsible for increased emissions elsewhere. This isn’t as weird as it sounds. The way national targets are calculated means some countries effectively “outsource” their emissions to other regions. If countries such as the UK don’t include the global emissions impact of their economy they run the risk of believing they are staving off climate change when they are not. Securing a “comprehensive, rules-based agreement” in Paris later this year is apparently one of the new UK government’s three priorities for energy and climate change – and this would no doubt include emissions targets. Such targets were also a key aspect of the previous coalition government’s intention to be the “greenest-ever”. It pledged to cut central government greenhouse gas emissions by 10% in the first year and to push Europe to increase the EU emission-reduction target to 30% by 2020. Greenhouse gas emissions are usually considered in terms of those emitted within national boundaries; for example by cars and industries. These “territorial emissions” are the basis of national commitments and governments can influence them through taxes or regulations on emissions.  In the UK, the general trend is towards a reduction in these territorial emissions, though it’s tough to say exactly what has happened over the past three years as there is a bit of a lag before annual statistics can be reported. However, it is instructive to take a wider perspective. Rather than considering the emissions that occur within the UK’s borders, we could think in terms of the emissions that are given off in the process of providing the goods and services enjoyed by its citizens.  This “consumption-based” perspective looks at the whole life cycle of products consumed in a nation and reveals that supporting British lifestyles actually causes the emission of far more greenhouse gases than those emitted in the UK alone. This is because the UK’s imported goods require, on average, more greenhouse gases to be emitted during their production than the goods it exports. While goods and services are imported, emissions are effectively “outsourced” to other nations. This matters because greenhouse gas emissions have a global impact – the UK will be affected just as much by one tonne of carbon emitted in Shanghai as Sheffield. The UK has developed and signed up to a number of strategies and targets to reduce emissions, most notably the Climate Change Act which legally obliges an 80% reduction in greenhouse gas emissions by 2050. However unless emissions are addressed on a global scale, we run the risk of frustrating the purpose of these strategies by shifting the burden from our shores to others. Looking at emissions based on consumption also gives us a clearer picture of which goods and services actually “drive” greenhouse gas pollution. It’s often not the processes directly responsible for the emissions. Steel, for example, requires lots of emissions to manufacture. But steel isn’t an end product – no one buys steel simply to own some metal. Rather demand for the alloy is driven by our demand for cars, white goods or buildings. This also applies to services; for example, the healthcare that we benefit from produces minimal emissions directly but relies on goods and services that result in far greater “upstream” emissions. We need to think in a more life cycle way in order to ensure we count impact effectively. This presents us with an opportunity. By reducing consumption of these goods and services and by making environmentally sound choices, we can have a far greater effect on global emissions. This can be seen clearly in the first chart above; between 2007 and 2009, the UK’s consumption-based emissions reduced far more dramatically than its territorial emissions have at any other time. This was, of course, due to the recession reducing imports of goods but these emissions remained at the lower level throughout 2010 and 2011. Reduced consumer confidence and deferrals in infrastructure investment will have played their part but the possibility of lower consumption-based emissions has been demonstrated and so there is the tantalising possibility of this lower emissions level persisting.  If so, this somewhat unintentional effect may turn out to be the most significant “green” achievement over this last government’s term. The challenge will be continuing this in the context of a stronger economy."
nan
"

I found the full page color advertisement on page 5E of the Sunday Enterprise Record quite interesting.
It lists a number of environmental reasons why the M&T Baldwin Gravel mine would be a good thing, not the least of which is the reduction of the number of truck miles traveled in Butte County due to trucking in building gravel from outside the county, and the reduction in gasoline burned and GHG’s avoided helping “Global Warming”.
And then there’s the angle that this mine pit will fill with water, and create an animal habitat just like the Teichert Ponds have done when it was used as a borrow pit to construct Highway 99 overpasses. There we have a clear example of how a lowly gravel mine got turned into a nature habitat, and there was no help or “kickstart” to nature as the M&T operators are proposing for their pits destined to be ponds.

It will be interesting to see how opponents argue against the project with these environmental assets it offers.
Here’s how Chico Creek Nature Center described the Teichert Ponds for a walking tour they sponsored of them:

April 8, Sunday – Teichert Pond/Birding By Ear – Trip co-leaders: Scott Huber and Dawn Garcia. Time: TBD. Chico’s Hidden Wetland – the view from Rte 99 is enticing; a large pond surrounded by tules and ringed with willows and oaks. Trip leader, Scott Huber, will direct you through the maze of streets that lead to the heart of Teichert Pond(s). Once in, you\’ll delight in the diversity of avian life found in this \’secret wetland\’ just blocks from downtown Chico. Co-leader Dawn Garcia, an expert at identifying local bird species by ear, will point out audio clues for ID\’ing species seen and perhaps some that are only heard! Expect at least three woodpecker species, a number of flycatchers, numerous sparrow species, a few raptors (possibly a Great Horned Owl), at least three warbler species, some ducks, geese and shorebirds and with any luck, some surprise migrants! Consider picking up one of the great “birding by ear” CD sets to prepare you for this trip: Bird Songs of California (Keller – Cornell Lab of Ornithology) or Western Birding by Ear (Peterson Field Guides). 
So what’s all the fuss about over this gravel mine?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea687dd57',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"Organizers in the youth climate movement plan an avalanche of activities beginning next week, determined to make the future of the climate the major issue of the 2020 election. Capitalizing on turnout in the September climate strikes, when 6 million people worldwide turned out to demand urgent action to address the escalating ecological emergency, young US organizers are making the leap from mobilization to demands. They’re planning widespread voter activation in the 2020 US presidential election as well as direct action targeting the fossil fuel industry and the banks and politicians that enable it.  “The headline message of the strikes in 2020 is: the kids are taking to the streets to strike for climate and they’re asking you to vote,” said Katie Eder, the executive director of Future Coalition, a communications and training hub for youth climate groups. There are more than 300 organizations involved in the efforts, Eder said, including some of the largest youth and adult coalitions in climate movement: Fridays for Future, the Sunrise Movement, 350.org, Zero Hour, Extinction Rebellion. “We’re starting to have the conversation within the strikes about how we’re balancing both the political targets and the financial targets,” Eder said, calling power and money “really key to the root causes of why we’re not addressing climate change”. Activists have planned widespread voter mobilization campaigns; three days of national strikes, marches, and direct action against politicians and banks beginning on Earth Day (22 April); regular strikes on Fridays and additional strikes targeting primary elections in every state; plus an international day of action, college divestment campaigns, and a huge push by the Sunrise Movement to turn out the vote for Bernie Sanders and a Green New Deal. Stories of climate awakening are a universal truth for young Americans living in the age of worsening climate conditions. Isabella Fallahi, who has asthma, for example, has lived within 14 miles of a coal-burning generating facility all of her life. “A lot of people don’t think of Indiana, or the midwest, for that matter, as frontline communities,” she said. “We face the silent killers. The terrible air quality because of mining projects, the terrible water quality because of mining projects, the health effects associated with both of those, droughts that cripple the agricultural economies that we’re based off of.” But Fallahi, like many other young organizers, is also galvanized by the tech proficiency of her generation, born into the era of hashtags and the ubiquitous touchscreen. She was a speaker at the UN climate change conference in Madrid (COP 25) in December 2019, when she and other youth were nearly ejected for protesting against the fossil fuel executives on panels and in negotiations. Approximately 200 young people in 40 countries, many of whom were at the conference, organized a response called Polluters Out. In 20 days, they had a website, a multilingual launch video, a press release in seven languages, and a list of demands. In actions set for March, they will call for the exclusion of fossil fuel interests at COP26 and transparency in the UN framework on climate change, as well as inclusion of indigenous and human rights in the Paris agreement. Regional efforts in the US will also call for a Green New Deal, a halt to new fossil fuel infrastructure, divestments from fossil fuels by universities and campaign funds, and the targeting of specific banks for fossil fuel investments. The groups stay in touch through conference calls, Slack, Zoom, and Google Drive. Activists in Cuba – who can’t download the apps because of the US trade embargo – tune in through WhatsApp. There’s a Slack channel for supporting scientists, too. And meetings wait for organizers in the Amazon who need to travel to towns for internet access. For all of them, there is community. People to cry with over the photo of the bay in Australia, so covered in smoke that it is hardly visible. “Overall, I think the way to look at this perspective on the youth movement is: it is cohesive in that they do communicate, but the orgs are not all the same,” said Natalie Mebane, associate director of US Policy for 350.org and an adult mentor for Zero Hour, both organizations intensely focused on voter turnout for climate for 2020. “We want to make sure that climate becomes the number one issue on voters’ minds.” The Sunrise Movement, like 350.org and Zero Hour, has taken that to a grassroots level. That is how Naina Agrawal-Hardin ended up in the guidance office at Washtenaw high school outside Ann Arbor, Michigan, for an interview. She’s a leader for the Ann Arbor hub of the Sunrise Movement. And she’s 16, a junior. Her school, she said, was supportive. “Whenever I have to take conference calls or interviews during school hours, they usually will write me a pass to get out of class,” she said. Agrawal-Hardin is planning a watch party at her home – a sneak peek at the forthcoming documentary Generation Green New Deal and a video about the Sunrise Movement’s 2020 campaign, as part of its annual kickoff. Sunrise provides the documentaries, discussion questions, and near-daily training on organizing and recruitment techniques. When Agrawal-Hardin hosts her event on 29 January, she’ll be one of more than 2,000 hosts nationwide. Through 2020, the Sunrise Movement will similarly activate its 300 chapters in support of Bernie Sanders, leveraging 10,000 volunteers in most of the 50 states for door knocks, voting pledges and climate education. Sunrise endorsed Sanders as the best-positioned candidate to lead a Green New Deal and address income inequality. “By primary/caucus day in Iowa and New Hampshire, we are going to have over 20,000 young people who have signed pledges to vote for Green New Deal champions” in the two states, said Sofie Karasek, a Sunrise Movement spokesperson. “What we’re really expecting to see in 2020 is Young Green New Deal voters really become the margin of victory for whoever becomes the Democratic nominee.”"
"The “thirst for oil” is often put forward as a near self-evident explanation behind military interventions in Libya, for instance, or Sudan. Oil, or the lack of oil, is also said to be behind the absence of intervention in Syria now and in Rwanda in 1994.   This of course clashes with the rhetoric around intervention, or its stated goal. No world leader stands before the UN and says they’re sending in the tanks because their country needs more oil. Such interventions are usually portrayed as serving directly non-economic goals such as preserving security, supporting democratic values, or more generally promoting human rights. But this is often met with scepticism and media claims that economic incentives played a key role. Was Iraq really “all about oil”? It’s worth asking whether this viewpoint has some mileage, or if it is instead purely conspiracy theory. It’s a question we’ve addressed in our research on the importance of oil production in attracting third party military interventions. In a new paper co-authored with Kristian Gleditsch in the Journal of Conflict Resolution we model the decision-making process of third-party countries in interfering in civil wars and examine their economic motives.  Our research builds on a near-exhaustive sample of 69 countries which had a civil war between 1945 and 1999. About two-thirds of civil wars during the period saw third party intervention either by another country or outside organisation. We found that the decision to interfere was dominated by the interveners’ need for oil – over and above historical, geographical or ethnic ties. Military intervention is expensive and risky. No country joins another country’s civil war without balancing the cost against their own strategic interests and what possible benefits there are.  We found countries producing lots of oil or those with higher reserves (and considerable market power) were more likely to attract military support. Most often this was to preserve oil prices on international markets. Indeed, there were on average more interventions in periods when there were only a few big oil producing countries and thus reduced competition (and more stable prices). Such interventions are more likely to be operated by countries highly dependent on oil imports. The US is the obvious example, but the USSR also fits this pattern – look at its intervention in oil-rich Indonesia in 1958, when Soviet oil production was still in its infancy. Consider also the UK’s military intervention in the Nigerian Civil War, also known as the Biafran War, between 1967 and 1970. At the time the UK was one of the largest net oil importers in the world, as North Sea oil production only started in 1975. The country also had, via BP, a direct interest in the stability of the region. It may seem tempting to attribute UK intervention in Nigeria to ties to its former colony. However, the UK did not intervene in civil wars in other, less oil-rich, former colonies such as Sierra Leone or Rhodesia (later Zimbabwe). On the flip side, oil independent nations don’t seem to do much intervening at all. The military aid Saudi Arabia provided to royalists during the civil war in 1960s Yemen is almost unique among the top exporting nations across the period we surveyed. The other Gulf states and regional oil powers such as Mexico or Indonesia have refrained from intervening in civil wars. The enduring record of geopolitical instability in oil producing regions and the likely increase in the global demand for oil means we’ll see more of these interventions in future. But there will be some differences. Shale gas should mean the US is becoming less energy dependent, whereas continued growth in China means the country will need energy imports more than ever. We’ll see some big changes in the specific states with the greatest incentives to intervene. We may see in coming years the first Chinese military assistance influenced by oil security. These interventions should in turn lead to stronger economic ties. Research we carried out with Leandro Elia, published in the Review of International Economics, found strong empirical evidence that US troop deployment and military aid provokes an expansion in bilateral trade flows.  Many claims are very often simplistic and are based on limited factual evidence, yet challenging them is best done by more rigorous and systematic analysis. Our work provides strong evidence that military interventions are indeed economically motivated."
"This year, more and more Australians are waking up to the need for change. Specifically, change on how we manage land and water to prepare for the fire season.  However, this change is not so simple. The logistics of such change are considerable, but they are not the biggest challenge. Instead, the biggest obstacles to creating change come in the form of those who profit from maintaining the system as it is, despite that system bringing little benefit to the country – either the people or the environment. The benefit this system brings is purely measured in terms of profit for the few and, in the eyes of the profiteers at least, far outweighs any and all other concerns. Waking up to a realisation that your political leaders may not actually have the best interests of the citizenry or the lands and waters we rely on at heart is a bitter pill for many to swallow. It can be hard to accept that their decisions are influenced more by lobby groups than by common sense or the greater good. Most Indigenous peoples are all too familiar with this reality, both in terms of promoting better management of land and water, and in dealing with the reality that governments do not have our best interests at heart. Some people have struggled to see a connection between Australia Day and the current concerns over Australia’s refusal to embrace climate change. The last Guardian article I wrote was met with countless people failing to understand how I could draw a connection to the colonial experiment that is Australia and the current situations that we find ourselves in. For me, however, they are intrinsically linked. For what single date better signifies all that is wrong with Australia than a country that blindly chooses to celebrate the worst of its own history? What date better paints a picture of a nation desperately clinging to an outdated colonial identity rather than embracing all that is special about the continent they now reside on and doing everything in its power to protect it? And indeed, what could be a better date to protest Australia’s inaction on climate change, inaction on Indigenous rights, inaction on putting the rights of the people in front of corporate interests? Despite conservative commentators wilfully pretending that opposition to Australia Day is solely about a date in the calendar, for at least the last 82 years since the first official Day of Mourning was held in Sydney, 26 January has provided a platform for Indigenous peoples to call for change. Be it in the form of self-determination, treaty, stopping Aboriginal deaths in custody, changing child removal practices, or any other number of issues. Australia Day protests, whether they are operating under the name of Invasion Day, Survival Day or Day of Mourning, have always been about highlighting injustice and rallying support to worthy causes. It is not simply a “White Australian v Indigenous peoples” issue, firstly because there’s lots of people living here who are neither, but also because there are millions of non-Indigenous people who support these calls for change and, to be fair to the other side, at least four Indigenous people who apparently want everything to stay how it is – probably more but conservative media only ever seem to bring out the same few for some reason. Change the Date has always been synonymous with Change the Nation, but as social media has brought this 80+ year-old debate into the mainstream, something seemed to have gotten lost in translation over the past few years, which is why many of us have abandoned the #Changethedate hashtag in favour of #Changethenation – to keep the conversation on track with what it has always been about. All that has changed this year with the fire season is that it has brought home, for many, the realities of a possible apocalyptic near-future for Australia if we do not change. Understanding how toxic patriotism has become in Australia is essential to this challenge. Patriotism that should be about a love of the land and people has become instead about a justification for bigotry and racism, about instilling hatred in the perceived “other”, and about providing a comfortable smokescreen for government looking after its own interests at the expense of the rest of us. That is why 26 January, the ultimate day for toxic patriotism, has always been the prefect day to call for change, to rally for support, to highlight problems and offer solutions, and to come together and grow the movement of those who want to change the nation for the better, because the only other option is to continue to blindly support the same course of action, the same way of thinking, that got us into this mess in the first place. Indigenous peoples are not the problem, but we may well be Australia’s best hope for a solution. • Luke Pearson is the founder of IndigenousX"
"

_The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
Could President Obama have picked a worse time to announce his Climate Action Plan?   
  
Global warming has been stuck in neutral for more than a decade and a half, scientists are increasingly suggesting that future climate change projections are overblown, and now, arguably the greatest threat from global warming—a large and rapid sea level rise (SLR)—has been shown overly lurid (SOL; what did you think I meant?).   
  
You hardly need an “action plan” when there is so little “action” worth responding to.   
  
As I frequently discuss the lack of warming and the decreases in the estimates of future climate change, I’ll focus here on new scientific findings concerning the potential for future sea level rise, interspersing a little travelogue.   
  
Projections of a large sea-level rise this century depend on rapid ice loss from Greenland and/or Antarctica. Yes, as ocean waters warm, they expand, but this expansion-induced rise is pretty well constrained and limited to being about 6 inches plus or minus a couple of inches by century’s end. And the contribution from melting glaciers/ice in other parts of the world (not counting Greenland and Antarctica) is even smaller, maybe 2-4 inches. So that adds up to about 8-12 inches of sea level rise by the year 2100—not much different than that which has already occurred over the past century. This is hardly catastrophic.   




So getting a good handle on the contributions from Antarctica and Greenland is essential if you want to develop a reasonable expectation for the future. Lacking a good handle leads to unreasonable projections.   
  
Here is an example of the latter.   
  
A breathless passage from the book version of Al Gore’s _An Inconvenient Truth_ :   




I flew over Greenland in 2005 and saw for myself the pools of meltwater covering large expanses on top of the ice. …These pools have always been known to occur, but the difference now is that there are many more of them covering a far larger area of ice. …In Greenland, as in the Antarctic Peninsula, this meltwater is now believed to keep sinking all the way down to the bottom, cutting deep crevasses and vertical tunnels that scientists call “moulins.”   
  
When water reaches the bottom of the ice, it lubricates the surface of the bedrock and destabilizes the ice mass, raising fears that the ice mass will slide more quickly towards the ocean.   
  
…If Greenland melted or broke up and slipped into the sea—or if half of Greenland and half of Antarctica melted or broke up and slipped into the sea, sea levels worldwide would increase by between 18 and 20 feet.   
  
Tony Blair’s advisor, David King, is among the scientists who have been warning about potential consequences of large changes in these ice shelves. At a 2004 conference in Berlin, he said: THE MAPS OF THE WORLD WILL HAVE TO BE REDRAWN. [all caps in original]



Gore went on to include page after page of now and then maps of the world’s major cities after a sea level rise of 20 feet (of course, assuming no adaptive measures put in place).   
  
But Gore’s disaster mechanism has been shown to be impotent and ineffective. In fact, a collection of recent papers published in the peer-reviewed scientific literature basically dispels all myths foretelling a large sea level rise this century coming from ice loss on Greenland. Recent research on Antarctica largely does the same.   
  
First off, research by Sarah Shannon and 18 co-authors takes direct aim at Gore’s mechanism in their paper “Enhanced basal lubrication and the contribution of the Greenland ice sheet to future sea-level rise.” Here is what they conclude, in direct opposition to Gore’s claims:   




Although changes in lubrication generate widespread effects on the flow and form of the ice sheet, they do not affect substantial net mass loss; increase in the ice sheet’s contribution to sea-level rise from basal lubrication is projected by all models to be no more than 5 percent of the contribution from surface mass budget forcing alone.



And “no more than 5 percent” turns out to be, by the year 2100, somewhere between 0 and 3 millimeters, or in English units, a tenth of an inch or less. Some disaster. Certainly “18 to 20 feet” is a lot scarier, but it is just plain wrong.   
  
Another new study looks at (among other things) the sea-level rise effect of the acceleration of the discharge rate of those glaciers across Greenland, which directly empty out into the sea. Heiko Goelzer and fellow researchers found that after an initial bump in the contribution to sea level rise as these glaciers retreat, once they draw back to the grounding line—the point where the outlet glaciers stop floating and instead rest on the bedrock—the loss rate slows dramatically. They conclude that the contribution from dynamical changes to the flow rate of outlet glaciers may contribute between 8 to 18 millimeters of sea level rise by the year 2100. That is about a quarter to three-quarters of an inch. Again, not even close to a disaster.   
  
Here’s your climate news scoop of the day: The highest discharge-volume glacier in the entire Northern Hemisphere—Greenland’s Jakobshavn—has grounded, which is really going to put the kibosh on the Greenlandic myth. Here’s a picture I took from my own Greenland sojourn* earlier this summer. It shows the southern end of Jakobshavn glacier, on June 24.   






  
  
_Looking south along the calving front of the Jakobshavn glacier, June 24, 2013. Photo by Patrick Michaels._   
  
You can see that it is grounded over most of its humongous 10-kilometer face. The calved ice drops off in smaller chunks, dramatically reducing the size of the bergs that will eventually float down the spectacular Ilulissat Icefjord.   
  
A small portion of the glacier was perhaps still floating when I was there, right near the north end, as indicated by a reduction in the height of the calving face, as shown in this photo.   






  
  
_Looking north along the calving front of the Jakobshavn glacier, June 24, 2013. Photo by Patrick Michaels._   
  
As a tidewater glacier, Jakobshavn regularly calves some tremendous icebergs that take a couple of years to make their way down the 35-mile fjord, only to ground on the terminal moraine near Ilulissat (and conveniently located in view of the Hotel Arctic’s live webcam, here). Because the glacier has largely grounded, these bergs are not the giants that they once were (although some sizeable icebergs continue to be produced in the early summer as the floating ice tongues established in the winter break up). _Hie thee to Ilulissat! The sooner the better!!_ Presumably some views through the webcam (which was near my room) will convince you!   
  
(The terminal moraine near Ilulissat dates to the end of the Little Ice Age—meaning that the productive fishery at the mouth of the fjord was probably inaccessible. Farther south, such an expansion of ice no doubt covered much of the Viking pastureland, chasing them to places elsewhere (including North America?)).   
  
A third new study examined the direct contribution of changes in the surface mass balance (SMB) of Greenland (that is, total run off from ice melting minus total gains from enhanced snowfall) to future sea level rise (they did not consider ice loss from glacier speed). In their study “Estimating the Greenland ice sheet surface mass balance contribution to future sea level rise using the regional atmospheric climate model MAR,” Xavier Fettweis and colleagues found that declines in the SMB by the year 2100 led to somewhere between 2 centimeters and 13 centimeters of sea level rise, depending of the carbon dioxide emissions scenario used in their model. That’s somewhere between 1 and 5 inches (and these projections are based on climate models which, according to the latest science, overestimate future warming by some 70 percent).   
  
So adding all of these effects up—basal lubrication, glacial dynamics, and enhanced melting—the total global sea level rise by the end of the 21st century originating from Greenland projected by the latest, greatest scientific studies averages out to be maybe 3 to 4 inches. Ho hum.   
  
Like I said, sea level rise disaster scenarios that are dreamed up by Greenland shedding large volumes of ice ( _a la_ Al Gore, Jim Hansen, etc.) are SOL.   
  
  
  
**References:**   
  
Fettweis, X., et al., 2013. Estimating the Greenland ice sheet surface mass balance contribution to future sea level rise using the regional atmospheric climate model MAR. _The Cryosphere_ , **7** , 469-489.   
  
Goelzer, H., et al., 2013. Sensitivity of Greenland ice sheet projections to model formulations, Journal of Glaciology, 59, 733-749, doi:10.3189/2013JoG12J182   
  
Shannon, S., et al., 2013. Enhanced basal lubrication and the contribution of the Greenland ice sheet to future sea-level rise. _Proceedings of the National Academy of Sciences_ , doi:10.1073/pnas.1212647110   
  
  
  
*Get that ticket to Greenland pronto! Travel hint: the shortest route is through Reykjavik on Iceland Air and then on Air Iceland to Ilullisat. Reserve in advance and you can get a Saga Class (business) seat for pretty cheap compared to the Majors (which will take you all the way to Copenhagen and then backtracking on Air Greenland’s A330 to Kangerlussaq (Sondre Stromfjord) and an additional connection to Ilulissat, i.e. $$$$).


"
"

An airliner traveling from Chile to New Zealand early today was in for an near miss from something you wouldn’t expect.

Flaming space debris — the remains of a Russian satellite — came hurtling
back to Earth not far from a passenger jet on its way to Auckland, New Zealand.
Here’s further proof for the growing concern of the  increasing amounts of space junk orbiting our planet. From the article: ‘The pilot of a Lan Chile Airbus A340 … notified air traffic controllers at Auckland Oceanic Center after seeing flaming space junk hurtling across the sky just five nautical miles in front of and behind his plane…’
Yikes!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea747bc4f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

 _Global Science Report is a weekly feature from the Center for the Study of Science, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Whenever the topic of rising seas comes up, we point out that Antarctica is expected to gain mass through enhanced snowfall in a warmer climate, and therefore its contribution to global sea level rise should be negative—that is, the water locked up in the added snowfall there will act to reduce the level of the globe’s seas. The models used by the Intergovernmental Panel on Climate Change (IPCC) in their 2007 Fourth Assessment Report project the sea level reduction from this mechanism by the end of the 21st century to amount to somewhere between 2 cm and 14 cm (roughly 1 to 6 inches). While this is not a lot, the main point is that Antarctica is not expected to be a contributor to rising seas as the climate warms. Without a large contribution from Antarctica, we will not approach alarmist projections of a meter-plus of sea level rise by century’s end.   
  
Up to now, though, Antarctica has not exactly been with the program.   
  
Instead of gaining mass through increased snowfall, there have been indications that Antarctica is losing ice (contributing to sea level rise) as ice discharge from its coastal glaciers exceeds gains from snow increases (which have been hard to find). One has to wonder whether Antarctica, contrary to expectations, will continue to lose mass and become an important contributor sea level rise, or whether the projected increases in snowfall have just not yet reached a magnitude sufficient to offset the loss from glacial discharge.   
  
Things are starting to change down there.   




The research that has gotten the most attention on the subject of Antarctic mass balance has been based on observations made by the Gravity Recovery And Climate Experiment (GRACE) satellite. This orbiter senses changes in gravity (i.e., mass) which can be caused by increasing snow and ice loads over the continent. One key piece of information which must be factored into the calculations of ice mass change is the change in the underlying geologic formations, which are still rebounding from enormous amounts of ice lost after the end of the last ice age. This geologic motion, known as the glacial isostatic adjustment (GIA), is largely modeled rather than directly observed. Our level of knowledge (or lack thereof) of the true GIA adds a sizable amount of uncertainty to GRACE-based estimates of the ice mass changes over time in Antarctica (and Greenland, the northern hemisphere’s cheap imitation of Antarctica).   
  
In a widely cited finding, Velicogna (2009) reported that Antarctica was losing ice at a rate of about 104 gigatons per year (Gt/yr) during the period 2002–2006, increasing to a loss rate of 246 Gt/yr during 2006–2009 (about 374 Gt of ice are equivalent to 1 mm of sea level). Rignot et al. (2011) also found an acceleration of ice loss there, increasing from a loss of about 209 Gt/yr (in 2003-2007) to about 265 Gt/yr from 2007 to 2010. However, Wu et al. (2010) argued that the GIA model used in these previous studies is incorrect, and that when a more accurate GIA model is incorporated in the GRACE-based ice mass change calculations, Antarctica was only losing about 87 Gt/yr during the period 2002–2008.   




Support for the GRACE-based calculations comes from the general agreement between the GRACE numbers and those calculated from studies of changes in the grounding lines of coastal glaciers and the ice flow across those grounding lines in association with the other aspects of the mass balance. This method is known as the Input-minus-Output Method (IOM). The IOM estimates of the average ice loss from Antarctica over the past several decades (1992–2007) lie somewhere around 136 Gt/yr, in rough agreement with the GRACE-based estimates. However, the IOM is also subject to a lot of uncertainty. An attempt by Zwally and Giovinetto (2011) to reduce the uncertainty and increase the accuracy resulted in an IOM-based estimate of a loss of only 13 Gt/yr over the same 18-yr period and led the researchers to conclude that: 



Although recent reports of large and increasing rates of mass loss with time from GRACE-based studies cite agreement with IOM results, our evaluation does not support that conclusion.



It seems that as the calculations and derivations are improved, the amount of ice mass that Antarctica is supposedly losing gets less and less.   
  
Or perhaps it isn’t losing any mass.   
  
Using a set of observations from a series of satellites that have been in orbit since 1992 and that measure changes in the height of the surface of the ice (ICESat), NASA’s Jay Zwally and colleagues (2012) report that Antarctica is gaining mass. Zwally recently presented his findings to a workshop of the Ice-Sheet Mass Balance and Sea Level expert group of the Scientific Committee on Antarctic Research and the International Arctic Science Committee. According to his abstract, Zwally reported that “During 2003 to 2008, the mass gain of the Antarctic ice sheet from snow accumulation exceeded the mass loss from ice discharge by 49 Gt/yr (2.5% of input), as derived from ICESat laser measurements of elevation change.”   
  
Zwally further added, ""A slow increase in snowfall with climate warming, consistent with model predictions, may be offsetting increased dynamic losses.""   
  
So the ""global warming, leading to increased snowfall, leading to a drawdown of global sea level"" mechanism may be operating after all.   
  
A paper to soon appear in _Geophysical Research Letters_ give us another enticing look at recent snowfall changes in Antarctica. In “Snowfall driven mass change on the East Antarctic ice sheet,” Carmen Boening and colleagues from NASA’s Jet Propulsion Laboratory report that extreme precipitation (snowfall) events in recent years (beginning in 2009) have led to a dramatic gain in the ice mass in the coastal portions of East Antarctica amounting to about 350 Gt in total (Figure 1).   






Figure 1. Timeseries of snow accumulation in coastal East Antarctica (shaded region in inset).   
(Source: Boening et al., 2012)Boening et al. reported that the increase in ice mass in East Antarctica has not completely offset the loss of ice mass during the same time in West Antarctica, but as this comparison is made using GRACE data, it is hard to know just how accurate it is.   
  
Also note that a few years with a lot of snowfall does not mean that a change in the long-term snowfall rate has occurred. Nevertheless, the situation bears careful watching.   
  
Putting everything together, we conclude that many of the claims that Antarctica is rapidly losing ice and increasingly contributing to a rise in global sea levels must now be, at the very least, tempered, if not overturned entirely. Time will certainly tell. And time will also tell just how much we need to worry about future sea level rise. Currently, the answer seems to be “not overly much.”   

"
"
An odd twist has developed in the past week regarding some data sets that surfacestations.org volunteers have been using to look at individual stations. The data has changed on NASA’s GISS website with no notice whatsoever.
My first indication that something changed came from surfacestations.org volunteer Chris Dunn who wrote to me complaining that one of the sites he’d recently surveyed, Walhalla, SC had been greatly adjusted at GISS for no good reason that he could ascertain, since the site is pristine by climate monitoring standards, and has not gone through any significant changes in the past, and has been operated at the same location (by the same family) since 1916. He wondered why NASA would have to adjust the data for a “good” station. The way I view it, shouldn’t good data stand on it’s own? That was September 7th. He was using data from NASA GISS published on 8/28.
So he continued to look at the data, and the site. The on Sept 11th he noticed a change when he downloaded the data again. Something had changed, the data was different. Not only the adjusted data but the “raw” data too.
Steve McIntyre of Climate Audit has a complete review at: http://www.climateaudit.org/?p=2077 where he traces data back to Detroit Lakes, MN the station that started this all. See my original post on this: http://www.norcalblogs.com/watts/2007/08/1998_no_longer_the_hottest_yea.html
This set other people into motion looking at the NASA GISS data sets. The conclusion? NASA published new raw and adjusted data on their website with no formal or informal notice. I don’t know what to make of this, by I think perhaps this could be a breach of the Data Quality Act. At the least, it flies in the face of accepted scientific courtesy, where if you publish data sets being used by researchers worldwide, scientific courtesy would dictate that you at least place notice of such a change, otherwise there can be a domino effect for hundreds of research projects that use the data. Which would cause researchers to wonder why things don’t look the same anymore and begin searching for answers. Well that is exactly what happened here. We had a citizen trying to figure out why a climate site with good data was “adjusted”, and then the data changed right in the middle of him looking at it.
Whether this was accidental or intentional I cannot say, but it certainly does not look good coming on the heels of NASA GISS’s most recent issue of a mistake causing a revision of our temperature history on August 8th. We deserve better accounting than this when so much hinges on this data.
Let’s give NASA and Hansen the benefit of the doubt and see what they have to say about it.
UPDATE: NASA has posted today, their explanation which you can read here: http://data.giss.nasa.gov/gistemp/ Note that this notice appears a full week after the data changed (about 9/10) and only after there was discussion of the issue on blogs such as Climate Audit over the weekend. Why would NASA GISS not announce the change at the same time the data did, particularly when the announcment of the change ammounted to one small paragraph?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea3fb385b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The good news is that the coming years hold out real promise for a new wave of market‐​oriented regulatory reforms here in the United States. Momentum on this front crested in the late 1970s and early ’80s and has been all but defunct for the past couple of decades, but that may well be about to change. The bad news is why: the reason we should be getting our political hopes up is that our economic hopes over the near to medium term are so likely to be dashed.



Let me spell out that bad news a bit. U.S. economic performance in the wake of the Great Recession of 2008-09 has been nothing short of dismal. More than three years passed before real output even returned to its prerecession level, and a return to the prerecession growth trend remains nowhere in sight. The story with employment is even worse: yes, the unemployment rate has gradually subsided, but only because the percentage of Americans actually in the work force has sunk to its lowest level since the late 1970s.



Debate still rages over how much of the economy’s continuing sluggishness reflects short‐​term, cyclical factors — in particular, a shortfall in aggregate demand or deleveraging in response to the financial crisis of 2008. It is becoming increasingly clear, however, that slower growth in output and a weak labor market are now the “new normal.” In other words, the ongoing economic slump is not just a matter of a temporary gap between current and “potential” output. Rather, the economy has suffered a decline in its potential or full‐​employment growth rate.



Consider the long‐​term trends for each of the four major components of economic growth: growth in labor participation, growth in labor skills, growth in investment, and growth in output‐​enhancing innovation. As I explained in my 2013 Cato paper “Why Growth Is Getting Harder,” those trends are now uniformly unfavorable. Average hours worked per capita have fallen since 2000. Growth in so‐​called labor quality (as measured by years of school completed) has slowed considerably. The net domestic investment rate has been trending downward for decades. And total factor productivity (TFP) growth, our best measure of innovation, has slumped again in recent years after an Internet‐​fueled surge between 1996 and 2004.



Consequently, there are strong reasons for believing that growth in the years ahead will fall well short of the long‐​term historical average. Between 1870 and 2010, growth in real (i.e., inflation‐​adjusted) GDP per capita averaged just under 2 percent a year. By contrast, recent long‐​term growth projections by top academic and government economists point to an average annual per capita growth rate in the range of 1.0 to 1.5 percent — a fairly dramatic decline from the historical trend line.



In a welcome bit of irony, such economic pessimism offers solid grounds for political optimism. Here’s the basic logic: there is an inverse relationship between the external conditions for growth, on the one hand, and the incentives for good economic policymaking on the other. When conditions for robust growth are favorable, politicians can indulge in the characteristic vices of their profession — a time horizon bounded by the next election cycle and an overriding focus on dividing the pie rather than making it bigger — and still preside over a thriving economy. When, however, times get tougher, politicians must up their game or else economic performance will suffer. In the latter event, the poll numbers of incumbents start to drop, those of their challengers start to rise, and thus opportunities for policy change improve.



Now, policy change could well proceed in the wrong direction and produce worse results than the status quo. But over the past several decades at least, the general pattern around the world — as documented in the annual _Economic Freedom of the World_ reports and similar sources — has been for economic policies to move toward less government control and greater reliance on market competition. And usually, progress in liberalization has been spurred by disappointing economic performance.



If this general pattern holds in the present case, then a protracted period of sluggish growth should open a window of opportunity for pro‐​market, pro‐​growth reforms here in the United States. In sunnier times, many bad policies widely understood to create obstacles for growth are left undisturbed because the political price to be paid for changing them doesn’t seem worth it: why borrow trouble by attacking policies with powerful defenders if things are going OK anyway? But when the economic climate worsens, politicians come under increasing pressure to do something.



 **OPPORTUNITY FOR REFORM**  
Here then is the challenge for supporters of free markets: how can we best take advantage of this window of opportunity? During good times, we are forced to argue that, even if the overall economy seems to be performing well, it could be doing even better with appropriate policy reforms. Now, by contrast, we can reframe our case with a much more compelling sense of urgency: if we do not make difficult but necessary reforms, the economy will perform much worse than in the past. Since people are typically loss averse (they are more concerned about losing what they have than not getting what they want), the growth slowdown makes our case much stronger and more persuasive. But once we reframe our argument, what policy agenda best fits the new frame?



There is no shortage of possible reforms to include. For evidence I refer you to _Reviving Economic Growth_, a new Cato ebook that I edited. The book is a collection of essays by 51 prominent economists and policy experts, all of whom were asked to offer suggestions for improving the U.S. economy’s long‐​term growth outlook. Although a number of reform ideas come up repeatedly, what is striking about the collection is just how wide‐​ranging and varied the proposals are. Which should not be at all surprising: fiscal and regulatory policies affect the allocation of resources and the climate for innovation along countless different margins, and thus the potential levers for improving overall economic performance are similarly numerous and diverse.



In particular, the major economic policy debates that have dominated Washington and the nation’s attention in recent years — the trajectory and composition of federal spending, the level and structure of taxation, health care policy, regulation of the financial sector, what to do about illegal immigration, and climate change and environmental regulation more generally — have important implications for growth. Yet precisely because these debates have already found the spotlight, they are unlikely to supply policy ideas that can take advantage of the political opportunity that the current growth slowdown affords. On all of these hard‐​fought fronts, battle lines are already clearly drawn and often reflect differences over goals and priorities other than growth. Moreover, opinions are highly polarized along partisan and ideological lines, which in the current political environment is often a recipe for stalemate and gridlock.



What we should be looking for, then, are policy ideas that are not already the subject of high‐​profile, politically polarized debate. America’s growth slowdown is a new problem, and policy responses that address that problem are more likely to gain traction if they are not recycled ideas originally put forward to address other problems. And if a policy idea is already clearly associated with either the left or the right, in today’s highly contentious environment it is all but guaranteed that the other side will fight tooth and nail against it — which makes progress of any kind difficult in the absence of large congressional majorities and unified partisan control of the White House and Congress.



Meanwhile, of course, the items on this new policy agenda need to be effective remedies for slow growth. Since innovation is the ultimate source of long‐​term growth in advanced countries at the technological frontier, we should focus especially on policy reforms that can facilitate the introduction and spread of new ideas. In other words, we should target policy barriers that inhibit entrepreneurship and the reallocation of resources through competition and “creative destruction.”



 **REFORMING REGRESSIVE REGULATION**  
In a new Cato White Paper with the curious title of “Low‐​Hanging Fruit Guarded by Dragons,” I take all these factors into account and propose a pro‐​growth reform agenda that focuses on regulatory policies whose primary effect is to inflate the incomes and wealth of the rich, the powerful, and the well established by shielding them from market competition. To apply a convenient label, let’s call these policies “regressive regulation” — regulatory barriers to entry and competition that work to redistribute income and wealth up the socioeconomic scale.



In the paper I identify four main areas of regressive regulation: excessive monopoly privileges granted under copyright and patent law, restrictions on high‐​skilled immigration, protection of incumbent service providers under occupational licensing, and artificial scarcity created by land‐​use regulation. Space constraints prohibit an in‐​depth discussion of those policies here; for details I refer you to the paper. Here I will simply offer some general observations about why targeting these policies seems to me to be the most promising strategy for reversing the growth slowdown — and for taking advantage of the growth slowdown to revive political momentum for economic freedom.



At first blush, these four policy areas seem completely unrelated. They cover highly disparate subject matters, they are administered at different levels of government, and they feature widely varying forms of regulatory apparatus. Notwithstanding all these obvious differences, there are also deep and important similarities. All the policy areas feature regulations that erect explicit barriers to entry — whether in the economist’s sense of barriers to market entry, or in the literal sense of barriers to geographic entry. Copyright and patent laws and occupational licensing limit who can engage in particular kinds of commercial activity; immigration laws and zoning regulations limit who can enter or do business within a designated geographic area.



All of these entry barriers undermine economic growth by restricting vital inputs to innovation. Excessive copyright and patent protections restrict the recombination of ideas that is the essence of innovation by making some ideas artificially inaccessible. Immigration laws restrict the inflow of highly skilled individuals who are disproportionately entrepreneurial and innovative. Occupational licensing restricts the formation of new businesses, which are frequently the vessels for new products or new production methods. And zoning restricts urban density, a vital catalyst for the innovative recombination of ideas.



Finally, all these policy domains have similar distributional consequences: all of them redistribute income and wealth to the well‐​off and privileged. Copyright and patent laws pinch consumers for the benefit of huge corporations. Immigration laws expose America’s lowest‐​skilled workers to intensifying competition from foreign‐​born workers while shielding high‐​skilled workers from equivalent competitive pressures. Occupational licensing boosts the earnings of protected incumbents by restricting supply, especially in higher‐​income professions. And zoning gives windfall gains to wealthy landowners.



 **LEFT-LIBERTARIAN SYNTHESIS**  
In all likelihood because of these underlying similarities, none of these policy areas have become zones of ideological or partisan conflict. To be sure, proper policy is vigorously debated in all these areas, but the contending sides are not divided along left‐​right or Republican‐ Democratic lines. In striking contrast to the polarization and gridlock that now dominate most national policy debates, opposition to regressive regulatory controls has brought together politicians and policy experts from across the political spectrum. Thus, in the field of intellectual property, Nancy Pelosi (D-CA) joined forces with Darrell Issa (R-CA) and Ron Paul (R-TX) to oppose the Stop Online Piracy Act, a failed legislative effort to toughen criminal penalties for copyright violations. Among policy experts, leading critics of copyright and patent law excesses include progressives Lawrence Lessig and Dean Baker and libertarians Tom Bell and Jerry Brito.



With regard to high‐​skilled immigration, a number of bipartisan reform bills have been introduced in recent years. To take a recent example, in January 2015 a group of six senators, including Orrin Hatch (R-UT), Mark Warner (D-VA), and Marco Rubio (R-FL), introduced the Immigration Innovation Act to boost the numbers of both temporary and permanent visas for highly skilled workers. And among policy experts, scholars from the libertarian Cato Institute and the progressive Center for American Progress supported the most recent comprehensive immigration legislation passed by the Senate in 2013. As to occupational licensing, the Obama administration’s latest budget contains a provision to nudge states toward reform; furthermore, this past summer the administration released an excellent report that makes the case for deregulation in this area.



Meanwhile, in July 2014, Rep. Paul Ryan (RWI) released a widely discussed plan for combating poverty. And in the section on regulatory reform, Ryan singled out occupational licensing laws as prime examples of the “regressive regulations” that too often constrict economic opportunity for the least advantaged. Among policy experts, Alan Krueger of Princeton University, who served as chairman of the Council of Economic Advisers under President Obama, is a leading critic of these regulatory restrictions, while the libertarian Institute for Justice has a long track record of challenging and overturning licensing rules in court.



Zoning is a local issue that has long been thought to have only local consequences, so to date it has not attracted much attention from Washington policymakers. Among policy experts, though, the pattern of support for reform across ideological dividing lines holds here as well. Edward Glaeser, who in addition to teaching at Harvard is affiliated with the libertarian‐​leaning Manhattan Institute, is among the nation’s leading critics of current land‐​use regulation. Another prominent critic is Matthew Yglesias of Vox, who wrote his book, _The Rent Is Too Damn High_ , while he was working for the Center for American Progress.



 **PUBLIC INTEREST VERSUS VESTED INTERESTS**  
It’s not simply the case that one can find policy experts on both sides of the ideological spectrum who support reform of these regressive regulatory policies. More than that, it’s very difficult to find disinterested policy experts anywhere on the spectrum who support the status quo. Certainly, there are strong defenders of both intellectual property protection and zoning, but even in their ranks you will find recognition that current policies are seriously flawed. Thus, the economist Carl Shapiro, a prominent supporter of patents generally, has written, “[While] there is no doubt that the patent system taken as a whole plays an important role in spurring innovation, the general consensus is that the U.S. patent system is out of balance and can be substantially improved.” In similar fashion, the economist William Fischel, who has written sophisticated defenses of zoning, acknowledges that its exclusionary impact has increased since 1970 and that the “social and economic costs” of contemporary land use regulation are “not trivial.” As far as high‐​skilled immigration restrictions and occupational licensing are concerned, it is difficult to find any scholar who has anything nice to say about the current state of either.



This combination of qualities — negative impact on entrepreneurship and innovation, absence of political polarization, and an intellectual consensus in favor of reform — makes regressive regulation an especially inviting target for any campaign to enact pro‐​growth policy reforms. For all who are interested in better long‐​term U.S. economic performance, this is the “low‐​hanging fruit.” Reforming these policies is something that we know will make a positive difference, and by “we” I mean the vast bulk of disinterested experts. Yes, it is true that plucking this fruit won’t be easy, because the interest groups that benefit from the status quo are politically powerful, well organized, and highly motivated. This is the “guarded by dragons” part of the story. But knowing clearly what needs to be done, however difficult it might be, is an advantage that should not be underestimated.



Pursuing an agenda of curbing regressive regulation would allow us to open a new front in the economic policy debate. Unlike the all too‐ familiar policy disputes now ongoing, a campaign against regressive regulation would feature issues new to the national policy spotlight — especially in the case of occupational licensing and zoning, because they occur at the state and local levels and thus are typically ignored by Washington. Meanwhile, the organizing rubric of regressive regulation packages together disparate issues in a novel way and can thereby impart new energy to reform efforts in each of its constituent policy domains. This new front would look very different from the other, ongoing policy debates. Instead of the opposing forces being arrayed along the left‐​right axis, here the contest pits an expert consensus across the political spectrum against the interest groups who profit from existing policy. Instead of yet another left‐​right fight, this time the contest could be framed as a choice between the public interest and vested interests.



The idea of a left‐​right coalition to push deregulation may sound far‐​fetched, but it is not without precedent. Consider the country’s last major episode of pro‐​market regulatory reform in the late 1970s and early 1980s. During that brief period, price‐​and‐​entry regulation of airlines, trucking, and railroads was systematically dismantled; price controls on oil and natural gas were lifted; interest‐​rate caps for checking and savings accounts were removed; and the AT&T monopoly was ended, paving the way for competition in long‐​distance telephony. Those too young to remember can be forgiven for associating all of this with Ronald Reagan, but in fact Democrats and progressives played a major role. Jimmy Carter signed the legislation that deregulated airlines, trucking, railroads, and natural gas. On Capitol Hill, Edward Kennedy led the fight for airline deregulation, ably assisted by his aide Stephen Breyer. Yes, the rise of Chicago‐​school economics and especially the law‐​and‐​economics movement supplied momentum for these sweeping policy changes, but so did the activism of Ralph Nader.



History never repeats itself, but sometimes it rhymes. As in the 1970s, the U.S. economy today is delivering disappointing results. Back then the problem was “stagflation”; today we worry about a “great stagnation.” And once again, the shifting currents of political debate are bringing together unlikely allies with a common interest in reviving prosperity and a common hostility to the entrenched interests that stand in the way. With luck, contemporary reformers can follow their predecessors’ good example.
"
"Evan Flint has his feet up, at last. It is day four of the final Test between England and South Africa and, as chief groundsman, all he can do is watch as Rassie van der Dussen and Dean Elgar grind out the beginnings of a doomed rearguard action. Flint has been at the Wanderers since last spring, lured north after 10 and a half years at Newlands, where he won groundsman of the year during his last two seasons. He had had a lot to cope with. In January 2018 Cape Town officials announced that, after three years of insufficient rain, the city was three months away from Day Zero (running out of water). The visiting India team were told not to shower for more than 90 seconds, the India and South Africa sides gave a combined donation of 100,000 rand to the Gift of the Givers Foundation (disaster relief in Africa), fights broke out over water from the nearby Newlands borehole and club and schools cricket was cancelled halfway through the season. Three weeks before Day Zero, Australia and South Africa played a Test in Cape Town – subsequently overshadowed somewhat by the discovery of sandpaper down Cameron Bancroft’s trousers. Newlands is well served by boreholes but Flint remembers it as a difficult time: “We were in the thick of it but it was an opportunity to show the rest of the world that we were doing our bit, almost like a badge of honour.” He had to water the pitch; it has a very high clay content so otherwise would have cracked, but the outfield was something else. “At one point we were watering it maybe twice a week at most and it was very brown. It looked terrible but the last thing we wanted as Day Zero approached was for people to put on the telly and look at a lush cricket field.” “You know what people are like, they announce water restrictions and nobody believes it, so it was a personal decision to adhere to what the authorities said. We used borehole water but we acknowledged there was a drought on and we were playing our part.” “It made me realise that the grass doesn’t need as much water as we think. It taught me that you are doing the turf a bit of a disservice to spoon-feed it every day. I learned that it is when you water that’s important, rather than how much.” Day Zero was eventually averted but sport in South Africa is under strain in a volatile changing climate. Temperatures in the interior of the country are rising at twice the global level according to the International Panel on Climate Change and southern Africa as a whole is facing unprecedented strain. The UN World Food Programme has warned of a hunger crisis on “a scale we’ve not seen before”, blaming, largely: “The cumulative effects of climate-related natural disasters in the form of recurrent widespread droughts. The region has had only one normal rainy season in the past five years amid cyclones and persistent flooding.” Malawi has also had to contend with an influx of fall armyworms, while further north plagues of locusts currently swarm over Kenya. In South Africa the Eastern Cape is still under severe strain, with many of the new boreholes not producing the desired results because ground water levels are so low, a situation the government has called critical. Level 2 water restrictions were announced in October, even in Johannesburg, but the big difference for Flint as a groundsman is the summer rainfall in Jo’burg – in Cape Town the rain falls, or should fall, in the winter. Nor does he miss the south-easterly wind that hits Cape Town from the spring: “They don’t put that in the brochures.” The Wanderers also has the benefit of wonderful ground irrigation, every drop of rain that falls on the stadium roof or gutters runs into a reservoir at the back of the ground to be stored away. The changing climate – declining rainfall and temperature changes – has made Flint question some of groundskeeping’s most cherished ideals. “Cape Town was an eye-opener,” he says. “We have great water harvesting at the Wanderers because we are an international ground but certainly a lot more could be done in council and club facilities and there should be more research into artificial surfaces because they don’t require the maintenance, the cutting and the fuel, etc. “Of course many municipalities have got greater things to worry about but more education would help. I say this with this beautiful turf in front of me, and professional sportsmen want to play on grass, but artificial surfaces do make sense. I’m being really sacrilegious now but think about golf courses. The enormous amounts of water those things guzzle is extraordinary.” Despite all this, the climate crisis is not something that Flint finds crops up in conversation much, either in cricket or groundskeeping circles. “It’s been a bit disappointing. We meet up once a year as groundsmen and we discussed the Cape Town drought but, apart from that, not really. Once it starts raining, people go back to their own ways. But if you don’t have any water, my goodness, you can’t live.” • This is an extract from the Guardian’s weekly cricket email. The Spin. To subscribe, just visit this page and follow the instructions.   "
nan
"Speak to someone working in conservation, almost anywhere in the world, and there is a good chance that Gerald Durrell played some role in inspiring them to their future career.  The naturalist and zookeeper, who died 20 years ago, was a great advocate for endangered species. Durrell was a regular on British TV for decades and a prolific author, with books translated into 31 languages. As a nine-year-old I was transported from my bedroom to the azure seas around Corfu, to watch the magical aquatic world from the gently bobbing “bootle bumtrinket” (the unforgettably named, home-made boat of Gerry’s childhood). Like so many others, my first impressions of what a rainforest is like – its smells and sounds – came from Gerald Durrell’s beguiling descriptions of his later trips throughout the tropics. Durrell’s accounts of his animal collecting expeditions in far flung countries can make the modern reader feel very slightly uncomfortable (in a way they might not be able to quite put a finger on). They were clearly written in, and for, a very different age.  However, in many ways Gerald Durrell was far ahead of his time. He believed that a zoo shouldn’t be simply a place for people to go and see unusual animals, but they should also play a real role in conservation and education. The success of his early books enabled him to found a zoo in Jersey, where he made breeding animals on the edge of extinction a major focus.  Durrell recognised that successful conservation of threatened species in the wild depended on the skills and dedication of people working in the countries where they were found. Back in 1984, he set up his international training centre (now the Durrell Conservation Academy). This brought government officials, those working for small, underfunded charities, and anyone else with a real influence over the future of the critically threatened species Durrell cared about, together in Jersey to exchange skills and learn new ones.  The real power of this approach was in creating a network of like-minded people around the world and ensuring conservationists on the ground had access to up to date methods and approaches. The academy has now trained 3350 conservationists from 135 countries. I have the privilege of working closely with the Durrell Wildlife Conservation Trust team in Madagascar: a dynamic crew of 45 conservationists delivering community-focused projects targeting threatened species across the island.  Durrell himself loved Madagascar, an island he described as “filled with magic”. The Trust has been active in the country now for nearly 30 years and their hard work has been very influential – though the threats to Madagascar’s dwindling natural habitats, so eloquently described in one of Durrell’s last books The Aye-Aye and I, remain pressing. Many have said that Durrell was a man who left the world a better place than he entered it. The trust which bears his name, and continues with his work and vision, have this week made an important step in trying to quantify the impact of that work. They have launched the Durrell Index, which tackles the difficult challenge of measuring and communicating their impact on biodiversity around the world.  Gerald Durrell’s biggest impact however is likely to be the hardest to measure. It is the millions of children inspired to care just a little more about wildlife, and it is the conservation professionals trained and supported by the Durrell Academy, working hard to protect animals and their habitats all around the world."
"Zero-carbon hydrogen has been injected into a UK gas network for the first time in a groundbreaking trial that could help to reduce carbon dioxide emissions. The 20% hydrogen and natural gas blend is being used to heat 100 homes and 30 faculty buildings at Keele University in Staffordshire. Unlike natural gas, when hydrogen is burned it produces heat and water as opposed to carbon dioxide.  “Heat hasn’t been particularly decarbonised to date and it’s a very big challenge,” said Lorna Millington, the future networks manager at Cadent, the gas distribution network that led the £7m HyDeploy project. “The aim was to turn the theoretical evidence into something real and tangible that the consumers within the Keele network are now getting to experience every day.” Heating homes and businesses accounts for half of the UK’s energy consumption and a third of its carbon dioxide emissions. Rolling the 20% hydrogen blend out across the country could save about 6m tonnes of carbon dioxide emissions a year, the equivalent of taking 2.5m cars off the road. The hydrogen is captured using an electrolyser, which runs electricity through water to split it back into hydrogen and oxygen. This can then be injected into existing modern gas networks, with no need for customers to change appliances or pipework. The Health and Safety Executive granted the project exemption in 2018 from the current 0.1% limit on hydrogen in the UK gas network after an extensive examination of evidence to ensure it would be safe. Keele University was identified as an ideal location, because it owns and operates a private gas network that can be isolated from the main network. Two years of preparation involved gas safety checks on all buildings in the trial, laboratory tests on gas appliances and research on the effect of hydrogen on materials found in the gas network. All appliances sold after 1996 must be able to sustain 23% hydrogen under current regulations. “The materials we use in our network are actually more than capable of dealing with the levels of hydrogen that we were looking at,” said Millington. The trial will run until July and if successful a pilot will be rolled out in the north-east to deliver the 20% hydrogen blend to 670 domestic and commercial properties in Winlaton, Gateshead. Mark Horsley, the chief executive officer at Northern Gas Networks, said: “We’re really excited about the opportunity. There is a strong groundswell for decarbonisation, and I think opportunities for each individual home to contribute to that is quite powerful.” The pilot, the first of its kind in the UK, would start in December and run for 10 months. Customers have already been notified about the upcoming change. “Early indications are that it’s going to be very positively received,” said Horsley. The project is part of global exploration into the potentials of a “hydrogen economy”, in which the gas could also be used to generate electricity and produce heat with benefits for transport, heavy industry and domestic energy use. The energy needed to produce hydrogen from water can be provided by solar, hydro or wind power to produce “green hydrogen”, but there are challenges in doing so at scale. It still may be some time before the technology can be rolled out nationally, but “the 20% is a massive step forward for us as an industry and for the UK in achieving that net zero goal”, Horsley said."
"
Share this...FacebookTwitterAt his German news commentary site, Gabor Steingart reports on the results of the latest ARD German public broadcasting trend analyses. Here it’s clear that German citizens are speaking loud and clear on the topic of climate protection: not so fast!
Rush to Green Deal without support
Lately the media and politicians have been pushing hard to start a fundamental change of society to a low-carb, de-industrialized organic garden society. For example, EU Commission President Ursula von der Leyen just announced a 1 TRILLION euro plan to decarbonate the continent.
Yet, public support for such an ambitious and adventurous transformation project appears to be rather soft at best in Germany.
According to Steingart:
Only 27 percent of Germans regard climate protection as a political priority. 73 percent have other concerns. Outside of the green voters (currently 23 percent in surveys), the alarmism of politicians (“climate emergency”) and scientists (“ecocide”) meets with discontent. The people want to say to the grand coalition government: Don’t throw the baby out with the bath.”
Greenland sets new record low
On another note, according to the Austrian www.wetter.at site, Greenland recorded a new all time record low temperature. The site reports:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




While 2019 is the warmest year in Australia since records began, as the Office of Meteorology stated in its annual climate report, Greenland has set a new record for cold temperatures: minus 66 degrees C.”
Mother Nature tricks Europe
Meanwhile, parts of central Europe have been experiencing record warm January temperatures, and thus reinforce the collective public’s impression that the global climate is out of whack. It’s not.
How unfortunate that Mother Nature would play such a mean trick on Europe. Everywhere else it’s pretty much winter as usual:

Image: Ventusky, via Pure Climate Skeptic. 
Now European policymakers are going to have an easier time getting the public to accept the EU “Green Deal”. Boy, are they in for a cold awakening.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn a new paper, atmospheric physicist Dr. Richard Lindzen summarizes the “implausible” claims today’s proponents of dangerous anthropogenic global warming espouse.
Dr. Richard Lindzen retired several years ago, and yet his immense contribution to the atmospheric sciences lives on. His research is still cited about 600 times per year.
Lindzen recently published another scientific paper (Lindzen, 2020) in The European Physical Journal criticizing the current alarmism in climate science.  Here are a few of the highlights.
1. Doubling the atmospheric CO2 concentration from 280 ppm to 560 ppm results in just a 1-2% perturbation to the Earth’s 240 W/m² energy budget. This doubled-CO2 effect has less than 1/5th of the impact that the net cloud effect has. And yet we are asked to accept the “implausible” claim that change in one variable, CO2, is predominatly responsible for altering global temperatures.
2. A causal role for CO2 “cannot be claimed” for the glacial-to-interglacial warming events because CO2 variations follow rather than lead the temperature changes in paleoclimate records and the 100 ppm total increase over thousands of years produce “about 1 W/m²” of total radiative impact.
3. Climate science didn’t used to be alarmist prior to the late 1980s. Scientists were instead sufficiently skeptical about claims of climatically-induced planetary doom. That changed during the years 1988-1994, when climate research centered on CO2 and global warming received a 15-fold increase in funding in the US alone. Suddenly there was a great financial incentive to propel alarming global warming scenarios.
4. Concepts like “polar amplification” are “imaginary”.
“The change in equator-to-pole temperature difference was attributed to some imaginary ‘polar amplification,’ whereby the equator-pole temperature automatically followed the mean temperature. Although the analogy is hardly exact, this is not so different from assuming that flow in a pipe depends on the mean pressure rather than the pressure gradient.”

Image Source: Lindzen, 2020
Share this...FacebookTwitter "
"When the Spanish conquered South America in the 16th century they took over the Incas’ mines and soon began to pump clouds of lead dust over the Andes. The silver the conquistadors sent back home made them wealthy. It also made them the world’s first industrial-scale toxic metal air polluters – perhaps causing us to rethink the timing of the moment when humans truly began to change the environment. Formal recognition of the Anthropocene epoch, the “Age of Humans”, will acknowledge the occurrence of an unprecedented impact of human activities on Earth. As scientists, we’ve begun using the term informally, especially in regard to anthropogenic (“human-caused”) climate change. Officially, though, we all live in the Holocene, the epoch named by geologists to mark the end of the last ice age. To officially say that we live in the Anthropocene – that is, declare the Holocene over and the Anthropocene already underway – we would have to draw an unequivocal line between the two. We’d have to agree on a point in time when human impacts on the environment became large enough to warrant an official change in scientific nomenclature. Some would assign it to the start of agriculture 11,000 years ago, while others tie it to the advent of the nuclear era in 1945, but most recognise the Anthropocene as beginning with the industrial revolution (1780s-1830s). However we now have evidence, from an ice core of the Quelccaya Ice Cap in Peru, of anthropogenic pollution of the South American atmosphere that precedes the industrial revolution by around 240 years. The discovery by my colleagues and I, published in the Proceedings of the National Academy of Sciences, underscores the difficulty in defining the onset of the Anthropocene. While we have plenty of information from around the world about pollution during the industrial period, pre-industrial pollution records are very rare. We have to look to special places on Earth where atmospheric chemicals would have been preserved chronologically, such as lake sediments or the accumulated snow on an ice cap. Quelccaya is one of those places. The largest ice sheet in the tropics is a fast-melting poster child for global warming. It’s also a perfect place to learn more about the past climate and environment – the ice core we drilled there in 2003 contained more than 1,200 years of accumulated atmospheric chemistry.  South America has a rich history of mining and metallurgy. We wondered, would the ice record evidence of ancient metallurgical activity? Air pollution would have to have existed on a truly continental scale to drift on the air from the heart of South American metallurgy in Bolivia across the Andes and onto Quelccaya, some 800 km away. It did. The story of South American metallurgy – from the rise of the Inca Empire to the Spanish conquest and even the industrial stagnation that followed the end of Spanish rule – is written in the ice. Like the native peoples before them, the Inca gathered metal ore from outcrops or exposed veins and smelted it in primitive wind-driven furnaces called huayra. The Quelccaya core first records evidence of pollution from Inca metallurgy around 1480 in the form of trace amounts of bismuth, likely released into the atmosphere during the creation of bismuth bronze, an alloy which has been recovered from the Inca citadel at Machu Picchu. Remarkably, no increases of other trace elements are apparent in the Quelccaya ice record during that period, indicating that the well-known metallurgic activities performed during the Inca reign had a negligible impact on the South American atmosphere. The Spanish conquistadors lead by Francisco Pizarro defeated the Incas in 1532, starting the colonial period of South America. Silver smelting quickly became the most important industrial activity on the continent, and the Spanish used imported and inefficient Castilian stone furnaces as well as thousands of local huayras as silver extraction spread across Bolivia and Peru. Increases in lead levels in the Quelccaya ice core date to approximately 1540 and document this initial phase of Spanish metallurgy. In 1572, the Spanish introduced a new technique called amalgamation, which allowed them to process even low-quality ores that contained much more lead than silver. This cold technique involved grinding the ore into powder, which could easily have become airborne. We believe this accounts for the sudden and dramatic spike in lead concentrations in the ice core starting around that time. Even the independence war of 1833, which marked the end of Spanish rule, is recorded in the ice. Elsewhere in the world, the industrial revolution was booming – and air pollution growing. But at Quelccaya, lead levels fell and remained low for years after the war, likely due to army destruction of mines in Bolivia and Peru and the post-war lack of infrastructure. The ice provides a detailed record of more than 1,000 years of South American history that can inform discussions of the Anthropocene timeline.  Did it spread out through South America with the trace bits of pollution from the Incas’ bismuth bronze? Or the lead concentrations from increased smelting upon the Spanish arrival? Or perhaps the more dramatic pollution created in the era of amalgamation marks the turning point. This discovery suggests that our new epoch emerged sporadically through space and time, at different points during human history. Only as we connect the Quelccaya ice core to records elsewhere on Earth can we assemble a clearer picture of the dawn of the Anthropocene."
"
Share this...FacebookTwitterGlobal warming should mean that the period of May 11-15 – known as the Ice Saints in Europe – when late spring frosts often occur, would become less frosty over the years. But the opposite has been the case since 1995. 
===============================================
Why have the Ice Saints gotten colder over the past 25 years?
By Die kalte Sonne
Authored by Josef Kowatsch
(Translated and edited by P. Gosselin)
In Germany the five days from  the 11th to 15th May are the so-called Eisheilige (Ice Saints). Farmers used to understand “ice” simply as late spring frost, so these are days with frost.
Our question is how have the Ice Saints behaved at various locations across Germany the last 25 years?
POTSDAM
Let’s start with Potsdam, the capital of Brandenburg. We take the last 25 years as the period under consideration:

Figure 1: The five Ice Saints days in the state capital Potsdam. They are getting much colder. 2020 was the low point of the last 25 May months. On three days there were night frosts.
BAD KREUZNACH
At Bad Kreuznach in the Upper Rhine Valley we show a southern Germany station from from Palatinate, a warm sunny region. The DWD Germany National Weather Service weather station is located north, outside the town.

Figure 2: Bad Kreuznach in the Upper Rhine, the trend line of the present day is even slightly lower for the Ice Saints than in Potsdam.
DRESDEN


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The DWD weather station is located in the Klotzsche suburb, at the airport north of the Saxon state capital.

Figure 3: DWD station Dresden Klotzsche. Ice Saints in the present. The trendline is a little bit lower than Potsdam and the Ice Saints 2020 were among the coldest since 1997.
GOLDBACH
Goldbach near Bischofswerda in eastern Saxony, a small suburb with about 500 inhabitants.

Figure 4: The Ice Saints 2019 were clearly the coldest, source of data: Station manager Dietmar Pscheidt.
SCHNEIFELFORSTHAUS
This DWD weather station is located in the Eifel region, near the Belgian border.

Figure 5: Even in the far west of Germany, the Ice Saints outside the cities have become significantly colder. The average of the five calendar Ice Saints days in 2020 was 4.77°C – the second lowest.
NUREMBERG
The DWD Station Netzstall is located near Nuremberg, well outside the city. The missing value of the year 2000 was interpolated using the neighboring stations at Nuremberg and Nuremberg-Roth.

Figure 6: The village near Nuremberg shows a falling trend line of the five Ice Saints days. 2020 was slightly colder than 2019 and this year was clearly the coldest in the last 25 years.

Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterRahmstorf way off: New study finds no robust relationship between shrinking sea ice, European cold waves
By Die Kalte Sonne
(German text translated/edited by P Gosselin)

Photo: Potsdam Institute for Climate Impact Research
A good six years ago, Potsdam climate researcher Stefan Rahmstorf was outraged by the German Weather Service (DWD) at his at Klimalounge site.
The DWD had the audacity to contradict Mr. Rahmstorf. Specifically, it was about the presumed connection between the expansion of Arctic sea ice and cold winter weather. Rahmstorf’s simple model: Less Arctic sea ice causes cold European winters. At the time, he led a conglomeration of studies and claimed:
In my view, the above studies provide clear evidence of a link between Arctic ice loss due to global warming, and more frequent winter high pressures, particularly over the Atlantic-European part of the Arctic, and the associated influx of cold air into Europe. As we have often experienced it in recent winters – for example spectacularly in the first half of February 2012.”
In the process Rahmstorf became verbally wild and didn’t hold back dishing it out: The DWD was embarrassing, incompetent in questions of climate change, that it could not even read scientific papers, the arguments were flat. It was an unusually aggressive style of discussion that is seldom encountered in science. Rahmstorf original:
However, the taz quoted [German paper] yesterday the spokesman of the German Weather Service [DWD in German] as saying that if there was a direct relationship with the sea ice cover,  the entire winter would have to be very cold in Germany.  I think this trivial argument with which he would like to wipe from the table the climate research results shown above  is pretty embarrassing for the DWD.  Of course open water in the Arctic does not prevent stochastic weather variability. There will always be warm and cold periods. In all these studies it comes down to changing probabilities in the prevailing weather patterns: Petoukhov and Semenov estimate that the probability of cold winter extremes could triple, that is even in the Abstract. One wonders whether the DWD representative has read the relevant studies at all – and if not, why he feels the urge to comment on them in the media. Unfortunately, it has a certain tradition that meteorologists dealing with weather, are not familiar with climate science.”
More than half a decade has passed by since Rahmstorf’s rumblings. In the meantime, research has taken up the topic professionally and now has certainty: Rahmstorf was completely off the mark. Sea ice does not play a major role in the cold waves. Press release of the University of Exeter from August 12, 2019:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Arctic sea-ice loss has “minimal influence” on severe cold winter weather, research shows
The dramatic loss of Arctic sea ice through climate change has only a ‘minimal influence’ on severe cold winter weather across Asia and North America, new research has shown.
The possible connection between Arctic sea-ice loss and extreme cold weather – such as the deep freezes that can grip the USA in the winter months – has long been studied by scientists. Observations show that when the regional sea-ice cover is reduced, swathes of Asia and North America often experience unusually cold and hazardous winter conditions. However, previous climate modelling studies have suggested that reduced sea ice cannot fully explain the cold winters.
Now, a new study by experts from the University of Exeter, the Royal Netherlands Meteorological Institute and the Energy and Sustainability Research Institute in Groningen, has shed new light on the link between sea-ice loss and cold winters. For the research, the international team combined observations over the past 40 years with results from sophisticated climate modelling experiments. They found that the observations and models agreed that reduced regional sea ice and cold winters often coincide which each other.
They found that the correlation between reduced sea ice and extreme winters across the mid-latitude occurs because both are simultaneously driven by the same, large-scale atmospheric circulation patterns. Crucially, it shows that reduced sea ice only has a minimal influence on whether a harsh and severe winter will occur. The study is published in leading science journal, Nature Climate Change.
Dr Russell Blackport, a Mathematics Research Fellow at the University of Exeter and lead author of the paper said: ‘The correlation between reduced sea ice and cold winters does not mean one is causing the other. We show that the real cause is changes in atmospheric circulation which moves warm air into the Arctic and cold air into the mid-latitudes.’ Over recent decades, the Arctic region has experienced warming temperatures through climate change, which has led to a large decline in sea-ice cover. This reduction in sea-ice cover means that areas of open water increase, which in turn allows the ocean to lose more heat to the atmosphere in winter – this can potentially alter the weather and climate, even well outside the Arctic.
Recent studies have suggested that the reduced sea ice or Arctic warming has contributed to recent cold winters experienced in the mid-latitude region – and that as the sea-ice reduces further through climate change, cold winters will become more frequent and severe. Now, this new study suggests that reduced sea ice is not the main cause of the cold winters. Instead, the cold winters are likely caused by random fluctuations in the atmospheric circulation.
Professor James Screen, an Associate Professor in Climate Science at the University of Exeter said: ‘The are many reasons to be concerned about the dramatic loss of Arctic sea ice, but an increased risk of severe winters in North America and Asia is not one of them.’ Dr John Fyfe, a Research Scientist at the Canadian Centre for Climate Modelling and Analysis, who was not involved in the research, writes in Nature Climate Change: ‘Blackport and colleagues put to rest the notion that Arctic sea-ice loss caused the cold mid-latitude winters, showing instead that atmospheric circulation changes preceded, and then simultaneously drove sea-ice loss and mid-latitude cooling.’
Minimal influence of reduced Arctic sea ice on coincident cold winters in mid-latitudes by Russell Blackport, James Screen, Karin van der Wiel and Richard Bintanja is published in Nature Climate Change. It was funded through a grant by the Natural Environment Research Council.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterToday, 30% of the globe’s CO2 emissions come from China. In 10 years, China’s emissions alone will match the rest of world’s emissions combined. China continues to build hundreds of coal plants today. So why are the rest of us spending $600 billion every year on CO2 emissions mitigation?
China overtook the United States as the world’s largest CO2 emitter in 2008 (Liu et al., 2019).

Image Source: Liu et al., 2019
It only took 7 years for China’s emissions percentage to double that of the USA’s. As of 2015, China accounted for 30% of global emissions (Shan et al., 2018) compared to the USA’s 15%.
Much of the reason for China’s emissions domination is because its citizens consume more than 50% of the world’s coal.
China is in the process of building 100s of new coal plants, with plans to add a new coal plant every 2 weeks for the next 12 years.
According to the People’s Daily, China, the country’s longest coal transporting railway, carrying 200 million tonnes of coal from north to east China every year, is now (October, 2019) in operation.

Menghua Railway, China’s LONGEST coal transporting railway line, is expected to be put in operation in Oct. The 1,837-km railway will carry 200 million tonnes of coal annually from N China's Inner Mongolia to E China's Jiangxi. pic.twitter.com/sFXpCjplaN
— People's Daily, China (@PDChina) July 23, 2019



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Due to its exponentially-growing energy demands, China will be responsible for 50% of the globe’s CO2 emissions within 10 years (Liu et al., 2019).
Why should the rest of us spend $89 trillion to reduce CO2 emissions?
According to proponents of CO2 mitigation policies, the cost of infrastructure changes required to reduce CO2 emissions to acceptable levels is $89 trillion by 2030.

Image Source: WorldBank.org
Per a scolding, we’re-not-spending-enough-on-climate article published in the journal Nature, we’re already spending about $600 billion annually on CO2 mitigation.
“[T]otal climate-related financing was $510 billion to $530 billion in 2017,” which is much higher than the $360 billion spent in 2012. “The UN’s Framework Convention on Climate Change (UNFCCC), put it at $681 billion in 2016” (Yeo, 2019).
So we’re spending 100s of billions to 10s of trillions to reduce CO2 emissions in Western countries.
Meanwhile, China continues to build hundreds of new coal plants and grow its carbon-intensive infrastructure, thwarting any and all efforts to reduce net global emissions.
Why are we doing this?
Share this...FacebookTwitter "
"

Fans of Cato@Liberty may have noticed two new features from the Center for the Study of Science. These are a weekly _Global Science Report_ and a monthly _Current Wisdom_.   
  
  
While the _Wisdom_ has been a monthly feature that can be found under my publications, _World Science Report_ is new and is modelled after my original blog, _Global Climate Report_ , which is the Web’s longest running climate change blog. Our first release was September 11, 1995. The enormous archive at http://​www​.world​cli​matere​port​.com is cross‐​referenced by subject and date, and can provide valuable information on virtually any climate question. We also reserved the right to write in a humorous fashion.   
  
  
As the Center adds new affiliates, you will see much more in the new _World Science Report_ than mere climate.
"
"The UK must recruit more than 100,000 people to fill green energy roles within a decade if the government hopes to meet its binding climate targets, National Grid has warned. A report by the company found that Britain needs to fill 120,000 roles in the green energy industry by 2030 to help develop projects that can cut greenhouse gas emissions to near zero. That number is likely to reach 400,000 by 2050, when the government expects to have developed a clean energy system based on renewable electricity, green heating systems and electric vehicles. The growing need for new recruits to power the UK’s climate targets is expected to emerge as Britain faces a green energy jobs crunch over the next 10 years. The report warned that a fifth of employees in the energy sector are due to retire by 2030. The UK’s energy industry faces stiff competition from other sectors and has a narrow pipeline of young people pursuing Stem (science, technology, engineering and mathematics) qualifications to draw from, it said. Nicola Shaw, the executive director of National Grid, said: “The time is now for the sector to rise to the challenge and overcome the longstanding issues we face in recruiting a diverse workforce with the right skills to deliver on the UK’s ambitions.” The UK’s plan to cut emissions to virtually zero, and offset unavoidable pollution through carbon capture schemes, will require major investments in offshore wind, clean heating schemes, electric vehicles and carbon-capture technology. The energy industry is expected to use its role in tackling the global climate crisis to encourage young graduates into the industry. Research carried out by YouGov has found that people of all ages, from all regions across the UK, are “looking for a job with environmental purpose”. More than eight in 10 women and seven in 10 men have said they are keen to play their part in tackling climate change. Over half of adults are specifically looking to work for an organisation that is helping the UK to achieve its climate goals. The rising need for green energy jobs could bring opportunities for skilled tradespeople, engineers and other specialists “across every region of the country”, the report said. A quarter of the green jobs required will need to be based in the north of the country, according to National Grid. It estimated that more than 21,000 new recruits will be needed to complete energy projects, including an offshore wind farm off the coast of Blyth and the new subsea power cable to Norway from the north-east of England. Meanwhile, the development of carbon capture and storage in the Yorkshire and Humber region is expected to support 17,000 new jobs. Another 28,000 roles will be needed to work on more offshore wind farms off the east of England. In Scotland, green energy workers will be needed to fill more than 48,000 jobs by 2050, with a further 25,000 roles expected in Wales. Kwasi Kwarteng, the minister of state for business, energy and clean growth, said: “Tackling climate change is not only saving the planet, but is significantly boosting our economy. As we work to reduce our emissions to net zero by 2050, the UK has the potential to support 2m green-collar jobs across our world-class renewables sector, among other industries.”"
"
Share this...FacebookTwitterMichel Gay at the French language Contrepoints here writes that “EU Commission President Ursula von der Leyen’s green madness is the best way to torpedo Europe”.

EU Commission President Ursula von der Leyen is all smiles as she proposes 1 trillion euros for “Green New Deal”. Photo: Bundesregierung.
1000 billion euros
The new EU President of the European Commission said she wanted to present a plan called The Green Deal that would cost 1000 billion euros over the next decade in order reduce the European Union’s (EU) greenhouse gas emissions by 55% by 2030 compared to 1990.
According to Ursula Von der Leyen, “The European Green Deal is Europe’s new growth strategy. It will reduce emissions while creating jobs and improving our quality of life. To do that, we need investments…. To do that, we will come up with a plan.”
Von der Leyen also announced a European climate law slated March 2020 which would make the transition to carbon neutrality… “irreversible”.
She added: “If some people talk about costs, we should always keep in mind the extra costs if we do not act now.”
Up to 300 billion euros per year
Frans Timmermans, Executive Vice-President of this Commission in charge of the Green Deal, told a press conference in Brussels that these investments would be “a combination of public money, loans and private money” estimated to run as high and 300 billion euros per year.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Public services will be hit hard
But Contrepoints writes that no matter wehere the money comes from, it will have a “siphoning effect” and thus “will be missing elsewhere where it might be more useful (health, public services, national security…).”
The EU’s new leadership is calling on Europe to be climate-neutral by 2050.
Taxpayers will pick up the tab
As to who will pay, Contrepoints says “in the end taxpayers and consumers will be largely involved, as usual.” Then, everyone will benevolently ask themselves “Why are local services disappearing? Why are taxes going up? And why is my standard of living falling?”
Collusion capitalism
Contrepoints calls the new EU Green Deal “collusion capitalism”, which describes an economy where business success depends on close relations with representatives of power: governments, and various commissions.”
“The aim is to create a climate of favoritism, for example, when it comes to the allocation of permits, government subsidies or tax cuts. It appears when cronyism, including ideological cronyism, pervades politics and elected officials,” comments Contrepoints. “It leads to collusion between elected officials and market players, in particular to win public contracts, obtain subsidies and guide legislation. Some of these systems are formalized and dominate an entire economy, but they are generally more subtle.”
Contrepoints writes that with 1000 billion euros to distribute, it will be necessary “to have to have friends at the European Commission to prosper in the shadow of all this public money.”
And due to the high costs and burdens, Contrepoints warns that Europe, with Germany now leading the charge, “wants to accelerate into a dead end!”
Share this...FacebookTwitter "
nan
