"

As we celebrate our 40th year, I’d like to recap some of Cato’s accomplishments and tell you where we’re heading.



Cato has been a vigorous proponent of entitlement restraints, the right to bear arms, marriage equality, fundamental tax reform, downsizing government, property rights, drug legalization, school choice, free trade, immigration liberalization, criminal justice reform, and term limits. We have energetically resisted corporate welfare, campaign finance restrictions, government constraints on the Internet, global warming excesses, overarching executive power, infringements on civil liberties, the administrative state, imperial wars, dubious foreign entanglements, and unnecessary government secrecy.



Cato was the first to address Social Security’s financial problems and offer a private‐​investment alternative. José Piñera, co‐​chairman of Cato’s Project on Social Security Choice, was the architect of privatization in Chile. We’re still fighting for entitlement reform here, where the fiscal implications for Social Security, Medicare, and Medicaid are dismal.



On the health care front, Cato’s efforts yielded Health Savings Accounts — a significant step toward free market health care. And we led the battle against Obamacare. The 2015 Supreme Court challenge was crafted by Cato’s director of health policy studies, Michael Cannon, who demonstrated Obamacare’s flawed structure and legal infirmities.



Our Center for Constitutional Studies, under Roger Pilon’s innovative leadership, has been a forceful advocate for a textual interpretation of the Constitution and a principled judicial engagement to bind the legislative and executive branches with the chains of the Constitution. Pilon and Ilya Shapiro, editor of the peerless _Cato Supreme Court Review_ , compiled an amazing record of amicus briefs, with the Court favoring the party we supported in an overwhelming percentage of cases.



In 2014, we expanded our Center for the Study of Science, which challenges the pseudoscientific claims of climate‐​change alarmists. That same year, Cato’s Center for Monetary and Financial Alternatives got under way — aggressively responding to the threat of an undisciplined central bank and fiat money.



We recognize, of course, that reaching a young audience is essential. Our Libertarian Leadership Project will enable us to dramatically expand our online contact with young, tech‐​savvy friends of liberty — complementing our intern program and Cato University. We’ve also fostered high‐​quality education through the work of the late Andrew Coulson, who directed Cato’s Center for Educational Freedom. Coulson produced a three‐​part documentary that embraces freemarket reforms to make schools more innovative and responsive. _School Inc. — A Personal Journey with Andrew Coulson_ is now available on PBS stations across the country.



Persuasion is key to our mission; and David Boaz’s latest book, _The Libertarian Mind_ , is the perfect messenger — a scholarly but readable work that popularizes and legitimizes libertarianism. Indeed, Cato books are in every major college library and have sold tens of thousands of copies. Cato scholars also deliver hundreds of college lectures annually, presenting the libertarian alternative to the next generation.



In the economic arena, our experts analyze tax reform and budget proposals — unraveling them so they’re digestible. Cato’s “Fiscal Report Card on the Nation’s Governors” is widely quoted, and our Herbert A. Stiefel Center for Trade Policy Studies shaped the debates over Trade Promotion Authority and the Trans‐​Pacific Partnership.



Cato has also been a prominent defender of civil liberties — especially those related to government surveillance and privacy. Meanwhile, our Project on Criminal Justice has steered public opinion against police militarization and the drug war.



In addition, we’re emphasizing the moral and philosophical arguments for liberty. Online courses are available from our Lib​er​tar​i​an​ism​.org website. They’re aimed at young people but accessible to a large and growing audience, as is our CatoAudio app for iOS devices, which contains our daily podcast, archived policy forums, _Classics of Liberty_ , the monthly CatoAudio magazine, and lots more.



A big focus of our 40th year has been finding ways to keep getting better and even more impactful. There is scope to be more connected and effective on Capitol Hill and in states, and in the academy, too, by hosting more visiting scholars. We also have great opportunities to expand the audience of our existing scholarship and research through the expanded use of technology and a broader array of content. We plan to strengthen our efforts to nurture young talent and the development of more organizations and initiatives across the liberty movement.



In short, Cato is an independent, nonpartisan source of intellectual ammunition to the public, government, educators, and the media. Ideas do matter. That’s the reason Cato is indispensable. As our 40th year draws to a close, we reaffirm our enduring commitment to the cause of human freedom.
"
"A graveyard of trees fixes into focus on the screen. The television footage zooms in on their burnt skeletons, thin and black. Dark ash is spread beneath their leafless arms. The camera then catches the peloton, a kaleidoscope of colour blurred against the barren landscape. The riders snake through the hillside; their movements are almost alien, otherworldly. It’s the Tour Down Under, at the opening of the 2020 UCI World Tour. Months earlier, across the Australian continent, wildfires raged. These fires spread their fury across the nation, reaching a devastating crescendo along the south-east coast. The red, black and grey ate up national parks, vineyards and farmland. It scorched the beautiful bush and damaged ancient Aboriginal cultural sites. With only a slight altering of circumstance, there might have been no Tour Down Under.  Cycling is a sport with no parallel. What other sport is played out across the world, traversing thousands of kilometres on countless roads, twisting and turning, up mountains, along rivers, next to the coastlines and through narrow towns? What other sport explores the edges of a nation, and races across its heart? It’s an ever-changing playing field. In pro cycling, only a thin line exists between the athlete and the natural world. It’s beautiful, it’s dramatic and it’s what makes cycling such a great sport. But it could also be its greatest threat. Heatwaves and unpredictable weather could easily stop a race in its tracks, changing outcomes, or even preventing any start. Though human-induced climate change may not be the sole cause of each bushfire, it certainly has increased their severity and destructiveness. While the weather has always been a key character in the world of cycling, what happens when that character becomes increasingly sinister? Professional cycling is a sport that has weathered many storms in its long history. The sport’s ongoing fight against the scourge of doping has been one of its greatest challenges. Yet, it is also arguably the case that not many other sports have had the same level of introspection, self-scrutiny and action on this front either. Cycling, in some ways, has faced up to the reality of doping and is actively working on solutions to it. The sport no longer views it as a scab that will simply heal itself; rather it is something that needs attention, action. The future and the integrity of the sport depends on it. A consequence of this has been the acknowledgment of doping’s existence and its impact in cycling. The same, however, cannot be said for the threat of climate change. It’s too easy to be passive in this regard. Admittedly sport does not have the power of a policy-making government. Professional cycling does not necessarily drive global economic investment. Rather than be a bystander, professional cycling and its constituents can be part of the solution. The future of the planet does not have to be marred in black ink; we do not need to be resigned to a desolate future. At its best, sport can be the expression of shared human experience, bringing us together. Professional sport – in its ability to display the possibility and artistry of the human body – is not an idle actor. It has currency and power. Policies and strategies, led by the governing body, need to be crafted in a manner that factors in the likelihood of worsening weather conditions due to climate change. The current extreme-weather protocols, enacted in 2016, are inadequate in the face of future climatic challenges. The protocols are not strategic, but rather reactionary, with no broader perspective outside of assessing individual weather events as they arise. Professional cycling could also take a more critical approach with sponsorship, considering what role a sponsoring entity may have in the context of climate change. These are not easy tasks, nor do they have straightforward or painless solutions. Professional cycling, almost exclusively, relies on sponsorship, and every year it seems as though teams are on the edge of collapse. Financial stability and sustainability are very real challenges for the sport. Unsurprisingly, money often speaks louder than the more complicated ethical considerations that may be attached to the cash. Yet with the predicted economic costs associated with climate change, perhaps a lack of cycling sponsorship could be another consequence of our warming planet. Plans for the sport’s future should be developed with this global reality factored in. Discussions on how pro cycling can be a part of the global movement to reduce emissions should take place. And now, as summer draws to an end and the peloton moves on from Australian leg of the World Tour, the fire and trauma for those on the front line remain. But perhaps, just maybe, a much-needed conversation will begin. Professional cycling’s future may depend on it."
"A few weeks ago I was visiting a colleague in Brazil who told me he had a new post-doctoral researcher working for him from West Africa, but that he was in 21 days quarantine. I asked him if the newest member of his staff was in the university’s hospital; he replied “no”, he is wandering around the streets of the city until his 21 days are up – he is just not allowed on the campus.  As disease researchers know the great problem of the modern era is transport: since the times of our great-grandfathers human ability to traverse the planet has increased exponentially. And the risk of disease epidemics such as Ebola has followed. The present Ebola crisis appears, like HIV before it, to have started with the disease jumping from a wild species, in this case bats, to humans. Ebola and bats have been battling out an evolutionary war for thousands of years and have more or less come to a stalemate whereby bats are infected and the virus can reproduce itself, but bats are not killed.  A similar situation is seen in the case of simian retroviruses (SIV), the precursors of HIV.  Both come about from the arms race that occurs between a disease and its host: if lions start to run faster then so do their prey, otherwise the prey and ultimately the lion will go to extinction.  These wars between diseases and their hosts can be found everywhere on the planet. But what about the native people who live in forests? Surely they have been fighting this same evolutionary arms race with these same diseases. The answer is perhaps not. Studies of the Ache tribe of hunter gathers in Paraguay show that they do not hunt species indiscriminately. While their jungle home contains thousands of potential animal species to consume, they basically focus on eating only 12. The items on their menu are selected in terms of their energetic profitability; that is, the minimum amount of search time for the maximum amount of calories. In this case the favoured food item is the armadillo. Historically hunter gather tribes were small and widely dispersed. Thus, if they did get Ebola everyone might have died but there would have been little transmission to other groups of humans – and no epidemic. Agriculture changes everything, as large well-connected groups can easily transmit diseases. It is also worth remembering that diseases can jump the species barrier in both directions. About five years ago in the Brazilian city of Belo Horizonte all of the wild urban marmosets in one area died out due to cold sore infections. Cold sores are caused by a herpes virus. This outbreak probably started unintentionally when a person with a cold sore gave a fruit they were eating to some marmosets. Disease transmission is very much a two-way street and there is increasing evidence of human diseases passing on to wildlife, especially primates. Wild animals hunted and eaten in tropical forests are known as bushmeat – and bushmeat represents a crisis of its own, as hunting threatens to make many species of wildlife extinct. The crisis has its origins in poverty. People simply need to eat animals to survive, a situation that is made worse by deforestation and the fragmentation of natural habitats.   There is the often romanticised view of native peoples as conservationists since they are generally not thought to have made animal species go extinct. But this situation is more to do with their limited technology and small populations relative to their environment, rather than because native people live in an ecologically friendly manner. As their traditional forests are hit, and the easy pickings dry up, eventually the menu of such people will need to include new less energetically profitable food. And access to technology such as firearms can make previously unattainable prey available. Much of the modern bushmeat trade is no longer connected to native people needing to exploit wildlife as a food resource, but the descendants of these people who have developed a taste for the food. It is for this reason that several hundred tonnes of bushmeat enter Europe each year, where its illegality has made it a status symbol in some sections of society.  Part of the problem with this trade in bushmeat is that judges in the countries where the hunting takes place often, naïvely, believe the hunter’s pleas of poverty and just “smack them on the wrists”. Research in the north-east of Brazil has proven conclusively that hunting birds for food is much more expensive than buying chicken from the supermarket. Humans spent the past few thousand years breeding chickens, cows and pigs for a reason: they make a nicer, cheaper and less dangerous dinner than bats, gorillas or armadillos. Unfortunately, the threat of picking up a dreadful disease from bushmeat may not save these animals from extinction. A few years ago there was a yellow fever outbreak in Brazil and it was announced on the television that monkeys can be a host for this disease: this lead to the killing of wild urban primates in some cities. If humans continue to increase the items on their bushmeat menu then we can expect more diseases like Ebola and HIV to appear."
"

WOW look at the SIZE of that seal! (photo added by Anthony, not NYT)
Bearing Up
By SARAH PALIN
Published: January 5, 2008,
Juneau, Alaska
ABOUT the closest most Americans will ever get to a polar bear are those cute, cuddly animated images that smiled at us while dancing around, pitching soft drinks on TV and movie screens this holiday season.
This is unfortunate, because polar bears are magnificent animals, not cartoon characters. They are worthy of our utmost efforts to protect them and their Arctic habitat. But adding polar bears to the nation’s list of endangered species, as some are now proposing, should not be part of those efforts.
To help ensure that polar bears are around for centuries to come, Alaska (about a fifth of the world’s 25,000 polar bears roam in and around the state) has conducted research and worked closely with the federal government to protect them. We have a ban on most hunting — only Alaska Native subsistence families can hunt polar bears — and measures to protect denning areas and prevent harassment of the bears. We are also participating in international efforts aimed at preserving polar bear populations worldwide.
This month, the secretary of the interior is expected to rule on whether polar bears should be listed under the Endangered Species Act. I strongly believe that adding them to the list is the wrong move at this time. My decision is based on a comprehensive review by state wildlife officials of scientific information from a broad range of climate, ice and polar bear experts.
The Center for Biological Diversity, an environmental group, has argued that global warming and the reduction of polar ice severely threatens the bears’ habitat and their existence. In fact, there is insufficient evidence that polar bears are in danger of becoming extinct within the foreseeable future — the trigger for protection under the Endangered Species Act. And there is no evidence that polar bears are being mismanaged through existing international agreements and the federal Marine Mammal Protection Act.
The state takes very seriously its job of protecting polar bears and their habitat and is well aware of the problems caused by climate change. But we know our efforts will take more than protecting what we have — we must also learn what we don’t know. That’s why state biologists are studying the health of polar bear populations and their habitat.
As a result of these efforts, polar bears are more numerous now than they were 40 years ago. The polar bear population in the southern Beaufort Sea off Alaska’s North Slope has been relatively stable for 20 years, according to a federal analysis.
We’re not against protecting plants and animals under the Endangered Species Act. Alaska has supported listings of other species, like the Aleutian Canada goose. The law worked as it should — under its protection the population of the geese rebounded so much that they were taken off the list of endangered and threatened species in 2001.
Listing the goose — then taking it off — was based on science. The possible listing of a healthy species like the polar bear would be based on uncertain modeling of possible effects. This is simply not justified.
What is justified is worldwide concern over the proven effects of climate change.
The Center for Biological Diversity, which petitioned for the polar bear to be protected, wants the listing to force the government to either stop or severely limit any public or private action that produces, or even allows, the production of greenhouse gases. But the Endangered Species Act is not the correct tool to address climate change — the act itself actually prohibits any consideration of broader issues.
Such limits should be adopted through an open process in which environmental issues are weighed against economic and social needs, and where scientists debate and present information that policy makers need to make the best decisions.
Americans should become involved in the issue of climate change by offering suggestions for constructive action to their state governments. But listing the polar bear as threatened is the wrong way to get to the right answer.

Sarah Palin, a Republican, is the governor of Alaska.
h/t to L Nettles


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d4159eb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Come the cold season, whenever there is some type of strong storm system near the U.S. Eastern Seaboard—be it a Nor’easter, a blizzard, or ex-hurricane Sandy—you don’t have to look very hard to find someone who will tell you that this weather is “consistent with” expectations of climate change resulting from human greenhouse gas emissions. The worse the storm, the more “consistent” it becomes.   
  
The complete collection of climate science describes just how complex the physical processes are governing such storm systems. Teasing out any anthropogenic influence, including even the direction of any influence, is darn near impossible. Claims to the contrary are usually based on a highly selective assessment of the science or the data.   
  
A case in point:   
  
The latest _en vogue_ explanation linking human greenhouse gas emissions to strong winter-season East Coast storms involves changes in the characteristics of the jet stream—a river of fast moving air in the atmosphere that influences both the strength and the forward speed of extratropical storm systems. A prominent (in the media, anyway) research study last year by Rutgers’s Jennifer Francis and University of Wisconsin’s Stephen Vavrus suggests that the declining temperature difference between the Arctic and the lower latitudes (adding greenhouse gases into the atmosphere warms colder, drier regions more so than warmer, wetter ones—with the notable exception of Antarctica) has led to changes in the jet stream which result in slower moving, and potentially stronger East Coast winter storm systems.   




Just Google “Jennifer Francis global warming” to see how this mechanism is supposedly tied to all sorts of extreme weather events.   
  
Even before the Francis and Vavrus study made it to print, we noted that their findings ran afoul of other existing literature which painted a far murkier picture of the influence (if any) that anthropogenic global warming was having in extratropical cold-season storm systems. After reviewing the literature, we cautioned:   




So where does this leave us? When the new paper by Francis and Vavrus comes to the attention of the mainstream press, it’ll play as if a warming Arctic and declining sea ice—an asserted consequence of human greenhouse gas emissions—has been definitively tied-in to all sorts of weather extremes across the U.S. No mention will be made to the fact that other research, which is many cases is more robust and detailed, has concluded nearly the opposite.



Conflicting research findings continue to be published.   
  
One was published several months ago in _Geophysical Research Letters_ by James Screen and Ian Simmonds, who looked for changes in jet stream characteristics using a different methodology than that of Francis and Vavrus. A robust signal should be apparent no matter how you look at it (within reason). But Screen and Simmonds found few statistically robust changes and what changes they did find were contrasting depending on the methodology they used. They noted that the changes they found were much smaller (and non-significant) than the large (and significant) changes reported by Francis and Vavrus. Screen and Simmonds concluded that their findings held “different and complex possible implications for midlatitude weather, and we encourage further work to better understand these.”   
  
In other words, the picture is far less clear than that described by Francis and Vavrus.   
  
This point is further driven home by a another paper just accepted in _Geophysical Research Letters_ by Colorado State University’s Elizabeth Barnes. Barnes, too, examined the relative warming in the Arctic and its possible link to extreme weather events in the Northern Hemisphere mid-latitudes. In a nutshell, she found little if any definitive relationship—again in contrast to the Francis and Vavrus results. Barnes discussed this discrepancy directly:   




We conclude that the mechanism put forth by previous studies (e.g. Francis and Vavrus [FV12]; Liu et al. [2012]), that amplified polar warming has led to the increased occurrence of slow-moving weather patterns and blocking episodes, is unsupported by the observations….A recent study by Screen and Simmonds [2013] also provides evidence that the trends in planetary waves suggested by FV12 may be an artifact of their methodology… The Arctic is changing rapidly, and these changes will likely have profound effects on the Northern Hemisphere. This study, however, highlights that the relationship between Arctic Amplification and midlatitude weather is complex.



The more folks look the less robust the popular global-warming-is-leading-to-more-extreme-winter-storms finding by Jennifer Francis and Stephen Vavrus seems to be.   
  
It’ll be interesting to see during this upcoming winter season how often the press—which seems intent on seeking to relate all bad weather events to anthropogenic global warming—turns to the Francis and Vavrus explanation of winter weather events, and whether or not the growing body of new and conflicting science is ever brought up.   
  
If you don’t see it in the morning paper, you will most certainly find it here!   
  
**References:**   
  
Barnes, E., 2013. Revisiting the evidence linking Arctic Amplification to extreme weather in the midlatitude. _Geophysical Research Letters_ , in press, doi: 10.1002/grl.50880.   
  
Francis, J. A. and S. J. Vavrus, 2012: Evidence linking Arctic amplification to extreme weather in mid-latitudes. _Geophysical Research Letters_ , **39 (6)** , L06 801, doi:10.1029/2012GL051000.   
  
Screen, J. and I. Simmonds, 2013: Exploring links between Arctic amplification and midlatitude weather. _Geophysical Research Letters_ , **40** , 1–6, doi:10.1002/GRL.50174.   
  
  
  
  
  



"
"Alok Sharma, the former international development secretary, is the surprise choice to take on the role of president of the crunch UN climate talks to be hosted by the UK this November. He has also been made business secretary as part of Boris Johnson’s cabinet reshuffle.  Sharma has garnered praise from campaigners for his role at the Department for International Development, but will face an uphill task after nearly two weeks of trouble surrounding the post of Cop26 president. The former energy minister Claire O’Neill was abruptly sacked from the role and unleashed a vitriolic attack on Johnson, while several other high-profile figures including David Cameron and William Hague turned down the role. Sharma has a mixed record on voting on green issues in parliament. The Guardian’s Polluters project scored MPs on how they swung on a range of key votes. Sharma scored only 15%, a poor showing, as he was present for 13 votes affecting climate and environmental issues, but voted positively on only two of them. Since 2010 he has been MP for Reading West, to the west of London and just over 20 miles from Heathrow, and he has both opposed and publicly favoured Heathrow expansion. TheyWorkForYou, which rates MPs on their voting records, found he “generally voted against measures to prevent climate change”. In the record of MPs’ interests, he has received a donation of £15,000 from Offshore Group Newcastle, which makes platforms for oil, gas and wind energy companies. However, he has used his role at DfID to promote action on the climate crisis, by assisting developing countries to improve their resilience to the impacts of extreme weather, and tackling issues such as deforestation and clean energy. Last October, he urged the World Bank to focus more of its funding on the climate crisis. Mohamed Adow, director of energy and climate at thinktank Power Shift Africa, said: “It’s a relief to finally have a Cop president in post. But now the hard work must start. For such a crucial summit it’s worrying that Alok Sharma takes up the reins with only nine months to go. He will need all the resources of government and the diplomatic service to ensure the UK Cop is not a failure. “This is the UK’s first real test post-Brexit and so far Britain has not looked like a serious player on the global stage. The eyes of the world are watching and the UK’s Commonwealth allies in Africa and around the world will be demanding an outcome that sees those of us on the front lines of the climate crisis protected.” Sharma will be expected to combine the Cop26 role with the job of business secretary, one of the key roles in cabinet, especially as the government wrestles with Brexit. Rachel Kennerley, climate campaigner at Friends of the Earth, asked whether they were compatible. “Can Alok Sharma serve as both business secretary and president of the UK’s biggest global climate summit? The presidency isn’t like a student picking up a few extra bar shifts, it’s about leading the world’s climate ambition during a crucial time for the environment,” she told the Guardian. “We cannot afford a part-time president.”  Kat Kramer, global climate lead at the charity Christian Aid, said Sharma faced a “grave and delicate” task but could take advantage of his business secretary role to ensure the UK presented itself as a credible leader in the climate fight, through a national plan of action for reaching net zero emissions. She said: “It would have been a big task had Alok Sharma been in post from the beginning, rather than coming in late in the process. It’s now vital [he] work very closely with the backing of the prime minister to both get other countries to commit to new pledges to tackle the climate crisis but also put the UK’s own house in order and enact policies to accelerate UK decarbonisation. As secretary of state for business, energy and industrial strategy, Sharma will be well placed to oversee this.” Several ministers had been mooted for the role, including Michael Gove, who told a conference on Tuesday there were “many, many, many, many” people who would do better as Cop26 president, such as Zac Goldsmith and Kwasi Kwarteng, the clean energy minister. Gove hesitated when asked what would make the summit a success, before answering that success would be for countries to “accept the need to change and that leads to irreversible, accelerating and inclusive action” on the climate crisis. That falls well short of requiring governments to come forward with concrete new plans on cutting carbon in line with the goals of the 2015 Paris agreement, which is what activists and supportive countries such as the EU are hoping for. The job was turned down by former prime minister Cameron, who is said to have been too busy, and former foreign secretary Hague, who is believed to have had concerns about the role. Their reluctance suggests a perceived low standing for the role, which will not help the new incumbent. Previous Cop summits have tended to be led by the host government’s most prominent minister with a relevant portfolio, usually the environment minister.  France signalled its determination to forge a new global agreement in Paris by appointing Laurent Fabius, then foreign secretary, who led a tireless round of visits to foreign capitals in his military jet, while also getting other major figures such as Ségolène Royal closely involved in the intensive diplomacy. The then French president, François Hollande, also played a leading role. If Cop26 is to be a success – and so far the government has promised success but set a low bar for what that might look like – then Sharma will need to be able to call on the international firepower of the whole government, and Johnson will have to shake up every major department to come forward with domestic plans to prove the UK is on the road to net zero."
"
Share this...FacebookTwitterThey all admit the IPCC is flawed and in need of an overhaul, yet they still insist the science is correct. Go figure. That’s pure rubbish, of course. Bad process = bad product. 
It sounds just like the National Academy of Sciences reaching the conclusion that Michael Mann’s hockey stick science had no value, but his answer was still correct.Here are the excerpts from some of the leading online papers in Germany, Austria and Switzerland. (I’ve also added some UK links below, h/t: http://thegwpf.org/
 SÜDDEUTSCHE  ZEITUNG in Germany (a favorite of Stefan Rahmstorf) writes:
Obligation to be more open
An examination of the IPCC reveals: The IPCC has to change the way it works. Yet, there’s no basis for the foaming attacks on its results.
DER STANDARD  in Austria writes:
In the expert team’s assessment, they recommend formulating stricter scientific guidelines for handling data on climate change. Forecasts and projections should be made only based on solid scientific evidence.
Sounds good. But if that were to be implemented, the entire IPCC 4AR would be reduced to only 2 pages:  a front and back cover.
DER SPIEGEL in Germany writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Consequence of the crisis: Experts urge an overhaul of the UN IPCC. They harshly criticised the work of the leadership. Not only the leadership, but also the scientific work of the climate panel is in need of reform.
 DIE ZEIT in Germany writes:

IPCC to be a reform project
Flawed data, hacked e-mails: The doubts about the IPCC were enormous. Now the UN draws a conclusion: Its work was correct, but its credibilty must be strengthened.
 DIE WELT  in Germany writes:
The IPCC which has been under fire because of series of follies has to be comprehensively reformed in order to remove doubts about its credibility. A report presented by the UN-appointed experts has reached the conclusion that a ‘fundamental reform’ of the IPCC is needed in order for it to strengthen its scientific standards and organisation.
NZZ in Zurich has a more lengthy piece, and writes:
The Interacademy Council (IAC) said critics were only partially right. In a report presented to the UN in NewYork on Monday, the IPCC was attested as having done, on the whole, good work. But it is criticised that its management structures and public relations work of the IPCC did not fulfill the requirements of today.
From the UK, h/t : http://thegwpf.org/
The Sun: UN ‘lacks Solid Evidence’ in Climate Warnings
Daily Express: Climate Change Lies Are Exposed
BBC News: Stricter controls urged for the UN’s climate body
The Times: Climate chief under pressure to quit after report on glacier blunder
The Guardian: Rajendra Pachauri, head of UN climate change body, under pressure to resign
The Independent: IPCC feels the heat as it is told to get its facts right about global warming
Share this...FacebookTwitter "
"Oil prices have fallen dramatically since August – and, rather counter-intuitively, this could be a bad thing. As of late October, the price of oil has fallen from US$110 per barrel (bbl) to below US$85. There are predictions it could further descend to US$60-$70. While a drop was expected, due to lower global demand and oversupply, such levels would be radical indeed. A major price fall is indeed a bad thing – and, if it continues, it could be a very bad thing. But why? Haven’t we been told for half a century that cheap oil is beneficial? Doesn’t it lower consumer fuel bills, control inflation, reduce transport expenses and thus the price of necessities like food – while stimulating industrial activity by slashing costs? And haven’t prices above US$100 been an anchor on economies recovering from the global financial crisis?  Let’s not forget that high oil prices also transfer huge amounts of cash and geopolitical influence to petrostates such as Russia or Iran – and also non-state actors that control oil fields, in particular, Islamic State. So low prices really are good, right? Not necessarily. Though all of the above remains largely true, there are now other considerations that outweigh those factors – and indeed should have outweighed them earlier. High prices have stimulated major innovations in energy, including efficiency. We are truly in the middle of a revolution in vehicle technology: hybrid cars are now a common sight in many of the world’s major cities, with every big car maker producing several models, including luxury brands like Mercedes, Lexus, BMW, and Cadillac.  Numbers in the US for plug-in and all-electric (EV) models have soared in the last three years, with as many as 20 new EV models released in 2014, eight of them priced below US$35K. Coming improvements in battery technology will continue to increase the driving range for electric vehicles and will continue to bring down prices, making them affordable for many millions.  There has also been growth in alternative fuels, now including natural gas for long-haul trucks and marine vessels, replacing far more polluting and carbon-emitting diesel and bunker fuels.  Taken together, this represents a substantial, but still early-stage, shift away from petroleum as the single source of global transport and towards a future most people would now call more sustainable.  But there is more. The past five years of consistently elevated prices have increased public support for renewables and have encouraged the idea of sustainability as a measure of the common good. This has aided local and national governments to choose more sustainable options in energy efficiency, carbon emissions, water use, and more.  High prices have also encouraged many businesses and industries to improve their image and lower costs by recycling more and reducing energy use and emissions. But what about geopolitical issues? Here, too, we find a different calculus in place. Over time, a shift away from petroleum would solve the problem of cash and power going to petrostates and might force them to diversify their economies. But don’t look for this to happen anytime soon.  In the meantime, however, high prices have brought the full-scale development of shale oil/gas in North America, making it a growing alternative to Russia and OPEC. While debate continues over the impacts of fracking, the US is on trend to match the country’s all-time production peak of 9.5m bbls/day. America has already replaced Russia as the world’s largest exporter of petroleum products and could begin exporting significant volumes of crude oil if its long-time ban on such exports were lifted.  Needless to say, reducing Europe’s dependence on Russian oil and natural gas would be beneficial, given that country’s aggressive foreign policy. Similarly, energy from North America (US and Canada) could reduce Japan and Korea’s reliance on OPEC, now at over 90%.  These considerations must be weighed against the negative effects of high prices, of course. In the oil cosmos, there are no easy alternatives. Yet prices matter a great deal; the lower they are, the fewer the alternatives. All of the positives noted above would end were oil to reach US$60-$70 and remain there. Does this seem exaggerated? We have historical precedents. The dramatic price collapse of 1986, after which oil stayed cheap for nearly two decades, effectively killed all sustained interest in energy improvements (although the 1990s did bring us a major vehicle innovation – the gas-guzzling SUV). Low oil prices are a narcotic and an enemy of a different future."
"

Spurred by tax competition, the flat tax revolution continues to generate positive results. Albania will have a 10 percent flat tax beginning in January 2008. The corporate rate also will be 10 percent, as will the payroll tax. The latter reform is particularly interesting since many of the flat tax nations in Eastern Europe retain punnitive payroll tax rates — a policy that undermines the pro‐​growth and pro‐​employment effects of the flat tax. The _Southest European Times_ reports: 



In a bid to promote growth and improve the business climate, the administration of Albanian Prime Minister Sali Berisha plans a major overhaul of the tax system. The biggest change is a switch to a flat tax. “As of January 1st, 2008, Albania will have implemented the 10% flat tax system, one of the lowest in Europe,” Berisha told a business community meeting in late March. Corporate taxes, currently at 20%, are to be slashed in half. Social security contributions from businesses will likewise be capped at 10%. The government and other supporters of the reform say it will widen the taxable base and simplify tax administration, while also making Albania an easier place to invest. According to Finance Minister Ridvan Bode, the changes will lead to a more streamlined fiscal system. “The flat tax helps eliminate the potential arbitrage between corporate tax, dividend taxes and the income tax,” he says. VAT and other taxes will also be gradually reduced in order to woo investors, the minister added. …In the past, the IMF has been wary of plans to reduce taxes in Albania. This time, however, it seems more receptive — provided the overhaul is combined with more effective revenue collection. “We will negotiate with the Albanian government about the tax reduction, depending on the tax collection,” IMF representative Ann Margaret Westin told the press.
"
"

 **MEMORIES  
LIGHT THE CORNERS OF MY MIND  
MISTY WATERCOLOR MEMORIES  
OF THE WAY WE WERE …  
WHAT’S TOO PAINFUL TO REMEMBER  
WE SIMPLY CHOOSE TO FORGET**



Raisa Burykina, an 85‐​year‐​old pensioner whose father was killed in the battle, said she was pleased to have seen Mr. Putin at the concert.





“Prices went down under Stalin; now they are going up,” said Ms. Burykina, who had a clutch of Soviet orders pinned to her red sweater.  
— _Wall Street Journal_ , February 7, 2018



  
 **BOOTLEGGERS AND BAPTISTS**  
“The president’s budget proposes to replace in significant part the very successful current system of having SNAP recipients use EBT cards to purchase food,”… Jim Weill, president of the Food Research and Action Center, said in a statement.



The proposal is also likely to enrage food retailers — particularly Walmart, Target and Aldi — which stand to lose billions if food‐​stamp benefits are cut, analysts say. On Monday, the Food Marketing Institute, a trade association for grocery stores, condemned the Harvest Box proposal as expensive, inefficient and unlikely to generate any long‐​term savings.  
— _Washington Post_ , February 13, 2018



 **HOW TO GET A HOUSE UNDER SOCIALISM**  
Sexual assault and harassment are rife across all sectors of North Korea’s misogynistic society, according to a new report. …



Many reported that men in positions of authority used their power to take advantage of women.



One woman described going to the mayor’s office to be allocated a house. “I was 32 years old and I must have looked attractive in his eyes. I was raped in his office and received a house in return.”  
— _Washington Post_ , March 8, 2018



 **NO TIME TO MAKE THE DONUTS**  
A French boulanger has been ordered to pay a €3,000 fine for working too hard after he failed to close his shop for one day a week last summer. …



Under local employment law, two separate regulations from 1994 and 2000 require bakers’ shops to close once a week — though exceptions can be made in specific cases.  
— _The Guardian_ , March 14, 2018



 **YOU MIGHT BE, ACTUALLY**  
I can’t be the only one concerned that an increasing amount of the orange juice Americans consume might be sourced in Brazil and Mexico rather than Florida and California.  
— Andrew Furman in the _Wall Street Journal_ , March 23, 2018



 **WHEN YOU PUT GOVERNMENT IN CHARGE OF SOMETHING, YOU’RE PUTTING POLITICIANS IN CHARGE**  
D.C. Council member Trayon White Sr. (D‐​Ward 8) posted the video to his official Facebook page at 7:21 a.m. as snow flurries were hitting the nation’s capital. …



“Man, it just started snowing out of nowhere this morning, man. Y’all better pay attention to this climate control, man, this climate manipulation,” he says. “And D.C. keep talking about, ‘We a resilient city.’ And that’s a model based off the Rothschilds controlling the climate to create natural disasters they can pay for to own the cities, man. Be careful.”  
— _Washington Post_ , March 18, 2018



 **SOME SAY ANTI-SEMITISM IS THE SOCIALISM OF FOOLS, BUT SOMETIMES THE SOCIALISM OF FOOLS IS JUST SOCIALISM**  
President Nicolás Maduro late Thursday briefly outlined his monetary rescue plan. In a country where a dozen eggs can cost 250,000 bolivars ($5) amid worsening inflation, he would chop three zeros off the currency — arguably bringing the price for those eggs down to 250.  
— _Washington Post_ , March 23, 2018



 **AS USUAL**  
Regulators say they may need more power  
— Headline in the _Washington Post_ , February 7, 2018



 **CITIES PLAN FURTHER HOUSING SHORTAGES**  
Lawmakers and advocates in California, Illinois and Washington State are pushing to repeal state laws that forbid rent control or place limits on cities’ ability to regulate rent increases.  
— _Wall Street Journal_ , February 5, 2018
"
"
Share this...FacebookTwitterA new temperature reconstruction carried out by a team of German/Russian scientists has yielded interesting results. It finds no correlation over the last 400 years between atmospheric CO2 and the temperature in the Arctic regions studied.


Yuri Kononov of the Institute of Geography, Russian Academy of Sciences in Moscow and Michael Friedrich of the Institute of Botany, University of Hohenheim collect tree samples of Scots pine in the Khibiny Low Mountains of the Kola Peninsula in Arctic Russia

Recall that CO2 concentrations have been rising steadily since the start of the Industrial Revolution, 1870, yet the press release starts with:
Parts of the Arctic have cooled clearly over the past [20th]century, but temperatures have been rising steeply there since 1990.
Rising since 1990? That’s more than 100 years after the start of the Industrial Revolution. The press release continues:
The reconstructed summer temperature on Kola in the months of July and August has varied between 10.4°C (1709) and [peaking at] 14.7°C (1957), with a mean of 12.2°C.  Afterwards, after a cooling phase, an ongoing warming can be observed from 1990 onwards.
The temperature fluctuated between 10.4°C  and a peak of 14.7°C in 1957 , and then cooled until 1990. The scientists say it correlated very well with solar activity until 1990. Then beginning in 1990, the temperatures started to rise rapidly again. Does anyone see a CO2 correlation there? I don’t.
The only time we have a correlation between CO2 concentrations and temperature is from 1990 until…? Unfortunately the press release does not even mention that.  Until today? 2005? 2000? It really is annoying that they didn’t specify the end of the time scale. If the reconstruction was only up to 2000, then we are only speaking about a 10-year period – a completely meaningless time period. Even 20 years would be highly dubious.
I called UFZ early this afternoon here in Germany to try to find out, but the secretary said that all scientists had already left for the weekend.
*****************************************************
UPDATE! The German press release now has the following graphic. The dataset ended 2001! The press spokesman just told me on the phone. So there was warming from 1990 until 2001!  As you see, the graphic iteself is misleading. It almost looks as if the curve goes until 2010.
Press spokesman Tilo Arnhold informed by telephone that the dataset goes only up to 2001, yet the press release graphic clearly shows a curve beyond 2001. Graphic Source: Stephan Boehme/UFZ
Interestingly, also, the graphic shows warming since 1650.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




******************************************************
The press release also states:
What stands out in the data from the Kola Peninsula is that the highest temperatures were found in the period around 1935 and 1955, and that by 1990 the curve had fallen to the 1870 level, which corresponds to the start of the Industrial Age.
The temperature fell to 1870-levels by 1990? Wait a minute – the CO2 theory say it’s supposed to go up, and not down.
The team compared their Kola region data to Swedish Lapland and the Yamal and Tamimyr Penninsula temperature reconstructions: Here’s what they found:
The reconstructed summer temperatures of the last four centuries from Lapland and the Kola and Taimyr Peninsulas are similar in that all three data series display a temperature peak in the middle of the twentieth century, followed by a cooling of one or two degrees.
Cooling from the middle of the twentieth century until 1990. Cooling!
Concerning Yamal, it is no surprise that it’s out of the ballpark because that dataset was not handled scientifically, see Yamal-gate
(Keith Briffa cherrypicks tree rings to get the temperature reconstruction he wants to see).
So if it’s not CO2, then what could be driving temperature?  The press release goes on:
What is conspicuous about the new data is that the reconstructed minimum temperatures coincide exactly with times of low solar activity. The researchers therefore assume that in the past, solar activity was a significant factor contributing to summer temperature fluctuations in the Arctic.
The only mystery left is why was there was a sudden increase in warming from 1990 until 2001? The scientists believe it has to do with local factors. Clearly it isn’t CO2.
Share this...FacebookTwitter "
"

As billions of little light bulbs brighten America this holiday season, Al Gore is calling for thousands across the nation to interrupt their regularly scheduled activities and hold house parties showing his environmental _cri de coeur_.



Gore announced recently on the Oprah Winfrey Show that Americans should congregate this Saturday, December 16, to watch and discuss his DVD, _An Inconvenient Truth_ , advertised as “a true story about the hard science and real threats of global warming.”



The idea is to demonstrate that “action” is wanted on climate change.



If climate alarmists are to be believed, Americans must cut their electricity use substantially, and soon, to reduce greenhouse‐​gas emissions associated with fossil‐​fuel combustion. Celebratory holiday lighting — what doomsayer Paul Ehrlich once called “garish commercial Christmas displays” — would surely be the first to go, coming before indoor lighting, cooking, heating, and air conditioning.



But are these changes really necessary for the United States, the world’s most prolific user of energy? The good news — and a reason for holiday cheer — is that the science behind rapid, disruptive global warming scenarios is murky at best. Though the debate is highly politicized and emotionally charged, good science is beginning to drive out bad.



The Kyoto Protocol and other sledgehammer approaches to cutting greenhouse‐​gas emissions in the advanced countries are coming under intellectual, not just political, assault.



A sampling of recent issues of _Science_ , the journal of the American Association for the Advancement of Science, shows that peer‐​reviewed studies dispute virtually all the tenets behind climate alarmism. A November 17 feature, “False Alarm: Atlantic Conveyor Belt Hasn’t Slowed Down After All,” rebuts the hyped hypothesis that melting ice from global warming (read: man‐​made global warming) would disrupt ocean currents and plunge Europe into an Ice Age.



The same _Science_ report takes on the idea that warming causes drastic cooling, the complicated, and ironic scenario Al Gore said “some scientists are seriously worried about.”



 _Science_ comments that even if global warming were cooling specific regions (a big if), “it would be decades before the change would be noticeable above the noise.”



And here, in a nutshell, is what the climatology debate is about: if and how much the human influence on climate is detectable above natural variability.



For instance, rapid rises in sea level produced by global warming is another popular alarm, one very relevant for residents of the Texas Gulf Coast area. But as the November 24 issue of _Science_ says, “It remains unclear whether the recent rate increase [since 1993] reflects an acceleration in sea‐​level rise or a natural fluctuation.”



Indeed, sea level has been rising for well over a century for the same natural reasons that brought the end of a little ice age. What scientists are measuring and debating concerns not feet but inches, and fractions thereof, over many decades. This hardly seems the crisis scenario that Al Gore portrays.



Gore claims, “There is now a strong, new emerging consensus that global warming is indeed linked to a significant increase in both the duration and intensity of hurricanes.”



But hurricane specialists disagree. The November 10 _Science_ says, “The best theory and modeling still indicate the ocean temperature has only a minimal effect on storms.”



Exaggerated forecasts of disrupted ocean circulation, rapid sea‐​level rise, and more intense hurricanes make for splashy headlines, but sober science suggests that these scares _du jour_ may go the way of yesterday’s alarms over global cooling, the population bomb, and mineral‐​resource exhaustion.



Nonetheless, one part of these scare stories is genuinely frightening: the heavy‐​handed government intervention that advocates always look to as the source of salvation. Yesterday’s foes of the free market were socialists, communists, and Keynesians. Today’s are greens who want government engineering to “stabilize” the climate and ensure “sustainability.”



I will not be watching Al Gore’s quasi‐​sci‐​fi horror movie this Saturday night. I’ll probably be driving through neighborhoods of people I don’t even know, enjoying the gift of their holiday lights. And to them I say: Don’t fall for exaggerations. Enjoy your regularly scheduled activities, and keep the lights on.
"
"In a remote village in north Norfolk, nine-year-old Amelia Bradbury has been standing alone outside her school gates every Friday for months. Like hundreds of thousands of young people across the world, she is following Greta Thunberg’s lead and campaigning for action on the climate crisis – but, far from any of the big city demonstrations, she’s having to go it alone. “I was quite scared the first time because no one was doing it with me,” says Amelia. “But I’m doing this because I care about something. I really want people to listen to me and to make a difference.”  She holds a handmade sign reading: “I’m striking for our nature”, and it is her passion for wildlife and the outdoors that keeps her going each week. On the weekends she volunteers for Norfolk Wildlife Trust with her family and enjoys birdwatching. Nevertheless, there are times when striking alone can be difficult. “It is quite hard in the cold, especially when it’s freezing,” she says. A few of her friends at school are interested, but their parents are not so sure – with only one person, it is hard to get the ball rolling. Although there are young people from all walks of life striking alone, it’s often those in rural areas who struggle to make themselves, and the issues they most care about, heard. Holly Gillibrand, 14, in Fort William has been striking for more than a year: “The bigger towns and cities get all this media attention, obviously, because a lot of people turn up. “But I think the media tend to forget about the people in the rural places around Scotland and the rest of the UK. We have a different perspective on things and our voices deserve to be put out there just as much as anyone else’s.” But social media has provided a platform for rural voices to be amplified. In November Amelia’s father uploaded a video of her to Twitter after the prime minister, Boris Johnson, failed to show up to the climate leadership debate before the election. In it she said: “Tomorrow I’m going to be standing outside in the rain and you couldn’t be bothered to turn up in a warm studio to debate the other leaders. How pathetic are you?” It generated more than 1,000 retweets and praise from the wildlife presenter Chris Packham. “It was a bit crazy but I feel really proud because it shows that people notice and care,” Amelia says. It was the power of social media that inspired Anna Kernahan, 17, Grace Maddrell, 14, and Helen Jackson, 21, to set up Solo But Not Alone, a Twitter page dedicated to sharing the stories of solo climate strikers. “People will say: ‘Oh, you’re not alone,’ but it’s hard to see that when you are sitting there at the strike and there’s no one else around you, everyone’s walking past,” says Anna. She strikes alone in Belfast from 12pm to 3pm every Friday, often reading a book or catching up on homework. Although she struggles to get friends to join her, she has one powerful supporter to keep her going – Greta Thunberg. “My phone crashes whenever she retweets me because she gets so many likes,” says Anna. Within weeks of setting up Solo But Not Alone at the end of 2019, the trio had hundreds of followers, and have been able to profile solo strikers across the globe. It has helped them connect with people such as Mulindwa Moses, a 23-year-old climate activist from Uganda who strikes alone on the roadside. At one point he did it for 55 days consecutively, but now just strikes on Fridays and Saturdays, raising awareness for the Save Congo Rainforest and Two Trees a Week campaigns. Moses was inspired to take action after speaking to people who had lost family members in landslides and floods, which he later found were being caused by the climate crisis. “There are literally no reports about the climate and ecological crisis in the media, which has kept the population ignorant, and leaders are taking advantage of this to not take action,” Mulindwa says. Living in Kampala, Uganda’s capital, he strikes alone not because he lives in an isolated area, but because of his country’s lack of tolerance for climate activism. “Being a climate activist in Uganda is very hard,” Mulindwa says. “You cannot hold a strike with large numbers to create awareness because the government [does not] allow it, and I have lost friends, who say they can no longer associate with me because I stand on the side of roads holding signs and spend most of my time planting trees.” But like other solo climate strikers around the world, his loneliness is eased by the support he receives from fellow climate activists online. Anna says: “We really want to make sure that even if only one person is striking, their voice is heard and it is loud.”"
"
The idea with measuring climate accurately, is to get as far away as possible from human/urban influences so that those things don’t bias the readings of the thermometer. For example, on my way from Las Vegas to Reno this week, I passed through the near-ghost town of Mina, Nevada, which has a USHCN station. Mina is about as in the “middle of nowhere” as you can be. In fact, the view to the east of the Mina USHCN station is stunning for it’s remote beauty:

According to Wikipedia, Mina has quite a varied range of temperature:
Average July high temperatures range from 61° to 96 °F, with January averaging between 22° and 47 °F. The highest temperature ever recorded in Mina was 110 °F in 1933, with a low in 1990 of –23 °F. Mina receives very little rainfall, and in an average year gets about six inches, with no month getting more than one inch in a normal year. The Mina Airport is at the southeastern end of town.
The USHCN station is at the private residence of the airport operator, who also runs a KOA type trailer/RV park. The airport is a simple dirt strip, so no runway to generate extra heat. I’ve been all over the USA looking at the USHCN network. In almost every station I visit, there’s some sort of surprise. Mina was no exception, and I discovered what Stevenson Screens are really used for:
– as mounts for other weather stations.

In this case, an Oregon Scientifc WMR-968 wireless weather station, which is quite possibly the worst electronic weather station on the market. I once sold these at my online store weathershop.com, and stopped doing so when failure rates started approaching 30% out of the box.
Fortunately, the WMR-968 is not the “primary” instrument of the USHCN station, though it appears to be used as backup for the primary MMTS/Nimbus instrument. In this photo, you can see the wire from the small solar panel running inside to the temperature sensor.
What is most interesting about this station, is that while it truly is in the “middle of nowhere” and has that great “rural” view to the east, the primary MMTS sensor is just a few feet from where all the RV’s park while they register at the office:

Click for a large image
Unlike the Stevenson Screen, The MMTS is just a few feet from the office due to the famous cabling issue. It also has some nice sized rocks to act as heat sinks for those cold desert nights:

View looking North – click for a larger image
Besides the mixture of shade, rock heat sinks, road and building proximity, there’s also the requisite BBQ or two:

View looking south – click for a larger image
The rain gauge also has issues, due to the wind ducting that is likely created by these two trailers:


Click for a larger image
You can see the complete set of photos at the Mina gallery at surfacestations.org
As for the temperature trend:

Data from GISS – see original plot here
The trend is up, curiously, even though the town appears to be dying, so urbanization shouldn’t be the cause. According to NCDC’s MMS database, the station switched from using the Stevenson Screen to the MMTS electronic sensor in 1986. MMTS is well known for building proximity, That may account for some of the trend. There was also a station move to the present location in August 2007.
US 95 is about 100 yards away from the Mina USHCN temperature sensor, so perhaps there is an urbanization component in the form of more traffic.  I simply don’t know.  Interestingly, the station at Bishop, to the west, shows very little long term trend, while the station to the south, Tonopah, does show a trend. But Tonopah is growing, unlike Mina, which is dying and is now listed on a Ghost Town forum. Tonopah also had it’s weather station converted to ASOS, which when combined with other airport improvements, tends to add a positive trend.
So it’s a puzzle, and I welcome comments with ideas.
The thing about the Mina station though, is that without knowing a site history and history of the surrounding changes, we simply don’t know how much of the signal is real or from land use changes around the sensor. In it’s new position at an active RV park, it is now in a dynamic environment within feet of daily vehicular traffic. We simply should not have to figure such things out for a climatic reference station, even if it is in the “middle of nowhere”.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b9586de',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The hallmark of liberalism has long been its perceived commitment to individual rights. But not this administration. Bill Clinton is a new kind of Democrat–a jackboot liberal.



While the president has been out lobbying weather forecasters about the alleged threat of global warming, his interior secretary, Bruce Babbitt has been attacking energy companies for criticizing administration scare‐​mongering. Mr. Babbitt charged the firms with attempting “to distort the facts and to mislead,” adding: “I think that the energy companies need to be called to account, because what they are doing is un‐​American in the most basic sense.” He left unsaid how he would call such “un‐​American” businesses “to account,” but climate scientists report that the administration has long used its control of grants to punish researchers who question the climatic Chicken Littles.



Mr. Babbitt’s implicit threat unfortunately reflects the administration norm. In many cases, Clinton officials have directly targeted critics. More generally, warns Timothy Lynch, assistant director of the Cato Institute’s Center for Constitutional Studies: “Although President Clinton has expressed support for an ‘expansive’ view of the Constitution and the Bill of Rights, he has actually weakened a number of fundamental guarantees.”



The administration has politicized the FBI, using it to justify the White House Travel Office purge. Presidential aides snooped through FBI files on potential administration opponents. The IRS is auditing not only Paula Jones, who has accused Bill Clinton of sexual harassment, but a suspiciously large number of conservative foundations and groups. No liberal organizations are undergoing similar reviews. The White House pressured the Treasury Department over the latter’s probe of Madison Guaranty, which financed the Clintons’ Whitewater investment.



Early in the first Clinton term, the Department of Housing and Urban Development launched dozens of investigations of local activists who opposed federally subsidized housing projects. HUD subpoenaed copies of organization membership lists and financial information, people’s diaries, and other records, demanded cessation of public criticism, and threatened protestors with prosecution for speaking out.



Similarly, in 1995 the U.S. Commission on Civil Rights issued subpoenas to leaders of two anti‐​immigration groups. The commission, whose chairman and staff director were appointed by President Clinton, wanted computer printouts, internal documents, reports and other information. Both HUD and the commission retreated only under public pressure.



The Justice Department supported draconian restrictions on abortion protestors, including a prohibition on the display of any “images” that could be “observed” within abortion clinics. The Defense Department attempted to gag millitary chaplains, preventing them from discussing the Catholic Church’s Life Postcard Campaign regarding the president’s veto of legislation banning partial‐​birth abortion. More recently, the administration has threatened to prosecute any physician who provides a prescription for marijuana under state law.



Intimidation has been a persistent administration tactic elsewhere. In 1994, President Clinton expressed outrage that radio talk show host Rush Limbaugh could get on the air and “have three hours to say whatever he wants. And I won’t have an opportunity to respond.” White House Communications Director Mark Gearan called for radio talk shows to put on opposition–meaning administration guests. Senior adviser George Stephanopoulos suggested resurrecting the misnamed “Fairness Doctrine” to be enforced by the Federal Communications Commission, to regulate political broadcasts.



The Energy Department created a press rating system. Reporters and sources were judged based on their opinion of the department. Department press secretary Barbara Semedo explained that a low rating “meant we weren’t getting our message across, that we needed to work on this person a little.” Of course, getting the message meant spouting the department’s line.



Advertising, too, has been an administration target. The Food and Drug Administration even sought to prohibit the use of brand names on non‐​tobacco products (such as lighters and T‐​shirts) and the use of non‐​tobacco brand names on tobacco products. The administration supported labeling restrictions, deemed unconstitutional by the Supreme Court, on beer producers. The president backed FCC Chairman Reed Hundt’s campaign to bar the advertising of distilled spirits on television.



“The Clinton civil liberties record is breathtaking in both the breadth and the depth of its awfulness.”



The administration supported the Communications Decency Act, which would have attempted to ban the transmission of “indecent” materials over the Internet. Although well‐​intentioned, the law, voided by the Supreme Court, would have meant heavy‐​handed censorship of today’s least regulated communication medium.



Although President Clinton has spoken of reforming affirmative action, his administration promotes it with a mailed fist. Perhaps the ugliest episode was his Justice Department’s support (recently reversed) for the Piscataway, NJ., school district that fired a teacher because she was white. The Education Department responded to California’s passage of Proposition 209 by threatening to prosecute the university system for dismantling its racial spoils program.



Within the administration “diversity” has become a code word for preferential treatment. HUD requires that employees not only implement federal diversity policy, but demonstrate “interest” and “personal commitment” to diversity, be active in “minority, feminist or other cultural organizations ” and participate in “cultural diversity activities outside of HUD.” The Agriculture Department reassigned an employee for criticizing, on his own time, the department’s policy of offering spousal benefits to same‐​sex partners.



But the harshest examples of jackboot liberalism have come from the Justice Department and federal law enforcement agencies. The Branch Davidian and Randy Weaver cases continue to stand as examples of government run‐​amok, persecuting people who wanted little more than to be left alone. The administration’s response to the Oklahoma City bombing was to impose sweeping new powers, such as restricting the right of habeas corpus and expanding the use of wiretaps, for itself, even though the president was unable to point to a single example where civil liberties protections had hampered efforts to combat terrorism.



The administration, the most wiretap‐​friendly in U.S. history, has sought to eliminate Fourth Amendment protections against government searches. The president claims to possess “inherent authority to conduct warrantless searches for foreign intelligence purposes.” The administration requires public housing residents to sign away their constitutional rights. The Justice Department backed warrantless (indeed, suspicionless) drug tests for high school athletes. The administration has requested greater FBI authority to conduct “moving wiretaps” without a court order. President Clinton pushed the Communications Assistance Act, which requires telephone companies to retrofit their systems to ease police surveillance, supported restrictions on the sale of Internet encryption technology, and requested legislation forcing firms to give the government the “keys” to such technology.



No squishy, compassionate liberal he, the president has sought to thwart Arizona and California voters who approved measures to allow the desperately ill–victims of AIDS and cancer, in particular– from using marijuana to ease their nausea and pain. Mr. Clinton responded to criticism that sellers of crack were being punished far more severely than those who peddled cocaine by arguing that penalties against the latter–which already ensure that minor drug dealers spend more time in jail than do many armed robbers, rapists, and murderers–should be raised. (He recently suggested a mode move in the other direction, reducing the differential from a hundredfold to tenfold.)



The administration also throws people in jail for resisting federal designation of their (very dry) property as “wetlands,” and committing other environmental offenses. In 1994, the Justice Department relaxed its control of environmental prosecutions in order to allow individual U.S. Attorneys greater latitude in prosecuting business. Of course, the department still retains the right to proceed if a local U.S. attorney refuses to bring charges.



Any particular presidential decision can be defended on one ground or another, but as Wired magazine’s John Heilemann observes, the Clinton civil liberties record is “breathtaking in both the breadth and the depth of its awfulness.” And there’s more– proposed curfews for kids, support for random drug tests of welfare recipients and kids seeking drivers licenses, attacks on the requirement of a jury trial, ex post facto tax increases, attempts to gain court sanctions for uncompensated property takings, prosecution implicating the double‐​jeopardy clause, pretentious claims of federal criminal jurisdiction, and infringements of the Second Amendment right to possess a firearm. Mr. Lynch details these and more in his devastating study, “Dereliction of Duty: The Constitutional Record President Clinton.”



Administration spokesmen argue that the president is carefully balancing rights and liberties. It’s not balance President Clinton wants, however. It’s power. There was a time when Democrats were genuinely liberal. No longer. The American people are paying for Bill Clinton’s philosophy of jackboot liberalism with their freedom.
"
"
by John Goetz
In what seems to be a script straight from a Monty Python classic, the good folks of Santa Coloma de Gramenet in Spain seem to have found a rather novel use for the dead: as a tool in the fight against global warming.
From the TimesOnline
November 28, 2008
by Graham Keeley in Barcelona
Spanish graveyard new front in the fight against global  warming
Solar panels are  installed in cemetery

Solar panel in Santa Coloma
A graveyard in Spain has become an unlikely front in the fight against global  warming, with hundreds of black panels placed on top of mausoleums providing  year-round power for homes.
The 462 panels produce 124,374 kilowatts of electricity, enough to supply 60  homes for a year in Santa Coloma de Gramenet, near Barcelona. The exorbitant  price of land in the densely populated satellite city inspired a solar energy  company to propose using one of the last remaining available plots of land – the  cemetery.
Conste-Live Energy and the local council spent three years persuading  relatives of those interred and near-by residents that the unusual proposal  would benefit the living without demeaning the dead. “The best tribute we can  pay to our ancestors is to generate clean energy for new generations,” Esteve  Serret, a company director, said.
The panels cost €720,000 (£612,500) to install and each year will keep about  62 tonnes of carbon dioxide out of the atmosphere, Mr Serret said.
“This is not much, but it will do something to help combat global warming,”  said Bartomeu Muñoz, the Mayor of Santa Coloma. The glinting blue-grey panels  are fixed on top of mausoleums, which in Spain hold five layers of coffins.
The panels, which face south to soak up maximum sunshine, were turned on last  week after three years of planning. Santa Coloma is so densely populated that  all 124,000 inhabitants live within a 4sq km area. Putting solar panels on  coffins was a tough sell, said Antoni Fogué, a city councillor. “Let’s say we  heard things like, ‘They’re crazy. Who do they think they are? What a lack of  respect’,” he said.
City hall and cemetery officials waged a public awareness campaign to explain  the worthiness of the project and the painstaking care with which it would be  carried out. 
Eventually they won over doubters, Mr Fogué said. The panels were erected at  a low angle to be as unobtrusive as possible. “There has not been any problem  because people who go to the cemetery see nothing has changed,” Mr Fogué said.  “This installation is compatible with respect for the deceased and for the  families of the deceased.” 
The cemetery holds the remains of 57,000 people. The solar panels cover less  than 5 per cent of the total area. Community leaders hope to erect more panels  and triple output. Santa Coloma has four solar parks, but the cemetery is the  biggest and the first to attach panels to graves.
When I read this I suddenly recalled the infamous “Bring out yer dead” scene from Monty Python and the Holy Grail:
The Dead Collector: Bring out yer dead.
Large Man with Dead Body: Here’s one.
The Dead Collector: That’ll be ninepence.
The Dead Body That Claims It Isn’t: I’m not dead.
The Dead Collector: What?
Large Man with Dead Body: Nothing. There’s your ninepence.
The Dead Body That Claims It Isn’t: I’m not dead.
The Dead Collector: ‘Ere, he says he’s not dead.
Large Man with Dead Body: Yes he is.
The Dead Body That Claims It Isn’t: I’m not.
The Dead Collector: He isn’t.
Large Man with Dead Body: Well, he will be soon, he’s very ill.
The Dead Body That Claims It Isn’t: I’m getting better.
Large Man with Dead Body: No you’re not, you’ll be stone dead in a moment.
The Dead Collector: Well, I can’t take him like that. It’s against regulations.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a73ec94',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The life story of any animal involves daily struggles and triumphs, twists and turns – and each individual has its own unique narrative. The new Life Story series from the BBC Natural History Unit shows in intimate detail both familiar and rarely seen examples of such life-determining events. Breath-taking footage provides viewers with the highest quality images ever seen of animals experiencing the challenges involved in making their way through life towards the ultimate achievement, leaving offspring. That, as narrator David Attenborough puts it, is “the next best thing to immortality”. We were the Open University consultants to the series, which covers everything from bower birds and cheetahs to weaver ants and elephants. While it helped us to appreciate just how much television can now allow viewers to experience the daily lives and struggles of wild animals, we all still know very little about the basic biology and survival needs of many of the animals we share the planet with.  Often it is the most recognisable animals that people actually know the least about, which is where David and I come in. We’re educational experts, as well as biologists specialising in reproduction and how animals use sound.  African elephants feature in Life Story but there are now thought to be two, not one living species of these (the bush elephant and the forest elephant). The series follows a family group of bush elephants in south Kenya. It tells the story of a new mother who is supported by the older more experienced mothers, but has to keep up with the herd as they travel.  There is a fascinating moment where they pass elephant bones and take the time to pause, exploring them with their trunks. The fact that they stop despite their need to keep moving, probably for food or water, and that the bones are important enough to stop and explore provides an insight into the intimate behaviour of these creatures.  There is also a third species that does not appear in the series: the Asian elephant. Unlike African elephants, which are mostly wild, people have since the third millennium BCE attempted to “domesticate” these slightly smaller animals. But it has always been hard to get them to breed in captivity, which sets them precariously apart from truly domesticated species such as dogs, chickens, sheep and cows that generally reproduce so well that they outnumber their wild counterparts.  Elephants are large, long-lived, travel far – and females have strong social bonds. These complications help to explain why captive Asian elephants have never become truly domesticated and their populations are endangered. Research conducted back in 1998 confirmed that captive Asian elephants breed less successfully than those with access to wild populations in a comparison with semi-captive environments including a logging camp in Myanmar.  Since then, managers have been slightly more successful at breeding these elephants in captivity. But this may be down to developments in artificial insemination and sperm preservation rather than any increases in our knowledge about the complicated reproductive biology of elephants, their social behaviour or management needs. Elephants are known to communicate with each other through smell, sight and touch, but their ability to use sound in different ways is particularly interesting. They produce rumbling sounds through the folds in their larynx. These rumbles are structurally very varied, which means that all sorts of social information could be exchanged.  The sounds are very low-frequency and travel well through air. Elephants have been observed to respond to sounds from another elephant 2.5 km away. We know quite a lot about the characteristics of the sounds, but their precise role in the social structure of different elephant groups is still not well understood. This takes us back to Life Story and the elephants stopping to explore the remains of a fellow traveller: is this because of curiosity, or a deeper social meaning? The long gestation period of the programme (four years in the making) surpassed that of even the Asian elephant (up to 22 months). When you work with such animals, you get used to things taking a long time. And trainee researchers need to learn how to study for long periods to be able to extract new findings from observations. Students get a taste of this by studying elephant behaviours and interactions they are unlikely to have seen before, that take place during an intimate glimpse of how these magnificent animals go about their daily business within their own life stories for up to four hours on specially recorded footage from Woburn Safari Park in Bedfordshire.  Soon after the elephant study filming was completed, Damini, a 20-year-old female successfully produced a healthy female calf. This followed artificial insemination with sperm from Raja, the resident male, in another success at breeding Asian elephants in captivity. As in the new BBC series, another life story begins."
"

 **A Review of _Reality and Rhetoric: Studies in the Economics of Development_ , by P.T. Bauer, Cambridge, Mass.: Harvard University Press. 184 pp. $15.00**



Professor Peter T. Bauer, of the London School of Economics, is one of those intellectually heroic figures who has stood fast against the fads and hysteria of his time. While the vast currents of “development economics” inundated us with “overpopulation” theories and “vicious cycle of poverty” doctrines that depicted massive foreign aid as the only salvation of the Third World, Bauer said “No!” loud and clear—but virtually alone.



Despite his scholarly achievements and personal experience in the underdeveloped world, Bauer was long ignored or disparaged as he poked holes in the prevailing orthodoxy. Today, he can no longer be ignored‐​not even in academic and media circles where the prevailing orthodoxy was once treated as the one true faith. Bauer’s message has begun to be heard, not only because of his own perseverance and insights, but also because the repeated failures and massive disasters of “development planning” have finally broken through the smug unanimity that long substituted for evidence or critical analysis.



 _Reality and Rhetoric_ is a compilation of Bauer’s essays over the years on such topics as foreign aid, “planning” versus markets in the Third World, imperialism, and the moralistic pronouncements of the clergy on the international economic order. These essays are written and reasoned in a very straightforward way. It is enough to make you forget that he is an economist.



Bauer’s criticisms of current thinking about Third World nations are both wide and deep. He questions the very concept of “foreign aid” or “the third World.” Whether international transfers of money to the less‐​developed nations are an aid or a hindrance to their economic progress is for Bauer a question rather than a foregone conclusion. His own reading of the evidence is that it has hindered more than it has helped.



The tremendous range of extremely different nations lumped together as “the Third World” likewise makes no sense to him. All that these nations have in common is that they receive “foreign aid.” A few of these recipient nations have even had higher per capita income than some of the donor nations. Most—if not all—of the poorer nations have classes of people who are more affluent than the average Western taxpayer in the donor nations—and it is precisely those affluent people who have the inside track in getting their hands on the foreign aid.



Bauer is not a mere Scrooge who says “Bah! Humbug!” to the poor. On the contrary, his vision of the world accords far more respect to the less‐​developed regions and peoples than does the conventional viewpoint. Bauer denounces the “contempt for ordinary people” that underlies development planning. 



Drawing on his own many years of research and observation, he punctures the idea that Third World people can progress only under the tutelage of foreign experts or their own westernized elite. Evidence to the contrary, he notes, is found in “the large scale capital formation in agriculture by the local people” in West Africa; the fact that over half the acreage planted with rubber trees in southeast Asia was owned by Asians, even before World War II; and large‐​scale international migrations by poor and illiterate people who were nevertheless “well informed about economic conditions in distant and alien countries.” 



Dramatic economic changes over time likewise belie the stereotypical picture of hopelessly stagnant peasants needing foreign “experts” or … So much for the notion that Third World masses cannot think beyond today.



Bauer also recognizes “the reality and importance of group differences” within the population of a given nation, even though this subject “is virtually proscribed in the profession.” Particular segments of the population of very poor and backward nations often have people who are entrepreneurial, hard‐​working, thrifty, and with great initiative and imagination. Far from making use of such people for advancing the economic level of the country, many Third World governments devote great efforts to stifling or even expelling such groups, especially when they are racial or ethnic minorities whose prosperity is envied and resented by others. The Chinese in Southeast Asia, the Indians in East Africa, the Lebanese in West Africa, and the Jews historically in Eastern Europe are only some of the more‐​prominent examples of this very widespread phenomenon.



Early in his career, Bauer was struck by these inter‐​group differences, which were largely ignored by other development economics: “The differences in economics performance and hence in achievement among groups were immediately evident, indeed startling.” Unskilled plantation workers in Malaya, working with primitive implements, nevertheless differed in output by a ratio of two‐​to‐​one as between Chinese and Indian workers, though both were “undedicated coolies.” Differences in other occupations‐​especially entrepreneurial occupations‐​are even greater.



Contrary to the prevailing egalitarian ethic, Bauer declares that “differences in incomes and rates of progress and regions…are not reprehensible. They are inevitable.” Egalitarianism is to Bauer simply the “legitimization of envy.” He rejects “the notion that the well‐​off have prospered at the expense of the poor” and calls it “the most pernicious of all economic misconceptions.” Implicitly, it assumes a zero‐​sum world, in which A gains only at the expense of B, turns attention away from the central issue of how to increase total wealth. Throttling the production of wealth, in the name of equality, is not humanitarianism but moralistic self‐​indulgence. So is guilt. Bauer regards “guilt in the West toward the Third World” as “a feeling which does nothing to assist the ordinary people” of the poorer countries.



If your purpose is to understand economic development in the poorer nations, you cannot get a better brief introduction to the subject than in Reality and Rhetoric. If your purpose is to learn the latest fashions in theories and buzzwords, this is not the place. “Statement of the obvious,” Bauer says, “has become a major task,” in part because “prominent economists have perpetuated the grossest elementary transgressions of fat and logic.” Words like infrastructure and phrases like the vicious cycle of poverty have created reputations and programs, even as they have soared above reality and left disaster after disaster in their wake.



Bauer not only mentions some of these disasters but points out how “foreign aid” subsidizes them. The international aid organizations’ emphasis on “need” in general and short‐​run crisis management in particular means that poor nations that have behaved responsibly, and lived within their means, are far less likely to get money than governments that have spent lavishly, engaged in grandiose social and economic experiments, and run up huge foreign debt without any concern for how—or whether—they would pay it off. Bauer is not afraid to call this “preferential treatment of the incompetent, the improvident or dishonest.”



In its effects on national well‐​being, the difference between responsible and irresponsible government is seen by Bauer as far more important than the sums transferred by international aid organizations. Insofar as these transfers reward counterproductive government policies, the losses they engender may readily exceed any benefits they can purchase. The sums involved in these international transfers are often not very large relative to total national output but are very impressive as a percentage of government discretionary spending. Therefore their effect on government policy may be very large—and very counterproductive—while they directly add relatively little to the available resources of the economy.



In India, for example, foreign aid in 1980 amounted to less than 2 percent of gross national product (GNP), but it was 18 percent as large as the government’s total tax receipts. In Tanzania, foreign aid was 18 percent of GNP and slightly larger than all taxes received by the government.



In short, foreign aid greatly increases the recipient government’s economic leverage in the economy. In addition, international development agencies tend to be biased toward statist policies, both inherently and as a matter of choice. Inherently because it is, after all, governments that receive both financial resources and the advisory personnel provided by the international development agencies. Moreover, “many staff members of the international organizations favour dirigiste policies (state economic planning),” according to Bauer.



“The international aid organizations and their staffs are not disinterested,” Bauer points out, but instead have heavy personal and institutional stakes in a large and growing amount of foreign aid. These aid organizations are politically active and effective in the Western nations. Their version of the world economic picture is constantly fed through the media to the public as the only humane and decent way to see it. They have patronage to offer academics in the form of jobs and consulting arrangements. At the same time, these international bureaucratic empires are dependent on the Third World nations to accept their aid—and often express fears that the aid would be refused if various conditions were attached to ensure responsible behavior by the recipients.



The moral climate generated by Western intellectuals—including the media and the clergy‐​is one of the key ingredients in the political success of this process of draining money from Western taxpayers for the benefit of Third World ruling classes and international bureaucracies. Guilt is one of the factors in this moral climate.



The idea that the poverty of some nations is caused by the affluence of other nations is taken as axiomatic in many quarters. Bauer, however, treats this notion as a hypothesis instead of an axiom and looks at the evidence. He finds that in fact poverty and backwardness are greatest in those Third World nations that have been least touched by Western imperialism, trade, or multinational corporations‐​for example, Ethiopia and Liberia in Africa, and Bhutan, Sikkim, Tibet, and Nepal in Asia.



Far from deferring to the moral authority of politically active clergy, Bauer characterizes their arguments as “immoral because they are incompetent.” He says: “There is profound truth in Pascal’s maxim that working hard to think clearly is the beginning of moral conduct.” Bauer sees these activist clergy as “seeking a new role for themselves in the face of widespread erosion or even the collapse of traditional beliefs.” Their susceptibility to any idea that calls itself “social justice” he regards as symptomatic of a lost religious faith that finds a substitute in secular credulity.



Professor Bauer is no longer alone, though he is still vastly outnumbered by those with a vested interest in the foreign‐​aid status quo. This book will make it harder for them to continue to pull the wool over the eyes of the taxpaying public.
"
"Plans for a new £600m “floating” cycle route along the edge of the River Thames in London have been announced. It is expected to stretch for around seven miles from Battersea, west of the city centre, to Canary Wharf, the financial district to the east. The idea has been put forward by the River Cycleway consortium, a group of architects, engineers and artists. The construction is expected to rise and fall with the river tides and to have a number of access points along the route. The “motion” of the cycleway will be used to generate energy to power lighting. It is expected that at least some of the funding will come from private finance – a charge, expected to be £1.50 per trip, will be used to cover ongoing maintenance costs of the infrastructure. In a large, congested city, the Thames stands out as an under-used transport corridor and local authorities want to encourage cycling. Such an expensive and high-profile scheme would help give the idea that cycling is being treated seriously as a mode of travel. So, on the face of it, this sounds like a great idea.  The proposal, however, raises a number of issues and worries, which the plans as published do not seem to adequately address. There does not appear to have been any serious attempt to study demand – who would use it and what sorts of numbers might be expected on a daily basis? In practice it seems the main focus may be on leisure: tourists and Sunday trippers who wish to see the sights of London from an unusual perspective. Will it really appeal to the daily commuter cyclist, especially given the costs of use? There is a major question mark over the cost; £600m seems very expensive for just seven miles of cycleway. Road-based cycleways typically cost significantly less than this and arguably give far greater value for money.  London is already a growing cycling success story. The capital has more cyclists than ever and has received considerable investment in cycling infrastructure, a public bicycle scheme (the so-called “Boris Bikes”) and support for cycling goes right to the top of City Hall. Focusing investment once again on the capital isn’t really the best and most equitable use of such a considerable sum of money. Compare the £600m with the £77.2m invested by the government in eight cities outside of London (with a further local contribution of £45.4m) through the Cycle City Ambition grants.  The £600m might be better spent elsewhere in the country, or even perhaps in London’s suburbs, areas where fewer people cycle and where dedicated cycle infrastructure is poor or non-existent. In the UK, aside from a handful of urban areas which perform as well or better than London, much of the country has lower levels of cycling and could benefit from investment. In any case, is this the kind of infrastructure cyclists really want? Recent research has clearly shown that cyclists do not fit neatly into a single category and that their views and ideas for what works best for them cover a wide spectrum. A single, expensive and very geographically focused piece of infrastructure is unlikely to appeal to large numbers and, more importantly, is unlikely to be of practical use to many cyclists.  Perhaps the proposers should start to talk with cyclists and those who currently do not cycle (but might be persuaded to do so) to gain a better understanding of what they want. The answer is likely to be rather more mundane: better cycle paths, lighting, signposting and possibly further controls on aggressive driving.  This isn’t to argue against thinking big – major investments of this type could have a huge and positive impact for many people. But they would have to be focused on something that helps many people, not just the lucky few who fancy a scenic trip to Westminster."
"
Magicians and Illusionists Penn and Teller have a popular TV show on the Showtime channel called, ahem, “Bullshit”. In homage to their debunking mentor, James Randi, they take on a number of subjects they feel could use a little “clarity”.

Click image to watch the video
They recently (last Thursday night) took on Al Gore and carbon credits. The entire 30 minute show is available via the website VREEL (update You Tube has it now, VREEL started installing  Zango a couple of days ago – a spyware) 
See YouTube Part1 Part2 Part3

Warning: more than a few obscenities are uttered in the show, but mostly for comic effect.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9de644e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Ten years ago we witnessed one of the worst natural disasters in history, when a huge earthquake off the coast of Sumatra triggered a devastating tsunami which swept across the Indian Ocean.  An estimated 230,000 people lost their lives, and 1.6 million people lost their homes or livelihoods.  The impact was greatest in northern Sumatra because of its proximity to the earthquake. Catastrophic shaking was followed within minutes by the full force of the tsunami.  Thousands of people were also killed in distant countries, where the earthquake could not be felt. If they had received a warning of the approaching tsunami, they could have moved inland, uphill or out to sea, and survived. Tsunami take several hours to cross an ocean, becoming much larger and slower as they reach the coast. Back in 2004 there were long-established tsunami warning systems in the Pacific Ocean, which has many subduction zones – places where two tectonic plates collide – capable of generating huge earthquakes or volcanic eruptions.  Other regions, including the Indian Ocean, did not have a warning system. The probability of a major tsunami was judged to be too low to justify the cost, especially for poorer countries.  The Boxing Day 2004 disaster changed all that.   In early 2005, the UN agreed to develop an international warning system including regional systems in the Indian Ocean, North East Atlantic & Mediterranean, and Caribbean. The Indian Ocean tsunami warning system was developed between 2006 and 2013, at a total cost of at least $19 million.  In the three years prior to October 2014, bulletins were issued about 23 Indian Ocean earthquakes, resulting in a small number of potentially life-saving coastal evacuations. Most of these 23 earthquakes did not actually generate a threatening tsunami because they did not cause significant uplift or subsidence of the seafloor. But false alarms can provide reassurance that communications work well, or highlight weaknesses.  Communications and evacuation procedures are also regularly tested by international mock drills, often based on worst case scenarios.   All warning systems work in the same general way. First, a network of broadband seismometers detects the seismic waves generated by an earthquake, which travel at speeds of several kilometres per second. When several seismometers have detected the seismic waves, the location and approximate magnitude of the earthquake can be computed. If the epicentre is under water and the magnitude large (greater than 6.5 on the Richter, or moment magnitude, scale) a tsunami bulletin, watch or warning is issued to local communication centres, ideally within three minutes of the earthquake.  If the epicentre is nearby and the probability of a tsunami is high, evacuation procedures will be initiated immediately.  Otherwise, local centres will standby for confirmation of whether a tsunami has actually been generated. Confirmation comes within about 30-60 minutes, using a network of tsunami buoys and seafloor pressure recorders. These detect the series of waves (usually less than a couple of metres high and travelling at about 800 km/h) in the open ocean, and transmit the data by satellite to a regional control centre.  Tsunami warnings reach the public via TV, radio, email, text messages, sirens and loudspeakers. You can sign up to receive tsunami alerts anywhere in the world by SMS on your mobile phone, thanks to a not-for-profit humanitarian service called CWarn.org. Many high-risk areas also have signage to alert people to “natural” warnings (such as strong shaking or a sudden withdrawal of the sea), and direct them to higher ground. The Pacific and Japanese warning systems helped to ensure the major tsunami generated off the coast of Japan on 11 March 2011 caused far fewer deaths (15,000) than the 2004 disaster. However, it showed that even a wealthy and well-prepared nation such as Japan cannot fully protect people from extreme hazards, and that warning systems can sometimes lead to a false sense of security.  The slow rupture of the subduction zone near Japan meant the initial warnings underestimated the magnitude of the earthquake and resulting tsunami. Many people did not move to higher ground in the vital few minutes after receiving the warning, because they wrongly assumed the tsunami would be stopped by 5-10 m high sea walls.  Japan has learned from this tragedy and, among other things, made changes to tsunami warning messages, improved coastal defences, and installed more seismometers and tsunami buoys.  It is impossible to predict exactly when or where the next major tsunami will occur. They are very rare events in our limited historical record. But by dating prehistoric tsunami deposits, we can see that major tsunamis happen on average every few hundred years in many coastal regions.  Future tsunami disasters are inevitable, but with better technology, education and governance we can realistically hope that a loss of life on the scale of the 2004 tsunami disaster will not happen again."
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




Using a simple, publically-available, climate model emulator called MAGICC that was in part developed through support of the EPA, we ran the numbers as to how much future temperature rise would be averted by a complete adoption and adherence to the EPA’s new carbon dioxide restrictions*.   
  
The answer? Less than two one-hundredths of a degree Celsius by the year 2100.   
  
0.018°C to be exact.   
  
We’re not even sure how to put such a small number into practical terms, because, basically, the number is so small as to be undetectable.   
  
Which, no doubt, is why it’s not included in the EPA Fact Sheet.   
  
It is not too small, however, that it shouldn’t play a huge role in every and all discussions of the new regulations.   
  
*********   
  
* Details and Additional Information about our Calculation   
  
We have used the Model for the Assessment of Greenhouse-gas Induced Climate Change (MAGICC)—a simple climate model emulator that was, in part, developed through support of the EPA—to examine the climate impact of proposed regulations.   
  
MAGICC version 6 is available as an on-line tool.   
  
We analyzed the climate impact of the new EPA regulations by modifying future emissions scenarios that have been established by the United Nation’s Intergovernmental Panel on Climate Change (IPCC), to reflect the new EPA proposed emissions targets.   
  
Specifically, the three IPCC scenarios we examined were the Representative Concentration Pathways (RCPs) named RCP4.5, RCP 6.0 and RCP8.5. RCP4.5 is a low-end emissions pathway, RCP6.0 is more middle of the road, and RCP8.5 is a high-end pathway.   
  
The emissions prescriptions in the RCPs are not broken down on a country by country basis, but rather are defined for country groupings. The U.S. is included in the OECD90 group.   
  
To establish the U.S. emissions pathway within each RPC, we made the following assumptions:   
  
1) U.S. carbon dioxide emissions make up 50 percent of the OECD90 carbon dioxide emissions.   
  
2) Carbon dioxide emissions from electrical power production make up 40 percent of the total U.S. carbon dioxide emissions.   
  
Figure 1 shows the carbon dioxide emissions pathways of the original RCPs along with our determination within each of the contribution from U.S. electricity production.   








_Figure 1. Carbon dioxide emissions pathways defined in, or derived from, the original set of Representative Concentration pathways (RCPs), for the global total carbon dioxide emissions as well as for the carbon dioxide emissions attributable to U.S. electricity production._



As you can pretty quickly tell, the projected contribution of U.S. carbon dioxide emissions from electricity production to the total global carbon dioxide emissions is vanishingly small.   
  
The new EPA regulations apply to the lower three lines in Figure 1.   
  
To examine the impact of the EPA proposal, we replace the emissions attributable to U.S. power plants in the original RCPs with targets defined in the new EPA regulations. We determined those targets to be (according to the EPA’s Regulatory Impacts Analysis accompanying the regulation), 0.4864 GtC in 2020 and 0.4653 GtC in 2030. Thereafter, the U.S. power plant emissions were held constant at the 2030 levels until they fell below those levels in the original RCP prescriptions (specifically, that occurred in 2060 in RPC4.5, 2100 in RCP6.0, and sometime after 2150 in RCP8.5).   
  
We then used MAGICC to calculate the rise in global temperature projected to occur between now and the year 2100 when with the original RCPs as well as with the RCPs modified to reflect the EPA proposed regulations (we used the MAGICC default value for the earth’s equilibrium climate sensitivity (3.0°C)).   
  
The output from the six MAGICC runs is depicted as Figure 2.   




_  


![Media Name: gsr_061114_fig2.jpg](/sites/cato.org/files/styles/pubs/public/wp-content/uploads/gsr_061114_fig2.jpg?itok=R2UJJXni)

_





_Figure 2. Global average surface temperature anomalies, 2000-2100, as projected by MAGICC run with the original RCPs as well as with the set of RCPs modified to reflect the EPA 30% emissions reductions from U.S power plants._



In case you can’t tell the impact by looking at Figure 2 (since the lines are basically on top of one another), we’ve summarized the numbers in Table 1.   






  
  
In Table 2, we quantify the amount of projected temperature rise that is averted by the new EPA regulations.   






  
  
The rise in projected future temperature rise that is averted by the proposed EPA restrictions of carbon dioxide emissions from existing power plants is less than 0.02°C between now and the end of the century assuming the IPCC’s middle-of-the-road future emissions scenario.   
  
While the proposed EPA plan seeks only to reduce carbon dioxide emissions, in practice, the goal is to reduce the burning of coal. Reducing the burning of coal will have co-impacts such as reducing other climatically active trace gases and particulate matter (or its precursors). We did not model the effects of changes in these co-species as sensitivity tests using MAGICC indicate the collective changes in these co-emissions are quite small and largely cancel each other out.


"
"
Share this...FacebookTwitterBy guest writer
Ed Caryl
Arctic stations near heat sources show warming over the last century. Arctic stations that are isolated from manmade heat sources show no warming. The plots of “isolated stations” and “urban stations” below clearly illustrate the differences. 
Stevenson Screen, Verhojansk, Russia
All the GISS temperature anomaly maps show the Arctic warming faster than the rest of the globe, especially northern Alaska and Siberia, but the satellite data shows a different pattern. See the 2 charts for 2009 that follow. The GISS surface map:


Satellite chart:

The baseline period selected for the GISS surface temperature chart is the 1933 to 1963 Atlantic Multi-decadal Oscillation (AMO) warm period. This period more closely matches today’s temperatures than the default 1951 to 1980 cool period that GISS uses. The satellite data uses the average over the satellite period since 1979, the modern warm period.
The satellite data show cooling in central Siberia, similar to the surface anomaly map, and very little warming for most of Alaska. It also shows cooling for the Antarctic Peninsula, where the surface map shows warming. But there is a scattering of hot grid squares across the HISS surface station map for the Arctic. So what is going on?
I selected the stations that correspond to those warm grid squares, as well as other stations in the same latitudes. In this age of everyone carrying a camera posting all photos on the Internet, there is a lot of information available on these stations. For some I could locate the Stevenson screens, for most I’ve found pictures of the surroundings, while others have investigated many of these sites already, and so links to that research are included. I downloaded the raw temperature data from GISS for 24 stations closest to the North Pole, which are all classified as “rural”.
“Urban” Arctic Stations
Contrary to GISS claims, many of these stations are actually not “rural” with respect to their siting quality. Many are at airports associated with sizable towns or research stations with sizable staff and infrastructure. In the Arctic, any town of more than a few families can be a large heat source. In the case of many towns in Russian Siberia, “central heating” takes on a whole new meaning. These towns have a central power plant that provides electricity and steam heat to the whole town. Large pipes, both insulated and un-insulated, carry steam, water, and sewage, up and down the streets to and from each dwelling. These pipes cannot be buried because of the permafrost, so they are elevated, and at street crossings are elevated 4 or 5 meters. The temperature differential between these pipes and the surrounding air can be 140° C in winter, and even more for a pressurized system.
But GISS applies the same Urban Heat Island (UHI) criteria to all stations globally, regardless of the latitude or average temperature. They look at the satellite night brightness and population to judge whether urban or rural. By GISS criteria, all the stations in the high Arctic are rural; there are no corrections for UHI.
But let’s look at each of these “urban” locations. Each name is also a link to the GISS surface temperature raw data.
List of Urban Arctic Stations (see the annex at the end of this post for details on each station)
  1. Kotzebue, Ral (66.9 N,162.6 W), Alaska
  2. Barrow/W. Pos (71.3 N,156.8 W) Alaska
  3. Inuvik (68.3N, 133.5W) Inuvik, Canada
  4. Cambridge Bay (69.1 N,105.1 W) Nunavut, Canada
  5. Eureka, N.W.T. (80.0 N,85.9 W), Canada
  6. Nord Ads (81.6 N,16.7 W Northeast Greenland
  7. Svalbard Luft (78.2 N,15.5 E), Norway
  8. Isfjord Radio (78.1 N,13.6 E), Norway
  9. Gmo Im.E.T.(80.6 N,58.0 E), Russia
10. Olenek (68.5 N,112.4 E), Russia
11. Verhojansk (67.5 N,133.4 E), Russia
12. Cokurdah (70.6 N,147.9 E), Russia
13. Zyrjanka (65.7 N, 150.9 E), Russia
14. Mys Smidta (68.9 N,179.4 W), Russia
15. Mys Uelen (66.2 N,169.8 W), Russia
The following graphic is a temperature chart for 10 of the above stations (5 of the shorter ones were left out to avoid over-crowding). All are warming, some faster than others. Barrow, for which we have the UHI study, is not the fastest warming.
Temperature trends of the ""urban"" stations.
Isolated Stations
Now let us look at the isolated stations, which are located at similar latitudes like the above “urban” stations. One important thing to note about these isolated stations – there is limited electrical power, and so incandescent light bulbs in the Stevenson screens is unlikely. Detailed descriptions of these stations are listed in the annex at the end of this report.
16. Alert,N.W.T.(82.5 N,62.3 W), Canada
17. Resolute,N.W. (74.7 N,95.0 W), Canada
18. Jan Mayen (70.9 N,8.7 W), Norway
19. Gmo Im.E. K. F (77.7 N, 104.3 E), Tamyr Peninsula, Russia
20. Ostrov Dikson (73.5 N,80.4 E, Russia
21. Ostrov Kotel’ (76.0 N,137.9 E), Russia
22. Mys Salaurova (73.2 N,143.2 E), Russia
23. Ostrov Chetyr (70.6 N,162.5 E), Russia
24. Ostrov Vrange (71.0 N,178.5 W) , Russia
Now here is the chart of the temperatures of these isolated stations, not subjected to manmade structures or heat sources.
Isolated Stations
Note that most of the trends are flat or decreasing. Only Resolute and Ostrov Vrange are increasing slightly. Both of those might be slightly influenced by UHI. The longest records clearly show warming in the late 1930’s and 40’s, and cooling in the 1960’s, and none show a hockey stick. The GISS data for Alert ends in 1991, though the weather station is still there, and still reporting. Data for Mys Salaurova and Ostrov Chetyr also ends at about that time, probably due to the fall of the Soviet Union.
Here is an average of all the isolated stations:
Isolated Stations - Average
Note that the peak-to-peak trend is nearly zero. The linear trend is about 0.4°C/century, but the R2 value (the statistical significance for the trend) is very low, 0.023.
 Here is a plot of the AMO versus the average temperature of the isolated stations.

The temperature as measured at stations isolated from any UHI is simply tracking the AMO. 
Looks like an awfully good fit. There is very little, if any, global warming. We need to wait until the bottom of the next AMO cycle to get a decent reading of global temperature change. That will be in about 2050 if the AMO cycles as it has since 1850.
———————————————————————————————-
Annex – station descriptions
The “urban” stations, nos. 1-15
1. Kotzebue, Ral (66.9 N,162.6 W), 
2. Barrow/W. Pos (71.3 N,156.8 W)
These towns are of similar size, and are growing at the same rate. In 1940, both towns had a population of 400. In 1980 both had just over 2000 population, and now they both have over 3000 people. Both have airports of sufficient size to handle multi-engine turboprop and small jet aircraft, and both are served daily by regional airlines. Kotzebue is on a peninsula and the airport is across the middle of the peninsula, somewhat restricting the growth of the town. Barrow has somewhat the same problem due to a series of small ponds around the town and the airport. Barrow was studied for UHI effects in 2003. That paper was in the International Journal of Climatology here. That paper describes the UHI average temperature increase in winter as 2.2°C compared to the surrounding hinterland. GISS data indicates that Barrow average temperature has increased over the years as population has increased. (See below, or click on link above.)
Barrow, AK


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Source: http://en.wikipedia.org/wiki/File:BRW-g.jpg
Source: http://en.wikipedia.org/wiki/File:OTZ-g.jpg
The Barrow NWS station (Stevenson Screen) is here. On the airport picture, it is at the base of the rotating beacon tower. Kotzebue NWS station is not visible in published pictures.
3. Inuvik (68.3N, 133.5W)
Inuvik is a relatively new town, begun in 1954. The population as of 2006 has grown to about 3500 people. Because it is a “planned” community in the arctic, built on permafrost, the water and sewage infrastructure is above ground in heated and insulated “utilidors”, like the heating systems in Siberia. The weather station, from weather reports, Google Earth and Google Street View, appears to be at the airport, in a compound just north of the entrance.
4. Cambridge Bay (69.1 N,105.1 W), Cambridge Bay, Nunavut, Canada
There is a Wikipedia picture of Cambridge Bay here. The population has grown from just a few people in the 1940’s to about 1500 today. It also has an airport with daily regional airline service.
5. Eureka, N.W.T. (80.0 N,85.9 W), Eureka, N. W. T., Canada
There are the only four stations at or north of 80° latitude, Eureka, Alert, Nord and Krenkle (Gmo. I.M.ET). Only Eureka has an unbroken temperature record to the present date, and it begins in 1947. The population at Eureka has
Eureka station
never been high. In winter it has always been 4 or 5 men. In summer, the population increases to as high as 20. The station infrastructure though, has expanded through the years. Each year, some of those 20 workers add or expand buildings. In the beginning, it was one or two buildings, with water and sewage handled in tanks and barrels internal to the buildings. The Stevenson screen was originally placed where the blue New Main Complex building is now. When that was built, the Meteorological instruments were moved to the current location. Over time, the water supply, plumbing, and sewage treatment was upgraded and the outfall pipe installed. It, of course, must be heated to facilitate flow to the sewage lagoon. All the water pipes exposed to the outdoors must be heated to prevent freezing.
Image from a recent article by Anthony Watts on WUWT here.
6. Nord Ads (81.6 N,16.7 W, Northeast Greenland
Nord Station
Nord is the furthest north inhabited place on earth, on the Northeast coast of Greenland. It was built in the period from 1952 to 1956 as an emergency airfield for aircraft operating out of Thule. Access is impossible by sea because the sea ice never moves away from the coast there. Legend has it that “Blowtorch” Murphy, a mythic arctic construction worker, scraped the first runway, using a parachute dropped caterpillar tractor after he himself parachuted onto the site. His nickname came from his habit of wearing a lit blowtorch hanging from his waistband on a wire; a lit blowtorch being somewhat useful when working outside when it’s 40° below zero.
There are about 40 buildings at Nord. Not all of them are continuously heated, but those near the Stevenson Screen are. The winter population is 5 or 6 men. More pictures here.
7. Svalbard Luft (78.2 N,15.5 E)
8. Isfjord Radio (78.1 N,13.6 E)
These two stations are only 47 kilometers apart. But data for both is fragmentary for 1976 and 1977, and there is no overlap. Svalbard Luft (airport) has been discussed on WUWT here and here, so I won’t cover it in detail here. Warwick Hughes has an article on Isfjord Radio here that makes the case for warming of Isfjord Radio due to moving of sea ice away from the islands in summer since 1912. Neither station in Svalbard shows on the anomaly map because there was no common station in both the base period and the anomaly period. Here’s a map with 1998 to 2008 as the base period where Svalbard appears.
9. Gmo Im.E.T.(80.6 N,58.0 E)
This is the Krenkel meteorological station on Hayes Island, or Ostrov Kheysa in Russian, in the Franz Josef Land Archipelago, Russia. Link The station has been moved or re-built twice since it was established. It was moved from Hooker Island (article in German) in 1957/58. A fire destroyed the power station in 2000, and it was rebuilt in 2004 closer to the shoreline. The GISS record is from 1958 with a gap from 2001 to 2009. The population was as high as 200 during Soviet times, but is down to 4 or 5 now. The population and the temperature seem to track roughly during Soviet days, and the move in 2004 was to a warmer location. In the picture you can see the old buildings on the ridge in the distance. The red grid-square on the anomaly map above corresponds to this station.
Source: http://www.sevmeteo.ru/foto/15/88.shtml
10. Olenek (68.5 N,112.4 E)
This is the town of Ust’-Olenek, Russia.
Photo sources here and here.
The town doesn’t look like much, but notice the Tundra Buggies parked next to the Stevenson Screens. It is on the Laptev Sea, on the northern Siberia Coast, but on a peninsula on a south-facing beach. The buildings are right on the shore. The wide view above was taken from out on the ice. This is one of the few places in Russia that the Google Earth satellite view actually has enough resolution to see the Stevenson Screens. They are much too close to the heated building.
11. Verhojansk (67.5 N,133.4 E)
This is one of the “centrally heated” towns in Russian Siberia. The picture at the top of this article is of the Stevenson Screen. Verhojansk is called the “cold pole” of the earth, but the measurements are too warm by far. Look closely at the picture. Any photographer will note that the warm glow inside the Stevenson Screen is just the color temperature of an incandescent light bulb. If the steam heat in the town isn’t enough, or the cattle in the pole-barns in the distance, the heat from the light bulb will warm up the measurements. This site was covered on WUWT here and here. Anthony Watts notes that warm anomalies would appear and disappear in this part of Russia “as if a switch were thrown”. Could it be as simple as the switch on that light bulb?
12. Cokurdah (70.6 N,147.9 E)
Also spelled Chokurdakh. The population has been dropping in recent years, but was still over 2500 people in 2002. The town is sandwiched between the Indigirka River and the airport. There is no way to tell where the Stevenson Screen is located, but the infrastructure at the airport blends right into the town. See an aerial photo here.
13. Zyrjanka (65.7 N, 150.9 E) Also spelled Zyryanka, another steam-heated town in eastern Siberia, well inland. The airport is in this picture on the north edge of town, along the Kolyma riverbank. This airfield was built during WWII as a stop for aircraft being ferried to the Soviet Union from Alaska. A second airport 7 miles west of town was probably built during the cold war for the military. The town was established in 1931. The population is currently about 3500. During the Soviet Union it was up to 15,000.
14. Mys Smidta (68.9 N,179.4 W)
Or Cape Schmidt.  John Daly wrote a bit about this location in 2000 (scroll way down in the article). The population was nearly 5000 in 1989, but has dropped since the fall of the Soviet Union. The population now is probably less than 1000. It is on the north coast of eastern Siberia, nearly at 180° longitude. The airbase there was built in 1954 as a staging base for any bombers headed for the U. S. It is still used by a regional airline.
15. Mys Uelen (66.2 N,169.8 W)
Or Cape Uelen. This is on the easternmost tip of Siberia, across Bering Strait from Kotzebue, Alaska. The current population is about 700 people. It is also centrally steam heated. The town is restricted by the geography, on a narrow spit sticking out into the sea, backed by a cliff on the landward side. The airport is a helipad. Cargo and fuel arrives by barge in the summer.
Below is a temperature chart for many of the above stations. All are warming, some faster than others. Barrow, for which we have the UHI study, is not the fastest warming.
16. Alert,N.W.T.(82.5 N,62.3 W), Alert, Canada
Alert, Canada has had a weather station since 1951. The population has never been more than 4 or 5 in the winter, with a higher population in the summer. I could not definitively locate the Stevenson screen, but there are two possibilities in this photo, both well away from the buildings.
THE ISOLATED STATIONS, NOS. 16-24
17. Resolute,N.W. (74.7 N,95.0 W)
The population of this Canadian station rose from zero prior to 1947, to 229 in 2006. There is an airport here, and the Stevenson Screen can be seen across the aircraft parking area from the airport terminal at the left edge of the photo.
18. Jan Mayen (70.9 N,8.7 W)
Pictures of the station are here, and a web site is here. The 18 people on the island live at Olonkinbyen, or Olonkin “City”. The meteorological station is 2.6 km away. The 4 people that work there live in Olonkin City. The Stevenson Screen appears to be well away from the station building, and the surroundings have probably not changed since the station was built.
19. Gmo Im.E. K. F (77.7 N, 104.3 E)
This is a Russian station on the Tamyr Peninsula at Cape Chelyuskin (Mys Chelyuskin). Nothing is visible at that location on the Google satellite view, but the resolution is very low. I found an article by Warwick Hughes dated September 2000 that speaks of cooling of the Tamyr Peninsula here. He also talks about “non-climate” warming of Verhojansk and Olenek.
20. Ostrov Dikson (73.5 N,80.4 E
Dikson, Russia airfield
This is Dickson Island in English. There is a town of Dikson 10 kilometers away on the mainland. The airport is on Dikson Island at the point called Ostrov Dikson on the map below. Pictures of the airport can be seen here. The town is pictured on this 1965 stamp.
Wikipedia link
21. Ostrov Kotel’ (76.0 N,137.9 E)
The full name is Ostrov Kotel’nyy. In English this is Kettle Island. The first documented explorer found a copper kettle, so obviously he was not the first person to find the island. A single building is barely visible on Google 3D mapsat the “settlement” known as Kalinina.  This may be the meteorological station. No other signs of civilization can be seen on the whole island.
22. Mys Salaurova (73.2 N,143.2 E)
This also spelled Mys Shalaurova. The station is on the south-facing shore of an island and is visible on Google Earth here. There is a tide gauge, and the tide data is on that same page.
23. Ostrov Chetyr (70.6 N,162.5 E)
The full name is Ostrov Chetyrekhstolbovoy. This is a small island in the East Siberian Sea in the Medvezhy Island (Bear Island) group.
Map source here.
A description of the place is found: here. “A polar meteorological station and a radio station are situated on the shore of a small bay which indents the S side of the island.”
24. Ostrov Vrange (71.0 N,178.5 W) 
This is otherwise known as Wrangle Island. It is about 125 kilometers off the Siberian coast on the 180th meridian. The weather station is at Ushakovskiy on a spit at Rogers Bay, at the right in this picture, well separated from the village. One building in the village is visible at the left. Link
The population in the village grew to as many as 180 people in the 1980’s, but when the Soviet Union dissolved, subsidies declined and the population moved to the mainland. The last villager was killed by a polar bear in 2003. The population at the weather station, when occupied, has always been 4 or 5.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEd Caryl has become a regular contributor here, and today he presents insights on the causes of glacial melt. Here he discusses how absorption of solar energy by soot and Black Carbon contribute significantly to glacial melting and that CO2 is a minor factor.
 
Glaciers – The Dark Side. It’s Not the CO2 Carbon

by Ed Caryl
The global warming “hockey stick”, invented by Dr. Michael Mann, has been proven to be a distortion. [i]  But if carbon dioxide is not significantly warming the planet, then why are most northern glaciers shrinking?
Since the end of the last ice age 12,000 years ago, glaciers have been receding, dramatically in the first few thousand years of warming when the oceans rose by 120 meters. The remaining glaciers have been receding since the end of the “Little Ice Age” in the early 1800s. This is normal. Compared to an ice age, it is warm.
There is evidence that this retreat has stopped and even slightly reversed in the last ten years for some glaciers; those on Mount Shasta in California are examples. These have increased in mass because of greater snowfall. Glaciers in Alaska, California, Europe, and South Greenland are still receding. Some of the melt of South Greenland is because of the Atlantic Ocean.[ii]
The following temperature plots are of the sea off the west coast of Greenland. For a full resolution, better quality graphic go to the link. The years shown are 1992 to 1999.

and 2000 – 2007:As the above chart shows, the Atlantic Ocean off southern Greenland began warming in the early 1990’s and is only now beginning to cool. This local warming is due to the Atlantic Multi-Decadal Oscillation (AMO), a natural Atlantic cycle with a period of about 70 years described here. But not all of the Greenland melting is due to the warm Atlantic.
Soot and Black Carbon
Glaciers are melting in the Alps, Alaska, Canada, most of the Sierra Nevada in California, and the Himalayas because of the other carbon emission: soot. This is known in the literature as Black Carbon. You can even see the geographic source in the map below, south Asia, China, and Russia. The emission sources are coal burning, bio-fuel (including dung), diesel engines, fuel-wood smoke, forest fires, and other incomplete combustion processes that take place in highly populated areas.
The result is clearly visible in nearly any photo of a melting ice field or glacier. Soot is visible also on Greenland glaciers. See here for a photo from National Geographic (also shown to the left). Note the black stains in the ice field; that is Black Carbon. The soot is deposited on the snow in the winter and spring. As the snow melts, it gets concentrated on the surface as the melt water drains away between the ice crystals. When the melt gets down to smooth ice, the soot concentrates in cracks. The sunlight heats the cracks, widening them.
Clean snow melts slowly because 99 percent of the sunlight is reflected away. Dirty snow or ice melts quite quickly because much more of the sunlight is absorbed as heat by the soot. 10 parts per billion of soot in freshly fallen snow is enough to significantly enhance melting.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Soot on glacial tongues in Northern Bhutan. This NASA photo shown below is from here.
The problem is very acute in places like the Swiss Alps, which are surrounded by industrial nations burning a lot of diesel fuel. Those glaciers are disappearing rapidly. The Alaskan and Canadian glaciers are receding because of soot from China. The problem has been recognized in the Himalayas. There is substantial disagreement (a factor of 200 on how much melt forcing takes place), though, on the extent of the problem. A quote from this document:
Black carbon on snow during spring melt in the Tibetan Plateau, for example, creates forcing rates 200 times higher than was assumed for black carbon on snow in the IPCC Fourth Assessment Report.
Here is a map of worldwide black carbon optical thickness from above, and concentration on the surface:

Source: NASA, Dorothy Koch and James Hansen here.
Note how much carbon is in the high Arctic, compared to that at the equator and further south. NASA admits that soot is part of the melting glacier problem, but downplays its importance. They hedge their bets on the subject.[iii] But some of the analysis views soot as an aerosol, and some of it as soot in freshly fallen snow. Only passing mention is made of concentrated soot resulting from melting.
A recent article states that half of the Arctic warming since 1890 may be due to Black Carbon. If that is true, perhaps some of the world’s warming in the last century is due to black soot, and not CO2. Read here.
Black Carbon is much easier to curb than CO2. The European Union has already put severe restrictions on Black Carbon emissions from diesel engines. In the U.S., the EPA has done the same. The time frame of effectiveness is also much shorter for Black Carbon. Eliminate a source, and the Black Carbon from that source is washed out of the atmosphere in days or just a few weeks. On a glacier, the problem will be much reduced in one snow season.
The problem is that there are many sources, all over the populated world. In Asia, cooking fires are a major source, so supplying and improving cook-stoves should be a priority. Low quality cooking fuel, such as animal dung, should be discouraged. In China, coal-fired power plants produce most of their electricity, and are planned to produce more in the future. China must insure that these plants use the very latest in technology to prevent Black Carbon emissions. In Africa and South America, forest clearing fires are a major source, so preventing rain-forest destruction should be a double priority.
For even more information on Black Carbon, see here, and here.

[i] For more on the hockey stick: http://www.john-daly.com/hockey/hockey.htm, The Hockey Stick Illusion by A. W. Montford, here, and many others.
[ii] South West Greenland Ocean Temperature. (2009). In UNEP/GRID-Arendal Maps and Graphics Library. Retrieved 19:53, January 3, 2010 from http://maps.grida.no/go/graphic/south-west-greenland-ocean-temperature.
[iii] See: http://earthobservatory.nasa.gov/IOTD/view.php?id=4082
http://www.nasa.gov/centers/goddard/news/topstory/2003/1223blacksoot.html
http://www.nasa.gov/centers/goddard/news/topstory/2003/0509pollution.html
http://www.nasa.gov/vision/earth/environment/arctic_soot.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn my last post I wrote about China’s aggressive, yet legitimate, expansion of its energy supply. I wonder what the western climate hand-wringers think about the following graphic? Will they ask China about this in Cancun?
My bet is that they’re going to ignore it and focus instead on USA’s and Europe’s emissions.
What I find particularly entertaining is that even if the USA did cut its CO2 emissions by 2o% by 2030 (to about 4.7 million tons annually), Chinese growth would wipe out the savings in just 1o years (2020) or less.
Germany wants to put its entire country through economic hardship to reduce its CO2 emissions by 30% by 2020. That’s a drop of about 250 million tonnes. China’s growth would wipe out the alleged benefit of that in just a matter of months.
And if small countries like Canada, Australia,  Denmark or New Zealand all make their cuts, China’s emissions growth will offset their reductions in a matter of days or hours.
Share this...FacebookTwitter "
"At Christmas, size is everything: so says an online “oven selector guide”. And it is true, ovens are designed and optimised for roasting large birds. As a result, they are typically oversized for regular use – making their total energy consumption greater than necessary. It is not only ovens that are designed to cope with the special demands of the festive season. Christmas is a moment of peak load within the kitchen, for parcel delivery services, and for road, rail and communication networks. Looking at this period helps us understand the relation between capacity, flexibility and energy demand. Let us start with the oven. For many ovens, December 25 is the day of peak demand – in the UK, something like 10m turkeys are roasted at roughly the same time. The key features of this event – many people and a turkey large enough to feed them all – are inscribed in the details of oven design. There are similarities between a Christmas dinner and a traditional Sunday roast, and it is perhaps not surprising that the European standard test of oven efficiency, the “wet brick test”, is designed to simulate roasting a chicken.   No one knows much about how ovens are actually used, but we do know that the traditional Sunday dinner is in decline and that oven-baked pizzas are increasingly common. It would therefore be possible, and more efficient, to produce ovens optimised for normal use rather than special occasions.  The problem is oven sizes are fixed: having spare capacity through the year avoids the need for special measures, like renting an extra oven for the day – a strategy adopted by one of our great grandmothers in the 1930s.  While there are year round implications for energy demand, oven design is inseparable from the social conventions of Christmas. If turkey was not the classic meal and if roasting was not the norm – a large stew produced on the hob could feed as many people – ovens would not be sized and designed as they are today. This is not the only situation in which year round capacity reflects anticipated peak demand and therefore leaves a surplus for much of the time. Cars, for instance, are generally able to carry four or more passengers plus all their luggage, yet on average they are occupied by just 1.6 people.   However, capacity is not always fixed in quite this way.  Consider systems for delivering parcels ordered online. In this case the Christmas peak reflects the need to dispatch a very large number of items and ensure they arrive before the critical day – in short, it is the need for simultaneity, not size as such, that matters. While there are physical limits in terms of capacity – the volume of warehousing and lorry space is fixed, much like the size of the oven – the difference is that there is scope to work harder: to hire seasonal staff, and to push more items through the system.  Shipping and logistics businesses are simply not optimised for Christmas peaks, and no one would expect this either. In this example measures adopted to cope with extreme pressure are layered on top of a system that is in fact designed for regular loads, unlike the oven which is sized for peak demand. Christmas peaks provide other insights into the patterning of social life and the energy demands that follow. In the UK there used to be a detectable spike in electricity demand associated with the Queen’s speech. At 3pm, other activities paused and televisions were switched on, closely preceded and followed by millions of kettles.  This is no longer so. Instead, the anticipated afternoon surge, once the turkey has been eaten, is in the flow of data. An estimated 136,000 extra gigabytes of data will be used on Christmas Day alone.  These details provide a glimpse of wider, year round, trends in forms of information, communication and entertainment. As ways of spending time evolve – even on Christmas day – new peaks and surges occur. Ironically, the extreme circumstances and the unusual synchronisation of Christmas reveal things about “normal” energy demand. Appliances and systems are sized to cater for what are essentially social loads like those of eating Christmas dinner together.    As Christmas also demonstrates, there are different types of peak and different types of response, such as maintaining idle standby capacity at all times (exemplified by turkey-sized oven), or designing and optimising for base load and augmenting that at critical moments (hiring seasonal staff).    Perhaps most important of all, Christmas peaks in demand arise as a consequence of what people do: as people use the internet more and watch less TV, new peaks emerge. Normal and extreme forms of energy demand are alike in being embedded in the ongoing dynamics of daily life."
"
This NOAA press release just showed up in my inbox, it seems to be a completely different take on the Hurricane season than that of Florida State’s COAPS and Ryan Maue who says:
Record inactivity continues:  Past 24-months of Northern Hemisphere TC activity (ACE) lowest in 30-years.
 
Global and Northern Hemisphere Accumulated Cyclone Energy: 24 month running sum through October 31, 2008. Note that the year indicated represents the value of ACE through the previous 24-months.
This was discussed at length at Climate Audit here


FOR  IMMEDIATE RELEASE
Contact: Carmeyia  Gillis
Nov. 26,  2008
301-763-8000,  ext. 7163 (office)
  240-882-9047  (cellular)
Dennis  Feltgen
305-229-4404  (office)
305-433-1933  (cellular)
 
Atlantic  Hurricane Season Sets Records 
The 2008 Atlantic Hurricane Season  officially comes to a close on Sunday, marking the end of a season that produced  a record number of consecutive storms to strike the United States and ranks as  one of the more active seasons in the 64 years since comprehensive records  began.
A total of 16 named storms formed this  season, based on an operational estimate by NOAA’s National Hurricane Center.  The storms included eight hurricanes, five of which were major hurricanes at  Category 3 strength or higher. These numbers fall within the ranges predicted in  NOAA’s pre- and mid-season outlooks issued in May and August. The August outlook  called for 14 to 18 named storms, seven to 10 hurricanes and three to six major  hurricanes. An average season has 11 named storms, six hurricanes and two major  hurricanes.
“This year’s hurricane season  continues the current active hurricane era and is the tenth season to produce  above-normal activity in the past 14 years,” said Gerry Bell, Ph.D., lead  seasonal hurricane forecaster at NOAA’s Climate Prediction  Center.
Overall, the season is tied as the  fourth most active in terms of named storms (16) and major hurricanes (five),  and is tied as the fifth most active in terms of hurricanes (eight) since 1944,  which was the first year aircraft missions flew into tropical storms and  hurricanes.
For the first time on record, six  consecutive tropical cyclones (Dolly, Edouard, Fay, Gustav, Hanna and Ike) made  landfall on the U.S. mainland and a record three major hurricanes (Gustav, Ike  and Paloma) struck Cuba. This is also the first Atlantic season to have a major  hurricane (Category 3) form in five consecutive months (July: Bertha, August:  Gustav, September: Ike, October: Omar, November: Paloma).
Bell attributes this year’s above-normal  season to conditions that include:

An ongoing multi-decadal signal. This  combination of ocean and atmospheric conditions has  spawned increased hurricane activity since 1995.
Lingering La Niña effects. Although  the La Niña that began in the Fall of 2007 ended in June, its influence of light  wind shear lingered.
Warmer tropical  Atlantic  Ocean temperatures. On average, the tropical  Atlantic was about 1.0 degree Fahrenheit  above normal during the peak of the season.

NOAA’s National Hurricane Center is  conducting comprehensive post-event assessments of each named storm of the  season. Some of the early noteworthy findings include:

Bertha was a tropical cyclone for 17  days (July 3-20), making it the longest-lived July storm on record in the  Atlantic  Basin.
Fay is the only storm on record to  make landfall four times in the state of Florida, and to prompt tropical storm  and hurricane watches and warnings for the state’s entire coastline (at various  times during its August lifespan).
Paloma, reaching Category 4 status  with top winds of 145 mph, is the second strongest November hurricane on record  (behind Lenny in 1999 with top winds of 155 mph).

Much of the storm-specific information  is based on operational estimates and some changes could be made during the  review process that is underway.
“The information we’ll gain by  assessing the events from the 2008 hurricane season will help us do an even  better job in the future,” said Bill Read, director of NOAA’s National Hurricane  Center. “With this season behind us, it’s time to prepare for the one that lies  ahead.”
NOAA will issue its initial 2009  Atlantic Hurricane Outlook in May, prior to the official start of the season on  June 1.
NOAA understands and predicts changes  in the Earth’s environment, from the depths of the ocean to the surface of the  sun, and conserves and manages our coastal and marine  resources.
A graphic track map of this season’s  storms and satellite visualization of the entire season is available at  http://www.noaa.gov.
On the Web:
NOAA’s Climate Prediction Center:  http://www.cpc.ncep.noaa.gov
NOAA’s National Hurricane Center:  http://www.hurricanes.gov
NHC 2008 Tropical Cyclone Reports:  http://www.nhc.noaa.gov/2008atlan.shtml
###


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9af631e5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Activists from Extinction Rebellion (XR) blocked traffic outside a London fashion week venue on Saturday and also staged a protest at Gatwick airport. Dozens of demonstrators prevented traffic from passing through a busy intersection leading to the Strand in Westminster, where the fashion trade show was being held.  Protesters carried placards reading: “No more false fashion” and “No fashion on a dead planet,” while others wore dresses made from chains. Last week, XR members delivered a letter to the British Fashion Council, calling for it to cancel the next London fashion week, due to be held in September. Sara Arnold, who helped coordinate the protest, said: “London is home to the cutting edge of sustainable and ethical design and yet London fashion week lags behind. “And despite having an active political engagement programme, you have done almost nothing to lobby for environmental policies, without which a transition with the urgency we need is simply impossible. “We have all failed, but now radical leadership is required. We need you, the British Fashion Council, as appointed industry administrators, to find the power and courage to centre a visionary process and protocol, without delay.” A handful of activists held a peaceful demo at Gatwick in Sussex, including one man dressed in a tiger onesie, to raise awareness of aircraft pollution. XR activists in disguise gathered in the airport’s south terminal at about 9.30am before revealing themselves. Protesters were instructed to arrive incognito and pretend to be waiting to meet someone amid fears they would not be allowed in the airport. The group of about 10 activists unveiled their full complement of XR banners, shirts and badges, and began mingling with the public. Passengers landing on flights from Salzburg, Madrid and Kingston were greeted by the protesters. Dan Burke, 16, a youth activist, said: ”We are already in climate crisis. We need to act now and, as we have seen in history, one of the best ways to bring forward actual legislation is to be in nonviolent disobedience.” Leaflets handed out apologised for the disruption but said: “We need your help.” A post on Facebook for the Gatwick Action event said: “Let’s get the message out – change can happen – and those who fly have the opportunity to make a big contribution by cutting their flights.”"
"To exaggerate is human, and scientists are human. Exaggeration and the complementary art of simplification are the basic rhetorical tools of human intercourse. So  yes, scientists do exaggerate.  So do politicians, perhaps even when, as the UK’s former environment secretary Owen Paterson did, they claim that climate change forecasts are “widely exaggerated”.  A more pertinent question is: does the way in which scientists and politicians speak publicly lead to wild exaggeration? When both are engaged in advocacy, there is little difference; both politicians and scientists will use whatever rhetorical devices they have to win an argument.  But this is not the case when scientists speak publicly through their own very special form of mass media, the peer-reviewed literature. In peer-review, statements that do not follow deductively from the data are subject to forensic examination and often expunged, or at least subjected to the “death by caveat” that makes so much academic writing almost indigestible.  Peer-review is by no means flawless, as it is a human procedure and so is subject to the same blind spots and groupthink of every human endeavour. Nevertheless, it does tend to make scientists cautious in their statements and wary of adversarial debate. The peer-review process is becoming more transparent, which may not remove its biases completely, but will allow them to be discovered more easily. Owen Paterson would have done better to focus on exaggeration’s rhetorical twin: simplification. Simplification is not expunged by the peer-review process. Far from it; simplification is at the very heart of how scientists, including climate scientists, do science.  The world is a complicated place, and understanding – let alone predicting – the climate requires consideration of each compartment of the environment: the atmosphere, ocean, land, and ice. In each compartment, physical, chemical, and biological processes interact strongly.  Faced with such complexity, scientists have adopted a hierarchical approach to knowledge generation. This means using highly simplified conceptual models to sketch out the limits of the system; analyses of intermediate complexity to scope out the broad-brush behaviour; and models of often mind-numbing intricacy to attempt to simulate the “real thing”.  These models of reality can be physical or computational: in the Birmingham Institute of Forest Research we are building a physical model of future ecosystem behaviour which bears comparison with the Large Hadron Collider in its ambition and complexity. Even so, we will be pressed on, and perhaps even criticised for, the simplifications we have had to make in order to set up a manageable and affordable experiment. Such is the life scientific.  Our results may lie at the extreme end of our current best guesses (forcing us to make further corroborative experiments), or may lie comfortably in the middle (hence improving our confidence) – in neither case will they be adequately described as exaggeration. In general, limiting or extreme results come about because a simplified analysis is missing an important feedback or because an intricate model is being “exercised” by simulating an extreme scenario.  Yet such simplifications are the stuff of science. Galileo, for instance, was only able to develop his ideas on gravity by ignoring air resistance. He simplified things to get at a fundamental property of the world about us. This process represents exaggeration only when the results are taken out of their experimental context. Scientists, wandering unwarily into the realm of advocacy, may be guilty of taking the results out of context, as may be activists and politicians, but it is not the science itself that is “widely exaggerated”. Is UK energy policy informed solely by the exaggerations of advocacy – political or scientific – or, at least in part, by the exaggeration-phobic scientific literature? As a taxpayer I would like to believe that physical and computer models provide evidence to politicians who use it to assess the strength of the arguments of the various advocacy groups. I am not so politically naïve as to believe that all policy is, or even should be, based solely on objective evidence.  I do hope, though, that claims of scientific exaggeration are seen for what they are: advocacy targeted not just at winning the rhetorical argument but also aimed, rather cynically, at undermining the evidence."
"The Earth’s climate has always changed. All species eventually become extinct. But a new study has brought into sharp relief the fact that humans have, in the context of geological timescales, produced near instantaneous planetary-scale disruption. We are sowing the seeds of havoc on the Earth, it suggests, and the time is fast approaching when we will reap this harvest.  This in the year that the UN climate change circus will pitch its tents in Paris. December’s Conference of the Parties will be the first time individual nations submit their proposals for their carbon emission reduction targets. Sparks are sure to fly. The research, published in the journal Science, should focus the minds of delegates and their nations as it lays out in authoritative fashion how far we are driving the climate and other vital Earth systems beyond any safe operating space. The paper, headed by Will Steffen of the Australian National University and Stockholm Resilience Centre, concludes that our industrialised civilisation is driving a number of key planetary processes into areas of high risk. It argues climate change along with “biodiversity integrity” should be recognised as core elements of the Earth system. These are two of nine planetary boundaries that we must remain within if we are to avoid undermining the biophysical systems our species depends upon.  The original planetary boundaries were conceived in 2009 by a team lead by Johan Rockstrom, also of the Stockholm Resilience Centre. Together with his co-authors, Rockstrom produced a list of nine human-driven changes to the Earth’s system: climate change, ocean acidification, stratospheric ozone depletion, alteration of nitrogen and phosphorus cycling, freshwater consumption, land use change, biodiversity loss, aerosol and chemical pollution. Each of these nine, if driven hard enough, could alter the planet to the point where it becomes a much less hospitable place on which to live. The past 11,000 years have seen a remarkably stable climate. The name given to this most recent geological epoch is the Holocene. It is perhaps no coincidence that human civilisation emerged during this period of stability. What is certain is that our civilisation is in very important ways dependent on the Earth system remaining within or at least approximately near Holocene conditions. This is why Rockstrom and co looked at human impacts in these nine different areas. They wanted to consider the risk of humans bringing about the end of the Holocene. Some would argue that we have already entered a new geological epoch – the Anthropocene – which recognises that Homo sapiens have become a planet-altering species. But the planetary boundaries concepts doesn’t just attempt to quantify human impacts. It seeks to understand how they may affect human welfare now, and in the future.   The 2009 paper proved to be very influential, but it also attracted a fair amount of criticism. For example, it has been argued that some of the boundaries are not in fact global in scale. There are very large regional variations in consumption of freshwater and phosphorus fertiliser pollution, for instance. 


Phosphorous pollution in croplands.
Steffen et al

 That means that while globally we may be in the green, there could be an increasing number of regions that are deep in the red. The latest research develops the methodology so that it now includes regional evaluations. For example it assesses basin-level freshwater use and biome-level species extinction rates. It also includes a new boundary of “novel entities” – new forms of life and novel compounds the likes of which the Earth system has not experienced and so impact of which is extremely challenging to assess. Ozone-depleting CFCs are perhaps the best example of how a seemingly inert substance can produce planetary damage. The paper also gives an update on where we stand on some of the planetary boundaries. At first sight, it looks as though there may be some good news in that climate change is no longer in the red. But then closer inspection reveals that a new yellow “zone of uncertainty with increasing risk” has been added to the previous green and red classification.  Climate change impacts are firmly within this new yellow zone. Our atmosphere currently has about 400 parts per million (ppm) of carbon dioxide. To recover back to the green zone we still need to get back to 350ppm – the same precautionary boundary as before. Perhaps most importantly the research produces a two-tier hierarchy in which climate change and biosphere integrity are recognised as the core planetary boundaries through which the others operate. This makes sense: life and climate are the main columns buttressing our continual existence within the Holocene. Weakening them risks amplifying other stresses on other boundaries. And so to the very bad news. Given the importance of biodiversity to the functioning of the Earth’s climate and the other planetary boundaries, it is with real dismay that this study adds yet more evidence to the already burgeoning pile that concludes we appear to be doing our best to destroy it as fast as we possibly can.  Extinction rates are very hard to measure but the background rate – the rate at which species would be lost in the absence of human impacts – is something like ten a year per million species. Current extinction rates are anywhere between 100 to 1000 times higher than that. We are possibly in the middle of one of the great mass extinctions in the history of life on Earth.  James Dyke is answering your questions about planetary boundaries in a Reddit AMA."
"It is 2050. Beyond the emissions reductions registered in 2015, no further efforts were made to control emissions. We are heading for a world that will be more than 3C warmer by 2100 The first thing that hits you is the air. In many places around the world, the air is hot, heavy and, depending on the day, clogged with particulate pollution. Your eyes often water. Your cough never seems to disappear. You think about some countries in Asia, where, out of consideration, sick people used to wear white masks to protect others from airborne infection. Now you often wear a mask to protect yourself from air pollution. You can no longer simply walk out your front door and breathe fresh air: there might not be any. Instead, before opening doors or windows in the morning, you check your phone to see what the air quality will be.  Fewer people work outdoors and even indoors the air can taste slightly acidic, sometimes making you feel nauseated. The last coal furnaces closed 10 years ago, but that hasn’t made much difference in air quality around the world because you are still breathing dangerous exhaust fumes from millions of cars and buses everywhere. Our world is getting hotter. Over the next two decades, projections tell us that temperatures in some areas of the globe will rise even higher, an irreversible development now utterly beyond our control. Oceans, forests, plants, trees and soil had for many years absorbed half the carbon dioxide we spewed out. Now there are few forests left, most of them either logged or consumed by wildfire, and the permafrost is belching greenhouse gases into an already overburdened atmosphere. The increasing heat of the Earth is suffocating us and in five to 10 years, vast swaths of the planet will be increasingly inhospitable to humans. We don’t know how hospitable the arid regions of Australia, South Africa and the western United States will be by 2100. No one knows what the future holds for their children and grandchildren: tipping point after tipping point is being reached, casting doubt on the form of future civilisation. Some say that humans will be cast to the winds again, gathering in small tribes, hunkered down and living on whatever patch of land might sustain them. More moisture in the air and higher sea surface temperatures have caused a surge in extreme hurricanes and tropical storms. Recently, coastal cities in Bangladesh, Mexico, the United States and elsewhere have suffered brutal infrastructure destruction and extreme flooding, killing many thousands and displacing millions. This happens with increasing frequency now. Every day, because of rising water levels, some part of the world must evacuate to higher ground. Every day, the news shows images of mothers with babies strapped to their backs, wading through floodwaters and homes ripped apart by vicious currents that resemble mountain rivers. News stories tell of people living in houses with water up to their ankles because they have nowhere else to go, their children coughing and wheezing because of the mould growing in their beds, insurance companies declaring bankruptcy, leaving survivors without resources to rebuild their lives. Contaminated water supplies, sea salt intrusions and agricultural runoff are the order of the day. Because multiple disasters are often happening simultaneously, it can take weeks or even months for basic food and water relief to reach areas pummelled by extreme floods. Diseases such as malaria, dengue, cholera, respiratory illnesses and malnutrition are rampant. You try not to think about the 2 billion people who live in the hottest parts of the world, where, for upwards of 45 days per year, temperatures skyrocket to 60C (140F), a point at which the human body cannot be outside for longer than about six hours because it loses the ability to cool itself down. Places such as central India are becoming increasingly challenging to inhabit. Mass migrations to less hot rural areas are beset by a host of refugee problems, civil unrest and bloodshed over diminished water availability. Food production swings wildly from month to month, season to season, depending on where you live. More people are starving than ever before. Climate zones have shifted, so some new areas have become available for agriculture (Alaska, the Arctic), while others have dried up (Mexico, California). Still others are unstable because of the extreme heat, never mind flooding, wildfire and tornadoes. This makes the food supply in general highly unpredictable. Global trade has slowed as countries seek to hold on to their own resources. Countries with enough food are resolute about holding on to it. As a result, food riots, coups and civil wars are throwing the world’s most vulnerable from the frying pan into the fire. As developed countries seek to seal their borders from mass migration, they too feel the consequences. Most countries’ armies are now just highly militarised border patrols. Some countries are letting people in, but only under conditions approaching indentured servitude.  Those living within stable countries may be physically safe, yes, but the psychological toll is mounting. With each new tipping point passed, they feel hope slipping away. There is no chance of stopping the runaway warming of our planet and no doubt we are slowly but surely heading towards some kind of collapse. And not just because it’s too hot. Melting permafrost is also releasing ancient microbes that today’s humans have never been exposed to and, as a result, have no resistance to. Diseases spread by mosquitoes and ticks are rampant as these species flourish in the changed climate, spreading to previously safe parts of the planet, increasingly overwhelming us. Worse still, the public health crisis of antibiotic resistance has only intensified as the population has grown denser in inhabitable areas and temperatures continue to rise. The demise of the human species is being discussed more and more. For many, the only uncertainty is how long we’ll last, how many more generations will see the light of day. Suicides are the most obvious manifestation of the prevailing despair, but there are other indications: a sense of bottomless loss, unbearable guilt and fierce resentment at previous generations who didn’t do what was necessary to ward off this unstoppable calamity. • This is an edited extract from The Future We Choose: Surviving the Climate Crisis by Christiana Figueres and Tom Rivett-Carnac, published by Manilla Press (£12.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15 • Christiana Figueres and Tom Rivett-Carnac will be in conversation at a Guardian Live event at the Royal Geographical Society, London SW7, on Tuesday 3 March, 7pm"
"
During our last check in, we had a look at northern Canada from the Arctic Circle to the North pole, and found we had quite a ways to go before we see an “ice free arctic” this year as some have speculated.
Today I did a check of the NASA rapidfire site for TERRA/MODIS satellite images and grabbed a view showing northern Greenland all the way to the North Pole.
There’s some bergy bits on the northeastern shore of Greenland, but in the cloud free area extending all the way to the pole, it appears to still be solid ice.

Click for a larger image – Note: image has been rotated 90° clockwise and sat view sector icon and time stamp added, along with “N” for north pole marker.
Link to original source image is here:
http://rapidfire.sci.gsfc.nasa.gov/realtime/single.php?T082121805
With more than half of the summer melt season gone, it looks like an uphill battle for an ice-free arctic this year.
Here is another view from today from the Aqua satellite:

Click for a larger image – Note: image has been rotated 90° counter- clockwise and sat view sector icon and time stamp added, along with “N” for north pole marker.
Source image is here:
http://rapidfire.sci.gsfc.nasa.gov/realtime/single.php?A082121655
This dovetails with a press release and news story about more ice than normal in the Barents Sea
From the Barents Observer:
http://www.barentsobserver.com/?cat=16149&id=4498513
New data from  the Norwegian Meteorological Institute shows that there is more ice than normal  in the Arctic waters north of the Svalbard archipelago.
In most years, there are open waters in the area north of the archipelago in  July month. Studies from this year however show that the area is covered by ice,  the Meteorological Institute writes in a press release.
In mid-July, the research vessel Lance and the Swedish ship MV Stockholm got  stuck in ice in the area and needed help from the Norwegian Coast Guard to get  loose.
The ice findings from the area spurred surprise among the researchers, many  of whom expect the very North Pole to be ice-free by September this year.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d8b805c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Germany is well on its way towards having a predominantly green electricity supply. The transition from nuclear and fossil fuel electricity to using renewables is happening faster than anyone had anticipated. This is a success, but there is a downside: it is hugely expensive.  The energy transition is an explicit policy goal in Germany, having been made a priority project by the German chancellor, Angela Merkel. It has four strands: reducing CO2 emissions, improving energy efficiency, promoting renewable energy and the gradual phase-out of nuclear power. Nuclear phase-out is actually an old story that started in 2000 when the Schroeder administration first announced a 20-year timetable. It was a bit of a “yes-no” rollercoaster until the Fukushima incident, after which the decision in favour was final. This is widely supported by the German public, meaning that nuclear power is politically not an option at the moment. Yet without a doubt, the most significant development within the energy transition project has been the growth of Germany’s renewable energy sources (RES). The chart below shows how it has developed in the past few years and where the government expects it to be by 2050.  Germany’s renewable energy supply The horizontal black line depicts the approximate maximum demand at any time, which is about 85GW (this will not change much in the future). This shows that installed renewable capacity is now already more or less equal to maximum demand. On a very sunny and windy day, renewables are now capable of meeting the demands of the entire country.   But as we all know, the weather is notoriously unreliable and variable. So a secure system needs more renewable capacity and also more reserve capacity from conventional power plants (mainly fuelled by natural gas) to make sure it can always meet demand. As the chart indicates, installed renewable capacity in 2050 is expected to be 180GW, which is roughly twice maximum demand. By that time, the target is that 80% of electricity supply will be from renewables (basically this is how much renewable power you need to meet this level of supply on a regular basis).  In common with other countries moving in the same direction, the government has various motives for this big shift. Renewables are carbon-free and rely on no fossil fuels, so they are an essential component of meeting European emissions targets. The government hopes for positive spin-off effects on exports, innovation and new jobs. And once the investment cost of the transition has been incurred, we would hope that electricity supply is actually quite cheap. After all, sun and wind are free. Germany sees the energy transition as an investment in the future: we pay for the next generation. The move to renewables has been a success. It has happened at high speed since the late 1990s. The debate is no longer whether it will succeed, but rather what do we do with “too much” renewable power. But behind this positive story, the dark side is the huge expense.  Early in 2013, the then-minister of environment Peter Altmaier mentioned the staggering amount of €1 trillion (£790bn) as the potential cost of the overall transition. This relied on a quick-and-dirty back-of-the-envelope calculation, which raises many questions and was never confirmed, but it does give a feel for the order of magnitude. The end-users – and thus the voters in Germany – are starting to feel the pain. Since the installation costs mean that renewables currently cost more per unit of power than conventional power, they are subsidised by a surcharge on the electricity price. In other words, electricity end-users directly pay for it. As you can see from the chart below, the surcharge for small end-users has soared since 2009 to cope with the rapid growth of installed capacity (the step-change that year reflected a sudden big rise in solar power, which is particularly expensive). The total subsidy is currently about €20bn/year, which amounts to €218/year per household on top of the normal electricity bill. Whether this is still affordable is a key question in the country right now. Germany’s rising renewable surcharge The energy transition has meanwhile changed the face of the electricity market, with severe consequences for traditional firms like E.ON and RWE. They are suffering badly at the moment and are having to rethink their business models completely. In short, they face three challenges. The nuclear phase-out means they have to make very significant write-downs on their nuclear plants, at a loss to the shareholders. They are still fighting the government for compensation payments.  Second, renewable power is suppressing electricity wholesale prices – essentially because they are cheaper to run per unit of power, which under the rules for calculating the wholesale price tends to bring it down across the board. This means that the revenues for conventional power plants are low and no longer cover the investment costs.  Third, conventional power from gas and coal is being pushed out of the market. This means that a lot of conventional power plants are largely standing idle and not making any money. Since the future business model for such plants is looking bleak, the power companies are sitting on investments, which are not going to be profitable. Of course, RWE and E.ON are adjusting their long-term strategies to survive. While this has been going on, the rising costs for residential end-users have become a political problem. In 2014 the government responded with a reform package, which slows down the energy transition in an attempt to control the costs. Basically the annual growth of new renewables has been capped to a pre-determined level.  This seems to be working. The surcharge for 2015 has been calculated at 6.17ct/kWh, which is a small decline compared to 2014. Politically, this may well have been a wise policy, as public support for the energy transition was dwindling. It means that green energy development will happen more slowly. So far the government appears to be standing by the same targets outlined in my first chart, perhaps because the explosion in development over the past few years had put it on an even faster track. Whatever happens from here, one thing remains key: without public support, the energy transition will not work."
"
Share this...FacebookTwitterFakta Menarik Akun Pro Di Situs BandarQQ – Banyak sekali pembicaraan mengenai akun pro situs bandarqq. Beberapa orang mengatakan bahwa akun pro hanyalah fiktif belaka. Namun sebagian orang juga ada yang percaya dengan adanya akun pro ini. Membingungkan, ya. Lalu, menurut kamu bagaimana? Apakah benar-benar ada akun pro itu? Atau memang hanya fiktif?
Nah, sebelum membahas lebih lanjut, kamu mesti tahu nih bahwa akun pro hanya ada di beberapa level atau kalangan. Jadi, wajar saja jika masih banyak orang yang tidak mengetahuinya. CS judi online pun tidak akan tahu jika di situsnya memang tidak mengadakan akun pro. Berikut adalah beberapa ulasan mengenai bocoran akun pro di situs judi online BandarQQ yang tidak diketahui banyak orang:
Rahasia High Profit Pengguna Akun Pro
Pada umumnya, situs judi online memang memiliki rahasia dan juga cara curang dalam bermain. Namun banyak orang yang tidak mengetahuinya atau mereka tahu tetapi takut untuk memberitahu. Biasanya, para pemilik situs high profit-lah yang tahu rahasia dan cara curang seperti akun pro. Termasuk akun pro situs BandarQQ.
Yang perlu kamu ketahui tentang situs high profit ialah keadaan jika kamu menang maka pemilik situs dapat mengambil upah dari member yang menang saat bermain. Maka dari itu kamu tidak perlu heran jika pemilik akun pro sangat mudah menang.
Penyebab Anda Sering Kalah Meskipun Sudah Menggunakan Akun Judi Pro di Situs Agen BandarQQ
Harap diingat bahwa menggunakan akun pro tidak akan membuatmu selalu menang. Keuntungan memakai akun pro ini ialah kamu mempunyai kesempatan menang lebih besar dibandingkan menggunakan akun biasa. Bukan akan terus menang. Maka, wajar saja jika kamu masih bisa kalah. Namun jika kamu kalah berturut-turut dan tidak pernah menang sama sekali mungkin  ada masalah. Berikut ini ulasan penyebab mengapa kamu tidak menang-menang meski menggunakan akun pro termasuk akun pro situs BandarQQ!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Belum Meng-upgrade Akun Pro
Kamu harus paham jika memakai akun pro, kamu wajib meng-upgradenya agar kelebihannya tidak berkurang.
Kurang beruntung
Kamu sudah meng-upgrade akun pro tetapi tetap kalah terus? Jika seperti itu kemungkinan besar adalah kamu sedang tidak beruntung. Karena, sekali lagi yang perlu kamu ingat, menggunakan akun pro termasuk akun pro situs BandarQQ juga bisa membuatmu kalah.
Penyebab Akun Pro Tidak Lagi Sakti
Akun pro memang sudah terbukti bisa membuat kamu mempunyai kesempatan menang lebih besar dibanding akun biasa. Namun ada masanya jika akun pro milik kamu tidak sakti lagi bahkan bisa membuat kamu sangat merugi! Berikut ini ulasan penyebab akun pro milik kamu tidak sakti lagi.
Nilai Deposit Milik Kamu Ternyata Menurun
Kamu tidak pernah meninggikan nilai depositmu. Ini adalah faktor yang paling memungkinkan jika akun pro milik kamu tidak lagi membawa keberuntungan. Termasuk akun pro situs BandarQQ. Maka, silakan naikkan nilai deposit yang kamu miliki.
Kamu Tidak  Pernah Lagi Melakukan Deposit
Penyebab berikutnya adalah akibat dari kesalahan kamu sendiri. Kamu tidak pernah lagi melakukan deposit. Banyak sekali penyebab orang tidak melakukan deposit lagi, salah satu contohnya karena sudah tidak pernah lagi bermain judi online.
Itulah beberapa penjelasan mengenai akun pro situs BandarQQ. Meski penjelasan ini mencakup semua situs judi online yang ada. Hal terpenting yang harus kamu ingat ketika bermain judi online adalah dengan tidak serakah karena keserakahan bisa mengantarkanmu pada kerugian. Semoga artikel ini bermanfaat, ya.
Share this...FacebookTwitter "
"
Share this...FacebookTwitter19 dead and up to 400 injured, many seriously.These are the latest gruesome numbers from yesterday’s Duisburg Love Parade, crowd-control disaster. It’s a classic case of what can go wrong when warnings are ignored or played down. City officials were warned that the location was seriously inadequate, but nobody wanted to be a party-pooper.
Germany’s techno-music Love Parade first started in 1989. Most have been held in Berlin, until the city got tired of the chaos and filth they left behind, and the event had lost money anyway.
Yet, everybody hates to see a good party end, and so other places were sought to host the million-plus visitor event. This year’s choice proved to be a disaster.
The catastrophe occurred at a tunnel under a street that served as the main entrance to the event. The following video vividly shows the catastrophe in motion. Especially interesting is the 0:31 mark of the clip. there you see the fully packed ramp leading down to the tunnel.
LOVE PARADE DISASTER VIDEO
Just the shear physics of the situation alone are staggering.
If you estimate 100,000 people on that sloped ramp, each with an average weight of 70 kg, you have a total weight of 7000 tons trying to move forward. If the ramp has a slope of 3°, then sine 3° times 7000 tones yields a gravity force vector of 366 tons pressing down against the wall. Not a good place to be. Granted that’s just a real rough calculation, but it gives you an idea.
A sure sign that the old rail yard location, where the event took place, was not going to work was its relatively small size. It had an area of 2.2 million square feet, meaning that the place was going to be overly packed with just half a million people. Organisers expected 1 million, 1.4 million showed up. 1.5 sq ft per person. Experts say the area was suitable for a maximum of only 300,000.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Throw in youth, alcohol and drugs and you have all the ingredients for a crowd control disaster.
The police had warned city officials that the risks were too high and had advised against approving the event. But reports say no top city official wanted to be the party-pooper. The Love Parade got the green light.
Now, after the catastrophe, with the nation and continent in shock, officials are scrambling and the finger-pointing has begun. According to German newspaper Bild:
Duisburg mayor Adolf Sauerland defended the safety concept of the Love Parade against criticism.
In his view the reasons for the tragedy were not due to a poor safety concept, but rather very likely had more to do with individual weak points.
Tell that to the prosecuting attorneys Herr Burgermeister.
The State’s Attorney has announced that an investigation for negligent homicide will be conducted, and all city and official documents relating to permitting and organisation of the event have been confiscated by police.
The Love Parade organisation has announced today that there will not be any more Love Parades in the future.
UPDATE – USEFUL LINKS:
http://www.thelocal.de/national/20100726-28737.html
http://www.spiegel.de/international/germany/0,1518,708462,00.html
Share this...FacebookTwitter "
"

Rarely is federal legislation something other than a vehicle for government overreach and aggrandizement. Despite its populist‐​sounding title, the Global Investment in American Jobs Act – introduced in the Senate last week – is one of those rarities. The bill calls for an assessment of U.S. policies that influence decisions by foreigners about investing in the United States.   
  
  
Properly modest in scope, the legislation simply authorizes to Commerce Department to produce a report that documents the importance of foreign investment, identifies home‐​grown impediments to such investment, and recommends policy changes that would make the United States a more attractive investment destination.   
  
  
What is so refreshing about the bill is that its premise is not that the practices of foreign governments or the greed of U.S. corporations that allegedly “ship jobs overseas” are to blame for U.S. economic stagnation – themes so prominent in the past couple of Congresses and the current White House. Rather, the premise is that U.S. policy and its accumulated residue have created a web of impediments that discourage foreign investment in the United States, and that changes to those policies could serve to attract new investment. This kind of thinking is long overdue.   
  
  
One of the themes of my 2009 Cato paper, “Made on Earth: How Global Economic Integration Renders Trade Policy Obsolete,” is that it is no longer apt to consider global commerce a competition between “Us” and “Them.” Trans‐​national production/​supply chains and cross‐​border investment have blurred the distinctions between “our” producers and “their” producers. Many products and services are created along supply chains that travel from idea conception to final consumption and that include value‐​added activities of varying degrees of labor‐, physical capital‐, and intellectual capital‐​intensity. Furthermore, the largest U.S. steel producer, Mittal, is a majority Indian‐​owned company with corporate headquarters in Luxembourg. The largest “German” steel producer, Thyssen‐​Krupp, recently completed construction of a $4 billion production facility near Mobile, Alabama for the purpose of serving U.S. demand for finished steel, particularly from the mostly foreign nameplate auto producers dotting the landscape of the American South. And, as of 2010, our beloved General Motors produces more vehicles in China than it does in the United States. So, really, who are “we” and who are “they”?   
  
  
Accordingly, instead of pursuing a 20th century trade policy model that seeks to secure market‐​access advantages for certain producers, policy should be recalibrated to reflect the 21st century reality that governments around the world are competing for business investment and talent, which both tend to flow to jurisdictions where the rule of law is clear and abided; where there is greater certainty to the business and political climate; where the specter of asset expropriation is negligible; where physical and administrative infrastructure is in good shape; where the local work force is productive; where there are limited physical, political, and regulatory barriers, etc. This global competition in policy is a positive development because — among other reasons — its serves to discipline bad government policy.   
  
  
We are kidding ourselves if we think that the United States is somehow immune from this dynamic and does not have to compete and earn its share with good policies. The decisions made now with respect to our policies on immigration, education, energy, trade, entitlements, taxes, regulations, industrial management, and the proper role of government in the economic sphere will determine the health, competitiveness, and relative significance of the U.S. economy in the decades ahead.   
  
  
Governments with the smartest policies will be the ones that secure the most investment, the strongest talent, and the best economic opportunities for their people. The legislation is step in the right direction.
"
nan
"It is often said that if something is repeated often enough, it becomes accepted as true. This has certainly been the case for the link between terrorism and the poaching of elephants for the ivory trade.  A wide range of public figures have repeated the claim that ivory plays a major role in bankrolling terrorist organisations in Africa. These include former US secretary of state Hillary Clinton, UK foreign secretary William Hague and Kenya’s president Uhuru Kenyatta.  The most recent voice to be added to the choir was that of cinema director Kathryn Bigelow. The Oscar-winning director teamed up with charity WildAid to create a short video asserting that trade in ivory is funding the Somali terrorist group al-Shabaab, responsible for the 2013 Westgate Mall attack in Kenya in which 67 people died.  As with any illegal activity, it is very difficult to obtain reliable data on the size of the ivory trade. Although there is evidence that it has been used to finance armed groups in Africa such as the Lord’s Resistance Army or the Janjaweed in Darfur, the allegations linking ivory to terrorist groups are much weaker. They essentially rest on a single report published by the Elephant Action League in 2012. The report asserts, based on a single unnamed “source within the militant group”, that al-Shabaab makes between US$200,000 and US$600,000 from ivory, up to 40% of its income. This over-reliance on a single source and the fact that only a short “journalistic summary” of the report was ever released, has led to scepticism.  Recently, a joint report by INTERPOL and the UN Environmental Program classified EAL’s claims as “highly unreliable” as they would require al-Shabaab to bring nearly all ivory poached from west, central and eastern Africa to a single Somali port. However, this same report establishes a solid link between al-Shabaab’s finances and another environmental crime: illegal charcoal production.  The trade in charcoal leads to widespread deforestation and is already driving erosion and desertification in parts of Somalia. Al-Shabaab’s main financing mechanism appears to be the taxing of charcoal coming to the port of Baraawe (and until recently Kismayo) south of Mogadishu, with the value of the trade estimated to be US$38–56m per year. This means that, even if the EAL’s inflated ivory estimates were true, the trade in charcoal would still generate 60 to 94 times more revenue for al-Shabaab.  We’ve known about the charcoal trade in the Horn of Africa for a while now – the UN, for instance, highlighted the issue in a 2013 monitoring report on the Somali conflict.  It is thus puzzling that some western political and conservation figures have decided to focus on the unproven link between ivory and terrorism instead of the more relevant and substantiated conservation issue. A possible (yet cynical) explanation is that those highlighting the issue are trying to gain notoriety by bringing together terrorism, a top issue for all western governments, and the elephant, one of the most widely used conservation flagship species.  This would surely generate more attention than the more abstract issue of desertification and a few obscure tree species. The increased visibility could then be used to generate extra votes, donations or simply a more environment-friendly image. If this was the case, then we would for example expect these efforts to focus on those more likely to vote or donate, instead of those more likely to buy ivory. In the case of Kathryn Bigelow’s video and the “Last days of ivory” campaign it spearheads, all materials are only available in English, a language not relevant for the key ivory markets in Southeast Asia.  All the first four actions proposed to those who visit the campaign’s website revolve around either sharing the campaign image and content on social networks or donating to the associated charities. This campaign does indeed appear to be targeting those who can donate rather than those who can directly impact the ivory trade. Those involved clearly have something to gain from pushing the link between ivory and terrorism beyond the available evidence. However, it is also clear that in the long run it is not only their own credibility that is at risk but that of a whole conservation movement. Conservationists have focused large on messages of doom and gloom that often sound as if holding humanity for ransom if the environmental crisis is not addressed. If we are serious about keeping the public’s trust, we must ensure that we are driven by evidence, not the hype, lest we become the boy who cried wolf."
"
UPADATED AT 8:30AM PST Sept 2nd-
More on SIDC’s decision to count a sunspeck (technically a “pore”) days after the fact. NOAA has now followed SIDC in adding a 0.5 sunspot where there was none before. But as commenter Basil points out, SIDC’s own records are in contrast to their last minute decision to count the sunspeck or “pore” on August 21.
There is an archive of the daily SIDC “ursigrams” here:
http://sidc.oma.be/html/SWAPP/dailyreport/dailyreport.html
If you select the ursigrams for August 22 and 23, you get the reported data for the 21st and 22nd:
August 21:
TODAY’S ESTIMATED ISN  : 000, BASED ON 07 STATIONS.
SOLAR INDICES FOR 21 Aug 2008
WOLF NUMBER CATANIA    : 011
10CM SOLAR FLUX        : 067
AK CHAMBON LA FORET    : ///
AK WINGST              : 004
ESTIMATED AP           : 005
ESTIMATED ISN          : 000, BASED ON 14 STATIONS.
August 22:
TODAY’S ESTIMATED ISN  : 000, BASED ON 11 STATIONS.
SOLAR INDICES FOR 22 Aug 2008
WOLF NUMBER CATANIA    : 013
10CM SOLAR FLUX        : 068
AK CHAMBON LA FORET    : ///
AK WINGST              : 003
ESTIMATED AP           : 003
ESTIMATED ISN          : 000, BASED ON 11 STATIONS.
In both cases, the daily estimated “International Sunspot Number” based on multiple stations, not just the Catania Wolf Number, was 000. So how did SIDC end up with positive values in the monthly report?
UPDATED at 2:42 PM PST Sept 1st – 
After going days without counting the August 21/22 “sunspeck” NOAA and SIDC Brussels now says it was NOT a spotless month! Both data sets below have been recently revised.
Here is the SIDC data:
http://www.sidc.be/products/ri_hemispheric/
Here is the NOAA data:
ftp://ftp.ngdc.noaa.gov/STP/SOLAR_DATA/SUNSPOT_NUMBERS/MONTHLY
The NOAA data shows July as 0.5 but they have not yet updated for August as SIDC has. SIDC reports 0.5 for August. It will be interesting to see what NOAA will do.
SIDC officially counted that sunspeck after all. It only took them a week to figure out if they were going to count it or not, since no number was assigned originally.
But there appears to be an error in the data from the one station that reported a spot, Catania, Italy. No other stations monitoring that day reported a spot. Here is the drawing from that Observatory:
ftp://ftp.ct.astro.it/sundraw/OAC_D_20080821_063500.jpg
ftp://ftp.ct.astro.it/sundraw/OAC_D_20080822_055000.jpg
But according to Leif Svalgaard, “SIDC reported a spot in the south, while the spot(s) Catania [reported] was in the north.” This is a puzzle. See his exchange below.
Also, other observatories show no spots at all. For example, at the 150 foot solar solar tower at the Mount Wilson Observatory, the drawings from those dates show no spots at all:
ftp://howard.astro.ucla.edu/pub/obs/drawings/dr080821.jpg
ftp://howard.astro.ucla.edu/pub/obs/drawings/dr080822.jpg
Inquires have been sent, stay tuned.
Here is an exchange in comments from Leif Svalgaard.
——-
REPLY: So What gives Leif….? You yourself said these sunspecks weren’t given a number. I trusted your assessment. Hence this article. Given the Brussels folks decided to change their minds later, what is the rationale ? – Anthony
The active region numbering is done by NOAA, not by Brussels. The Brussels folks occasionally disagree. In this case, they did. Rudolf Wolf would not have counted this spot. Nor would I. What puzzles me is this:
21 7 4 3
22 8 4 4
The 3rd column are ’spots’ in the Northern hemisphere, and the 4th column are ’spots’ in the Southern hemisphere [both weighted with the ‘k’-factor: SSN = k(10g+s)]. But there weren’t any in the south. The Catania spot was at 15 degrees north latitude, IIRC. Maybe the last word is not in on this.
——–
Hmm….apparently there’s some backstory to this. There is a debate raging in comments to this story, be sure to check them. – Anthony
# MONTHLY REPORT ON THE INTERNATIONAL SUNSPOT NUMBER #
# from the SIDC (RWC-Belgium) #
#——————————————————————–#
AUGUST 2008
PROVISIONAL INTERNATIONAL NORMALIZED HEMISPHERIC SUNSPOT NUMBERS
Date Ri Rn Rs
__________________________________________________________________
1 0 0 0
2 0 0 0
3 0 0 0
4 0 0 0
5 0 0 0
6 0 0 0
7 0 0 0
8 0 0 0
9 0 0 0
10 0 0 0
11 0 0 0
12 0 0 0
13 0 0 0
14 0 0 0
15 0 0 0
16 0 0 0
17 0 0 0
18 0 0 0
19 0 0 0
20 0 0 0
21 7 4 3
22 8 4 4
23 0 0 0
24 0 0 0
25 0 0 0
26 0 0 0
27 0 0 0
28 0 0 0
29 0 0 0
30 0 0 0
31 0 0 0
__________________________________________________________________
MONTHLY MEAN : 0.5 0.3 0.2
========================================================
ORIGINAL STORY FOLLOWS:
Many have been keeping a watchful eye on solar activity recently. The most popular thing to watch has been sunspots. While not a direct indication of solar activity, they are a proxy for the sun’s internal magnetic dynamo. There have been a number of indicators recently that it has been slowing down.
August 2008 has made solar history. As of 00 UTC (5PM PST) we just posted the first spotless calendar month since June 1913. Solar time is measured by Coordinated Universal Time (UTC) so it is now September 1st in UTC time. I’ve determined this to be the first spotless calendar month according to sunspot data from NOAA’s National Geophysical Data Center, which goes back to 1749. In the 95 years since 1913, we’ve had quite an active sun. But that has been changing in the last few years. The sun today is a nearly featureless sphere and has been for many days:

Image from SOHO
And there are other indicators. For example, some solar forecasts have been revised recently because the forecast models haven’t matched the observations. Australia’s space weather agency recently revised their solar cycle 24 forecast, pushing the expected date for a ramping up of cycle 24 sunspots into the future by six months.
The net effect of having no sunspots is about 0.1% drop in the TSI (Total Solar Irradiance). My view is that TSI alone isn’t the main factor in modulating Earth’s climate. 
I think it’s solar magnetism modulating Galactic Cosmic Rays, and hence more cloud nuclei from GCR’s, per Svensmark’s theory. We’ve had indications since October 2005 that the sun’s dynamo is slowing down. It dropped significantly then, and has remained that way since. Seeing no sunpots now is another indicator of that idling dynamo.
Graph of solar Geomagnetic Index (Ap):

Click for a larger image
Earth of course is a big heat sink, so it takes awhile to catch up to any changes that originate on the sun, but temperature drops indicated by 4 global temperature metrics (UAH, RSS and to a lesser degree HadCrit and GISS) show a significant and sharp cooling in 2007 and 2008 that has not rebounded.In the 20 years since “global warming” started life as a public issue with Dr. James Hansen’s testimony before congress in June 1988, we are actually cooler.

Click for a larger image
Reference: UAH lower troposphere data
Coincidence? Possibly, but nature will be the final arbiter of climate change debate, and I think we would do well to listen to what it’s saying now.
Joe D’Aleo of ICECAP also wrote some interesting things which I’ll reprint here.
…we have had a 0 sunspot calendar month (there have been more 30 day intervals without sunspots as recent as 1954 but they have crossed months). Following is a plot of the number of months with 0 sunspots by year over the period of record – 23 cycles since 1749.

See larger image here.
Note that cluster of zero month years in the early 1800s (a very cold period called the Dalton minimum – at the time of Charles Dickens and snowy London town and including thanks to Tambora, the Year without a Summer 1816) and again to a lesser degree in the early 1900s. These correspond to the 106 and 213 year cycle minimums. This would suggest that the next cycle minimum around 2020 when both cycles are in phase at a minimum could be especially weak. Even David Hathaway of NASA who has been a believer in the cycle 24 peak being strong, thinks the next minimum and cycle 25 maximum could be the weakest in centuries based on slowdown of the plasma conveyor belt on the sun.
In this plot of the cycle lengths and sunspot number at peak of the cycles, assuming this upcoming cycle will begin in 2009 show the similarity of the recent cycles to cycle numbers 2- 4, two centuries ago preceding the Dalton Minimum. This cycle 23 could end up the longest since cycle 4, which had a similar size peak and also similarly, two prior short cycles.

See larger image here.
Will this mean anything for climate in our near future? Possibly.  But we’ll have to wait to see how this experiment pans out.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c91240c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

During the 2017 presidential campaign, then‐​candidate Donald Trump was open about his hostility toward Iran and his disdain for the Obama administration’s diplomacy with that country. Since January, the Trump administration has been engaged in an Iran policy review. News reports and leaks suggest the review is highly likely to recommend a more confrontational approach to Iran, whether within the framework of the Iranian nuclear deal or by withdrawing from it. This paper examines the costs of four confrontational policy approaches to Iran: sanctions, regional hostilities, “regime change from within,” and direct military action.



Increased economic sanctions are unlikely to succeed in producing policy change in the absence of a clear goal or multinational support. Indeed, sanctions on Iran are likely to meet with strong opposition from U.S. allies in Europe and Asia, who continue to support the nuclear deal. The second policy we examine — challenging Iranian proxies and influence throughout the Middle East — is likewise problematic. There is little coherent, effective opposition to Iran in the region, and this approach increases the risks of blowback to U.S. forces in the region, pulling the United States deeper into regional conflicts.



The third option, so‐​called regime change from within, is a strategy that relies on sanctions and on backing for internal Iranian opposition movements to push for the overthrow of the regime in Tehran. This approach is not feasible: regime change — whether covert or overt — rarely succeeds in producing a stable, friendly, democratic regime. The lack of any good candidates for U.S. support inside Iran compounds this problem. The final policy alternative we explore is direct military action against Iranian nuclear or military facilities. Such attacks are unlikely to produce positive outcomes, while creating the risk of substantial escalation. Worse, attacking Iran after the successful signing of the nuclear deal will only add to global suspicions that the United States engages in regime change without provocation and that it cannot be trusted to uphold its commitments.



We suggest an alternative strategy for the Trump administration: engagement. This approach would see America continue to uphold the nuclear deal and seek continued engagement with Iran on issues of mutual interest. Engagement offers a far better chance than confrontation and isolation to improve Iran’s foreign policy behavior and empower moderate groups inside Iran in the long term.



In July 2015, the P5+1—the United States, United Kingdom, France, Russia, China, and Germany—reached a diplomatic agreement with Iran to roll back and significantly limit the Iranian nuclear program in exchange for the lifting of economic sanctions. The Joint Comprehensive Plan of Action (JCPOA) was the result of years of meticulous diplomatic negotiations and represented an historic compromise between two long‐​standing adversaries, the United States and Iran. At the time, the Obama administration presented the agreement as a strict nonproliferation agreement that would extend Iran’s so‐​called breakout time—the time it would take Iran to “sprint” to the creation of a useable nuclear weapon—from a few months to a year or longer. Many also hoped that the JCPOA could help to reduce bilateral tensions and quiet calls for U.S. military action against Iran for the foreseeable future. The unexpected election of Donald Trump in 2016 dashed these hopes. With renewed tensions and open debate within the Trump administration as it conducts a “comprehensive review of our Iran policy,” the future of the JCPOA and of U.S.-Iranian relations is uncertain. 1 There are certainly many options for the Trump administration if it wishes to take a more confrontational approach to Iran, four of which are examined in this paper. Yet each is difficult, costly, and carries far higher risks than continuing a policy of engagement.



The JCPOA has been successful, placing strong restrictions on Iran’s ability to engage in even peaceful nuclear development. Iran removed 98 percent of its stockpile of enriched uranium, dismantled two‐​thirds of its uranium enrichment centrifuges, disassembled the core of its heavy water reactor (a potential source of weapons‐​grade plutonium), and converted two major enrichment sites into peaceful research facilities. In addition, Iran agreed to engage in uranium enrichment exclusively at a single facility—the Natanz complex—and to produce only low‐​enriched uranium for 10 years. Because uranium must be enriched to 90 percent for use in a nuclear weapon, Iran’s agreement to restrict enrichment to 3.67 percent constitutes a significant barrier to weapons development. Iran also agreed to limit its stockpile of low‐​enriched uranium to 300 kilograms for 15 years, making it extremely difficult to covertly enrich excess material. 2



To ensure compliance with the JCPOA’s restrictions, Iran agreed to submit what remained of its nuclear program to what Georgetown University’s Ariane Tabatabai describes as “the most intrusive inspections regime ever voluntarily agreed to by any party.” 3 International monitors perform daily inspections of all of Iran’s declared facilities, with some facilities subject to 24‐​hour video surveillance. As critics note, these inspections and many of the deal’s other restrictions eventually expire, phased out over the next 10 to 25 years. 4 As part of the deal, however, Iran rejoined the Nuclear Nonproliferation Treaty (NPT) and ratified its Additional Protocol, a provision that mandates inspections of Iran’s civilian nuclear facilities. In doing so, Iran made a commitment to never become a nuclear weapons state and agreed to monitoring under the NPT indefinitely, far beyond the life of the JCPOA.



Indeed, more than two years after the adoption of the JCPOA, Iran is in full compliance with the deal. Though there has been some debate about the interpretation of certain issues—Iranian missile testing and the extent of U.S. sanctions relief—the deal continues to be implemented by both sides. As of this writing, the International Atomic Energy Agency (IAEA) has reported eight times that Iran is meeting its obligations under the deal. 5 Even the Trump administration, despite public denigration of the agreement, has formally certified that Iran is fulfilling its JCPOA commitments. In exchange, economic sanctions related to Iran’s nuclear program have been lifted, including United Nations and European Union sanctions on Iran’s energy sector and a variety of U.S. secondary sanctions related to Iran’s financial and energy sectors. 6 In addition, Iran has regained access to wealth stored in offshore banks previously interdicted by sanctions. 7



Nonetheless, the change in presidential administration has altered the political climate surrounding the nuclear deal in Washington, D.C. There have been prominent calls from both within the Trump administration and outside it to kill the JCPOA. As a candidate, Donald Trump himself repeatedly boasted that his “number‐​one priority is to dismantle the disastrous deal with Iran,” which he described in his typical hyperbole as “the worst deal ever negotiated.” 8 The recertification process (required every 90 days) has become increasingly politicized as a result: in July 2017, some advisers persuaded the president to refuse certification of Iran’s compliance with the JCPOA, only for other advisers to succeed in persuading him, at the last minute, to accept the IAEA’s conclusions and certify compliance. 9 Trump told journalists following the episode that he intends not to repeat the incident, reportedly informing White House staff that “he wants to be in a place to decertify 90 days from now and it’s their job to put him there.” 10 As David S. Cohen, former deputy director of the Central Intelligence Agency (CIA), notes, President Trump’s “reported demand for intelligence to support his policy preference to withdraw from the Iran nuclear deal risks politicizing intelligence analysis, with potentially grave consequences.” 11



Calls to end the deal have also come from outside the administration. In July, Sens. Tom Cotton (R-AR), Ted Cruz (R-TX), David Perdue (R-GA), and Marco Rubio (R-FL) wrote a letter to Secretary of State Rex Tillerson to “urge that you not certify … that Iran is complying with the terms of the [JCPOA].” 12 John Bolton, United Nations ambassador under George W. Bush and an early candidate to be Trump’s secretary of state, called for bombing Iran’s nuclear facilities months before the JCPOA was signed. 13 In July 2017, he wrote, “withdrawing from the JCPOA as soon as possible should be the highest priority.” 14



Opponents of the deal have little factual basis for their arguments: the IAEA has repeatedly found Iran in compliance with the deal’s restrictions, and the Joint Commission of the JCPOA has not identified any violations. 15 Instead, opponents typically argue that Iran is violating the “spirit” of the deal, pointing to Iran’s ballistic missile tests or its support for violent groups throughout the Middle East. 16 Yet the JCPOA was narrowly written specifically to exclude non‐​nuclear questions; it was never intended to solve all problems in the U.S.-Iranian relationship. Ironically, if any JCPOA signatory is in violation of the deal, it may be the United States. 17 At the G-20 summit in July, President Trump reportedly urged fellow world leaders to stop doing business with Iran, an action that violates the American commitment under the JCPOA to “refrain from any policy specifically intended to directly and adversely affect the normalization of trade and economic relations with Iran.” 18



President Trump appears determined to undermine the JCPOA. The administration is considering using the deal’s “snap inspections” provision—which allows inspectors to demand access to undeclared sites in Iran reasonably suspected of illicit enrichment activity—to make Iran appear noncompliant. 19 In the absence of any clear evidence of illicit enrichment activity, Iran would likely decline the Trump administration’s demand to inspect undeclared military sites, allowing the White House to portray Iran as violating the deal. As Mark Fitzpatrick, executive director of the International Institute for Strategic Studies, notes, this approach is “the route that White House political operatives suggest as a way to meet President Trump’s pre‐​determination not to again certify that Iran is in compliance, even when the facts clearly say otherwise.” 20 This approach also plainly misuses the relevant provisions of the JCPOA: as Daryl Kimball, director of the Arms Control Association put it, the Iran deal’s “special access provisions were designed to detect and deter cheating, not to enable [a] false pretext for unraveling the agreement.” 21 The administration appears to be simply “seeking trumped up reasons to sink [the] Iran deal.” 22



The Trump administration’s approach to Iran approximates the Bush administration’s approach to Iraq in the lead up to the 2003 invasion. Fitzpatrick compares the two situations, noting that “unfounded assumptions, false claims, and ideologically‐​tinged judgements are driving a confrontational approach that could well lead to another war in the Middle East.” 23 As in the case of Iraq, the risk exists for politicization of intelligence findings. As Steve Andreasen and Steve Simon, both former members of the National Security Council, describe in a recent op‐​ed in the _New York Times_ : “It’s a good bet that [administration officials] will cherry‐​pick facts to give the president what he wants: an excuse to scuttle the Iran deal.” 24



President Trump’s commitment to a harder line against Iran—independent of the nuclear deal—is obvious, though the Trump White House’s vicious internal power struggles suggest clear differences inside the administration on the best approach. In June, for example, the _New York Times_ reported that the administration was ramping up a covert action program against Iran, and that “Mr. Trump has appointed to the National Security Council hawks eager to contain Iran and push regime change, the groundwork for which would most likely be laid through CIA covert action.” 25 Yet Trump’s National Security Adviser H. R. McMaster fired the council’s former senior director for intelligence, Ezra Cohen‐​Watnick, in August. Cohen‐​Watnick had previously expressed to administration officials “that he wants to use American spies to help oust the Iranian government.” 26 Along with Derek Harvey, who was the administration’s top Middle East official on the National Security Council, Cohen‐​Watnick had also advocated broadening U.S. involvement in the Syrian civil war as a means of pushing back against Iran. McMaster likewise fired Harvey in July 2017.



Prominent Iran hawks remain in the administration, and some go well beyond arguing for abrogating the JCPOA to make the case for a regime change policy toward Iran. In June, Tillerson testified before the House Foreign Relations Committee that the administration intended to “work toward support of those elements inside of Iran that would lead to a peaceful transition of that government,” 27 though other high‐​level administration officials have denied this is current policy. 28 While he was a member of Congress in 2016, Trump’s current CIA director, Mike Pompeo, publicly called for the United States to “change Iranian behavior, and, ultimately, the Iranian regime.” 29 Senator Tom Cotton (R-AR)—known to be close to the Trump administration—likewise has stated that “the policy of the United States should be regime change in Iran.” 30 Defense Secretary James Mattis as recently as June described Iran as “the most destabilizing influence in the Middle East.” 31



Outside the federal government, other hawkish voices have also made forceful calls for regime change. Soon after Trump was inaugurated, the well‐​connected conservative think‐​tank Foundation for the Defense of Democracies (FDD) submitted a memo to Trump’s National Security Council that argued for “coerced democratization” in Iran, a euphemism for regime change. 32 John Bolton said in a speech in July, “The behavior and the objectives of the regime are not going to change, and therefore the only solution is to change the regime itself.” 33



The debate on Iran in Washington today includes many options, some—though not all—of which begin with killing the JCPOA. Deliberately scuttling the JCPOA would have negative ramifications. The international community and Iran, recognizing U.S. intransigence, could conceivably continue to uphold the nuclear deal without the United States, isolating the United States from allies and handicapping its pursuit of unrelated diplomatic initiatives, notably the question of North Korea’s nuclear program. Alternatively, U.S. termination of the JCPOA could motivate Iran to unburden itself from the deal’s restrictions, expel international monitors, and begin once again to pursue a nuclear weapons capability in earnest. Either possibility puts the United States in a weaker, more dangerous position. Given the momentum in Washington behind pursuing a more hostile approach toward Iran, this policy analysis will explore the likely costs and consequences of four different approaches to confronting Iran, whether as alternatives to the JCPOA or supplementary to it.



The first approach we assess is applying economic pressure in the form of ratcheting up sanctions on Iran, including those the international community agreed to lift under the JCPOA. The second approach looks at the options for challenging Iranian influence in the Middle East, particularly its proxies in Iraq and Syria. The third approach considers the viability of what is called “regime change from within,” where the United States would support internal opposition groups in an effort to undermine or overthrow the government in Tehran. The fourth and final approach we evaluate is military action against Iran, most likely in the form of limited airstrikes against Iranian nuclear or other military facilities. We conclude by proposing a fifth strategy for the Trump administration: uphold U.S. commitments under the JCPOA, refrain from adding new sanctions, and engage with Tehran where U.S. and Iranian interests overlap. There is no silver bullet that can solve the problems in the U.S.-Iranian relationship, but continued engagement carries lower costs and a higher chance of success than any of the other approaches examined here.



Opponents of the JCPOA frequently argue that they could negotiate a better deal through the aggressive use of U.S. sanctions. These sanctions would be extraterritorially applied, forcing European companies to adhere to U.S. law, in theory making Iran willing to concede more of its nuclear program or to make other security and governance concessions. For example, former Connecticut senator Joe Lieberman proposed in December that President Trump “designate the entire Iranian Revolutionary Guards Corps as a foreign terrorist organization … support legislation in Congress punishing sectors of the Iranian economy … propose measures to curb Iranian access to U.S. dollars … and then to walk away, with cause, from the JCPOA.” 34 Such arguments are not restricted only to those who wish to abrogate the JCPOA. Various authors argue that while there are no grounds to “tear up” the deal, the president and Congress should nonetheless seek to impose new sanctions on Iran related to its regional activities and support for the Assad regime in Syria.



Indeed, Congress has already acted in this regard, passing an extensive sanctions bill in July 2017, including North Korean, Russian, and new Iranian sanctions. The bill, “Countering America’s Adversaries through Sanctions Act,” targets a number of new individuals and entities—particularly in relation to Iran’s ballistic missile program—and includes an arms embargo and several new reporting requirements. 35 Congress made last minute changes to the bill to ensure that it did not technically violate the JCPOA, 36 yet as Senator Bernie Sanders (I-VT) pointed out when justifying his vote against the bill: “I believe that these new sanctions could endanger the very important nuclear agreement that was signed between the United States, its partners, and Iran in 2015. That is not a risk worth taking.” 37 Sanders is correct; new sanctions on Iran for its missile programs and human rights abuses raise tensions within the framework of the JCPOA while adhering to the narrowest possible definition of its terms. In response to the new sanctions bill and the threat of further sanctions, Iranian leaders voted to increase the state’s military budget and threatened to restart the nuclear program, highlighting the escalatory potential of new sanctions. 38



Opponents of the JCPOA support the imposition of new sanctions, particularly the designation of the Islamic Revolutionary Guard Corps (IRGC) and IRGC‐​associated businesses, often with little regard for whether new sanctions could torpedo the deal or worsen relations. Council on Foreign Relations Senior Fellow Ray Takeyh has repeatedly said renewed sanctions are the first step in a broader strategy of pressure on Iran, arguing that “we must return to the days of warning off commerce and segregating Iran from global financial institutions. Designating the Revolutionary Guards as a terrorist organization and reimposing financial sanctions could go a long way toward crippling Iran’s economy.” 39 Likewise, the editors of the conservative _National Review_ advised the Trump White House to abrogate the deal through sanctions: “Better to declare an end to this diplomatic farce … and establish a robust sanctions regime that might actually force Tehran to change its ways.” 40



The central problem with this option—whether as a replacement for the JCPOA or in addition to it—is the utter lack of international support. Though often overlooked, the JCPOA is in reality a multinational arms control agreement, negotiated by the P5+1, the five permanent members of the United Nations Security Council, plus Germany. The other parties to the deal have been unequivocal in affirming that Iran is indeed abiding by its commitments under the deal. On August 3, a spokeswoman for European Union foreign policy chief Federica Mogherini told a press conference: “So far, we consider that all parties have been implementing their commitments under the deal.” 41 Sergei Lavrov, Russian foreign minister, likewise confirmed Iran’s compliance and questioned the Trump administration’s motives, saying in August that the Trump administration “continue[s] calling these agreements wrong and erroneous, and it’s a pity that such a successful treaty is now somewhat being cast into doubt.” 42



European support for the deal is strong. As Carl Bildt, former prime minister of Sweden, noted in an opinion piece in August, canceling the deal would be a nonstarter in Europe: “Europe would certainly not go along with this, for one because it would risk undercutting the elaborate inspections systems that the agreement depends on. But primarily because Europe has seen that the deal actually works … and Europe has absolutely zero appetite for a new cascade of conflicts in a region on its doorstep.” 43 As a result, European leaders are also keen to prevent the imposition of further non‐​nuclear U.S. sanctions that could potentially undermine the deal. Indeed, on July 11, Mogherini told reporters: “The nuclear deal doesn’t belong to one country; it belongs to the international community. We have the responsibility to make sure that this continues to be implemented.” 44



It is unlikely that any additional U.S. sanctions would be successful without multinational support. The United States has long had an extensive array of sanctions focused on Iran, including on weapons procurement and development, U.S.-Iranian trade, and terrorist financing. Yet the long‐​term effect of these sanctions on the Iranian economy was relatively minimal prior to 2005. Technology sanctions have undoubtedly been successful in slowing progress on nuclear and missile‐​related projects but have done little to impact Iran’s import and development of conventional weapons. 45



Two changes in the mid‐​2000s substantially increased the efficacy of sanctions on Iran. First, the Treasury department aggressively pursued a strategy of outreach, lobbying (and threatening) foreign banks to ensure that U.S. sanctions would be adhered to extraterritorially. Second, the European Union decided in 2012 to embargo Iranian oil exports. This decision was motivated by increasing concerns over Iran’s nuclear program, even though it was politically and economically costly for the Europeans. In 2010 alone, Iran’s exports to the EU totaled $19 billion, 90 percent of which were energy related. 46 By March 2013, Iran’s oil exports had dropped from 2.5 million barrels per day to 1 million barrels per day, resulting in an Iranian budget deficit of $28 billion that year. 47 While U.S. sanctions alone were relatively ineffectual, these punitive economic costs helped to drive Iran to the negotiating table.



Proponents of increased sanctions therefore typically advocate for more assertive enforcement of secondary sanctions penalties against European and Asian companies. A recent report from the Washington Institute for Near East Policy, for example, called for the United States to step up the extraterritorial enforcement of existing sanctions on terror financing and IRGC‐​affiliated companies, arguing that enforcement and public warnings could discourage European companies from re‐​entering the Iranian market. As Stuart Levey, at the time undersecretary for terrorism and financial intelligence, described the use of extraterritorial sanctions prior to the JCPOA: “Those who are tempted to deal with targeted high‐​risk actors are put on notice: if they continue this relationship, they may be next.” 48 Yet the decision to sanction Iran was costly for European companies. A number of companies, most notably French energy company Total, which signed a $5 billion investment deal with Iran and with China’s National Petroleum in July to develop the South Pars gas field, have begun to re‐​enter the market following the successful conclusion of the JCPOA. 49 In the absence of any concrete evidence of Iranian cheating on the deal, European and Asian governments are likely to push back strongly against new U.S. barriers to trade and investment in Iran, and on the excessive extraterritorial application of existing sanctions.



Another problem with sanctions is that they are rarely successful in producing policy change. Indeed, though targeted sanctions may impose costs on the targeted regime, it is less clear that these costs actually produce policy change. 50 Proponents of increased sanctions point to high profile successes like the JCPOA, while skeptics point to the many cases, from Syria to Zimbabwe, where sanctions have failed to produce policy change. More broadly, academic studies have repeatedly shown sanctions to be ineffective in achieving policy change. As Arne Tostensen and Beate Bull note in the journal _World_ _Politics_ , “The voluminous literature that has accumulated over the years tends to conclude that sanctions are rarely effective, even though exceptions have been documented.” 51 In one of the earliest broad‐​based studies of comprehensive sanctions, for example, researchers found an average sanctions success rate of only 34 percent. 52 Even the research on more recent “smart sanctions,” which are presumed to be more effective thanks to their “targeted” nature, shows that they are also largely ineffective. A wide‐​ranging study of United Nations targeted sanctions found them to be effective in only 10–20 percent of cases, 53 while another survey of post‐​9/​11 U.S. sanctions found them to be effective in only 36 percent of cases. 54



Policy change is especially unlikely when sanctions do not have clear, attainable goals or when the issue is of prime national security importance to the target state. 55 Sanctions focused on economic issues such as trade often seem to be qualitatively different than those focused on security. 56 When University of Chicago’s Robert Pape examined sanctions as an alternative to the use of force, he found they had only been successful in around 5 percent of national security–related cases. 57 Sanctions also tend to fail when they are unilateral; as the Washington Institute’s Katherine Bauer notes, even with the power of U.S. extraterritorial sanctions, “there are limits to U.S. jurisdiction and the ability to compel foreign compliance.” 58 Further sanctions on Iran thus fall into a worst‐​case scenario: security‐​focused sanctions with no clear goals other than securing “a better deal” or weakening the Iranian regime. In the absence of strong support from European or other Security Council nations, there is very little chance that further sanctions will compel Iran’s leaders to capitulate.



An alternative option is a deliberate strategy of challenging Iranian proxies throughout the Middle East. That option would not necessarily require the Trump administration to abrogate the JCPOA. Indeed, as Brookings Institution Senior Fellow Daniel Byman recently noted in congressional testimony: “Because the JCPOA … has put Iran’s nuclear program on the back burner, there is an opportunity to focus on Iran’s support for militant groups and other problems Iran causes in the region.” 59 This approach runs counter to Washington’s current regional strategy: though there are arenas where the United States is engaged in hostilities with Iranian‐​associated proxies—such as U.S. support for the Saudi‐​led campaign in Yemen—America’s anti‐​ISIS campaign typically means that it is de facto fighting on the same side as Hezbollah and other Shi’a militias. The most moderate alternative proposals call for U.S. support for regional allies, such as military and diplomatic support for a peace settlement in Yemen designed to split the Houthi rebels from Tehran’s limited support. 60 Other options include increased maritime presence to help disrupt Iranian arms shipments. 61 Still others call for building the capacity of regional actors: one recent report from the Center for a New American Security suggests maintaining U.S. influence in Iraq and increasing U.S. logistical support for the conflict in Yemen, in hopes of marginalizing Iranian influence in those conflicts. 62



However, there are also a variety of more aggressive proposals. Two senior former administration officials on the National Security Council, Derek Harvey and Ezra Cohen‐​Watnick, were reportedly in favor of direct U.S. military action against Iranian proxies in Syria. 63 Escalating clashes between U.S. troops and militias in southern Syria in recent months, including U.S. airstrikes on several militias, suggest that such clashes will happen even in the absence of a formal policy change. Several recent policy papers also make the argument for a more formalized anti‐​Iran strategy in Syria, often using proxies to challenge Iranian‐​allied groups. The Washington Institute’s Nader Udowski, for example, argued in June 2017 for “a new U.S. policy, the chief component of which should be a strategy targeting Iran’s Quds force and its Shi’a militias.” 64 Similarly, Max Peck of the Foundation for Defense of Democracies has argued that the Trump administration should seek to codify in law that the United States seeks the overthrow of the Assad regime in Syria, and “increase the costs of Iran’s engagement by maintaining the pressure on Assad … through its support for the armed opposition.” 65



Perhaps the most bellicose option is actively increasing U.S. participation in the war in Syria and Iraq. A report from the Institute for the Study of War (ISW) called for the United States to “seize and secure a base in southeastern Syria … create a de facto safe zone … then recruit, train, equip, and partner with local Sunni Arab anti‐​ISIS forces.” The report called for American troops to “fight alongside” these forces. 66 The goals would include not only “defeating al Qaeda, as well as ISIS,” but also “expelling Iranian military forces and most of Iran’s proxy forces from Syria.” This strategy extends to Iraq: as a follow‐​on report argued, America should also “take urgent measures to strengthen Iraqi Prime Minister Abadi,” and work to minimize Iranian influence in Iraq. 67 Though the extent of American military involvement varies widely across these proposals, they all share a common theme: direct or indirect military action against Iranian proxies in Syria, Iraq, Yemen, and elsewhere.



The central problem with this approach is that there is no coherent anti‐​Iranian axis in the Middle East to rely upon in a campaign to challenge Iranian influence in the region. Indeed, observers have often described the region using sectarian narratives—portraying conservative Sunni states in conflict with Iran’s more revolutionary Shi’a axis—that are largely exaggerated.



For example, despite Saudi efforts to form a united regional front against Iran, the conflicts of the Arab Spring have frequently seen the states of the Gulf Cooperation Council (GCC) act against each other’s interests. 68 In Syria, the conflict between Saudi and Qatari proxies helped to radicalize and doom the anti‐​Assad opposition, while a Qatari‐​Emirati rivalry fueled the Libyan conflict. Today’s GCC crisis only serves to highlight this problem: though clearly motivated by a desire to rein in Qatar’s independent foreign policy, the Saudi and Emirati embargo has in reality driven Qatar closer to Iran and Turkey, undermining a common GCC front. 69



Other regional attempts to form anti‐​Iranian movements have likewise failed. A widely‐​publicized Saudi Arabian attempt in December 2015 to create an Islamic Military Alliance to fight terrorism—which pointedly included no Shi’a majority states—has largely failed to develop since that time. 70 Nor is there any guarantee that regional partners will actually promote U.S. interests if the United States increases its support; the actions of allies in the region have all too often served to destabilize and worsen conflicts in Syria, Yemen, and elsewhere, rather than improve them.



Indeed, the lack of a solid anti‐​Iran coalition among existing U.S. partners—capable of achieving America’s often expansive foreign policy goals—is a key reason why the most extreme options for regional confrontation with Iran often involve fabricating an effective anti‐​Iranian bloc from whole cloth, whether that is the creation of a “credible and moderate Syrian opposition,” a regional “multinational Joint Task Force with Arab partners targeted at countering … the IRGC,” or “a new Syrian Sunni Arab partner … to conduct population‐​centric counterinsurgency.” 71 Each of these options is likely to fail. Previous U.S. efforts to create regional coalitions to fight terror groups have been largely unsuccessful. The 2014 collapse of the Iraqi army in the face of ISIS advances is also a salutary lesson; years of training commitments and substantial blood and treasure on the part of the U.S. military were not enough to overcome deeper societal problems like corruption. 72 Without coherent, effective local proxies, and given the major political differences that divide U.S. regional allies, any attempt to build an anti‐​Iranian force or coalition in the region is likely to falter.



A strategy of regional pushback against Iran is also likely to pull the United States more deeply into a variety of regional conflicts and increase the risks of blowback to U.S. troops in the region. The United States is already heavily overcommitted in the Middle East, with tens of thousands of troops engaged in conflicts in Iraq, Syria, Afghanistan, Libya, and Yemen, and stationed at permanent bases elsewhere throughout the region. Indeed, despite the Obama administration’s attempts to draw down American commitments to Middle Eastern conflicts, the number of troops engaged in fighting Middle East conflicts has been increasing again since 2014. 73 A stepped‐​up campaign against Iranian proxies throughout the region will require further troop increases, both in direct combat roles and to train and support local forces.



It is these troops who will bear the brunt of any Iranian military response to this strategy. Several hundred U.S. troops were killed by Iranian‐​associated groups in Iraq during the post‐​invasion occupation, a number likely to rise in any new conflict with these groups. 74 And while Hezbollah has been largely occupied in recent years with fighting on behalf of the Assad regime, if faced with a concerted campaign against it by U.S.-allied forces, it is likely to respond with the kind of asymmetric attacks that have characterized their long‐​running conflict with Israel. 75 Indeed, one potential response to a concerted attack on Iranian proxies throughout the region is retributive attacks on Israel; during the 2006 war, Hezbollah enjoyed substantial success against Israeli forces, disabling a number of tanks and even an Israeli warship. 76 The potential for Iranian retaliation against U.S. troops, regional partners, or shipping in the region suggests that a strategy of regional confrontation with Iran will not make the region safer or more stable, but will instead introduce additional conflict and uncertainty.



Another possible option for dealing with Iran is an explicit U.S. policy of regime change. This is not a new idea; for decades, hawks in Washington have called for regime change in Tehran. Justifications have ranged from the 1979 hostage crisis to Iran’s nuclear program in the mid‐​2000s to the anti‐​regime protests known as the Green Revolution after 2009. 77 Yet the failure of U.S. regime change campaigns in both Iraq and Libya to produce a stable, democratic state has led most proponents of regime change to back away from overt military options and instead suggest that the Trump administration pursue “coerced democratization” or “regime change from within.” In this approach, the United States would pressure the Iranian regime and simultaneously back groups that oppose it—whether the exiled extremist National Council of Resistance of Iran (NCRI), pro‐​democracy Green Revolution factions, or ethnic minorities within Iran—a strategy advocates often compare to Reagan’s support for civil society groups in the Soviet Union. As Reuel Gerecht and Ray Takeyh argue in a _Washington Post_ op‐​ed: “Today, the Islamist regime resembles the Soviet Union of the 1970s … if Washington were serious about doing to Iran what it helped to do to the U.S.S.R., it would seek to weaken the theocracy by pressing it on all fronts.” 78



Another proponent of “coerced democratization,” the Foundation for Defense of Democracies’ Mark Dubowitz, urged President Trump to “go on the offensive against the Iranian regime” by “weakening the Iranian regime’s finances” through “massive economic sanctions,” while also “undermin[ing] Iran’s rulers by strengthening pro‐​democracy forces” inside Iran. 79 This option appears to be gaining traction in the Trump administration’s ongoing Iran policy review and has received public support from Tillerson. CIA Director Mike Pompeo also favored such an approach during his time in Congress. Yet there are important reasons to doubt that such a strategy would actually yield constructive results in Iran or benefit U.S. national interests.



Regime change often fails, particularly when it is covert. According to one study of covert regime change operations by the United States during the Cold War, such efforts succeeded only one‐​third of the time. 80 Indeed, as an administration official said in August, “With Iran, they are looking at regime change but coming up empty. There are no good plans, no decapitation strikes possible.” 81 Arming or funding for local insurgencies also rarely succeeds; a leaked CIA report commissioned in 2012 found that most past attempts to covertly arm insurgencies had minimal impact on long‐​term outcomes and often backfired. 82



Even when successful in unseating one government and establishing another in its place, foreign‐​imposed regime change “generally does not improve relations between interveners and targets. Rather, it often makes them worse,” according to Georgetown University’s Alexander B. Downes and Boston College’s Lindsay A. O’Rourke. 83 Changing the leadership of a state typically fails to alter that country’s perception of its interests, and foreign‐​imposed regimes tend to diverge from the preferences of the intervener as they begin to face domestic political pressures. Contrary to the depiction of many regime change advocates, the Iranian regime enjoys substantial public support, and the population would not welcome a U.S.-imposed government. Any new regime that tried to implement policies that reflect U.S. interests instead of Iranian interests would “attract the ire of domestic actors,” leading to an unstable government viewed as illegitimate by the population. 84



Research shows that “when a country overthrows another’s government, it increases the likelihood of civil wars and usually doesn’t establish a democracy.” 85 The recent experiences of the United States in Iraq, Afghanistan, and Libya only confirm this finding. Sixteen years of U.S. military presence have done little to stabilize war‐​torn Afghanistan. 86 The war in Iraq essentially destroyed the Iraqi state, killing hundreds of thousands of Iraqis and displacing millions more. More than 4,400 U.S. troops were killed in combat, and more than 30,000 were wounded, with direct costs estimated to exceed $2 trillion and indirect costs as high as $4 trillion. 87 A widespread insurgency and civil war led to the rise of the Islamic State, prompting further U.S. intervention to fight against the group. In Libya, the U.S. choice to overthrow the regime of Muammar Gaddafi on humanitarian grounds resulted in a lengthy civil war and the deaths of more Libyans than would likely have perished without the intervention. 88 The likelihood of successful regime change and a subsequent stable, democratic state in Iran are vanishingly small.



Though regime change proponents highlight a variety of groups inside Iran as potential candidates for U.S. support, none are truly viable. The exiled opposition group Mujahideen‐​e‐​Khalq (MEK) (or its political wing, the NCRI) is one such example. The MEK began in the 1960s and 1970s as a paramilitary Marxist‐​Islamic resistance group opposed to the former Shah of Iran, the authoritarian ruler put in power following a 1953 coup sponsored by the United States and Great Britain. The group allied with Saddam Hussein during the 1980s Iran‐​Iraq War, and analysts widely agree that it is an undemocratic group that has no popular support inside Iran. 89 Indeed, the MEK has largely tried to win external support for its agenda of regime change in Iran. Until 2012, it was even designated a terrorist organization by the U.S. State Department and had lobbied hard over the years to win support from prominent current and former U.S. officials to have that designation removed. 90 It has won primarily the support of those who favor a hardline approach to Iran, such as former CIA directors James Woolsey and Porter Goss, former New York City Mayor Rudolph Giuliani, former governors Howard Dean and Ed Rendell, former U.N. Ambassador John Bolton, and former House Speaker and close Trump confidant Newt Gingrich. Yet in the absence of popular support outside certain Washington circles, backing the group in a bid to overthrow the Iranian regime would likely fail. 91



Regime change advocates also suggest supporting the so‐​called Green Movement that emerged amid the protests over the contested Iranian presidential elections in 2009. Unfortunately, according to Ariane Tabatabai and Madison Schramm, the Green Movement “essentially faded away a few months after the elections” and “was never a cohesive faction.” 92 Green Movement leaders Mir Hossein Mousavi and Mehdi Karroubi remain under house arrest in Iran today, and have made clear that their goal was to dispute the 2009 election results, not to overthrow the government. In fact, the best hope for the Green Movement is to avoid association with the United States; whatever popular support it continues to have would quickly evaporate with any whiff of U.S.-backing for regime change. As Michael Axworthy of the University of Exeter writes, “Given the long history of foreign meddling in the country (the CIA‐​inspired coup that removed Prime Minister Mohammad Mosaddeq in 1953 is just one example), any suspicion of foreign backing is political poison in Iran.” 93



The third option—seeking to stoke discontent among Iran’s minority populations—is similarly infeasible. Iran’s ethnic minorities include Kurds (10 percent), Baluchis (2 percent), Arabs (2 percent), and Azeri Turks (16 percent). 94 But Iran is not a country beset by ethnic, cultural, and religious cleavages in the way the former Yugoslavia was. Neighboring Iraq, with its mix of Shia, Sunni, and Kurds, was a comparatively disjointed state held together by a powerful centralized dictatorship. Iran is very different. Any strategy that seeks to foment political upheaval in Iran via these various minority groups ignores the fundamental cohesion that characterizes Iran as a national unit. 95 If anything, such an approach would be more likely to bolster Iranian nationalism than to subvert it. As Vali Nasr, dean of the Johns Hopkins School of Advanced International Studies and an Iranian‐​American, told the _New Yorker_ in 2008, “Iran is an old country—like France and Germany—and its citizens are just as nationalistic. The U.S. is overestimating ethnic tension in Iran … working with the minorities will backfire, and alienate the majority of the population.” 96



Direct military action against Iran is the least likely of the options being considered under the Trump administration’s policy review. Indeed, the focus on nonmilitary options among Iran hawks is likely a response to the widespread distaste among the American public for engaging in another open‐​ended regime change war in the Middle East. Yet some have argued that the Trump administration should “rebuild military leverage over Iran,” including “contingency plans to neutralize Iran’s nuclear facilities,” engage in regional military exercises, and direct the U.S. navy to “fully and responsibly utilize rules of engagement to defend themselves and the Persian Gulf against rising Iranian harassment.” 97



There are various contingencies in which U.S. policymakers may face a decision on the use of military force against Iran, whether it is a purposeful strike against Iran’s nuclear facilities in the wake of U.S. withdrawal from the JCPOA, or a more gradual escalation following military confrontations in Syria, the Gulf, or elsewhere. As the Trump administration considers these options, however, it would do well to remember that the lack of good military options was the key reason behind the Bush and Obama administrations’ decision to pursue diplomacy with Tehran in the first place.



The United States should only undertake military action against another state if its core security interests are threatened. Yet there is no plausible near‐​term scenario in which Iran poses a direct threat to the U.S. homeland. Nor do Iranian actions in the Middle East pose a significant threat to U.S. interests in the region. Taking military action against Iran to thwart the purported threat of its nuclear program would harken back to the preventive war doctrine adopted by the Bush administration after the September 11th terrorist attacks and codified in the 2002 National Security Strategy. 98 Though proponents of military action often describe such action as “preemptive,” one RAND report notes that “generations of scholars and policymakers have defined preemption more restrictively,” limiting it to cases of imminent threat. 99 This is a crucial difference; as the authors highlight, international law holds that truly preemptive attacks are an acceptable use of force in self‐​defense, while preventive attacks are not. As the historian and former Kennedy administration adviser Arthur Schlesinger Jr. put it when criticizing the Bush administration’s case for war against Iraq, this doctrine of preventive war “is alarmingly similar to the policy that imperial Japan employed at Pearl Harbor, on a date which, as an earlier American president said it would, lives in infamy. Franklin D. Roosevelt was right, but today it is we Americans who live in infamy.” 100 With no imminent threat from Iran, there is no legal justification for direct military action.



At the very least, the Trump administration is constitutionally obligated to seek approval from Congress for any military action against Iran. Trump himself may disagree. He previously declined to seek or secure congressional authority for his missile strike on a Syrian military base controlled by the Assad regime in April 2017 and has repeatedly made public statements arguing that military action should be kept secret to preserve the tactical advantage of a surprise attack. If Trump does seek congressional approval for military strikes on Iran, he is likely to face strong opposition from many Democratic members of Congress and at least some Republicans. Senator Chris Murphy (D-CT) argued in February that “Trump and his most radical advisers are begging for war with Iran. This would be a disaster of epic scale, perhaps eclipsing the nightmare of the Iraq war.” 101 Congressional Democrats, already concerned about the administration’s domestic policy proposals, are unlikely to cut him a blank check on Iran.



Even small‐​scale military attacks on Iran—whether targeted strikes on nuclear facilities or clashes with Iranian forces in the Gulf or elsewhere—are likely to lead to escalation. In March 2012, the Pentagon held a classified war simulation “to assess the repercussions” of an Israeli attack on Iran’s nuclear facilities. The results showed that such a targeted strike would provoke immediate Iranian retaliation against U.S. military bases and naval assets in the region, drawing the United States into “a wider regional war.” 102 General James Mattis, now Trump’s secretary of defense, was then head of Central Command and supervised the war game. The _New York Times_ reported that Mattis told aides a strike “would be likely to have dire consequences across the region and for U.S. forces there.” Following a similar war game in 2004, retired Air Force Colonel Sam Gardiner concluded, “There is no military solution for the issues of Iran.” 103



It is not clear that a narrow or targeted strike is even possible. To strike Iran’s nuclear facilities, the United States would also need to bomb Iran’s air defense systems and command and control facilities, which itself carries risks of escalation. Writing in 2006, retired General Thomas McInerney suggested one such plan for attacking Iran’s nuclear facilities, requiring a massive commitment of 700 aircraft, 500 cruise missiles, and 28,000 bunker‐​buster bombs in the initial 36–48 hours. 104 Moreover, airstrikes of this kind, to accomplish any long‐​term objective, could not be limited to a single one‐​off mission. As explained in a 2012 study by the Iran Project, a nongovernmental organization founded to improve official contacts between the American and Iranian governments, for targeted strikes to “fulfill the stated objective of ensuring that Iran never acquires a nuclear bomb, the United States would need to conduct a significantly expanded air and sea war over a prolonged period of time, likely several years.” 105



Under bombardment from the world’s most dominant military superpower and uncertain of U.S. intentions, Iran would be likely to engage in retaliatory strikes against U.S. bases and military assets in Iraq, Syria, Bahrain, Qatar, and the United Arab Emirates. Iran’s Shahab‐​3 intermediate range ballistic missile can hit targets up to 2,000 kilometers away, while its Soumar cruise missile can potentially hit targets up to 2,500 kilometers away, meaning all U.S. forward‐​deployed bases in the Middle East and at least some bases in Europe are within range for conventional retaliation. 106 Likewise, the potential for asymmetric retaliation should not be underestimated. As Afshon Ostavar of the Naval Postgraduate School notes, “While Iran’s neighbors have poured billions of dollars into conventional weaponry, Iran has invested in comparatively cheap proxy forces that have proven effective in numerous theaters.” 107 Proxy groups such as Hezbollah or even Iran’s Quds force, a special unit of the IRGC, could engage in terrorist attacks against U.S. forces or allies in the region.



Anything beyond a limited military strike would have even more dire and counterproductive consequences. Taking military action to topple the Iranian regime, for example, would require a massive, lengthy, and costly military commitment. America’s experience in Iraq should be instructive in this context: Bush administration officials and their allies in the think‐​tank community and news media made bold predictions about the ease with which America would win the war, that Iraq would be reborn as a functioning democracy, and that the costs to the United States in lives and dollars would be minimal. These predictions proved wrong. In addition to bolstering Iran’s strategic position, the war helped to destabilize the region and to exacerbate America’s terrorism problem. A 2006 National Intelligence Estimate concluded that “the American invasion and occupation of Iraq … helped spawn a new generation of Islamic radicalism.” 108 The war had “become the ‘cause celèbre’ for jihadists, breeding a deep resentment of U.S. involvement in the Muslim world and cultivating supporters for the global jihadist movement.” 109



A large‐​scale ground war in Iran would be immensely damaging. Comparisons to Iraq are illuminating. The U.S. invasion was initially successful against a relatively ineffectual Iraqi military with approximately 389,000 men under arms. But U.S. forces have struggled in the years since to control territory, build a functioning Iraqi state, and deal with mass insurgency among the population of around 37 million. In comparison, Iran has a larger (about 523,000 active duty) and more effective military, a bigger population (80.3 million), and territory more than three times the size of Iraq. 110 A study by the Iran Project concluded: “If the United States decided to seek a more ambitious objective, such as regime change in Iran or undermining Iran’s influence in the region, then an even greater commitment of force would be required to occupy all or part of the country.… Given Iran’s large size and population, and the strength of Iranian nationalism, we estimate that the occupation of Iran would require a commitment of resources and personnel” greater than the costs of the wars in Afghanistan and Iraq combined. 111



A direct military attack on Iran, whatever the specific goals, is likely to be counterproductive in terms of nuclear nonproliferation. Military action short of regime change cannot eliminate Iran’s nuclear program or the knowledge behind its existence. 112 Given U.S. interventions in recent years, even targeted strikes may be seen by Tehran as a precursor to more intensive military action that must be deterred. A 2010 Defense Intelligence Agency study concluded that the main goal of Iran’s military strategy is regime survival, with a key focus on deterrence. 113 As Kenneth Pollack, a former CIA and National Security Council analyst, noted in 2006: “The Iraq example coupled with the North Korea example probably is part of the motivation for some in Iran to get a nuclear weapon.” 114 The 2011 U.S. intervention in Libya only intensifies this dilemma for Iran; Muammar Gaddafi voluntarily gave up his nascent nuclear program before being removed by a joint American‐​European intervention. Thus, while targeted strikes could delay Iran’s ability to develop nuclear weapons by destroying infrastructure, they would probably incentivize Iran to redouble its enrichment efforts under the conviction that only a nuclear deterrent can ensure its future survival.



This logic also implies broader strategic costs to an attack on Iran: it would exacerbate the problem of nuclear proliferation more generally. As the current Director of National Intelligence Dan Coats recently acknowledged at the Aspen Security Forum, U.S. actions against Saddam Hussein’s Iraq and Muammar Gadhafi’s Libya have made it clear to other states, like North Korea, that a nuclear deterrent may be the best way to ensure regime survival in the context of a war‐​prone United States. 115 North Korea itself confirmed this logic, releasing a statement after a 2016 nuclear test arguing that “the Saddam Hussein regime in Iraq and the Gaddafi regime in Libya could not escape the fate of destruction after … giving up nuclear programs of their own accord.” 116 As Nobel laureate Thomas Schelling has famously pointed out, American nonproliferation policies are ironically a prime driver of nuclear proliferation. 117 If, after successfully negotiating a nuclear deal, the United States then engages in an aggressive war against Iran despite Tehran’s full compliance with the JCPOA, other potential proliferators would have no reason at all to believe that the United States can be trusted to negotiate in good faith.



Though the Trump administration’s Iran policy review appears predestined to produce a more belligerent approach towards Iran, each of the options explored in this paper has significant flaws. Indeed, each option is unlikely to achieve its stated objectives, while at the same time creating an unacceptably high risk of exacerbating the very problems the Trump administration seeks to resolve. At a fundamental level, a more assertive U.S. policy towards Iran—whatever the details—will inevitably intensify Iranian fears about the country’s national security, worsening the very behaviors that the United States seeks to forestall. Even adopting one of these more hostile approaches to Iran while nominally upholding the JCPOA presents greater problems than embracing the nuclear deal and using it as a vehicle for further engagement designed to temper Iranian behavior.



As this paper highlights, it is doubtful that ratcheting up economic sanctions will alter Iranian policies in a more constructive direction, especially in the absence of international cooperation. Likewise, by pushing back harder against Iranian influence throughout the Middle East, the United States would incur substantial long‐​term costs in exchange for negligible gains in regional security. Moreover, a more aggressive approach could lead to unintended military escalation. Supporting internal opposition groups to pressure the regime or foment domestic upheaval is a hopeless strategy, given Iran’s domestic political realities and America’s long history of failed regime change endeavors. Finally, direct military action would have little public support, no legal basis, and most likely produce profoundly negative consequences for regional security and American interests.



Such actions would effectively return U.S.-Iranian relations to the cycle of enmity in which they were trapped prior to the negotiation of the JCPOA, with the nuclear issue dominating as a justification for continued hostility. Indeed, prior to the JCPOA, American allies in the region, particularly Saudi Arabia and Israel, often used the issue of Iran’s nuclear program to steer American policy toward Iran in a more confrontational direction. In private conservations with U.S. officials early in the Obama administration, then‐​king Abdullah bin Abdulaziz al‐​Saud pushed U.S. military action against the Iranian regime. 118 From 2010 to 2012, there were reports that Israel was close to initiating military strikes against Iranian nuclear facilities, knowing it would likely trigger U.S. involvement. Israeli Prime Minister Benjamin Netanyahu’s cabinet officials reportedly blocked him from taking this step. 119



Maintaining and strengthening the JCPOA will help to minimize the future potential for such pressure. Though he fought hard to subvert the JCPOA, for example, Netanyahu has been relatively silent since its adoption. Carmi Gillon, former head of the Israeli security agency Shabak, wrote in July that, thanks to the JCPOA “the threat of an Iranian nuclear weapon is more remote than it has been in decades.” Gillon added, “the majority of my colleagues in the Israeli military and intelligence communities supported the deal once it was reached, [and] many of those who had major reservations now acknowledge that it has had a positive impact on Israel’s security and must be fully maintained by the United States and the other signatory nations.” 120



If the United States is to avoid returning to high levels of tension and conflict in the U.S.-Iranian relationship, it must avoid the more belligerent options explored in this paper. The alternative—the option most likely to produce a positive outcome for all parties—is to uphold the JCPOA, carefully enforce its terms and conditions, and build on it to further engage Iran where its interests overlap with the United States. Pursuing greater diplomacy and engagement with Iran is, ironically, low‐​hanging fruit at this time. Iranian President Hassan Rouhani, who in his first term helped shepherd the JCPOA to fruition, won reelection this year by a wide margin, receiving 57 percent of the vote (compared to 38.5 percent for his chief opponent). 121 The idea of greater engagement with the West was a key component of Rouhani’s electoral platform; both centrists like Rouhani and reformers like former President Mohammed Khatami have argued in favor of what they describe as “JCPOA 2.0,” a series of internal policy compromises that will allow Iran to continue to engage with the West and begin to reintegrate into the global economy. 122



The key to reaping the benefits of a more conciliatory approach is recognizing that Iran is not a unitary actor. Iranian politics, though not fully democratic, are dynamic and competitive, and include various factions, from conservative hardliners to moderate reformists. The nuclear deal is widely popular in Iran, but antagonism from the Trump administration will bolster the prominence of Iranian hardliners who felt Tehran capitulated too much in the negotiations and who use fears of U.S. duplicity to undermine the idea of constructive engagement with Washington. 123 Similarly, perceptions that the United States is failing to live up to its side of the bargain—or is taking new steps that may undermine Iranian security—weaken political support for pragmatic reformists who see value in making concessions to the West in exchange for sanctions relief and integration with the outside world. Ultimately, unlike the more aggressive policy options explored in this paper, further engagement with Iran when possible will strengthen Iran’s more moderate political factions and weaken hardliners, providing a more hopeful future for U.S.-Iranian relations.
"
nan
"Southern Africa’s game farms are private reserves that house wildlife such as giraffes, zebras and antelope to be used for restocking national parks, meat production or trophy hunting. But these farms have a problem. Warthogs and porcupines want to move around the reserves too, and they have an annoying habit of making large holes under the boundary fences to burrow their way in and out. For cheetahs, these holes are an ideal way to get inside and prey on the valuable game.  Cheetahs are considered pests in these reserves and, in Namibia, game farmers end up killing more cheetahs than livestock farmers do. These game farms needed to find a way to let the warthogs in while keeping the big predators out. The solution is simple but unusual – have you ever considered whether your cat flap might be used by other animals besides your feline friend? It turns out “swing gates” (a technical term for a glorified cat flap installed along a fence line) are ideal. According to research I carried out with colleagues in Namibia, recently published in the African Journal of Ecology, warthogs and porcupines have learned how to use these gates, but big cats such as cheetahs and leopards don’t seem to have figured this out yet.  It could be that the big cats see an intact fence and do not bother to investigate the integrity of it, whereas warthogs may be more inquisitive and spend more time rummaging around the fence line looking for holes. This is great news for both livestock and wildlife farmers because it now stops movement of large carnivores into farms and can limit the amount of expensive antelope or buffalo that are killed by predators.  With the threat removed, farmers now do not need to resort to hunting carnivores to limit the damage that they cause on the farms, and it also means that they no longer kill hole-digging species like warthogs, aardvarks and porcupines to limit the number of fence breaches. Our study determined that the number of holes dug by burrowing animals under game fences decreased over time and this was most evident when the swing gates were easily accessible and ideally placed. This means installing gates in open areas with harder soil, dense vegetation, close to the watering holes where water-loving, perpetually-thirsty warthogs like to hang out. What’s more, swing gates are far cheaper than electric fencing – the conventional way to stop animals burrowing their way in. So it’s really a win for both people and wildlife. Electric fencing requires continual application of weed killer to stop the grass from short-circuiting the electric current, but the only maintenance that is needed with swing gates is the occasional hole-filling from the rogue warthog that does decide to dig a new hole under the fence. If we want to conserve cheetahs and promote co-existence between humans, wild game and wild predators, then using fences to exclude big cats from their natural habitat won’t work in the long term. But for the time being, this is a quick and simple fix. Farmers can add swing gates to their tool box of effective yet non-lethal techniques to combat predation of livestock. Laurie Marker, founder of the Cheetah Conservation Fund and one of the study’s co-authors, points out why it is so crucial that this works for game farmers too. Predators, she says, create large financial losses on game farms. “Swing gates will enable us time to work on a permanent solution that will enable all species to peacefully coexist on the same land, such as the development of conservancies”.  In these conservancies wildlife is allowed to roam free, without high game fences. As Marker points out, “neighbouring farmers and land occupiers then manage the resources collectively, allowing for predators to be managed within the larger landscape system.” In the meantime, you better watch out what other critters enter your house via your cat flap, as it might not just be Felix that is coming inside."
"

When we began drafting this study of U.S. military spending and force posture, we had no way of knowing the tremendous challenge that COVID-19 would pose. It has wreaked havoc on the economy. It has disrupted every facet of American life. The impact will reverberate for generations. The global pandemic—and the U.S. government’s response to it—has threatened the lives and liberties of Americans as well as the United States’ standing in the world.



This disaster is a call to action. The threat posed by nontraditional security challenges, including pandemics, climate change, and malicious disinformation, should prompt a thoroughgoing reexamination of the strategies, tactics, and tools needed to keep the United States safe and prosperous.



As of this writing in late April 2020, and well before the full impact of COVID-19 is known, it seems obvious to us that the United States can no longer justify spending massive amounts of money on quickly outdated and vulnerable weapons systems, equipment that is mostly geared to fight an enemy that might never materialize. Meanwhile, the clearest threats to public safety and political stability in the United States are very much evident and all around us. Just how demonstrations of force or foreign stability operations contribute to U.S. national security is particularly questionable at a time when a microscopic enemy has brought the entire world to a standstill.



This analysis mostly examines where the U.S. military was as of December 31, 2019, with a few observations from early 2020. Where it will be on December 31, 2020, will be guided by a critical set of questions. The authors, and the entire team of scholars in the Cato Institute’s Defense and Foreign Policy Studies Department, intend to help frame those questions—and to answer as many as possible—over the coming year.



Security politics will be different in the future, but the goal of security policy hasn’t changed and is clearly outlined in this report: to identify the most effective and efficient means for advancing Americans’ safety and prosperity. That entails ending the forever wars, terminating needless military spending, rethinking the fundamentals of strategic deterrence, and focusing the entire defense establishment on innovation and adaptation.



 **Online Policy Forum | June 4, 2020 1:00 PM to 3:00 PM EDT**



Even before COVID-19, military spending was unsustainable. In a new Cato report, scholars argue that nontraditional security challenges require a thorough reexamination of national defense strategy. The report prioritizes ending the forever wars, terminating needless military spending, rethinking the fundamentals of strategic deterrence, and focusing the entire defense establishment on innovation and adaptation.



Budgetary and strategic inertia has impeded the development of a U.S. military best suited to deal with future challenges. Over the past several decades, the military has repeatedly answered the call to arms as American foreign policy privileges the use of force over other instruments of power and influence. The era of near endless war has now stretched into its third decade. Going forward, Washington should realign national security objectives and motivate allies and partners to become more capable as America’s relative military advantage wanes and the focus inevitably turns to domestic priorities, including public health.



As policymakers transition from primacy and unilateral military dominance, and beyond the post‐​9/​11 wars in the greater Middle East, the force must also be reoriented. The defense establishment’s most urgent requirement is prioritization. The nation’s resource constraints are real, and hard choices cannot be postponed. In particular, all military branches should emphasize innovation over the preservation of legacy systems and practices. This will require cooperation from Congress, which must address the budget pathologies that stifle new thinking and keep the Pentagon locked into old ways of doing business. Senior defense officials must orient the future force around a different approach to power projection, one less dependent on permanent forward bases, and toward a renewed focus on the requirements for strategic deterrence. The services must also think anew about how to best capture and use information.



Despite recent challenges and setbacks, most importantly the COVID-19 outbreak and response, the United States still enjoys many advantages, including a dynamic economy, political stability, and favorable geography. Securing the United States from future threats should sustain and build on those advantages. Restraining the impulse to use force, imposing limits on military spending, and relying more heavily on diplomacy, trade, and cultural exchange would relieve the burdens on our overstressed military. The ultimate objective should be to build an agile and adaptable military that can address a range of future challenges but is used more judiciously in the service of vital U.S. interests and to deter attacks against the homeland.



Building a modern military requires a clear conceptualization of the realities of international conflict and tight alignment with a country’s foreign policy. Strategic planners must have a clear‐​eyed view of both the threats facing the country and the tools necessary to defend its vital interests. Planners in the United States should take account of the country’s fortunate circumstances, including its geography, dynamic economy, and political stability, and recognize that maintaining these advantages does not require a massive military apparatus that is constantly active in nearly every part of the world.



For decades, however, U.S. national security policy has been oriented around a military‐​centric approach, variously called primacy, liberal hegemony, or deep engagement. Primacy is based on the idea that U.S. military power explains the absence of a major‐​power war since the end of World War II and the attendant rise in productivity and living standards. Harvard political scientist Samuel Huntington predicted in 1993, for example, that “a world without U.S. primacy will be a world with more violence and disorder and less democracy and economic growth.”1 Former secretary of state George Shultz put it even more succinctly in the 2016 documentary _American Umpire_ : “If the United States steps back from the historic role [it has] played since World War II, the world will come apart at the seams.”2



Such sentiments reflect why, despite the fact that the United States enjoys relative safety, U.S. officials see only grave and urgent dangers. They see any challenge to U.S. military dominance as a threat to global liberty and peace. The 2018 _National Defense Strategy_ (NDS), for example, notes that the “central challenge to U.S. prosperity and security is _the reemergence of long‐​term, strategic competition_ by … revisionist powers.” The goal then, according to the NDS, is to “remain the preeminent military power in the world.”3 The 2017 _National Security Strategy_ (NSS) goes further, noting that the “United States must retain overmatch—the combination of capabilities in sufficient scale to prevent enemy success and to ensure America’s sons and daughters will never be in a fair fight.”4



And while the United States is purportedly orienting around great power competition against China and Russia, the post‐​9/​11 conflicts grind on. The National Defense Authorization Act (NDAA) for Fiscal Year 2020 makes clear that the Pentagon envisions those conflicts continuing indefinitely.5 Today’s U.S. military budget, after adjusting for inflation, vastly exceeds that of the Cold War and now approaches levels during the height of the wars in Iraq and Afghanistan in the early 2010s (see Figure 1). Operationally, the Pentagon has been bogged down in Afghanistan and caught in the ongoing struggle between Saudi Arabia and Iran for dominance in the Persian Gulf region and beyond; in December 2019, the Trump administration was considering sending an additional 14,000 troops to the Middle East, including a substantial ground presence in Saudi Arabia, for the first time in nearly 17 years.6



Perceptions of looming threats or fear of potential peer competitors should not distract from the obvious need to take a strategic pause and reconsider the United States’ core defensive needs, especially during a global pandemic and associated economic disaster.7 Washington should realign its national security ends and means to better match the emerging geopolitical reality—especially America’s waning relative military power.8 The desire for one‐​sided “overmatch” is understandable but impractical given the extensive commitments it entails. The time is ripe to make a clean break from the past.



The dramatic shock of COVID-19 adds urgency to the need for new strategic priorities. This report acknowledges that the nation’s resource constraints are real and that the United States faces a period of grave economic uncertainty. The Pentagon is not immune to these pressures. Politicians are unlikely to undertake a concerted campaign to build public support for massive increases in taxes or deep cuts to popular domestic programs in order to fund a military that an ambitious grand strategy calls for, and they would likely fail if they tried. The U.S. military is spending beyond its means due mostly to inertia and strategic indecision. To that end, this report is founded on three pillars: articulating a force that meets the _realities of the geopolitical situation_ and contemplating the current budget pathologies that impede change; reexamining _force construction_ ; and evaluating the posture needed for _modern strategic deterrence_. These pillars drive the recommendations contained herein with an aim toward developing a more realistic and prudent military budget.



The Trump administration’s budget proposal for fiscal year (FY) 2021 aims for “U.S. military dominance in all warfighting domains—air, land, seas, space, and cyberspace,” echoing its FY 20 budget proposal, which supported “dominance across all domains.”9 This view is consistent with the desire for “overmatch” in the NSS. Overmatch requires the United States to “restore our ability to produce innovative capabilities, restore the readiness of our forces for major war, and grow the size of the force so that it is capable of operating at a sufficient scale and for ample duration to win across a range of scenarios.”10



This quest for global dominance is taking place as the United States’ capacity for sustaining supremacy is waning. The NDS observes, for example, that “we are emerging from a period of strategic atrophy, aware that our competitive military advantage has been eroding. We are facing increasing global disorder, characterized by decline in the long‐​standing rules‐​based international order—creating a security environment more complex and volatile than any we have experienced in recent memory.”11



The NDS does not treat this diagnosis as a recognition of the limits of American military power but rather as a rallying cry to marshal additional national resources and maintain the globe‐​spanning posture to which Washington has grown accustomed. Raging against the dying light of uncontested military primacy will run into severe budgetary and strategic obstacles.



The international order faces many challenges, and these cannot be reversed by attempting to restore U.S. dominance across all domains and in all regions. Instead, U.S. grand strategy should encourage allies and partners—the leading beneficiaries of global peace and stability—to take a greater role in sustaining it. The United States cannot be the world’s police force or coast guard.



The United States needs a prudent military strategy that can protect U.S. interests without turning into an open‐​ended pursuit of anachronistic, grand goals. “Overmatch” extending across all regions, domains, and weapons systems is simply untenable. The “America First” view of primacy focuses on military hardware and manpower, not the elements of smart power that have traditionally been the real sources of American strength and influence.12 Simply put, the United States today is overinvested in the military. As a recent Cato book explains, “a less expansive foreign policy agenda will allow the United States to reduce military spending significantly.”13 Washington should take advantage of the current period of relative geopolitical stability to adopt a military posture consistent with grand strategic restraint.14 Such a reorganization would bring much‐​needed coherence to U.S. military strategy.



The recommendations in this report are not driven by perceptions of waste and bloat within the U.S. defense establishment—though there is certainly much of that. Rather, the authors assess international politics today as well as the probable nature of future threats and fix on what is required to defend the U.S. homeland and vital interests.



The current approach relies heavily on the use of force and coercion at the expense of other instruments of power and influence. A military‐​centric strategy seems particularly ill‐​suited to a post–COVID-19 world.15 The primary tools of American global engagement under a grand strategy of restraint should be trade, diplomacy, and cultural exchange. The military instrument, while still vital, should be geared toward defense, in the strictest sense of the word, enabling allies and partners to counter adversaries. A grand strategy of restraint would leverage innovation and modernization to refocus on a narrower range of future challenges and to rethink how strategic deterrence could better serve the needs of the nation.



Adopting a new grand strategy, and fashioning a new force posture to suit, also requires a reconsideration of the value of forward deployment. The United States should reduce its permanent overseas presence, especially in forward‐​operating bases that will be vulnerable if conflict erupts. Under a strategy of restraint, the U.S. Navy and Air Force would be a surge force capable of deploying to crisis zones if local actors prove incapable of addressing threats.



The United States can support allies and prepare for future combat by enabling others to defend themselves and their interests. U.S. force planning should be oriented around how the U.S. military can contribute to such operations from a distance as U.S. interests dictate. In those rare instances where vital national interests necessitate the deployment of U.S. personnel well outside of the Western Hemisphere, Pentagon planners must ensure adequate facilities and resources to resupply their operations. Relying on forward‐​deployed forces as we currently do risks inadvertently creating a security dilemma that encourages prospective rivals to match such deployments. By focusing on modernization and interoperability, U.S. forces could assist others while reducing the risk of escalation. Equally important, an over‐​the‐​horizon posture would reduce demands on the U.S. military—especially on active‐​duty personnel.



A grand strategy of restraint calls for a less active conventional military, one that is not deployed in permanent bases or routinely engaged in offensive operations on multiple continents. Even so, restraint is not synonymous with disarmament; the United States will continue to rely on nuclear weapons to deter some strategic attacks. However, the current concept of “strategic deterrence” and the role of nuclear weapons in U.S. defense strategy would have to change. The main problem with Washington’s approach to strategic deterrence—as with U.S. military strategy in general—is that it suffers from mission creep.



At its core, “strategic deterrence” is preventing a first use of nuclear weapons against the U.S. homeland or an ally. But that is not the only behavior that U.S. officials currently seek to deter. The 2018 _Nuclear Posture Review_ (NPR), for example, says that the United States would consider using nuclear weapons to respond to “significant non‐​nuclear strategic attacks” against U.S. or allied civilians, infrastructure, and early warning capabilities.16



An overly broad definition of strategic threats drives demands for a large and diversified nuclear arsenal and missile defense capability in order to have many flexible response options.17 As a result, Washington’s approach to strategic deterrence places great weight on adversary capabilities. For example, according to the 2018 NPR, “Moscow’s perception that its greater number and variety of non‐​strategic nuclear systems provide a coercive advantage in crises at lower levels of conflict.”18 This is the supposed justification for new, low‐​yield U.S. nuclear weapons.19 Similarly, the _2019 Missile Defense Review_ cites the threat of hypersonic glide vehicles—high-speed maneuvering warheads that take an unpredictable rather than ballistic route to their target and that China and Russia are developing as a response to U.S. missile defense expansion—as a rationale for deploying more missile defense sensors on satellites.20



Having a flexible nuclear arsenal and missile defense system that can be tailored to respond to the unique characteristics of different threats sounds sensible. However, the failure to prioritize produces a kind of paralysis. In a world where dangers loom around every corner, doing anything less than deterring all of them at once is considered a failure. This encourages wasteful spending and invites potential adversaries to create counter strategies that increase the likelihood of inadvertent nuclear escalation. Such moves damage deterrence instead of strengthening it. Deterrence under restraint would have a narrower set of objectives and clearer priorities and would privilege clarity and reliability over flexibility.



If the United States would prefer to engage adversaries at a distance, strategists need to rethink how the future force should be organized. Improving the ability of the different services to communicate with one another and have smoothly functioning command and control during a conflict is especially critical.21 A traditional focus on raw firepower and the impulse to base personnel and equipment at great distances from the United States will likely need to give way to an emphasis on developing a technologically proficient force that relies on new layers of sensors (radar, sonar, etc.) that can direct long‐​range attacks and control unmanned vehicles at greater distances. Another overlooked capability in debates over the defense budget is the redundancy of reconnaissance systems—the ability of America’s intelligence‐​gathering satellites and aircraft to perform their functions if they are disrupted.



The current approach of massive investment in the military, displays of force, and direct challenges to multiple adversaries in their respective regions is often counterproductive. As Sen. Angus King (I-ME) notes, with respect to Iran “the unanswered question is who is provoking whom. As we escalate sending more troops, moving aircraft carriers, we view it as preventative and as defensive. They view it as provocative and leading up to a preemptive attack.”22 Michael O’Hanlon of the Brookings Institution made a similar point in 2017. Arguing for a new approach to European security, O’Hanlon explained that the United States “may be able to help ratchet down the risks of NATO‐​Russia war … by recognizing that NATO expansion, for all its past accomplishments, has gone far enough.”23 U.S. messaging must be consistent, and budgetary maneuvers should not introduce justifications for war. The overarching recommendation here is to halt policies that exacerbate regional security dilemmas and to restructure U.S. military power accordingly. Such a restructuring is made more difficult, however, by the rigidity of the budgeting process.



Spending patterns driven by inertia and habit privilege the military, the use of force, and coercion over diplomacy and other instruments of American power. Accordingly, the Pentagon’s budget continues to reflect strategic errors of the past, including searching for a peer competitor, continuing support for a counterproductive war on terror, and propping up dangerous and unreliable strategic partners. To complicate matters, Congress and the White House are sparring over new distractions, including potentially diverting funds from the military budget for border wall construction24 We refer to these distractions as “budget pathologies”: abnormalities and malfunctions inherent in how the U.S. government secures funding for the military, a process that often impedes the creation of a viable national strategy. The executive branch initiates many of these pathologies, but Congress also plays a key enabling role by not exercising its traditional power of the purse.



On December 9, 2019, for example, a House and Senate conference committee passed the FY 20 NDAA. The bill authorized $738 billion for national defense spending, and President Trump proudly signed it into law.25 The U.S. government continues to spend and act as if its wars in the Middle East will never end. Secretary of Defense Mark Esper described such operations as not “necessarily unusual” and noted that “we continue … ‘to mow the lawn.’ And that means, every now and then, you have to do these things to stay on top of [the threat].”26 In fact, these operations represent sunk costs and reflect misguided assumptions about what actually makes Americans safe and prosperous.



The U.S. military budgeting process is supposed to reflect a delicate balance between executive‐​level strategic guidance, Department of Defense (DOD) budget requests related to the overarching strategy, and legislative approval and appropriations to fund the requests. The actual process of funding the nation’s military, however, bears no resemblance to that ideal. A recent, clear sign of just how badly this process has broken down was revealed when the Trump administration tried to strip $3.6 billion from existing Pentagon projects to fund improvements for physical barriers at the U.S.-Mexico border. The funds earmarked to be ripped away would have served to upgrade and maintain the surface fleet, improve basic services on military bases, and expand the nation’s offensive cyber capabilities.27



This is just one example. There are many pathologies—spending decisions that serve partisan or parochial interests but do not advance U.S. security—that consistently undermine the entire federal budget, not merely what winds up in the Pentagon’s coffers. The most serious problem pertains to the unwillingness of American elected officials to reconcile spending and revenue. Despite the occasional attempt to reverse the tide, nothing has had lasting success. When Congress passed the Budget Control Act in 2011, the annual budget deficit stood at $1.3 trillion. Four years later the annual deficit fell to $438 billion. However, this figure has risen each subsequent year, exceeding $984 billion at the end of FY 19—the highest since 2012.28



Very few Americans appreciate the scale of the federal government’s spending. A poll taken in early 2017 found that only 1 in 10 Americans could correctly identify the amount spent on the military within a range of $250 billion.29 And yet, according to a 2019 Gallup survey, only 1 in 4 Americans believe that U.S. military spending should increase at all, while a slightly higher percentage (29 percent) thinks the United States is spending too much.30



The main budgetary problem for the Pentagon, therefore, is political. It refuses to budget based on what is possible and realistic and instead spends to satisfy perceptions of need (often indistinguishable from desires) with too little consideration of constraints and tradeoffs. While most Americans want a military that is prepared to prevail in combat, we all must take account of the resources available to make that a reality, both now and into the future. 



Beyond this overarching problem of ends misaligned to means, the Pentagon budgeting process is afflicted by two other related pathologies: overseas contingency operations funds and reprogramming. Both allow the government to spend without consequence and fail to distinguish between needs and wants.



Supplemental appropriations to pay for wars are not a novel idea. In fact, the first was passed in 1818. Historically, however, legislators moved such “emergency” spending (today known as “nonbase nonrecurring” or “contingency” funding) for unforeseen operations back to the base military budget within a few years once leaders had a clearer idea of operational needs.31



The DOD has received $2 trillion in overseas contingency operations (OCO) funding since September 11, 2001.32 In December 2019, Congress appropriated $71.5 billion for the OCO budget in FY 20.33 To put these numbers in perspective, in 2020, if OCO were its own government agency, it would have the fourth largest budget in terms of discretionary spending.34 The use of OCO funding for almost two decades following 9/11 has systematically undermined the established appropriations process. Supplemental appropriations fund activities unrelated to the wars but are not counted as part of the base DOD budget. In other words, reliance on OCO funding lets the military services avoid setting priorities that should guide long‐​term strategy and makes it too easy to undertake present‐​day combat operations without formal legislative consent and funding. Other departments and agencies have also gotten into the habit; even the U.S. Agency for International Development and the Department of State now rely on OCO funding to supplement their base budgets.35 Aside from its blatant dishonesty, OCO represents a larger pattern of runaway U.S. government spending and especially the legislative branch’s tendency to avoid oversight of either Pentagon spending or the nation’s perpetual conflicts. 



The other factor fueling the abuse of OCO funding is the Budget Control Act (BCA) of 2011. That legislation set limits on discretionary budget authority from 2012 through 2021 to slow the growth of public debt after the 2008 financial crisis.36 The spending limits are supposed to be enforced through what is commonly called sequestration. Under sequestration, any appropriations that go above set funding levels—or “caps”—are canceled.37 However, funding designated for OCO, ostensibly for counterterrorism efforts, including the wars in Afghanistan, Iraq, and Syria, is exempt from BCA caps and separate from the Pentagon’s base budget (hence “nonbase”).38 In other words, executive branch officials and legislators have a massive loophole for expanding military spending while seeming to abide by discretionary spending limits. The BCA’s OCO exemption allows elected officials to feign concern about out‐​of‐​control federal spending without doing anything to stop it.



Force development and planning requires funding that is on‐​time, stable, and proportional to the scope of military operations. Using OCO to skirt the BCA’s budget caps does not reflect a military establishment that can prioritize according to coherent long‐​term strategies. This technique has allowed civilian leaders to evade tough choices, including how to resolve ongoing conflicts and whether to enter new ones. While the war on terror presents unique challenges, using OCO only helps perpetuate the cycle of U.S. involvement in never‐​ending conflicts.



With BCA caps officially expiring in 2021, policymakers may be less tempted to rely on OCO funding. However, moving OCO back into the base budget inevitably raises concern about overall spending increasing at an unreasonable rate. Congress should move enduring costs back to the base budget without increasing topline military spending. Presenting the DOD with less budget flexibility should spur more creativity and budget management, not less, while still allowing the military to rely on supplemental funding for truly dire, unforeseen overseas expenses. When such emergencies arise, Congress can authorize additional funds as necessary.



Agencies often reprogram funds to deal with unforeseen challenges, but it is technically illegal to spend taxpayer dollars in ways not explicitly authorized by Congress.39 However, “as there are no government‐​wide reprogramming rules,” note Georgetown University researchers Michelle Mrdeza and Kenneth Gold, “prohibitions against reprogramming funds within an appropriations account … vary among agencies and appropriations subcommittees.”40 The Government Accountability Office (GAO) concurs. Agencies have the “implicit” authority to shift funds within a department or agency as long as the intended use of the funds remains broadly within the same goal.41 Regulations governing DOD require congressional approval for any funds reprogrammed over 20 percent or $20 million over the original allocation.



Reprogramming has become a national security issue as the executive branch seeks ways to seize control of the budget from Congress. The Trump administration’s threat to use funds allocated for other purposes to build the border wall, for example, contributed to the longest government shutdown in history in winter 2018–2019.42 Although the DOD continued operations, the budget impasse adversely affected many contractors, researchers, and production line managers.43



Past NDAAs restricted reprogramming funds for priorities that Congress expressly declined to fund, but the FY 20 NDAA did not include such language. A loophole in U.S. law allows for unassigned military construction funds to be used for construction projects during periods of national emergency.44 Other legislative language allows the secretary of defense to provide support for counterdrug activities to other departments and agencies.45 These two provisions provide the leeway to reprogram a significant amount of funding. Yet, the president can declare almost anything a “national emergency” at will.46



Thus, the moves to reprogram funds defy Congress’s traditional power of the purse and allow federal agencies to use money from the DOD budget to support domestic political initiatives. Such efforts create a dangerous precedent, both in undermining constitutional checks and balances and potentially limiting the funds vital to the nation’s defense.



These budgetary pathologies insulate the U.S. military from resource constraints, allowing it to proceed mostly by inertia. But the U.S. military also remains mired in the post‐​9/​11 Global War on Terror. Nearly two decades of continuous operations have put enormous strain on the force. The military branches continue to lower eligibility requirements to meet their recruitment goals and have increased retention bonuses to discourage services members from leaving.47



More ominous developments include rising suicide rates among veterans and active‐​duty service members, an increase in reported sexual assaults, and the need for expanded counseling to deal with post‐​traumatic stress disorder and other psychological challenges.48 In short, the well‐​being of U.S. service members is a pressing national concern.



The force of the future is likely to be smaller, particularly in terms of numbers of personnel in uniform, and thus will need to be more adaptable. That, in turn, will require increasing the academic aptitude and physical fitness standards for recruits.49 A focus on improving the force—as opposed to simply growing it—through retention programs for critical staff and expanded educational and retraining opportunities is key to creating a healthy and socially viable military. This should be a DOD‐​wide imperative.



Beyond recruitment and retention, each service branch confronts its own unique challenges. Pentagon officials must reconceptualize how the U.S. military plans to fight. The wars of the recent past, against chiefly nonstate actors in the greater Middle East, South Asia, and Sub‐​Saharan Africa, are unlikely to be an adequate guide for future conflicts.



In particular, the potential for direct engagement with technologically capable adversaries in contested environments means that the era of U.S. dominance can no longer be assumed. Within that framework, the following sections outline a few key choices that service leaders need to make.



There are two clear challenges for the joint forces of the United States: standardizing a system for operations across multiple domains (e.g., land, sea, air, space, and cyber) and pushing innovation. Addressing the first challenge demands that every branch of the U.S. military agrees to a joint, all‐​domain command and control (C2) system. As combat systems and advanced artificial intelligence (AI) platforms continue to develop, they must be seamlessly integrated within and between all U.S. forces. Currently, however, each military branch is pursuing its own C2 design. For example, the Navy has the Naval Integrated Fire Control‐​Counter Air system, and the Air Force has the Advanced Battle Management System.50 This duplicates effort, wastes funds, and impedes unifying C2.51



Defense contractors and other interested parties will lobby for their respective systems, but the choice should be based on the ability to implement the system across all services, agreement among the branches, and a clear standard for cybersecurity. Because a standard C2 platform is the optimal solution for the modern battlefield, all U.S. forces should streamline and upgrade to ensure that they meet the new compatibility standards. The U.S. military should not move forward with designing protections for these networks, and redundancy for forward C2 deployment, without first establishing a joint system. It is premature to estimate the eventual cost of such a unified system, but deciding on this system now will inevitably save money by facilitating coordination between every branch of the U.S. military.



The DOD, for its part, must decide on a platform, take bids on delivery of the system, and obtain executive branch and congressional approval on a process and timeline for implementation. Congress should use its legislative authority to ensure compliance through reporting requirements.52



The second overarching challenge for the U.S. military is the need to prioritize innovation. This entails empowering individuals at all levels to bring forward new ideas and establishing a process to deliver design options through a full development cycle in the most expeditious and cost‐​effective ways. Service members have a critical role to play in determining future priorities since these systems and platforms will have a direct impact on their daily lives and their ability to function on the battlefield.



Following the Army’s example, each branch of the military should develop its own Futures Command to push for branch‐​wide innovation. The mission of a Futures Command is modernization. It does away with old “industrial age” approaches, which are mostly piecemeal and often slowed by bureaucracy, and puts them all under one roof with a set of defined goals. If each branch has its own innovation command center, the Pentagon would be well‐​positioned to coordinate across branches. A futures reserve unit in each branch would prove critical given the recent effort to recruit and fund PhDs in the military and DOD.53 Members of the armed forces with advanced degrees could then naturally transition into the reserve system to support innovation.



The service branches should also develop practices for curating the massive amounts of data generated for AI systems. Given the high probability that this technology will be critical to future fights, branches should use “data wranglers”—individuals whose primary task is to collect information that can be plugged into various systems.54 There is currently no method to identify U.S. service members able to work with data, generate statistical analysis, and assure the accuracy of data.



In addition, Kessel Run‐​type programs in each branch could be successful for fostering innovation, as it has been for the Air Force. In the _Star Wars_ universe, the Kessel Run refers to an impossible task that is completed in a short time. The Air Force had that in mind when it set out to develop software quickly and in response to an uncertain environment.55 The inability to negotiate contracts with external parties who will build software or hardware in a timely and efficient manner are typically the main impediments to developing innovative programs in the military.



U.S. defense planners should consider what the nation’s defense needs will be in the future, but too often their efforts are stymied by inertia or shortsighted demands that defense programs serve domestic political and economic interests. The United States should be investing in innovation and research rather than stale production lines for weapons that have outlived their usefulness or new weapons that can never meet their design objectives.56 Developing weapons platforms should be based on the needs of the future military, not short‐​term concerns, such as the parochial interests of defense‐​industry workers or the politicians who shield them. The U.S. military must abandon weapons platforms that cost too much to maintain and retrofit and have limited or no value in future conflicts.



Future increases to the DOD’s research and development (R&D) budget should be funded by reducing spending on outdated weapons systems. As part of a renewed push for R&D, the U.S. government should revisit its approach to basic research funding. Instead of bolstering the National Science Foundation and encouraging scholars to seek trivial connections to national security in research projects, the DOD should be granted additional authority to invest in other public and private research startups and incubators through the individual service research offices (e.g., the Office of Naval Research). These funds should not be restricted and should be open to every research university and think tank capable of doing advanced research that will help drive innovation within the defense ecosystem.



This is not an argument for expanding federal funding for research but rather extending existing research opportunities to a much wider pool of qualified institutions. For too long the U.S. government has steered research funding to federally funded R&D centers. This has driven up R&D costs while failing to integrate the talent and ingenuity of research institutions outside traditional networks. The United States must leverage its deep technological base to meet coming challenges; as of now the U.S. government’s vision of research and research funding is tied to past processes that have a decidedly mixed record of delivering essential equipment and materials in a timely and cost‐​effective manner.



Research should be focused on applying novel technical capabilities to the modern battlefield. The idea that the United States has fallen behind China in the AI arms race is only true based on a measurement of research quantity, not quality. And such claims do not take full account of the vast array of innovative enterprises in the United States, most of which are completely outside the federal government’s control or purview.



For example, Google recently published a paper demonstrating quantum supremacy, when a quantum device (such as a quantum computer) can solve a problem that no traditional computer realistically can.57 This represents a leap over classical computing power by orders of magnitude, but U.S. defense planners must think about how to employ these tools in combat. AI is only as useful as the data fed into the algorithms.58 Moving forward with a clear vision of how the U.S. military can leverage AI and quantum power, therefore, requires investments in basic data science education, data assurance and retention, and data integrity.



These proposals are generally cost‐​neutral as they entail reorganization of existing lines of effort. Ensuring that the U.S. military develops multidomain battle systems without redundancy, establishing a clear process for managing data on the battlefield, and putting platform development in the hands of the individual soldier, sailor, airman, and Marine are all clear needs as relevant as massive outlays for modern weapons platforms. Reorganizing around Futures Command groups and using data wranglers would enable all service branches to innovate as the United States still enjoys a number of political, economic, and strategic advantages relative to prospective rivals.



Since its formal inception in 1947, the Air Force has fended off challenges to its place in the structure of the U.S. military, and a few respected scholars still call for its abolition.59 Many critics, however, aim to fix apparent inefficiencies within the force rather than doing away with it. A recent Center for Strategic and International Studies report, for example, notes that while spending on the Air Force has reached new heights, its force capabilities—as measured by the number of aircraft in its inventory—have fallen to an all‐​time low.60 This is partly explained by the overall focus on quality over quantity but is also due to the fact that the Air Force is more than just planes, just as the Army is more than the infantry and the Navy is more than surface ships. Still, the Air Force has struggled to introduce new aircraft. The service’s experience with the F-35 Lightning II aircraft, a fifth‐​generation fighter jet that is significantly more advanced than its predecessors and supposed to replace several other aircraft currently in service, has not been promising. In general, the Air Force has spent a lot of money to get less capacity.



A change of direction is in order. The structure and capabilities of the Air Force should maximize operational readiness, taking into consideration procurement difficulties associated with current weapons systems still under production.61 The bitter experience with the F-35, which will be delivered to the force nearly a decade late and at an inflation‐​adjusted cost well above original estimates, is only one sign of the overall challenge facing the Air Force.62 The service needs capable aircraft at a cost that will allow it to purchase them in adequate quantities, and it needs to obtain them in a timely fashion.



Per the objectives spelled out in the 2018 _National Defense Strategy_ (NDS), the U.S. Air Force is tasked with dominating the air, outer space, and cyberspace by using advanced and emerging technology. The Air Force needs to be an innovative service to keep up with the rapid pace of technical change. Specifically, the service should focus on countering China and Russia’s investments in anti‐​access/​area‐​denial systems, including long‐​range surface‐​to‐​air missiles.63



This will be difficult. As previously noted, the Air Force’s rising budgets have coincided with a declining number of active aircraft, along with fewer pilots and Air Force civilian employees.64 Such trends signal broader challenges with basic budgetary management, including the expanding costs of operation and maintenance. In other words, today’s Air Force paradoxically does less while spending more. This is perplexing to say the least.



While the service has emphasized incorporating advanced technology for air and space operations, overall readiness and pilot training have decreased substantially, contributing to a steady rise in aircraft mishaps.65 These operational problems are exacerbated by a shortage of qualified maintenance technicians. According to the GAO, the Air Force does not have a strategy to improve retention. If the Air Force is unable to hold onto its best people, it will struggle to adapt to changing operating environments (including outer space and cyberspace) and new technology (such as AI and quantum computing).66 The Air Force must undertake a service‐​wide initiative to reverse this trend, especially by incentivizing qualified personnel to remain in the force.



With respect to hardware, the Air Force is developing the F-35A, the B-21 Raider long‐​range bomber, and the KC-46A Pegasus tanker aircraft while also seeking to replace current intercontinental ballistic missiles and developing a Space Force, which is still officially under the Air Force’s auspices. That is unsustainable. The service’s goals must be aligned to present and future realities and should take account of the demands of modern combat. As the airspace in which the Air Force operates becomes increasingly crowded and contested, this places a premium on unmanned vehicles that can loiter and are capable of executing strike, surveillance, and resupply missions.



Forward basing poses both operational and doctrinal challenges to air operations because long‐​range precision strikes by an adversary can decimate aircraft and fuel supplies long before U.S. aircraft can engage the target. What good is a force of 100 F‐​35s if they never leave the ground?



A focus for now on drones and a reliance on a revitalized F-15 Eagle aircraft through the F-15EX platform is certainly warranted. The recent move to establish the 16th Air Force, which is focused on cyberspace and electronic warfare, is also a welcome development.67 On the whole, however, the Air Force is trying to do too much, including a focus on space, support for counterterror operations, unmanned reconnaissance, nuclear deterrence, transport, air defense, air‐​to‐​air combat, ballistic missiles, and precision bombing. A strategic pause and reset are desperately needed.



The Army’s strategy, posture, and budget should reflect and adapt to evolving geopolitical circumstances. The U.S. Army posture assessment fails to do that, placing dominance through military overmatch, as outlined in the NDS, at the forefront of the Army’s vision.68 Day‐​to‐​day operations, ongoing conflicts, allied engagement, and crisis response all continue to put unnecessarily high demands on the force. A realistic assessment of threats would allow the Army to prioritize and eliminate or offload unnecessary missions. Enabling and encouraging allies to do more in their respective regions would reduce the Army’s requirements, including especially numbers of active‐​duty personnel.



In 2018, the Army created the Army Futures Command.69 This organization has been critical for pushing the service to modernize. It originally established six priorities:



Of these, long‐​range precision fires (i.e., modern artillery) and networked air and missile defense are critical. The United States should divest from other outdated weapons systems—including, in particular, the Abrams tank—that are unlikely to serve a major purpose on the future battlefield, or at least in the battlefields that are truly critical to U.S. security and prosperity.



Above all, the active‐​duty U.S. Army should be substantially smaller and postured mostly for hemispheric defense. A grand strategy of restraint would eliminate most permanent garrisons on foreign soil and rely more heavily on reservists and National Guard personnel for missions closer to the U.S. homeland. Such a posture would reduce the likelihood that U.S. troops would be drawn into protracted civil conflicts that do not engage core U.S. national security interests. That, in turn, would generate substantial savings over the next decade.



Developing better and modern versions of artillery is another key task for the Army. That would allow the U.S. military to support allies from a distance, when U.S. leaders deem such assistance appropriate, while also ensuring that U.S. troops mostly remain out of harm’s way when such missions are not truly essential for U.S. security.



The development of better unmanned vehicles for long‐​range fires in support of ground operations is also critical. While drones for surveillance and precision strikes are useful, in a future war the United States will need functional unmanned vehicles that can deliver artillery support and fire weapons from a distance, minimizing harm to U.S. forces. Future platforms used to deliver long‐​range fires also need the ability to be undetected despite increased sensors employed by adversaries.



Finally, the Army needs to develop better air and missile defensive platforms to protect forward‐​operating units. These tools would benefit the entire U.S. military, but the greatest gain would go to the Army, whose ability to fight will be challenged by opponents’ long‐​range munitions. The Army needs portable sensors ready to detect incoming fires. A modern military is too vulnerable to long‐​range attack, including from artillery, ballistic missiles, and drones. Real‐​time battlefield awareness is essential, as is the need to defend our allies once the U.S. commits to pulling back from forward deployment. Thinking about this critical function is more important than developing a new helicopter or other vertical lift platform (e.g., tilt‐​rotor aircraft) or a next‐​generation tank. If the U.S. military cannot protect its forces in the field from short‐​range ballistic and cruise missiles, units will not survive long enough to bring these new weapons to bear against the enemy.



To meet current recruitment goals, the Army has waived certain requirements and increased enlistment bonuses.71 If these reforms draw capable people into the service, then they should continue, but careful oversight is needed. An emphasis on quality, rather than quantity, could reduce turnover, ensure new enlistees complete their requisite training, and ultimately improve retention.



A focus on readiness could also help. Service members should know that they have adequate support to complete their missions and be confident that policymakers will not send them to fight open‐​ended wars that are not vital to U.S. national security. A failure to meet those basic requirements has driven qualified personnel from the force. No branch of the U.S. military has reached its readiness goals, however, and the budget priority has since shifted to modernization. While the increase in research, development, testing, and evaluation is an important step in creating a more lethal and agile force, a failure to meet readiness goals will impede force transformation.



The Army needs to rethink the size of the force needed given the effort to modernize overall. At a time when two successive presidential administrations have pledged to draw down operations in the greater Middle East, the United States should refocus on establishing a lean and agile ground force that can retain the best people while allowing the marginal performers to transition out. This process of attrition should be used to reduce the size of the active‐​duty Army by 20 percent over the next decade. Recruiters need to employ what marketers call “microtargeting” to ensure that the U.S. Army has high‐​quality soldiers that can innovate on the battlefield, not just follow orders.72 Eliminating unnecessary forward bases, improving existing facilities, and rethinking education and training would be easier with reductions in the size of the force.



In recent years, the U.S. Navy has operated under the assumption that it can get all that it wants without a clear articulation of what it needs—though the situation may be changing. An October 2019 Congressional Budget Office (CBO) report warned that the Navy “would not be able to afford its 2020 shipbuilding plan.” CBO estimated that the Navy would need $28.8 billion per year for new‐​ship construction, more than double the historical average of $13.8 billion per year (in 2019 dollars).73 This is hardly the first time that CBO has observed the looming gap between the Navy’s plans and fiscal realities.74 Although the sea service has avoided a bitter reckoning, the responsible course would bring its goals in line with its available resources.



In early December 2019, Acting Navy Secretary Thomas Modly publicly reaffirmed his commitment to achieving a 355‐​ship Navy, and he separately issued a memo to the fleet calling for a plan to achieve it by the end of the next decade.75 But more recent evidence suggests that the Trump administration has scaled back its shipbuilding plans and backed away from the 355‐​ship goal. The president’s budget submission for FY 21 actually cut $4.1 billion from shipbuilding.76 Navy leaders acknowledge the tradeoffs between operations and maintenance and money for new construction. “We definitely want to have a bigger Navy, but we definitely don’t want to have a hollow Navy either,” Modly told _Defense News_. “If you are growing the force by 25 to 30 percent, that includes people that have to man them. It requires maintenance. It requires operational costs. And you can’t do that if your top line is basically flat.”77



Many strategy documents simply assume that considerably more money _must_ be made available to the military—and leave it to the politicians to figure out how.78 The Heritage Foundation, for example, calls for a 400‐​ship Navy even as it concedes that such a force “may be difficult to achieve based on current DOD fiscal constraints and the present capacity of the shipbuilding industrial base.”79



The Navy should reject such advice, prioritize among competing desires, and focus on what is genuinely needed to achieve vital national security objectives. In the near term, this means prioritizing current operations. High‐​profile disasters at sea, including the tragic accidents aboard USS _John S. McCain_ and USS _Fitzgerald_ , which claimed 17 sailors’ lives in 2017, raised obvious questions about the state of the surface Navy. A GAO report released two years before the _McCain_ and _Fitzgerald_ incidents concluded that “the high pace of operations the Navy uses for overseas‐​homeported ships limits dedicated training and maintenance periods,” which had “resulted in difficulty keeping crews fully trained and ships maintained.”80



The Navy must expand both its capacities and capabilities. Prioritizing less‐​expensive vessels could make up for certain shortfalls and grow the fleet at a faster rate. Newer platforms would also translate to less maintenance time, further increasing the number of vessels ready for service at any given time. On occasion, the Navy has gone in a different direction, privileging very high‐​end platforms that often take many years to reach the fleet. In the interim, this leaves more older ships in service longer, along with their additional repair and maintenance costs.



The Navy has made recapitalizing the ballistic missile submarine (SSBN) fleet—the _Columbia_ -class SSBNs that will replace the _Ohio_ -class—its top shipbuilding priority. The tradeoffs are most apparent with respect to fast‐​attack submarines (SSNs).81 Although these vessels are unsuited to perform many routine Navy missions—including escort operations and visible presence—they are critical and should be maintained in some quantity.



Other hard choices cannot simply be imagined away. This report focuses on two key acquisitions programs to highlight tradeoffs within the surface fleet: the _Gerald R. Ford_ -class aircraft carrier (CVN) and the new guided‐​missile frigate FFG(X).82



As designed, _Ford_ -class ships are the largest and most capable warships on the planet. But little else about the ships—including whether their actual performance matches their designed capabilities or when the ships will attain full operability—can be predicted with any confidence. Former Navy Secretary Richard Spencer staked his reputation on ensuring that the advanced weapons elevators—large lifts that transport bombs and missiles from inside the ship to the flight deck—aboard USS _Gerald R. Ford_ (CVN-78) would all work before the ship set out for trials. They didn’t—only 4 of 11 were operational by the end of October 2019.83



Three other critical technologies—the ship’s new electromagnetic aircraft launching system, an advanced arresting gear used to recover aircraft on deck safely, and a dual band radar—have also failed to meet the service’s expectations.84 A December 2018 report by DOD’s director of operational test and evaluation (DOT&E) identified a host of concerns, ranging from “poor or unknown reliability of systems critical for flight operations” to inadequate crew berthing.85



Most damning, perhaps, were the DOT&E’s conclusions pertaining to the ship’s core mission: the ability to launch and recover aircraft at high tempo and over extended periods (sorties, in Navy jargon). The report warned, “Poor reliability of key systems … on CVN 78 could cause a cascading series of delays during flight operations that would affect CVN 78’s ability to generate sorties.”86 In the end, DOT&E concluded that the Navy’s sortie generation requirements for the _Ford_ were based on “unrealistic assumptions.”87



Other critics fault a systemic lack of accountability throughout the Navy. Industry analyst Craig Hooper wrote in October 2019, “The naval enterprise struggles to bring bad news to the higher levels of the chain of command. It is a habit that perpetuates something of a complacent ‘not my problem’ or career‐​protecting sluggishness in the face of avoidable disaster.” This has ramifications that go well beyond catapults and arresting gear.88



As difficult as the design and development process for the Navy’s capital ship has been, however, even tougher questions swirl around the employment of these massive platforms. In an era of defense dominance, when adversaries can use relatively cheap but accurate weapons to attack large and exquisite platforms, how will the carriers perform? Not well, according to some knowledgeable critics, including retired Navy Capt. Henry J. Hendrix, who in 2013 warned, “The queen of the American fleet is in danger of becoming like the battleships it was originally designed to support: big, expensive, vulnerable—and surprisingly irrelevant to the conflicts of the time.” The national security establishment, he concluded, had ignored “clear evidence that the carrier equipped with manned strike aircraft is an increasingly expensive way to deliver firepower” and that the ships might struggle “to operate effectively or survive in an era of satellite imagery and long‐​range precision strike missiles.”89



National Defense University’s T. X. Hammes imagines an even more dramatic transformation that would merge “old technologies with new to provide similar capability at a fraction of the cost.” Specifically, Hammes proposes using container ships loaded with hundreds or thousands of drones and cruise missiles—but very few people—to eventually take the place of the iconic flattops hurling and recovering manned aircraft. “Flying drones,” Hammes writes, “can provide long‐​range strike, surveillance, communications relay, and electronic warfare” and can be launched and recovered vertically. Cruise missiles deployed in standard shipping containers, meanwhile, could effectively convert “any container ship—from inter‐​coastal to ocean‐​going” into “a potential aircraft carrier.”90



For now, Congress has conspired to thwart any fundamental reconsideration of the centrality of the aircraft carrier to the modern surface fleet. The 11‐​carrier legislative mandate remains despite serious concerns about the _Ford_ ’s timeline and even as “the Navy is finding it increasingly difficult to deploy carriers and keep them on station.”91 A reckoning has been postponed but cannot be avoided forever.



According to the _Force Structure Assessment_ issued in December 2016, the Navy seeks to procure 52 small surface ships, 20 of which are to be a new class of guided‐​missile frigates, the FFG(X).92 Some analysts contended that reactivating the _Oliver Hazard Perry_ -class frigates, the last of which was retired in 2015, would help the Navy achieve its force structure goals faster, but the decision to commission new vessels signaled the Navy leadership’s commitment to modernization.93



The Navy requested $1.28 billion in its FY 20 budget to procure the first FFG(X), awarding conceptual design contracts to five different companies.94 Despite the purported reduction in scheduling, risk, and price with the Navy’s approach to the FFG(X), the CBO predicted in October 2019 that the total cost of the 20‐​ship program will be closer to $23 billion than the Navy’s estimated $17 billion.95



Although the House and Senate fulfilled the administration’s request for $1.28 billion in procurement, plus another $59 million for research and development in the FY 20 NDAA, doubts remain about this program’s ability to fill the capability gaps in the fleet.96 The key questions will revolve around unit cost and the length of the design, development, and build phases. Congress has put significant pressure on the Navy to implement cost‐​effective capabilities on realistic timelines. If the Navy is truly committed to expanding fleet capacity quickly, and with minimal risk, it is imperative that it hold the line against anything likely to lead to costly delays.



Before the Navy can decide what it needs, however, it must decide what it’s going to do. The Trump administration’s _National Security Strategy_ (NSS) and NDS would appear to be good news for the Navy. Both documents focus on the rise of peer or near‐​peer competitors, chiefly China and Russia, with reference also to regional rivals such as North Korea and Iran. These types of adversaries would privilege the need for naval and air power over ground forces, which have been geared to fighting nonstate actors and insurgents over the past two decades.



The U.S. Navy has an extraordinarily ambitious set of objectives, and the demands placed on the service already exceed its ability to meet them. These demands mostly originate with the various regional combatant commands and further reflect a long‐​standing assumption that the Navy’s forward presence is essential to global security. The Heritage Foundation’s _Index of U.S. Military Strength_ , for example, argues that “the Navy must maintain a global forward presence both to deter potential aggressors from conflict and to assure our allies and maritime partners that the nation remains committed to defending its national security interests and alliances.”97



What the Heritage Foundation casts as a requirement is a choice. Strategic requirements are not handed down from on high but reflect the dominant strategic paradigm. A commitment to maintaining the free movement of raw materials, essential commodities, and finished goods was a core mission for the U.S. Navy during the Cold War and was driven by a concern that a globe‐​straddling Soviet Navy was both motivated to close—and capable of closing—critical sea lanes of communication and maritime choke points.98



Today, the situation is much different. Most international actors, including even modern rivals such as Russia and China, depend on the free flow of maritime trade and are therefore highly incentivized to try to keep these waterways open. For decades, however, U.S. allies and partners have neglected their own maritime forces, coming to rely on the U.S. Navy deploying small, surface combatants in their home waters. In effect, therefore, the U.S. Navy has been operating as a global coastal constabulary.



This practice should stop. U.S. policy should aim to encourage these nation‐​states to play a key role in securing access to vital sea‐​borne trade. The presumption that the U.S. military must be constantly on station, including in waters thousands of miles away from the Western Hemisphere, merits scrutiny, not least because the U.S. Navy alone cannot meet the demands of being a de facto coast guard for all other nations—nor is it in America’s interest to try.



Sea‐​lane control in the modern era aims to ensure the free flow of goods and is primarily defensive. The aim should be to prevent others from limiting access to the open oceans while not threatening to deny anyone else the peaceful use of those same seas. That mission can and should be shared with other countries, most of whom will be operating near their shores, and thus highly motivated—and able—to defend their sovereign waters.



Marine Corps Gen. David Berger’s appointment as the 38th commandant of the service was met with a question by a Marine Corps major: “Sir, who am I?”99 With a founding mission of being able to carry out contested amphibious operations, it is unclear today who the United States is preparing to invade, and how it would do so. Would the United States deploy landing craft like those used on Normandy beaches in 1944 or at Inchon Korea in 1950 during an age of highly sophisticated surface‐​to‐​surface missiles? Does the U.S. military have functional aviation or naval vehicles that can support large modern amphibious invasions?



The response to these sorts of questions was dramatic and forceful. Berger’s _Commandant’s Planning Guidance_ (CPG) sought to kill many sacred cows and institute a new path forward where the Marines would focus on sea denial, interoperability with the Navy, and wargaming to understand current and future combat options.100 The CPG stated, without evocation, “the Marine Corps will be trained and equipped as a naval expeditionary force‐​in‐​readiness and prepared to operate inside actively contested maritime spaces in support of fleet operations.”101



The Marine Corps’ decision not to request amphibious platforms in the 2020 budget was formalized in the CPG, which called amphibious operations “impractical and unreasonable.” Such conclusions recognize the need for a swift, agile force that can operate in forward positions without the resources and protection of the core force.



While a future great power war in the Asia‐​Pacific is possible, the probability of a near‐​term conflict is very low; this supports the decision to move the Marine Corps from a focus on amphibious operations. In fact, recent reports note that China’s navy is rethinking its spending plans given the economic uncertainty brought on by the trade war with the United States.102 China is not a peer competitor; its grandiose naval ambitions remain unfulfilled, as massive investment would be needed for surface ships, landing craft, advanced weapons platforms, aircraft, and personnel—all when the demands of an expanding middle class are increasingly going unmet. And those domestic challenges all preceded the COVID-19 pandemic that began in late 2019 that has wreaked havoc on China’s economy.



As Marine Corps planners recognize, there is a great need for large numbers of cheap autonomous naval systems that can overwhelm the enemy, and these are preferable to expensive and manned systems.103 Berger stated, “I see potential in the ‘Lightning Carrier’ concept … however, [I] do not support a new‐​build CVL [light aircraft carrier].”104 The CPG suggests a possible focus on high‐​mobility artillery systems to deny sea access and landing routes.



The Navy and Marine Corps should not be pushing new amphibious platforms when they are unable to maintain their current craft in a steady state of readiness.105 If the Marines are truly the “first to fight,” they need to focus on modernization, rework force structure for quality over quantity, and reset their priorities after years of focus on the Global War on Terror. Senior leaders in the Marine Corps have the correct vision, but implementing their plans within a change‐​resistant bureaucracy will be a challenge.



The United States has failed to undertake a much‐​needed reevaluation of its approach to strategic deterrence. The nuclear triad, the array of land‐, air‐, and sea‐​based capabilities that can deliver nuclear weapons to targets, has been a fixture since the early Cold War. Since then the triad has become dogma. A reexamination of its value considering technological developments, advances in intelligence, surveillance, and reconnaissance, and changes in adversary capabilities is overdue.



That hasn’t occurred under the Trump administration, which seems to be settling on a kitchen‐​sink approach to solving the country’s alleged “deterrence gaps” vis‐​à‐​vis other great powers.106 Its 2018 _Nuclear Posture Review_ retains the triad and adds two new capabilities—a low‐​yield warhead for the Trident (the nuclear‐​armed ballistic missile carried by U.S. submarines) and a new nuclear sea‐​launched cruise missile—to the Obama administration’s nuclear modernization plan. A 2017 report from the CBO estimated that this plan would cost roughly $1.2 trillion over 30 years.107 That 30‐​year estimate is likely to increase as programs face unforeseen problems and delays. The United States is also trying to improve its capabilities for defeating ballistic and cruise missile threats to both forward‐​deployed forces and the American homeland.108



These investments in nuclear weapons and missile defense demonstrate that strategic deterrence remains central to U.S. strategy, but is the United States making the right policy choices? What are the threats the United States wants to deter, and can nuclear weapons and missile defense help mitigate them? Raising these questions reveals that some elements of the nuclear modernization plan are superfluous and that some missile defense choices are likely to push rivals to develop destabilizing counterstrategies.



Most of the nuclear modernization plan’s spending will fund new delivery platforms—aircraft, submarines, and missiles—with some money going toward updated nuclear warheads. The plan is not meant to expand the arsenal; as new systems get introduced, old ones will be phased out.



Supporters of the nuclear modernization plan claim that it will only eat up a small portion of overall military spending. That is true given the very high topline for the budget, but this does not imply that nuclear modernization will be cheap and easy. Initial cost estimates are already growing. For example, recent delays in the B61-12 nuclear gravity bomb life extension program (LEP) will add an extra $600–$700 million, and the W80-4 nuclear warhead LEP’s estimated project cost had increased from $9.4 billion in November 2017 to $12 billion by summer 2019.109 Delivery platforms are also prone to cost overruns. The B-2 Spirit bomber program (which wildly overran its initial cost projections) offers a cautionary tale for its secretive and expensive successor, the B-21 Raider.110



Before developing new nuclear capabilities, we need to decide whether they are necessary for strategic deterrence. Arguments about the relatively low price of systems are hardly compelling if the United States does not need to buy them in the first place. The B61-12 gravity bomb, for example, is superfluous given U.S. efforts to develop an air‐​launched cruise missile that could hold the same targets at risk from long distance.111 The decision to deploy a low‐​yield tactical warhead for the Trident missile rests on faulty understandings of Russian nuclear strategy.112 Similarly, the United States should eliminate the nuclear mission for the F-35, cut the purchase of new intercontinental ballistic missiles in half, and delay procurement of the B-21 for 10 years.113



Increased spending on strategically dubious capabilities also extends to missile defense. The _2019 Missile Defense Review_ calls for a wide‐​ranging expansion of missile defense capabilities to counter both rogue states and great powers.114 That includes expanding the stock of existing interceptors and developing new technology to counter offensive capabilities that U.S. adversaries have fielded to defeat existing U.S. defenses.115 Rather than enhancing strategic deterrence, America’s missile defense posture is encouraging adversaries to develop new offensive platforms that increase the risk of conventional conflicts going nuclear.



Adjusting American grand strategy toward restraint would mandate a different approach to strategic deterrence. Modernizing the U.S. nuclear arsenal is important, but the pursuit of maximum flexibility to deter an amorphous set of strategic threats will waste billions of dollars on capabilities the United States doesn’t need. The primary goal of strategic deterrence, preventing nuclear first use against America and its allies, would remain the same under restraint. Instead of pursuing flexibility to respond to a wide variety of threats, the three pillars of strategic deterrence under restraint are removal of peripheral threats through diplomacy; shifting a greater defense burden to allies; and adopting a conventional military posture that enables deterrence by denial—discouraging enemy action by denying a quick and easy victory.



Greater reliance on diplomacy could contain or remove potential threats that current U.S. military doctrine casts as strategic imperatives. For example, the Joint Comprehensive Plan of Action with Iran allowed the United States to reduce nuclear proliferation risks through diplomacy.116 The case also illustrates the negative consequences of abandoning diplomacy. Since the Trump administration’s withdrawal from the agreement, the region has witnessed a constant tit‐​for‐​tat escalation of tensions.117 Arms control agreements with other great powers such as China and Russia are another important feature of restraint’s approach to strategic deterrence. Arms control measures can help set guardrails on the most dangerous aspects of great power competition, allowing for a degree of strategic trust and stability that is important for averting nuclear disaster.



Another key component of a redesigned U.S. strategic deterrent would entail empowering allies to respond to the coercive activities of regional rivals. Given the stakes involved for all parties, the deterrent threats by local actors might prove more credible than those issued by a distant United States.118 Under restraint, regional disputes might prove less likely to escalate into great power conflict, and more capable local deterrent forces would help reduce—though not eliminate—demands on the U.S. military and U.S. taxpayers.



China and Russia demonstrate how effective asymmetric strategies—those that avoid matching an opponent’s capabilities but instead try to exploit weaknesses with other means—can frustrate an otherwise stronger foe that depends on power projection to achieve its interests.119 U.S. allies in East Asia, for example, don’t need to build a lot of expensive aircraft or ships to defend themselves from China’s growing air and naval forces. A mix of unmanned systems, long‐​range precision strike conventional weapons, and strong air defense could be an effective and affordable counter to Chinese power. Encouraging allies to develop their own asymmetric capabilities would empower them to contribute more to deterring regional conflicts. Gradually reducing the forward deployment of U.S. forces could facilitate this transition.120



The United States would still have an interest in deterring nuclear first use against its allies—or the use of nuclear weapons in any context. But stronger, more capable allies armed with conventional weapons, combined with a reduced forward‐​deployed U.S. military presence, would shorten the list of strategic threats that U.S. officials feel obliged to deter or eliminate.



The third pillar of a new U.S. strategic deterrence posture under restraint is a greater reliance on conventional weapons to deter other great powers. Instead of threatening an attacking country through punishment (damaging the attacker’s population and economy) this approach would depend on a concept known as deterrence by denial, which resists enemy action by denying a quick, easy military victory for the aggressor.121 Credibly increasing the costs of aggressive action would leverage U.S. advantages in sensors, regional missile defense, and conventional long‐​range precision strike to deter military action that U.S. allies are unable to address.122 Allies equipped with similar capabilities would further improve deterrence by denial.



Such an approach would reduce the risk of inadvertent nuclear escalation in conventional conflicts by focusing on defeating military units rather than engaging in deep strikes against an adversary’s command and control networks.123 Technical developments in both the United States and its potential great power adversaries have blurred the lines between conventional and nuclear forces. The military strategies adopted by the United States, China, and Russia that emphasize early, deep conventional strikes further increase the escalation risks.124



Under this new approach, nuclear weapons and homeland missile defense would play reduced roles. On the missile defense side, U.S. defense planners should pivot to improving regional systems and increasing the stock of associated interceptors while moving away from expanded homeland missile defense.126 That would make it harder for great power adversaries to both initiate and prevail in quick, limited conflicts. U.S. leaders would also face less pressure to rapidly escalate to conventional attacks against Chinese or Russian territory. The U.S. way of war emphasizes strikes against command and control facilities, some of which are located far behind a country’s borders. Such strikes could be interpreted as an attack on a country’s leadership or an effort to reduce the effectiveness of its nuclear forces. If U.S. forces could deflect an initial attack against land‐​based, anti‐​access/​area denial weapons such as surface‐​to‐​air and anti‐​ship missile batteries, it could reduce the incentive to target adversary command and control early in a conflict.



The United States should take advantage of a strategic pause, adopt a grand strategy of self‐​reliance and restraint, and develop a comprehensive plan for dealing with peer and near‐​peer competitors and rivals. For at least two decades, the U.S. military has been trapped in a cycle of small‐​scale wars and nation‐​building fiascos that have eroded America’s unique advantages. Reconstructing U.S. security, therefore, requires a conscious decision to remove U.S. forces from past conflicts, and a fundamental reconceptualization of how the United States will use its forces in the future. Security budgets need to view U.S. power along economic, diplomatic, and cultural dimensions. These alternatives are often more effective than force and can produce a positive lasting impact by creating a period of stability that endures and that can be sustained by many like‐​minded actors, not merely the U.S. military.



Diplomacy, for example, has grown stagnant, but the Trump administration seems determined to hasten its demise.127 President Trump has scaled back on many diplomatic initiatives, but the COVID-19 pandemic laid bare the shortcomings of the military‐​centric approach. The United States can divest some of its legacy military apparatus and focus on innovating for the future while also investing a small fraction of these funds to deal with a range of threats to public safety that are not amenable to military solutions. The U.S. government will almost certainly need to prepare for a role in coordinating supply and delivery of vital equipment in future disasters and pandemics. Our true strategic reserve is more than the manpower that the military can marshal and the expertise in delivery, logistics, and analysis that the military can offer. The capacity and the expertise of the American people is a strength that will see us through crises.



This report has outlined a plan for moving the United States toward a more sustainable national security posture predicated on restraint.128 The budgeting process and the design and development of new military systems are riddled with inefficiencies that have wasted time and money that could be put toward fixing the social and structural problems the military faces. Conventional forces should be modernized for future fights, not geared toward sustaining the war on terror. Finally, the United States needs a modern approach to strategic deterrence that places greater emphasis on denying the ability of other great powers to project offensive military forces by using conventional capabilities rather than the nuclear triad.



Security comes through prudence, not overwhelming force, permanent alliances, or massive investments in weapons platforms. Defending the United States requires a judicious application of the many instruments of American power, not reckless overseas military adventures that have cost too many lives and too much treasure. A clear consideration of U.S. capabilities, appreciation of our fortunate geopolitical situation, and confidence in our ability to address future challenges will allow the United States to build and maintain a leaner and more efficient military, one that is more than capable of defending U.S. vital interests and deterring attacks against the homeland.



 **Advanced Battle Management System (ABAMS):** the technical engine that would manage all communications, orders, and sensors used by the Air Force



 **Anti‐​access/​area‐​denial (A2/AD):** an operational concept that complicates an opponent’s ability to use air, naval, and land power at long distance; typically entails the use of land‐​based sensors and precision strike systems to target opponent ships, aircraft, and bases



 **Aircraft carriers (CVNs):** the largest ships in the U.S. Navy and the centerpiece of U.S. fleet operations, capable of carrying about 60 aircraft of varying types



 **Arresting gear:** mechanical system that rapidly decelerates aircraft when landing on a platform such as an aircraft carrier



 **Ballistic missile submarines (SSBNs):** the sea‐​based leg of the nuclear triad, these vessels carry Trident missiles, each capable of delivering up to eight nuclear warheads



 **Command and control (C2):** set of organizational and technical processes employed to accomplish missions



 **Dual band radar:** combines radar systems into one integrated system for easier operation, maintenance, upgrade, and targeting



 **Fast‐​attack submarines (SSN):** the U.S. Navy’s primary undersea platform, capable of both offensive action at sea or against targets on land; also used for intelligence gathering



 **Frigates (FFG):** mixed‐​armament warship lighter than a destroyer; typically focused on anti‐​ship and anti‐​submarine warfare



 **FFG(X):** class of future multimission guided‐​missile frigates



 **Integrated Fire Control‐​Counter Air System (NIFC-CA):** the Navy’s multidomain battle management system



 **Low‐​yield nuclear weapon:** a nuclear weapon with a relatively small explosive yield thought to be useful for limited nuclear operations on the battlefield or for controlling escalation



 **Maritime choke points (e.g., straits and narrows):** a heavily trafficked narrow waterway



 **Micro‐​targeting:** direct marketing methods utilizing datamining techniques to segment consumers by tastes or attributes



 **Nuclear Posture Review (NPR):** major policy document that sets out the nuclear strategy and policies of a new administration; typically includes overviews of the U.S. nuclear arsenal, arms control policy, and nuclear strategy broadly defined



 **Operational readiness:** capacity of a unit to perform its designated combat or combat support function



 **Smart power:** strategic use of both hard (military) and soft power (diplomacy and trade) to achieve foreign policy ends



Eric Gomez is director of defense policy studies; Christopher Preble is vice president for defense and foreign policy studies; Lauren Sander is external relations manager for defense and foreign policy studies; and Brandon Valeriano is a senior fellow at the Cato Institute. 



ShowHide

Endnotes



1 Samuel P. Huntington, “Why International Primacy Matters,” _International Security_ 17, no. 4 (Spring 1993): 83.



2 George Shultz, _American Umpire—Teaser_ , trailer for film by James Shelley and Elizabeth Cobbs, 2016, 0:47, https://​vimeo​.com/​1​4​6​7​22638.



3 Office of the Secretary of Defense, _Summary of the 2018 National Defense Strategy of the United States of America: Sharpening the American Military’s Competitive Edge_ (Washington: Department of Defense, 2018), pp. 2, 4. Emphasis in original.



4 Office of the President of the United States of America, _National Security Strategy of the United States of America_ (Washington: The White House, December 2017), p. 28.



5 National Defense Authorization Act for Fiscal Year 2020, S. 1790, 116th Cong. (2019).



6 Gordon Lubold and Nancy A. Youssef, “Trump Administration Considers 14,000 More Troops for Mideast,” _Wall Street Journal_ , December 4, 2019; and Jared Malsin, “U.S. Forces Return to Saudi Arabia to Deter Attacks by Iran,” _Wall Street Journal_ , February 26, 2020.



7 John Glaser and Christopher Preble, “High Anxiety: How Washington’s Exaggerated Sense of Danger Harms Us All,” Cato Institute Study, December 10, 2019.



8 Jennifer Lind and Daryl G. Press, “Reality Check: American Power in an Age of Constraints,” _Foreign Affairs_ 99, no. 2 (March/​April 2020): 41–48.



9 Office of Management and Budget (OMB), _A Budget for America’s Future_ (Washington: Government Publishing Office, February 2020), p. 34; and OMB, _A Budget for a Better America: Promises Kept. Taxpayers First._ (Washington: Government Publishing Office, March 2019), p. 23.



10 Office of the President, _National Security Strategy_ , p. 28.



11 Office of the Secretary of Defense, _2018 National Defense Strategy_ , p. 1.



12 Smart power is the strategic use of both hard (military) and soft power (diplomacy and trade) to achieve foreign policy ends.



13 John Glaser, Christopher A. Preble, and A. Trevor Thrall, _Fuel to the Fire: How Trump Made America’s Broken Foreign Policy Even Worse (and How We Can Recover)_ (Washington: Cato Institute, 2019), p. 181.



14 See, for example, Barry R. Posen, _Restraint: A New Foundation for U.S. Grand Strategy_ (Ithaca, NY: Cornell University Press, 2014); and A. Trevor Thrall and Benjamin Friedman, eds., _U.S. Grand Strategy in the 21st Century: The Case for Restraint_ (New York: Routledge, 2018).



15 Mark Hannah, “Stop Declaring War on a Virus,” _War on the Rocks_ , April 17, 2020.



16 Office of the Secretary of Defense, _Nuclear Posture Review_ (Washington: Department of Defense, February 2018), p. 21. The Nuclear Posture Review (NPR) is a major policy document that sets out the nuclear strategy and policies of a new administration. A typical NPR includes overviews of the U.S. nuclear arsenal, arms control policy, and nuclear strategy, broadly defined.



17 The 2018 NPR is explicit about the demand for flexibility in nuclear deterrence. It refers to this as “tailored deterrence.” See Office of the Secretary of Defense, _Nuclear Posture Review_ , pp. vii–viii.



18 Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 53.



19 Low‐​yield nuclear weapons are nuclear weapons with a relatively small explosive yield (a smaller amount of energy released when detonated) thought to be useful for limited nuclear operations on the battlefield or controlling escalation.



20 Office of the Secretary of Defense, _2019 Missile Defense Review_ (Washington: Department of Defense, January 2019), p. xvi.



21 Command and control refers to the actual efficacy of commanding officers and leaders to move and direct troops on the battlefield as well as the infrastructure that enables the commander to provide direction to their troops (i.e., radios and tactical operation centers providing intelligence and support).



22 Angus King, interview by Andrea Mitchell, _Andrea Mitchell Reports_ , MSNBC, May 23, 2019, https://​www​.youtube​.com/​w​a​t​c​h​?​v​=​S​s​f​a​I​B​w2f7A.



23 Michael O’Hanlon, “President Trump Might Be on to Something with Russia,” _USA Today_ , December 19, 2017.



24 Benjamin Denison, “Confusion in the Pivot: The Muddled Shift from Peripheral War to Great Power Competition,” _War on the Rocks_ , February 12, 2019.



25 Senate and House Armed Services Committees, _FY2020 NDAA Summary_ , December 2019, https://www.armed-services.senate.gov/imo/media/doc/FY20%20NDAA%20Conference%20Summary%20_%20FINAL.pdf.



26 Quoted in Shawn Snow, “Esper Says U.S. Forces Combating ISIS in Libya ‘Continue to Mow the Lawn,’” _Military Times_ , November 14, 2019.



27 Claudia Grisales, “These Are the Military Projects Losing Funding to Trump’s Border Wall,” NPR, September 4, 2019. 



28 Bureau of the Fiscal Service, _Final Monthly Treasury Statement: Receipts and Outlays of the United States Government: For Fiscal Year 2019 through September 30, 2019, and Other Periods_ (Washington: Department of the Treasury, September 2019). The deficit in fiscal year 2011 was $1.3 trillion according to the Congressional Budget Office. See Elizabeth Cove Delisle et al., “Federal Budget Deficit for Fiscal Year 2011: $1.3 Trillion,” Congressional Budget Office, November 8, 2011.



29 “Foreign Policy Poll,” Charles Koch Institute, January 2017, https://​mk0qeluyepi9​drvw7c​ng​.kin​stacdn​.com/​w​p​-​c​o​n​t​e​n​t​/​u​p​l​o​a​d​s​/​2​0​1​7​/​0​2​/​C​h​a​r​l​e​s​-​K​o​c​h​-​I​n​s​t​i​t​u​t​e​-​a​n​d​-​C​e​n​t​e​r​-​f​o​r​-​t​h​e​-​N​a​t​i​o​n​a​l​-​I​n​t​e​r​e​s​t​-​J​a​n​-​2​0​1​7​-​F​o​r​e​i​g​n​-​P​o​l​i​c​y​-​P​o​l​l​-​1.pdf, p. 22. See also “New Poll: Americans Crystal Clear: Foreign Policy Status Quo Not Working,” Charles Koch Institute, February 7, 2017.



30 Lydia Saad, “Demand Wanes for Higher Defense Spending,” Gallup, March 12, 2019; and Mark Hannah and Caroline Gray, _Indispensable No More? How the American Public Sees U.S. Foreign Policy_ (New York: Eurasia Group Foundation, November 2019).



31 F. Matthew Woodward, _Funding for Overseas Contingency Operations and Its Impact on Defense Spending_ (Washington: Congressional Budget Office, October 2018), pp. 3, 4.



32 Neta C. Crawford, “United States Budgetary Costs and Obligations of Post‐​9/​11 Wars through FY2020: $6.4 Trillion,” 20 Years of War: A Costs of War Research Series, Brown University, November 13, 2019, p. 3; and Elizabeth Field, _Overseas Contingency Operations: Alternatives Identified to the Approach to Fund War‐​Related Activities_ , GAO-19–211 (Washington: Government Accountability Office, January 2019), p. 1.



33 Senate and House Armed Services Committees, _FY2020 NDAA Summary_ , p. 1.



34 “OCO Is Fourth Largest ‘Agency,’” Taxpayers for Common Sense, February 10, 2020. The Trump administration’s 2021 budget request, _A Budget for America’s Future_ , outlines plans to reduce the overseas contingency operations budget each fiscal year (FY 2021 proposal is $69 billion), but Taxpayers for Common Sense also noticed the steady proposed increases to the base budget that will counterbalance this decrease in OCO. See “More OCO Details—the Devil’s in the Footnotes!,” Taxpayers for Common Sense, February 10, 2020.



35 Emily M. Morgenstern, “Foreign Affairs Overseas Contingency Operations (OCO) Funding: Background and Current Status,” Congressional Research Service In Focus, December 30, 2019.



36 Brendan W. McGarry, _The Defense Budget and the Budget Control Act: Frequently Asked Questions_ , CRS Report R44039 (Washington: Congressional Research Service, September 30, 2019). The Budget Control Act did not address the largest share of U.S. federal spending, so‐​called mandatory programs such as Medicare, Medicaid, and Social Security.



37 McGarry, _Defense Budget and the Budget Control Act_ , pp. 4–5. Congress has repeatedly amended the Budget Control Act (BCA) to change discretionary spending limits through the Bipartisan Budget Acts (BBA) of 2013, 2015, 2018, and 2019. The original cap for defense spending in 2020 under the BCA was to be $630 billion and was raised to $667 billion in the most recent BBA. For discretionary spending, which accounts for most defense spending, the Office of Management and Budget calculates the percentage and dollar amount to be taken from affected programs to achieve the total mandatory cut required by the BCA. See “FAQs on Sequester: An Update for 2020,” posted on the House Committee on the Budget’s website, https://​bud​get​.house​.gov/​p​u​b​l​i​c​a​t​i​o​n​s​/​r​e​p​o​r​t​/​F​A​Q​s​-​o​n​-​S​e​q​u​e​s​t​e​r​-​A​n​-​U​p​d​a​t​e​-​f​o​r​-2020.



38 Brendan W. McGarry and Emily M. Morgenstern, _Overseas Contingency Operations Funding: Background and Status_ , CRS Report R44519 (Washington: Congressional Research Service, September 6, 2019), p. 6.



39 For background, see Office of the General Counsel, _Principles of Federal Appropriations Law, Chapter 2: The Legal Framework, Fourth Edition, 2016 Revision_ , GAO-16–463SP (Washington: Government Accountability Office, 2016).



40 Michelle Mrdeza and Kenneth Gold, “Reprogramming Funds: Understanding the Appropriator’ Perspective,” Government Affairs Institute at Georgetown University, https://​gai​.george​town​.edu/​r​e​p​r​o​g​r​a​m​m​i​n​g​-​f​u​n​d​s​-​u​n​d​e​r​s​t​a​n​ding/.



41 Susan J. Irving (associate director of budget issues at Government Accountability Office) to Steve Horn (chairman of the Subcommittee on Government Management, Information and Technology under the Committee on Government Reform and Oversight), June 7, 1996, B-272080, https://​www​.gao​.gov/​a​s​s​e​t​s​/​9​0​/​8​5​6​2​0.pdf.



42 Mihir Zaveri, Guilbert Gates, and Karen Zraick, “The Government Shutdown Was the Longest Ever. Here’s the History,” _New York Times_ , January 25, 2019.



43 According to the White House, the deal that broke the impasse took $1.4 billion in the fiscal year 2019 budget bill, $3.6 billion from military construction projects, $2.4 billion from the Department of Defense counterdrug account, and $600 million from a Treasury Department forfeiture fund to fund the border wall construction. “President Donald J. Trump’s Border Security Victory,” Fact Sheet, The White House, February 15, 2019.



44 See “Construction Authority in the Event of a Declaration of War or National Emergency,” 10 U.S. Code § 2808.



45 The definition of a national emergency in U.S. code simply means a declaration of emergency by the president. See “Support for Counterdrug Activities and Activities to Counter Transnational Organized Crime,” 10 U.S. Code § 284.



46 See “Termination of Existing Declared Emergencies,” 50 U.S. Code § 1601.



47 Matthew Cox, “Army Scaling Back Recruiting Goals after Missing Target, Under Secretary Says,” Mil​i​tary​.com, March 21, 2019.



48 Jamie Crawford, “Military Sexual Assaults Increase Sharply, Pentagon Report Finds,” CNN, May 2, 2019; and Patricia Kime, “Active‐​Duty Military Suicides at Record Highs in 2018,” Mil​i​tary​.com, January 30, 2019.



49 The fiscal year 2020 National Defense Authorization Act increased overall military end strength by 1,400 troops. See Pat Towell, _FY2020 National Defense Authorization Act: P.L. 116–92 (H.R. 2500, S. 1790)_ , CRS Report R46144 (Washington: Congressional Research Service, January 2, 2020), p. 8.



50 Integrated Fire Control‐​Counter Air System is the Navy’s multidomain battle management system; the Advanced Battle Management System is the technical engine that manages all communications, orders, and sensors that the Air Force uses.



51 Dan Gouré, “A New Joint Doctrine for an Era of Multi‐​Domain Operations,” _Real Clear Defense_ , May 24, 2019; and Grant J. Smith, “Multi‐​Domain Operations: Everyone’s Doing It; Just Not Together,” _Over the Horizon_ , June 24, 2019.



52 Theresa Hitchens, “Navy, Air Force Chiefs Agree to Work on All Domain C2,” _Breaking Defense_ , November 12, 2019.



53 Examples of these efforts include the Commandant of the Marine Corps Strategist Program (https://​www​.usm​cu​.edu/​A​c​a​d​e​m​i​c​-​P​r​o​g​r​a​m​s​/​C​M​C​-​F​e​l​l​o​w​s​-​S​t​r​a​t​e​g​i​s​t​s​-​F​o​r​e​i​g​n​-​P​M​E​-​O​l​m​s​t​e​d​-​S​c​h​o​l​a​r​s​/​C​o​m​m​a​n​d​a​n​t​-​o​f​-​t​h​e​-​M​a​r​i​n​e​-​C​o​r​p​s​-​S​t​r​a​t​e​g​i​s​t​-​P​r​o​gram/) and Department of Defense STEM scholarships (https://​dod​stem​.us/​s​t​e​m​-​p​r​o​g​r​a​m​s​/​s​c​h​o​l​a​r​ships).



54 The military doesn’t currently use the term “data wrangler,” but it is a common term in the film industry used to identify the person responsible for collecting and storing digital footage. This process is much the same in the military, where all data generated needs to be collected, transformed, stored, and analyzed. In short, the U.S. military needs to establish a system to identify and task battlefield data managers.



55 See Kessel Run (website), U.S. Air Force, https://​kessel​run​.af​.mil/​roles.



56 For example, the Pentagon has repeatedly tried to terminate production of new M1 Abrams tanks, but Congress continued to fund them over these objections. See Associated Press, “Army: Thanks but No Tanks,” _Politico_ , April 28, 2013. Similarly, Congress authorized 12 more F-35 Lightning II aircraft than the Trump administration requested in fiscal year (FY) 2020. See Towell, _FY2020 National Defense Authorization Act_ , pp. 20–21. The continued legislative requirement for aircraft carriers similarly complicates long‐​range shipbuilding plans. The Department of Defense’s request for the _Columbia_ -class ballistic missile submarine for FY 20 was $1.7 billion for procurement and $533 million for research and development; however, Congress authorized $1.8 billion for procurement and $548 million in research and development for FY 20. See Towell, _FY2020 National Defense Authorization Act_ , p. 10.



57 Quantum computing is the computational method utilizing superposition and entanglement to process calculations orders of magnitude faster than current microprocessors. Note: Quantum supremacy is different from quantum advantage, which is when a quantum device solves a problem _faster_ than a traditional computer. Thanks to James Knupp for clarifying this concept. See Frank Arute et al., “Quantum Supremacy Using a Programmable Superconducting Processor,” _Nature_ 574 (2019): 505–510. 



58 Benjamin Jensen, Scott Cuomo, and Chris Whyte, “Wargaming with Athena: How to Make Militaries Smarter, Faster, and More Efficient with Artificial Intelligence,” _War on the Rocks_ , June 5, 2018. 



59 See, for example, Robert M. Farley, _Grounded: The Case for Abolishing the United States Air Force_ (Lexington: University Press of Kentucky, 2014).



60 Todd Harrison, _The Air Force of the Future: A Comparison of Alternative Force Structures_ (Washington: Center for Strategic and International Studies, October 2019).



61 Operational readiness is the capacity for a unit to perform its designated combat or combat‐​support functions.



62 Dan Grazier, “F-35: Is America’s Most Expensive Weapon of War the Ultimate Failure?,” _National Interest_ , March 19, 2019; Kristin Houser, “Hard Landing: U.S. Military’s Trillion‐​Dollar F-35 Fighter Jet Is Almost Unflyable,” _Futurism_ , June 13, 2019; Michael P. Hughes, “What Went Wrong with the F-35, Lockheed Martin’s Joint Strike Fighter?,” _The Conversation US_ , June 14, 2017; Jonathan Lowell, “A U.S. Air Force Pilot Describes How He Landed His F-35 Safely after a Mid‐​Air Power Failure,” _Business Insider_ , August 27, 2019; and Eric Tegler, “WTF-35: How the Joint Strike Fighter Got to Be Such a Mess,” _Popular Mechanics_ , July 27, 2018. The program initially called for 2,000 aircraft of all variants by the end of fiscal year 2019 but was to have produced only 500 over that period: Michael J. Sullivan, _F-35 Joint Strike Fighter: Action Needed to Improve Reliability and Prepare for Modernization Efforts_ , GAO-19–341 (Washington: Government Accountability Office, April 2019), p. 6. The F-35A model, flown by the Air Force, is set to drop from $89.2 million to $77.9 million in 2022: Marcus Weisgerber, “Price of F-35 Falls, but Not as Much as Pentagon Hoped,” _Defense One_ , October 29, 2019.



63 Anti‐​access/​area‐​denial is an operational concept that complicates an opponent’s ability to use air, naval, and land power at long distance, which typically entails the use of land‐​based sensors and precision strike systems to target opponent ships, aircraft, and bases.



64 Harrison, _The Air Force of the Future_.



65 Harrison.



66 Brenda S. Farrell, _Military Personnel: Strategy Needed to Improve Retention of Experienced Air Force Aircraft Maintainers_ , GAO-19–160 (Washington: Government Accountability Office, February 2019).



67 Rachel S. Cohen, “USAF’s New Info Warfare Group Coming into Focus,” _Air Force Magazine_ , September 18, 2019. Regarding the F-15EX, see Kyle Mizokami, “After Nearly 20 Years, the Air Force Will Fly Brand New F‐​15s,” _Popular Mechanics_ , January 29, 2020.



68 Pete Geren and George W. Casey Jr., _A Statement on the Posture of the United States Army 2009_ (Washington: U.S. Army, May 2009).



69 For more information about Army Futures Command, visit https://​www​.army​.mil/​f​u​tures.



70 The Army later added two additional cross‐​functional teams, “Synthetic Training Environment” and “Assured Positioning, Navigation and Timing.”



71 Dave Philipps, “As Economy Roars, Army Falls Thousands Short of Recruiting Goal,” _New York Times_ , September 21, 2018.



72 Microtargeting is a direct marketing technique that segments consumers by tastes or attributes.



73 Eric J. Labs, _An Analysis of the Navy’s Fiscal Year 2020 Shipbuilding Plan_ (Washington: Congressional Budget Office, October 2019), p. 3.



74 The prior year, for example, the Congressional Budget Office similarly concluded that the cost of the Navy’s plan for new‐​ship construction ($26.7 billion) would nearly double its historical average of $13.6 billion. See Eric J. Labs, _Analysis of the Navy’s Fiscal Year 2019 Shipbuilding Plan_ (Washington, Congressional Budget Office, October 2018), p. 3. A report 10 years earlier had reached a similar conclusion: the estimated costs to fulfill all the Navy’s wishes were nearly double what it was likely to receive given historical funding averages. See Dale Eisman, “Navy’s Shipbuilding Wish List Sails into Troubled Waters,” _The Virginian‐​Pilot_ , March 15, 2008.



75 David B. Larter, “Acting US Navy Secretary: Deliver Me a 355‐​Ship Fleet by 2030,” _Defense News_ , December 9, 2019.



76 Rebecca Kheel, “Pentagon Proposes $704B Budget with Boost for Nukes, Cuts to Ships,” _The Hill_ , February 10, 2020.



77 Quoted in David B. Larter, “In a Quest for 355 ships, US Navy Leaders Are Unwilling to Accept a Hollow Force,” _Defense News_ , January 13, 2020. See also Nick Blenkey “Acting Secnav Commits to 355 Ship Navy, but Not at $2 Billion Apiece,” _MarineLog_ , January 10, 2020.



78 See, for example, Eric Edelman et al., _Providing for the Common Defense: The Assessment and Recommendations of the National Defense Strategy Commission_ (Washington: United States Institute of Peace, November 2018), p. 63.



79 Dakota L. Wood, ed., _2020 Index of U.S. Military Strength with Essays on Great Power Competition_ (Washington: Heritage Foundation, November 2019), p. 349.



80 John Pendleton, _Navy Force Structure: Sustainable Plan and Comprehensive Assessment Needed to Mitigate Long‐​Term Risks to Ships Assigned to Overseas Homeports_ , GAO-15–329 (Washington: Government Accountability Office, May 2015); see also Geoff Ziezulewicz, “Navy’s 7th Fleet No Stranger to High Ops Tempo,” _Navy Times_ , August 21, 2017.



81 Ballistic missile submarines (SSBNs) are the sea‐​based leg of the nuclear triad. These vessels carry Trident missiles, which are each capable of delivering up to eight nuclear warheads. Fast‐​attack submarines (SSN) are the U.S. Navy’s primary undersea platform, capable of offensive action both at sea and against targets on land.



82 Aircraft carriers (CVNs) are the largest ships in the U.S. Navy and the centerpiece of U.S. fleet operations, capable of carrying about 60 aircraft of varying types. The “N” in the hull classification of a CVN denotes that it employs nuclear propulsion. Guided‐​missile frigates (FFG) are a mixed‐​armament warship lighter than a destroyer, which are typically focused on anti‐​ship and anti‐​submarine warfare. The FFG(X) is the next generation of multimission guided‐​missile frigates.



83 Kyle Mizokami, “USS Ford Will Set Sail with Only 2 out of 11 Weapon Elevators,” _Popular Mechanics_ , October 12, 2019.



84 Arresting gear is the mechanical system that rapidly decelerates aircraft when they land on a platform, such as an aircraft carrier. Dual band radars combine radar systems into one integrated system for easier operation, maintenance, upgrade, and targeting.



85 Robert F. Behler, _Director, Operational Test and Evaluation: FY 2018 Annual Report_ , Department of Defense, December 2018, p. 131.



86 Behler, _FY 2018 Annual Report_ , p. 134.



87 Justin Katz, “As Navy Touts $14.9B Dual Carrier Buy Contract, DOT&E Report Calls Out ‘Unrealistic Assumptions’ about CVN-78,” _Inside Defense_ , February 1, 2019.



88 Craig Hooper, “The Most Expensive Ship in the World Is Broken. The U.S. Navy Secretary Should Be Held Accountable,” _Forbes_ , October 16, 2019.



89 Henry J. Hendrix, _At What Cost a Carrier?_ , Disruptive Defense Papers (Washington: Center for a New American Security, March 2013), p. 3.



90 T. X. Hammes, “We Need to Start Thinking Differently about Maritime Airpower—and We Can,” _Task and Purpose_ , September 19, 2018.



91 Paul McLeary, “All 6 East Coast Carriers in Dock, Not Deployed: Hill Asks Why,” _Breaking Defense_ , October 28, 2019.



92 _Executive Summary: 2016 Navy Force Structure Assessment (FSA)_ (Washington: U.S. Department of the Navy, December 15, 2016), p. 2; and Ronald O’Rourke, _Navy Frigate (FFG[X]) Program: Background and Issues for Congress_ , CRS Report R44972 (Washington: Congressional Research Service, April 28, 2020), p. 1.



93 John Cole and Thomas Ulmer, “Bad Idea: Reactivating the U.S. Navy’s Oliver Hazard Perry‐​Class Frigates,” _Defense 360_ , December 7, 2017; and David B. Larter, “Don’t Reactivate the Old Frigates, Internal US Navy Memo Recommends,” _Defense News_ , November 12, 2017.



94 David B. Larter, “The US Navy’s New, More Lethal Frigate Is Coming into Focus,” _Defense News_ , January 28, 2019. In late April 2020, the Navy selected a design by Italian shipmaker Fincantieri to be built at the Marinette Marine shipyard in Wisconsin. David B. Larger, “The US Navy Selects Fincantieri Design for Next‐​Generation Frigate,” _Defense News_ , April 30, 2020.



95 Labs, _Analysis of the Navy’s Fiscal Year 2019 Shipbuilding Plan_ , p. 25.



96 O’Rourke, _Navy Frigate (FFG[X]) Program_ , p. 26, Table 3.



97 Wood, _2020 Index of U.S. Military Strength_ , p. 375.



98 Maritime choke points are heavily trafficked, narrow waterways such as straits and narrows.



99 Leo Spaeder, “Sir, Who Am I? An Open Letter to the Incoming Commandant of the Marine Corps,” _War on the Rocks_ , March 28, 2019.



100 David H. Berger, _Commandant’s Planning Guidance: 38th Commandant of the Marine Corps_ (Washington: U.S. Marine Corps, 2019).



101 Berger, _Commandant’s Planning Guidance_. The Marine Corps _Force Design 2030_ offers more details about how the service will turn Berger’s guidance into reality. See _Force Design 2030_ (Washington: U.S. Marine Corps, March 2020). To learn more about how the 2030 force design could help the service implement a restraint‐​focused grand strategy, see Eric Gomez, “Marine Corps Changes Inch U.S. Closer to a Restraint‐​Friendly Military Posture,” _Cato at Liberty_ (blog), Cato Institute, March 24, 2020.



102 Minnie Chan, “China’s Navy Is Being Forced to Rethink Its Spending Plans as Cost of Trade War Rises,” _South China Morning Post_ , May 26, 2019.



103 Scott Cuomo et al., “How the Marines Will Help the U.S. Navy and America’s Allies Win the Great Indo‐​Pacific War of 2025,” _War on the Rocks_ , September 26, 2018.



104 Quoted in Richard R. Burgess, “Marine Commandant Berger: Force Design Is Top Priority,” _Seapower Magazine_ , July 18, 2019.



105 Sam LaGrone and Megan Eckstein, “Failure of Two Ships to Participate in RIMPAC Highlight Amphibious Readiness Gap,” _USNI News_ , August 1, 2018.



106 For more on the “deterrence gap” concept, see Keith B. Payne, “The Emerging Nuclear Environment: Two Challenges Ahead,” National Institute for Public Policy Information Series no. 436, January 2, 2019; and Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 55.



107 Michael Bennett, _Approaches for Managing the Costs of U.S. Nuclear Forces, 2017 to 2046_ (Washington: Congressional Budget Office, October 2017), p. 1; and Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 55.



108 The exact amount of fiscal year (FY) 2019 and FY 20 appropriations for the Missile Defense Agency (MDA) was $10.491 billion and $10.452 billion, respectively. For a breakdown of funding by program, see Wes Rumbaugh, “FY 2020 Missile Defense Agency Budget Tracker,” Missile Threat, Missile Defense Project, Center for Strategic and International Studies, December 30, 2019. The Trump administration proposed $20.3 billion for missile defense and defeat in its FY 21 budget submission, including $9.2 billion for the MDA. See Jon Harper, “Budget 2021: Trump Proposes Flat Pentagon Budget,” _National Defense_ , February 10, 2020.



109 Rachel S. Cohen, “B61-12 Nuclear Warhead Delay Drives Up Price Tag,” _Air Force Magazine_ , September 25, 2019; and Sara Sirota, “GAO: B61-12 LEP First Production Unit Delayed, W80-4 LEP Cost Estimate Increased,” _Inside Defense_ , June 18, 2019.



110 As Kingston Reif and Mandy Smithberger note, “the B-2 bomber program overran its cost so badly that a mere 20 aircraft emerged from a $40 billion program [that originally] intended to buy 135 to 150 aircraft.” Kingston Reif and Mandy Smithberger, “America’s New Stealth Bomber Has a Stealthy Price Tag,” _Defense One_ , May 21, 2018.



111 Dennis Evans and Jonathan Schwalbe, _The Long‐​Range Standoff (LRSO) Cruise Missile and Its Role in Future Nuclear Forces_ (Laurel, MD: Johns Hopkins Applied Physics Laboratory, 2017), p. 8.



112 Olga Oliker and Andrey Baklitskiy, “The Nuclear Posture Review and Russian ‘De‐​Escalation:’ A Dangerous Solution to a Nonexistent Problem,” _War on the Rocks_ , February 20, 2018; and Olga Oliker, “U.S. and Russian Nuclear Strategies: Lowering Thresholds, Intentionally and Otherwise,” in _America’s Nuclear Crossroads: A Forward‐​Looking Anthology_, eds. Caroline Dorminey and Eric Gomez (Washington: Cato Institute, July 2019), pp. 37–46.



113 For more on these recommendations, see Caroline Dorminey, “Buying the Bang for Fewer Bucks: Managing Nuclear Modernization Costs,” in _America’s Nuclear Crossroads_, pp. 1–15.



114 Eric Gomez, “It Can Get You into Trouble, but It Can’t Get You Out: Missile Defense and the Future of Nuclear Stability,” in _America’s Nuclear Crossroads_, p. 17.



115 Michael D. Griffin and Rebeccah L. Heinrichs, “Ensuring U.S. Technological Superiority: An Update from Under Secretary Michael D. Griffin,” (interview, Hudson Institute, Washington, August 23, 2019); and Patrick Tucker, “Trump’s New Missile Policy Relies Heavily on Largely Unproven Technologies,” _Defense One_ , January 17, 2019.



116 Kelsey Davenport, “The Joint Comprehensive Plan of Action (JCPOA) at a Glance,” Arms Control Association, May 2018; and Maggie Tennis, “Preserving the U.S. Arms Control Legacy in the Trump Era,” in _America’s Nuclear Crossroads_, pp. 81–84.



117 Ben Hubbard, Palko Karasz, and Stanley Reed, “Two Major Saudi Oil Installations Hit by Drone Strone, and U.S. Blames Iran,” _New York Times_ , September 14, 2019; and Dan Lamothe, “U.S. to Send 1,800 Additional Troops to Saudi Arabia to Boost Defenses against Iran,” _Washington Post_ , October 11, 2019.



118 David Barno and Nora Bensahel, “Fighting and Winning in the ‘Gray Zone,’” _War on the Rocks_ , May 19, 2015.



119 Stephen Biddle and Ivan Oelrich, “Future Warfare in the Western Pacific: Chinese Antiaccess/​Area Denial, U.S. AirSea Battle, and Command of the Commons in East Asia,” _International Security_ 41, no. 1 (Summer 2016): 7–48; and Michael Kofman, “It’s Time to Talk about A2/AD: Rethinking the Russian Military Challenge,” _War on the Rocks_ , September 5, 2019.



120 Ted Galen Carpenter and Eric Gomez, “East Asia and a Strategy of Restraint,” _War on the Rocks_ , August 10, 2016.



121 For more, see Eric Gomez, “The Future of Extended Deterrence: Are New U.S. Nuclear Weapons Necessary?,” in _America’s Nuclear Crossroads_, p. 58.



122 Terence Roehrig, _Japan, South Korea, and the United States Nuclear Umbrella: Deterrence after the Cold War_ (New York: Columbia University Press, 2017), p. 15.



123 Barry R. Posen, _Inadvertent Escalation: Conventional War and Nuclear Risks_ (Ithaca: Cornell University Press, 1991), pp. 2–3; and Caitlin Talmadge, “Would China Go Nuclear? Assessing the Risk of Chinese Nuclear Escalation in a Conventional War with the United States,” _International Security_ 41, no. 4 (Spring 2017): 53–55.



124 For information about technical developments that blur the nuclear/​conventional distinction, see James M. Acton, “Escalation through Entanglement: How the Vulnerability of Command‐​and‐​Control Systems Raises the Risks of an Inadvertent Nuclear War,” _International Security_ 43, no. 1 (Summer 2018): 63–65. On military strategies that increase escalation risks, see Talmadge, “Would China Go Nuclear?,” p. 53; and Tong Zhao and Li Bin, “The Underappreciated Risks of Entanglement: A Chinese Perspective,” in _Entanglement: Russian and Chinese Perspectives on Non‐​Nuclear Weapons and Nuclear Risks_ , ed. James M. Acton (Washington: Carnegie Endowment for International Peace, 2017), pp. 58–59.



125 Gomez, “It Can Get You into Trouble, but It Can’t Get You Out,” pp. 25–28.



126 For a brief explainer on what a space sensor layer for missile defense could look like, see “Missile Defense Tracking System, Space Sensor Layer (SSL), Hypersonic and Ballistic Tracking Space Sensor (HBTSS),” Glob​alse​cu​ri​ty​.org, https://​www​.glob​alse​cu​ri​ty​.org/​s​p​a​c​e​/​s​y​s​t​e​m​s​/​h​b​t​s​s.htm.



127 William J. Burns, “The Demolition of U.S. Diplomacy: Not Since Joe McCarthy Has the State Department Suffered Such a Devastating Blow,” _Foreign Affairs_ , October 14, 2019, https://www.foreignaffairs.com/articles/2019–10-14/demolition-us-diplomacy.



128 There are several issues that we did not deal with in this analysis, but the Cato Institute intends to issue defense policy and budget analyses annually, with each report focusing on three to four core challenges. Next year, for example, will include a focus on two relatively ignored aspects of the budgeting process: cybersecurity and technology, in general, and workforce issues, including retention, recruitment, and education.
"
"The Trump administration has offered oil companies a chunk of the American west and the Gulf of Mexico that’s four times the size of California – an expansive drilling plan that threatens to entrench the industry at the expense of other outdoor jobs, while locking in enough emissions to undermine global climate policy. Energy companies have leased 9.9m acres from the unprecedented 461m acres put up for rent by the Trump administration, according to a new analysis from the Wilderness Society. The fossil fuels extracted from those leases could equal half a year of emissions from China, the world’s top carbon polluter. The administration has jump-started this plan, independent government analysts say, by offering energy leases at bargain rates. That has lured drilling companies to pristine lands where an outdoor economy had already grown up around wildlife and the natural landscape. Trump’s Democratic opponents vow to close public lands to new drilling. But they probably can’t stop the extraction that Trump has already started. Despite a glut in US oil supply, the federal government has proposed leasing places like the Slickrock Bike Trail in Moab, Utah, where visitors pedal through petrified sand dunes and ancient seabeds. Locals have warned the potential air and water pollution aren’t worth damaging this national treasure. “If we really get to the point that we need to burn the Picassos to heat the house for an hour, we could still do that, but there’s no reason to lease these parcels now when they have a higher and better use,” said Ashley Korenblat, CEO of Western Spirit Cycling and managing director of the not-for-profit group Public Land Solutions. The leases are being signed as world scientists stress that pollution from oil, gas and coal needs to decline rapidly to avert catastrophe. Conservationists say the developed land will never again be wild and experts have repeatedly shown the sales aren’t even earning a fair return for taxpayers, costing the federal government and states billions of dollars. “We’re in an era now where fundamental questions need to be raised about whether there should be more leasing or not. Millions of acres are already under lease that are not being developed,” said David Hayes, the former deputy secretary of the interior department under President Barack Obama. “So why is the administration going so hard and fast over putting additional acreage up?” Just as the Obama administration halted coal leasing on federal lands, Hayes argues the federal government should consider a moratorium for oil and gas. At the least, Trump officials should not allow drilling on frontier and sensitive areas, said Hayes, who is now the director of the State Energy and Environmental Impact Center at New York University School of Law. Nearly all the Democrats running for presidentwould ban fossil fuel extraction on federal land. Chase Huntley, energy and climate change director with the Wilderness Society, said the “tremendous area” the administration has offered to private companies and its disregard for the environment and climate “suggests the administration’s real interest here: which is advancing their agenda of energy dominance regardless of who it hurts”. The interior department and its Bureau of Land Management, which handles onshore leasing, did not respond to requests for comment. Neither did the trade group for the US oil and gas industry, the American Petroleum Institute. Much of the drilling on Trump-leased areas won’t happen now, while global supply is high and oil and gas prices are so low that the acres wouldn’t turn a large enough profit, but the Wilderness Society report shows that as a portion of the leases are acted upon they will be responsible for substantial pollution. On the low end, the leases could result in emissions equal to the annual output of Brazil. Pete Erickson, a senior scientist with the Stockholm Environment Institute, reviewed the analysis and called the low-end estimates “conservative”. Earth is already 1.1C hotter than it was before humans began to burn fossil fuels for industry. To keep the planet from climbing to 1.5C hotter, oil emissions would need to drop from 13 gigatons of carbon dioxide per year to less than 8 gigatons by 2030, according to an analysis by the climate change news organization Carbon Brief. Gas emissions would need to decline from 8 gigatons to fewer than 5 gigatons. In a report ordered by the Obama administration, the US Geological Survey in 2018 found that public lands account for 24% of the country’s emissions and vegetation on the same lands absorbs 15% of the nation’s carbon dioxide pollution. But the Trump administration has largely refused to consider what its actions on public lands will mean for the climate crisis. Last month the White House proposed changes to how it applies a bedrock environment law – the 50-year-old National Environmental Policy Act. Under the revisions, the government would no longer need to count climate impacts when approving infrastructure such as pipelines or leasing land for oil and gas drilling. The administration has also made revisions to rules that protected the environment but proved inconvenient for the oil and gas industry. “If there’s anything in the way of oil and gas leasing, it’s going to be pushed aside,” Hayes said, noting the reinterpretation of the Migratory Bird Act, changes to an agreement for protecting sage grouse, the shrinking of national monuments in Utah and the opening of sensitive portions of the Arctic National wildlife refuge and National Petroleum reserve to development. Independent analyses have found the US is not earning a fair amount from oil and gas leasing either. The Congressional Budget Office in 2016 concluded the federal government could increase its share of income from onshore leases by up to $1.2bn over 10 years. That figure would be about twice as large if it counted the money that would go to states. Audits by the Government Accountability Office have drawn similar conclusions that the government could demand more money. If the government had raised royalty rates years ago, it could have earned up to $12bn more for taxpayers between 2009 and 2019, according to the non-partisan fiscal policy group Taxpayers for Common Sense. States would have received about half that income. “We have policies that govern the way we lease our federal lands for oil and gas development that are a century old, and they just haven’t kept pace. And we have others that haven’t been looked at in decades. This has led to really a tremendous giveaway [to the oil and gas industry],” said Autumn Hanna, vice-president of the non-partisan fiscal policy group Taxpayers for Common Sense.  In addition to expanding leasing opportunities, the Trump administration has opened lands that were once protected from industry. Last week the interior department finalized plans that allow mining, drilling and other development on lands recently removed from Bears Ears and Grand Staircase-Escalante national monuments in Utah. The landscapes there feature “classic red rock canyons”, “forested mesas”, and “fantastical geologic features”, ranging from standing rock pillars to streams that flash-flood several times a year, according to Steve Bloch, the conservation director at the Southern Utah Wilderness Alliance. The region has a dense concentration of prehistoric sites, where researchers are able to study the cultures of indigenous tribes. Bloch said that doesn’t matter to the federal government. “It’s clear from the administration’s approach to public lands, to their energy dominance agenda, that they believe the highest and best use of these places is for fossil fuel development,” Bloch said. Of the 9.9m acres leased, 2.2m acres are in Wyoming and 1.3m acres are in Alaska. Nearly 5m acres are in the Gulf of Mexico. Adam Kolton, executive director of the Alaska Wilderness League, said “at the broadest level, what we’re witnessing is a sort of wholesale turnover of America’s Arctic to the oil and gas industry”. Kolton said drilling on public lands was once done alongside arguments that the US needed to be energy independent, but the country is now a net exporter of oil. He believes the expansion of drilling, particularly in the Arctic, will be among Trump’s most damaging and longest-lasting environmental legacies. “Right now there’s an attempt to not just roll back what was done under Obama in Alaska, but to turn back the clock half a century and look to an era where oil, mining and logging operations were unchecked,” Kolton said. "
"BP’s new chief executive has set an ambitious target to shrink the oil firm’s carbon footprint to net zero by 2050 by cutting more greenhouse gas emissions every year than produced by the whole of the UK. Bernard Looney, who replaced Bob Dudley as chief executive this month, said it was clear that BP needed to change. He said BP would aim to become a net zero company by 2050 or sooner by tackling “all the carbon we get out of the ground as well as all the greenhouse gases we emit from our operations”. BP is following the lead of other large oil firms by setting a target to reduce its contribution to the climate crisis, which will require the company to remove more than 400m tonnes of carbon emissions a year from its oil and gas business. Looney expects BP will “invest more in low-carbon businesses – and less in oil and gas – over time”, but will not set out the detail of how BP plans to meet the goals until an investor meeting in September. “Today is about a vision, a direction of travel,” he told an audience of investors and industry analysts. “I appreciate you want to see more than a vision. We don’t have that for you today, but we will in September. The direction is set. We are heading to net zero. There is no turning back.” The former head of BP’s oil production business said BP would still be producing oil and gas in 2050, but less than it produces today. The net zero strategy will require BP to cut or offset around 360m tonnes of greenhouse emissions created by the oil and gas it produces every year through measures such as tree-planting, and carbon capture technologies. Looney also assured investors that BP would safeguard the $8bn paid out in shareholder dividends every year by becoming “a force for good as well as a provider of competitive returns”. Helge Lund, the chairman, said: “The board supports Bernard and his new leadership team’s ambition for BP. Aiming for net zero is not only the right thing for BP, it is the right thing for our shareholders and for society more broadly.” BP’s green goals have divided environmental campaigners. Some welcomed the company’s ambition; others criticised the absence of a clear strategy. Murray Worthy, a senior campaigner at Global Witness, said BP’s net zero pledge “looks like an attempt to grab some positive headlines by a new CEO but with little of substance to show how it will achieve these grand claims. “Saying that they will invest more in low-carbon tech and less in oil and gas ‘over time’ is not a credible plan for reaching net zero.” The ambitions were welcomed by shareholder pressure group Climate Action 100+, the Institutional Investors Group on Climate Change (IIGCC), green investor body Follow This and the Church of England Commissioners, which have all collaborated with BP’s executives to help tackle the climate crisis. Stephanie Pfeifer, a member of Climate Action 100+ and the chief executive of the IIGCC, said investors would continue to look for progress from BP including how it would invest more in non-oil and gas businesses and ensure its lobbying activity supports delivery of the Paris agreement. An investigation by the Guardian last year revealed that the company planned to grow its production of oil and gas by about a fifth between 2018 and 2030, despite warnings that an increase in fossil fuel production would put the world on track for catastrophic global heating and a runaway climate crisis. “I know many may doubt our intentions, based on seeming inconsistencies between what we say and what we do. I get that,” Looney said. “We are taking steps to more firmly and visibly align our intentions with our actions and become much more transparent.”  BP said it aimed to play a more active role in lobbying for policies that would spur action on the climate crisis, and would cut its spending on corporate reputational sponsorship and redirect the funds towards promoting climate action. Looney set out the plans for a greener BP alongside one of the most radical corporate overhauls for the company in decades. The shakeup will replace BP’s traditional distinction between upstream, which produces oil and gas, and downstream, which refines the fossil fuels to sell petroleum products. Instead there will be four new business areas: production and operations, customers and products, gas and low-carbon energy, and innovation and engineering. “We’ll still be an energy company but a different kind of energy company,” Looney said."
"

Since 9/11, the U.S. government has poured a breathtaking amount of resources into investigating suspected terrorism operations within the United States. A key component of these investigations is known as “ghost‐​chasing” — the thousands of leads and tips investigated daily, and classified as “threats,” despite the fact that only one in 10,000 fails to be false.



These efforts are often criticized on the basis of civil liberties abuses. But, convinced that terrorism is an “existential” threat, many people are perfectly comfortable overlooking these abuses. In their new book, _Chasing Ghosts: The Policing of Terrorism_ , Cato’s John Mueller and Mark G. Stewart of the University of Newcastle, Australia, take aim at the very premises of U.S. counterterrorism operations. Is terrorism truly a significant threat? Are most would‐​be terrorists actually skilled enough to pull off an attack? Is it true that we can “never be safe enough”? Mueller and Stewart examine the methods of the FBI, National Security Agency, the Department of Homeland Security, and local policing agencies, revealing the government’s exaggerated claims about the “threats” they divert. The question, they write, is not whether any real terrorists exist — but whether the chase is worth the cost.



 **You Might Be a “Lukewarmer” If …**  
When it comes to global warming, most people think there are two camps: “alarmist” or “denier” being their respective pejoratives. Either you acknowledge the existence of manmade climate change and consider it a dire global threat, or you deny it exists at all. But there’s a third group: the “lukewarmers.” As Cato scholars Pat Michaels and Paul C. Knappenberger write in their new ebook, _Lukewarming: The New Climate Science that Changes Everything_ , “Lukewarmers believe the evidence of some human‐​caused climate change is compelling, but it is hardly the alarming amount predicted by models.”



Lukewarmers are skeptical that government pacts, like those sought at the 2015 United Nations Climate Change Conference in Paris, will do much to temper climate change’s effects. They also tend to question the incentive structure of climate science, where scientists are vying for millions of dollars of government funding — meaning that any proposal that global warming’s effects have been overforecast “threatens to derail everyone else’s gravy train.” This, they argue, has brought about “a systemic distortion in the direction of alarmism.” Lukewarmingtells a different story — one that ends with optimism. “Lukewarmers know,” they write, “that economic development is the key in adaptation to the vagaries of weather and climate, even climate change induced by people.”



 **Property Rights after _Kelo_**  
When Cato published the first edition of _Cornerstone of Liberty: Property Rights in 21st Century America_ , the infamous Supreme Court case of _Kelo v. New London_ had only recently been decided, declaring that the government can seize private property by eminent domain under a broad definition of “public use.” In the decade since, by one estimate, the government has taken over a million homes from their owners. Cato adjunct scholar Timothy Sandefur of the Pacific Legal Foundation and his wife Christina, vice president for policy at the Goldwater Institute, set out to revise the book for its second edition — but, as they write, “So much has happened in the years after _Kelo_ that what started as a simple update to this book became a complete renovation.”



As in the first edition, the Sandefurs narrate the heartrending stories of Americans forced from their homes, explaining along the way how property rights became eroded. But this updated edition also contains a wealth of new material on the ever‐​changing threats to property owners. The Sandefurs conclude by examining the backlash from _Kelo_ and suggesting a new path forward. As Washington Post columnist George Will wrote, “Not since Babe Ruth and Lou Gehrig has there been a one‐​two punch quite like Timothy and Christina Sandefur. Both lawyers. Both authors. Both helping shape the country.”



 **Paving the Path to Growth**  
“If you could wave a magic wand and make one or two policy or institutional changes to brighten the U.S. economy’s long‐​term growth prospects, what would you change and why?” Brink Lindsey, Cato’s vice president for research, posed this question to 51 prominent economists and policy experts for his ebook _Reviving Economic Growth_.



Their ensuing essays constitute a “brainstorming” session from an eclectic group of contributors, featuring libertarian, progressive, and conservative perspectives. “By bringing together thinkers one doesn’t often see in the same publication,” writes Lindsey, “my hope is to encourage fresh thinking about the daunting challenges facing the U.S. economy — and, with luck, to uncover surprising areas of agreement that can pave the way to constructive change.”



In a second ebook, _Understanding the Growth Slowdown_ , Lindsey and his contributors dive yet again into the pressing questions surrounding the disappointing performance of the U.S. economy in recent years. Lindsey asks whether this could be more than a temporary trend, but rather the “new normal” — and if so, why. “The U.S. economy is a phenomenon of mind‐​boggling complexity,” Lindsey observes. These collected essays don’t aim to provide all the answers, but to provoke new ideas — without which an economic revival will certainly not be possible.
"
"Tim Flannery (The age of the megafire is here, and it’s a call to action, Journal, 7 February) writes: “As far as swift climate action is concerned, all good choices have gone up in smoke”. That may not be the case, however. There has been abundant support by now for the claim made by Martin Fleischmann and Stanley Pons in 1989 to have observed nuclear fusion at ordinary temperatures, but the hope that such a fossil-fuel-free process might contribute usefully to energy production has not been fulfilled because it is very unpredictable, and we do not as yet know the conditions needed to produce large amounts of energy. Suitably funded research on a large scale might lead to a resolution of this issue.Prof Brian JosephsonEmeritus professor of physics, University of Cambridge  • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"

The heinous bombing of the Alfred P. Murrah federal building in Oklahoma City has understandably raised public fears of terrorism. As is common after sensational crimes, some persons have revived their call for a bigger federal government and a narrower Constitution. This Policy Analysis examines various restrictions on civil liberty which have been proposed as a response to Oklahoma City.



Since draconian legislation is sometimes justified as being what the people demand, two points should be kept in mind. First, a large fraction of the population, not just a tiny fringe, is afraid of the federal government. According to a recent Gallup poll, 39 percent of Americans believe “the federal government has become so large and powerful it poses an immediate threat to the rights and freedoms of ordinary citizens.” If the word “immediate” is omitted, 52% of the population is afraid of the federal government.



Contrary to stereotypes about “angry white men,” people fearful of misuse of federal power tend to be female more than male, black more than white, and liberal more than conservative. Repressive measures, rather than reassuring the American public, will intensify the fears which are already widely shared.



Second, in the aftermath of a tragedy, it is not hard for insta‐​polls to report large majorities in favor of various repressive measures, especially when described at a high level of generality, with all the repressive details left out. (For example, “Should the government have more power to keep an eye on terrorist groups?” will obtain higher poll numbers than “Should the government be allowed to infiltrate non‐​violent, non‐​criminal dissident groups and be allowed to use wiretaps without a court order”?) In the long run, public officials are expected to exercise judgement, and not blindly rush into measures which may have short‐​term popularity. The most thorough public opinion survey of Americans’ attitudes towards Constitutional rights included this question: “Suppose the President and Congress have to violate a Constitutional principle to pass an important law the people wanted. Would you support them in this action?” Twenty‐​eight percent said yes, “because the Constitution shouldn’t be allowed to stand in the way of what the people need and want.” Forty‐​nine percent said no, “because protecting the Constitution is more important to the national welfare than any law could possibly be.”(1)



Indeed, the precise reason for putting certain fundamental rights in the Constitution is to protect them from transient majorities.(2) In long‐​term perspective, the herding of American citizens of Japanese descent into concentration camps during WWII was a horrible human rights violation.(3) But at the time, public opinion and the press heavily favored the concentration camps, despite the total lack of evidence that these Americans were disloyal. And certainly national security was in far graver danger in early 1942 than it is today.



But because terrorism, like child abuse (or Communism in previous decades) provokes such intense concerns, there is temptation to be careless in choosing the weapons to fight these evils. From the Alien and Sedition Acts in 1798 to the Palmer Raids in 1920 to McCarthyism in the 1950s, cynical politicians who have manipulated popular fears of aliens and radicals have done tremendous damage to the lives of innocent people and to the fundamental principles of Americanism.



Today, Congress stands poised to repeat the mistakes of the past, as vast numbers of people are smeared with guilt by (very tenuous) association. It is not often that one sees the Presbyterian Church and the American Friends Service Committee standing shoulder‐​to‐​shoulder with the National Rifle Association and Americans for Tax Reform.



That such diverse groups can find common ground, along with other organizations, to warn about the dangers of proposed legislation sharply curtailing civil liberty should indicate just how serious is the current threat to our Constitution.(4) President Clinton was right to characterize the Oklahoma City bombing as “an attack on our way of life.” If Oklahoma City becomes a pretext for the constriction of the Bill of Rights, then we will have handed terrorism a victory which it could never have won for itself.



 **There is No Terrorism Crisis**



“By enabling terrorists to appear much stronger than they really are, the media often find themselves working Pour le roi de Prusse,” observed one historian.(5) Contrary to the imagery of some irresponsible segments of the media (and their Congressional analogues), there is no need to legislate an atmosphere of panic and hysteria. According to the State Department, international terrorist attacks are at their lowest level in 23 years.(6) In the United States in the last eleven years, according to the FBI, there have been only two international terrorist incidents. (One was the World Trade Center bombing; the other was a trespassing incident at the Iranian mission to the United Nations, in which five critics of the Iranian regime took over the mission’s offices, and refused to leave.)(7)



As for incidents of domestic terrorism, there were none in the United States in 1994, nor were there any preventions of terrorist incidents. In 1993, there were 11 incidents classified by the FBI as “terrorist.” Nine of those eleven incidents took place one night in Chicago when animal rights activists set off small incideniary devices in four department stores that sell fur.(8)



Combining domestic and international terrorism, and also accounting for suspected terrorist acts, the total terrorist incident count in the United State is as follows:



 **Terrorist Incidents in the United States**



1994 0 0 1 1993 11 7 2 1992 4 0 0 1991 5 4 1 1990 7 5 1 1989 4 7 16 Of these incidents, only one (the 1993 World Trade Center bombing) was classified as international in origin.(9)



The Oklahoma City bombing was one of the most terrible single crimes in American history, but it was just that: an isolated, single crime. It was emphatically not part of a trend towards increasing terrorism.



 **The British Tragedy**



More government secrecy, more police powers to detain people at will, less governmental accountability, and less freedom are not novel responses to terrorism. They are precisely the approach that has been taken in Great Britain since the early 1970s. The British lesson should be a caution to American politicians who feel confident that the only thing wrong with anti‐​terrorism policy is that the Bill of Rights has been taken too far.



In 1974, Irish Republican Army terrorists bombed pubs in Birmingham, killing twenty‐​one people. Home Secretary Roy Jenkins introduced the Prevention of Terrorism (Temporary Provisions) Bill. Approved without objection in Parliament, the Bill was supposed to expire in one year, but has been renewed every year. The Bill included a smorgasboard of civil liberties restrictions, most of which are now being proposed, with some variation, in the United States.



Under the Bill, the police may stop and search without warrant any person suspected of terrorism. They may arrest any person they “reasonably suspect” supports an illegal organization, or any person who has participated in terrorist activity. An arrested person may be detained up to forty‐​eight hours and then for five more days upon the authority of the Secretary of State.



Of the 6,246 people detained between 1974 and 1986, 87 percent were never charged with any offense. Many detainees reported that they were intimidated during detention and prevented from contacting their families.



The Prevention of Terrorism Bill also makes it illegal even to organize a private or public meeting addressed by a member of a proscribed organization, or to wear clothes indicating support of such an organization.(10)



The Act allows the Secretary of State to issue an “exclusion order” barring a person from ever entering a particular part of the United Kingdom, such as Northern Ireland or Wales. Persons subject to this form of internal exile have no right to know the evidence against them, to cross‐​examine or confront their accusers, or even to have a formal public hearing.(11)



The European Court of Human Rights ruled the Prevention of Terrorism Act to be in violation of Article Five, Section Three of the European Convention on Human Rights, which requires suspects to be “promptly” brought before a judge.(12) Nevertheless, the British government refuses to abandon its preventive detention policy, and evades the European Court’s ruling by invoking Article 15’s provision for countries to ignore the Convention on Human Rights “in time of war or other emergency threatening the life of the nation.”(13)



One of the most important lessons from Britain is that even a huge dose of restrictions on civil liberties, such as the Prevention of Terrorism Bill, does not long remain “sufficient” in the eyes of the government. At least in regard to civil liberties, the Domino Theory has proven correct, as one traditional Anglo‐​American freedom after another has fallen under the government’s assertion of the need for still more anti‐​terrorist powers.



In Northern Ireland the jury has been “suspended” for political violence cases; judges in the Diplock courts hear the cases instead. Confessions are admitted without corroboration. Confessions are extracted through “the five techniques”: wall‐​standing, hooding, continuous noise, deprivation of food, and deprivation of sleep. Convictions may be based solely on the testimony of “supergrasses” (police informers).(14)



In 1988, the Thatcher government enacted additional laws restricting civil liberties. Television stations were forbidden to broadcast in‐​person statements by supporters of a legal political party, Sinn Fein.(15) The ban even applied to rebroadcasts of archive films taped many decades ago, such as footage of Eamon de Valera, the first president of Ireland. A confidential British Broadcasting Corporation memo announced the government’s intention to keep journalists from broadcasting any statement by U.S. Senator Edward Kennedy supporting Sinn Fein.(16) The BBC also banned Paul McCartney’s “Give Ireland Back to the Irish,” and a song by another group urging the release from prison of the Guildford Four (discussed below).(17)



A suspect’s decision to remain silent under interrogation may now be used against him in court. Although terrorism in Northern Ireland was the stated basis for the change, the change will also apply in England and Wales. No‐ one who has seen Great Britain’s slide down the slippery slope can feel confident that repressive measures introduced solely for terrorism will not eventually seep into the ordinary criminal justice system.



 **Wiretaps do not even need judicial approval.(18)**



The Security Service Act of 1989 provides: “No entry on or interference with property shall be unlawful if it is authorized by a warrant issued by the secretary of state.” If committed pursuant to an order from the secretary of state, acts such as theft, damage to property, arson, procuring information for blackmail, and leaving planted evidence are not crimes.(19)



As in America, gun prohibitionists have hitched their wagon to “anti‐​terrorism,” with little regard for an actual terrorist nexus. Although British laws regarding possession of actual firearms were already quite severe, the Firearms Act of 1982 introduced restrictive licensing for imitation firearms which could be converted to fire live ammunition.(20) The sponsor of the new law against imitation firearms promised that it would help stem “the rising tide of crime and terrorism”–although there had never been a crime or terrorist act committed with a converted imitation weapon.(21) The first time the Prevention of Terrorism Act was used was after another pub bombing, in the English town of Guildford. Four people were arrested, held incommunicado in prison for a week, and coerced into false confessions by administration of drugs and by threats against their families. While the “Guildford Four” were being held, the police used the time to fabricate evidence against them.



Although members of the Irish Republican Army already in prison confessed to the Guildford bombings, the Guildford Four were tried, convicted, and sentenced to life in prison.



Several leading English statesmen, including Roy Jenkins, felt that the defendants had been framed. A campaign to free them continued for fifteen years, until, upon discovery of police notes of fabrication of evidence, the Guildford Four were released from prison.(22)



The Birmingham bombings that had led to the Prevention of Terrorism Act resulted in the conviction of a group of defendants called the “Birmingham Six.” Amnesty International charged that their confessions were extracted under torture. The forensic scientist whose testimony convicted the Birmingham Six later admitted that he lied in court. The Birmingham Six confessed while being held incommunicado by the police; the various confessions were so factually inconsistent that they could not have been true.



(Civil libertarians fear that the Birmingham case is only one of many instances of police obtaining coerced confessions.(23))



The Birmingham Six were also eventually freed.



Britain, fortunately, has no death penalty. In America, where before anyone had even been indicted President Clinton announced that the perpetrators of the Oklahoma City bombing should be executed, the federal death penalty would mean that vindication of persons wrongfully convicted of terrorism might be post‐​mortem.



To state the obvious, all the legislation has hardly immunized Britain from terrorism. But Britain has, in two decades, eviscerated the magnificent structure of liberty and limited government that took over a millennium to construct. For centuries, “the rights of Englishmen” were proudly held up in contrast to the absolutism of the Continent. Far from being an examplar to the world, the modern “anti‐​terrorist” United Kingdom has been found culpable of human rights violations under the European Convention on Human Rights more often than any other member of the Council of European States.(24) To a student of Britain’s magnificent history in the story of freedom, it is a pitiful sight to see modern Britons forced to turn to Brussels and the European Court of Human Rights as the last protector of what were formerly the unquestioned rights of Englishmen.



Britain was once the freest nation in the world; today, it is one of the unfreest in Western Europe. As Britain illustrates, no matter how great a country’s tradition of freedom, freedom can be lost in less than a generation if public officials, and the public, allow terrorism to destroy their traditional way of life.



 **Weakening Restraints on FBI Political Surveillance**



Within days after the Oklahoma City bombings, conservative talk show host Rush Limbaugh began casting blame on civil libertarians such as former Ohio senator Howard Metzenbaum who had promoted strict guidelines on FBI surveillance of dissident groups in the United States.(25) Other persons have also called for abolition of the remaining limitations on FBI investigations.



First of all, there is at present no evidence that the FBI wanted to spy on anyone suspected in Oklahoma City bombing, but was prevented from doing so by the current guidelines. Thus, persons demanding the abolition of FBI guidelines are demanding a “solution” for which there is no demonstrated problem.



Second, the FBI guidelines exist for a very good reason. Before the guidelines were implemented, the FBI spied on literally hundreds of thousands of Americans who were doing nothing more than exercising their Constitutional right to question government policies. Victims of these abuses ranged from Dr. Martin Luther King, Jr., to the Ku Klux Klan, to the Congress on Racial Equality and the civil rights movement. The Counter‐​intelligence Programs (COINTELRPO) invaded the Constitutional rights of American people who simply were expresssing in public what Secretary of Defense Robert McNamara had concluded in private. Far from being confined to a single type of dissident, or to a few years of excess, FBI abuses dated back to the 1940s and were pervasive until brought to light by fifteen months of hearings before Senator Frank Church’s special committee in 1975–76. Altogether, there were 675 FBI operations against civil rights, white supremacist, or anti‐​war groups, which led to only four convictions.(26)



Even after all the public hearings, and the implementation of guidelines, the FBI continued to abuse the rights of dissident Americans, through a massive surveillance of people in CISPES (Committee in Solidarity with the People of El Salvador) who opposed to President Reagan’s policy in El Salvador in the mid‐​1980s. The CISPES investigation, justifiably regarded today as shameful, would have been lawful if the anti‐​terrorism bills current being considered had been law.



The first set of FBI guidelines were implemented by President Ford’s attorney general Edward Levi in 1976. In 1983, the “Levi guidelines” were replaced by President Reagan’s attorney general William French Smith. These “Smith guidelines” were far less restrictive. FBI director William Webster stated that the Smith guidelines “should eliminate any perception that actual or imminent commission of a violent crime is a prerequisite to investigation.” Thus, the recent highly‐​publicized claim of a former FBI official “you have to wait until you have blood in the streets before the bureau can act” is patent nonsense.(27)



In fact, the Reagan/​Smith guidelines, which are still in force, nowhere require the completion of a violent crime. Rather they state that a:



domestic security/​terrorism investigation may be initiated when facts for circumstances reasonably indicate that two or more persons are engaged in an enterprise for the purpose of furthering political or social goals wholly or in part through activities that involve force or violence and a violation of the criminal laws of the United States.



Specifically, the guidelines already allow investigations based upon mere words:



When, however, statements advocate criminal activity or indicate an apparent intent to engage in crime, particularly crimes of violence, an investigation under these Guidelines may be warranted unless it is apparent from the circumstances or the context in which the statements are made, that there is no prospect for harm.



While the Smith guidelines would prevent infiltration of Second Amendment groups simply because they are sharply critical of government policy, the guidelines do not now prevent infiltration of groups which actually threaten violence. For example, in Virginia, a group of fifteen men who allegedly wanted to resist the federal government managed only three meetings before being arrested for weapons violations as a result of government infiltrator’s secret tape recordings.(28)



Rather than being obliterated, guidelines on FBI domestic surveillance should be brought up to full strength.



A statutory version of the Levi guidelines should be enacted.



Persons who eager to “unleash” the FBI against dissident groups who are not threatening illegal activity might first want to go through the mental exercise of imagining their worst nightmare as President. Liberals might imagine Pat Buchanen or Pat Robertson. Conservatives could imagine Dianne Feinstein or Jesse Jackson. In such a scenario, would we want the FBI free to spy on whomever the President does not like? Under Presidents Nixon, Johnson, and Kennedy, who were far more moderate than Jesse Jackson or Pat Buchanan, the FBI did so, with baleful results.



An official at the Treasury Department, who works closely with the BATF, warned that there is “a tremendous potential for abuse” in administration proposals to loosen controls on the FBI.(29)



It must be remembered that many of America’s greatest organizations were, in their day, radical extremists. The abolitionists were extremists, as were the suffragettes, the civil rights movements, and many of the opponents of the War in Vietnam. If these groups seem vindicated by history, they were bitterly attacked in their day as radical and anti‐​American.



Finally, before any additional powers are granted to the FBI, it is appropriate to investigate FBI abuses of existing powers, including the events in Waco.(30) At the least, it is well‐​established that the FBI used a chemical warfare agent which is banned in international warfare, against children indoors, even though Army and manufacturer manuals specifically warn that the agent indoors is flammable, and can severely injure unprotected children. In securing Attorney General Reno’s consent, the FBI falsely told her that the chemical warfare agent was “a mild form of teargas.” The FBI also ignored the advice of its own behavioral experts, and pressured at least one of them to reverse his advice, so as to justify an assault. This fact too was concealed from the Attorney General.



 **FBI Foreign Jurisdiction**



It has been proposed that the FBI’s foreign jurisdiction be expanded. Firstly, the expansion is unnecessary, since the CIA can operate overseas against terrorists. Second, allowing domestic American law enforcement agents to operate on foreign soil against foreign soil against foreign citizens creates a dangerous precedent, and will inevitably lead to demands for reciprocity. Do we really want the Russian secret police, or even the Mexican federales, operating on American soil? The Clinton bill also removes most of the limitations regarding use (including overseas) of American trainers for foreign law enforcement, and removes the restriction against American tax dollars being used to pay the salaries of foreign police.(31) Internationalizing criminal law is even more dangerous to civil liberty than is federalizing it.



 **Felonizing Support for Peaceful Activities of Foreign Organizations**



 **Presidential Designation of “Terrorist” Groups**



The Clinton and Dole bills empower the President to designate “foreign terrorist” organizations which are illegal for Americans to provide any “material support.”(32) Recently, the Clinton administration has retreated from its insistance that the Presidential designation be unreviewable. At the least, the potential for judicial review will reduce the risk of the terrorist designation being used against domestic dissident groups. (Since they would be able to show in court that they were not foreign.) But it should be remembered that American courts have historically been extremely deferential to Presidential foreign policy decisions. If there were even a scintilla of evidence in favor of the President’s designation of a foreign group as “terrorist,” then it is virtually certain that courts would not overturn the designation.



Again, the reader might consider imagining this legislation in the hands of one’s worst political nightmare.



An organization which provides support to the government of Israel or to the Israeli Defense Forces (which are considered “terrorist” in some political circles) could be outlawed, as could (by a different President) a group which provides support to Palestinian refugees.



 **Material Support**



Current federal law appropriately forbids the providing of material support to any foreign terrorist organization.(33) The law forbids investigations of people for violating this law unless there is some reasonable suspicion that they have violated or may violate the law.



The restriction should of course be retained; targetting people for FBI investigations when there is not a scintilla of suspicion is not only an invitation to harassment of dissidents, it is a waste of law enforcement resources.



One important distinction between the Clinton and Dole bills is that the Dole creates an explicit exception to the “material support” statute: “ ‘Material support’…does not include humanitarian assistance to persons not directly involved in such violations.”(34) Thus, sending a Christmas food package to an I.R.A. or A.N.C. prisoner would constitute material support, but giving money to a fund which assisted the orphaned children of I.R.A. or A.N.C.



members would not be, under the Dole approach.



Under the Clinton bill, however, the donor to the I.R.A. orphanage would be a federal felon, subject to ten years in prison, as would be a person who spent five dollars to attend a speech of a visiting lecturer from the African National Congress.



When pressed about this fact at recent Congressional hearings, a Clinton administration spokesperson acknowledged that minor support for the A.N.C.‘s peaceful activities could have been felonized, but that the American people should simply trust the President not to abuse the immense power which President Clinton was requesting.



But as President Lyndon Johnson put it: “You do not examine legislation in light of the benefits it will convey if property administered but, in light of the wrongs it would do and the harms it would cause if improperly administered.”



The “terrorism” bills’ overbreadth is astonishing. The Palestine Liberation Organization is permanently defined as a terrorist organization by the proposal, no matter what its future conduct.(35) Thus, if the P.L.O. should live up the peace treaty that it signed with Israel, President Clinton would be guilty of providing “material support” to a terrorist organization should he invite Yassir Arafat to the White House and give him a free meal and a night’s lodging.



 **Licensed Donations**



Theoretically, a license can be procured allowing humanitarian contributions to the blacklisted group. The licensing procedure is, however, very difficult to comply with. Not only does recipient group have to open its books to the Treasury Department, so does the donor. In other words, if a person wants to make a $50 contribution to buy clothes for Palestinian orphans, the person must make his financial records open for inspection, and be able to show “the source of all funds it receives, expenses it incurs, and disbursements it makes.”(36) There is no limitation that the complete accounting of receipt, expenses, and disbursements be limited to the charitable donation. Virtually no‐​one in the United States keeps such detailed records. Knowing that a charitable donation to a politically blacklisted group would expose the donor to a nightmare audit, few donors would be courageous or foolish enough to give anyway.



In addition to criminal penalties of up to ten years in prison, civil fines of $50,000 per offense may be imposed, and in civil prosecutions, the government may, upon approval of the court, introduce secret, classified evidence which remains hidden from the defendant.(37) (The Clinton and Dole bills grant similar authority to use secret evidence in proceedings under the International Emergency Economic Powers Act, which gives the President unilateral authority to regulate or prohibit all foreign exchange transactions, all imports and exports of securities and currency and foreign currency transactions, and all banking transactions involving foreigners.(38))



 **The Constitutional View**



The Constitution mandates that if a person is to be punished for association with a group which has unlawful objectives, the government must prove that the individual specifically intended to further the unlawful objectives.(39) What the Clinton/​Dole bills propose is a return to practices which the Supreme Court outlawed over half a century ago.



Then, the Immigration and Naturalization Service attempted to deport labor organizer Harry Bridges because of his affiliation with the Communist party. Bridges had supported only lawful Communist activities, rather than the party’s unlawful ends. The INS argued that if an organization had unlawful purposes, the fact that a supporter had supported only lawful purposes was irrelevant. The Supreme Court disagreed, and dismissed the case.(40)



More recently, the Court declared unconstitutional a law that was “a blanket prohibition of association with a group having both legal and illegal aims.” Unless there was proof that the defendant specifically intended to support the group’s illegal aims, the prohibition was a violation of “the cherished freedom of association protected by the First Amendment.”(41)



 **Defining Everything as “Terrorism”**



Current federal law already provides a comprehensive, realistic definition of “terrorist activity.”(42) Some proposals define virtually any crime as “terrorism.” For example, the Clinton and Dole “terrorism” bills define as “terrorism” virtually every violent or property crime, whether or not related to actual terrrorism. The bills impose a prison terms of up to twenty‐​five years (for property damage, more for violent crimes) for “terrorist” offenses which are defined as follows: any assault with a dangerous weapon, assault causing serious bodily injury, or any killing, kidnapping, or maiming, OR any unlawful destruction of property.(43) Snapping someone’s pencil, breaking someone’s arm in a bar fight, threatening someone with a knife, or burning down an outhouse would all be considered “terrorist” offenses. Any attempt to perpetrate any of these terrorist crimes would be subject to the same punishment as completed offense.



Even a threat to commit the offense (i.e. “One of these days, I’m going to snap your pencil.”) is a felony subject to ten years in federal prison.(44) Again, the extra federal power granted by the legislation is superfluous to genuine anti‐​terrorism. It is already a serious federal felony to make a real terrorist threat, as by threatening to set off a bomb, or to assassinate the President.(45)



In order for the offense to be considered “terrorism,” all that would be necessary would be jurisdictional predicate that would cover almost every crime. The jurisdictional predicate requires one of any of the following: the crime “affects commerce in any way” (not necessarily interstate commerce); the criminal used “any facility used in any manner in commerce”; the victim was “traveling in commerce” (again, not necessarily interstate); the victim was a federal employee, or the property damaged was federal; the victim was not an American national; or any of the offenders “travels in commerce.”(46) If anyone involved in the crime meets the jurisdictional predicate, then jurisdiction is invoked for the entire crime.(47)



Finally, in order for a prosecution to take place, the Attorney General must certify in writing that the offense “transcended national boundaries” and was intended to intimidate a foreign government or “a civilian population, including any segment thereof.”(48) There is no provision for review of whether the Attorney General’s certification was even remotely accurate. Nor is there any requirement that there be an actual international border crossing.



Just because the law allows it, the federal government probably will not prosecute every Canadian tourist who snaps a policeman’s pencil or everyone who scratches anti‐​war graffiti on post office tables. The proponents of these bills may expect that the essentially limitless discretion granted to the federal government will not be abused. But a fundamental principle of American law has always been that the law should control the government; citizens should not be at the mercy of the good judgement of government officials. As the Supreme Court put it, “It could certainly be dangerous if the legislature could set a net wide enough to trap all possible offenders, and leave it to the courts to step inside and say who could rightfully be detained, and who should be set a large.”(49)



The justification for federalizing all of the criminal law is that such federalization is necessary to make sure that every possible terrorist crime is covered. For example, it is asserted that the bombing of a Jewish hospital in, for example, St. Louis, might not be covered by current federal law. In fact, the federal arson statute has successfully been applied to the burning of a trailer that was hooked up to a power system which was part of the interstate electricity grid.(50) Thus, the fact that the hospital drew power from the same electrical grid would justify application of the current federal arson law, without the need for a new statute. Even if it is possible to imagine some bizarre hypothetical crime that would not be covered by the (very expansive) interpretation of current federal criminal statutes, every conceivable terrorist crime is subject to severe punishment under current state criminal laws.



The dangers posed by the hidden federalization of the entire criminal law (all the way down to petty vandalism) become all the greater when coupled with the bill’s other provisions to make the overbroad federal RICO,(51) money laundering,(52) and wiretapping laws(53) applicable to “terrorist” offenses and to authorize use of the military in domestic law enforcement for “terrorism.”(54) No bail is allowed even if it is uncontroverted that the accused will not flee and will pose no danger to anyone.(55)



Likewise, mandatory prison sentences, with no possibility of probation, are required for “terrorist” crimes, no matter what the circumstances.(56)



Having used state law definitions to define petty property crimes as “terrorism,” the bills then forbid defendants from invoking state constitutional law protections of the state where the alleged offense took place.(57)



Turning every state and local petty property crime (or even a local violent crime) into a federal felony may be unconstitutional, as the Supreme Court recently ruled in the Lopez “gun‐​free‐​school‐​zones” case. Putting aside questions of Constitutionality, it is inappropriate that the draconian federalization of state crimes be pushed through Congress under the mask of anti‐​terrorism.



 **Resisting Foreign Dictatorships**



Solicitude for foreign governments should not blind us to the fact that most governments in the world are dictatorships. Under the principles on which America is based, governments without the consent of the governed have no legitimacy, and it is the right of the people of that nation to overthrow the dictatorship.



Yet the Clinton and Dole bills define as “terrorism” any act which plans the destruction of government property in foreign nation with which the United States is “at peace.”(58) Thus, if Chinese refugees living in the United States planned a jailbreak to liberate political prisoners in China, they would be guilty of “terrorism.” If Americans in 1940 had plotted the destruction of railways leading to Nazi concentration camps, they too would have been guilty of “terrorism.” And so would the countless American Jews who smuggled firearms to the Jewish resistance movement in Palestine in the 1940s, making possible the eventual establishment of the state of Israel. Had such a “terrorism” law been universal in 1776, the Dutch, French, and other private citizens who provided material assistance to the American Revolution (even though their governments were at peace with the British Empire) would have been “terrorists” too. It ill becomes a nation which was born in violent revolution with foreign assistance to felonize the very types of charity which allowed our own nation to become free. Resistance to dictatorships and empires is not terrorism.



 **Wiretapping**



Various proposals have been offered to expand dramatically the scope of wiretapping. For example, the Clinton bill defines almost all violent and property crime (down to petty offenses below misdemeanors) as “terrorism” and also allow wiretaps for “terrorism” investigations.(59)



Other proposals would allow wiretaps for all federal felonies, rather than for the special subet of felonies for which wiretaps have been determined to be especially necessary. Notably, wiretaps are already available for the fundamental terrorist offenses: arson and homicide. Authorizing wiretaps for evasion of federal vitamin regulations, gun registration requirements, or wetlands regulations is hardly a serious contribution to antiterrorism, but amounts to a bait‐​and‐​switch on the American people.



Currently, FBI wiretapping, bugging, and secret break‐ ins of the property of American groups is allowed after approval from a seven‐​member federal court which meets in secret.(60) Of the 7,554 applications which the FBI has submitted in since 1978, 7,553 have been approved.(61)



Making the request for vast new wiretap powers all the more unconvincing is how poorly wiretap powers have been used in the past. Terrorists are, of course, already subject to being wiretapped. Yet as federal wiretaps set new record highs every year, wiretaps are used almost exclusively for gambling, racketeering, and drugs. The last known wiretap for a bombing investigation was in 1998. Of the 976 federal electronic eavesdropping applications in 1993, not a single one was for arson, explosives, or firearms, let alone terrorism. From 1983 to 1993, of the 8,800 applications for eavesdropping, only 16 were for arson, explosives, or firearms.(62) In short, requests for vast new wiretapping powers because of terrorism are akin to a carpenter asking for a pile driver to hammer a nail, while a hammer lies nearby, unused.



Even more disturbing than proposals to expand the jurisdictional base for wiretaps are efforts to remove legal controls on wiretaps. For example, wiretaps are authorized for the interception of particular speakers on particular phone lines. If the interception target keeps switching telephones (as by using a variety of pay phones), the government may ask the court for a “roving wiretap,” authorizing interception of any phone line the target is using. Yet while roving wiretaps are currently available when the government shows the court a need, the Clinton and Dole bills allow roving wiretaps for “terrorism” without court order.(63) (Again, remember that both bills define “terrorism” as almost all violent or property crime.)



The Foreign Intelligence Surveillance Act (FISA) provides procedures for authorizing wiretaps in various cases. These procedures have worked in the most serious foreign espionage cases.(64) Yet the Clinton and Dole bills would authorize use of evidence gathered in violation of FISA in certain deportation proceedings.



 **Warrantless Data Gathering**



Proposals have also been offered to require credit card companies, financial reporting services, hotels, airlines, and bus companies to turn over customer information whenever demanded by the federal government.(65) Document subpoenas are currently available whenever the government wishes to coerce a company into disclosing private customer information. Thus, the proposals do not increase the type of private information that the government can obtain; the proposals simply allow the government to obtain the information even when the government cannot show a court that there is probable cause to believe that the documents contain evidence of illegal activity.(66)



Similar analysis may be applied to proposals to increase the use of pen registers (which record phone numbers called, but do not record conversations, and thus do not require a warrant). If a phone company has a high enough regard for its customers’ privacy so as to not allow pen registers to be used without any controls, the government may obtain a court order to place a pen register. Business respect for customer privacy ought to be encouraged, not outlawed.



 **Curtailing First Amendment Rights of Computer Users**



For some government agencies, the Oklahoma City tragedy has become a vehicle for enactment of “wish list” legislation that has nothing to do with Oklahoma City, but which it is apparently hoped the “do something” imperative of the moment will not examine carefully.



One prominent example is legislation to drastically curtail the right of habeas corpus.(67) Although Supreme Court decisions in recent years have already sharply limited habeas corpus,(68) prosecutors’ lobbies want to go even further. Two obvious points should be made: First, habeas corpus has nothing to do with apprehending criminals; by definition, anyone who files a habeas corpus petition is already in prison. Second, habeas corpus has nothing to do with Oklahoma City in particular, or terrorism in general.



A second example, of piggybacking irrelevant legislation designed to reduce civil liberties are current FBI efforts to outlaw computer privacy.



If a person writes a letter to another person, he can write the letter in a secret code. If the government intercepts the letter, and cannot figure out the secret code, the government is out of luck. These basic First Amendment principles have never been questioned.



But, if instead of writing the letter with pen and paper, the letter is written electronically, and mailed over a computer network rather than postal mail, do privacy interests suddenly vanish? According to FBI director Louis Freeh, the answer is apparently “yes.”



Testifying before the Senate Judiciary Committee about Oklahoma City, director Freeh complained that people can communicate over the internet “in encrypted conversations for which we have no available means to read and understand unless that encryption problem is dealt with immediately.”(69) “That encryption problem” (i.e. people being able to communicate privately) could only be solved by outlawing high quality encryption software like Pretty Good Privacy”.



First of all, shareware versions of Pretty Good Privacy are ubiquitous throughout American computer networks. The cat cannot be put back in the bag. More fundamentally, the potential that a criminal, including a terrorist, might misuse private communications is no reason to abolish private communications per se. After all, people whose homes are lawfully bugged can communicate privately by writing with an Etch-a-Sketch”.(70) That is no reason to outlaw Etch‐ a‐​Sketch.



Although Mr. Freeh apparently wants to outlaw encryption entirely, the Clinton administration has been proposing the “Clipper Chip.” The federal government has begun requiring that all vendors supplying phones to the federal government include the “Clipper” chip. Using the federal government’s enormous purchasing clout, the Clinton administration is attempting to make the Clipper Chip into a de facto national standard.(71)



The clipper chip provides a low level of privacy protection against casual snoopers. But some computer scientists have already announced that the chip can defeated. Moreover, the “key”–which allows the private phone conversation, computer file, or electronic mail to be opened up by unauthorized third parties–will be held by the federal government.



The federal government promises that it will keep the key carefully guarded, and only use the key to snoop when absolutely necessary. This is the same federal government that promised that social security numbers would only be used to administer the social security system, and that the Internal Revenue Service would never be used for political purposes.



Proposals for the federal government’s acquisition of a key to everyone’s electronic data, which the government promises never to misuse, might be compared to the federal government’s proposing to acquire a key to everyone’s home.



Currently, people can buy door locks and other security devices that are of such high quality that covert entry by the government is impossible; the government might be able to break the door down, but the government would not be able to enter discretely, place an electronic surveillance device, and then leave. Thus, high‐​quality locks can defeat a lawful government attempt to bug someone’s home, just as high‐​quality encryption can defeat a lawful government attempt to read a person’s electronic correspondence or data.



Similarly, it is legal for the government to search through somebody’s garbage without a warrant; but there is nothing wrong with privacy‐​conscious people and businesses using paper shredders to defeat any potential garbage snooping. Even if high‐​quality shredders make it impossible for documents to be pieced back together, such shredders should not be illegal.



Likewise, while wiretaps or government surveillance of computer communications may be legal, there should be no obligation of individuals or businesses to make wiretapping easy. Simply put, Americans should not be required to live their lives in a manner so that the government can spy on them when necessary.



Thus, although proposals to outlaw or emasculate computer privacy are sometimes defended as maintaining the status quo (easy government wiretaps), the true status quo in America is that manufacturers and consumers have never been required to buy products which are custom‐​designed to faciliate government snooping.



The point is no less valid for electronic keys than it is for front‐​door keys. The only reason that electronic privacy invasions are even discussed (whereas their counterparts for “old‐​fashioned” privacy invasions are too absurd to even be contemplated), is the tendency of new technologies to be more highly restricted than old technologies. For example, the Supreme Court in the 1920s began allowing searches of drivers and automobiles that would never have been allowed for persons riding horses.



But the better Supreme Court decisions recognize that the Constitution defines a relationship between individuals and the government that is applied to every new technology. For example, in United States v. Katz, the Court applied the privacy principle underlying the Fourth Amendment to prohibit warrantless eavesdropping on telephone calls made from a public phone booth– even though telephones had not been invented at the time of the Fourth Amendment.(72) Likewise, the principle underlying freedom of the press– that an unfettered press is an important check on secretive and abusive governments–remains the same whether a publisher uses a Franklin press to produce a hundred copies of a pamphlet, or laser printers to produce a hundred thousand. Privacy rights for mail remain the same whether the letter is written with a quill pen and a paper encryption “wheel,” or with a computer and Pretty Good Privacy.



Efforts to limit electronic privacy will harm not just the First Amendment, but also American commerce. Genuinely secure public‐​key encryption (like Pretty Good Privacy) gives users the safety and convenience of electronic files plus the security features of paper envelopes and signatures. A good encryption program can authenticate the creator of a particular electronic document–just as a written signature authenticates (more or less) the creator of a particular paper document.



Public‐​key encryption can greatly reduce the need for paper. With secure public‐​key encryption, businesses could distribute catalogs, take orders, pay with digital cash, and enforce contracts with veriable signatures–all without paper.



Conversely the Clinton administration’s weak privacy protection (giving the federal government the ability to spy everywhere) means that confidential business secrets will be easily stolen by business competitors who can bribe local or federal law enforcement officials to divulge the “secret” codes for breaking into private conversations and files, or who can hack the clipper chip.



 **The New Star Chamber**



Although the United States has suffered exactly one alien terrorist attack in the last eleven years, special harsh rules for aliens are at the top of the “antiterrorism” agenda. The most ominous proposals are those that allow secret evidence for deportation cases in which the government asserts that secrecy is necessary to the national security.(73) Georgetown University Law Professor David Cole calls the secret court the new “Star Chamber,” since its powers resemble those of the inquisitorial court which the British monarchy, in violation of the common law, used to terrorize dissident subjects. Star Chamber was one of the most hated abuses of the British government.



Modern Star Chamber proceedings are to be before a special court (one of five select federal district judges)(74), after a an ex parte, in camera showing that normal procedures would “pose a risk to the national security of the United States.”(75) Based upon further ex parte, in camera motions, evidence which the government does not which to disclose may be withheld from the defendant, who will instead be provided a general summary of what the evidence purports to prove. In other words, secret evidence may be used.(76) Of course any of the “showings” that the government makes in camera and ex parte may be based on allegations regarding the unreviewable claims of a secret informant.



Wiretap evidence is usable even if it was illegally obtained.(77) Normal procedural rules allowing for disclosure of circumstances relating to illegally obtained evidence are abolished.(78)



Legal aliens do not, of course, have the full scope of Constitutional rights guaranteed to American citizens; for example, they cannot exercise rights associated with citizenship, such as voting, or serving on a jury. But it is well‐​settled that legal aliens enjoy the same right to freedom of speech as do citizens. Likewise, legal aliens have always been accorded the same due process protections in criminal cases. After all, the Fifth Amendment’s guarantee of Due Process protects “all persons,” not just “all citizens.”(79)



Procedures like those proposed in the Clinton and Dole bills have already been found unconstitutional. As the District of Columbia Court of Appeals, put it:



Rafeedie–like Joseph K. in The Trial–can prevail before the [INS] Regional Commmissioner only if he can rebut the undisclosed evidence against him, i.e., prove that he is not a terrorist regardless of what might be implied by the government’s confidential information. It is difficult to imagine how even someone innocent of all wrongdoing could meet such a burden.(80)



The argument for allowing secret evidence in deportation proceedings is that otherwise the identity or operational mode of a confidential informant might be jeopardized. First of all, the very purpose of the Constitution’s Confrontation Clause is to prevent people’s lives from being destroyed by the type of secret accusations which had characterized the European justice systems.



Moreover, the argument against endangering the secrecy of confidential accusers in deportation cases proves too much. The very same argument applies in every other case, including criminal violence or drug sales cases. Obeying the Confrontation Clause in those cases may likewise impede the short‐​term interests of law enforcement. But the Constitution has conclusively determined that a criminal justice system without a right of confrontation poses a far greater long‐​term risk to public safety than does requiring the government to disclose the reason why it wants to imprison, execute, or deport someone.



Simply put, confidential informants often lie.



Informants are rarely good citizens who come forward to help prevent a crime. Rather, informants are criminals who have been caught, and have turned informant in order to protect themselves from prosecution; informants have every reason to lie and falsely accuse people.(81)



Confidential informants who are not professional criminals may have other reasons for lying. The type of miscarriage of justice that can occur based on confidential informants was illustated in 1950 case, in which the Supreme Court held that secret evidence could be used to prevent an alien from entering the United States.(82) (She was married to an American.) When the alien was granted a hearing, it was discovered that the confidential informant was her husband’s angry ex‐​girlfriend.



Some persons who would oppose Star Chamber proceedings for criminal trials might approve of such procedures in deportation hearings since deportation is, under most circumstances, a less severe sanction than prison. Yet if the alien cannot find a country that wants to take him (or if the State Department can quietly convince other countries not to take him), then the alien may be imprisoned for the rest of his life in the United States, at the sole discretion of the Attorney General, without even the right to ask for a writ of habeas corpus based on governmental violation of statutes.(83)



Finally, some persons may accept Star Chamber for legal resident aliens under the presumption that such procedures would never be used against American citizens. Yet if there is anything the experience of Great Britain proves, it is that special, “emergency” measures implementented in a limited jurisdiction (such as Northern Ireland) soon spread throughout the nation. Cancers always start small. If one international terrorist incident in eleven years is a sufficient interest to justify a Star Chamber for certain terrorism suspects, then it is hard to resist the logic that crimes which actually are widespread (such as homicide, rape, or drug trafficking) should be entitled to their own Star Chamber.



 **More Informants**



One of the reasons that many people are so frightened of the federal government is how it already uses informants to attempt to infiltrate suspicious organizations. One of the most notorious cases which helped create the militia movement was started by the attempt to creat an informant.



Randy Weaver was a white separatist who lived with his family in a remote cabin in northern Idaho. There was no indication that he had ever advocated or participated in illegal violence. When he was approached by federal agents who wanted him to infiltrate violent white supremacist groups and serve as an informer, he refused. He was later entrapped (a jury later found) by repeated pestering from undercover agents into selling undercover BATF agents two shotguns whose barrels had been shortened (at the request of the undercover agents) to a fraction of an inch below the 18″ legal limit. Weaver failed to appear for a court hearing resulting from the illegal firearms sale; as it later turned out, the order to appear which had been mailed to him gave an incorrect date for the hearing. A fugitive arrest warrant was issued for Weaver.



United States Marshals showed up one day in August 1992. The Weavers’ three dogs (two collies and a labrador) began barking, and Randy Weaver, his friend Kevin Harris, and Weaver’ fourteen‐​year‐​old son Sammy grabbed their guns to run and investigate.



The Marshals, wearing camouflage and carrying silenced machine guns, did not identify themselves or their purpose, but they did shoot one of the dogs. Sammy Weaver returned fire, and was promptly shot by a Marshal. Sammy turned and fled, with his nearly severed arm flopping as he ran. Sammy was promptly shot in the back. Nearby, Kevin Harris concluded that if he fled, he too would be shot; Harris fired his rifle in the direction of the marshal who had shot Sammy; the bullet killed the marshal who had shot Sammy Weaver.



Randy Weaver had only heard the shooting, but had not seen what had happened. “Come on home, Sam. Come home,” he yelled over and over. At last, Sammy called “I’m coming, Dad.” Those were apparently the last words Sammy Weaver said before he died.



Harris’s shot had disordered the Marshals, and Weaver and Harris used the opportunity to retreat to their cabin.



Later that day, Randy Weaver and his wife Vicki picked up Sammy’s dead body and carried it to a building near the cabin, where they prepared their son’s body for burial.



Over 300 government agents, led by the FBI “Hostage Rescue Team” descended on Ruby Ridge, Idaho, where Weaver’s two‐​story cabin was located. Commanding the FBI at Ruby Ridge was Richard M. Rogers, who would later serve as a field commander at Waco.(84)



The FBI rules of engagement allow use of deadly force only when necessary to protect an innocent person from imminent peril. But on the plane out to Idaho, Rogers wrote new rules of engagement for Ruby Ridge. The new rules allowed FBI snipers to shoot any adult who was armed. Since virtually everyone besieged in Idaho went outside armed (in full compliance with the laws of Idaho, and of most other states, because the armed people were on their own property) everyone was a target outside the cabin.



At Weaver’s trial in 1993, HRT Director Rick Rogers was unable to cite any authority allowing the FBI, in violation of state law, to shoot people who were posing no threat to anyone. (A provision in the 1994 federal crime bill, removed during the bill’s final movement through Congress, would have immunized federal agents from state criminal prosecution for crimes committed while on the job.)



As at Waco, a siege ensued, with the “Hostage Rescue Team” surrounding the residence of people who, far from being held hostage, simply wanted to be left alone.



At about six p.m. the next day, sixteen‐​year‐​old Sarah Weaver, her father Randy, and Kevin Harris walked out to the nearby shed to pay their last respects to Sammy. They were carrying firearms. Standing by the open door was Mrs. Vicki Weaver, holding her 10 month old daughter Elisheba.



FBI sniper Lon T. Horiuchi said that he could hit a quarter at 200 yards. Horiuchi fired, and hit Randy Weaver in the shoulder. Horiuchi later testified that Weaver was shot to keep Weaver from shooting at a helicopter overhead.



At the subsequent trial, Associate Marshal Service Director Wayne Smith testified that no helicopter was over the Weaver cabin that day, and the judge threw out the charge that Weaver had aimed a firearm at a helicopter. Sarah Weaver, Randy Weaver, and Kevin Harris fled back towards the cabin.



Sniper Horiuchi fired again, this time at a person he said he thought was Kevin Harris. (Although Harris was not even alleged to have raised any gun at any helicopter.) Horiuchi later testified that he could not identify his target clearly because he could not see through the curtains of the door. After Horiuchi had testified, the government (illegally late) turned over Horiuchi’s official report of the shooting; the drawing showed two figures standing in an open door.(85)



The FBI sniper’s .308 slug crashed into Vicki Weaver’s head with such force that skull bone fragments ricocheted into Harris, as the slug exited her body and entered his.(86) Vicki Weaver’s body fell to its knees, and her head came to rest on the floor, like a person at prayer. Randy Weaver took baby Elisheba from her arms, and lifted his wife’s head; half her face had been blown away. Her dead body was laid out on the cabin floor, and covered with a blanket.



An FBI psychological profile, prepared before the attack, called Vicki Weaver the “dominant member” of the family, thus implying that if she were “neutralized” everyone else might surrender.(87)



During the next week, “the FBI used the microphones to taunt the family. ‘Good Morning Mrs. Weaver. We had pancakes for breakfast. What did you have?’ asked the agents in at least one exchange. Weaver’s daughter Sarah, 16, said the baby, Elisheba, often was crying for its mother’s milk when the FBI’s messages were heard.”(88)



Bo Gritz, a highly‐​decorated American soldier in Vietnam, who is now a talk‐​show host and a right€wing political figure, offered to try to negotiate with Weaver.



Eight days after Vicki Weaver was shot, Gritz succeeded in convincing Weaver to surrender based on a promise that Weaver could meet with famed criminal defense attorney Gerry Spence.



Spence agreed to take the case pro bono, and in April 1993, Kevin Harris went on trial for murder, with Randy Weaver charged with conspiracy to commit murder.(89) As with the Branch Davidians, the government attempted to portray Weaver as a political and religious zealot who prophesied and then sought to create a holy war with federal agents, even though his clear goal had been to avoid government agents.(90) Weaver and Harris claimed self‐ defense, and that the government unjustifiably fired first.



With no defense evidence even introduced, the jury acquitted the accused of all charges of criminal violence, and the court fined the federal government for falsifying evidence, for withholding evidence, and for lying.(91) Weaver was convicted only of his failure to appear for the court hearing growing out of the BATF sting.(92)



The Justice Department conducted an internal review of the incident which strongly condemned governmental actions, and recommended criminal prosecution. The report has never been released the public. Its recommendations were over‐ ruled by high‐​ranking Justice Department officials.



Instead, trivial sanctions were imposed. For example, Larry Potts, the supervisor of the siege, who had approved the “shoot‐​to‐​kill” rules of engagement was given a censure, the same punishment inflicted on FBI Director Louis Freeh for losing his portable phone. Potts was then promoted to the second‐​ranking position at the FBI. The new training center for US Marhsals in New Orleans was named the “William F.



Degan” center, in honor of the marshal who had killed Sammy Weaver.



If President insists that wishes to convince the tens of thousands of Americans who belong to militia, the millions who support the patriot movement, and the 39 percent who told the Gallup poll that they think the federal government is an immediate threat to their liberty, then the President should stop the government from acting like a terrorist organization, and then slapping itself on the wrist. Rather than encouraging more use of informants, Congress should create a special prosecutor to investigate homicides perpetrated by the federal government, starting with the Weaver case.



Preserve Our National Commitment to Freedom of Speech Many people, particularly people who abhor “right‐​wing” political viewpoints, have asserted that talk show hosts, commentators, and others who speak strongly about the need to restrain the federal government are indirectly responsible for the events in Oklahoma City. Such claims are disgraceful.



When President Kennedy was assassinated in Dallas in 1963, some people attempted to link the assassination to the climate of “hate” which characterized the intense Southern opposition to President Kennedy’s legislative program, including civil rights. But quite plainly, Southern segregationsists, wrong as they were on policy matters, had nothing to do with the President’s murder.



In 1970, anti‐​war radicals blew up a math building at the University of Wisconsin. These radicals lived in an “Amerika” where important intellectual, political, and media voices proclaimed that the Vietnam war was immoral, illegal, and imperialist, and the American government was guilty of crimes against humanity. The young Bill Clinton enunciated some of these views. Yet it would be improper to blame the opponents of the Vietnam war, including young Mr. Clinton, for the criminal acts of the Wisconsin bombers.



Today, the Southern Poverty Law Center (SPLC) di
"
nan
"

TUCSON, July 31 (UPI) — Scientists  confirmed Thursday that water, considered an essential building block of life,  does indeed exist on the planet Mars.An analysis of a soil sample collected by the Phoenix lander  detected traces of water, which exists as ice just below the red soil on the  Martian surface.

“We’ve seen evidence for this water ice before in  observations by the Mars Odyssey orbiter and in disappearing chunks observed by  Phoenix last month,” scientist William Boynton said in a  written statement released by NASA and the Jet Propulsion Lab, “but this is the  first time Martian water has been touched and tasted.”
Boynton is lead scientist for the Thermal and Evolved-Gas  Analyzer team based at the University of Arizona.
Details of the composition of the water were not immediately  released. The sample came from a 2-inch deep trench carefully carved by the  lander’s robotic arm.
The presence of water is one of more dramatic discoveries  made by the Phoenix since it touched down on Mars near the pole May 24. NASA  announced it had secured funding to extend the Phoenix mission through Sept. 30.
More here: http://phoenix.lpl.arizona.edu/



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d675474',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Car sales have raced to a ten-year high according to new figures that are being celebrated as part of wider signs of a UK recovery. In the latest monthly figures published by the Society for Motor Manufacturers and Traders 425,861 new cars were registered; a rise of 5.6% and the biggest September sales figure since 2004. These numbers represent the 31st consecutive month of growth in the new car market and bring registrations for the year so far to nearly 2m. The steady rise of new car sales is being taken as vote of confidence for the UK economy, suggesting that consumers feel comfortable enough to spend their cash. Indeed, new car sales are now at their highest since before the recession – recovering at a greater rate than most of Europe.  In part, September’s soaring sales are reflective of the fact that it is always a popular month for new cars, as it brings with it the latest registration plate. But those readily parting with their cash should be wary of the too-good-to-refuse finance deals.  These deals are the main driver of improving sales figures. Last year, almost three quarters of new cars were sold to consumers on credit. Volkswagen alone reports its loans and leasing business has increased 40% in the last two years. And these personal contract purchases are usurping the second-hand car market. Propping up the market with such deals may be unsustainable in the long run as many consumers will get a nasty shock when they realise they do not have the equity they imagined in the car and are not in a position to buy. Not only might they be put off future purchases but they find themselves unable to afford a new car following the three-year contract period.  Our insatiable appetite for new cars also raises environmental concerns. How can we ensure that our love of new wheels is sustainable? Admittedly, new petrol and diesel cars are more efficient than their forebears and have shown ever improving emissions levels since the turn of the century, but there is no room for complacency if the UK is to meet strict European Union targets for 2020. And while overall sale volumes remain extremely small at just over 3,000 (less than 1% of September’s total sales), electric cars have seen an increase of a whopping 426% on 2013. Plus, alternatively fuelled vehicles are increasingly grabbing market share from internal combustion engines as a result of more models becoming available. Private car ownership, though, remains destructive for the environment. Petrol and diesel cars still emit damaging levels of CO2 that cause localised pollution and contribute to worldwide climate change. Most electrics will be powered from fossil-fuel fired power stations, potentially causing twice the global warming of internal combustion engines. And mass-produced vehicles of all stripes require intensive production processes for their steel bodies and use up finite mineral reserves.  Even the growth in electric car sales is not necessarily good news for the environment. Their limited range means most drivers see them as suitable only for use within cities, not between them. They are often used as second cars, so supplementing not replacing larger ones. And, within urban areas, use of electric cars will replace buses, trains, trams, metros, cycling and walking – each more environmentally sustainable than any private vehicle. Some of us need to own a car, most notably in rural areas where 73% of drivers in rural areas rely on a car for shopping and 81% require one for work, in contrast with figures of 46% and 48% for towns and cities. But, with most of the UK living in urban areas, the number of cars being bought reflects desire rather than a need for new vehicles. And, with the biggest growth in owners being among those on lower incomes, the credit boom is surely responsible, meaning we need to question just how much of a good news story the new car sales figures are in the long term."
"The megafires of Australia’s summer “are a harbinger of life and death on a hotter Earth”, a climate summit has said in a forceful declaration for urgent and dramatic climate action. The Climate Emergency Summit, held in Melbourne this week and of which Guardian Australia was a partner, released a declaration saying the warming world was a clear threat to Australian society and civilisation.  “The climate is already dangerous – in Australia and the Antarctic, in Asia and the Pacific – right around the world. The Earth is unacceptably too hot now,” the declaration said. “If the climate warms 1.5 degrees above pre-industrial levels, the Great Barrier Reef will likely be lost, sea levels could rise metres and massive global carbon stores such as the Amazon and Greenland, will hit tipping points, releasing millions of tonnes of carbon into the atmosphere.” Signatories to the declaration included Ian Dunlop, Carmen Lawrence, John Hewson, Tim Costello and Kerryn Phelps. It warned that even the Paris agreement emissions reduction targets would put the world on a path to 3.5C warming by 2100, and 4C to 5C warming “when long-term climate-system feedbacks were factored in”. Does climate change cause bushfires? The link between rising greenhouse gas emissions and increased bushfire risk is complex but, according to major science agencies, clear. Climate change does not create bushfires, but it can and does make them worse. A number of factors contribute to bushfire risk, including temperature, fuel load, dryness, wind speed and humidity.  What is the evidence on rising temperatures?  The Bureau of Meteorology and the CSIRO say Australia has warmed by 1C since 1910 and temperatures will increase in the future. The Intergovernmental Panel on Climate Change says it is extremely likely increased atmospheric concentrations of greenhouse gases since the mid-20th century is the main reason it is getting hotter. The Bushfire and Natural Hazards research centre says the variability of normal events sits on top of that. Warmer weather increases the number of days each year on which there is high or extreme bushfire risk. What other effects do carbon emissions have? Dry fuel load - the amount of forest and scrub available to burn - has been linked to rising emissions. Under the right conditions, carbon dioxide acts as a kind of fertiliser that increases plant growth.  So is climate change making everything dryer?  Dryness is more complicated. Complex computer models have not found a consistent climate change signal linked to rising CO2 in the decline in rain that has produced the current eastern Australian drought. But higher temperatures accelerate evaporation. They also extend the growing season for vegetation in many regions, leading to greater transpiration (the process by which water is drawn from the soil and evaporated from plant leaves and flowers). The result is that soils, vegetation and the air may be drier than they would have been with the same amount of rainfall in the past. What do recent weather patterns show? The year coming into the 2019-20 summer has been unusually warm and dry for large parts of Australia. Above average temperatures now occur most years and 2019 has been the fifth driest start to the year on record, and the driest since 1970. Is arson a factor in this year's extreme bushfires? Not a significant one. Two pieces of disinformation, that an “arson emergency”, rather than climate change, is behind the bushfires, and that “greenies” are preventing firefighters from reducing fuel loads in the Australian bush have spread across social media. They have found their way into major news outlets, the mouths of government MPs, and across the globe to Donald Trump Jr and prominent right-wing conspiracy theorists. NSW’s Rural Fire Service has said the major cause of ignition during the crisis has been dry lightning. Victoria police say they do not believe arson had a role in any of the destructive fires this summer. The RFS has also contradicted claims that environmentalists have been holding up hazard reduction work. “National security analysts warn that 3C may result in “outright social chaos”, and 4C is considered incompatible with the maintenance of human civilisation. “Climate change must be accepted as an overriding threat to national and human security, with the response being the highest priority at national and global levels.” The declaration called on governments to commit to rapidly reducing greenhouse gas emissions to zero, to drawing down carbon concentrations already in the atmosphere, and to integrating adaptation and resilience measures into restructured national and global economies. The executive director of Micah Australia, Tim Costello, told the Guardian the declaration was a rallying cry to emphasise the critical nature of the climate challenge. “This summit was people from military, agriculture, from politics, from economics: we’re all frustrated, we want to see action and a breakthrough, we’re all working hard in our areas, but none of us actually know what will be the tipping point, when it will finally be widely realised that this is an emergency and we have to decarbonise,” Costello said. “Like people understood the emergency of war, there will be a suspension of politics and human rights if we don’t deal with climate.” He said party politics had failed Australia, and shown itself incapable of dealing with the climate emergency. And, he argued, climate change as an existential threat had long been a reality for communities across Australia’s region. “I have seen the poorest communities already losing lives and livelihoods for years from climate change. Now it is our existential challenge after these bushfires, whereas at the Pacific Islands Forum our prime minister was told ‘it is only economic for you, it is existential survival for us’.” The declaration said Australia’s political leaders were especially culpable, guilty of short-term political expediency, which had left Australians acutely exposed to the impacts of climate change. “The first duty of a government is to protect the people, their well-being and livelihoods. Instead, Australian governments have left the community largely unprepared for the disasters now unfolding, and for the extensive changes required to maintain a cohesive society as climate change impacts escalate.” The declaration argued it was in Australia’s self-interest to demand greater global action on climate change, and a continued reliance on fossil fuel resources was unsustainable, both economically and environmentally. Australia was the world’s fourth largest carbon polluter, exports included, and one of the countries most exposed to climate change, the declaration said. “It makes no sense to build our economy on fossil fuel resources, practices and technologies which are unsustainable, particularly when Australia has some of the best clean energy resources and opportunities in the world.”"
"As the electric saw cuts into the base of the horn of the live rhino lying at my feet, I feel an uncomfortable guilt. The rhino shakes and judders and there is an unpleasant smell reminiscent of burning hair. I glance nervously at the friends around me, clad in khaki and camouflage.  But luckily for this rhino, I wasn’t a poacher and there was no blood or bounty – I was there as part of a conservation drive. Just over a month ago I was involved in a local operation to de-horn white rhinoceros in South Africa. The idea is that by removing the horn, we remove the motive for poaching. However, I have a conflict of emotions about this process: taking the horn from a rhino is what the bad guys do. In an ideal world we wouldn’t do this, but it’s for the good of both the individual rhino and the species. At the end of the operation, our rhino walks off to go about its life. Much like your hair and nails, rhino horn is made up of keratin – removing it is painless and it will slowly grow back. But if poachers get to our rhino first, they will almost inevitably kill it to cut off its horn. And its death is unlikely to be painless. Another method to put off poachers is to relocate the rhino. I’ve been involved in transporting rhino hundreds of miles away in massive crates on the back of giant trucks to move them to a safer location.  De-horning and translocation both require rhino capture, helicopters, fast 4x4 drivers, wildlife vets, and a general lack of fear of a very large animal (which is never totally asleep). It is an epic and dangerous operation; for the rhino and the team. Yet, despite these drastic and costly measures on the ground, Africa continues to lose rhino to poaching at an alarming and unsustainable rate, fuelled by demand from the medicinal black market in Asia. Wildlife (or environmental) crime goes way beyond the White Rhinoceros, but this species serves as a flagship for Africa and for conservation. If we cannot save the iconic rhino from extinction at the hands of an illegal trade, then what hope have we for controlling the bushmeat killing fields, saving baby chimpanzees from abduction into the pet or entertainment trades, halting ecosystem-destroying industrial pollution, or preventing the loss of great swathes of forest and their dependent biodiversity? The list goes on.  Wildlife crime is pervasive across the world and ultimately undermines the functioning of ecosystems that we (the human species) depend on for life. Wildlife crime is often international; goods are trafficked across borders from where the natural resource is available to where it is desired. This is why we need an effective international investigative and enforcement network and why we need it operating on the ground in Africa. Interpol aims to connect enforcement agencies and partners across international boundaries – and it recently announced the formation of a new dedicated wildlife crime team in East Africa.  It is not that nations with wildlife crime lack motivation to solve their problems. I have seen first-hand the prevention and enforcement efforts that the Department of Environment and Nature Conservation, the South African government unit tasked with protecting biodiversity, employ to tackle rhino poaching. But current efforts to tackle wildlife crime across the continent and beyond are clearly not sufficient, and any assistance or improved resourcing can only help.  The budget of African enforcement and conservation organisations is dwarfed by the money that international criminal organisations command. One kilogramme of rhino horn is estimated to be worth US$65,000. That is more expensive than the equivalent weight of gold or cocaine. With an adult white rhinoceros averaging a mass of six kilogrammes across its two horns, the horn of a single rhino can represent a staggering US$390,000 to the seller. The support that Interpol can bring will help to redress the resourcing imbalance. The impetus for the creation of the new Interpol team stems from a recent report highlighting increasing losses of African elephants, alongside rhino, to poaching in East Africa. Global ivory seizures reached a new high in 2013 and, like the rhino, elephants are similarly in serious need of effective conservation action.  Sharing of intelligence between agencies in Africa and Asia and ports-of-call in between is likely to be the most effective route to successfully reducing the kill rate of the African elephant and the white rhino, the two largest land mammals on Earth.  Attack the problem from both ends of the chain; target the poachers in Africa (supply) and the dealers in Asia (demand), while providing long-term environmental education and introducing appropriate legalisation of some wildlife products, and we may be better able to save the elephant, rhino, and other species. By setting up the dedicated environmental crime team, which will be based in Nairobi, Interpol can provide the resourcing and support to boost local efforts and integrate response. Basing the team in Kenya is also a good move. Kenya has one of the highest elephant and rhino populations in Africa. This makes it important for wildlife, but also vulnerable to poaching. Kenya lost at least 59 rhino and 302 elephant to poaching in 2013 alone.  Kenya is home to both the southern and northern subspecies of white rhinoceros in addition to the black rhinoceros. Only a few days ago, the death of Suni, the first captive-born northern white rhino, means that this branch of the rhino family is now probably functionally extinct. Suni was one of only two surviving males (the other, Sudan, is 40 (at the time of publication) – fairly old in rhino years – and there is now a population of just six individuals remaining. The northern subspecies was pounded into its current depressing state by years of illegal slaughter. The depletion of this subspecies is an indication of what happens when we fail to act effectively against illegal wildlife crime. We ultimately lose the wildlife. Further investment in fighting wildlife crime, and international collaboration, in tandem with the aforementioned long-term actions, gives us hope that we can avoid the same fate befalling other species. This article was amended on August 24 2016 to correct the surviving northern white rhino male to Sudan, age 40."
"
A Yogi Berra moment – “it’s deja vu all over again…”
From NHC Public Advisory #25
DATA FROM AN AIR FORCE RECONNAISSANCE AIRCRAFT INDICATE THAT MAXIMUM SUSTAINED WINDS HAVE INCREASED TO NEAR 150 MPH…240 KM/HR…WITH HIGHER GUSTS.  GUSTAV IS AN EXTREMELY DANGEROUS CATEGORY FOUR HURRICANE ON THE SAFFIR-SIMPSON HURRICANE SCALE. SOME FLUCTUATIONS WITH AN OVERALL SLIGHT STRENGTHENING IS FORECAST DURING THE NEXT 24 HOURS…AND GUSTAV COULD REACH CATEGORY FIVE INTENSITY DURING THIS PERIOD. GUSTAV IS FORECAST TO REMAIN A MAJOR HURRICANE THROUGH LANDFALL ALONG THE NORTHERN GULF COAST.
Here is my own hurricane track imagery of Gustav and Hanna:

Click for a Hi Definition image
I’m sure this will become mass-media fodder again for the ever popular “global warming causes more damaging hurricanes”, but it is important to note that NHC’s own science officer, Christopher Landsea, co-authored a paper that claims otherwise. So have other scientists.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cec94d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

When Senate Majority Leader‐​to‐​be Tom Daschle asked Jim Jeffords what he wanted in return for switching political sides, the junior senator from the Green Mountain State asked to be chairman of the Environment and Public Works Committee.



That’s the only take‐​home prize Jeffords gets for his defection, so it must be important. And telling.



One can imagine, after this promise, what happened at Jeffords’ meeting with Messrs. Bush and Cheney: “OK, Senator, what does it take to keep you with us?”



Jeffords: “Change your position on the Kyoto Protocol.”



Cheney: “Hit the road, Jim.”



The truth is that Kyoto and global warming will be the focus of Jeffords’ Environment Committee. Expect him to parade witness after witness, on the hottest summer days, decrying Bush & Co. for being “out of touch” with the world on this issue. Robert Watson, head of the United Nations’ Intergovernmental Panel on Climate Change, which bills itself as the “consensus of scientists,” is sure to get top billing.



When he appears, Watson will trot out the U.N.’s new “Third Assessment Report” on climate change, a compendium of more than 1,000 pages that’s due to hit the streets in a couple of months. With this report, Jeffords will show us how loony the current Administration is about planetary heating.



Unlike the U.N.’s first and second Assessments, published in 1990 and 1996, this one purports to tell world leaders what to do about climate change. Here’s what the “consensus of scientists” prescribes (it can be found on page 12 of the “Policymakers Summary” of the new report):



“emissions/​carbon/​energy taxes, tradable or non‐​tradable permits [indirect taxes], subsidies [which require taxes], deposit/​refund systems [taxes and regulations], technology or performance standards [regulations by fiat], product bans [let’s outlaw coal!], voluntary agreements [ha!], government spending and investment [taxes], and support for research and development [paid for by taxes].”



Who is “out of touch” here? Each of these general policy prescriptions requires confiscation of individual wealth to cool the planet or adapt to warming. Apparently, the “consensus of scientists” is that people and markets are too stupid to do this on their own. Did it ever occur to the United Nations that if global warming is as terrible and costly as it thinks it is, there will be a substantial market for technologies and products that would reduce that cost? The U.N. thinks we would rather sit around and fry, unless we are taxed into mending our evil ways.



In fact, there’s good scientific evidence that we adapt well to hot days. Some of today’s most in‐​demand technology, in fact, is designed to prevent death from heat stroke, an affliction that was more common decades ago. The technology is called “air conditioning.”



Are we quietly adapting or passively frying? My University of Virginia colleague Robert Davis and I recently looked at heat‐​related mortality data from major American cities for the last 40 years. At first, we found what everyone seems to know: In some cities, mainly older ones, daily death rates skyrocket on exceedingly hot days. Therefore, the United Nations tells us, if we heat things up more, “several thousand” more people will die every summer in North America because of global warming. The truth is that cities have been heating up by themselves, without global warming, for hundreds of years, compromising our ability to measure the earth’s true temperature.



But when we looked at the trends in heat‐​related deaths, we found that in most cities, the deaths occurred early on–in the 1960s and 1970s. By the time we get to the 1990s, we have engineered heat‐​related deaths out of most cities, with electrically driven air conditioning. In fact, the last big urban die‐​off, in Chicago in July 1995, occurred largely because there was a power failure. Our results have been presented at several peer‐​screened professional conferences and written up many times, all to favorable review.



We don’t expect Jeffords to invite us before his Committee, because we’re “out of touch,” too. Our results show that free markets, not taxation, create the capital that people use to invest in technologies that shield them from the vagaries of our naturally hostile environment. And there’s the problem: When it comes to the environment, Jeffords, the Kyoto Protocol, and the United Nations believe more in taxation and coercion than they do in free markets and free will.
"
"

If you’re free Friday morning, you might want to hop on over to the Russell Senate Office Building to learn about the amazing, inexplicable, short‐​sighted market bias against straw‐​bale buildings and the need for the feds to do something about it. The Environmental & Energy Study Institute, the sponsor of this event, 



Invites you to learn how the ‘new but old’ method of straw‐​bale construction can help address some of our most serious national policy challenges, such as record energy prices and unemployment, inadequate supply of affordable housing, the threat of climate change, and pressing needs in transportation and infrastructure funding. The modern building industry places heavy demands on the energy and transportation sectors. Straw is a locally‐​sourced, widely available, and renewable resource that builders, architects, engineers, and home owners are turning into affordable, safe, durable, and energy‐​efficient buildings in many climates. The following presenters will discuss the benefits of using this American invention, the regulatory barriers and institutional biases against straw‐​bale construction, and the role of the federal government in resolving these issues.



And that parable about the three little pigs? A PR smear spun by “Big Brick” no doubt.
"
"
Share this...FacebookTwitterNOAA 8 inch rain gage. Source: http://www.crh.noaa.gov/iwx/?n=coop_station
German Weather Service meteorologist Christoph Hartmann writes what I think is a surprising essay on measuring precipitation, and the errors in doing so. Indeed Hartmann says precipitation may be understated by up to 50%, or much more at some locations.
As Hartmann explains, measuring precipitation is by no means an exact science, and results have to be taken with a lump of salt.
There are many sources of errors, and in his essay here he looks at just two main sources: wind and instrumentation.
But first, let’s take a look at how precipitation is measured. In his previous essay he described two types of precipitation measuring gages. In Germany precipitation is measured with the unit of liters/m², e.g. 25.4 liters is an inch of rain.
Two methods of measuring precipitation
Hartman explains that precipitation is generally measured by a rain gage with a known opening area, for example 200 cm² in Germany, which is positioned 1 meter above the ground surface. The gage funnel catches the precipitation and leads it to either
1) a graduated measuring tube or a
2) an optical drop counter 
Optical rain gage (drop counting). Source: atmos.washington.edu
With the measuring tube system, the tube is graduated and the amount of precipitation can be simply read off. With the optical rain gage (drop counter), the amount of precipitation is derived from the number of drops. If the precipitation is snow or ice, then the measuring tube or optical gage are brought inside and the captured precipitation is melted and measured.
Wind and errors up to 400%
Hartmann explains that the biggest sources of error are wind-related. This is easily seen when measuring snowfall. Just before a snowflake falls into the gage, air turbulence sucks it back out tosses it overboard. Just taking a look around after a blizzard, it’s easy to imagine how difficult it is to measure snowfall. Places exposed to wind are barren, while other places are covered by meter-deep snowdrifts. How much snow really fell?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Hartmann says measurement errors of up 400% can occur over time when measuring powdery snowfall in alpine, polar or windy areas.
One way to reduce error is to place the instrument in a wind-protected area. By measuring the wind speed, it is then possible to adjust precipitation measurements. But Hartmann writes:
Wind effects lead to an under-estimation of the actual fallen precipitation. The level of deviation depends on the speed of the wind and the type of precipitation.
Because wind speeds are factored into precipitation measurements, climatological precipitation trends without taking changes in wind speeds into account should always be deduced very carefully.
The second problem encountered arise from the two above described measurement instruments, especially with the optical rain gage, writes Hartmann. With frozen precipitation, the gages are heated up in order to melt the precipitation. But this involves evaporation. And under torrential rains, the optical gage becomes much less accurate. The result, writes Hartmann:
Under equal precipitation amounts, the optical gage measures less precipitation than the measuring tube, both in summer and in winter.
So if two different stations use different instruments, them they will show different precipitation amounts even when the actual precipitation is the same. In summary, Hartmann writes his stunning conclusion:
In total these two sources of errors lead to a precipitation deficit of 5 to 15% for liquid precipitation, and between 20 and 50% for solid [frozen] precipitation. In very windy locations, the deficits are substantially more.
Because instruments measure less precipitation than what actually falls, it means we have a worldwide precipitation deficit solely because of the measurement method.
What does it all mean? Are many of the reported droughts solely the product of faulty readings? And we all thought that the network of temperature measurement stations was a mess. This is a huge open floodgate to potential climatological data manipulation and bogus assertions. See here for example: motherjones – the coming mega-drought (h/t NTZ reader DirkH).
Share this...FacebookTwitter "
"Glasgow recently became the first European university to join the rapidly-expanding fossil free divestment movement. Following hot on the heels of the Australian National University, Glasgow promised to move £18m of investment over the next ten years. The international, grass-roots, student-led fossil-free movement now has the support of religious, medical and charitable bodies across the world (181 and counting). These organisations have divested because they can no longer endorse the activities of the fossil fuel sector. The movement is inspired by the success of the anti-apartheid divestment campaign, where financial and moral pressure on companies doing business with South Africa contributed to the fall of the apartheid regime. The campaign is beginning to rattle fossil fuel companies. A fight-back has begun. Pro-coal Australian prime minister Tony Abbott has called divestment “stupid”. Academics, too, have criticised the campaigners as hypocritical. Such criticisms are wrongheaded. Anyone who cares about climate change should support the divestment campaign. Viewed at a global level, existing solutions aren’t working. The ability of market-based instruments to reduce carbon emissions is more a matter of faith than empirical evidence. Carbon reductions from renewables, while growing fast, are offset by increases elsewhere. Greater efficiency stimulates growth and consumption, not parsimony. Existing measures are like “squeezing a balloon”: reductions in one place lead to increases elsewhere. The Fifth IPCC assessment warned that we have five times more fossil fuel reserves than we can safely extract if we are to stand a decent chance of staying under 2°C warming. This puts the question starkly: how can we leave this carbon in the ground? The divestment movement confronts the core logic – licence, extract, profit – of fossil fuel companies. One key tactic to make it harder for them to extract carbon is to erode their political legitimacy. Fossil fuel companies use their economic clout to sow doubt about climate science. They lobby for generous subsidies and flout indigenous rights. They commission toys and sponsor art at the Tate, the British Museum, the Royal Shakespeare Company and other cultural institutions to normalise the presence of big oil in our everyday lives. By divesting, organisations such as the World Council of Churches send a strong message: we find your activities immoral. The moral case for divestment is based on the clear environmental damage and the undemocratic power of these corporate behemoths. By stigmatising fossil fuel companies, the divestment movement aims to reduce their political room for manoeuvre. When mainstream figures such as the governor of the Bank of England says fossil fuel reserves can’t be burnt, or the Rockefellers start divesting from fossil fuels for financial reasons, people take notice. The financial case for divestment is based on the carbon bubble. The financial health of fossil fuel companies relies on 2795 gigatonnes of “unburnable” carbon – reserves that have to stay in the ground if we are to have a decent chance of staying under 2C warming. This creates enormous financial risk, as a change in policy (or indeed in climate) could leave these reserves and their associated infrastructure stranded. Long-term financial sustainability is at odds with carbon investment. So far, £30 billion has been divested; small beer compared to the £441 billion spent on exploration by the top 200 companies in 2012. For deeper success, divestment will need to break out beyond churches and charities to affect wider market norms. If this happens, debt will likely become less accessible and capital-intensive projects at the margins less feasible. This can only be a good thing for the climate. Eventually, campaigners hope fossil fuels will face a regulatory and legislative environment that forces the whole company – not just the green-tinged outliers – to move beyond petroleum, or to make way for those who will. All this fossil fuel bashing will be too much for some. “We all use fossil fuels, you included!” says the critic when she leaps to the defence of big oil. This is true, as far as it goes, but naïve. Energy use is not a matter of individual choice – whether we like it or not we are locked into world systems whose very life-blood is oil. We can’t choose a decentralised grid, renewable supply, or decent cycling infrastructure, thanks to historic legacies and the continued power of big oil. We need divestment to work because fossil fuel companies distort politics and stand in the way of a sustainable future. “We should engage fossil fuel companies, not demonise them,” runs another counter-argument. Investor engagement can work, but only if clear goals and timelines are set. Research that helps companies extract more efficiently just gets carbon out of the ground faster; working with companies on renewables, carbon capture and storage, or low-carbon technology can work, but does nothing to transform the core business of big carbon. And when the laws of coercive competition squeeze, big carbon will always retreat to its core business. We are well past the point where the good delivered by fossil fuel companies outweighs the environmental, social, and economic negatives. We need any and all tactics to achieve a post-carbon world. Divestment puts fossil fuel companies in the spotlight, names them responsible for climate change, and confronts their power. Divestment should be supported by everyone who cares about climate change."
"
Note: I don’t normally allow the discussion of things related to Nazi Germany here, including discouraging the use of the word “denier” due to it’s “Holocaust Denier” connotations. But this full page ad in the Sunday papers in Britain, touting “climate crime” and “climate cops” is just a bit over the top, and deserves some attention. It is particularly relevant since the sponsoring website climatecops.com has a teachers section, and we’ve just seen some sensibility from Schwarzenegger in Sacramento on this very issue. I find this method of indoctrinating school children to normal everyday living being harmful to the earth with the “climate crime” connotation as distasteful and wrong headed. I have no problems with energy conservation, in fact I encourage it. But combining  such advice with a “climate cop” idea is the wrong way to get the message across. Can you imagine what sort of reaction the neighbors will have to the kids hanging this door hanger on their front door? Will the result of this now be hiding your electric dryer behind false walls so the kids and neighbors don’t see it?
At the very least, npower could have chosen a different color scheme: red, black and white are the same three colors used in the flag of Nazi Germany What were they thinking? – Anthony
Reposted from the website EU referendum:

Can I be the only one more than a little disturbed by the latest campaign to be fronted by energy company npower?
Launched today with large colour ads in the Sundays, it appeals directly to children, urging them to enlist as “climate cops”, to root out “climate crimes“, and thus “save the planet”.
In a luridly-designed website, mimicking the style of “yoof” cartoons, it offers a bundle of downloads, including a pack of “climate crime cards“, urging its recruits to spy on families, friends and relatives, inviting each of them to build up a “climate crime case file” in order to help them ensure their putative criminals do not “commit those crimes again (or else)!”
Quite what the “or else!” should be is not specified, but since the “climate cops” are being encouraged to keep detailed written records (for those who can read and write), there is nothing to stop these being submitted to the “Climate Cops HQ” for further sanctions, the repeat offenders being sent to re-education camps. And for those “climate cops” that successfully perform the “missions” set (or turn in their own parents), there is the reward of “training” in the “Climate Cop Academy”.
In a system which has echoes of Hitler’s Deutsches Jungvolk movement, and the Communist regime Pioneers, perhaps successful graduates can work up to becoming block wardens, then street and district “climate crime Führers”, building a network of spies and informers.
How nicely this ties in with James Hansen’s call to put the chief executives of large fossil fuel companies on trial for high crimes against humanity and nature, accusing them of actively spreading doubt about global warming.
No doubt, with a willing band of “climate cops”, the prosecutors can spread their nets wider, reaching into the homes of all climate change deniers, until the insidious virus of doubt is exterminated (final solution, anyone?). Then we can all march on the sunlit uplands of a “carbon-free” planet – to the tune of Ode to Joy no doubt.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9df59f46',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Today at 00 GMT (5PM PST) a new month started. Every time a new month of statistics starts being logged by WordPress for Watts Up With That, I say to myself, “there’s no way I’ll get this sort of traffic again”. And yet, again I’m surprised that WUWT not only met last months stats, but significantly exceeded them.
Thank you again, loyal readers.

Click for full sized image
It was one year ago that I moved from the Typepad blog to WordPress, and as you can see from above, the growth has been steady, except for one month, April. which had a slight dip.
For September 2008 the total was 846,193 page views, up from 667,215 page views in August 2008.
But there is a caveat, I think the real numbers are just shy of 800,000, because on the weekend of 09/20 and 09/21 I got quite a bit of unexpected traffic that I’m not sure is real or not. During that time, we got a lot of Spam on one particular older entry comparing UAH, RSS, HadCRUT, and GISS, but not anywhere near the numbers specific to that post, shown below:
Blog Stats Increase due to DOS “something”

Saturday 09/20     23,486
Sunday   09/21 25,319
Monday  09/22       1,006
Total: 49,811
You can read about it here in this entry
I checked with WordPress support, twice, and they assured me that the numbers are real, saying:
Hi,
Our stats expert has had a look and found no evidence of a DoS or anything untoward.  He says “the most plausible reason is an email newsletter featuring the URL, or else some other non-browser app loading the URL such as a feed reader. I have not been able to find any evidence of of a DDOS attempt or other “foul play.”
Separately, I’ve checked our security logs and see no other signs of activity that would normally indicate a blog under attack.
In short: we’re quite sure the traffic is genuine and doesn’t correspond with an attack of any kind.
Kind regards,
Alex
WordPress Support
Even so, I’m unconvinced. I got not one single comment added on that posting during the onslaught of traffic, almost 50,000 page views, which tells me the numbers aren’t real, no matter what WordPress support says.
Therefore I have decided to take the step of publishing an “adjusted” set of numbers this month. The difference is that instead of inflating the numbers, such as GISTEMP and USHCN adjustments do, I’m reducing them to what I consider a truly representative value for the month.
Raw WUWT September numbers:           846,193 page views
WUWT Spam Uncertainty numbers:           -49,811 page views (from 09/20 to 09/22)
Final Adjusted WUWT September numbers:    796,382 page views
Still, not too shabby.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bea60b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Scientists tell us that the world is warming and greenhouse gas emissions are to blame. Yet climate change framed by scientists, politicians and economists as a straightforward pollution problem will neither convince sceptics nor advance the difficult decision-making process. Mark Maslin, a climatologist at University College London, recently offered his analysis of climate denial and the sceptics’ reluctance to accept the science. In an article published here he correctly observed that “the lack of acceptance of the science of climate change is neither due to a lack of knowledge, nor due to a misunderstanding of science”. Yet by framing climate change as “a massive pollution issue that shows the markets have failed and it requires governments to act collectively to regulate industry and business” he continues the vicious circle. If climate science provides the main argument for governments to regulate private enterprise, sceptics will continue to deconstruct the supposedly legitimising science. Therein lies a problem not only with Maslin’s generally well-informed analysis, but also with the orthodox political approach to climate change. In the US and anglophone countries, any efforts to enforce emission regulations to solve the “massive pollution issue” are regularly stunned by the sceptical critique of the “underlying” scientific facts. It should be clear by now that climate change, although frequently described by scientists, economists and politicians as a straightforward pollution problem, requires more than a technocratic “solution”. By now, everything from trade policy or global inequality to animal extinctions or indigenous peoples’ rights has been woven into the tangled knot of climate change politics. To be sure, scientists have done a great job of bringing climate change onto the political agenda: because of new scientific knowledge, carbon-consuming power plants or gas-guzzling cars that were once tolerated and applauded are increasingly perceived as undesirable or immoral. Yet the orthodox approach to climate change, epitomised in the Kyoto Protocol, has proven ill-suited to adequately address the wicked problem of sovereign states all pursuing their own national, carbon-consuming interests. However useful it has been in bringing the topic onto the political agenda, the scientific framing of climate change as a global pollution problem to be fixed by governmental intervention has run aground. So if you don’t want to talk with sceptics about the science of climate change, then you probably shouldn’t call it a massive pollution issue either, or you will be reminded also of the benefits of industrial pollution. The debate climate scientists are understandably tired of – but unknowingly contribute to – is American at heart. Politicians in the US put a higher premium on science to resolve their ideological conflicts than anywhere else – just look at the dozens of Congressional hearings on climate science over the past three decades.  Similar debates are going on over petrochemical pesticides such as DDT, genetically modified organisms (GMOs), or the smoking ban. In each of these cases the protagonists wanted to rely on the authority of science to settle their value disputes. The various parties are committed to different ethical and ideological positions but they are united in their focus on science and claim to derive the legitimacy of their policy position in one way or another from scientific evidence. Why should we ban GMOs? Because science says so. Unless it doesn’t. Many of those who oppose the power of agro-giants have taken the bait and discuss scientific uncertainties rather than the political economy of GMOs. The authority of science, it is understood, trumps the opponent’s “irrational” value judgements. However, in political practice, the scientific framing of these debates – how harmful is DDT, do GMOs affect human health, does second-hand smoke cause lung cancer? – has made them only worse. To be sure, a science-based approach to decision-making does have great normative force. It allows governments to claim the high ground, a place from which they can be seen to be acting for the benefit of all. But in the case of climate change it does not work, simply because we are not dealing with a pollution problem to be solved cost-benefit style. Climate change is not a hole in the earth’s ozone layer caused by a set of manageable chemicals. This time an international treaty won’t do the trick. Of course the climate controversy as it thrives in the US has now taken on an almost global significance. Facilitated by the English language and internet media in which it is conducted, the debate over climate science gives support to those in other countries where US climate sceptics’ talking points have become part of the political rhetoric. While we can appreciate efforts to understand the reluctance of accepting the science of climate change, we should pay attention also to how the problem is framed. While the pollution narrative has served to keep climate scepticism alive and scientists on their toes it has led to very little real progress – emissions are still rising and a global treaty is yet another climate conference away. The only rational solution would be to drop the “science says” arguments altogether and foster pragmatic climate policies that do not hinge on scientific truth. What a relief this would be for science and scientists to reclaim their right to be wrong.  These policies do not tackle global warming directly via emission reduction targets, as if one could control the global thermostat. Environmental and human health benefits rather emerge as positive side-effect of policies dealing primarily with energy security and the modernisation of inefficient energy producers. In this view climate policy should be connected with established institutions and forms of decision-making, for instance with national health policy-making. The first line of this article originally read “Scientists tell us the world is warming and that a climate catastrophe is imminent. They’re probably right.” It was changed on February 6 at the author’s request."
"Ask people to describe what they associate with butterflies, and you will probably get an image of a sunny summer’s day, with a beautiful peacock drifting gently on the cooling breeze.  Ask the same question but for moths, and you are more likely to be told about holes in a favourite woollen jumper, or something small and brown beating itself to death against a bathroom light fitting. We don’t view moths with the same affection as their day-flying cousins, and the irrational fear of moths is even common enough to have a name: mottephobia. To my mind, this is unfair. Let’s start with the clothes-eating accusation. According to the charity Butterfly Conservation (which also protects moths), there are around 2,500 species of moths found in the UK; of these, only two will attack clothes. They prefer dirty items in undisturbed places: that means that if your jumper has been attacked, you probably weren’t wearing it often enough anyway.  In fact, there are also moth species that are crop pests, such as the diamondback moth, and some which produce irritant hairs as caterpillars – the non-native oak processionary moth currently infesting London is one such example. But again, these examples are very much in the minority among the total diversity of moths, and there are very many more reasons to love moths than to hate them. Many moths are just as beautiful as butterflies; some even look like butterflies! A particular favourite of mine is the brimstone moth: exactly like the brimstone butterfly, it is so-called for its vivid sulphur-yellow colour. Some are superbly camouflaged, like the buff-tip – easily mistaken for a broken twig of silver birch. And some have attractive names to match their appearance, such as the fabulous Merveille du Jour (“marvel of the day”), pictured at the top of this article. But beyond their beauty, moths also perform vital roles in the natural communities to which they belong. Along with colleagues, I recently examined the scientific literature to establish how important moths are to flowering plants as providers of pollination services. Most moth species that feed as adults do so by drinking nectar, and in doing so accidentally carry pollen between flowers.  We found examples of moths serving as important pollinators across many important habitats and in every continent except Antarctica. Several studies suggested that moths were the second most important pollinators in the area surveyed, behind only bees. In the UK, moths might be pollinating wildflowers near you including honeysuckle, bramble, white campion, wild carrot, thistle and ragwort. In North America, moths pollinate many types of cactus, and milkweed (beloved of caterpillars of the monarch butterfly). Globally, they are also well-known pollinators of many orchid and lily species. If you’re a bird or a bat, a moth makes for a tasty treat. Researchers from the University of Bristol have examined the diet of two species of long-eared bat in England, revealing that their main food source was large-bodied moths from the Noctuidae (or owlet moth) family. Whereas bats (and the eerie nightjar) eat adult moths, moth caterpillars provide a vital food source in the spring for young birds including blue tits, which can munch their way through 35 billion caterpillars in the UK every year.  Especially important in this regard are those moth species that are active as adults through the winter, including the aptly named winter moth; these species reach the caterpillar stage of their life cycle at exactly the right time of year for busy blue tit parents to exploit. And did you know that one species of moth has been domesticated into a valuable commercial livestock species, at the centre of a multi-billion dollar global industry? When spinning a cocoon in which to undergo the transformation to an adult moth, the caterpillars of the domesticated silkmoth Bombyx mori produce lustrous threads several hundred metres in length. These threads, once unravelled, are spun into the fine textile silk. And if none of this has convinced you, know this: butterflies are actually just a minor subset of moths!"
"
While sunspots are often cited as the main proxy indicator of solar activity, there is another indicator which I view as equally (if not more) important. The Average Planetary Magnetic index (Ap), the strength of which ties into Svensmark’s cosmic ray theory modulating Earth’s cloud cover. A weaker Ap would mean less cosmic rays are deflected by the solar magnetic field, and so the theory goes, more cosmic rays provide more seed nuclei for clouds in Earth”s atmosphere. More clouds mean a greater albedo and less terrestrial solar radiation, which translates to lower temperatures.
I’ve always likened a sunspot to what happens with a rubber band on a toy balsa wood plane. You keep twisting the propeller beyond the normal tightness to get that extra second of thrust and you see the rubber band start to pop out knots. Those knots are like sunspots bursting out of twisted magnetic field lines.
The Babcock model says that the differential rotation of the Sun (the sun being a viscous fluid, the poles rotate at a slower rate than the equator) winds up the magnetic fields of it’s layers during a solar cycle. The magnetic fields will then eventually tangle up to such a degree that they will eventually cause a magnetic break down and the fields will have to struggle to reorganize themselves by bursting up from the surface layers of the Sun. This will cause magnetic North-South pair boundaries (spots) in the photosphere trapping gaseous material that will cool slightly. Thus, when we see sunspots, we are seeing these areas of magnetic field breakdown.

Sunspots are cross connected eruptions of the magnetic field lines, shown in red above. Sometimes they break, spewing tremendous amounts of gas and particles into space. Solar flares and coronal mass ejections (CME’s) are some examples of this process. Sometimes they snap back like rubber bands. The number of sunspots at solar max is a direct indicator of the activity level of the solar dynamo.
As many of you may recall, a few months ago, I had plotted the Average Geomagnetic Planetary Index (Ap) which is a measure of the solar magnetic field strength but also daily index determined from running averages of eight Ap index values. Call it a common yardstick (or meterstick) for solar magnetic activity.

Click for a larger image
I’ve updated the graph today, to include July 2008 Ap data as you can see below:

Click for a larger image
Source data, NOAA Space Weather Prediction Center:
http://www.swpc.noaa.gov/ftpdir/weekly/RecentIndices.txt
As you can see, the Ap Index has continued along at the low level (slightly above zero) that was established during the drop in October 2005. As of July 2008, we now have 34 months of the Ap hovering around a value between 5 to10, with occasional blips of noise.
Since it is provided in the same dataset, I decided to also plot the smoothed Ap Index.
I also plotted my own 24 month smoothing window plot, shown in magenta.

Click for a larger image
I also decided to update the plot of the 10.7 centimeter band solar radio flux, also a metric of solar activity. It is in the same SWPC dataset file as the Ap Index, in columns 8 and 9. The smoothed 10.7 CM flux value provided by SWPC has also dropped about the same time continues a downward trend.
I also provided my own 24 month wind smoothed value which is plotted in magenta.

Click for a larger image
Note the lower flux values during this solar minimum than the last
We continue to remain in a deep solar minimum, and with the forecasts being modified to push back the real “active” start of Solar cycle 24, it remains anybody’s guess as to when the sun will come out of it’s funk.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d0d54cd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Binge-watching Netflix doesn’t just fry your brain; it may also be frying the planet. The streaming service’s global energy consumption increased by 84% in 2019 to a total of 451,000 megawatt hours – enough to power 40,000 average US homes for a year. Netflix disclosed these figures in its inaugural environmental, social and governance report, noting it matched 100% of its 2019 non-renewable power use “with renewable energy certificates and carbon offsets”. While these may help the brand, they don’t address the inconvenient fact that our love of streaming has unfortunate side-effects – most of which we are only starting to comprehend.  Digital technology has ushered in an age of inconspicuous consumption. It is easy to understand the environmental impact of buying “stuff” or flying across the Atlantic. It is harder to wrap your head around how much energy it takes to fly data across the web. We may feel that we are consuming less thanks to the internet, but digital technologies account for more carbon emissions than the aerospace industry, according to a study by the Shift Project, a Paris-based thinktank. Transmitting and viewing online video accounts for a large portion for this, generating nearly 1% of global emissions. Similarly, a study from the universities of Glasgow and Oslo found that streaming music has led to “significantly higher carbon emissions than at any previous point in the history of music”. Being a conscientious consumer does not mean you have to turn off your wifi or chill with the Netflix. But we should think more critically about our data consumption. Apple already delivers screen-time reports; perhaps tech services should start providing us with carbon counts. Or maybe Netflix should implement carbon warnings. Caution: this program contains nudity, graphic language and a hell of a lot of energy. •Arwa Mahdawi is a Guardian columnist"
"The dairy sector in the UK is going through a period of high uncertainty. Not only are suppliers having to cope with retail price wars and the fact that milk prices are being reduced by the increasing alignment of domestic and world dairy prices, they are also facing the fact that milk quotas will be eliminated in March, which promises to fully expose the sector to market forces.  So it is not surprising that recent discussions have focused on how the dairy sector in the UK should adapt. Some view the future as inevitably going towards mega-dairy farms, imitating those existing in the US, where thousands of cows are milked under huge sheds without seeing the sun, in something that is closer to an intensive factory than a farm.  Whether this comes to pass depends on whether these mega-dairy farms are the only solution to maintain the competitiveness of the sector and therefore its survival. In the UK there are just a handful of dairy farms with herds of over 1000 cows, whereas in the US they can be ten times that size. The prospect raises two questions: are farms whose practices resemble mega-dairies, including feeding cows in-house all year round, more profitable, and does this necessarily mean low animal welfare standards? First some context. The average price received by farmers in 2014 (up to October) was 32.17p/litre (though it can vary by over 10p/litre depending on the nature of a farmer’s supply contract). This is much about 6.7 times higher than the average milk price received in 1973, though in fact the price has fallen since then once you adjust for inflation. This doesn’t take into account the effect of the cost of production, however. If the cost of production has fallen faster than the wholesale price of milk, this might not be a problem.   Unfortunately historic production costs are not available. But we do have historic feedstuff costs going back to 1988 from the department of agriculture, which are the highest component of costs (more than double labour, which is second). So these can give us some idea of what has been happening. As can be seen from the chart that I have used these figures to plot below, this suggests that the true price of milk that farmers receive has fallen since 1995.  The true farmers’ price of milk The decrease in milk prices reflects how competitive the sector has become, partly because retailers drive down prices and partly because of increasing competition from other food products. The chart below shows the decline in purchases of milk products, driven by a strong decline in the consumption of full-fat milk that has not been replaced by skimmed milks (note that cheese consumption has been rising but not very strongly).     UK weekly dairy consumption per capita  This drop in demand for milk products has led to a structural adjustment in the sector, reducing both the number of dairy cows and dairy farms in the UK. Here’s the trend going back to the mid-1990s: Dairy farms and dairy cows Notice that the number of cows has been falling more slowly than the number of farms. This implies that the number of cows per farm has been growing, which might suggest that we are heading towards mega-farms. It is also worth pointing out that there has not been a corresponding drop in the volume of milk being produced. This is because yields per cow have been growing steadily thanks to a combination of technological advances and more high-yielding cows.  Milk production and yields A recent UK survey by Scotland’s Rural College found that of the country’s 863 dairy farms, the traditional British dairy management style of all-summer grazing and winter-only indoor feeding was practised by less than a third of respondents, and on average, herd sizes were larger within systems that feed indoors. This would mean that the proportion of cows being milked in such farms will be rather higher.  This decline in grazing has happened in many countries. Reasons include difficulty in controlling feed rations for high-yielding animals, uncertainty of grass supply in some countries, practical difficulties such as walking distances and lying times, and the availability of a stable labour force. High-yielding cows can also demand up to five times as much energy, which can be difficult to achieve by a grazing and silage-based diet. This can mean that some additional feeding is required. But if this is the trend, what about profitability? Interestingly well managed grazing-based farms do not seem to be less profitable than those that rely more on indoor feeding. Milkbench+, the dairy benchmarking agency, found it is possible to produce milk efficiently at almost any scale and at any level of outputs, as detailed in this figure:  Profitability and herd size But while the above figures are based on profitability per litre of milk produced, a very important point is that most probably very small herds will not provide sufficient income for owners. This can make the size of the enterprise a key aspect, depending on the farmer’s objectives.  Grazing-based systems have on average fewer cows than farms with indoor-based feeding systems. And certainly, housing and management style can affect the welfare of dairy cows. It has been reported that the British public is opposed to indoor dairy systems – though many will not be aware that traditional British systems have cows spending winters indoors anyway.  The Farm Animal Welfare Council says that housed dairy cows in the UK can have an acceptable standard of welfare as long as suitable housing is provided together with skilled animal husbandry and veterinary practice. Nevertheless, continually housed dairy cows can be susceptible to a range of health issues in feet and legs and are at greater risk of health disorders such as mastitis and retained placenta.  There are techniques that may lower the incidence of some health issues, however. And dairy cows maintained in grazing systems may also be at risk of health issues such as lameness and milk fever and are also exposed to prevailing weather conditions. There are still a variety of dairy management systems in use but there does certainly seem to be a clear trend towards farming that feeds more time indoors. If larger farms don’t necessarily mean more profitability – or lower welfare standards, thanks to our UK legislation – it may well be that farmers are making up for lower margins by seeking to generate more income from their holdings.  A couple of positive closing thoughts: We always have the option of buying our dairy products from farmers that support higher welfare standards. And a shift towards larger indoor farms is unlikely to result in US-style mega-dairy farms – we simply don’t have the space. Hard Evidence is a series of articles in which academics use research evidence to tackle the trickiest public policy questions."
"

The recent Earth Summit, the United Nations Conference on Environment and Development (UNCED), in Rio de Janeiro was to have established an ambitious environmental agenda for the 21st century. The conference addressed virtually every field of human endeavor: energy, industry, water and land use, agriculture, human health, and transportation, among others. An international congregation of tens of thousands of bureaucrats, environmentalists, technocrats, planners, and others arrived in Rio to guide humanity on its course toward the brave new world defined by Agenda 21, the Rio Declaration, the Conventions on Climate Change and Biological Diversity, and the Forest Principles.



The principal theme for UNCED was “sustainable development,” a concept popularized by the report of the World Commission on Environment and Development. That report, entitled Our Common Future, was presented to the United Nations in 1987 by commission chairman Gro Harlem Brundtland, the Labor prime minister of Norway.



Our Common Future was compiled in response to a 1983 request of the UN General Assembly for “a global agenda for change.” Heavily influenced by former West German chancellor Willy Brandt’s North‐​South Report, the commission called for restructuring mankind’s future so that economic growth would be “based on policies that sustain but expand the environmental resource base.”[1]



“Sustainable development” means different things to different people. Its definition is intentionally vague to increase the possibility of compromise on thorny issues on which reasonable people may differ. To those inclined toward balancing economic and environmental goals–as are the authors of this study–“development” implies economic growth, and “sustainable” implies full consideration of environmental factors. It is becoming abundantly clear, however, that to others the term implies virtually no additional economic development.[2] The latter position is based on the argument–made with great emotion but insufficient facts and analysis–that the current path of development is clearly unsustainable because the planet is about to choke on humanity’s wastes and there is not enough land to meet everyone’s demands. Even though that faction, inherently suspicious of technological change and economic growth, was unable to obtain all that it wanted, it is today much closer to its goal as laid out explicitly in the early drafts of the Agenda 21 documents.



The draft Agenda 21 documents specified various measures that nations “should” undertake. Ultimately, the conference shied away from that heavy‐​handed approach and decreed those measures to be optional. If acted upon faithfully, many of those measures would impose national planning on virtually every economic sector, regulate almost all human activity, and subordinate all social and economic goals to environmental goals. Thus, while calling for a massive transfer of wealth and technology from developed to developing nations, the recommended measures would deny all nations the means of creating more technology and wealth by putting in place legal and institutional frameworks that would be incompatible with either technological progress or economic growth. Proponents of those measures, in part because of their refusal to balance environmental and socioeconomic goals, would needlessly reduce agricultural productivity in the name of sustainability–even as they lament that the world has too many people to feed and that agriculture uses too much land and denudes forests, thereby threatening species and reducing biodiversity. Such measures, in addition to being poor social and economic policy, would be counterproductive; they would cause more environmental harm than good, and they might even bring about the very catastrophe environmentalists strive to avert.



In particular, the Convention on Biological Diversity, which the United States rightly refused to sign, does not even address the single most important reason for the loss of biological diversity and deforestation: the loss of habitat and land conversion to meet fundamental human needs for food, clothing, and shelter. Moveover, the convention would reduce the incentives to research and develop the very technologies that would–if anything could–help meet the competing demands made on land by human beings and other species.



Using as an example the historical increase in U.S. agricultural productivity, this study will show that–but for technological progress–all our forestlands and croplands, including those that would have been only marginally productive, would have had to have been plowed to produce the quantities of food we produce today. The accompanying wholesale destruction of forests and other natural habitats and the reduction of biodiversity would have resulted in environmental problems that would have dwarfed those we face today and matched environmentalists’ worst nightmares.



The only way to feed, clothe, and shelter the greater world population that the future will inevitably bring–while limiting deforestation and loss of biodiversity and carbon dioxide sinks–is to increase, in an environmentally sound manner, the productivity of all activities that use land.[3] Such increases are possible only within a legal, economic, and institutional framework that relies on free markets, fosters decentralized decisionmaking, respects individual property rights, and rewards entrepreneurship. Such an approach is the best hope for a world facing severe pressure on its land base, yet it has been noticeably absent from recent, much‐​publicized strategies that purport to lead us to sustainable development, conserve biological diversity, and combat global deforestation.
"
"Ten years on from the devastating Boxing Day earthquake and tsunami, our understanding of very large earthquakes has grown enormously. From satellites monitoring changes on the Earth’s surface to drilling deep below the ocean floor, new techniques are constantly being developed to help us figure out why earthquakes are sometimes so big, and so deadly.  The 2004 earthquake ruptured the fault marking the contact between two tectonic plates at a subduction zone, where one plate slides beneath another – the locations on Earth where the very largest earthquakes and tsunami are generated. Before 2004, the last earthquake of magnitude nine or larger occurred in 1964, in Alaska. However in the past decade there have been several very large quakes, including Chile in 2010 and the 2011 Tohoku-oki earthquake in Japan that caused the Fukushima nuclear power station meltdowns.  We know a great deal more about these recent events than we did the Alaska earthquake as, not surprisingly, the technology we use is a lot more advanced than in the 1960s. Satellites can now track the movement of the Earth’s surface during, before and after an earthquake using GPS, and they can track a tsunami wave in the deep ocean. Techniques to record the earthquake waves and to resolve the details of the earthquake fault slip are now much more sophisticated. And we can gain much higher resolution details of geological layers and structure under the seafloor, helping us find out more about earthquake faults. The 2004 earthquake occurred when the Indian plate slipped beneath the Sunda plate. Though scientists were well aware this was an active fault line, the plate slipped over a much larger area and further into the shallow fault zone than anticipated. The 2011 Japan earthquake and tsunami that followed was even more unexpected in character: the fault slipped into its very shallowest section and the slip was the greatest ever recorded on a fault – the fault (and in turn the overlying seabed) moved by up to 50-60m. To put this into context, in a “typical” large earthquake, the fault slips by about 5-10m and still causes very damaging effects. For the first time, equipment on the seafloor offshore Japan in 2011 measured the changes in pressure as the seafloor moved, and was able to record the magnitude of the fault slip. This very large and shallow slip certainly amplified the tsunami, leading to devastating results. These recent earthquakes have truly called into question the existing models for very large earthquake generation and have resulted in scientists rapidly generating new ideas and re-examining the hazard potential at other subduction zones around the world. Although we now have very sophisticated techniques to remotely record the earthquake process and the geology below the seafloor, we really need to sample the fault rocks themselves, where the real action goes on and where earthquakes are generated, typically 5km or more below the surface or seafloor. Sampling this zone “in situ” is one of the holy grails of modern fault and earthquake studies – and the only way to do this is through ocean drilling using the same techniques as the petroleum industry.  In the past 10-15 years projects tackling active faults and the earthquake process, particularly at subduction zones, have increased in number and in the boldness of their objectives, drilling in very challenging conditions.  Drilling is ongoing on the tectonic plate margin of Japan as part of the NanTroSEIZE programme, south of the region affected by the 2011 earthquake. Scientists have drilled lots of boreholes to sample different parts of the system and the project is currently 2km from the primary target fault zone. This part of the Japanese plate boundary generates damaging tsunami-genic earthquakes about every 150-200 years and the project will not only sample the fault rocks but also continuously monitor the fault to help reduce potential risk.  Further north along the Japan coast, and soon after the 2011 earthquake, a team of scientists rapidly prepared a ship to drill through the shallow fault where the extremely large slip occurred. The rock samples they brought back to the ship were unexpectedly weak and we now believe this is why there was such large and shallow slip.  Weakness of the earthquake fault may be the answer for the surprising 2011 earthquake but what about other subduction zones?  A drilling expedition will sail to the tectonic margin off Sumatra in 2016 to drill into the sediments supplying the fault zone that generated the 2004 earthquake. Here we hypothesise that the sediments may be especially strong and that this resulted in shallower fault slip than expected – the opposite to what has been proposed for the Japan 2011 earthquake fault.  If both theories are correct, we will need to revisit global subduction zones to re-assess sediment and fault properties and how these faults may slip in future. Since the 2004 earthquake and tsunami we’ve learned a lot but, as is often the case in science, we now have new questions to answer. However new technology is giving us the ability to answer them – this is a very important time to be an Earth scientist."
"The UK’s environment agency has confirmed quagga mussels, a fresh or brackish water mollusc, have been found in a reservoir near London.  Quagga were previously identified as one of the single greatest threats to UK wildlife, yet also one predicted to be highly likely to invade UK waters.  Originally found in the Dnieper River in Ukraine, these thumbnail-sized mussels have since invaded freshwater systems across the world, particularly in North America. Quagga form a dense mat of shells on river, lake and reservoir floors, taking up space that might be otherwise used by native species. They are big zooplankton eaters, which has a knock-on effect on the abundance and structure of food available to native fish. Quagga arrive in the UK with a costly reputation. Together with their close relative the Zebra mussel, they are known for clogging pipes, reducing water quality and fouling boats.  In the Great Lakes, quagga now blanket the bottom of Lakes Michigan and Huron, depriving creatures further up the food chain of vital nutrients. Lake Huron, once a fishing paradise, saw its salmon population crash after quagga took over and began competing with the smaller fish that salmon feed on. Even the Hoover Dam on the Colorado River has had to temporarily shut its turbines in order to clear out the mussels. Experience teaches us it is highly unlikely we can eradicate these species. But we have also learnt that there are many things we can do to reduce their effects and spread. Much of the reason that eradication of quagga mussel is unlikely is due to scale. If we have found them in one place they are likely to be in others. Scientists dealing with the early invasion of the zebra mussel in the Great Lakes of the US and Canada confirmed that eradication was unlikely once they realised the new population had already spawned several times before detection.  However, the Great Lakes are a huge interconnected waterbody and Britain has nothing of comparable size; there’s no need to give up just yet. If the UK has an opportunity to eradicate this current mussel invasion then action must be immediate and drastic. Given the scale of the eco-economic burden the quagga will bring once established, it is entirely reasonable to think outside our normal set of tools and apply extreme methods. There are emerging direct methods to try to kill off invasive mussels. In North America poisonous fertiliser and a new bacterial product are being trialled this year against zebra mussels. But some indirect methods may also work. In Scandinavia acid rain, caused by air pollution, and changes in soil chemistry following uplift have caused some coastal lakes to turn acidic, wiping out native mussel populations. Acute decreases in water pH as a management action will cause massive short term change in the biology of the invaded lakes. Fish become less fertile, some species may become extinct, and the makeup of local invertebrates and zooplankton will change. However it is possible that in low-pH water the quagga (and all other molluscs) will become locally extinct due to the effect of the acid on their shells. It’s a long shot. A second, perhaps more palatable, option is to manage fish populations to maximise their negative effects on quagga. The normal way to harvest fish involves taking only the largest fish down to some minimum size. An alternative idea is to remove those medium sized fish which prevent young fish from thriving, increasing their numbers so they can compete with and consume quagga. These are all untested ideas, but are based on observations of natural ecological dynamics (that is, acidification leading to extinction and competition leading to population decline). We largely know what causes the spread of non-native invasive aquatic species once they have arrived at our ports in the ballast of shipping vessels. Larvae have been found to hitch rides between lakes and rivers in fishing equipment – waders and fishing reels being particularly vulnerable – on boats and other water sports equipment.  It is vital that the UK authorities provide advice and equipment for recreational lake and river users in the south of England to wash their equipment as soon as they have finished at each site, and certainly before they switch to a new site. The realists will state clearly, and probably correctly, that the quagga is here to stay. Or at least that if such extreme measures for eradication above did work then it would only be a temporary measure until another invasion occurs, or another population that is already here is discovered. But given the huge economic cost of a full-scale quagga invasion, it would be better to have tried and failed than not to have tried at all."
"

Current image from Terra Satellite, rotated 90 degrees to improve view, plus annotation and world view inset added by Anthony
Source image is available here at the NASA Terra website
North Pole to remain frozen
By            Bill Scanlon, Rocky Mountain News
Originally published 02:57 p.m., August 29, 2008
Updated 02:57 p.m., August 29, 2008
Santa can rest easy.
It’s looking like the ice at the North Pole won’t melt to water next month, as had been feared. It would have been the first time in thousands of years that the most northerly place on the planet would have been ice-free.
“It’s quite unlikely at this point,” Walt Meier a research scientist at the University of Colorado’s National Snow and Ice Data Center, said today.
The ice in the Arctic Ocean is at near historic lows, and breaks records every couple of years due to human-caused global warming, the scientists at NSIDC say.
This spring, it was looking like the ice might retreat so far that the North Pole itself would be ice-free for at least a day in September – the height of the ice-melt season.
The chances were great enough that the scientists at NSIDC were laying almost even odds on it in an office pool.
But while global warming is playing an important role, seasonal variability does, too. And this summer turned out to be a little cooler than last summer, when the record for ice retreat was set, Meier said.
“We only have about two or three weeks more of ice melt, and it’s not going to make it to the North Pole,” Meier said.
Read the rest of the article here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d2f17a8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

“This is the age of maximal surveillance,” says Bruce Schneier — the so‐​called “security guru” who spoke at Cato’s Second Annual Surveillance Conference in October. Surveillance is now ubiquitous and virtually unescapable for those who wish to enjoy the conveniences of modern life. And while the government downplays the importance of its access to citizen’s metadata, as Schneier observed, this data is about us — everything about us. “Metadata reveals who we are,” he said. “Google knows more about me than I know — because Google remembers better.”



Throughout the day‐​long conference, experts from around the country discussed the perils of national and global surveillance, as well as prospects for encryption and other tools to protect privacy in an ever‐​changing technological landscape. Sen. Patrick Leahy (D-VT), a longtime proponent of surveillance reform in Congress, lauded the passage of the USA Freedom Act, a reform bill first introduced at Cato’s surveillance conference in 2013 and signed into law in 2015. He particularly praised Cato’s role as a consistent champion for privacy rights. “I want to thank the Cato Institute,” he said. “You worked very hard on this — when we had people starting to back away, you helped give them courage.”



 **Immigration from 1965 to 2015**  
Fifty years ago, President Lyndon B. Johnson signed the Immigration Act of 1965 — a defining component of the American legal immigration system. Its passage meant that, after years of discriminatory Progressive Era immigration policies, immigrants from Western Europe no longer had legal preference over immigrants from places like Asia and Eastern and Southern Europe. At the same time, however, the Act introduced new limitations on immigrants from countries like Mexico and Canada. To commemorate the anniversary of this law, Cato hosted a conference, “Fifty Years after Reform: The Successes, Failures, and Lessons from the Immigration Act of 1965.”



Rep. Ruben Gallego (D-AZ), the son of immigrants from Colombia and Mexico, opened the morning by praising immigration as an engine of economic growth. Jim Gilmore, a 2016 Republican presidential candidate and former governor of Virginia, warned that deporting all illegal immigrants would require turning America into a “police state.” And Bill Richardson, the former governor of New Mexico, called for expanding visas for high‐​skilled workers. “While American businesses try to adapt and grow amidst economic and technological revolutions, our outdated immigration policies are holding us back from our full potential,” he said.



 **Preparing for the UN’s Climate Change Conference**  
Just before November’s highly anticipated gathering of world leaders in Paris for the United Nations Climate Change Conference, Cato hosted its own conference: “Preparing for Paris: What to Expect from the U.N.‘s 2015 Climate Change Conference.” Speakers discussed what is at stake in Paris, where leaders will attempt to negotiate a climate change agreement; the potential legal implications of such an agreement; as well as the latest scientific developments in climate science. “There is increasing evidence that the threat from global warming is overstated,” said Judith Curry of the Georgia Institute of Technology.



She denounced the “stifling” of more moderate positions on the effects of climate change, saying that anyone who diverges even slightly from the Intergovernmental Panel on Climate Change consensus is considered a “denier.” Richard Tol of the University of Sussex, whom Cato’s Pat Michaels called the “preeminent environmental economist in the world,” delivered the keynote address, in which he predicted that not much will happen in Paris. “For the last 20 or 25 years, governments have tried to reduce greenhouse gas emissions,” he said — to little success. “We should be dismayed,” he said, that so much money has been wasted on these efforts.



 **Rethinking Monetary Policy**  
This year’s 33rd Annual Monetary Conference, attended by over 200 people, was the first hosted by Cato’s new Center for Monetary and Financial Alternatives — a center dedicated to moving monetary and financial regulatory policies toward a more rules‐​based, free‐​market system. The conference featured distinguished speakers like St. Louis Fed president James Bullard, Richmond Fed president Jeffrey Lacker, and Stanford economist John B. Taylor. Bullard gave the opening address, in which he argued that a stable interest rate peg is “a realistic theoretical possibility.”



Claudio Borio of the Bank for International Settlements, whom _The Economist_ has called “one of the world’s most provocative and interesting monetary economists,” proposed challenging some of the “deeply‐​held beliefs” of monetary policy, including the idea that monetary policy is “neutral,” or that deflations are always disastrous. Rep. Bill Huizenga (R‐​Mich.), who chairs the House Financial Services Subcommittee on Monetary Policy and Trade, discussed his legislation requiring the Fed to adopt an explicit policy rule, among other reforms. “The Fed ultimately must be accountable to the people’s representatives, as well as to the hard‐​working taxpayers themselves,” he said.



 **Evaluating the TTIP**  
“It’s been quite a year for trade policy,” Cato’s Dan Ikenson remarked at the beginning of Cato’s conference, “Will the Transatlantic Trade and Investment Partnership Live Up to Its Promise?” “I think we are about to embark on a robust debate in the United States about the TTIP.” Cato’s conference helped prime for that debate, featuring leading trade experts who analyzed TTIP’s status and geopolitical implications. The day opened with a keynote address from Shawn Donnan of the Financial Times, who predicted that the deal may prove difficult to get through the Obama administration. Despite the fact that negotiations began in 2013, he said, “A lot of the conversations feel like they’re just getting started.” The conference, which was broadcast on C-SPAN, continued with discussions of what is at stake in the negotiations — from GMO regulations, to labor and environmental standards, to intellectual property issues.



Speakers including Michelle Egan, a professor at the American University’s School of International Service, and Swedish economist Fredrik Erixon, succinctly explained some of the complex issues under negotiation, like standards‐​related trade barriers and TTIP’s effect on global trade policy. The participants also wrote essays on crucial aspects of TTIP, all of which are available online at www​.cato​.org.
"
"To take a look at the cars on your street, it might not be apparent that the automotive industry is going through one of its most dramatic periods of change. Yet rising purchasing power in emerging markets, tightening emission regulations and advances in electronics are driving a major shift in how vehicles are powered, constructed and driven.   Nothing will supersede the internal combustion engine but vehicles will increasingly resemble computers integrated with home applications, their surrounding infrastructure and each other. The line between car-maker and tech company will become increasingly blurred. By 2020, what a car looks like will depend very much on where you live. The biggest story in the next five years however is not an issue of technology but economics: hundreds of millions of people in emerging markets will buy cars, often for the first time. In 2009 China overtook the US as the world’s largest car market, and by 2016 India and Brazil will displace Japan in fourth and fifth place respectively. This is an enormous opportunity for manufacturers in emerging markets to acquire economies of scale and launch themselves on the world stage. Within a few years we are likely to see these new brands spread into developed economies. The signs are already there. Companies such as the Indian manufacturer Tata, which bought Jaguar-Land Rover, or China’s Geely, which now owns Volvo, are well placed to use these high-status brands and dealership networks to move into mature markets. This won’t just mean different brands on the same old cars either. Car makers in emerging markets have to deal with price-sensitive first-time consumers, urban congestion, poor air quality and (in some cases) concerns over fuels costs. This creates a fertile space for innovation in smaller vehicles, where China is already dominant, and alternative fuels where emerging economies stand to gain a technological lead. Recent launches of models such as the long-range, low-cost electric Denza model in China and the super-low cost e2o in India highlight this trend. The fact air pollution affects all nations, rich or poor, means there is real momentum behind carbon emission regulations. Yet for all the hype about fuel cells and electric cars, the first priority for car makers will not be to gamble on anything radically new but to make the best of what they’ve got. In practice, that means making the engine smaller, reducing aerodynamic and rolling drag and above all, making everything lighter. The recently launched BMW i3 shows how seriously this is being taken. Using sports car-style carbon-fibre materials for a (nominally) mass-market vehicle reduces weight enough to enable a large battery pack. Although the i3 is expected to have modest sales, it is widely seen as firing the starting pistol for suppliers and venture capitalists to invest in these more exotic materials.  Lighter materials, at cheaper costs are the key to both extending the life of large conventionally fuelled vehicles and unlocking the potential of battery electric propulsion. Plug-in and pure electric vehicles have proven most popular in wealthy suburban environments such as California or the Netherlands where people can charge cars in their driveway overnight. The relative ratio of electricity to fuel costs also plays a part: in Norway cheap power and tough “polluter pays” policies have seen the adoption of electric vehicles soar, whereas sales disappoint in Germany with its more expensive electricity supply. Towards the end of the decade we’ll reach a tipping point. The cost of batteries is anticipated to halve and energy density to almost double, reducing costs and improving driving distances to the point the when EVs become a much more affordable option. At the other end of the spectrum, compressed natural gas is an option for long-distance travel. Long used in Asia and South America, CNG emits less CO2 than petrol engines and fewer harmful pollutants than diesel and can be filled either quickly or slowly depending on the purpose of the filling station. Hydrogen fuel cells represent perhaps the greatest hope (and gamble) for the industry. Like petrol engines they are quick to fuel and carry a lot of energy by volume which makes them suited for long-range driving – all this while emitting only water-vapour.  Yet despite tumbling costs fuel cell vehicles are currently the most expensive on the market and the fuelling infrastructure is costly to establish. Hyundai and Toyota have launched models targeted at the Japanese and California markets and other manufacturers are eagerly watching. If these cars achieve even modest sales, other brands are sure to invest strongly in the technology (if they are not doing so already). The launch of BMW’s DriveNow car-sharing scheme in London highlights another trend, the growth of the connected car and “pure mobility” services as an alternative to car ownership. In dense urban centres, smartphones have enabled car fleet operators to devise ever more sophisticated services. Uber’s cab services or car sharing schemes such as Daimler’s Car2Go or Autolib’ in Paris provide an alternative to costly car-ownership and limited on-street parking. For now, this is largely confined to large, rich cities and compared to public transport the impact is minimal. Yet continued smartphone penetration should see these models adopted in emerging markets too and especially in Russia. By 2020 we may see this process start to step up, as Nissan and Google vie to be the first to commercialise autonomous vehicles. Although likely to be relatively slow moving and still limited to big cities, autonomous vehicles would revolutionise the efficiency, safety and attractiveness of car sharing. The increasingly computerisation of vehicles also points towards a perhaps more significant, if unseen trend in vehicle development. Today, change across the vehicle fleet takes a decade or more, mediated by the development and production cycles of car-makers and the churn of the accumulated fleet. Yet integration with cloud-based services means that cars are no-longer a closed system, but capable of being remotely monitored and upgraded. Tesla has made a virtue of providing real-time vehicle upgrades to improve performance and India’s Mahindra & Mahindra has adopted the slogan of “fixing your problem, before you notice it”.  As automotive manufacturers start to converge with consumer electronics and cloud-based services development cycles are contracting with BMW speaking of four months from idea to production for new applications. This also poses the question of whether car-makers will even remain the chief players in auto-mobility. In the future might we prefer to buy our transport services from Google? The ground-work is being laid now and by 2020 we may start to see an answer."
"It has long been news that overfishing persists in many of the world’s oceans. Fish and invertebrate stocks have been over-exploited for our ever-hungry, growing human population, leaving some species in dangerous decline. The establishment of marine protected areas (MPAs) across the globe has been hailed as the silver bullet for conservation, with reports of increased catch, and spillover of recovered populations into adjacent fisheries, helping to replenish overfished stocks. 
But there may be unintended consequences if these areas are left unchecked. As populations of certain species are restored, disease can increase too.  Lundy Island, off the coast of Devon, was the UK’s first MPA. It was established as a marine nature reserve in 1986, incorporated a no take zone in 2003 and was designated a marine conservation zone in 2010.  Four years of monitoring from 2003 to 2007 saw a marked increase in commonly fished species, such as lobster, inside the no take zone when compared to fished areas. But in 2010, a study of Lundy called for a cost-benefits review of marine reserves, after it was found that shell disease in European lobsters may be increasing inside the protected area, supposedly caused by the high density of certain species.  We returned to Lundy the following year to monitor the populations of European lobster. When we compared a fished area to the eight-year-old, unfished, no take zone, we found more abundant, and larger lobsters inside the no take zone This phenomenon is a well known upshot of establishing MPAs and one of the reasons they are celebrated. Local fishermen agreed that since the no take zone was implemented, there has been an increase in catch around the area.  But in the same survey, we found that there was a higher probability of lobsters being injured inside the Lundy no take zone. Injury is thought to be induced by the European lobsters’ aggressive and solitary nature, so naturally in areas of high density such as the no take zone we expected to find a lot. Still, injury is known to be a precursor to disease. The shell of a lobster is its first line of defence and once breached, this may give rise to entry of pathogens.  This is crucial to understand because other studies have shown that pathogens in marine ecosystems are on the rise, a phenomenon which may be exacerbated by climate change.  In the past, disease in American lobsters is thought to have contributed to the collapse of a lobster fishery in southern Massachusetts. It is important to monitor disease and understand the effects on populations elsewhere in the world, especially those species which are commercially exploited. Our study is interesting in that it introduces the idea that un-fished populations in marine parks may eventually reach a threshold at which conditions become unhealthy. This may even introduce the possibility of controlled fishing in long-standing no take zones.  This may be a controversial move but studies have shown high abundance in marine reserves may render animals vulnerable to disease particularly because infections can no longer be “fished out”. A total ban on fishing is certainly positive in allowing recovery of populations back to unexploited densities, but they may have a finite time span of success. There is no doubt that fishery closures and marine protected areas do help contribute to the conservation of species, but the important message here is that we must monitor them closely. In November 2013, the UK designated 27 new MPA sites. Monitoring species richness, abundance and disease in these areas will be crucial to avoid any unwanted byproducts such as disease increase."
"
Share this...FacebookTwitterWe want to be fair, and so I’m obligated to inform readers that Professor Stefan Rahmstorf, has testily replied to EIKE’s report, which I wrote about just 2 days ago. Rahmstorf retorts at his website: Headlines From Absurdistan here.
First, recall that Rahmstorf is that alarmist scientist who projects an oddball sea level rise of 1.4 meters over the next 90 years, something that almost all scientists dismiss as nonsense (Rahmstorf may not be aware that sea level rise has indeed been slowing down; a few years ago the rate was 3.4 mm per year). Read more here.
Sea level rise is slowing. TOPEX U. of Colorado
Rahmstorf appears to be miffed that EIKE has labelled him as a lowly sceptic. Let us look at what he has written in reply.
1. First off, we’re all bought off. In his retort he first whines about all the money that big business is pouring into the sceptic machinery and to US denier politicians. (I haven’t seen a cent of it). Rahmstorf writes:
Also European companies don’t pinch their pennies when it comes to buying up candidates for the US Senate who deny anthropogenic climate change.
2. Next he points out that the focus of his 2003 paper was an analysis of  ice core data from Greenland with respect to the timing of climate changes during the ice age only (Dansgaard-Oeschger events).
3. He then claims that EIKE is confusing local with global temperature fluctuations. Rahmstorf writes:
The Greenland data are mainly characterised by fluctuations in the Atlantic currents (the Dansgaard-Oeschger events), which practically have no impact on the global temperature.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




4. In the Holocene in Greenland, like the northern latitudes overall, there was a cooling trend (it was warmer earlier), which was caused by regional solar radiation due to orbital cycles – not a global phenomena.
5. He counters the sceptic argument that past warmings show that today’s warming is natural using the following analogy:
It is as logical as saying that there cannot be forest arson today because there were completely natural forest fires in the past, too.
Of course there were warmer climates in the past than today (but as far as we know, not in the last 2 million years).
Today is the hottest it’s been in the last 2 million years! That’s quite the claim. Rahmstorf then poses the question:
Doesn’t the internet report [by EIKE] simply confirm the wisdom that the deniers of anthropogenic climate change no longer have any arguments – and so they have to resort to the most absurd possible twisting of the facts?
Finally, Rahmstorf takes one last shot at EIKE:
The source of all this nonsense, by the way, is the EIKE “climate sceptic” lobby group with the cute name of “European Institute for Climate and Energy”. For more information read Süddeutsche or the Spiegel.
Needless to say, Rahmstorf cites two lefty sources that don’t exactly say the nicest things about EIKE, calling it a hack organisation that’s headquartered in a mailbox. Funny though how Rahmstorf takes EIKE so seriously that he feels compelled to reply to the “mailbox operation” in less than 72 hours. Must be quite the mailbox.
Share this...FacebookTwitter "
"The Antarctic has registered a temperature of more than 20C (68F) for the first time on record, prompting fears of climate instability in the world’s greatest repository of ice. The 20.75C logged by Brazilian scientists at Seymour Island on 9 February was almost a full degree higher than the previous record of 19.8C, taken on Signy Island in January 1982. It follows another recent temperature record: on 6 February an Argentinian research station at Esperanza measured 18.3C, which was the highest reading on the continental Antarctic peninsula. These records will need to be confirmed by the World Meteorological Organization, but they are consistent with a broader trend on the peninsula and nearby islands, which have warmed by almost 3C since the pre-industrial era – one of the fastest rates on the planet. Scientists, who collect the data from remote monitoring stations every three days, described the new record as “incredible and abnormal”. “We are seeing the warming trend in many of the sites we are monitoring, but we have never seen anything like this,” said Carlos Schaefer, who works on Terrantar, a Brazilian government project that monitors the impact of climate change on permafrost and biology at 23 sites in the Antarctic. Schaefer said the temperature of the peninsula, the South Shetland Islands and the James Ross archipelago, which Seymour is part of, has been erratic over the past 20 years. After cooling in the first decade of this century, it has warmed rapidly. Scientists on the Brazilian antarctic programme say this appears to be influenced by shifts in ocean currents and El Niño events: “We have climatic changes in the atmosphere, which is closely related to changes in permafrost and the ocean. The whole thing is very interrelated.” The impacts vary across Antarctica, which encompasses the land, islands and ocean south of 60 degrees latitude. This region stores about 70% of the world’s fresh water in the form of snow and ice. If it were all to melt, sea levels would rise by 50 to 60 metres, but that will take many generations. UN scientists predict oceans will be between 30cm and 110cm higher by the end of this century, depending on human efforts to reduce emissions and the sensitivity of ice sheets. While temperatures in eastern and central Antarctica are relatively stable, there are growing concerns about west Antarctica, where warming oceans are undermining the huge Thwaites and Pine Island glaciers. Until now, this has led to a relatively low amount of sea-level rise, but this could change rapidly if there is a sustained jump in temperature. The Antarctic peninsula – the long finger of land that stretches towards Argentina – is most dramatically affected. On a recent trip with Greenpeace, the Guardian saw glaciers that have retreated by more than 100 metres in Discovery Bay and large swathes of land on King George Island where the snow melted in little more than a week, leaving dark exposed rock. While some degree of melt occurs every summer, scientists said it had been more evident in recent years, with temperatures rising more quickly in winter. This is believed to be behind an alarming decline of more than 50% in chinstrap penguin colonies, which are dependent on sea ice. Schaefer said monitoring data from these areas could indicate what is in store for other parts of the region. “It is important to have sentinel areas like the South Shetlands and the Antarctic peninsula because they can anticipate the developments that will happen in the future, the near future,” he said."
"
Share this...FacebookTwitterYesterday I just happened to be listening to German NDR news radio (during my after-lunch snooze) and heard an interview with meteogroup’s Frank Abel, one of Germany’s better known meteorologists.Regrettably I can’t post the German interview audio due to copyright reasons. But you can get an mp3 audio clip e-mailed to you by calling Frau Renate Genz-Kreher, at the Hamburg studios, Tel.: (+49) 40/4156-2788. Just pick up the phone – I’m sure she speaks pretty good English
The NDR newsman starts the interview with a list of dramatic weather events that have occurred in northern Germany recently, then questions Abel if these events were something we’d have to get used to. Abel answered (paraphrased):

It’s really to early to say. Such events have happened in the past. A couple of months ago Austrian experts cautioned against premature conclusions on climate change and rainfall. It is quite certain that global temperatures are rising now but it is unsure how this impacts rainfall. We have to be careful not to spread too much certainty on the topic.
In a nutshell, all these reports claiming climate change leads to more frequent extremes are premature. There’s too much uncertainty out there.
The newsman then asks why northern Germany has had such lousy weather lately. Abel explains that it has to do with so-called 5B Lows. These are Lows that form in the Alps and move up to the Baltic Sea, which leads to warm moist air from southeast Europe colliding with much cooler air and thus results in heavy deluges. The same phenomenon happened in 2002, and in Poland in 1997 – all caused by so-called 5B Lows.
Of course the newsman doesn’t really care about that. He wants to here that it’s due to global warming. So he presses Abel.
Also for us here in North Germany it feels like it has rained harder than ever before. What has come down over the last few weeks is just unbelievable. Can you, as a weather expert, confirm this?
Abel agrees that it has rained a lot, and that August was one of the wettest months on the records, and then explains some of the geographic factors and goes into the current high water situation that people in some regions have to deal with right now. But he doesn’t deliver the goods. So the newsman persists:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So is it the changes in climate that are behind these heavy rains?
Abel refuses to say yes.
The situation is that nobody is really certain, and you have to be very careful…
The newsman interrupts:
Yes, but what do you think?
Frank Abel:
I can’t say. If you look back at the past, you’ll certainly find years and phases where you had similar conditions and people said ‘this has never happened”‘ Man has a relatively short memory for these things. Many have forgotten how we had similarly wet summers at the end of the 1970s.
It really sucks when the interviewee doesn’t cooperate. Next time the newsman will know better and invite a more reliable expert – like Prof Mojib Latif, who we now know will say anything. Damn meteorologists!
Don’t get me wrong, I don’t know what Abel’s position is on global warming…I’d say he’s a warmist, but not an alarmist. You’d have to ask him yourself (privately, off the record). He aint no Joe Bastardi – pretty sure about that.
I live in northern Germany and what do I think of the current weather? It was one of the shortest summers I can remember my 20 years here. We had about 3 weeks of hot weather from end of June to middle of July, and that was it. After that it was cool and rainy. This September has been really cool, with very few days getting up over 20°C. It’s rained a lot – but that’s what it does in northern Germany. That’s nothing new.
Share this...FacebookTwitter "
"
Now I’ve heard everything. Talk about your “Kyoto protocol”. The original source of this silliness comes from the city of Kyoto. In June, in a bid to reduce greenhouse gases and perhaps become a nationally  designated “model environmental city,” the municipal government indicated it would request convenience stores to “voluntarily refrain” from staying open all night.
No Slushee for you!
You can read the complete story here in Japan Today. The worst part about this is the complete lack of understanding about where the major energy use is. Closing the store may result in some energy savings from lighting, but the main power use, refrigeration systems, and that Slushee machine, will still operate.
No more midnight Slushee! Maybe the real reason is the “exploitation of the polar bear” on the cup.
Here is more, a response from the  Japan Franchise Association
Convenience stores defend  24-hour operations
September 27th, 2008 by Jame, Japanprobe.com

Facing attack from critics that want convenience stores to shut down at night  as a measure to prevent global warming, the Japan Franchise Association has  responded by stating that convenience stores play a crucial  role as safe havens for lost children and victims of crime:
More than 13,000 cases of women finding refuge in convenience stores across  the country were reported during fiscal 2007. Nearly half of them occurred after  11 p.m. and about 40 percent were due to stalkers and molesters, the association  said.
In addition, there were 6,000 cases of lost children requiring assistance and  12,000 cases of elderly people found wandering the streets alone.
The 12 companies that comprise the JFA operate around 42,000 convenience  stores.
Explaining the significance of convenience stores, a JFA official said they  provide a “substitute for ‘koban’ (police boxes) and streetlights in the middle  of the night.”
The National Police Agency says that koban and “hashutsujo” police branch  offices are located at about 13,000 places across the country, but that number  is down by around 1,000 from five years earlier.
In addition, the JFA has also stated that convenience stores with limited  nighttime hours would still have to keep on their refrigeration systems when  closed, so the reduction in greenhouse gas emissions would be  negligible.
From the Japan Times article:
Behind moves to limit 24-hour business is concern about the environmental impact of round-the-clock operations. “Definitely, 24-hour operations eat up electricity,” he said.
Although acknowledging that some people are active late at night, for example because of their jobs, Ando went on to claim “the vast majority have standard lifestyles and get up in the morning and come home from school or work and sleep at night.”
“With no time left to waste to combat global warming, we are very concerned about whether it is really good (that stores) stay lit up even past midnight,” Ando said.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c80c691',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"There aren’t many animals high up in the Himalayas, but the odd bird passes by. Each year bar-headed geese (Anser indicus) travel from their breeding grounds in China and Mongolia to spend winter in the Indian subcontinent. It’s a journey that takes them through some of the tallest mountains in the world and there are even old reports of these geese flying over Annapurna and even once over Mount Everest. We knew these birds were well adapted to flying at lofty altitudes but until now we assumed they hit some chosen top altitude and flipped on cruise control. However, new research published in the journal Science by myself and an international team of scientists found otherwise. After tracking the geese we discovered they actually stick close to the mountains, following the peaks and valleys like a rollercoaster. It’s an unexpected approach to a tricky problem. At higher altitudes, air gradually becomes less dense and the atmospheric pressure drops. This means birds must flap their wings ever faster, deeper or both in order to generate the lift and thrust required to keep them flying fast through the air.  At the same time, the availability of the oxygen required to sustain these flights is reduced. At 5500m – just above Everest Base Camp – a lung full of air will contain around half the oxygen obtained at sea level. Herons, egrets and other birds that fly high above the ground, but not necessarily over mountains, often climb steadily and then level off at a specific altitude (typically between 2,000 and 7000m) when they find suitable tailwinds which help propel them along at high speeds.  Bar-headed geese could follow a similar strategy. After all, their lungs are 25% larger than other geese and their cardiac and flight muscles contain more blood vessels – all ideal adaptations for a low-oxygen environment. Their blood even contains slightly modified haemoglobin that gives it a higher affinity for oxygen than many other birds.  However, when we tracked these geese we found they seldom maintained a constant altitude at all. They might descend many hundreds of meters only to have to climb again later in the same flight. The birds mostly stayed within 100m or so of the undulating terrain and only flew as high as was necessary. Due to the many ups and downs recorded on longer flights, we termed this the “rollercoaster” strategy. To understand how and why bar-headed geese made these flights we developed a customised data logger which we implanted into their abdomens (after catching them in Mongolia) where it stayed all year. We put a coloured ring around their necks so that we could identify them the following year.  The logger would record flight altitudes, body temperature, heart rate and body motions (used to determine wingbeat frequency and overall accelerations). Both heart rate and wingbeats went up with increasing altitude and decreasing air density, indicating that at high altitudes even horizontal flight was hard work.  Using this data we calculated that, without a helpful tailwind, staying at high altitude would not be worth the extra flapping. For the geese the most efficient route is to track the underlying terrain and stay in reasonably dense air. This helps to reduce the energy used up flying while at the same time making more oxygen available.  Extra flapping uses a great deal of energy. Even just a 5% increase in flap frequency results in an average 19% increase in heart rate and a 40% increase in oxygen consumption. This is because, unusually, bar-headed geese increase the size of their flaps along with their frequency.  Given we know what the air pressure at any given altitude is likely to be, we can calculate the flaps necessary for the goose to stay airborne. However the increasingly extreme extra effort involved in eking out those extra flaps means even this species will eventually hit its ceiling. Our calculations say “peak goose” could be somewhere nearer 8,000m than 9,000m (perhaps depending on the weather), they simply don’t have the heart or the lungs to keep flapping beyond this point. So the Mount Everest (8848m) sightings might just about be plausible, perhaps with assistance from updrafts deflected from the valleys, but seem unlikely. I would love to hear from anyone with some good evidence of geese at these altitudes. I would certainly not expect them to stay there for very long! What is clear is that these birds have evolved to keep their flight costs to a minimum by regularly returning to lower altitudes whenever possible and only flying as high as the terrain dictates. Bar-headed geese are exceptionally well adapted to flying at high altitude when necessary, but they chose to make it the exception rather than the rule."
"Europe’s largest asset manager is backing a shareholder vote urging Barclays to stop offering loans to fossil fuel companies. Amundi, an influential investor with more than €1.65tn (£1.4tn) in assets under management, is the latest shareholder to throw its weight behind a resolution calling on Barclays to phase out services to energy companies that fail to align with Paris climate goals.  It comes amid rising concerns over Barclays’ role as Europe’s largest financier of fossil fuel companies. The resolution, spearheaded by campaign group ShareAction, was filed in January by a group of 11 pension and investment funds managing more than £130bn worth of assets. The Church of England has also thrown its weight behind the climate resolution, which is the first to be lodged against a UK bank. A spokesperson for Amundi, whose headquarters are in Paris, told the Guardian that the firm would vote in favour of the resolution at Barclays’ annual investor meeting in May. “According to its voting policy for 2020, Amundi reiterates its priority on the question of the energy transition and the decarbonisation of the economy. And as such, we are favourable to any resolutions from shareholders that ask issuers to be more transparent around environmental and social factors. Therefore, the resolution of ShareAction is perfectly in line with our policy,” the spokesperson said. While Amundi holds a minor stake in Barclays of about 0.02%, its decision to publicly support the resolution could put pressure on other asset managers to follow suit. Some of Barclays’ largest shareholders, including BlackRock, have refused to disclose whether they will vote in favour of the resolution. That is despite BlackRock’s chairman and chief executive, Larry Fink, having pledged last month to strengthen the asset management firm’s “commitment to sustainability and transparency in our investment stewardship activities”. Instead, BlackRock plans to disclose key votes after they are cast. Legal & General Investment Management, which holds a 2% stake in Barclays, has also declined to comment on its voting plans. ShareAction’s campaign manager, Jeanne Martin, welcomed Amundi’s support. “It’s further proof that the most powerful investors are dissatisfied with Barclays’ financing of extreme fossil fuels such as coal, tar sands and fracking, and failure to keep pace with its competitors,” she said. A recent study commissioned by campaign groups, including the Rainforest Action Network, singled out Barclays as the largest financier of fossil fuels in Europe, with lending and underwriting to carbon-intensive companies and projects totalling $85bn (£64bn) between 2015 and 2018. Barclays is expected to disclose its position on the resolution in March. A spokesperson said: “We continue to engage with ShareAction and other stakeholders on this issue and will make a further statement at the appropriate time.”"
"
I’m sure we’ll see other dead people talking about climate change soon. Jerry Garcia perhaps? Albert Einstein, John Lennon, why maybe even John Wayne could be utilized. I can see him now, “Listen up Pilgrim…climate change is gonna kill you unless you get off your sorry butt and do something about it!”.
We all know these long dead people had opinions about climate change, but they just never had a chance to express them before they died. Right?
Anything to “save the planet”, including putting words in dead people’s mouths. – Anthony
h/t to Jeff Alberts
Greenpeace Resurrects JFK for Global Warming Ad Campaign
Web video depicts dead president warning climate change ‘threatens our very existence,’ claims ‘technology and renewable energy offers the last remaining hope.’
By   Jeff Poor
Business & Media Institute
10/30/2008 1:31:03 PM
There’s something a little creepy about historical figures being brought back to life to promote climate change alarmism, but the over-the-top environmentalists at Greenpeace have no qualms with using it as a tactic.
A video posted on Greenpeace’s YouTube site portrays former President John F. Kennedy, who was assassinated Nov. 22, 1963, in Dallas, making a plea for environmental activism to save the planet from the perils of global warming.
“When man first walked upon the moon, it defined a generation,” Kennedy is depicted saying. “As this new millennium dawns, we face a greater challenge – climate change threatens our very existence. What further disasters will convince world leaders that the existing technology and renewable energy offers the last remaining hope for sustainable future?”
The ad is part of a Greenpeace campaign labeled “Energy [R]evolution” that sets greenhouse gas goals far in excess of the Kyoto treaty. The ad promotes the eradication of coal-fired plants, using more expensive unproven sources of energy, blames industrialized nations for the plight of poor nations, and calls for a radical overhaul of the European auto industry.
Watch the video:

Other aspects of the campaign include an energy policy based on “equity and fairness,” reduction of greenhouse gas emission by “up to 30 percent below 1990 levels,” “strict mandatory efficiency standards” for home and office appliances and the phasing out of nuclear power, which has been deemed as a greenhouse gas-free energy solution to the climate change issue by some.
The ad makes an emotional plea by linking climate change disaster scene after disaster scene, including a post-Hurricane Katrina shot from Reynoir St. in Biloxi, Miss, followed by shots of children, calling for an “energy revolution.”
“Hollow words and spineless resolution have failed,” the voice continued. “Now is the time for an energy revolution. Will we look into the eyes of our children and tell them that we had the opportunity but lacked the courage? Will we look into the eyes of our children and tell them that we had the technology, but lacked the vision? Or, will we look into the eyes of our children and tell them that we faced our challenge and that we fought – we fought for the energy revolution?”
Kennedy is considered by some historians to be a great orator. However, the choice to use his likeness in the ad is curious because, although he is credited for laying some the foundation of modern federal environmental policy, it was his Republican rival and successor, President Richard Nixon, who made the Environmental Protection Agency a reality in 1970.
This isn’t the first time environmental messages have been linked to historical images. Al Gore’s “We Can Solve It” campaign had one commercial spot that began with video from the D-Day invasion of Normandy and included clips from the moon landing and the civil rights movement. Time magazine doctored the famous Iwo Jima photograph by Joe Rosenthal of the Marines raising the American flag and replaced the flag with a tree to push the “war on global warming.”
The use of deceased celebrities in advertising has been considered controversial by some. A recent DirectTV ad starring Craig T. Nelson as the father in the 1982 movie “Poltergeist,” shows the daughter – played by Heather O’Rourke – reciting the movie’s memorable line, “Theeeyyy’re heeerrre” However, O’Rourke died in 1988 and some critics claimed that crossed the line.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b67d40d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Many people that have have an interest in the interaction between the Sun and Earth have been keeping a watchful eye on several metrics of solar activity recently. The most popular of course has been sunspot watching.
The sun has been particularly quiet in the last several months, so quiet in fact that Australia’s space weather agency recently revised their solar cycle 24 forecast, pushing the expected date for a ramping up of cycle 24 sunspots into the future by six months.
On August 31st, at 23:59 UTC, just a little over 24 hours from now, we are very likely to make a bit of history. It looks like we will have gone an entire calendar month without a sunspot. According to data from NOAA’s National Geophysical Data Center, the last time that happened was in June of 1913. May of 1913 was also spotless.
With the current space weather activity level of the Sun being near zero, and the SOHO holographic imaging of the far side of the sun showing no developing spots that would come around the edge in the next 24 hours, it seems a safe bet to conclude that August 2008 will be the first spotless month since June 1913.
Here is the sun today,  at 09:14UTC August 30th:

Click for a very large image
Some people who watch the sun regularly might argue that August wasn’t really spotless, because on August 21st, a very tiny plage area looked like it was going to become a countable sunspot. Here is an amateur astronomer’s photo of the event:

August 21st, 2008 spots – Photo: Pavol Rapavy
But according to solar physicist Leif Svalgaard, who regularly frequents this blog:
According to NOAA it was not assigned a number on Aug.21st nor on Aug.22.
So without an official recognition or a number assigned, it should not be counted in August as actual sunspot.
It has also been over a month since a countable sunspot has been observed, the last one being on July 18th. Since then, activity has been flat. Below is a graph of several solar metrics from the amateur radio propagation website dxlc.com for the past two months:

Click image for original source
They have a table of metrics that include sunspots, and their data also points to a spotless August 2008. See it here: http://www.dxlc.com/solar/indices.html
So unless something dramatic happens on the sun in the next 24 hours, it seems a safe bet that August 2008 will be a spotless month.
Update: As commenter Jim Powell points out,
There was a stretch of 42 spotless days from 9/13/1996 to 10/24/1996. Today we have equaled this period. Check out Jan Janssens spotless days page http://users.telenet.be/j.janssens/Spotless/Spotless.html.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cd73fd2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

My youngest daughter, Abby, is graduating from college in a month. So like most university seniors, she’s been in the throes of a job search. After an interview in Boston two Fridays ago, she was sitting at a gate in Logan Airport waiting for a flight to D.C. (She still likes to visit her parents!) When the aircraft arrives, who should alight from it but Elizabeth Warren. As she walks through the gate area, the waiting passengers proceed to stand up and give her an ovation, calling out, “Thank you, Senator!” and “We love you, Senator!” Oh, boy. I’m proud to say Abby’s voice was dripping with disdain when she told me this story. And, no, she didn’t join in the ovation.



How I wish people — of all political and ideological persuasions — wouldn’t have such misplaced faith in politicians. One of the great lessons I learned from Cato, in my many years as a donor, is that it’s the power and advancement of ideas that will create positive change in our world and build a free and prosperous society. The outcome of elections and the machinations of politicians alone won’t do it. And so much of our politics is partisan tribalism: both Democrats and Republicans support elected officials of their respective party even when they abdicate on issues that would appear core to that party.



I have plenty of Republican friends who defended George Bush for years despite the out‐​of‐​control spending and growth of government under his administration, including initiatives that would have left them outraged had a Democratic president been responsible. Steel tariffs, No Child Left Behind, Medicare Part D, and TARP are just a few examples.



And where are the angry Democrats protesting the sorry civil liberties record of the Obama administration? PATRIOT Act abuses such as national security letters and warrantless eavesdropping or data collection got them exercised when Bush was president — today, their silence is deafening. And partisans on each side seem to believe in “executive power for me, but not for thee.…” There have been frightening grabs of presidential power under each of the last two administrations. This has elicited complaints from both sides of the aisle: but, with few exceptions, from the left only of Bush, and from the right only of Obama.



As believers in markets, we know people respond to incentives. If partisan voters don’t insist that the people whom they elect adhere to principle, why would they? As a result, no matter which party holds power, the results are similar: too much spending, too much regulation, an unbridled Federal Reserve, a bias toward military intervention, and too little respect for civil liberties. In fact, a friend of Cato’s once shared a brilliant analogy. During campaigns, we hear from the marketing departments of each party, and they sound very different. But with a handful of key exceptions we get similar results when they’re in power: he speculated that they must each outsource to the same place when governing!



And let’s not have too much faith that simply pitching bad leaders overboard will change things dramatically for the better. Last summer, we were paid a visit here at Cato by Kim Kataguiri, an impressive and courageous 20‐​year‐​old Brazilian who has catalyzed the protests against President Dilma Rousseff and the drive for her impeachment. It will be wonderful to see a corrupt and ineffective leader get the comeuppance she deserves. But the vice president faces corruption charges, too, so his ascension wouldn’t likely change much. Rather, it will take a change of values to transform Brazil and allow it to reach its potential.



Politics is ultimately a necessary ingredient for the world to move in the direction we want. But a country and a world steeped in liberty can’t be accomplished politically without changing the terms of the debate and the climate of ideas: precisely Cato’s role. Scott Rasmussen once spoke at a Cato event, and contended that politicians only follow — and don’t lead — the rest of the country. It is the very contempt in which citizens hold the political class that made him optimistic about the future despite the current policy environment. It’s our job to continue making the compelling case for freedom through the media, in the academy, and to the policy community. Our objective is to lead policymakers in the direction of liberty. Only when they get there will they deserve ovations.
"
"
Share this...FacebookTwitterPia Heinemann reports in Die Welt today, Ocean Acidification Does Not Lead To Species Die-Off, on a new study appearing in the latest edition of Science. The study contradicts the assumption that ocean acidification leads to species die-off, surprising scientists.Abstract in Science here
Manmade emissions of CO2 are thought to be partly absorbed by the oceans, which in turn would acidify and pose a huge threat to calcareous organisms like corals and plankton. This is the horror story that has been widely circulating in the media for the last couple of years, and with ever-growing alarmism, at a time the dangers of global warming are turning out to be wildly exaggerated.
Italian and Swiss scientists have found answers by looking at 120 million year old sediment deposits. The team directed by Elisabetti Erba of the University of Milan describes new findings in the latest issue of Science.
It is not unusual for CO2 concentrations in the atmosphere to surge after large volcanic eruptions. This has happened often in the past.
They examined microscopic fossils and nannoplankton from a time period just after large volcanic eruptions 120 million years ago, when the air’s CO2 content rose to about twice today’s level. Their studies contradicted their expectations. Die Welt writes:
Contrary to what was expected, no large-scale die-offs occurred among the organisms when acidification increased. The species simply adapted: They formed smaller shells and remained small.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




They endured the changes far better than first thought.
Heinemann writes that the study also delivered yet another surprise:
Apparently, the oceans acidify with a delay. After the volcanoes erupted and the surface water pH value began to sink, it took 25,000 to 30,000 years longer for the CO2 effect to reach the sea bottom.
These new findings deal a massive blow to those hoping to exploit ocean acidification as the next disaster scenario to replace the discredited catastrophic AGW story. Expect the MSM to bury or spin the story.
Update/Note: Keep in mind that the plankton and coral studied were from 120 million years ago, meaning the species has since survived climate extremes and changes that were off the charts when compared to today’s mild natural changes. They’ve handled much colder and much warmer conditions with widely varying ocean chemistry.
Update 2: The Alfred Wegener Institute in Germany [Read here] is planning years of research on acidification, costing millions of euros, to study a bogus non-problem. They’ve teamed up with neutral Greenpeace, and so you can be sure they’ll come up with “catastrophic” findings and demand more money for reasearch. Whatever it takes to bilk the taxpayer out of money.
Share this...FacebookTwitter "
"
You may remember last month I posted adjusted numbers for WUWT due to a SPAM attack, dropping my September count by about 50K:

SEPTEMBER NUMBERS Click for full sized image
For September 2008 the total was 846,193 page views, up from 667,215 page views in August 2008.
But there is a caveat, I think the real numbers are just shy of 800,000, because on the weekend of 09/20 and 09/21 I got quite a bit of unexpected traffic to one old post that I’m not sure is real or not. During that time, we got a lot of Spam on that particular older entry comparing UAH, RSS, HadCRUT, and GISS, but not anywhere near the numbers specific to that post, shown below:
Blog Stats Increase due to DOS “something”

Saturday 09/20 23,486
Sunday   09/21 25,319
Monday  09/22   1,006
Total: 49,811
Raw WUWT September numbers:           846,193 page views
WUWT Spam Uncertainty numbers:           -49,811 page views (from 09/20 to 09/22)
Final Adjusted WUWT September numbers: 796,382 page views
Today at 5PM (00UTC) I tallied my October numbers, and while they are slightly lower than last month’s total, I didn’t have any perceptible spam attacks like last month, so I don’t need to adjust October’s numbers:

OCTOBER NUMBERS Click for larger image
At 826,633 page views for October, that would put me up slightly from last month’s adjusted numbers of 796,382, but there was also an extra day involved (September hath 30 days). Since that difference is close to my daily average, we’ll just call it “no change” for October.
Of course many of you are either hoping for “change” or “no change” in November. 😉
Thanks again to all of you, and especially those whom have donated recently to help keep this effort going. My UHI test in Reno was funded in part through your donations. Another trip this weekend to retrieve two dataloggers that have been out for 45 days will also be assisted by those same donations. Again my thanks.
Have a wonderful Halloween!- Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b32455e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

When a business isn’t making money, it usually tries to market new products and attract new investors. When a bureaucracy’s reason for existence is threatened, it typically generates new missions. One difference between businesses and bureaucracies, though, is that businesses have to satisfy customers and investors. Because bureaucracies are under no such constraint, they require more oversight. 



The most recent evidence for this is _Environmental Diplomacy_ , the State Department’s first annual report on the environment and foreign policy. According to the report, the end of the Cold War has reduced the demand for the agency’s traditional tasks and given it time to ponder other threats to U.S. vital interests. And what do you know? The world is filled with environmental crises that the State Department must solve.



What are the environmental problems that warrant the attention of the State Department? According to the report, they fall into five areas: climate change, toxic chemicals, species extinction, deforestation and marine degradation. In each area, terrifying scenarios abound that necessitate reliance on the State Department to save the day. For example, “Forests four times larger than Switzerland are lost every year. 70% of the world’s marine fish stocks are fully to over‐​exploited. The people of the world annually release 23 billion tons of CO2 into the air … The range of impacts [from CO2 release] is likely to include: threats to human health including increases in heat‐​related deaths and illnesses, and in the incidence of infectious diseases.… There is no way to estimate the potential benefits that may come from millions of species yet to be studied, or yet to be discovered. And there is no way to estimate the health, economic, and spiritual costs to our children who could inherit a world robbed of a drug to cure AIDS, stripped of a strain of disease‐​free wheat, or bereft of the wonder of such diverse creatures as tigers and sea turtles.”



Let’s avoid speculating on the desperation that led the State Department to produce a post‐​Cold War mission that required the invocation of children, AIDS, wheat, sea turtles and tigers all in the same sentence. Instead, consider the paucity of evidence for the existence of those problems. Take, for instance, the panic over global warming. Observed warming since the late 19th century is only .5 degrees centigrade rather than the 1990 prediction of the United Nations’ climate change panel of 1.3 to 2.3 degrees centigrade, according to Patrick Michaels, professor of environmental sciences at the University of Virginia. Greenhouse physics predicts that the driest air masses — those in the polar regions during the winter night — should respond first and most strongly to CO2 emissions. In fact satellite data confirm that over the last 18 years the globe has cooled in general, but the coldest winter regions in Siberia and Canada have warmed. Are warmer winters in the Arctic Circle a problem?



If the State Department is serious about solving environmental problems, it should educate the world about the establishment of property rights, the enforcement of contracts, and the role of courts in resolving dispute. And then it should get out of the way.



Although the State Department claims there is no way to estimate the potential benefits from species yet to be discovered, the loss of biodiversity as a threat to pharmaceutical discoveries has been studied by economists for Resources for the Future. In a February 1996 article in the _Journal of Political Economy_ , they estimate the pharmaceutical benefits from saving one additional plant species from extinction to be, at most, about $10,000. That translates into an estimated willingness of pharmaceutical companies to pay about $8 an acre for land in tropical regions. The authors conclude that both the value of species preservation for use in pharmaceutical research and, by extension, the incentive to conserve threatened habitat are “negligible” in comparison with other development uses.



Let’s assume, however, that some of the problems are real and as serious as the report alleges. What will the State Department do if we give it more money to save us from those impending disasters? It will “help stabilize” regions where pollution contributes to political tensions. It will “enable nations … to work cooperatively to develop initiatives to attack regional environmental problems.” It will “strengthen our relationship with allies by working together on internal environmental problems.” In short, it will expend a amount of hot air to create domestic political capital.



If _Environmental Diplomacy_ were a prospectus for a company trying to raise additional money in the stock market for its new mission to solve environmental problems, would you invest your money? I think not, because the solution to environmental problems is not more meetings or negotiations but property rights. To the extent environmental problems exist, both within and between countries, the cause is the lack of adequately defined and enforced property rights. If people own resources, they seek to conserve their value rather than waste them. Rhinos and tigers are slaughtered to near extinction because no one owns them. Water and air resources are used with little thought because no one owns them.



If the State Department is serious about solving environmental problems, it should educate the world about the establishment of property rights, the enforcement of contracts, and the role of courts in resolving dispute. And then it should get out of the way. No one need decide the difficult tradeoffs about the use of land, energy, water or air. Prices will communicate the value of those commodities to all and they will be used to everyone’s best advantage. Short of that, the State Department should stop preaching and concentrate on developing a coherent post‐​Cold War national security strategy.
"
"You don’t need to observe politics for too long before realising that hypocrisy is the natural scent of the politics. It is a stench that pervades much of what is said and policy that is enacted. There are two particular types – the hypocrisy where politicians pretend they care about something and then do nothing, and the one where they pretend to think something and do the opposite. We have seen it this week with former foreign affairs minister Julie Bishop suddenly discovering climate change. She told Guardian Australia “that I’ve always been of the view that Australia, as a leading industrialised and developed nation, with one of the best standards of living in the world, needs to be a leader in the international response to climate change”. The nation as one responded with a resounding, “Seriously, what?” Name one instance during her time as foreign minister or even just as a member of the Howard or Abbott-Turnbull governments where she ever proffered such a view or supported such a policy position. Her very last mention of “climate change” in parliament came in 2017 when she lovingly talked up “high efficiency, low emissions” coal plants and decried in loud tones the ALP’s “obsession with a 50% renewable energy target” which she said was “destroying business confidence in South Australia, threatening jobs and threatening industries”. Gotta love that leadership. Or perhaps she was leading in 2015 when she told parliament: “I think it is important not to engage in hyperbole when one is talking about climate change. I remember in 2011 when the deputy leader [Tanya Plibersek] tried to scare the senior citizens on the Central Coast by saying that they were going to be subject to the ravages of climate change.” Or maybe she suggested being a “leader in the international response” was what she was doing when she told parliament that the government’s 26% to 28% target of emissions cuts, “is a responsible contribution” and that “what is not responsible is Labor’s endorsement of a carbon reduction target of up to 60% on 2000 levels by 2030”. Do nothing but say you care about doing lots. The other hypocrisy was writ large across the LNP this week with regards to the budget surplus. Gone is the boasting of a surplus already delivered, now the Treasurer says “well the surplus has never been an end in itself” and “delivering a surplus has not been our priority in the face of these crises” and “our focus is on is not necessarily delivering the surplus”. Last year, when David Speers questioned Frydenberg about the government’s boast of delivering a surplus that had actually yet to happen, Speers suggested “there might be a collapse in China, there might be another terrible drought … things can happen that could prevent you actually achieving this”. The treasurer responded by arguing: “Well this is not a wafer thin surplus. This is a very significant surplus.” Don’t expect to hear such arguments being made now. Now the talk is of things outside the control of the government – indeed they will use all the arguments they criticised the ALP for making when the GFC smashed the economy and the budget. Hypocrisy? Yep. But here’s the thing – we actually want them to be hypocrites, it is better for the nation if they are – so long as it is this brand of hypocrisy. The absolute worst thing the government could do right now is desperately try to deliver a surplus. Forget stories such as those that appeared on the front page of the Australian this week about new taxes – nothing on the revenue side will help the government now because any new taxes will only come into effect next financial year at the earliest. But they can stop spending right now. The only reason the government has been able to project a surplus is because they project that they will be awash with tax revenue. The government was not projecting a budget surplus because of big austerity campaigns, but because of factors outside their control that looked set to deliver a massive increase in tax. In the face of less than expected revenue, the only way to still deliver a surplus is to cut government spending – but that would further reduce economic activity and thus further reduce revenue, which would necessitate further cuts. The smart thing to do is to accept a budget deficit, because that will help keep the economy afloat. Will the government attempt to sell such a move with hypocritical statements? For sure. But that is better than the alternative. And so it is with climate change. Members of the LNP have spent more than a decade warning about not getting ahead of the rest of the world, or that a carbon price is actually a tax, or that we need to think more about adaptation than lowering emissions, or that electric cars will kill weekends or that renewable energy will deliver blackouts. It would be a measure of gross hypocrisy for them now to start shouting about the need to do more on emissions, that we need to lead the world and we need to put a price on carbon and to stop faffing about. The LNP account for around 99% of everyone in Australian politics who is to blame for the lost decade of climate change policy inaction. Were they to turn around now and argue there is no time to waste, we would be able to fill the MCG with print outs of transcripts that demonstrate their hypocrisy. And yet, bring it on. Because while hypocrisy should be addressed, I would much prefer the hypocrisy of a Liberal party pretending that their past actions did not contradict their new outlook on climate change than the current version we have of the LNP saying we need to do something but then purposefully and perpetually doing the opposite. The hypocrisy of their position on the budget surplus looks to have been forced on them. With luck all sides may use this as a moment to remember that it is actually true that “the surplus has never been an end in itself”. And on climate change we can only hope the LNP discover a new fragrance of hypocrisy. Yes it would be best if they were to admit their errors, and apologise for the past decade of waste that has cost us all so greatly. But if they start to act on reducing emissions and actually do attempt to lead the world, then I’ll take the emissions cuts and live with the hypocrisy."
"Royal Bank of Scotland’s new chief executive is renaming the group NatWest in a corporate overhaul designed to put its 2008 government bailout, and the fallout from a a string of scandals, behind it. The lender said it was ditching the 293-year-old RBS company name, saying it was the right time to make a change at the parent company and reflect that NatWest is its biggest brand. However, the existing RBS bank branches – most of which are in Scotland – will keep their name, as will Ulster Bank in Northern Ireland. The chief executive, Alison Rose, who took over in November, described her strategy as the “start of a new era”. The bank’s reputation was shattered after its near collapse in 2008, when the government was forced to shell out £45bn to save the bank, which is still 62% taxpayer-owned. Since then the RBS brand has been sullied by claims that it mis-sold toxic mortgage-backed securities in the lead-up to the global credit crunch and that it pushed small businesses towards failure in order to sell their assets for profit. Despite ditching the Scottish name, the chairman, Howard Davies, said the bank’s headquarters would remain in Edinburgh. “We’re not unscrewing any brass plaques at this point,” he said. However, Rose, the first woman to run one of the “big four” lenders, will be based in London. The strategy was revealed as the bank released its full-year earnings, which showed that 2019 pre-tax profits surged by 93% to £3.1bn from £1.6bn a year earlier. Its results were boosted by its disposal of a stake in the Middle Eastern bank Alawwal last year. This is the third consecutive year of profit for RBS since its bailout in 2008. Shareholders will pocket another £968m in dividends as a result of the bank’s strong performance, £600m of which will go to the government coffers. Rose was widely expected to reveal job losses on Friday but refused to disclose how many colleagues or bank branches would be axed as part of plans to cut costs by £250m this year. Its NatWest Markets investment bank has been singled out for a “significant transformation”, with plans to exit parts of the business and cut its basket of risky assets in half. Rose said it was part of efforts to build a “smaller and simpler bank”. As part of the strategy, Rose announced that RBS will also stop lending and offering underwriting services to major oil and gas producers that do not have credible transition plans in line with Paris climate agreement targets, which aim to limit global heating to below 2C. The group is threatening similar moves against companies with more than 15% of their activities related to coal unless they have similar plans prepared. RBS also pledged to fully phase out coal financing by 2030 and is aiming to “at least halve” the climate impact of its lending activity by the end of the decade. The value of loans to oil, gas and coal companies is relatively small compared to its peers, worth about £6bn. At its rival Barclays, lending and underwriting to carbon-intensive companies and projects totalled $85bn (£64bn) between 2015 and 2018, according to a study commissioned by campaign groups including Rainforest Action Network.  However, Rose said it was a step in the right direction: “On our balance sheet, only 1% of our lending is to the oil and gas sector, and in coal it’s marginal, down at 0.3%. But, for me, the key message here is we’re all going to need to work together; no one organisation is going to solve the climate challenge on their own and we’re very keen to work with business, with regulators, with industry to help make the plans to transition over the next 10 years.”"
"
As many of you may know, I produce a variety of weather imagery maps for web and broadcast in SD and HD. Since there is a lot of interest in the path of hurricane Gustav, I thought I’d post a near-live image, which will update every 30 minutes.

Click image for full size or animate this image: Click for loop>>>   
What is interesting to note, is that as of this writing, Gustav seems to be losing organization. The eye, which was well defined just before making landfall on Cuba, seems very nebulous. Watch and wait.
Update: 3:30PM PST, while there was some weakening earlier, it now looks like signs of increased angular momentum are showing up in the satellite imagery. A defined eye may appear again.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cb0b9fe',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Mr. President, justices of the Constitutional Court, distinguished guests, ladies and gentlemen, good morning.



I’m honored to be here today to speak as you celebrate the 70th anniversary of Italy’s Constitution. In a world of political and constitutional uncertainties, it’s altogether fitting that you should be marking this milestone in Italy’s history, and I wish you many more such celebrations in the years to come.



Before I begin, let me thank President Giorgio Benvenuto, our event coordinator Professor Luigi Troiani, and their colleagues at the Pietro Nenni Foundation for bringing this occasion about and for inviting me to speak before you. When they visited me in Washington in October, we talked about work I’d done over the years on American constitutional theory and history. And we talked in particular about the Cato Institute’s annual Constitution Day Symposium, which I started 16 years ago to mark the day in 1787 when the American Constitution was completed and sent out to the states for ratification. Since that Constitution has endured now for more than 230 years, Senator Benvenuto and Professor Troiani thought it might be useful to share some of that experience with you, especially as it might bear on European constitutionalism. And so we settled on the topic I’ll discuss this morning, “American Constitutional Theory and History: Implications for European Constitutionalism.”



That’s a large subject, of course, with many moving parts drawn from several disciplines. And the inevitable ambiguities of translation complicate understanding even further. So rather than dive straight into my argument, I’ll first say a word about my subtitle and then outline the argument so you’ll know where I’m going.



By European constitutionalism I’m alluding not to the national constitutions of the European nations, about which I know little, but to the European Union Treaties that serve as a “Constitutional Charter” for the European Communities. As an American constitutionalist looking east and seeing everything from Brexit to Grexit, plus events in European capitals, I’m struck by the tension in the EU between exclusion and inclusion in its many forms, including individualism and collectivism. Those themes underpin my talk today. The issues surrounding them are universal. They’re at the heart of the human condition.



In America we wrestled with them at our founding over 200 years ago, again in the aftermath of our Civil War, and yet again with the advent of Progressivism, which culminated in our New Deal constitutional revolution. And we’re still wrestling with them. Because America was founded on philosophical principles — First Principles, coming from the Enlightenment — it’s particularly appropriate that we look at that experience to shed such light as we can on this more recent European constitutional experience.



But my more immediate concern is this: In liberal democracies today — nations constituted in the classical liberal tradition — we see the same basic problem, albeit with significant variations. It’s that the growth of government, responding mainly to popular demand, has raised seemingly intractable moral and practical problems. First, increasing intrusions on individual liberty; and second, the unwillingness of people to pay for all the public goods and services they’re demanding. So governments borrow. And that’s led to massive public debt that saddles our children and grandchildren, to bankruptcy, and to the failure of governments to keep the commitments they’ve made.



In Italy, we need only look east, to the birthplace of democracy. But Greece isn’t alone in this. Nor are we in America immune. Cities like Detroit have gone bankrupt. So too, just recently, has the American territory of Puerto Rico. The state of Illinois has a credit rating today just above junk status, and Connecticut and New Jersey, among other states, aren’t far behind. At the national level, America’s debt today exceeds $20 trillion — that’s trillion — more than double what it was only a decade ago. And our unfunded liability vastly exceeds that.



What has this got to do with constitutionalism? A great deal. Constitutions are written, after all, to discipline not only the governments they authorize but the people themselves. The point was famously stated by James Madison, the principal author of the U.S. Constitution. “In framing a government which is to be administered by men over men, the great difficulty lies in this,” he wrote: “you must first enable the government to control the governed; and in the next place oblige it to control itself. A dependence on the people is, no doubt, the primary control on the government,” Madison concluded, “but experience has taught mankind the necessity of auxiliary precautions.“1



The principal such precaution, of course, is a well‐​written constitution. But no constitution is self‐​executing. It’s people who ultimately execute constitutions. In the end, therefore, the issue is cultural, a point I’ll come back to.



America’s Founders were deeply concerned with the problem of undisciplined, unlimited government. After all, they’d just fought a war to rid themselves of distant, overbearing government. In drafting the Constitution, therefore, they weren’t about to impose that kind of government on themselves. In fact, during the ratification debates in the states, there were two main camps — the Anti‐​Federalists, who thought the proposed Constitution gave the government too much power, and the Federalists, who responded by pointing to the many ways the proposed Constitution would guard against that risk. The Federalists eventually won, of course, but the point I want to secure is that there wasn’t a socialist in the group! There were _limited_ government people, the Federalists, and _even more_ limited government people, the Anti‐​federalists.



So under a Constitution that hasn’t changed all that much, how did we go from limited to effectively unlimited government? The answer lies in the fundamental shift in the climate of ideas that began with Progressivism at the end of the 19th century, which the New Deal Supreme Court institutionalized in the 1930s. To illustrate that, I’ll first look closely at America’s founding documents: the Declaration of Independence, signed in 1776; the Constitution, ratified in 1788; the Bill of Rights, ratified in 1791; and the Civil War Amendments, ratified between 1865 and 1870, which corrected flaws in the original Constitution. Together, those documents constitute a legal framework for individual liberty under limited government, however inconsistent with those principles our actual history may have been.



Then I’ll show how progressives rejected the libertarian and limited government principles of America’s Founders and how they eventually turned the Constitution on its head, not by amending it but through political pressure brought to bear on the Supreme Court. The problems that have ensued include the ones just noted: less liberty, increasing debt. But perhaps of even greater importance, for eight decades now the Supreme Court has struggled to square its post‐​New Deal decisions with the text and theory of the Constitution. That amounts to nothing less than a crisis of constitutional legitimacy.



And again, the basic reason for that crisis is the fundamental shift in outlook. Many Americans today no longer think of government as earlier generations did. Whereas the Founders saw government as a “necessary evil,” to be restrained at every turn, many today think that the purpose of government is to provide them with vast goods and services, as decided by democratic majorities.



 **The Importance of Theory**



I come, then, to the first important point I want to flag. You cannot understand the U.S. Constitution unless you understand the moral and political theory that stands behind it. And that was outlined not in the Constitution but in the Declaration of Independence.2 The Constitution was written in a context, as were the later Civil War Amendments, and that context was one of natural law, Anglo‐​American common law, and even elements of Roman Law, all of which is captured succinctly in those famous words of the Declaration that I’ll quote in a moment. Indeed, President Abraham Lincoln’s famous Gettysburg Address, written in the throes of a brutal Civil War, begins with these words: “Fourscore and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.” Lincoln was reaching back to the Declaration, not to the Constitution.



Yet no less than my good friend and Italy’s gift to American constitutionalism, the late Justice Antonin Scalia, all but dismissed the Declaration as “philosophizing,” contrasting it with the Constitution’s “operative provisions.“3 And his conservative colleague when the two served on the nation’s second highest court, the late Judge Robert Bork, wrote that “the ringing phrases [of the Declaration] are hardly useful, indeed may be pernicious, if taken, as they commonly are, as a guide to action, governmental or private.“4 Is it any wonder that there’s constitutional confusion in America today when the document that’s essential to understanding it plays little or no part in that understanding?



With that outline now before us, let me flesh out the argument. I’ll do that by focusing on the underlying moral, political, and legal principles at stake, after which I’ll offer just a few reflections on how they might illuminate issues in the European context. Again, I want to show how the shift from limited to effectively unlimited government took place in America, despite very few constitutional changes. I should note, however, that it will be some time before I get to the Constitution. If a proper understanding of the Constitution requires a proper understanding of the theory behind it, and if that theory is found implicitly in the Declaration, then that should be our initial focus, and will be for some time.



That will take us into some of the deeper reaches of moral and political theory, so please bear with me. My hope is that at the end you’ll better understand the Constitution itself — and especially the broad principles that underpin it.



So let’s get started. The first thing to notice about the American constitutional experience is how relatively different its beginnings were from those of many other nations. Constitution making and remaking in many nations has taken place in the context of an often stormy history stretching back centuries, even millennia. By contrast, America was a _new_ nation. We came into being at a precise point in time, with the signing of the Declaration of Independence. True, American patriots had to win our independence on the battlefield. And before that we had a colonial history of roughly 150 years. But America was created not by a discrete people but by diverse immigrants with unique histories all their own.



A second, crucial feature distinguishing America’s constitutional experience is that it unfolded during the intense intellectual ferment of the Enlightenment, including the Scottish Enlightenment, with its focus on the individual, individual liberty, and political legitimacy, all of which reflected the sense of “a new beginning.” Indeed, the motto on the Great Seal of the United States captures well the spirit of America’s origins: _Novus ordo seclorum, “a_ new order of the ages.”



 **The Declaration of Independence**



Let’s turn, then, to that new order, as outlined in the Declaration. Penned near the start of our struggle for independence, the Declaration in form is a _political_ document. But were it merely that, it would not have so endured in our national consciousness. Nor would it have inspired countless millions around the world ever since, leading many to leave their homelands to begin life anew under its promise, including millions from Italy who now enrich America. It has so inspired because, fundamentally, it is a profound _moral_ statement. Offered from “a decent Respect to the Opinions of Mankind” and invoking “the Laws of Nature and of Nature’s God,” it was written not only to declare but to _justify_ our independence. And it did so not simply by listing the king’s “long Train of Abuses and Usurpations,” which constitute the greater part of the document, but by first setting forth the moral and political vision that rendered those acts unjust.



And so we come to those famous words that flowed from Thomas Jefferson’s pen in 1776, words that capture fundamental principles concerning the human condition:



We hold these Truths to be self‐​evident, that all Men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty, and the Pursuit of Happiness — That to secure these Rights, Governments are instituted among Men, deriving their just Powers from the Consent of the Governed.



The first thing to notice about that justly famous passage is that its propositions are asserted as “truths,” not mere opinions. The Founders were not moral relativists. They were confident in their claims. And why not? Their truths were said to be “self‐​evident,” grounded in universal reason, accessible by all mankind — and the evidence supports that.



Notice too the structure of the passage: There are two parts — and the order is crucial. The moral vision comes first, defined by equal rights. The political and legal vision comes second, defined by powers, as derived from the moral vision. And right there is the second major point I want to flag: Unlike today, where politics, grounded in will, so often determines what rights we have, for early Americans, morality, grounded in reason, determined our rights. The Founders were concerned fundamentally with moral and political _legitimacy_. Rights first, government second, as the means for securing our rights.5



Given that order of things, the Founders were engaged in what’s called “state‐​of‐​nature theory,” a rudimentary form of which can be found in the writings of your own Seneca,6 a fuller discussion much later in those of Thomas Hobbes7 and, especially, John Locke8 — often said to be the philosophical father of America.



State‐​of‐​nature theory is a thought experiment. The idea is to show how, without violating any rights, a legitimate government with legitimate powers might arise from a world with no government. Thus, the first step is to show, from pure reason, what rights we’d have in such a world.



For that, as the Declaration implies, we turn to the natural law tradition — more precisely, the natural rights strain coming from the Reformation and the Enlightenment. Simply put, natural law stands for the idea that there’s a “higher law” of right and wrong, grounded in reason, from which to derive the positive law, and against which to criticize that law at any point in time. There’s nothing suspect about that idea, as modern moral skeptics argue. We appeal to natural law when the positive or actual law is morally wrong. In America, the abolitionists, the suffragists, and the civil rights marchers all invoked our natural rights in their struggles to overturn unjust law.



The origins of this law are in antiquity. Many of its particulars are found in Roman Law, especially the law of property and contract. Over some 500 years in England, prior to the American Revolution, this law was refined and reduced to positive law by common‐​law judges consulting reason, custom, and what they knew of Roman Law as they adjudicated cases brought before them by ordinary individuals.9 And John Locke drew largely on that body of common‐​law rights as he crafted a theory of natural rights, much as Jefferson drew on Locke when he drafted the Declaration.



To correct a common misunderstanding, these are the rights we hold _against each other_ , and would hold in a state of nature. Later, once we create a government, they’ll serve as rights we hold against that government, and likely be included in a bill of rights.



To discover and justify these rights in detail, as I and others have done,10 we’d need to delve into the complex issues of moral epistemology and legal casuistry, and this isn’t the occasion for that. Suffice it to say that when that foundational work is finished, the conclusion one reaches is the same one America’s Founders reached intuitively, that our basic right is the right to be free from the unjustified interference of others, and all other rights are derived from that basic right, as the facts may warrant. What results approximates largely the judge‐​made common law of property, torts, contracts, and remedies, a law that defines our private relationships, as it did in early America, both before and long after the Revolution. It’s a law that says, in essence, that each of us is free to pursue happiness, by his own subjective values, either alone or in association with others, provided we respect the equal objective rights of others to do the same. In short, it’s a live‐​and‐​let‐​live law of liberty.



And I can summarize it with three simple rules, so simple that even a child can understand them.



Rule 1: Don’t take what belongs to someone else. That’s the whole world of property, broadly conceived as Locke did — our property in our “Lives, Liberties, and Estates.”



Rule 2: Keep your promises. That’s the whole world of contracts and associations.



Rule 3. If you’ve wrongly violated rules 1 or 2, give back what you’ve wrongly taken or wrongly withheld. That’s the whole world of remedies.



There’s a fourth rule, but it’s optional: Do some good. You’re free not to be a Good Samaritan, but you should be one if you’re a decent human being and the cost to you is modest. Unlike much continental law, Anglo‐​American law never compelled strangers to come to the aid of others.11 It didn’t because individual liberty is its main object. And it saw that there’s no virtue in forced beneficence. We’re free to criticize those who don’t come to the aid of others, and we should, even as we defend their right not to.



Why have I mentioned this fourth, voluntary rule? Again, it’s because, when we start from a theoretical state of nature, we need to know what rights we do and do not have for government to enforce once we bring it into the picture. And the Good Samaritan is the modern welfare state writ small. If there’s no right to be rescued, there’s no correlative obligation for government to enforce. Recognizing that raises important questions about the very legitimacy of the welfare state.12



 **Leaving the State of Nature and the Problem of Political Legitimacy**



To get to the Constitution, however, we need now to take the last step in the argument. We need to derive a legitimate government with legitimate powers — and that’s not easy. I’ve said little about enforcement so far. The Declaration says that government’s purpose is to secure our rights, its _just_ powers derived “from the consent of the governed.” Thus, the Founders invoked the social contract, which grounds political legitimacy in consent.



But there are well‐​known problems with consent‐​based social‐​contract theory as a ground for political legitimacy. The question is how to move legitimately from self‐​rule to collective rule. Unanimity will achieve legitimacy, of course, but rarely if ever do we get it. Majoritarianism won’t solve the problem, because it amounts to tyranny over the minority that hasn’t consented. Nor will the social contract work, except for those in the original position who agree thereafter to be bound by the will of the majority. Nor, finally, will so‐​called tacit consent work — “you stayed, therefore you’re bound by the majority” — because it puts the minority to a choice between two of its rights, its right to stay where it is, and its right not to be ruled by the majority, precisely what the majority must justify on pain of circularity. As for elections, an occasional vote hardly justifies all that follows.



As a _practical_ matter, the social contract argument may be the best we can do, but recognizing its infirmities leads to a compelling conclusion — and to the third basic point I want to flag, namely, that there is an air of illegitimacy that surrounds government as such. Government is not like a private association that we join or leave at will. It’s a _forced_ association. Its very definition entails force. And once we recognize its essential character, that should compel us, _from a concern for legitimacy_ , to do as much as we can through the private sector where it can be done voluntarily and hence in violation of the rights of no one, and as little as possible through the public sector where individuals will be forced into programs they may want no part of.



In short, as a _moral_ matter, there’s a strong presumption against doing things through government. We should turn to government not as a first but only as a _last_ resort, when all else fails.



Still, we can refine this conclusion. We can distinguish three distinct powers in decreasing degrees of legitimacy. The first is the police power — the power, through adjudication or legislation, to more precisely define and enforce our rights. As such, it’s bound by the rights we have to be enforced, although it includes the power to provide limited “public goods” like national defense, clean air, and certain infrastructure, goods described by non‐​excludability and non‐​rivalrous consumption, as economists define them.13



When we leave the state of nature, we give government that power to exercise on our behalf. But because we had the power in the state of nature — Locke called it the “Executive Power” each of us has to secure his rights — to that extent it’s legitimate. Only the anarchist who would prefer to remain in the state of nature can be heard to complain. Fortunately, there are few of those.



Less legitimate is the eminent domain power — the power to condemn and take private property for public use after paying the owner just compensation — because none of us would have such a power in the state of nature. Such legitimacy as this power enjoys, at least in America, is because we gave it to government when we ratified the Constitution’s Fifth Amendment, which includes the Takings Clause; and it’s “Pareto optimal,” as economists say, meaning that at least one person is made better off by it — the public, as shown by its willingness to pay — and no one is made worse off — the owner, provided he’s indifferent as to whether he keeps the property or gets the compensation.



The third great governmental power, ubiquitous today, is the least legitimate. In fact, from a natural rights perspective, it enjoys no legitimacy. It’s the redistributive power, and it takes two forms, material and regulatory. Through redistributive taxation, government takes from _A_ and gives to _B_. Through redistributive regulation, government prohibits _A_ from doing what he would otherwise have a right to do or requires him to do what he would otherwise have a right not to do, all for the benefit of _B_. Those powers describe the modern redistributive and regulatory state. No one would have them in the state of nature. How then could government get them legitimately, since governments, in the classical liberal tradition, get whatever powers they have from the people, who must first _have_ those powers to yield up to government?



There are three main answers. First, if that redistribution arose through unanimous consent, there would be no problem, but again, rarely if ever does that occur in the public sector. Second, majorities gave governments those powers. That raises the classic problem of the tyranny of the majority. And third, special interests have learned how to work the system for their benefit, as public choice economists have long explained.14 That’s the tyranny of the minority — and the main source today of such schemes.



We can conclude this examination of the moral foundations of the classical liberal vision by imagining a continuum, with anarchy or no government at one end — our state of nature — and totalitarianism at the other end, where everything possible is done through government. At the anarchy end, individuals are free to plan and live their lives as they wish, alone or in cooperation with others. They will soon find, however, that there are some things best done collectively, like the provision and enforcement of law, national defense, clean air and water, limited infrastructure, and the like — public goods — and most will consent to the public provision of such goods. But as we move up the continuum toward totalitarianism and try to bring more and more _private_ goods under _public_ provision — education, health care, child care, jobs, housing, ordinary goods and services — people start voting with their feet. The Berlin Wall was not built to keep West German workers out of the workers’ paradise to the east.



The moral, political, and legal vision implicit in the Declaration of Independence is closer to the anarchy end of that continuum. America’s Founders envisioned a land in which people were free to live as they wished, respecting the equal rights of others to do the same, with government there to secure those rights and do the few other things it was authorized to do.



That basic moral vision is perfectly universalizable. How to secure it through the rule of law is another matter. Certain basic legal principles are themselves universalizable and are common to most legal systems, but whether a nation has a parliamentary system as in much of Europe, or a republican form of government as in America, or some other arrangement is not a matter of natural law. Let’s now see how the Founders framed a constitution to secure the Declaration’s moral vision.



 **The Constitution**



After we declared independence, and during our struggle for it, we lived under our first constitution, the Articles of Confederation. As its name implies, it was a loose agreement among the 13 states, authorizing a national government that hardly warranted the name. Three main problems lay ahead. Surrounded on three sides by great European powers, our national defense was painfully inadequate. Second, states were erecting tariffs and other barriers to free interstate trade. And finally, our war debts remained unpaid. After 11 years, the Framers met in Philadelphia to draft a new Constitution.



The main problem they faced was how to strike a balance. They needed to give the new government enough power to address those problems and accomplish its broad aims, yet not so much as to risk our liberties. Those aims were set forth in the Constitution’s Preamble:



We the People of the United States, in Order to form a more perfect Union, establish Justice, insure domestic Tranquility, provide for the common defence, promote the general Welfare, and secure the Blessings of Liberty to ourselves and our Posterity, do ordain and establish this Constitution for the United States of America.



Notice: States aside, regarding the proposed new government, we’re right back in the state of nature, about to “ordain and establish” a constitution to authorize it and bring it into being. All power rests _initially_ with “we the people.” _We_ bring the constitution and the government that follows into being through ratification. _We_ give it its powers, such as we do. The government does not give us our rights. We already have our rights, natural rights, the exercise of which creates and empowers this government.



So how does Madison strike the balance between power and liberty in service of those aims? First, through federalism: Power was _divided_ between the federal and state governments, with most power left with the states, especially the general police power, the basic power of government to secure our rights, as just discussed. The powers we delegated to the federal government concerned national issues like defense, free interstate commerce, rules for intellectual property, a national currency, and the like.



Second, following Montesquieu, Madison _separated_ powers among the three branches of the federal government, with each branch defined functionally. Pitting power against power, he provided for a bicameral legislature, with each chamber constituted differently; a unitary executive to enforce national legislation and conduct foreign affairs; and an independent judiciary with the implicit power to review legislative and executive actions for their constitutionality — a novel institution at that time, and a crucial one as time went on.



Third, although the Constitution left most of the rules for elections with the states, it provided for periodic elections to fill the offices set forth in the document, thus leaving ultimate power with the people.



But while each of those provisions and others struck a balance between power and liberty, the main restraint on overweening government took the name of the _doctrine of enumerated powers_. And I can state it no more simply than this: If you want to limit power, don’t give it in the first place. We see that doctrine in the very first sentence of the Constitution, after the Preamble: “All legislative Powers _herein granted_ shall be vested in a Congress .…” By implication, not all powers were “herein granted.” Look at Article I, section 8, and you’ll see that Congress has only 18 powers or ends that the people have authorized. And the last documentary evidence from the founding period, the Tenth Amendment, states that doctrine explicitly: “The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.” In other words, the Constitution creates a government of delegated, enumerated, and thus limited powers. If a power is not found in the document, it belongs to the states — or to the people, never having been given to either government.



As I noted earlier, when the Constitution was sent out to the states for ratification, it met stiff resistance as Anti‐​Federalists thought it gave too much power to the national government. Only after the Federalists agreed to add a bill of rights was it finally ratified. During the first Congress in 1789, Madison drafted 12 amendments, 10 of which were ratified in 1791 as the Bill of Rights. It sets forth rights that are good against the federal government, such as freedom of religion, speech, press, and assembly, the right to keep and bear arms, to be secure against unreasonable searches and seizures, to due process of law, to compensation if private property is taken for public use, to trial by jury, and more.



But it’s important to note that the Bill of Rights was, as Justice Scalia said, an “afterthought.“15 Unlike with many European constitutions, which begin with a long list of rights, many aspirational, the Framers saw the Constitution’s _structural_ provisions as their main protection against overweening government.16 And on that score, it’s crucial to mention the Ninth Amendment, which reads: “The enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people.”



The history behind that amendment is instructive. During the ratification debates, there were two main objections to adding a bill of rights. First, it would be unnecessary. “Why declare that things shall not be done,” asked Alexander Hamilton, “which there is no power to do?“17 Notice, he was alluding to the enumerated powers doctrine as the _main_ protection for our liberties: Where there is no power, there is a right.



But second, it would be impossible to list all of our rights, yet by ordinary principles of legal construction,18 the failure to do so would be construed as implying that only those rights that were listed were meant to be protected. So to guard against that, they wrote the Ninth Amendment, which reads, again, “The enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people.” Notice: “retained by the people.” You can’t retain what you don’t first have to be retained. They were alluding to our natural rights, which we retained when we left the state of nature, save for those we gave up to government to exercise on our behalf, like the police power.



For a proper understanding of the Constitution, the importance of the Ninth Amendment, which speaks of retained rights, and the Tenth Amendment, which speaks of delegated powers, cannot be overstated.19 Taken together, as the last documentary evidence from the founding period, they recapitulate the vision of the Declaration. We all have rights, enumerated and unenumerated alike, to pursue happiness by our own lights, to plan and live our lives as we wish, provided we respect the rights of others to do the same, and federal and state governments are there to secure those rights through the limited powers we’ve given them toward that end. There, in a nutshell, is the American vision, reduced from natural to positive law.



There was a problem, however. There were too few checks on the states, where most power was left. And the reason was slavery. To achieve unity among the states, the Framers made their Faustian bargain. They knew that slavery was inconsistent with their founding principles. They hoped it would wither away in time. It didn’t. It took a brutal civil war to end slavery, and the Civil War Amendments to complete the Constitution by incorporating at last the grand principles of the Declaration, especially equality before the law.



The Thirteenth Amendment ended slavery in 1865. The Fifteenth Amendment, ratified in 1870, protected the right to vote. And the Fourteenth Amendment, ratified in 1868, defined federal and state citizenship and, for the first time, provided _federal_ remedies against a state’s violating the rights of its own citizens.20



Unfortunately, only five years after the Fourteenth Amendment was ratified, a deeply divided 5–4 Supreme Court eviscerated the principal font of substantive rights under the amendment, the Privileges or Immunities Clause.21 Thereafter the Court would try to do under the less substantive Due Process Clause what was meant to be done under privileges or immunities, and the misreading of the Fourteenth Amendment has continued to this day. Among other things, the upshot was Jim Crow racial segregation in the South, which lasted until the middle of the 20th century.



 **Progressivism**



We turn now to the great ideological watershed, the rise of Progressivism at the end of the 19th century. Coming from the elite universities of the Northeast, progressives rejected the Founder’s libertarian and limited government vision.22 They were social engineers, planners enamored of the new social sciences. Insensitive when not hostile to the power of markets to order human affairs justly and efficiently, they sought to address what they saw as social problems through redistributive regulatory legislation. They looked to Europe for inspiration: Bismarck’s social security scheme, for example, and British utilitarianism, which in ethics had replaced natural rights theory. The idea was that policy, law, and judgment were to be justified not by whether they protected our natural and moral rights but whether they _gave_ us positive rights purporting to produce the greatest good for the greatest number.



A particularly egregious example of that rationale concerned a sweetheart suit brought against a Virginia statute that authorized the sterilization of people thought to be of insufficient intelligence.23 Part of the bogus “eugenics” movement, the law was designed to improve the human gene pool. Writing for a divided Supreme Court in 1927, the sainted Justice Oliver Wendell Holmes upheld the statute, ending his short opinion with the ringing words, “Three generations of imbeciles are enough.” There followed some 70,000 sterilizations across the nation.



Some of what the progressives did was long overdue, like promoting municipal health and safety measures and attacking corruption. Yet they also sowed the seeds for later corruption, especially through regulatory schemes ripe for special interest capture, replacing markets with cartels.24 And their record on racial matters was abysmal.25



During the early decades of the 20th century, progressives directed their political activism mostly at the state level, but they often failed as the courts upheld constitutional principles securing individual liberty and free markets. With the election of Franklin Roosevelt in 1932, however, progressive activism shifted to the federal level. Still, during the president’s first term the Supreme Court continued mostly to uphold limits on federal power, finding several of Roosevelt’s programs unconstitutional.



With the landslide election of 1936, however, things came to a head. Early in 1937, Roosevelt unveiled his infamous Court‐​packing scheme, his threat to pack the Court with six new members. Uproar followed. Not even an overwhelmingly Democratic Congress would go along with the plan. Nevertheless, the Court got the message. The famous “switch in time that saved nine” justices followed. The Court began rewriting the Constitution, in effect, not through amendment by the people, the proper way, but by reading the document as it hadn’t been read for 150 years — as authorizing effectively _unlimited_ government.26



The Court did that rewrite in three basic steps. First, in 1937 it eviscerated the very centerpiece of the Constitution, the doctrine of enumerated powers. Then in 1938 it bifurcated the Bill of Rights and gave us a bifurcated theory of judicial review. Finally, in 1943 it jettisoned the non‐​delegation doctrine. Let me describe those steps a bit more fully so you can see the importance of recognizing and adhering to the theory that stands behind and informs a constitution.



The evisceration of the doctrine of enumerated powers involved three clauses in Article I, section 8, where Congress’s 18 legislative powers are enumerated: the General Welfare Clause, the Commerce Clause, and the Necessary and Proper Clause. All were written to be shields against government. The New Deal Court turned them into swords of government through which the modern redistributive and regulatory state has arisen.



In relevant part, the General Welfare Clause, the first of Congress’s enumerated powers, authorizes Congress to tax to pay for the “general Welfare of the United States.” As Madison wrote in _Federalist 41_ , that qualifying language was simply a general heading under which Congress’s 17 other powers were subsumed, for which Congress may tax, but only if they serve the _general_ welfare _of the United States_ , not particular or local welfare.



Instead, the New Deal Court read the clause as an _independent_ power authorizing Congress to tax for whatever it thought might serve the “general welfare.“27 That reading could not be right, however, because it would enable Congress to tax for virtually any end, thus rendering Congress’s other powers superfluous, as Madison, Jefferson, and many others noted when the issue arose early in our history. Indeed, it would turn the Constitution on its head by allowing Congress effectively unlimited power. Such is the result from ignoring the document’s underlying theory of limited government.



Similar issues arose that year with the Commerce Clause, which in relevant part authorized Congress to regulate interstate commerce. Recall that under the Articles of Confederation states had begun erecting tariffs and other protectionist measures, and that was leading to the breakdown of free trade among the states. Thus, the Framers gave Congress the power to regulate — or _make regular_ — commerce among the states, largely by negating state actions that impeded free trade, but also through affirmative actions that might facilitate that end.28



Over several decisions, however, beginning in 1937,29 the New Deal Court read the Commerce Clause as authorizing Congress to regulate anything that “affected” interstate commerce, which of course is virtually everything. Thus, in 1942 the Court held that, to keep the price of wheat high for farmers, Congress could limit the amount of wheat a farmer could grow, even though the excess wheat in question in the case never entered commerce, much less interstate commerce, but was consumed on the farm by the farmer and his cattle. The Court held that the excess wheat he consumed himself was wheat he would otherwise have bought on the market, so “in the aggregate” such actions “affected” interstate commerce.30 Such were the economic theories of the Roosevelt administration.



The last of Congress’s 18 enumerated powers authorizes it “to make all laws which shall be necessary and proper for carrying into execution the foregoing powers.” Thus, the clause affords Congress _instrumental_ powers — the _means_ for executing its other powers or pursuing its other enumerated ends. “Necessary” and “proper” are words of limitation, of course: Not any means Congress desires will do. Yet the New Deal and subsequent Court’s, until very recently, have hardly policed those limitations.31



Turning now to the second step, despite the demise in 1937 of the doctrine of enumerated powers, one could still invoke one’s rights against Congress’s expanded powers. So to address that “problem,” the New Deal Court added a famous footnote to a 1938 opinion.32 In it, the Court distinguished two kinds of rights: “fundamental,” like speech, voting, and, later, certain personal rights; and “non‐​fundamental,” like property rights and rights we exercise in “ordinary commercial relations.” If a law implicated fundamental rights, the Court would apply “strict scrutiny” and the law would likely be found unconstitutional. By contrast, if non‐​fundamental rights were at issue, the Court would apply the so‐​called rational basis test, which held that if there were _some_ reason for the law, if you could _conceive_ of one, the law would be upheld. Thus was economic liberty reduced to a second‐​class status. None of this is found in the Constitution, of course. The Court invented it from whole cloth to make the world safe for the New Deal programs.33



Finally, in 1943 the Court jettisoned the non‐​delegation doctrine,34 which arises from the very first word of the Constitution: “ _All_ legislative Powers herein granted shall be vested in a Congress .…” Not some; all. As government grew, especially during the New Deal, Congress began delegating ever more of its legislative power to the executive branch agencies it was creating to carry out its programs. Some 450 such agencies exist in Washington today. Nobody knows the exact number.



That is where most of the law Americans live under today is written, in the form of regulations, rules, guidance, and more, all issued to implement the broad statutes Congress passes. Not only is this “law” written, executed, and adjudicated by unelected, non‐​responsible agency bureaucrats — raising serious separation‐​of‐​powers questions — but the Court has developed doctrines under which it defers to _agencies_ ’ interpretations of statutes, thus largely abandoning its duty to oversee the political branches. Governed largely today under administrative law promulgated by the modern executive state, we’re far removed from the limited, accountable government envisioned by the Founders and Framers.35



This completes my overview of American constitutional theory and history. From it, as I mentioned early on, the main lesson to be drawn is that culture matters. The Founders and Framers were animated by individual liberty under limited government. When the post‐​Civil War Framers saw the need to revise our original federalism, they did it the right way, by amending the Constitution to make it consistent with its underlying moral and political principles. The New Deal politicians, having less regard for the Constitution and its underlying principles, rejected that course, choosing instead to browbeat the Court into effectively rewriting the Constitution, undermining its moral and political principles in the process.



But don’t take my word for it. Here’s Franklin Roosevelt, writing to the chairman of the House Ways and Means Committee in 1935: “I hope your committee will not permit doubts as to constitutionality, however reasonable, to block the suggested legislation.“36 And here’s Rexford Tugwell, one of the principal architects of the New Deal, reflecting on his handiwork some 30 years later: “To the extent that these [New Deal policies] developed, they were tortured interpretations of a document intended to prevent them.” They knew exactly what they were doing. They were turning the Constitution on its head.37



Thus, the problem today is not, as so many America progressives think, too little government. It’s too much government, intruding on our liberties and driving us ever deeper into debt. And it isn’t as if our Founders didn’t understand that. As Jefferson famously wrote, “The _natural_ progress of things is for liberty to yield, and _government_ to gain ground.“38 The remedy is a good constitution, but it must be followed. And that takes good people.



 **A Few Implications for European Constitutionalism**



So what lessons might we draw from the American experience for European constitutionalism? Recall my mentioning earlier of being struck by the tension in the EU between exclusion and inclusion in its many forms, including individualism and collectivism. As we’ve seen, that same tension runs through America’s constitutional history as well. To address deficiencies in the Articles of Confederation, the original Constitution moved toward greater inclusion to form “a more perfect Union.” But the resulting federalism didn’t get the balance right either. It left too much power with the states, enabling the southern states to continue enforcing slavery. So the Civil War Amendments increased the inclusion, correctly. The adjusted federalism gave more power to the federal government, enabling it to block states from oppressing their own citizens — a higher power checking a subsidiary power.



But that balance, reflecting the nation’s underlying principles, was upended again by the far more inclusive New Deal constitutional revolution. Giving vastly more power to the federal government, contrary to the nation’s limited government principles, this change swept ever more Americans into public programs, leading many to want out. They wanted to be _excluded_ from the socialization of life, as reflected by the rise of the conservative and libertarian movements in the second half of the 20th century.



Are there parallels with post‐​War developments in Europe? To this sometime‐​student of European affairs, there seem to be, but the inclusion that began with the 1951Treaty of Paris and continued through the many treaties since makes it difficult if not impossible to speak of three distinct periods, as in America, much less point to a “golden mean” in this evolution akin to America’s post‐​Civil War settlement. In recent years, however, the impetus toward exclusion, in many forms, is unmistakable, Brexit being only the most prominent example, the ongoing refugee resettlement crisis being another.



Federalism _within_ nations is a delicate balance. Federalism _among_ sovereign nations, which is what the EU amounts to, is far more difficult, especially when cultural differences loom large. And on that score, here’s a paradox. Europeans have always been more comfortable than Americans with collectivization in the form of the welfare state, certainly within their respective nations.39 But with collectivization _among_ nations, cultural differences — rich and poor being only one axis — can easily exacerbate the cooperation that is required if collectivization is to work at all, much less with any measure of efficiency. The evidence suggests that the EU has gone too far in that direction. At the same time, the evidence is equally clear that the failure to make EU border security an EU responsibility, leaving it instead to individual members, has raised serious problems too.40



In America, border security became a federal government function once the Constitution was ratified. Within our borders, however, to keep states honest, the Founders instituted _competitive_ federalism, whereby states compete for the allegiance of citizens, and it’s largely worked as states with high taxes and excessive regulations lose firms and people to states with low taxes and reasonable regulations. People vote with their feet, musg as in the Schengen Area. But the federal income tax plus the direct election of senators, both enacted as constitutional amendments and both promoted by progressives, unleashed _cooperative_ federalism whereby federal and state officials collude, using federal funds and enacting federal regulations, to undercut state autonomy and the discipline that competitive federalism was meant to secure.41



Earlier I said that you can’t understand the American Constitution unless you understand the theory behind it. Well what’s the theory behind the treaties that comprise the EU Constitution? Peace through trade and cooperation, yes — given Europe’s long history of wars. But beyond that, what? We’ve seen how a radical shift in the climate of ideas in America, especially in the direction of collectivism, has led, as many lonely voices predicted, to a reaction that today reflects a deeply divided nation, unable to restrain its appetite for “free” goods and services, even in the face of crushing debt. The divisions surfacing recently in Europe are no accident. People and peoples yearn to breathe free — in an earlier understanding of that idea. The balance needed to ensure that freedom may be difficult to find. But to discover it, as we celebrate Italy’s Constitution today and reflect on Italy’s place within the larger European Community, we could do no better than to appeal to the First Principles that are the very foundation of civilized nations. Thank you.
"
"A Dutch government scientist has proposed building two mammoth dams to completely enclose the North Sea and protect an estimated 25 million Europeans from the consequences of rising sea levels as a result of global heating. Sjoerd Groeskamp, an oceanographer at the Royal Netherlands Institute for Sea Research, said a 475km dam between north Scotland and west Norway and another 160km one between west France and south-west England was “a possible solution”. In a paper to be published this month in the American Journal of Meteorology, Groeskamp and Joakim Kjellsson of the Geomar centre for ocean research in Kiel, Germany, say the idea is affordable and technically feasible – if intended more as “a warning of the immensity of the problem hanging over our heads”. Based on existing projects, the scientists estimate the cost of building a so-called North Sea Enclosure Dyke at between €250bn and €500bn. Spread over 20 years, the annual cost to the 14 countries that would be protected by it would amount to just over 0.1% of their combined GDP, they calculate. Groeskamp said it also appeared technically viable. The depth of the North Sea between France and England rarely exceeded 100 metres, he said, while between Scotland and Norway it averaged about 127 metres, peaking at just over 320 off the coast of Norway. “We are currently able to build fixed platforms in depths exceeding 500 metres, so such a dam seems feasible,” he said. International experts agreed that the plans looked theoretically viable. “I guess it depends on what timescale we’re thinking of,” said Hannah Cloke, a professor of hydrology at the University of Reading. “If you look back hundreds and hundreds of years, then we’ve made some significant adaptations to our landscape, and the Netherlands is an example of that … We can, as humans, do amazing things.” She added that it was “good that we’re thinking outside the box. I think it is really important that we keep thinking about these ideas, because the future looks very scary. If you look back into the 1940s in the UK, the Thames Barrier probably seemed equally ridiculous. It depends what happens in the next 20-30 years, how bad it gets, and then perhaps we will need something like this.” However, Cloke cautioned that a dam may not be the best use of the money. “Maybe we should be thinking about making populations resilient to flooding in different ways, and also think about what we can do to stop the climate getting worse – invest in keeping ourselves safe in the long term.” The authors acknowledge that over time, their project would eventually turn much of the North Sea into a vast tide-free freshwater lake, radically changing its ecosystem. “We estimated the construction costs by extrapolating the costs for large dams in South Korea,” Groeskamp said. “But in the final calculation, we must also take into account factors such as the loss of income from North Sea fishing, the increased costs for shipping across the North Sea, and the costs of gigantic pumps to transport all of the river water that currently flows into the North Sea to the other side of the dam.” However, the costs and consequences of doing nothing about rising sea levels would ultimately be “many times higher”, they warned. The project “makes it almost tangible what the consequences of rising sea levels will be”, Groeskamp said. “A rise of 10 metres by the year 2500 is predicted, according to the bleakest scenarios. This dam is therefore mainly a call to do something about climate change now. If we do nothing, this extreme dam might just be the only solution.” While experts have criticised Boris Johnson’s plan to build a 20-mile, £15bn bridge between Scotland and Northern Ireland as fraught with possibly insurmountable technical difficulties, the Dutch have been successfully protecting themselves against the sea with dykes for centuries. Their original 32km Enclosure Dyke, or Afsluitdijk, officially opened in 1933, sealed off a large North Sea inlet that became the freshwater IJsselmeer lake, and subsequently allowed the largest land reclamation project in history. The Zuiderzee works – of which the Afsluitdijk was a key part – and Delta works, a vast series of dams, sluices, locks and storm-surge barriers protecting the south-west Netherlands from the sea, are widely seen as marvels of hydraulic engineering. The Intergovernmental Panel on Climate Change predicts that sea levels will rise by 30cm-60cm by 2100, even if the Paris climate accord pledges are met. If emissions continue on their present trends, it foresees an 84cm rise by 2100 and up to 5.4 metres by 2300. The threat is particularly acute in the Netherlands, about a third of which lies below sea level. Last year, the Dutch government assembled a committee of specialists to monitor the threat closely and devise possible responses. The government has warned that while it expects its own defences to hold until about 2050, bolstering them further could take years: the past three decades of Dutch sea defence works are designed to cope with a rise of only 40cm. • This article was amended on 13 February 2020 to clarify that the estimated cost of the project as a percentage of the GDP of the countries affected is annual and spread over 20 years."
"
Title with apologies to Crosby, Stills, Nash, and Young.
In my last post, part 77 of “How not to measure temperature” I pointed out that the National Weather Service in Upton NY has a weather station that is way out of compliance due to the way it is setup and the proximity to bias factors such as the parking lot.
There are thousands of weather stations across the USA, some run by various agencies. Often we’ll see them at national parks with interpretive displays. This one I encountered in Ely Nevada on my last road trip to finish the Nevada USHCN station surveys was part of an air quality and environmental monitoring program jointly run by the Department of Energy (DOE) and the Desert Research Institute (DRI).
It is an impressive station with multiple state of the art sensors, solar power, and a datalogger with a satellite uplink to DRI’s HQ. You can look at hourly data from the station at the CEMP DRI website here.
It is located about 2 miles northeast of town on government property, BLM land:
Ely, NV Weather Station operated by DOE/DRI -click for larger image
What is unique about this station is that it has an interpretive exhibit with live data readouts. I applaud DRI/DOE for doing this. Here are what the they look like closeup:

Click for larger images to read the text on the interpretive displays
As I said, I applaud DRI/DOE for doing this. Taking the effort to make such a wonderful educational display is a good use of taxpayer funds.
Except, that is, when they miss one critical detail.
Ely, NV Parking Lot Education Weather Station operated by DOE/DRI - click for larger image
Yes, the expensive satellite uplinked state of the art interpretive educational weather station is sited in the middle of two asphalt parking lots. One is for RV storage, the other is the parking lot for the Ely District Office for the Bureau of Land Management.
Here is the the view to the northeast of how the temperature sensor sees the BLM land:
What the temperature sensor sees - click for a larger image
Here is the aerial view of the placement:
Aerial view - Ely, NV Weather Station operated by DOE/DRI -click for larger image
With the parking lots on both sides being active with cars and RV’s, I would imagine that a fairly variable albedo exists, especially on weekends and holidays.
This wouldn’t be so bad if it was only an educational station with an interpretive exhibit, as one could explain it was placed here for the convenience of viewing and science really doesn’t advocate measuring the temperature of parking lots.
Except that this station is used for an active science project. How much of the other data measurements and calculations for such things as Tritium dispersal, gaseous pollutant volumes, etc are dependent on the temperature, humidity, and dewpoint data gathered here, all of which would be affected by the siting?
Contrast it to the ASOS station siting at the airport across the road. The ASOS is about 1000 feet NW of the southern runway intersection which you can see here in Google Maps

Normally ASOS stations are much more poorly sited than state of the art stations, but this example  illustrates how spending tens of thousands of dollars on hi-tech measurement gear can be undone by lack of simple planning.
Happy Thanksgiving everyone!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ad5d699',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.   
  
_The Current Wisdom_ only comments on science appearing in the refereed, peer-reviewed literature, or that has been peer-screened prior to presentation at a scientific congress.   
  
**History to Repeat: Greenland’s Ice to Survive, United Nations to Continue Holiday Party**   
  
This year’s installment of the United Nations’ annual climate summit (technically known as the 16th meeting of the Conference of the Parties to the Framework Convention on Climate Change) has come and gone in Cancun. Nothing substantial came of it policy-wise; just the usual attempts by the developing world to shake down our already shaky economy in the name of climate change. News-wise probably the biggest story was that during the conference, Cancun broke an all time daily low temperature record. Last year’s confab in Copenhagen was pelted by snowstorms and subsumed in miserable cold. President Obama attended, failed to forge any meaningful agreement, and fled back to beat a rare Washington blizzard. He lost.   
  
But surely as every holiday season now includes one of these enormous jamborees, dire climate stories appeared daily. Polar bear cubs are endangered! Glaciers are melting!!   
  
Or so beat the largely overhyped drums, based upon this or that press release from Greenpeace or the World Wildlife Fund.   
  
And, of course, no one bothered to mention a blockbuster paper appearing in _Nature_ the day before the end of the Cancun confab, which reassures us that Greenland’s ice cap and glaciers are a lot more stable than alarmists would have us believe. That would include Al Gore, fond of his lurid maps showing the melting all of Greenland’s ice submerging Florida.   
  
Ain’t gonna happen.   




The disaster scenario goes like this: Summer temperatures in Greenland are warming, leading to increased melting and the formation of ephemeral lakes on the ice surface. This water eventually finds a crevasse and then a way down thousands of feet to the bottom of a glacier, where it lubricates the underlying surface, accelerating the seaward march of the ice. Increase the temperature even more and massive amounts deposit into the ocean by the year 2100, catastrophically raising sea levels.   
  
According to Christian Schoof of the University of British Columbia (UBC), “The conventional view has been that meltwater permeates the ice from the surface and pools under the base of the ice sheet….This water then serves as a lubricant between the glacier and the earth underneath it….”   
  
And, according to Schoof, that’s just not the way things work. A UBC press release about his _Nature_ article noted that he found that “a steady meltwater supply from gradual warming may in fact slow down the glacier flow, while sudden water input could cause glaciers to speed up and spread.”   
  
Indeed, Schoof finds that sudden water inputs, such as would occur with heavy rain, are responsible for glacial accelerations, but these last only one or a few days.   
  
The bottom line? A warming _climate_ has very little to do with accelerating ice flow, but _weather_ events do.   
  
How important is this? According to University of Leeds Professor Andrew Shepherd, who studies glaciers via satellite, “This study provides an elegant solution to one of the two key ice sheet instability problems” noted by the United Nations in their last (2007) climate compendium. “It turns out that, contrary to popular belief, Greenland ice sheet flow might not be accelerated by increased melting after all,” he added.   
  
I’m not so sure that those who hold the “popular belief” can explain why Greenland’s ice didn’t melt away thousands of years ago. For millennia, after the end of the last ice age (approximately 11,000 years ago) strong evidence indicates that the Eurasian arctic averaged nearly 13°F warmer in July than it is now.   
  
That’s because there are trees buried and preserved in the acidic Siberian tundra, and they can be carbon dated. Where there is no forest today—because it’s too cold in summer—there were trees, all the way to the Arctic Ocean and even on some of the remote Arctic islands that are bare today. And, back then, thanks to the remnants of continental ice, the Arctic Ocean was smaller and the North American and Eurasian landmasses extended further north.   
  
That work was by Glen MacDonald, from UCLA’s Geography Department. In his landmark 2000 paper in _Quaternary Research_ , he noted that the only way that the Arctic could become so warm is for there to be a massive incursion of warm water from the Atlantic Ocean. The only “gate” through which that can flow is the Greenland Strait, between Greenland and Scandinavia.   
  
So, Greenland had to have been warmer for several millennia, too.   
  
Now let’s do a little math to see if the “popular belief” about Greenland ever had any basis in reality.   
  
In 2009 University of Copenhagen’s B. M. Vinther and 13 coauthors published the definitive history of Greenland climate back to the ice age, studying ice cores taken over the entire landmass. An exceedingly conservative interpretation of their results is that Greenland was 1.5°C (2.7°F) warmer for the period from 5,000-9000 years ago, which is also the warm period in Eurasia that MacDonald detected. The integrated warming is given by multiplying the time (4,000 years) by the warming (1.5°), and works out (in Celsius) to 6,000 “degree-years.”   
  
Now let’s assume that our dreaded emissions of carbon dioxide spike the temperature there some 4°C. Since we cannot burn fossil fuel forever, let’s put this in over 200 years. That’s a pretty liberal estimate given that the temperature there still hasn’t exceeded values seen before in the 20th century. Anyway, we get 800 (4 x 200) degree-years.   
  
If the ice didn’t come tumbling off Greenland after 6,000 degree-years, how is it going to do so after only 800? The integrated warming of Greenland in the post-ice-age warming (referred to as the “climatic optimum” in textbooks published prior to global warming hysteria) is over seven _times_ what humans can accomplish in 200 years. Why do we even worry about this?   
  
So we can all sleep a bit better. Florida will survive. And, we can also rest assured that the UN will continue its outrageous holiday parties, accomplishing nothing, but living large. Next year’s is in Durban, South Africa, yet another remote warm spot hours of Jet-A away.   
  
References:   
  
MacDonald, G. M., et al., 2000. Holocene treeline history and climatic change across Northern Eurasia. _Quaternary Research_ **53** , 302-311.   
  
Schoof, C., 2010. Ice-sheet acceleration driven by melt supply variability. _Nature_ **468,** 803-805.   
  
Vinther, B.M., et al., 2009. Holocene thinning of the Greenland ice sheet. _Nature_ **461** , 385-388.


"
"
A Guest Post By Steve Goddard
In my most recent article in The Register, and also posted here on WUWT, I incorrectly speculated  that NSIDC graphs appeared to show less growth in Arctic ice  extent than had actually occurred.  My calculations were based on counting ice  pixels from Cryosphere Today maps.  Since then, I have had further discussions  with Dr. Walt Meier at NSIDC and William Chapman at Cryosphere Today, to try to  understand the source of the problem.   Dr. Meier has confirmed that counting  pixels provides a “good rough estimate” and that NSIDC teaches pixel counting to  CU students as a way to estimate ice extent.  William Chapman has confirmed that  the projection used in CT maps is very close to what it appears and to what I  had assumed it to be.  It is an astronaut’s view from about 5,000 miles above  the north pole.
What I have learned
In 2008, CT and NSIDC maps show excellent agreement – as can be seen  in this video which overlays an  August 14 NSIDC map on top of the August 14 CT map.  The borders of ice extent  are nearly identical in the two maps. (The videos show overlays of the two maps.)

The discrepancy occurred in August, 2007, when agreement between  NSIDC and CT was not so good.  The equivalent video from August 15, 2007  shows that the CT map was missing a significant amount of low concentration ice  on the Canada/Alaska side.  I have since confirmed from AMSR maps and NASA  satellite photos that the NSIDC map is probably more accurate than the CT  map.
The reason that CT provides their side-by-side image viewer is apparently to encourage visitors to make a visual comparison of two dates,  which is exactly what myself and others here did when we observed the  discrepancy vs. NSIDC graphs.  The human brain is quite good at making estimates  of relative areas from images, and pixel counting is nothing more than a way to  quantify what has already been observed.  Since writing The Register piece I  have made adjustments to the CT pixel counts for map distortion, and as I  expected that makes the discrepancy slightly larger.

Because CT maps showed less ice in 2007, the increase in 2008 ice extent appears to be much greater. There is little doubt now  that the NSIDC reported ice growth is absolutely correct.  But  wasn’t the ice supposed to shrink this year due to an excess of  “thin first-year ice?”  In May, NSIDC’s mean forecast (based on previous year’s melt) was that Arctic ice extent would be 13% lower  than last year.  (NSIDC has more recently posted on their web site some reasons  why they believe the May estimates didn’t work out.)

Click for a larger image
The next graph shows the NSIDC May  forecast superimposed on the AMSR graph of what has actually happened this year.  Ice extent has increased rather than decreased, and is not  tremendously lower than most other years this decade.   Note that the AMSR and  NSIDC graphs bottom out at 2Mkm2, not zero, which creates the visual impression  that ice extent is lower than it actually is.

Click for a larger image
Looking at the NSIDC map below, several things are apparent.
Firstly, the widely  publicised news stories predicting a possible “ice free  Arctic,” an “ice free North  Pole,” and a record  low extent aren’t likely to happen. (See this WUWT post with a quote from Dr. Meier) The North Pole is nearly as far away  from ice free water as in any other year.  In order to get to ice free water  from the pole, one would have to travel over 500 miles of ice to near Svalbard.   Lewis Pugh’s kayak trip, as reported by the BBC August 30 – “Swimmer aims to kayak to N  Pole” is going to be an impossible one.  That critical error didn’t stop the  BBC from highlighting it on the front page of their web site this weekend.
Secondly, while Arctic ice is well below the 30 year mean,  it is above expectations and nowhere near gone.  NSIDC graphs show Arctic ice  extent at greater than 70% of normal – hardly a six sigma event.  As of August  1, NSIDC was even considering a possible return to normal extent this summer.  If not for a few weeks of stormy Siberian  weather, the map (and story) would be quite different today.

There has been lots of press publicity  for predictions of an ice free Arctic by  the year 2013 or within the next  ten years.  Looking at the CT ice area graph below, that would clearly  require some major non-linear changes to the 30 year trend.

Click for a larger image
So how do we know if the trend is linear,  exponential or sinuous? As seen below, many long-term GISS Arctic temperature  records (except for stations close to the Bering Strait) show the last 30 years  as being the warming leg of a possibly cyclical pattern – with current  temperatures no warmer than 70 years ago.  Most stations close to the Bering  Strait show a sharp upwards shift of several degrees (corresponding to the PDO  shift) in 1976, and relatively flat temperatures since.  University of Alaska data shows that on average, state temperatures have been nearly flat since 1977.  In  fact, parts of Alaska are coming off one of their coldest  summers on record – possibly corresponding to another shift of the PDO to  pre-1976 conditions.
Is it possible that the 30 year  satellite record coincidentally represents only one leg of a waveform?   Greenland temperature records would hint at that.  If you examine only one leg  of a waveform, you will absolutely come to the wrong conclusion about the long  term behaviour – just as some did during the 1970s ice age panic.

Click for a larger image – original source image from NASA GISS

We know from official US Weather Bureau  records that it was possible to sail as far as 81N latitude in ice-free water, during a  similar warm period in 1922.  That is about as close to the pole as you can sail  now.  Temperatures around Spitzbergen, Norway warmed a remarkable 12C during a  few years prior to 1922.  From the November, 1922 Weather Bureau report – “He says that he first noted wanner conditions in 1915, that since that  time it has steadily gotten warmer, and that to-day the Arctic of  that region is not recognizable as the same region of 1865 to 1917.“



Is there cause for concern?   Perhaps.  But unfortunately much of the press coverage has been little more than  science fiction so far.  How do we separate the science from the fiction?  Dr.  Meier has graciously agreed to answer that question (and others) in my next  article.
One thing we can state with a degree of  certainty, is that there likely will be more multi-year ice in 2009 than there  was in 2008.  This is  because the 2008 melt season is ending with more ice area  than 2007.  Barring asteroid impact, after the winter freeze there will be (by  definition) more multi-year ice than what we started with this year.  Any ice  which survives the summer will be classified as multi-year ice in 2009.  If next  year is cool like this year, is it unreasonable to hypothesise that ice extent  will again increase?  Or are we on a non-linear trend which will lead to  ice-free summers and a collapsing Greenland ice sheet?  Hopefully Dr. Meier can  help sort this out for us.







			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cc3e7b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
For Europe, the huge welcoming applause Palin got after being introduced just before her speech was loud thunder accompanied by tectonic tremors. The signs for catastrophic (political) climate change are clearer than ever.

UPDATED 8/29 12.15 p.m.This blog is supposed to focus on climate and energy, and not politics. But the future of both issues are tangled up in politics, and so it is unavoidable.
Make no mistake about it. The ruling elitists in Europe and Germany are about as far from America’s foundational principles as they have ever been. Looking at the German media headlines yesterday, Beck’s rally was labelled as a gathering of “ultra-conservatives”, “religious right”, “God-fearing, gun-toting Americans”-patriotic folks in T-shirts and caps who chant “USA! USA!” and “God bless America!”. It’s just so unsophisticated – so say the euro-elitists through their noses. Europe’s elite media are marginalising and villainizing American conservatism. Call it the Cold War of the 21st Century. That’s the reality.
The euro-media of course would never broadcast the speeches, fearing their listeners might draw the “wrong conclusions”. So the media select the pictures they think their viewers ought to see and deliver pre-packaged thoughts and commentary to tell them how to think. It’s the responsible way of informing citizens.
Europe’s spite for original American ideals is deep.
Should someone like Palin take the Presidency in 2012, then prepare for headlines from Europe that normally would be reserved for history’s worst dictators. It’ll be “freeze it, personalise it and polarize it” in über-doses. In secular environmental Europe, things like Christianity, God, freedom, small government, life, capitalism are extreme, and thus talk about such things must be reigned in and controlled by the heavy hand of the state and media.
America itself is deeply divided, and it’s going to stay that way. And there’s no question which side Europe’s elites in media and government align themselves with. It’s the believers of the USA’s Constitution, i.e. conservatism, against the rest of the world. It really is that precarious. Those who believe in the founding principles of America have very few allies left in the world.
The differences between the American right and European left (the American left is the European left) are now far beyond reconciliation. The two sides could not be more polarised. What’s the way out? What can be done when reconciliation has no chance? The American right, the believers of its Constitutional principles, are alone on the globe. Do you compromise? Do you try to change the mind of the rest of the world? Conflict and uprising seem to be pre-programmed.
Although American conservatism appears to be making a comeback now, the reality is that it is completely surrounded on all fronts, almost hopelessly outnumbered, with very few political weapons. The NGOs, the UN, the elite media, academia, Hollywood, Russia, China, Europe, the Third World, the “environmentalists”, the billionaires, the special interests, the powerful US Left and all dictators are arrayed against American conservatism. People are to be ruled by governments, and not vice versa.
As President, who could someone like Palin possibly find as an ally? Other than Poland, the Czech Republic, Israel and Georgia, you’d be hard pressed to name one that’s influential. I’m not saying Palin should not be elected for that reason, I’m just telling you what the world is like for the last bastion of American ideals.
The odds are very long.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGuest writer Ed Caryl, author of One Of Our Hemispheres Is Missing and A Light In Siberia, now brings us his latest essay.
——————————————————————————————————————–
You Want Me To Believe What?
By Ed Caryl
The proponents of Anthropogenic Global Warming claim that man’s use of fossil fuels has released extra carbon dioxide (CO2) into the atmosphere, and that the extra CO2 is warming the earth catastrophically due to greenhouse effects, causing great “disruption” in climate.
There are many parts to this hypothesis. The statement starts out with a truth, and then gets hazier as it progresses, each step suggesting an increasing level of calamity, but a decreasing level of believability. So let us look at the claims one by one:

Man’s use of fossil fuels has released extra CO2 into the atmosphere.

Stipulated. The increase since the dawn of the industrial age is nearly 100 ppm.

The earth is warming in a catastrophic way.

Since the Maunder Minimum, the earth has warmed by perhaps 1°C. There is good evidence that any measurement of more warming than that has been tampered with or subject to confirmation bias. Only warming in the last 100 years, and in particular, in the last 50, can possibly be due to greenhouse effects. Supposedly, any AGW will be seen first in the Arctic. Yet, many Arctic weather stations show no warming.
The warming is due to greenhouse effects
Knut Ångstrom. Source: http://www.angstrom.uu.se/historia.php
There are nearly as many numbers cited for what a doubling of CO2 will do as there are scientists working in the field. The most often cited expert on the subject is Svante Arrhenius, even though he was a physical chemist, not an atmospheric scientist, lived and worked a hundred years ago, and considered atmospheric science a hobby. He published the first numbers in 1896, 4.7 to 6°C. These numbers were criticized by Knut Ångstrom (one of the first true atmospheric scientists) in 1900 as being much too high. Later, in 1906, Arrhenius adjusted that number downward to 1.6°C. A hundred years ago, there was no consensus, even in one man’s head. Yet today, Arrhenius’ first numbers are the ones most often cited, and Ångstrom’s criticism is forgotten.
The value is estimated by the IPCC Fourth Assessment Report (AR4) as likely to be in the range 2 to 4.5°C with a best estimate of about 3°C, but is very unlikely to be less than 1.5°C. Values substantially higher than 4.5°C cannot be excluded, but agreement of models with observations is not as good for those values.[1]
Other estimates range from 0.4°C to as high as 10°C. The later number seems obviously too high, given the current lack of warming, and even the IPCC seems to agree. Another, more complete list of estimates is here. Given the amount of confirmation bias among the workers in this field, it is surprising that the average estimate is still less than 3°C. Arrhenius’ 1906 number, 1.6°C, may end up closest to the truth.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The point here is that there is wide disagreement on what the affect of doubling CO2 will do. The numbers cover a range of more than 20. None of these numbers can be more than guesses, and they could all be wrong.
Oh, and other tiny points: Who says CO2 is going to double? And why?
And what is the role of water? Water vapor is a greenhouse gas also. And water, unlike CO2, exists in all three phases. As water changes phase, it takes up and gives up heat. I suspect that Dr. Michael Mann has never seen a southwest desert thunderstorm. Under those towering thunderheads, the temperature can drop from 40°C to 10°C,  in just a few minutes, accompanied by heavy rain and hail. Cubic miles of hot air rise in those clouds, expanding and cooling as it rises. The rising air gives up heat to condensing water droplets, and then more heat to fusing hailstones. All that heat is transferred ultimately to the top of the clouds to be radiated to space. This goes on 24/7, all around the world. Is this factored into the climate models?
AGW will cause great climate disruption
Here, the proof is in. It’s not happening! This is the core of the whole debate. James Hansen predicted in 1988 that the West Side Highway in New York City would be under water by 2008. The Battery tide gauge shows a two-inch rise, and the rise has been linear since the gauge was installed in 1856. Worldwide sea levels show the same modest rise at the same linear rate. If Michael Mann’s “hockey stick” temperature rise was true, we should see the same sharp rise in sea level. We do not.
Tornados – also not happening. The peak years were in the early 1970’s, and it has been downhill since.
Hurricanes? This is a bit tricky. Before the satellite era, hurricanes had to have been spotted by ship or plane. The Hurricane Hunter aircraft beginning in WWII covered ocean areas of interest to the military, but the Hurricane formation regions off the African coast were not watched until later. So we don’t have accurate counts until satellites watched everywhere, consistently. We do have some records of hurricanes that made landfall. Here is one long-term record for the Apalachee Bay, Florida. It shows a hurricane frequency peak 2500 years ago and another during the Medieval Warm Period. But it shows no increase in the present. Tropical cyclone formation is driven by sea-surface temperature, so it is reasonable to assume that tropical cyclone formation will be a function of ocean temperature cycles, such as the AMO and the Southern Oscillation. Currently, the Tropical Cyclone Energy is near the 30-year low. This is a point that Al Gore has given up, and he has removed this slide from his presentation.
What about drought, floods, heat waves, cold snaps, insect infestations and other Biblical plagues? All (except the first-born son one, unless you count the 10:10 video) have been mentioned, and all blamed on AGW. The problem is that at any given moment, somewhere in the world, a record is being broken. This is just statistics in action. Any noisy phenomenon, given enough time and space to act, will produce the occasional exceptional spike, but the record phenomenon have no pattern in time.
For a branch of science to have any validity, it must be testable. Tests of a science include: Does it make predictions that can be verified? Do those predictions match observations? So far, the predictions have not come to fruition.

The North Pole has not become ice-free
The South Pole ice is expanding
The icecaps at the poles are not collapsing. One iceberg doesn’t make a collapse.
The oceans are not flooding land anywhere.
The deserts are not expanding
The polar bears are doing just fine, thank you
So are the penguins, (except in South Africa, where they froze last winter).

So what is it again we’re supposed to believe?
And why?
Share this...FacebookTwitter "
nan
"
Solar cycle 23 as seen from SOHO - click for larger image
Below is a note forwarded to me by John Sumption from Jan Janssens. For those who do not know him, Jan runs a very comphrehensive solar tracking website here.
Jan included the caveat:
This  topic’s sure to start another heated discussion on the solar blogs 
So I’m happy to oblige by posting it here. Jansen makes some good points about the possible first month that cylce 24 spots exceed cycle 23 spots. But when you are in a deep minimum like this one, it is hard to pinpoint the transition, because next month may bring the reverse condition. He writes:
Prior  to August 2008, only 3 SC24-sunspot groups appeared. This was in January, April  and May. During these 3 months, SC23-activity was higher than SC24-activity.  Based on the NOAA-numbering, there were respectively (SC23 to SC24) 2 to 1, 2 to  1, and 4 to 1 sunspotgroups visible. 
In  August, there were no sunspotgroups numbered by NOAA. However, on 21-22 August  “something” was visible well enough to be seen by several observers and to  prompt the SIDC to give a (preliminary) non-zero sunspotnumber for those days. 
This group had a SC24-polarity but appeared on a moderate latitude of 15  degrees. Based on previous cycle transits, it is not unusual that some “early”  new cycle groups appear this low. If one considers this as a sunspotgroup and  belonging to SC24, then August was the month during which SC24-activity  outnumbered SC23 activity. 
However, if one adheres strictly to the NOAA-numbering, then September  ***might*** be that month. I stress “might”, because -unless some group appears  tomorrow or tuesday- the score will still be 1 to 1: On September 11th, NOAA did  number an even tinier group than the August one, and it was a SC23 group (NOAA  1001). SC24-activity then wins on “points”, because the Wolfnumbers for 22-23  September produced by NOAA 1002 (SC24) were higher than the NOAA 1002  Wolfnumber. 
Last  but not least, I want to emphasize that SC24-activity will be considered higher  than SC23’s when its smoothed group (or Wolf) number exceeds that of the old  cycle. This might happen in the coming months (or whenever her Majesty the Sun  feels up to it 😉


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c69f706',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.   
  
_The Current Wisdom_ only comments on science appearing in the refereed, peer-reviewed literature, or that has been peer-screened prior to presentation at a scientific congress.   
  
**  
Better Model, Less Warming**   
  
Bet you haven’t seen this one on TV: A newer, more sophisticated climate model has lost more than 25% of its predicted warming! You can bet that if it had predicted that much more warming it would have made the local paper. __   
  
The change resulted from a more realistic simulation of the way clouds work, resulting in a major reduction in the model’s “climate sensitivity,” which is the amount of warming predicted for a doubling of the concentration of atmospheric carbon dioxide over what it was prior to the industrial revolution.   
  
Prior to the modern era, atmospheric carbon dioxide concentrations, as measured in air trapped in ice in the high latitudes (which can be dated year-by-year) was pretty constant, around 280 parts per million (ppm). No wonder CO2 is called a “trace gas”—there really is not much of it around.   




The current concentration is pushing about 390 ppm, an increase of about 40% in 250 years. This is a pretty good indicator of the amount of “forcing” or warming pressure that we are exerting on the atmosphere. Yes, there are other global warming gases going up, like the chlorofluorocarbons (refrigerants now banned by treaty), but the modern climate religion is that these are pretty much being cancelled by reflective “aerosol” compounds that go in the air along with the combustion of fossil fuels, mainly coal.   
  
Most projections have carbon dioxide doubling to a nominal 600 ppm somewhere in the second half of this century, absent no major technological changes (which history tells us is a very shaky assumption). But the “sensitivity” is not reached as soon as we hit the doubling, thanks to the fact that it takes a lot of time to warm the ocean (like it takes a lot of time to warm up a big pot of water with a small burner).   
  
So the “sensitivity” is much closer to the temperature rise that a model projects about 100 years from now – assuming (again, shakily) that we ultimately switch to power sources that don’t release dreaded CO2 into the atmosphere somewhere around the time its concentration doubles.   
  
The bottom line is that lower sensitivity means less future warming as a result of anthropogenic greenhouse gas emissions. So our advice… keep on working on the models, eventually, they may actually arrive at something close puny rate of warming that is being observed   
  
At any rate, improvements to the Japanese-developed Model for Interdisciplinary Research on Climate (MIROC) are the topic of a new paper by Masahiro Watanabe and colleagues in the current issue of the _Journal of Climate_. This modeling group has been working on a new version of their model (MIROC5) to be used in the upcoming 5th Assessment Report of the United Nations’ Intergovernmental Panel on Climate Change, due in late 2013. Two incarnations of the previous version (MIROC3.2) were included in the IPCC’s 4th Assessment Report (2007) and contribute to the IPCC “consensus” of global warming projections.   
  
The high resolution version (MIROC3.2(hires)) was quite a doozy – responsible for far and away the greatest projected global temperature rise (see Figure 1). And the medium resolution model (MIROC3.2(medres)) is among the Top 5 warmest models. Together, the two MIROC models undoubtedly act to increase the overall model ensemble mean warming projection and expand the top end of the “likely” range of temperature rise. 



FIGURE 1





Global temperature projections under the “midrange” scenario for greenhouse-gas emissions produced by the IPCC’s collection of climate models. The MIROC high resolution model (MIROC3.2(hires)) is clearly the hottest one, and the medium range one isn’t very far behind.   
  
The reason that the MIROC3.2 versions produce so much warming is that their sensitivity is very high, with the high-resolution at 4.3°C (7.7°F) and the medium-resolution at 4.0°C (7.2°F). These sensitivities are very near the high end of the distribution of climate sensitivities from the IPCC’s collection of models (see Figure 2). 



FIGURE 2





Equilibrium climate sensitivities of the models used in the IPCC AR4 (with the exception of the MIROC5). The MIROC3.2 sensitivities are highlighted in red and lie near the upper und of the collection of model sensitivities. The new, improved, MIROC5, which was not included in the IPCC AR4, is highlighted in magenta, and lies near the low end of the model climate sensitivities (data from IPCC Fourth Assessment Report, Table 8.2 and Watanabe et al., 2010).   
  
Note that the highest sensitivity is not necessarily in the hottest model, as observed warming is dependent upon how the model deals with the slowness of the oceans to warm.   
  
The situation is vastly different in the new MIROC5 model. Watanabe _et al_. report that the climate sensitivity is now 2.6°C (4.7°F) – more than 25% less than in the previous version on the model.[1] If the MIROC5 had been included in the IPCC’s AR4 collection of models, its climate sensitivity of 2.6°C would have been found near the low end of the distribution (see Figure 2), rather than pushing the high extreme as MIROC3.2 did.   
  
And to what do we owe this large decline in the modeled climate sensitivity? According to Watanabe _et al._ , a vastly improved handling of cloud processes involving “a prognostic treatment for the cloud water and ice mixing ratio, as well as the cloud fraction, considering both warm and cold rain processes.” In fact, the improved cloud scheme—which produces clouds which compare more favorably with satellite observations—projects that under a warming climate low altitude clouds _become a negative feedback_ rather than acting as positive feedback as the old version of the model projected.[2] Instead of enhancing the CO2-induced warming, low clouds are now projected to retard it.   
  
Here is how Watanabe _et al_. describe their results: 



A new version of the global climate model MIROC was developed for better simulation of the mean climate, variability, and climate change due to anthropogenic radiative forcing….   
  
MIROC5 reveals an equilibrium climate sensitivity of 2.6K, which is 1K lower than that in MIROC3.2(medres).... This is probably because in the two versions, the response of low clouds to an increasing concentration of CO2 is opposite; that is, low clouds decrease (increase) at low latitudes in MIROC3.2(medres) (MIROC5).[3]



Is the new MIROC model perfect? Certainly not. But is it better than the old one? It seems quite likely. And the net result of the model improvements is that the climate sensitivity and therefore the warming projections (and resultant impacts) have been significantly lowered. And much of this lowering comes as the handling of cloud processes—still among the most uncertain of climate processes—is improved upon. No doubt such improvements will continue into the future as both our scientific understanding and our computational abilities increase.   
  
Will this lead to an even greater reduction in climate sensitivity and projected temperature rise? There are many folks out there (including this author) that believe this is a very distinct possibility, given that observed warming in recent decades is clearly beneath the average predicted by climate models. Stay tuned!   
  
References:   
  
Intergovernmental Panel on Climate Change, 2007. Fourth Assessment Report, Working Group 1 report, available at http://www.ipcc.ch.&#13;  
  
Watanabe, M., et al., 2010. Improved climate simulation by MIROC5: Mean states, variability, and climate sensitivity. _Journal of Climate_ , **23** , 6312-6335.   

"
"
More Signs of the Apocalypse
From Medindia.com
Posted online: Friday, August 29, 2008 at 2:48:25 PM
Report Confirms Four Austrians Suffer Tick-borne Encephalitis from Cheese
Medical experts confirmed on Thursday that four people recently fell ill with tick-borne encephalitis (TBE) in western Austria after eating homemade goats’ cheese.
A shepherd in western Vorarlberg province, who had checked into hospital in July with flu-like symptoms, was found to have the illness following a blood test.
But the man said he had noticed no tick bites, the usual method of transmission, two experts from the Institute of Virology at Vienna Medical University wrote in an article published Thursday.
Doctors finally traced the cause of the illness to the cheese, which the shepherd had made from unpasteurised goat’s and cow’s milk on an isolated pasture at over 1,560-metre (5,120-feet) altitude.
Three other members of his family, who had not been on the pasture, also exhibited flu-like symptoms and headaches.
Further tests found that one of the goats, whose milk had been used to make the cheese, as well as other animals who had eaten leftovers, had developed TBE anti-bodies, meaning they had also been infected.
Ticks were believed until now to be found only below 1,350-metre altitude, but this may have changed due to global warming, the experts said.
Cases of TBE infections via dairy products were reported in recent years almost exclusively in Baltic countries.
Unfortunately, the article does not say just who the experts are.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d00014b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Ukraine crisis caused relations between Russia and the EU to fall to their lowest point since the Cold War. But despite the bickering and outright conflicts, both still need each other: Europe relies on Russian gas to keep warm, and Russia in turn needs revenues. With winter on its way and capital flight from Russia reaching dangerous levels, the outlook should draw the EU and Russia back together. Gazprom is the only supplier of Russian natural gas to Europe. In 2013, it exported 161.5 billion cubic meters (bcm) of gas to the rest of the continent (Germany, Turkey and Italy were the biggest recipients). That works out to around 30% of Europe’s consumer market. This in turn means European consumers are responsible for around 40% of Gazprom’s annual revenues.  Russia currently exports gas to Europe through five routes: via Ukraine (supplying Slovakia, the Czech Republic, Hungary, and feeding into western Europe, as well as to south-eastern Europe and Turkey); via Belarus to the Baltic states, Poland and Germany; via the Blue Stream pipeline directly into Turkey; and via Nord Stream directly to Germany and Finland. Both parties are interdependent and should pull together an action plan for cooperation to gain from their relationship. However, dependencies vary from country to country. Most nations in eastern Europe rely heavily on Russian gas, while others, such as Spain, Portugal, Ireland or Denmark, don’t import any at all. The most precarious countries are those which rely totally on imports of gas from Russia specifically via Ukraine. Bulgaria, for instance, is particularly at risk. It has very small conventional gas reserves (5bcm per 2010 statistics) and imports 94% of its domestic consumption, it has very little storage capacity, and relies heavily on Russian gas imported through Ukraine. Moldova is highly vulnerable too. After disruptions to the supply of gas from Russia in 2006 and 2009, the EU aimed to wean itself off Russian gas. Connections between national gas grids were put in place, giving countries a back-up plan in case of further disruption.  New suppliers were also sought to replace Russia. In 2013 Europe imported 48.5 bcm of natural gas from North Africa, 13 bcm from Iran and Azerbaijan, and 46 bcm in liquefied natural gas (LNG), mainly from Qatar.  But to put these numbers in context, this year alone Gazprom plans to sell 157 bcm to Europe. All of Europe’s various options for new energy supply have their own problems. The US’ large shale gas reserves could end up heating Europe homes, however LNG terminals must be built on both continents before large amounts can be shipped across the Atlantic. This is unlikely to happen in the near future. It means high investment costs, which would be passed on to European consumers and industries – the EU might not like Russia, but it still likes cheap energy. Gas from Africa and the eastern Mediterranean could help, but this would require billions to be spent on new drilling, refining, ports and security. A proposal to bring gas from the Middle East and the Caspian Sea region to Europe via Turkey will become a reality by 2018-2020. This southern corridor is another option, but again there are lots of logistical and security hurdles to overcome. In reality, Europe’s options are limited. A more diversified supply is still some way off. The EU’s supply contracts are long term, and this doesn’t allow to make short-term changes that may account for political factors – Russia is guaranteed stable gas sales until at least 2022. Individual countries can’t even do much about it by themselves as a 1998 EU directive restricts the power of national governments. Italy couldn’t suddenly sign a big deal with Namibia, for instance, without first going through EU channels.  Since the USSR broke up, Russia has wanted to diversify its own exports, reducing its reliance on gas sent to Europe through the Ukraine. Thanks to new pipelines across Belarus and under the Black Sea, this is already happening: in the 1990s around 90% of gas exports from Russia to Europe went through Ukraine Pipelines. This was down to 70% in 2007 and to 50% in 2013.  Once the South Stream pipeline through Bulgaria is built, reliance on exports through the Ukraine will decrease even further, but will still remain significant.  For Russia, the obvious long-term solution is to find alternative gas buyers elsewhere in the world. This explains the recent Russia-China energy deals. However, Europe still demands more energy than emerging economies and has better connections with Russia. For the moment, despite Russia’s best efforts, Europe is the main source of Gazprom’s revenue and profits. Russia’s efforts to diversify away from Euros and Dollars are unlikely to take off in the short run.  The EU and Russia therefore need to get together in the short and medium future – before new suppliers or supply routes can be developed – to safeguard energy security for Europe and revenues for Russia. This mutual interdependency could eventually draw Europe and Russia back together, despite all the political rhetoric."
"The asteroid that wiped out the dinosaurs set off an intense heat wave that briefly boiled the Earth’s atmosphere – but it didn’t burn off all the plants. Humanity has not been unlucky enough to observe at first hand the effects of a large impact, so to investigate whether a massive asteroid would spark off a global wildfire we had to turn to the laboratory. We have modelled, for the first time, the heat generated by the impact and what it meant for the planet’s plants. Our research is published in the Journal of the Geological Society. This all happened 65m years ago at the end of the Cretaceous period, when dinosaurs still roamed the earth. Suddenly, between 60 and 80% of all living species became extinct. Until the 1980s, this catastrophic loss of life was a mystery, but then scientists found a clue – traces of the element iridium in rocks of this age. Iridium generally falls to Earth with extraterrestrial objects. This suggested a massive asteroid collided with the planet and that this could be responsible for the mass extinction. Ten years later, scientists found the 65m year-old, 180km wide, Chicxulub crater on the Yucatan Peninsula in Mexico, which finally provided the smoking gun that could explain the apparent chaos at the end of the Cretaceous era. A crater that wide suggests that an asteroid or comet approximately 10km wide hit the Earth.  The impact would have released a huge amount of energy – equivalent to more than a billion Hiroshima bombs. The asteroid itself was vaporised as it smashed into the Earth and in doing so vaporised and blasted out particles of the rock that it hit.  A huge glowing ball of hot rock and vapour rushed up from the impact site at huge speeds, ejecting it way above the atmosphere up into space. As it hit the cold of space it decelerated, cooled and rained back through the atmosphere, re-solidifying and forming tiny droplets of rock known as “spherules”.  As these spherules fell through the atmosphere they were subject to the same frictional-drag which causes space shuttles to become super-heated as they return to earth. This in turn meant as the particles rained down through the atmosphere they delivered a massive blast of heat to the ground. Scientists call this a “thermal pulse”. This heat pulse has widely been suggested to have ignited global wildfires and has been cited as a cause of the mass extinction. New computer modelling techniques have enabled us to generate better estimates of the heat pulse resulting from this impact. We found it wasn’t evenly distributed across the surface of the Earth. Areas close to the impact site experienced a strong but very short-lived pulse – reaching a peak heat flux of around 50kW/m2 (20 times higher than a human can tolerate) for around one and a half minutes. Further away, the maximum heat flux was lower – a peak of 20kw/m2 – but lasted much longer – up to seven and a half minutes. We teamed up earth scientists with fire safety engineers to investigate whether this heat would lead to a massive global wildfire. The heat pulse from the asteroid impact was recreated using state-of-the-art apparatus usually used to test the flammability of furnishings and materials. This provided, for the first time, the ability to test whether the heat pulse from the impact could have set fire to the world’s plants. Our research reveals that the short sharp blast of heat felt closer to the impact could not have ignited live plants.  However the longer drawn out pulse further away from the impact may have started fires in some locations, implying that localised fires may have occurred. But critically “global firestorms” were unlikely. This turns our understanding of the heat pulse on its head as its effects would have been greater further away from the impact. Earth scientists will have to reassess their understanding of the fossil record of life. Until now they have assumed the heat pulse was strongest closer to the crater, but now patterns of extinction and survival must be reinterpreted by considering a more severe heat pulse further away."
"
Share this...FacebookTwitterThe blogosphere is now filled wth Arctic sea ice projections and opinions, and why not? It’s a game that adds fun to the science. For example Steve Goddard at WUWT projects 2010 will finish above 5.5 million sq km.
My forecast remains unchanged. 5.5 million, finishing above 2009 and below 2006. Same as it has been since May.
August 22, 2010: Source: UI Atmos-Cryosphere
I’d be happy if that became true because it would be real sand in the gears of the death-spiral worshippers. But seriously I doubt it will keep above or at the 5.5 million mark. That’s a real long shot.

Maybe a different dataset is being used for this claim. The shot above shows there is still plenty of potential for melt.
Weeks ago I projected, using my crystal ball, that we would finish a bit below 5 million. That’s turning out to be a wee bit too pessimistic, but not too much though.
On August 22, Arctic sea ice according to JAXA was at 5.628 million sq km. That means only 128,000 sq km over the 5.5 million mark with still about 20 days left before reaching the average low point. That’s a lot of time.
August 22, 2010
Over the last 8 years we have seen that from August 22 until reaching the low point, Arctic sea ice has lost an average of 535,000 sq km.
The minimum loss for that period was 279,000 sq km in 2006 and the maximum was 837,000 in 2008. Over the last 4 years, the average was a loss of 582,000 for the period.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Taking the average of the last 4 years, and we get a 2010 minimum ice extent of 5.628 – 0.582 = 5.04 million sq km. So, forget the 5.5 million mark being the minimum. It aint gonna happen.
The Arctic is not projected to get very cold in the next two weeks, see here: http://wxmaps.org/pix/temp2.html.
The temperature above 80°N latitude is also staying stubbornly near the freezing point. So expect Arctic sea ice, using JAXA, to get down closer to the 5 million mark.

Source: Danish Meteorological Institute.
Yes, of course, there are lots of other factors, wind, blah blah, blah, but only 100,000 sq. km of melt over the next 20 days could only happen if Al Gore showed up at the North Pole.
UPDATE: Steve Goddard at WUWT has backed off his 5.5 million sq km projection. http://wattsupwiththat.com/2010/08/24/sewa-ice-news-arctic-mid-week-update/
 
Share this...FacebookTwitter "
"

Well, Paul Krugman sure smeared me in his May 29 column (sub. req’d.) where he accused me of “fraud pure and simple” in congressional testimony eight (!) years ago.   
  
  
Krugman’s screed was just another salvo in the current global warming charm offensive, coinciding with Al Gore’s screeching movie, demonstrations against Max Mayfield, director of the National Hurricane Center, because he had the audacity to NOT blame last year’s Hurricane Katrina on global warming (which would have been “fraud pure and simple”), and multiple smearings of any climate scientist who dares to speak out against the current hysteria.   
  
  
Krugman was incensed with my July 27, 1998 testimony before the House Committee on Small Business. In it, my purpose was to demonstrate that commonly held assumptions about climate change can be violated in a very few short years.   
  
  
One of those is that greenhouse gas concentrations, mainly carbon dioxide, would continue on a constant exponential growth curve. NASA scientist James Hansen had a model that did just this, published in 1988, and referred to in his June 23, 1988 Senate testimony as a “Business as Usual” (BAU) scenario.   
  
  
BAU generally assumes no significant legislation and no major technological changes. It’s pretty safe to say that this was what happened in the succeeding ten years.   
  
  
He had two other scenarios that were different, one that gradually reduced emissions, and one that stopped the growth of atmospheric carbon dioxide in 2000. But those weren’t germane to my discussion. Somehow, Krugman labelled my not referring to them as “fraud.”   
  
  
The BAU scenario produced a whopping surface temperature rise of 0.45 degrees Celsius in the short period from 1988 through 1997, the last year for which there was annual data published by the United Nations’ Intergovernmental Panel on Climate Change at the time of my testimony. The observed rise was 0.11 degrees.   
  
  
I cited the reasons for this. In fact, the rate of carbon dioxide increase in the atmosphere was quite constant–rather than itself increasing like compound interest–during the period. Ten years later, Hansen published a paper in which he hypothesized that “apparently the rate of uptake by carbon dioxide sinks, either the ocean, or more likely the forests and soils, has increased.” This was not assumed in any of his scenarios. In fact, the general hypothesis has been that, as the planet warms, the ocean takes up carbon dioxide at a slower rate.   
  
  
Then, contrary to everyone’s expectation, the second most‐​important global warming emission, methane, simply stopped increasing. Some years have shown an actual drop in its atmospheric concentration. To this day, no one knows why.   
  
  
There’s also the nagging possibility that we haven’t yet figured out the true “sensitivity” of surface temperature to changes in carbon dioxide. Scientifically, that’s a chilling possibility.   
  
  
On May 30, Roger Pielke, Jr., a highly esteemed researcher at University of Colorado’s Center for Science and Technology Policy Research, examined Hansen’s scenarios. Of the two “lower” ones, he concluded, “Neither is particularly accurate or realistic. _Any conclusion that Hansen’s 1988 prediction got things right, necessarily must conclude that it got things right for the wrong reason_.” (italics in original)   
  
  
That’s precisely the keynote of my testimony eight years ago: in climate science, what you think is obviously true can literally change overnight, like the assumption of continued exponential growth of carbon dioxide, or how the earth responds.
"
"The UK is to spearhead a major global crackdown on illegal timber and deforestation, with plans to form a coalition of developing countries against the trade as part of its hosting of crunch UN climate talks this year. Deforestation is a leading factor in rising global greenhouse gas emissions, but many developing nations lack the means and institutions to combat illegal logging and regulate forest industries. The Department for International Development (DfID) will shortly lay out plans to help countries strengthen the rule of law, support the trade in responsible forestry and provide on-the-ground assistance to stamp out illegal logging.  “The illegal timber trade robs the earth of trees, which not only help stop climate change, they also play a critically important role in maintaining the world’s threatened biodiversity,” said Zac Goldsmith, a minister for international development. “This is a huge success story for the UK and for the world, and sets the scene for what we hope will be a successful year of international cooperation in the run-up to COP 26.” The UK will need to form a global coalition of developing countries to put pressure on leading economies to act swiftly on carbon, if this year’s UN climate talks are to succeed. The UK will host the COP 26 talks in Glasgow in November, but the government has faced a troubled start to its presidency, with the abrupt sacking of the intended president, the former energy minister Claire O’Neill, and delays in setting out a clear plan. All countries are expected to come forward with tougher plans to reduce global emissions as part of COP 26, and experts have said this will only happen if the UK takes the lead in forming a coalition of small and big developing countries, including forested African nations and Indonesia, as well as major economies such as the US, China, India and the EU. Offering assistance to developing countries, in the form of finance and technical expertise, will be vital to that effort. Lord Goldsmith, who is rated by Ladbrokes as favourite to take over the reins of COP 26 in Boris Johnson’s expected reshuffle, pointed to some notable victories against deforestation so far. He said targeted intervention by the UK had recently led to an “extremely important” prosecution of a major illegal trading operator in Indonesia, and had encouraged China to strengthen its legal commitments to ending the trade. “These are vital steps towards making sure there is no safe harbour for illegal timber anywhere in the world,” he said. “The UK will continue to work with China, Indonesia and our other international partners to protect the world’s forests for future generations.” The new project, still in the planning stages, will build on the government’s forests governance, markets and climate programme, the focus of which includes strengthening the rule of law in affected countries in the developing world, influencing international partners to increase their efforts, supporting responsible trade and helping stakeholders on the ground to act. The DfID also helps developing countries to access new technology in the fight against deforestation, including electronic wood tracking, which marks trees with digital barcodes that officials can scan at each stage of the supply chain, and a GPS-enabled smartphone app, to enable local communities to monitor and report illegal logging in real time. In 2005, only about a fifth of Indonesia’s timber trade was legal. But today, after interventions by the UK and other partners, 100% of exports are sourced from independently audited factories and forests. In Liberia, the programme helped forest communities to negotiate fair contracts with logging companies to stop illegal deforestation, and a new land rights act was signed into law which for the first time recognised women’s rights to land."
"I am always repeating the mantra that we should “work hard and play hard”. But is having fun professionally productive?  As someone who studies animal behaviour I sometimes look to my experimental subjects for an answer. There are many things that give both animals and humans pleasure, such as eating tasty food, but these are not all necessarily fun.  When I look at my children I see that fun for them involves playing and scientists who investigate the biology of fun also focus on play.   Evolutionary ecologist Gordon Burghardt, an expert in the field, defines play as:  … repeated, seemingly non-functional behaviour differing from more adaptive versions structurally, contextually, or developmentally, and initiated when the animal is in a relaxed, unstimulating, or low stress setting.  Thus, play is something animals repeatedly do that appears to be without function, such as a cat chasing a ball of string.  It can be related to other behaviour (training to hunt, for instance), but tends to be expressed in a more exaggerated manner and only under relaxed conditions. It is arguable whether or [not all animal species play](http://www.cell.com/current-biology/fulltext/S0960-9822(14), and the debate goes back to our definition. When we think of animals playing, our minds fill with images of dogs, monkeys and dolphins not those of birds or octopuses.  Thus, we think of play as an activity undertaken by species we regard as sociable – and intelligent.   And yet octopuses are some of the smartest creatures in the sea, while parrots and crows can outperform primates in animal cognition tests. But scientists have seen these animals at play too. Does this mean that other, less intelligent animals have fun-free lives? The problem with the word “fun” is it induces anthropomorphism.  A bird “skiing” down a snow covered roof looks like fun to us humans. We assume that birds must also do it for fun.   But what looks like fun to us may in fact be the opposite, even in highly intelligent social species.  Those wonderful, apparently, [coordinated leaps](http://www.cell.com/current-biology/fulltext/S0960-9822(14) by wild dolphins are often aerial fights.  Wave surfing by black swans off the coast of Australia looks like enormous fun, but may just be the most efficient way to get to shore.  Or the animals may simply be being dragged along by the laws of physics. One solution to this problem would be to look at animal brains to determine if they have the same features which permit humans to feel like they are having fun.  Unfortunately, we simply do not have this information for the vast majority of species.  Thus, we do not know whether fun is restricted to a select group of animals.  Although adults of certain species such as bonobos, domestic dogs or ourselves are known to play, playing is still predominately done by juveniles. This makes sense, as playing often helps develop and hone the skills necessary for later life.  A kitten chases a ball of string to improve its predatory skills, a male baboon play-fights to develop the combat skills needed to climb up in the adult hierarchy, and chimpanzees play with objects to refine the hand-eye coordination skills necessary to use tools and process food. In humans, there is a very interesting link between [playfulness and creativity](http://www.cell.com/current-biology/fulltext/S0960-9822(14) – something that has not been looked at in animals. Playful people are known for being very innovative.  This link is perhaps most well known in the world of art where great composers such as Mozart or innovative painters such as Picasso were known for their playfulness.  In science as well there are many cases of this link, for example, the physicist Richard Feynman who said he played at physics and therefore his work was fun. These people generally have the ability to bring together apparently unconnected bits of information to create a new solution to a problem. In the animal kingdom, within a species, there is a variation in how much individuals play.  It would be interesting to see if the more playful ones are the innovators.  For example, was “Imo” the first wild Japanese macaque observed washing her food more playful than her peers?  And can inducing play increase creativity in animals? The mantra of my research group will now change slightly to “play hard and work hard” – not only will this make our lives more fun, but, hopefully, we will reap the benefits of increased creativity in our research."
"

Four years ago President Clinton announced a 50‐​point plan to curb so‐​called greenhouse gases, principally carbon dioxide. This week talks continue in Bonn, Germany, on an agreement to cut future emissions below 1990 levels. “The only thing we know for absolute certain is that voluntary programs won’t work,” contends Jessica Tuchman Mathews, president of the Carnegie Endowment for International Peace.



Actually, we also know that activists are misusing science in demanding draconian restrictions to avert global warming. In fact, there is no consensus among climatologists that uncontrolled, human‐​induced warming threatens the planet. Or that the kinds of measures being discussed in Bonn would avert such a disaster.



The climate has long been a favorite of apocalyptics. Two decades ago there were fears of– a new ice age. Publications like National Geographic reported shorter growing seasons, summer frosts, and advancing glaciers. Time magazine observed that “the atmosphere has been growing gradually cooler for the past three decades. The trend shows no indication of reversing.” There were also books, including “The Weather Conspiracy: The Coming of the New Ice Age and Ice: The Ultimate Human Catastrophe.”



The latter, written by Fred Hoyle and published in 1981, proclaimed: “It is 12,500 years since the last ice age ended, which means the next one is long overdue. When the ice comes, most of Northern America Britain, and Northern Europe will disappear under the glaciers.” Mr. Hoyle advocated warming the oceans.



There should be no controls without genuine consensus both that disaster threatens and that new regulations would avert disaster.



But, happily, that crisis passed. So we moved on to global warming. The basic theory is that pollutants– so‐​called greenhouse gases– are accumulating in the atmosphere, holding in the heat, and causing the world’s temperature to rise. It remains just a theory, however, since climate change is a complex business.



Indeed, there is no one right temperature. After all, there was once an ice age. If we could choose, we should choose a warmer climate. Fewer people die in the cold, less money is spent on energy, growing seasons are longer. The real issue, then is whether the Earth faces an uncontrolled catastrophic increase in average temperatures.



Unfortunately, the debate has become highly political. Stephen Schneider, who once warned of a new ice age, has complained that “it is journalistically irresponsible to present both sides.” Despite being a scientist, he admitted: “I don’t set very much store by looking at the direct evidence.” Why not? “To avert the risk we need to get some broad‐​based support, to capture public imagination. So we have to offer up some scary scenarios make some simplified dramatic statements and little mention of any doubts one might have.” So much for genuine scientific discourse. Explained Mr. Schneider: “Each of us has to decide what the right balance is between being effective and being honest.”



He’s not interested in direct evidence, presumably because, as even the Sierra Club’s Bruce Hamilton acknowledges, “If you look at the science, it’s all over the map.” Past polls have found that most climatologists do not believe human‐​induced warming has occurred. Activists cite the latest report of the UN.-sponsored Intergovernmental Panel on Climate Change, but lead author Benjamin Sanger complains that “it’s unfortunate that many people read the media hype before they read the chapter.” He cites the report’s many caveats: “We say quite clearly that few scientists would say the attribution issue was a done deal.”



Disputes begin over data collection and temperature trends. The best evidence suggests far less warming so far this century than predicted by the models. Moreover, 90 percent of the warming occurred before 1940, when emissions of supposed greenhouse gases began to climb dramatically. In fact, as John Shanahan of the Alexis de Tocqueville Institute points out, “the government’s own satellite data and balloon measurements over the last 18 years show a very slight cooling,” the opposite of “what the climate models predict should have occurred.”



Thus, there is good reason to avoid burdensome treaty commitments. Since the United States is already one of the globe’s most efficient energy consumers, massive emission cutbacks would mean fewer jobs, less production, and a lower standard of living. A Heritage Foundation study estimates the cost of proposed controls from 2001 through 2020 to be $3.3 trillion, or about $30,000 per household. It obviously matters whether environmental activists are choosing effectiveness or honesty when making their claims. It is not enough to delay the agreement’s compliance date as proposed by Mr. Clinton. There should be no controls without genuine consensus both that disaster threatens and that new regulations would avert disaster.



And that requires facts, not rhetoric. The burden of proof falls on those demanding the power to levy new taxes and impose new regulations. Unless such evidence appears, Americans should reject the Chicken Littles who cry that the sky is falling.
"
"


A guest post by: Russ Steele from NCWatch

We can only hope the most people in the US are shopping on Black Friday and not watching the Oprah Winfrey Show today.  Al Gore has brought his global warming propaganda machine to share with Oprah.  You can find the details on Oprah’s web page.  Here are some of the topics that Gore is pushing:
Classic Gore:
“Some of the leading scientists are now saying we may have as little as 10 years before we cross a kind of point-of-no-return, beyond which it’s much more difficult to save the habitability of the planet in the future,” Gore says.
Yes, but Al you have been saying that for over ten years and we are still here. And in the last ten years the global temperatures stopped rising and are now in decline.

Click for a larger image
Really Al, show me where the temperatures are beyond natural fluctuations:
Gore agrees that the planet’s temperature has indeed experienced up and down cycles, but he says the current up cycle is too extreme. “It’s way off the charts compared to what those natural fluctuations are,” he says.
Here is look at long term temperatures
 
One word of caution, these are USHCN numbers, which [have been] adjusted. Past temperatures are going down and the more recent going up.
Going, going Gored:
No place is immune to global warming, Gore says. “Of the thousand largest glaciers on every continent, 997 of them are receding,” he says. “And it’s not seasonal.”

Glaciers have been retreating long before CO2 was problem. (Graphic from Climate Skeptic) Now we learn that the glaciers have stopped retreating and are expanding:
After years of decline, glaciers in Norway are again growing, reports the Norwegian Water Resources and Energy Directorate, as reported in Daily Tech.
DailyTech has previously reported on the growth in Alaskan glaciers, reversing a 250-year trend of loss. Some glaciers in Canada, California, and New Zealand are also growing, as the result of both colder temperatures and increased snowfall.
Al needs to take a second look at the North Pole:
“The North Pole is melting.”

Here is comparison of the ice in November 1980 and 2008. Do you see some major differences, like the “North Pole is melting.” (Note: Earlier photos do not show snow coverage) Details at Cryosphere  Today
Katrina again:
“Temperature increases are taking place all over the world, including in the oceans. Gore warns that when the oceans get warmer, storms get stronger. In August 2005, millions of Americans were left homeless by Hurricane Katrina, one of the most powerful hurricanes in recent history. Gore says people should expect more Category 4 and 5 hurricanes if the ocean waters continue to warm.”
 
Looks like a decline in cyclone energy to me, not an increase.
Please let Oprah know that you expected more from someone of her intelligence and veracity here.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9aa0e681',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

We are in the process of putting the final touches on our Public Comment concerning the Department of Energy’s use of the social cost of carbon in its recent rulemaking regulating the energy efficiency of microwave ovens. (We’ll make our Comment available as soon as we file it.) The social cost of carbon (SCC) is the government’s idea of how much future damage will be caused by each ton of carbon dioxide emitted today.   
  
Our DOE Comment focuses entirely on the new science concerning the equilibrium climate sensitivity, that is, how much the earth’s average surface temperature will increase from a doubling of the atmospheric carbon dioxide content. We argue that had the new science indicating a lower equilibrium climate sensitivity been properly incorporated into the determination of the SCC used by the DOE, it would have had a significant impact on the cost/benefit analysis used to justify the new regulation. The “benefits” of requiring lower energy consumption from microwave ovens would have been reduced.   
  
But, as we have discussed previously, the new, lower estimate of the equilibrium climate sensitivity is just one of several key variables to which the SCC is very sensitive.   
  
Another is whether or not the social cost of carbon used in U.S. government cost/benefit analyses of proposed rules and regulations should reflect an estimate of global or domestic costs. Currently, in a departure from its own guidelines, the government uses the global SCC in determining the benefits accrued by reducing domestic carbon dioxide reductions. Unsurprisingly, the global SCC is many times higher than the domestic SCC.   
  
And another variable is the “discount rate” used in the SCC calculation. The discount rate reflects how much we are willing to pay now to reduce the costs of projected carbon-related damages in the future. The lower the discount rate, the more we must pay now and thus the higher the current SCC seems to be. In another departure from its own guidelines, the government’s calculation uses an especially low discount rate, resulting in a high SCC and thus more “benefits” from regulations reducing carbon emissions.   




There was an insightful column in the _New York Times_ earlier this week by Eduardo Porter that is one of the clearest explanations we have read on the effects and rationale of the choice of the discount rate when determining the social cost of carbon. He draws a distinction between what he terms a “moralist” approach (which prefers a lower discount rate) and a “business” approach (which requires a higher discount rate):   




The discrepancy between the estimates of the value of climate damage stem from radically different views on how much weight the people of the present should give to damages caused by the climate in the distant future.   
  
The estimate of [the SCC of] $65 a ton is inspired by a moral stance: if warming will impose a cost of 1 percent of the world’s income in the future, we should spend about 1 percent of our income to prevent it—or perhaps somewhat less to account for the trend that people 100 years from now are likely to be much richer than people today.   
  
By contrast, $13.50 a ton comes from the business world. Essentially, it requires that spending to prevent climate change should yield at least the same rate of return, in terms of reduced damages from warming, as any other capital investment.   
  
The two outlooks lead to entirely different decisions. The government’s rendition of the moral approach implies that it is worth making every investment to reduce carbon emissions that has a rate of return of at least 2.5 percent, in terms of avoided damages. Business logic suggests that no investment should be made if the return—after taxes—is less than 5 percent.



Ultimately, Porter thinks that the business logic will win the day. He astutely points out:   




Multiple challenges compete for the world’s resources, from economic development and ending poverty to eradicating AIDS and malaria. The climate is not the world’s only priority. Even if we were to agree that improving the well-being of future generations is worth an enormous investment, there might be better things to invest in than reducing greenhouse gas emissions.



We are happy to see more critical attention being paid to the social cost of carbon, as it represents a huge, but little-known back door that is silently swinging open for costly government regulation of carbon emissions. Economics and science argue that door should be barred and locked.


"
"

 _The Current Wisdom is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press. Occasionally — as in this edition — we examine recent global warming perceptions that are at odds with reality._   
  
“The habitability of this planet for human beings really is at risk.”   
\--Al Gore, July 18, 2007   
  
The notion that people just can’t adapt to change (and therefore that governments must regulate change) is known as “Dumb People Syndrome” (DPS). Given the fact that the planet is “habitable” (meaning that there large numbers of people) over a mean annual temperature range of approximately 40°C , Gore’s statement—which is about a few degrees C, at best—is quintessential DPS.   
  
DPS has its subtypes, such as “Dumb Farmer Syndrome”, in which there’s agricultural Armageddon as the world’s farmers fail to adapt to warming conditions. It’s not only preposterous, it’s inconsistent with history.   
  
Farmers aren’t dumb, and there are incentives for their supply chain—breeders, chemical manufacturers, equipment companies, etc.—to produce adaptive technologies. Corn is already much more water-use efficient than it was, thanks to changes in genetics, tillage practices, and farm equipment. The history of U.S. crop yield bears strong witness (Figure 1).   






Figure 1. U.S. national corn and wheat yields, 1900-2012 (source: USDA National Agricultural Statistics Service).   
  
A look at the horrible crop year of 2012 is instructive. Corn yield drops about 38 bushels per acre from what’s known as the “technological trend line.” Because the “expected” yield—thanks to technology—with good weather is so high (around 160 bushels/acre), that’s a drop of about 24%, which is simply unremarkable when compared to the other lousy weather years of 1901 (36%), 1947 (21%), 1983 (29%) and 1988 (30%). Did we mention that the direct fertilization effect of atmospheric CO2 has resulted in a corn yield increase of approximately seven per cent?   
  
Most assessments of the impacts of climate change give some credence to DPS. Below is one of the “Key Findings” from the report _Global Climate Change Impacts in the United States_ produced by the U.S. Climate Change Global Change Research Program (USGCRP), which was used as a major support for the U.S. Environmental Protections Agency’s “Endangerment Finding” that human carbon dioxide emissions are a threat to health and welfare. According to the USGCRP:   




**Crop and livestock production will be increasingly challenged**.   
  
Many crops show positive responses to elevated carbon dioxide and low levels of warming, but higher levels of warming often negatively affect growth and yields. Increased pests, water stress, diseases, and weather extremes will pose adaptation challenges for crop and livestock production.



Now compare that to the corresponding “Key Finding” from our report _Addendum: Global Climate Change Impacts in the United States_ which is an independent (from the USGCRP) assessment of the scientific literature relating to environmental changes and how they may impact U.S. agriculture:   
  
  




**Crop and livestock production will adapt to climate change.**   
  
There is a large body of evidence that demonstrates substantial untapped adaptability of U.S. agriculture to climate change, including crop-switching that can change the species used for livestock feed. In addition, carbon dioxide itself is likely increasing crop yields and will continue to do so in increasing increments in the future.



Another example of the DPS relates to projections of the effects of more or stronger heat waves on human mortality. Everyone has heard—especially after last summer—how human use of fossil fuels to produce energy will increase the frequency and severity of killer heat waves.   
  
Here is how the USGCRP sees it, according to the “Key Messages” from the “Human Health” chapter of their report:   




Increases in the risk of illness and death related to extreme heat and heat waves are very likely.



History shows that things don’t work this way.   
  
Why? Because people are not dumb. Instead of dying in increasing numbers as temperatures rise, people take better precautions to protect themselves from the heat.   
  
Numerous examples of this abound, including some pioneering work that we did on the subject about 10 years ago. We clearly demonstrated that across the U.S., people were becoming less sensitive to high temperatures, _despite the fact that high temperatures were increasing_. In other words, adaptation was taking place in the face of (or, perhaps even because of) rising temperatures. Adaptations include expanding use of air conditioning, increasing public awareness, and more widespread community action programs.   
  
What was interesting about our work is we didn’t even need global warming to drive increasing heat waves. All we needed was economic activity that concentrates in cities. As they grow, buildings and pavement retain the heat of the day and impede the flow of ventilating winds. In recent years, the elevation of night temperatures here in Washington (where your tax dollars virtually guarantee economic growth), compared to the countryside, has become truly remarkable. But you won’t find an increase in heat-related mortality. Instead, there’s been a decrease.   
Our research was limited to major cities across the United States. But similar findings have since been reported for other regions of the world, the most recent being the from the Czech Republic.   
  
Czech researchers Jan Kyselý and Eva Plavcová recently published the results of their investigation of changes in heat-related impacts there from 1986 through 2009. What they found sure wasn’t surprising to us, but surely must come as quite a shock to the fans of DPS.   




Declining trends in the mortality impacts are found in spite of rising temperature trends. The finding remains unchanged if possible confounding effects of within-season acclimatization to heat and the mortality displacement effect are taken into account. Recent positive socioeconomic development, following the collapse of communism in Central and Eastern Europe in 1989, and better public awareness of heat-related risks are likely the primary causes of the declining vulnerability. The results suggest that climate change may have relatively little influence on heat-related deaths, since changes in other factors that affect vulnerability of the population are dominant instead of temperature trends. It is essential to better understand the observed nonstationarity of the temperature-mortality relationship and the role of adaptation and its limits, both physiological and technological, and to address associated uncertainties in studies dealing with climate change projections of temperature-related mortality.



Findings like these, along with our own work, caused us to conclude in our _Addendum_ report that:   
  
“In U.S. cities, heat-related mortality declines as heat waves become stronger and/or more frequent.”   
  
Evidence is much more compelling in support of a “smart people” diagnosis than its opposite. In fact, if humankind was really as dumb as the fans of DPS would have us believe, we wouldn’t be around today to hear their doomsaying, because _Homo sapiens_ would have been wiped out during vastly larger environmental swings (in and out of ice ages, for example) in our past, than those expected as a consequence of the burning of fossil fuels to produce the energy that powers our world—a world in which the human life expectancy, perhaps the best measure of our level of “dumbness” or “smartness”—has more than doubled over the last century and continues to grow ever longer.   
  
Simply put, we are not “dumb” when it comes to our survival and our ability to adapt to changing environmental conditions, but “scientific” assessments that assume otherwise most certainly are.   
  
**References:**   
  
Davis, R.E., Knappenberger, P.C., Novicoff, W.M., Michaels, P.J., 2002. Decadal changes in heat-related human mortality in the Eastern US. _Climate Research_ , **22** , 175–184.   
  
Davis, R.E., Knappenberger, P.C., Novicoff, W.M., Michaels, P.J.,2003a. Decadal changes in summer mortality in U.S. cities. _International Journal of Biometeorology_ , **47** , 166–175.   
  
Davis, R.E., Knappenberger, P.C., Michaels, P.J., Novicoff, W.M., 2003b. Changing heat-related mortality in the United States. _Environmental Health Perspectives_ , **111** , 1712–1718.   
  
Davis, R.E., Knappenberger, P.C., Michaels, P.J., Novicoff, W.M., 2004. Seasonality of climate-human mortality relationships in US cities and impacts of climate change. _Climate Research_ , **26** , 61–76.   
  
Jan Kyselý, j., and E. Plavcová, 2012.Declining impacts of hot spells on mortality in the Czech Republic, 1986–2009: adaptation to climate change? _Climatic Change_ , **113** , 437-453.


"
"
Share this...FacebookTwitterIn my last post I summarized the results of Ed Caryl’s analyses of stations far up in the Arctic, where temperature trends appear to follow the 60-year AMO cycle, and do not correlate at all with CO2.
Now I’ve been made aware that temperatures following the AMO are not exclusive to the Arctic. Blogsite digging in the clay has plotted temperatures from other cities located about the globe and came up with the same AMO sine wave trend, see below:

Source: http://diggingintheclay.wordpress.com/2010/09/01/in-search-of-cooling-trends/
And what follows are more plots from Iceland, Norway and Russia on the left, and from USA on the right (Sorry about the poor quality. Better quality graphics can be seen at diggingintheclay here). Again the AMO wave appears there as well. Moreover, the 1930s and 40s in USA even look a bit warmer than today’s temps.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So poor little trace-gas CO2 just doesn’t get no respect, no matter where it goes. Every corner of the globe is ignoring this pip-squeak, climate-driver wannabe. CO2 is fading from the picture.
If the temperature has followed the AMO sine trend over the last 150 years, then why do we keep seeing hockey blades (nowadays without the stick-part) showing temperatures straight-lining up over the last 100 years or so?
Probably because GISS and others have changed historical data to make them fit their ideologized models – as unbelievable as it may sound. Steve Goddard’s site here explains how GISS has precisely done this with US temperatures by going back and flattening the graph.

The above comparator is from: Steve Goddard’s site. His post is short, but a very worthwhile read.
Before 2000, GISS showed a warmer 1930s and cooler current period. The AMO wave is clear to see.  But today, after having fiddled with the data, GISS now shows a cooler 1930s, a warmer present day and a somewhat straighter line that tries to cloak the AMO effect.
 Now you really know why they call it “man-made global warming”.
Share this...FacebookTwitter "
"It’s no surprise that dirty air kills. In fact, air pollution was recently placed in the top ten health risks faced by human beings globally. In the UK high concentrations of polluting particles cost health services around around £20 billion per year. The country’s Environmental Audit Committee recently described air pollution as a “public health crisis”. Despite this very real, very widespread risk, the quality of air we breathe doesn’t seem to attract the same level of concern that health crises such as obesity or smoking do. But this is a universal risk that impacts us in every breath we take. When such a fundamental human resource is at risk, we cannot afford to be ambivalent. We should invest in information technology that will enable smarter decisions, better planning and greener cities. It’s not enough to simply try to encourage improved individual behaviour. Air pollution, monitoring and planning needs national and international investment and attention. The evidence is clear. At present, many of the world’s cities are unable to comply with air pollution standards, in many cases far exceeding them and resulting in millions of premature deaths. Technology companies, universities and start-ups across the world are working to bring futuristic smart cities to life. Whether it’s automatic transportation infrastructure or digital government systems, this concept is spawning great ideas to improve our lives through the use of data and technology. But in the race to transform our cities, we need to ensure these smart systems enable our smarter lives to be longer lives, and by putting data to use we can help combat the ill-effects of bad air. It all starts with data. Scientists of course love data, but so do the public when they can discern real benefits from its results. There are a number of air monitoring systems in place in the UK already, such as Defra’s Automatic Urban and Rural Network of over 175 monitoring sites across the UK. This is an example of a traditional fixed monitoring station.  Likewise, several air pollution monitoring stations are run in the US under the umbrella of US Environmental Protection Agency to maintain and preserve the quality of the nation’s air.   Unfortunately these kinds of stations are expensive to install and maintain, which rather defeats the purpose of supposedly sustainable smart infrastructure.  Also, such systems often only provide a snapshot of a particular area and miss some pollution hotspots within city environments. However, there are already projects that show the potential for small, low-cost air sensing networks and demonstrate how the public could tap into this data to make a real impact on individual lives.  Air Quality Egg is a community-led air quality sensing network for urban air pollution, driven by inexpensive, DIY sensors. Similarly, Airtext provides daily forecasts of air quality in London.  On a small-scale these allow the public to make informed health decisions by avoiding areas of high pollution. Our latest work has looked at the future of low-cost sensing for managing air pollution in cities and how agencies can benefit from such inexpensive monitoring alternatives. Quite simply, the expansion of data projects such as these could save lives. The UK’s Environmental Audit Committee has argued for schools, hospitals and care homes to be built away from pollution hotspots. I would agree, and take this further, giving the public open and accurate data on how their own homes are affected by air pollution, based on a smart system of low-cost monitoring. This not only gives the option for people to consider where to live, but also provides individual impetus for understanding and acting on potentially polluting behaviour. More widely this kind of open data gives governments a reason to act. Smart cities also mean smart citizens, involved and in-touch with the facts. Where the facts show our health is at risk, governments will no longer be able to hide behind smokescreens, but will have to act on the data which is at all our fingertips."
"The Liberal MP Trent Zimmerman says the Morrison government should work towards adopting a target of net zero emissions by 2050 to bring itself into line with commitments Australia made under the Paris agreement, and to align Canberra’s policy with emissions reduction targets adopted by the states. In an interview with Guardian Australia’s politics podcast, Zimmerman said it was reasonable for the prime minister to want to do due diligence on what net zero would cost, and to consider what a policy roadmap would look like, “but this is something that we should be looking very seriously at”. He said the Paris agreement required Australia to move to net carbon neutrality in the second half of the century, and to take measures to help keep temperature rises to between 1.5C and 2C. Zimmerman – who is one of the backbench moderates pushing for the government to step up the level of ambition on climate action – predicted the 2050 target would trigger more infighting within the government. But he said the best way to persuade colleagues to accept a pivot was to explain to them, and to their communities, that the transition to low emissions energy in Australia was “not going to be a disaster”. “Not only will this not be a disaster, there will be opportunity.” Zimmerman said he wasn’t sure whether those arguments would ultimately succeed, but “we have to give it a go”. He said Australia had “lost half a decade because of the debate about what we needed to do” and expressed hope that a durable solution could be reached in this parliament. “I think the challenge of climate change is so real and so serious that if there can be some kind of consensus, that would be a positive outcome.” Zimmerman confirmed he had spoken to the independent MP Zali Steggall – who took the neighbouring electorate of Warringah from the government at the last election on a climate change platform – about a bill she wants to pursue in parliament that would impose the net zero target. “I’ve spoken to Zali and I’m open to having a conversation when we come back [to Canberra] in a couple of weeks’ time. “It is watch this space on that front.” Zimmerman said Steggall wanted to bring on the bill in March, but that timetable was too compressed to allow the government to resolve its own stance on the 2050 target. “I think the government should be given the opportunity to do a due diligence process on the 2050 target before the parliament is expected to sign up to that.” He said the leadership was very unlikely to grant MPs a conscience vote, and there was an open question about whether the bill would ever come on for debate. But he wanted to have the conversation “out of respect for Zali”. “For me, the important thing is the goal. Zali’s bill may not be the only way to achieve that goal.” The MP repeated comments he made last week that governments should not be in the business of bankrolling coal projects that the private sector will not finance. While Queensland Nationals are pushing assertively for a new taxpayer-backed coal plant at Collinsville, a number of Liberals are concerned about sending a negative signal in parts of the country where their constituents are worried about a lack of climate action, and about the fiscal exposure. Despite the government’s partisan criticism of Labor’s electric vehicles policy at the last election, including hyperbole that emissions standards were a “war on the weekend”, Zimmerman argued the Coalition needed to be at the forefront of managing the transition to electric vehicles, because “the global car market is only heading in one direction”. He said the states needed to look at consumer incentives, such as concessions on registration fees and rebates on toll roads for drivers of electric cars. At the commonwealth level, Zimmerman said, “the charging network will be really crucial”. The MP said an electric vehicle strategy would work best with the public if the focus in the first phase was on carrots rather than sticks, and once the transition was under way, regulatory options could come into play, such as emissions standards to drive faster take-up as prices came down."
"
NOTE: Zip file downloads of models with data have been fixed, see end of this post – Anthony


Source: Mantua,            2000
The essay below has been part of a back and forth email exchange for about a week. Bill has done some yeoman’s work here at coaxing some new information from existing data. Both HadCRUT and GISS data was used for the comparisons to a doubling of CO2, and what I find most interesting is that both Hadley and GISS data come out higher in for a doubling of CO2 than NCDC data, implying that the adjustments to data used in GISS and HadCRUT add something that really isn’t there.
The logarithmic plots of CO2 doubling help demonstrate why CO2 won’t cause a runaway greenhouse effect due to diminished IR returns as CO2 PPM’s increase. This is something many people don’t get to see visualized.
One of the other interesting items is the essay is about the El Nino event in 1878. Bill writes:
The 1877-78 El Nino was the biggest event on record.  The anomaly peaked at +3.4C in Nov, 1877 and by Feb, 1878, global temperatures had spiked to +0.364C or nearly 0.7C above the background temperature trend of the time.
Clearly the oceans ruled the climate, and it appears they still do.
Let’s all give this a good examination, point out weaknesses, and give encouragement for Bill’s work. This is a must read. – Anthony

Adjusting Temperatures for the ENSO and the  AMO
A guest post by: Bill Illis

People have  noted for a long time that the effect of the El Nino Southern Oscillation (ENSO)  should be accounted for and adjusted for in analyzing temperature trends.  The same point has been raised for the  Atlantic Multidecadal Oscillation (AMO).   Until now, there has not been a robust method of doing so.
This post will  outline a simple least squares regression solution to adjusting monthly  temperatures for the impact of the ENSO and the AMO.  There is no smoothing of the data, no  plugging of the data; this is a simple mathematical calculation.
Some basic  points before we continue.
–         The ENSO and the AMO both affect temperatures and, hence, any  reconstruction needs to use both ocean temperature indices.  The AMO actually provides a greater impact on  temperatures than the ENSO.
–         The ENSO and the AMO impact temperatures directly and continuously on  a monthly basis.  Any smoothing of the  data or even using annual temperature data just reduces the information which  can be extracted.
–         The ENSO’s impact on temperatures is lagged by 3 months while the AMO  seems to be more immediate.  This model  uses the Nino 3.4 region anomaly since it seems to be the most indicative of the  underlying El Nino and La Nina trends.
–         When the ENSO and the AMO impacts are adjusted for, all that is left  is the global warming signal and a white noise error.
–         The ENSO and the AMO are capable of explaining almost all of the  natural variation in the climate.
–         We can finally answer the question of how much global warming has  there been to date and how much has occurred since 1979 for example.  And, yes, there has been global warming but  the amount is much less than global warming models predict and the effect even  seems to be slowing down since 1979.
–         Unfortunately, there is not currently a good forecast model for the  ENSO or AMO so this method will have to focus on current and past temperatures  versus providing forecasts for the future.
And now to the  good part, here is what the reconstruction looks like for the Hadley Centre’s  HadCRUT3 global monthly temperature series going back to 1871 – 1,652 data  points.

Click for a full sized image

I will walk you  through how this method was developed since it will help with understanding some  of its components.
Let’s first look  at the Nino 3.4 region anomaly going back to 1871 as developed by Trenberth  (actually this index is smoothed but it is the least smoothed data  available).
–         The 1877-78 El Nino was the biggest event on record.  The anomaly peaked at +3.4C in Nov, 1877 and  by Feb, 1878, global temperatures had spiked to +0.364C or nearly 0.7C above the  background temperature trend of the time.
–         The 1997-98 El Nino produced similar results and still holds the  record for the highest monthly temperature of +0.749C in Feb, 1998.
–         There is a lag of about 3 months in the impact of ENSO on  temperatures.  Sometimes it is only 2  months, sometimes 4 months and this reconstruction uses the 3 month lag.
–         Going back to 1871, there is no real trend in the Nino 3.4 anomaly  which indicates it is a natural climate cycle and is not related to global  warming in the sense that more El Ninos are occurring as a result of  warming.   This point becomes important because we need  to separate the natural variation in the climate from the global warming  influence.

Click for full sized image

The AMO anomaly  has longer cycles than the ENSO.
–         While the Nino 3.4 region can spike up to +3.4C, the AMO index rarely  gets above +0.6C anomaly.
–         The long cycles of the AMO matches the major climate shifts which  have occurred over the last 130 years.   The downswing in temperatures from 1890 to 1915, the upswing in temps  from 1915 to 1945, the decline from 1946 to 1975 and the upswing in temps from  1975 to 2005.
–         The AMO also has spikes during the major El Nino events of 1877-88  and 1997-98 and other spikes at different times.
–         It is apparent that the major increase in temperatures during the  1997-98 El Nino was also caused by the AMO anomaly.  I think this has lead some to believe the  impact of ENSO is bigger than it really is and has caused people to focus too  much on the ENSO.
–         There is some autocorrelation between the ENSO and the AMO given  these simultaneous spikes but the longer cycles of the AMO versus the short  sharp swings in the ENSO means they are relatively independent.
–         As well, the AMO appears to be a natural climate cycle unrelated to  global warming.

Click for full sized image

When these two  ocean indices are regressed against the monthly temperature record, we have a  very good match.
–         The coefficient for the Nino 3.4 region at 0.058 means it is capable  of explaining changes in temps of as much as +/- 0.2C.
–         The coefficient for the AMO index at 0.51 to 0.75 indicates it is  capable of explaining changes in temps of as much as +/- 0.3C to +/-  0.4C.
–         The F-statistic for this regression at 222.5 means it passes a 99.9%  confidence interval.
But there is a  divergence between the actual temperature record and the regression model based  solely on the Nino and the AMO.  This is  the real global warming signal.

Click for full sized image

The global  warming signal (which also includes an error, UHI, poor siting and adjustments  in the temperature record as demonstrated by Anthony Watts) can be now be modeled against the rise in CO2 over the period.
–         Warming occurs in a logarithmic relationship to CO2 and,  consequently, any model of warming should be done on the natural log of  CO2.
–         CO2 in this case is just a proxy for all the GHGs but since it is the  biggest one and nitrous oxide is rising at the same rate, it can be used as the  basis for the warming model.
This regression  produces a global warming signal which is about half of that predicted by the  global warming models.  The F statistic  at 4,308 passes a 99.9% confidence interval.

Click for full sized image

–         Using the HadCRUT3 temperature series, warming works out to only  1.85C per doubling of CO2.
–         The GISS reconstruction also produces 1.85C per doubling while the  NCDC temperature record only produces 1.6C per doubling.
–         Global warming theorists are now explaining the lack of warming to  date is due to the deep oceans absorbing some of the increase (not the surface  since this is already included in the temperature data).  This means the global warming model  prediction line should be pushed out 35 years, or 75 years or even 100s of  years.
Here is a  depiction of how logarithmic warming works.   I’ve included these log charts because it is fundamental to how to  regress for CO2 and it is a view of global warming which I believe many have not  seen before.
The formula for  the global warming models has been constructed by myself (I’m not even sure the  modelers have this perspective on the issue) but it is the only formula which  goes through the temperature figures at the start of the record (285 ppm or 280  ppm) and the 3.25C increase in temperatures for a doubling of CO2.   It is  curious that the global warming models are also based on CO2 or GHGs being  responsible for nearly all of the 33C greenhouse effect through its impact on  water vapour as well.

Click for larger image

The divergence,  however, is going to be harder to explain in just a few years since the ENSO and  AMO-adjusted warming observations are tracking farther and farther away from the  global warming model’s track.  As the RSS  satellite log warming chart will show later, temperatures have in fact moved  even farther away from the models since 1979.

Click for larger image

The global  warming models formula produces temperatures which would be +10C in geologic  time periods when CO2 was 3,000 ppm, for example, while this model’s log warming  would result in temperatures about +5C at 3,000 ppm.  This is much closer to the estimated  temperature history of the planet.
This method is  not perfect.  The overall reconstruction  produces a resulting error which is higher than one would want.  The error term is roughly +/-0.2C but the it  does appear to be strictly white noise.    It would be better if the resulting error was less than +/- 0.2C but it  appears this is unavoidable in something as complicated as the climate and in  the measurement errors which exist for temperature, the ENSO and the  AMO.
This is the  error for the reconstruction of GISS monthly data going back to 1880.

Click for larger image

There does not  appear to be a signal remaining in the errors for another natural climate  variable to impact the reconstruction.   In reviewing this model, I have also reviewed the impact of the major  volcanoes.  All of them appear to have  been caught by the ENSO and AMO indices which I imagine are influenced by  volcanoes.  There appears to be some room  to look at a solar influence but this would be quite small.  Everyone is welcome to improve on this  reconstruction method by examining other variables, other indices.
Overall, this  reconstruction produces an r^2 of 0.783 which is pretty good for a monthly  climate model based on just three simple variables.  Here is the scatterplot of the HadCRUT3  reconstruction.

Click for a larger image

This method  works for all the major monthly temperature series I have tried it  on.
Here is the  model for the RSS satellite-based temperature series.

Click for a larger image

Since 1979,  warming appears to be slowing down (after it is adjusted for the ENSO and the  AMO influence.)
The model  produces warming for the RSS data of just 0.046C per decade which would also  imply an increase in temperature of just 0.7C for a doubling of CO2 (and there  is only 0.4C more to go to that doubling level.)
Click for a full sized image

Looking at how  far off this warming trend is from the models can be seen in this zoom-in of the  log warming chart.  If you apply the same  method to GISS data since 1979, it is in the same circle as the satellite  observations so the different agencies do not produce much different  results.

Click for larger image

There may be  some explanations for this even wider divergence since 1979.
–         The regression coefficient for the AMO increases from about 0.51 in  the reconstructions starting in 1880 to about 0.75 when the reconstruction  starts in 1979.  This is not an expected  result in regression modelling.
–         Since the AMO was cycling upward since 1975, the increased  coefficient might just be catching a ride with that increasing trend.
–         I believe a regression is a regression and we should just accept this  coefficient.  The F statistic for this  model is 267 which would pass a 99.9% confidence interval.
–         On the other hand, the warming for RSS is really at the very lowest  possible end for temperatures which might be expected from increased GHGs.  I would not use a formula which is lower than  this for example.
–         The other explanation would be that the adjustments of old  temperature records by GISS and the Hadley Centre and others have artificially  increased the temperature trend prior to 1979 when the satellites became  available to keep them honest.  The  post-1979 warming formulae (not just RSS but all of them) indicate old records  might have been increased by 0.3C above where they really were.
–         I think these explanations are both partially correct.
This temperature  reconstruction method works for all of the major temperature series over any  time period chosen and for the smaller zonal components as well.  There is a really nice fit to the RSS Tropics  zone, for example, where the Nino coefficient increases to 0.21 as would be  expected.
Click for a full sized image

Unfortunately,  the method does not work for smaller regional temperature series such as the US  lower 48 and the Arctic where there is too much variation to produce a  reasonable result.
I have included  my spreadsheets which have been set up so that anyone can use them.  All of the data for HadCRUT3, GISS, UAH, RSS  and NCDC is included if you want to try out other series.  All of the base data on a monthly basis  including CO2 back to 1850, the AMO back to 1856 and the Nino 3.4 region going  back to 1871 is included in the spreadsheet.
The model for  monthly temperatures is “here” and for annual temperatures is “here” (note the  annual reconstruction is a little less accurate than the monthly reconstruction  but still works).
I have set-up a  photobucket site where anyone can review these charts and others that I have  constructed.
http://s463.photobucket.com/albums/qq360/Bill-illis/
So, we can now  adjust temperatures for the natural variation in the climate caused by the ENSO  and the AMO and this has provided a better insight into global warming.  The method is not perfect, however, as the  remaining error term is higher than one would want to see but it might be  unavoidable in something as complicated as the climate.
I encourage  everyone to try to improve on this method and/or find any errors.  I expect this will have to be taken into  account from now on in global warming research.   It is a simple regression.

UPDATED: Zip files should download OK now.

SUPPLEMENTAL INFO NOTE: Bill has made the Excel spreadsheets with data and graphs used for this essay available to me, and for those interested in replication and further investigation, I’m making them available here on my office webserver as a single ZIP file
Downloads:
Annual Temp Anomaly Model 171K Zip file
Monthly Temp Anomaly Model 1.1M Zip file
Just click the download link above, save as zip file, then unzip to your local drive work folder.
Here is the AMO data which is updated monthly a few days after month end.
http://www.cdc.noaa.gov/Correlation/amon.us.long.data
Here is the Nino 3.4 anomaly from Trenbeth from 1871 to 2007.
ftp://ftp.cgd.ucar.edu/pub/CAS/TNI_N34/Nino34.1871.2007.txt
And here is Nino 3.4 data updated from 2007 on.
http://www.cpc.ncep.noaa.gov:80/data/indices/sstoi.indices
– Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b0adf78',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.” In this edition, we cover an important story that we missed back in 2008._   
  
  
People send us stuff. As a result of our recent _Global Science Report_ on global warming ruining our bananas, one of our fans directed our attention to an important effect of climate change that we somehow missed, back in 2008, when the alarmists at the BBC wrote that it was threatening _haggis_.   
  
  
Haggis, for the uninitiated, is sheep stomach stuffed with minced lung, liver, heart, tongue, suet, onions and oats. How offal!   
  
  
While there’s no accounting for taste, it tastes as bad as it smells.   
  
  
According to the story, there has been a rise in a parasite effecting Scottish sheep that renders the lung “unfit for consumption” (something that many of you probably thought was the case already).   
  
  
And, so as not to miss the bandwagon, an official from the Scottish Agricultural College Veterinary Investigation Centre told the BCC that:   




Part of the reason will be the parasite is able to live a pretty happy life on the ground because of higher temperatures. Maybe it’s climate change.



Or maybe not.   
  
  
It turns out that another potential cause of the increase in the lung parasite is that Scottish farmers have reduced their application of parasite treatment due to declining infections of roundworm. The treatment of roundworm also killed the lung parasite.   
  
  
There was no mention made in the BCC article as to whether global warming was behind the decrease in roundworm infestations.   
  
  
Instead, the article went on the describe the events which took place in the World Haggis Eating Championship, won by Willie Robertson from Dunkeld, who managed to put away a pound of haggis in 125 seconds. For his victory, Mr. Robertson was awarded a trophy and a bottle of whiskey—no doubt a key feature in the rest of the day’s merrymaking.   
  
  
BBC’s writing in the haggis story appears similarly merry. Here are the last three paragraphs of their report, verbatim, a candidate for first place in the 2008 International Nonsequitur Competition.   




The championship was held as part of the 125th Birnam Highland Games, and attracted competitors from Australia, New Zealand and the US.   
  
  
Climate change, meanwhile, has been blamed for affecting natural habitats in Scotland and across the world.   
  
  
Most notably, scientists and conservationists say it threatens survival of polar bears.
"
"The growing Ebola virus outbreak not only highlights the tragedy enveloping the areas most affected but also offers a commentary on they way in which the political ecology in West Africa has allowed this disease to become established. The narrative goes that the virus appeared spontaneously in the forest villages of Guinea in December 2013. But this is debatable given that there is evidence of antibodies the Ebola virus in human blood from Sierra Leone up to five years previously. Previously only one case of Ebola had been reported in the region, and it was the Ivory Coast strain of the virus. The strain detected in the blood samples is of the more virulent Zaire strain of Ebola, the same strain responsible for the current epidemic. After months of very little concerted action it’s clear that the disease is now seriously in danger of spreading out of control. The global health community has declared it a crisis of international importance, which has led the host nations to implement draconian preventions strategies, tantamount in some places to martial law in terms of surveillance, quarantine, border controls and other logistical aspects of control. But this is too little, too late. There are several mechanisms through which the virus may have emerged, and it is unlikely that this latest outbreak was spontaneous. It is poverty that drives villagers to encroach further into the forest, where they become infected with the virus when hunting and butchering wildlife, or through contact with body fluids from bats  – this has been seen with Nipah, another dangerous virus associated with bats. The likelihood of infection in this manner is compounded by inadequate rural health facilities and poor village infrastructure, compounded by the disorganised urban sprawl at the fringes of cities. The virus then spreads in a wave of fear and panic, ill-conceived intervention and logistical failures – including even insufficient food or beds for the severely ill.  Take for example the global palm oil industry, where a similar trend of deep-cutting into forests for agricultural development has breached natural barriers to the evolution and spread of specific pathogens. The effects of land grabs and the focus on certain fruit crop species leads to an Allee effect, where sudden changes in one ecological element causes the mechanisms for keeping populations – bats in this case – and viruses in equilibrium to shift, increasing the probability of spill over to alternative hosts. This is not unheard of; the introduction of fruit tree crops in cleared forests and agricultural expansion in Malaysia was associated with the emergence of Nipah virus. Bats feeding on fruit trees infected pigs in pens, which provided a vector for the virus to humans. Another example is with vector-borne diseases such as the Japanese Encephalitis, a virus carried by wild birds which expanded its range due to growing rice and pig farming. Chikungunya and Dengue Fever viruses exploited deforestation for secondary epidemiological cycles, which increased at the forest edge until the virus was able to adapt to secondary hosts and expand globally. Certainly the complexity of the agro-ecological changes in West Africa warrant scrutiny. Guinea’s new agriculture is in an early stage of development, identified by the World Bank as the highest investment potential for industrial agriculture. As global markets shift – and tariffs and taxes on multinational companies are removed, farmers with small land holdings are faced with a choice: either sell off or scale up to meet the competition. Forests are one of the first casualties.  Alongside this subtle effect is the dismantling of traditional governance, violence under colonial, neo-colonial and more recent kleptocratic governments and the economic movements of people towards urbanisation. Such turbulence, poverty, the influx of refugees from neighbouring wars and crumbling health systems have all created an ecosystem in which the natural friction that prevents Ebola from gathering pathogenic momentum has been all but eroded. Any international response can do little to remedy these contributing factors. In fact the response has been little more than a recognition of the complete failure of neo-liberal development strategies to contain the virus.  The “success” of the Ebola virus is fundamentally based on the sociological factors and population biology of those it infects. But the data required to test the hypothesis – detailed records about what people eat, where they go and how they interact – is presently unavailable. Instead research has focused on virus hunting, and with little success: more than 40,000 samples have not yet conclusively determined where the natural reservoir of Ebola lies. All the while, the socio-ecological factors that are critical to the spread of any disease are ignored."
"Chemicals are all around us. They are crucial in all manner of industries, from agriculture to food to cosmetics. Most people give little thought to how these chemicals are made – and certainly very few would consider the chemical industry as a contributor to our society’s dependence on oil. But it is. Historically petroleum has been used to develop the chemicals needed for products such as pesticides, food supplements and make-up. Although many of the building blocks required to make these chemicals occur naturally, trying to take those natural materials and use them in large-scale industrial processes has proved difficult and costly. So petroleum is used instead. Until recently, oil was seen as a cheap commodity which was available in abundance, so petroleum was perfect for use in the chemical industry. However, the world has changed. We now recognise the need to reduce our reliance on oil in order to protect the environment and maintain our national security. There are also health concerns over the use of petroleum in products we eat and apply to our bodies. This is why new advanced methods for industrial biotechnology are so important; they are enabling the use of engineered bacterial cells, rather than petroleum, in developing chemicals to be used in these products. Importantly, the bacteria can be grown on a range of cheap and renewable resources, even various kinds of farmland waste. However, in order to use bacteria effectively – and in a manner which can be scaled up by industry – we need to know a lot more about bacterial cell biology. Only by investigating the machinery and processes at the heart of cells can we learn how to use them to develop organic chemicals in a manner that was previously unfeasible for industry. At Newcastle University’s Centre for Bacterial Cell Biology we’ve spent years studying Bacillus subtilis, a bacterium that lives peacefully in soil or even in the human gut. This organism and its relatives are very good at making and secreting enzymes which are catalysts for all sorts of useful processes. It means Bacilli are already used widely by industry, for example in producing the enzymes that are used in biological washing powders such as proteases (which break down blood, egg and other protein stains) or amylases (which dissolve starch). However, the range of enzymes they can secrete efficiently is much more limited than we would like. Studies on fundamental structures and processes of the bacterium are now beginning to give us the ability to engineer the cells to secrete a wider range of proteins from a diverse range of sources. This means that before long Bacillus will be used to make all kinds of enzymes, including those needed in the chemical industry to replace processes presently dependent on petroleum. This is a huge opportunity. The European industrial biotechnology industry has an estimated annual turnover of more than €60 billion, and the global industrial enzyme market is predicted to be worth US$7.1 billion by 2018. Detergent enzymes alone make up a billion dollar business. However, continued reliance on oil-based solutions will hamper growth and could have significant societal and environmental consequences. Replacing petroleum with bacteria will have a real impact on people’s lives. Suncream is a good example. One of the projects we are working on at Newcastle is to develop organic UV-absorber compounds from renewable materials to be used in sunscreens. Damage from exposure to UV radiation is a major worry, and there is increasing demand for cosmetics that block UV rays. The industry relies on oil-based technology and inorganic metal oxide particles to create materials that block UV rays for use in sunscreen. However, we know that photosynthetic bacteria called cyanobacteria that grow in the sea make their own organic sunscreen molecules. By taking the relevant genes from cyanobacteria and transplanting them into a bacterium that is already widely used in chemical production, we hope to be able to change this. If we are successful, the process could easily be scaled up so the cosmetics industry will be able to develop cheap organic sunscreen. This is just one example of the way in which bacteria could support a future without oil. Work is already in progress to explore the potential of using waste to grow bacteria or other micro-organisms that could make chemicals such as ethanol for use as “biofuel” for cars and aeroplanes, further reducing the use of oil.  There is a lot of work still to be done to make this vision a reality, but by continuing to investigate how bacterial cells work and how they could be used in chemical production we can see a future in which waste becomes energy and we can live without oil."
"

In this month’s issue of the _Economists’ Voice_ , Robert Whaples, chair of the economics department at Wake Forest, reports on a survey he recently conducted in which he sent questionnaires to 210 Ph.D. economists randomly selected from the American Economic Association. His charge: to find out how much disagreement there is within the profession and a number of high profile public policy issues.   
  
  
What did his respondents have to say about the impact that global warming will have on the economy? 



In short, the number of economists who thought global warming would improve the U.S. economy outnumbered the number of economists who thought that global warming would harm the economy to the extent feared by the _Stern Review_.   
  
  
Will those who demand that we bow down to the consensus of scientific opinion likewise demand the same regarding the consensus of economic opinion? Not bloody likely.
"
"

Media Contact: (202) 789‑5200





While the DC Court of Appeals has just ruled in favor of the Obama Administration in rejecting challenges to the Environmental Protection Agency’s rules concerning carbon dioxide emissions from cars and light trucks (the so‐​called “tailpipe emissions standards”), Senior Fellow Patrick J. Michaels believes the larger battle is still to come:



“On June 25, the public comment period for the EPA’s proposed regulations on coal‐​fired power plants ended,” said Michaels. “After thorough review, I found that the report from the U.S Global Change Research Program (USGCRP), which served as the source for the scientific opinions underlying the original endangerment finding in 2010, is unrepresentative of the larger body of scientific research on the topic of anthropogenic climate change and its potential impacts on the United States.”



Michaels, working with a team of experts and scientists, assembled an addendum to the USGCRP report, which they submitted as comments on June 22.



“Our review represents the most comprehensive scientific critique of the EPA Endangerment Finding on coal‐​fired plants ever written, and directly counters their claims on how climate change impacts in the United States, using a much more exhaustive survey of peer‐​reviewed science than the EPA relied upon,” said Michaels. 



Michaels also cautions against relying too much on static reports in rulemaking on climate change.



“No static report can provide long‐​term guidance as to the nature of climate change and its impacts, as this field is constantly evolving under the weight of new scientific findings. Consequently, it is imperative that the EPA reassess the current scientific understanding on at least an annual basis,” said Michaels.



The EPA is expected to finalize regulations regarding emissions from coal‐​fired plants later this year.



###
"
"Whether you’re into mining, energy or tourism, there are lots of reasons to explore space. Some “pioneers” even believe humanity’s survival depends on colonising celestial bodies such as the moon and Mars, both becoming central hubs for our further journey into the cosmos. Lunar land peddlers have started doing deals already – a one-acre plot can be yours for just £16.75.  More seriously, big corporations, rich entrepreneurs and even US politicians are eyeing up the moon and its untapped resources. Russia has plans for a manned colony by 2030 and a Japanese firm wants to build a ring of solar panels around the moon and beam energy back to Earth. We need to be clear about the legal validity of extraterrestrial real estate as the same ideas that were once used to justify colonialism are being deployed by governments and galactic entrepreneurs. Without proper regulation, the moon risks becoming an extra-planetary Wild West. To figure out whether “earthly” laws can help decide who owns what in space – or if anything can be owned at all – we must first disentangle sovereignty from property. Back in the 17th century, natural law theorists such as Hugo Grotius and John Locke argued that property rights exist by virtue of human nature but that they can only have legal force when they are recognised by a sovereign government. Within the context of space law, the big question is whether sovereignty reaches infinity – how high must you go to escape your country? When the US was confronted with this query in the early 1950s, it lobbied for the recognition of outer space as a global commons. The Soviet Union was difficult to infiltrate to gather intelligence, so open access to Soviet air space was crucial for the US during the Cold War. Perceiving outer space as a commons was also another way of preventing national sovereignty in space. But neither the USSR nor the US was keen to fight out the Cold War on yet another front. Geopolitics dictated the decision to treat outer space as being non-appropriable. This principle can be found back in Article II of the 1967 Outer Space Treaty which clearly forbids “national appropriation by claims of sovereignty, means of use or occupation by any other means”. It has been widely accepted: no one complains the various moon landings or satellites in space have infringed their sovereignty. However, legal commentators disagree over whether this prohibition is also valid for private appropriation. Some space lawyers have argued for the recognition of real property rights on the basis of jurisdiction rather than territorial sovereignty.  Historical records of the Space Treaty negotiations clearly indicate people were against private appropriations at the time, but an explicit prohibition never made it into Article II. Lessons have been learned from this omission and the ban was far more explicit in the subsequent Moon Agreement of 1979. However only 16 countries signed the agreement, none of which were involved in manned space exploration, leaving it somewhat meaningless as an international standard.  Consequently, space entrepreneurs such as Dennis Hope from the Lunar Embassy Corporation seem to think that there is a loophole in Article II which allows private citizens to claim ownership of the moon. Most space lawyers disagree however. They point out that states assume international responsibility for activities in space, whether by national companies or private adventurers, and therefore that the same prohibition extends to the private sector. So while the idea of buying some lunar real estate might be fun, in order for these plots to be recognised as property there needs to be legal recognition by a superior authority such as a nation state. As states are not allowed to claim sovereign rights in outer space, landed property on the moon and planets will in all likelihood be outlawed. Legal commentators are hopeful that states will remain loyal to the treaty and refrain from recognising or endorsing a private property claim. If there is a precedent, it lies at the bottom of the ocean. In 1974, the US government refused to recognise the exclusive mining rights of Deepsea Ventures to the seabed beyond the limits of national jurisdiction. But all of these arguments are rather theoretical. If you just simply occupy a place and no one else can access or use it, aren’t you the de facto owner? Lawyers call this corporate possession (corpus possidendi) and it represents another reason why title deeds cannot be a legal proof of lunar ownership – no one is physically there. In order to possess something, both mind and body need to be involved. Intention alone is not sufficient; possession also requires a physical act.  The difficulty of physically establishing an act of possession on the moon should protect it from private development, but it seems technology is once again outsmarting the law. Back in the late 1990s commercial firm SpaceDev intended to land robotic prospectors on an asteroid to conduct experiments and claim it as private property. The project eventually ran out of funds and was shelved, but advocates of such “telepossession” point to cases of salvage companies claiming undersea wrecks as property after exploring them with robots. After all, if an undersea probe with a TV camera was all that was required to take possession of a (previously owned, earthly) shipwreck, why shouldn’t a space probe be enough to take possession of an unowned and unclaimed patch of celestial real estate? Though legal ownership of the moon or Mars is prohibited, the appropriation of material is a whole different matter. It looks like entrepreneurs could claim something like “enterprise rights” that allows them to explore and exploit natural resources in outer space.  I get the uncomfortable feeling of a déjà vu. Was it not Locke’s property theory that justified possession over nature and vacant land and eventually led to the colonisation of the Americas?  Let’s hope that the international community and individual states come to their senses before it’s too late and get to sign and ratify the Moon Agreement which might give us a little bit of hope that we can avoid another enclosure movement. Recent conflicts over Ukraine, the South China Sea or Syria have raised talk of a “new era in geopolitics”. They may also rekindle the realisation that outer space should not become the next playground for conquest."
"A cross-party committee of MPs, green campaigning groups, business leaders and climate experts is needed to advise the government on crunch UN climate talks later this year to put the UK’s hosting of the COP26 talks back on track, the Liberal Democrat acting leader, Sir Ed Davey, is expected to say. His call, which will form part of a speech on climate delivered at Birkbeck College in London on Thursday, comes after the energy minister, Kwasi Kwarteng, told a meeting of ambassadors that the UK could not afford to allow the talks to fail because of the additional pressures of Brexit.  “If you want global leadership [on the climate crisis], you have to show that global action on the climate can be politically popular,” Davey, former energy and climate secretary in the Conservative-Lib Dem coalition government, told the Guardian. “We need NGOs, politicians and business leaders to form a committee to help to reach greater dialogue on international action. The UK needs to lead on this, for COP26 and beyond.” Davey criticised the government’s handling of COP26 planning so far, which got off to a bad start nearly two weeks ago with the abrupt sacking of the intended president, former energy minister Claire O’Neill, days before the scheduled launch. O’Neill went on to make vitriolic public attacks on Johnson. “It has been shambolic,” said Davey. “Insiders are tearing their hair out. If you want to be credible hosts, you have to engage with people, with society, with NGOs far more.” Leading figures in international climate diplomacy have expressed increasing disquiet over the UK’s conduct of the talks so far, warning that ministers and officials are falling behind on the preparations needed. Mary Robinson, former president of Ireland and twice a UN climate envoy, said the perception that no one in the UK wanted to serve as president – after the post was turned down by David Cameron and former foreign minister William Hague – was damaging. Kwarteng was forced on Wednesday morning to reassure international participants that the talks would be a success and that the government was treating them as a priority. He attended a high-level meeting of previous COP presidents held by the International Energy Agency in Paris, where he was questioned by the Kenyan government and others on the UK’s commitment. “We can’t guarantee success, it’s not something we can gold-plate, but this is absolutely our No 1 priority as a government,” he said. “We really cannot afford it to be a failure, at an international level and at a national level, given where we are with Brexit and other issues.” On Wednesday, fresh controversy over the Glasgow venue for the talks added to the sense of confusion, after a government source told the Financial Times that ministers were considering booking the ExCeL conference centre in London as an alternative. The Scottish government had received no notification of any intention to move the venue, a spokesperson said. “Scotland looks forward to welcoming the UN delegates and participants from around the world. Glasgow is recognised internationally for its strong track record at hosting major international events, and we are working collaboratively with the UK government, Glasgow city council, Police Scotland and other partners to ensure the conference is a success.” UN officials visited the Glasgow venue this week and liaised with all the authorities involved. Nicola Sturgeon, Scotland’s first minister, told a conference in London on Tuesday that she hoped there could be a constructive relationship between her government and Boris Johnsons’s in the approach to COP26. Davey said changing the venue would “send the wrong signals, especially post-Brexit. We are stronger in the climate change debate internationally if we are a united kingdom.” Greenpeace welcomed Davey’s call for an independent committee that would include civil society. Rebecca Newsom, head of politics, said: “There’s certainly a need for a more transparent, inclusive and joined-up approach to the UK climate summit, and this could be one of the ways to achieve it. Cross-party committees have a strong record of holding governments to account, and given how planet-critical this summit is, the case for more scrutiny and transparency is overwhelming.” The Green party’s co-leader Jonathan Bartley said: “At the moment COP26 is heading for failure because the government is unwilling to step up and take the responsibility and action necessary to facilitate its success. So, we’re on board with the fact that scrutiny of the process is crucial. “But we would need to see what actual powers this cross-party committee would have and to what extent the government will be required to act on its recommendations. It also needs to be linked to a grassroots-led proper democratic programme of engagement that genuinely seeks to empower and strengthen communities.”"
"It was our 14th expedition to the trenches of the Pacific Ocean, where depths can exceed 10,000m. And it was due to be our last for the foreseeable future. We had been aboard the Schmidt Ocean Institute’s (SOI) vessel RV Falkor for 30 days. It was almost over. Then, it turned out to be “the big one”. For this was the expedition in which my colleagues and I discovered a snailfish living some eight kilometres below the waves, deeper than any fish we know of. My colleagues from the University of Hawaii even recovered some in their traps.  In the past six years we have made many discoveries in the depths, such as the missing order of Decapoda (shrimps) that were long thought absent from the trenches but are actually rather conspicuous.  In the Kermadec Trench off New Zealand we found the “supergiant” amphipod, a crustacean 20 times larger than its shallow-sea relatives. We also filmed large numbers of tadpole-like snailfish in multiple trenches, and as deep as 7700m in the Japan Trench. Based on these observations we predicted that when exploring the Mariana Trench – the world’s deepest – we would find the the Mariana’s own personal snailfish, probably living between 6500m and around 7500m, with more being found at the deeper end of that range. We also predicted that we would see the decapods and supergiants in the upper depths of the trench, and right enough there they were. A device used to gather samples of ocean floor had an inspection camera on it to monitor the equipment.  One night after a dive to 7900m when watching the footage coming back in, a strange ethereal little fish swam past.  That got our eyebrows raised.  It looked like a snailfish, but was extremely fragile (even for a snailfish) and had a very distinctive appearance. This prompted a case of “game on”, to find it again, and sure enough we did.  The deepest we found it was at 8145m, nearly 500m deeper than our personal record from the Japan Trench. This of course means that our predictions were slightly wrong, but also makes it very exciting: there  are still fish, and perhaps other things, down there to discover and this is what drives us to do more. Our work at the deepest place on Earth is not done yet. As much as we are excited about finds such as these, we are typically chased up by people who ask “why do we bother?”, and add rather deflating comments such as “what benefit does this have to society?” In response I explain that such exploration benefits responsible stewardship of the oceans. In the long term, conservation and maintenance of the our seas relies on us really understanding the ocean – that is, the ocean in its entirety from the surface to what lies beneath the deepest seafloor. The anthropocentric opinion of “out of sight, out of mind” simply doesn’t cut it, and is sadly still common place. The deep ocean is far deeper than a person can dive to or fish from, but that doesn’t mean that the things down there are of no consequence to society.  We must not, however, confuse curiosity-driven exploration with the search for entertainment or stockpiling consumables. We know that the deep sea is not exempt from a changing climate or man-made disturbances such as plastic pollution. The depths are intrinsically linked to processes in the upper ocean that we humans are continually meddling with. Changes that happen in the upper ocean will have an effect on the largest habitat on Earth, yet people question why we study the deep sea.  We say, how can we conserve the largest habitat on Earth if we know nothing about it?  In the quest to understand the entire ocean, people have to study the shallow bits, the deepest bits and everything in-between."
"Although only a small area of land has been offered to companies exploring the potential for fracking in the UK so far, much more is likely to come. But opposition to fracking is growing – and growing fast. More than 180 local groups are already in operation, which is somewhat inconvenient for a government wanting to go “all out for shale”. In interviews, online surveys and correspondence with anti-fracking protesters, we’ve heard personal testimony that suggests that problems with fracking are not simply environmental. More than 400 peaceful protesters have been arrested – and people in the anti-fracking movement have claimed political policing and intimidation are being used, citing the actions of the Greater Manchester Police and Sussex Police at these two locations as particular examples. Our analysis of interview and questionnaire data suggests that protesters consider their rights to freedom of peaceful assembly, freedom of expression, liberty and security of person, a fair trial and respect for a private and family life, have been threatened. Each of these rights is protected by the Human Rights Act, the European Convention on Human Rights, and the International Covenant on Civil and Political Rights – all of which the UK is legally bound to observe. A report launched on October 30 from the Bianca Jagger Human Rights Foundation details the UK’s human rights commitments and calls for a fully independent human rights impact assessment of fracking developments before exploration gets underway. Protesters in Balcombe and Barton Moss reported facing violence, forcible removal without arrest and kettling. Interview respondents told us they were “kicked and pushed and punched”, “pushed and shoved in the back”, “pushed off the road by the police”, and “shoved in the back repeatedly”. Police behaviour was often described as “brutal”, “violent”, “thuggish”, “rough” and “very, very aggressive”. References were made in several interviews to “arrest quotas”. At Barton Moss, throughout the autumn and winter of 2013, one interview respondent reported that there were five arrests every day and that officers were heard saying “we need one more arrest”. The same respondent said they believed that the use of arrest quotas was “almost certainly planned in advance” and was being used as a long-term plan that would ensure “eventually everyone would be arrested”. Greater Manchester Police has denied these claims. Our data suggests the anti-fracking movement is concerned that such patterns of arrest may effectively serve to shut down protests – and certainly if protesters are arrested at one event and then again at another for breaching their bail, the logical conclusion is that, over time, fewer protesters would attend demonstrations. Queries were also raised about the legality of some of the arrests made at Barton Moss. Protesters say they were were accused of obstructing a public highway when they were on a private road leading to a drilling site. Interviewees said protesters at both Balcombe and Barton Moss were arrested for “obstructing a police officer” if they fell over in front of them. Several interview respondents raised concerns that emails, phones and social media were being monitored by the police. Although, as one interviewee indicated, such activities are difficult to prove, some respondents were insistent in their belief that it was happening. One even reported having been visited at home by two members of the Counter Terrorism and Domestic Extremism Unit after filming at a potential drilling site. The increasing opposition to fracking suggests the government has failed to adequately respond to local and national concerns over the human rights implications of fracking or provide sufficient opportunities for public participation in decision making. Both interview and survey respondents expressed dissatisfaction with the amount of consideration being given to their concerns as this controversial energy source is explored. No wonder, then, that some said the process of introducing fracking in the UK is eroding democratic values. Respondents expressed the feeling that a particular disregard had been shown for people living in proximity to exploratory sites and a disillusionment with the official avenues of complaint on offer. Not all anti-fracking activity in the UK has been lawful. It has included the occupation of energy firm Cuadrilla’s offices in Blackpool and the blockading of roads to exploratory drilling sites. But these events indicate the extent to which people have lost faith in the government’s approach. It seems likely that acts of both civil disobedience and peaceful protest will occur with increasing frequency as local communities realise the extent of the extractive industry’s impact. In anticipation of fracking development, the government needs to commission genuinely independent human rights impact assessments for all communities before any extractive activity begins. The potential rights violations described above suggest that fracking development can no longer be considered in separation from the civil and political sphere. These examples show the extent to which the fundamental rights of UK citizens who oppose governmental policy are at risk."
"
Share this...FacebookTwitterClimate journalism is like being at a third-world bazar where the media behave like merchants all shouting, pitching their catastrophe stories.Die Welt’s recent piece From Proud Jordan River, To A Smelly Trickle (roughly translated) features the crisis of water consumption and the injustice of water’s uneven distribution. Now water needs to be redistributed, along with wealth and misery.
Although the article is mainly a rant against Israeli water policy, its other objective is to admonish western societies for their profligate use of water.
This is a theme that’s steadily gaining traction on the environmental front here in Europe – along with biodiversity, ocean acidification, manmade microscopic aerosols and climate change. It’s the latest hot-seller catastrophe joining the enviro-bazar.
Die Welt doesn’t hold back citing environmental and activist groups for its reliable, “unbiased” and shocking information. At first the story focusses on Israeli water management and how it’s unfair to neighboring countries.
Die Welt writes:
According to Amnesty International, the average Israeli consumes 300 liters of water daily, while a Palestinian consumes only 70 liters. In poor regions a mere 20 liters is available daily for each person.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the end of the story, Die Welt admonishes western lifestyles and its excessive use of water.
The story concludes with a photo gallery that informs readers how much water consumption is needed to manufacture some basic daily products we enjoy in our daily lives. Examples:
1 hamburger: 2400 litres
1 hardboiled egg for breakfast: 135 litres
1 slice of bread: 40 litres
10 grams of cheese: 50 litres
1 cup of coffee: 140 liters
1 German breakfast: 365 liters
200 grams of potato chips: 185 liters
2-gram computer chip: 32 liters
1 sheet of paper: 10 liters
1 cotton T-shirt: 4100 liters!
1 pair of cowhide shoes: 8000 liters
1 new car: 450,000 liters
The idea is to tell us consumers that we are simply consuming too much water and that it’s having catastrophic impacts on the environment and poor people. It’s unfair and it has to be regulated. We need to feel guilty about it.
Wikipedia lists the potential manifestations of excessive water consumption:
There are several principal manifestations of the water crisis.
– Inadequate access to water for sanitation and waste disposal for 2.5 billion people
– Groundwater over drafting (excessive use) leading to diminished agricultural yields
– Overuse and pollution of water resources harming biodiversity
– Regional conflicts over scarce water resources sometimes resulting in warfare.
Expect the water crisis to get worse (not in reality, but in the media and political world). Get ready to hear a lot more about this in the future. Water-saving devices will be joining energy-saving devices soon in the government’s force-the-people-to-buy-list.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterScience turns malignant
Next week on August 3, I’ll be releasing my latest list of climate scandals, a gate-update (see Current list of climate scandals). This is a month earlier than originally planned.
The list that’s posted now is visited on average about 100 times a day. Clearly it has become some sort of resource.
Unsurprisingly, the new list coming out has grown, and it will continue to grow. This is assured because of the way climate science is operated, funded, rewarded and politicised.
The disease is hopelessly chronic and there is no treatment in sight. The system is designed, built and programmed to keep producing many more scandals. Already I see dozens of new gates in incubation.
“Climate science” even has its own immune system that works to keep out the deadly virus called “truth”.  The reality of that immune system became clear with the Muir Russell, Oxburgh and Penn State enquiries.
Soon I will have to break the climate-gate list into Volumes 1 and 2.  I have no doubts about this.
The real threat, unfortunately, is that “climate science” risks becoming a malignant cancer that will threaten to spread to other fields of science and to our public institutions. In some cases it already has. Because of “climate science”, the public is losing trust in science and the civic institutions that are supposed to police it. As mistrust of climate science reaches ever higher levels, so will the public mistrust in other fields.
Scientists in these other fields need to take notice.
Share this...FacebookTwitter "
"
Recently we’ve been discussing products for the AIRS satellite instrument (Atmospheric InfraRed Sounder) onboard the Aqua satellite. For example we’ve been looking at the only global image we can find of CO2 from its data made in 2003, wondering where the remainder of them are.
In my digging I discovered that the Apache webserver had open directory listings for folders, and this allowed me to explore a bit to see what I could find. in the \images folder I found a few images that I did not see published on the AIRS website. I’ve saved them to my server should they go offline, but have provided links to the original source URL.
One for Sea Surface Temperature at the tropics seems interesting, though the data period is too short to be meaningful. Note that to eliminate cloud issues, the soundings are done when the satellite has a lookdown to “clear sky”.
Original source image: http://airs.jpl.nasa.gov/images/Aumann_SST_graph_543x409.jpg
I find it interesting that there is a slight global cooling of the oceans during this period of September 2002 to August 2004. The question is: where is the rest of the data and why has the AIRS group not been presenting it on their website? It is after all a publicly funded NASA program.
It is also interesting that this goes against one of the “signatures” of an AGW driven warming. Dr. David Evans writes in this essay:

“The signature of an increased greenhouse effect  is a hotspot about 10 km up in the atmosphere over the tropics.”

“The signature of an increase in well-mixed greenhouse gases (such as due to carbon emissions). Warming would be concentrated in a distinct “hot spot” about 8 – 12 km up over the tropics, less warming further away, turning to cooling above 18 km.”


What I’d REALLY like to see is the January version of this map:

Unfortunately, the January version of this image is unavailable. It would be interesting to see if the concentrations in the northern hemisphere maintain which would point to industrialization sources. Or, if the pattern flips, and we see concentrations decrease in the NH and increase in the SH, that would point to seasonal variation and thus likely be driven by biomass.
I’ve put in a request to the AIRS group for the January 2003 image, and others, we’ll see what happens.
UPDATE: 7/31/08 I got a response, see this new posting


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d9da21d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

This was an eventful week for two government institutions, the Supreme Court and Senate. More than a year after Justice Antonin Scalia’s death, the high court will on Monday finally return to a full complement of nine justices. But the confirmation of the newest justice, Neil Gorsuch, happened only after the Senate decided, on a party‐​line vote, to exercise the “nuclear option” and remove filibusters for Supreme Court nominations.



These developments sound like a really big deal, but they were easily predictable given our toxic political climate and won’t actually change the operation of either institution. But here are five takeaways for our post‐​nuclear‐​option world:





By filibustering the milquetoast Gorsuch despite the high probability and repeatedly expressed intention of the Republicans to go nuclear, the Democrats have destroyed any leverage they had over the next nominee.



 **1.** The Supreme Court. The court effectively returns to the status quo before Scalia’s death. No two justices are the same, but Gorsuch could have been expected to vote the same as Scalia on all the hot‐​button cases that broke down 5–4, and also on the cases (especially in criminal procedure) that joined the court’s left and right against the middle. As it turns out, Scalia’s absence only changed the result in a handful of cases and the court has largely succeeded in avoiding 4–4 splits. Adding a ninth justice will, however, make it marginally easier to get the four votes needed to “grant cert” (have a case accepted for review), especially on potentially controversial issues.



 **2.** The Senate. The exercise of the “nuclear option” returns Senate procedures to what they were 15 years ago. The filibuster was simply not employed for partisan purposes against a nominee who had majority support before Harry Reid started filibustering George W. Bush’s lower‐​court nominees in 2003. (Infamously, the Senate denied Miguel Estrada an up‐​or‐​down vote seven times to prevent Bush from later having the opportunity to elevate the first Hispanic justice.) Reid used the “nuclear option” to eliminate that sort of filibuster a decade later, so perhaps this week’s action should be called “thermonuclear.” A Senate majority will still be able to stall a nomination made by a president of the opposing party—we could see more Merrick Garlands—but a Senate minority will lack that power.



 **3.** The next nominee. By filibustering the milquetoast Gorsuch despite the high probability and repeatedly expressed intention of the Republicans to go nuclear, the Democrats have destroyed any leverage they had over the next nominee. Should there be another vacancy under President Trump while the GOP controls the Senate, there will be zero incentive for the President to moderate his choice. It’s not at all clear that Republican senators such as Susan Collins of Maine, Lisa Murkowski of Alaska, Lindsey Graham of South Carolina and other “institutionalists” would’ve gone along with a “nuclear option” to replace Justice Ruth Bader Ginsburg with a nominee more controversial than Gorsuch. But now they won’t face that dilemma.



 **4.** Our political culture. Given the highly charged battle we’ve seen — only three Democrats, from states Trump won bigly (Indiana, North Dakota, West Virginia), voted for Gorsuch, and just one more, fellow Coloradan Michael Bennet, voted against a filibuster — too many people will now think of the justices in partisan terms. That’s too bad, but not a surprise when contrasting methods of constitutional and statutory interpretation largely track party politics. Relatedly, confirmation hearings will continue to be Kabuki theater, educational to some about various legal doctrines but not illuminating anything of the nominee’s judicial philosophy. On the other hand, perhaps nominees will occasionally feel free to express themselves, knowing that they don’t need any of the minority party’s votes.



 **5.** Neil Gorsuch. You may not agree with him on every case, but his opinions will be well‐​reasoned and clearly written. Gorsuch’s mentor, Justice Byron White, liked to say that each new justice makes for a new court, and I look forward to the breath of fresh air, intellectual rigor, collegiality, and constitutional seriousness that Justice Gorsuch will bring. Neil Gorsuch will serve with distinction.
"
"
Share this...FacebookTwitterClimate nonsense will lead to this trend in electricity prices.

German journalist Günter Ederer has a piece at the online Fuldaer Zeitung called The Electricity Bill Is Going Up And Up. Hat-tip Detmar Doering.
Politicians in all parties in Germany, from the communists to the conservatives, and everything in between, are all racing to be the first to jump off the let’s-save-the-climate cliff.
All have made rescuing the climate a target that absolutely has to be achieved – no matter the cost. Politicians of every stripe have pledged to cut Germany’s CO2 emissions 80% by 2050. Minister of the Environment Norbert Röttgen, of the conservative (in name only) party, even sets it up as a life and death matter, as absurd as it sounds:
That rescues our climate.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




No kidding. Everything else takes a back seat to this imperative – consumers be damned. Taking the little guy to the cleaners in Germany knows no limits. And so, expensive green energy sources like wind and solar are being subsidized with total abandon, and consumers are getting the big-time shaft through skyrocketing electric bills- The government profits in the end. Ederer writes:
Renewable energy must be fed into the power grid at a mandated fixed price, which for the consumer will mean a price increase of 6 cents per kwh for 2011 alone.
What benefit will all this pain render consumers? It’s a fact that CO2 emissions globally are going to continue increasing, as most countries outside of Europe could give a rat’s rear about foregoing prosperity in order to play the make believe game of rescuing the planet.
If Germany succeeded in reducing its CO2 emissions 80% by 2050, what theoretical impact would it have on the environment? Ederer tells us:
If Germany reduced its share of CO2 by 80%, or even 100%, then it help to warm the planet 0.0072 °C less. As I said earlier, that’s if all figures and calculations of the IPCC are correct.
Meanwhile, the numbers are in from the German Weather Service. October 2010 was 0.9°C colder than average. If Germany’s greedy politicians get their way. This may someday be corrected to 0.9072°C cooler.
Share this...FacebookTwitter "
nan
"
I don’t know what it is with weather stations at some universities. Of course we have the station at University of Arizona Tucson in the parking lot, and this one isn’t too far from that arrangement. It has a long and uninterrupted history, but what is it really measuring?

Click for a larger image
More pictures here
Thanks to surfacestations.org surveyor Craig Limesand we get to see the official USHCN climate station of record at Mount Mary College in Milwaukee, Wisconsin. You can see that the Stevenson Screen is just a few feet from parked cars.
This aerial view shows it better:

Click for a larger live interactive image
Note that in addition to being surrounded by asphalt and parked cars, the station is also about 35 yards from the college power plant.
According to NCDC MMS database, the station has been in this location since at least 1948, unmoved and using mercury max-min thermometers even today.
But without doing a historic evaluation to look at what transpired around the station during that history, how would we know how much is this signal is “climate change”, “UHI from Milwaukee”, or “increased parking capacity” or all of the above?
From NASA GISS, click for original source plot
 As much as I like weather stations, it is becoming clearer to me that looking for a clean climate change signal in surface data is a complex excercise in uncertainty.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9dd7a201',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"BP’s got a new boss, Bernard Looney. He doesn’t wear a tie, he’s on Instagram and he’s going to shrink its carbon footprint to “net zero” by 2050. Is this for real? It’s a sign the tide is turning. Maybe not enough to save us from catastrophic sea-level rises, but a turn nonetheless. The oil industry is incredibly savvy when it comes to public opinion, and can see the steady erosion of its “social licence to operate” (a company’s ability to go about its business without too much challenge). It has been struggling to recruit young people for years, well before the school climate strikes started. The Royal Shakespeare Company and National Galleries Scotland have both turned their back on BP sponsorship in recent months, and last weekend more than a thousand people turned up at the British Museum to protest at the firm’s involvement there. BP is not the first oil company to give itself a lick of green paint to appear more acceptable in this era of increasing climate concern. We’ve seen Statoil dropping the word “oil” with the refreshed identity of Equinor, and Dong (Danish Oil and Natural Gas) relaunched as the renewable energy company Ørsted. It’s hard to see Equinor as anything more than greenwash while it’s still drilling the Arctic, but Ørsted may yet become the world’s first green energy supermajor, betting on the power of offshore wind to eventually see off natural gas. Shell is also keen to show off its green credentials, as anyone who has seen its multimillion pound advertising campaign can tell. It’s scared, or it wouldn’t bother. BP has been here before, with a £100m “Beyond Petroleum” rebrand in the early noughties. Alongside a new sunflower logo, this emphasised the company’s commitment to wind, solar and biofuels alongside oil and gas, but renewables remained a small part of its portfolio. Clean energy is good for press photos, but not really central to BP’s core business model. Environmentalist Jonathon Porritt originally tried to engage in its initiatives, but turned away in disgust, declaring it was impossible for today’s oil majors to change at the radical speed required. If BP is serious about rising to the challenge of the climate crisis, it will have to go the extra mile to convince us. It’s one thing for a computer company to make bold claims on climate action (as Microsoft did last month, announcing it would be “carbon negative” by 2030), but it’s another for an oil major. And yet, so far, Looney’s new vision is light on specifics. Apparently we have to wait until September for the details, but it does seem clear that BP will still be selling oil and gas. BP seems to be banking on the “net” in net zero doing a lot of heavy lifting. Which leaves the rather basic question: how does it plan to balance out carbon emissions produced by burning fossil fuels? Trees are great for helping us mop up some of the damage we’ve already caused, and various other technological options might turn out to be useful on a small scale. Capturing carbon from the air and using it to make plastic; adding iron to the ocean to speed up its ability to absorb carbon; or genetically modifying trees so they have larger, carbon-sucking roots, for example. But a lot of this is still a bit sci-fi, and none of it is ever likely to soak up the quantities of oil and gas BP plans to keep selling. Looney says he wants to help the world to get to net zero, not just BP. It is, apparently, shutting down the “Possibilities Everywhere” ad campaign that showcased its (relatively small-scale) work on clean energy, instead funnelling resources into campaigns that foster “the right kind of change”. This is the part of the project we should perhaps be most wary of. It’s worth remembering that with the Beyond Petroleum push in the noughties, BP popularised the idea of a “carbon footprint”. The approach seemed to mean well but was effective at individualising the causes of climate crisis, pulling attention away from the sorts of large-scale change that would really challenge the fossil fuel companies. Beware oil execs in environmentalists’ clothing. They may simply wish to seize the growing energy for change and steer it towards their own ends: the continued burning of fossil fuels. The most offensively disingenuous idea at the heart of this flashy new strategy is that BP finally “gets” the climate crisis. Oil runs off many things, and one of them is strong science. Fossil fuel companies have long known their business was dangerous and yet they chose to keep profiting off it. Indeed, new data shows quite how much they profited ($332bn in the last three decades in the case of BP). BP hasn’t suddenly got a dose of the climate heebie-jeebies, it’s just worried that, finally, you have. The Instagramming, tie-free Looney might reflect a new phase in the oil business, but is it still business as usual underneath? It’s hard not to see the rhetorical use of the “net” in net zero as a bit of smoke and mirrors, a way of making it sound as if it is taking the crisis seriously while avoiding the simple truth: we need to stop burning fossil fuels. If BP really wants us to believe that a company that made its name in oil can be part of the solution, it needs to stop drilling. It’s as simple as that. Hold retirement parties for refineries and retrain workers for a zero-carbon future, and do it fast. That might actually be something worth Instagramming about. • Alice Bell is co-director at the climate change charity Possible"
nan
"

From this page (h/t Dave Hagen)
The U.S. Environmental Protection Agency (EPA) is inviting comment from all interested parties on options and questions to be considered for possible greenhouse gas regulations under the Clean Air Act. EPA is issuing an advance notice of proposed rulemaking (ANPR) to gather information and determine how to proceed.
The Advance Notice
The ANPR is one of the steps EPA has taken in response to the U.S. Supreme Court’s decision in Massachusetts v. EPA. The Court found that the Clean Air Act authorizes EPA to regulate tailpipe greenhouse gas emissions if EPA determines they cause or contribute to air pollution that may reasonably be anticipated to endanger public health or welfare. The ANPR reflects the complexity and magnitude of the question of whether and how greenhouse gases could be effectively controlled under the Clean Air Act.
The document summarizes much of EPA’s work and lays out concerns raised by other federal agencies during their review of this work. EPA is publishing this notice at this time because it is impossible to simultaneously address all the agencies’ issues and respond to the agency’s legal obligations in a timely manner.
Key Issues for Discussion and Comment in the ANPR:

Descriptions of key provisions and programs in the CAA, and advantages and disadvantages of regulating GHGs under those provisions;
How a decision to regulate GHG emissions under one section of the CAA could or would lead to regulation of GHG emissions under other sections of the Act, including sections establishing permitting requirements for major stationary sources of air pollutants;
Issues relevant for Congress to consider for possible future climate legislation and the potential for overlap between future legislation and regulation under the existing CAA; and,
Scientific information relevant to, and the issues raised by, an endangerment analysis.

EPA will accept public comment on the ANPR for 120 days following its publication in the Federal Register.
Background
In April 2007, the Supreme Court concluded that GHGs meet the CAA definition of an air pollutant.  Therefore, EPA has authority under the CAA to regulate GHGs subject to the endangerment test for new motor vehicles – an Agency determination that GHG emissions from new motor vehicles cause or contribute to air pollution that may reasonably be anticipated to endanger public health or welfare.
A decision to regulate GHG emissions for motor vehicles impacts whether other sources of GHG emissions would need to be regulated as well, including establishing permitting requirements for stationary sources of air pollutants.
How to Comment

Comments should be identified by the following Docket ID Number: EPA-HQ-OAR-2008-0318
Comments should be submitted by one of the following method

www.regulations.gov: Follow the on-line instructions for submitting comments.
Email: a-and-r-Docket@epa.gov
Fax: 202-566-9744
Mail: Air and Radiation Docket and Information Center, Environmental Protection Agency, Mailcode: 2822T, 1200 Pennsylvania Ave., NW., Washington, DC 20460. In addition, please mail a copy of your comments on the information collection provisions to the Office of Information and Regulatory Affairs, Office of Management and Budget (OMB), Attn: Desk Officer for EPA, 725 17th St. NW., Washington, DC 20503.
Hand Delivery: EPA Docket Center, EPA West Building, Room 3334, 1301 Constitution Ave., NW, Washington DC, 20004. Such deliveries are only accepted during the Docket’s normal hours of operation, and special arrangements should be made for deliveries of boxed information.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9dc4ba27',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Striking students have joined Valentine’s Day rallies across the world as the protest movement attempts to ratchet up pressure on governments and companies before crunch UN climate talks in Glasgow later this year. In London, the young demonstrators held banners proclaiming “Roses are red, violets are blue, our Earth is burning and soon we will too” and “Climate change is worse than homework” as they marched through Parliament Square on Friday to mark the first anniversary of nationwide climate strikes in the UK. Students in Durham, Glasgow, Brighton and dozens of other cities also braved often wet and cold condition to march through the streets chanting, “What do we want? Climate justice. When do we want it? Now.” Greta Thunberg, who initiated the movement as a solitary striker in Stockholm in August 2018, said climate strikes were planned in 2,000 cities across the world on Friday, and that bigger actions were planned for the coming months. In many countries, the protests have expanded to include local environmental concerns, new strategies and stronger emphasis on global climate justice. In India on Friday, strikers turned their focus on government plans to deforest swathes of the Aravallis mountain range, which is a conservation area that provides freshwater and oxygen for Delhi and other cities. Some carried banners in English reading: “I love Aravallis”, “Our green lungs” and “Protectors are turning destroyers”. In Sydney, climate strikers demonstrated with banners that depicted the devastating bushfires and blamed the government of Scott Morrison for the “climate chaos” that has hit Australia. In the Philippines, climate strikers organised an educational storytelling campaign to raise public awareness. In Scotland, Holly Gillibrand, who was one of the first strikers in the UK when she started a vigil outside Lochaber high school in Fort William in the Highlands, said the growth of the movement had been incredible. “When I began striking over a year ago, Greta Thunberg and Fridays for Future [campaign] were not well known at all and I was one of very few strikers in the UK, but since then, everything has changed. The movement has gone from one person to 7.5 million. “Even if we still aren’t getting the radical action we need from governments, politicians are feeling the pressure to act and we just need to keep pushing, keep shouting, keep rebelling until they do.” Holly continued her strike on Friday, with a hot chocolate to help get her through the wet weather. Among those striking for the first time on Friday was a group in Rwanda, where protesters tweeted Timages of themselves holding signs that said: “Rwanda stand for climate.” Friday’s action was not intended as a mega-strike like those in September, when more than 6 million people took part, but it showed how the campaign has evolved. Maryam Grassly, 17, who was among the hundreds of strikers in central London, said: “The most enthusiastic and passionate people have stayed on, and more people have got involved and been inspired by it, but at the same time we’ve lost the people who didn’t really care.” But in her mind there was no question about the relevance of the protest. “When the election happened and Boris [Johnson] got voted in I just kind of gave up on the world, I thought that’s it,” she said. “We’ve got the climate emergency and net zero by 2050 but nothing is happening as fast as it should be.” Similarly she was “excited but worried” about the UK hosting Cop26 later this year, saying the previous conference of the parties in Madrid did not go well. But the protesters were still hopeful about the difference they have made over the past few months. Maude Brown, 17, said: “I’ve been very concerned about the issue for almost a decade, I was taught about it in year 4, and no one really cared then so I’m happy that everyone is concerned about it now. I think as more politicians from our generation come in we can make a lot of change in the future.” Most of the strikers were teenagers, but there were some younger children too. Claire Bullivant, from Essex, has been bringing her children Imogen, 12, Max, 9 and Theodore, 5, to the strikes for a few months. “I just think it’s so important to get them involved and to support them because they’re so aware of what’s happening in the world, and they see it in the news, and we don’t want to sit back and do nothing,” Bullivant said. While her daughter’s school has been supportive of her decision to take time off for the strikes, her sons’ school has been less so, but Bullivant said the time off was “totally justified”. “It’s educational, it’s empowering to be part of a community and show that they’re trying to make a difference to the world, I just don’t think you can underestimate experiences like this,” she said. Asked why she was so keen to get involved, Imogen said: “If we don’t save the Earth there’s not going to be anywhere else for us to live.” A year ago, the size of the protests in the UK took police by surprise, as thousands defied their teachers to skip school and join the still nascent movement. The students are now backed by longer established environmental organisations, including Global Justice Now, Greenpeace and the Green party. Among those at Friday’s march in London was a trade union climate bloc. Friends of the Earth are backing the school climate strikers, who it credits for shifting public opinion. There is still a long way to go, but with technology developments and strong policies, the group said there was cause for hope. “Huge change is possible. In 2019, the UK went coal-free for 19 days. That’s the longest break since the 1880s, and something that would have been unthinkable a few decades ago,” it said."
nan
"Demon vs killer shrimp sounds like the latest CGI movie to come out of Hollywood. But in fact these are two particularly pernicious crustaceans that have been making their way westward across Europe from countries surrounding the Black Sea, eradicating native freshwater rivals en route. Unlike the meatier species we might be more familiar with from our dining plates, the killer shrimp (Dikerogammarus villosus) and the demon (D. Haemobaphes) are technically amphipods, a group of small, flattened, shrimp-like creatures. They’re barely bigger than a finger nail, yet they possess the most striking and dramatic of common names.  The titles are somewhat justified. These shrimp not only out-compete the slightly smaller native (Gammarus pulex) for space and food, as often happens with invasive species, they go much further – killer and demon shrimp directly prey on the hapless locals. The equivalent would be grey squirrels not only out-competing and spreading disease among the arguably cuter native red squirrel but also actively hunting and eating them. The killer shrimp reached the UK in 2010 and within a year was denounced by the Environment Agency as the nation’s single most damaging invasive species. The demon was discovered in Britain in 2012 but has spread substantially further throughout the country’s river systems; in shrimp terms, it seems demons are even worse than killers. Yet it is the evocative names given to their worst enemies that may yet save the native shrimp and their ecosystem. When a charismatic animal faces eradication through habitat loss, pollution or competition from invaders, the public are likely to take notice. Great news if you’re an elephant, say, or a red squirrel. Very often though, the tiniest animals go ignored despite their hugely important role in sustaining food chains – there’s a reason the WWF uses a panda for its emblem rather than a shrimp. Whoever coined the phrases “killer” and “demon” shrimp, therefore, did so in a moment of genius. The names have captured the public imagination and the idea of an evil crustacean has generated considerable interest. Invasive species are said to cost the UK economy a staggering £1.7 billion through eradication programmes and the economic and ecological damage to fisheries and tourism.  Given the expense and upheaval involved in fighting the invaders, perhaps we should just leave things alone – after all, does it really matter to the ecosystem if one tiny crustacean replaces another? It’s an important question. Invasive species don’t always slot neatly into food-webs where the last species disappeared. Instead, as in the case of killer shrimp and potentially the demon, these omniverous creatures can occupy multiple parts of a food chain. Consequently, this has the potential to change the food transfer from plants right through to certain fish, bird and mammal species. The native shrimp primarily feeds on fallen leaf litter and vegetation while the invaders also eat fish eggs/larvae, insects and “rival” shrimp. The new arrivals might also breed at different times, grow at different rates, have different numbers of young and carry different diseases and parasites. The various species of micro-shrimp may look fairly similar to us humans, but any switch has the potential to deeply upset the natural balance of an ecosystem. Invasive species often bring with them diseases and harmful parasites and it seems these shrimp are no different. Our research at the University of Portsmouth found that demon shrimp have carried multiple parasites with them across Europe.  We don’t yet know what these parasites will do in newly invaded areas but we do know they are playing havoc with sexuality. Our research found at least 50% of the invasive male shrimp show sexual abnormalities. These sexually abnormal (termed intersex) shrimp display female characteristics caused by feminising parasites either caught in the UK or most likely carried inside the invaders. Interestingly, these parasites don’t appear to feminise as well in British waters.  Such gender issues however haven’t hampered the demon’s ability to wipe out its British cousin. Therefore we also have to consider the parasites who live “within” the native species which can become collateral damage in the shrimp wars.  These parasites are important too as they often pass through snails or crustaceans and finally into fish or birds. Gammarus shrimp carry worm-like parasites called Acanthocephala (spiny-headed worms), for instance. These parasites can “take over” the shrimp, flooding it with serotonin and altering its behaviour so that the shrimp swims nearer the surface and is more likely to eaten by birds such as ducks. Other species of Acanthocephala have fish such as trout as their final hosts. The extinction of an intermediate host, such as the native shrimp, therefore breaks the link in the parasite life cycle, with impact felt elsewhere in the food web. Who cares, one might ask, parasites are bad right? Not necessarily. Studies have shown that ecosystems with fewer parasites are less diverse than those with many, therefore the replacement of one shrimp species for another might not necessary replace the parasites which strengthen the food web. It’s difficult to know how to stop the spread of these invasive shrimp. Public awareness campaigns are reminding people to check, clean and dry themselves and their equipment to hinder the movement of these critters but only time will tell whether we are witnessing the extinction of the native Gammarus shrimp in British waterways. Britons may soon have to get used to killers lurking in their lakes, and demons in their rivers."
"BP, Shell, Chevron and Exxon have made almost $2tn in profits in the past three decades as their exploitation of oil, gas and coal reserves has driven the planet to the brink of climate breakdown, according to analysis for the Guardian. The scale of their profits is revealed as experts say the fossil fuel boom is coming to an end, with big oil entering a “death knell” phase, according to one prominent Wall St commentator. Analysis for the Guardian by Taxpayers for Common Sense in the US reveals that since 1990 – at which point the impact of fossil fuel extraction on the climate had been well known to industry leaders and politicians for years, experts say – the big four companies have accumulated $1.991tn in profits. Critics say the findings highlight how a few corporations have generated extraordinary wealth by pursuing policies that were known to be driving the climate crisis. The climate scientist Michael Mann said he was witnessing first-hand in Australia the environmental impact of fossil fuel extraction. “Here in Sydney, where we’ve seen record drought, heat, bushfires and floods all in the short two months I’ve been here, this latest report provides a sobering reminder that we’re all paying the price – in the form of a planetary climate crisis – so that a few mega-corporations can continue to make record profits,” he said. The analysis shows that Exxon was the most profitable of the big four over the past three decades, making a total of $775bn. Shell was second with $524bn, followed by Chevron on $360bn and BP on $332bn. Autumn Hanna, of Taxpayers for Common Sense, said: “For decades, oil and gas companies have been pocketing trillion-dollar profits and padding their bottom line with tens of billions of dollars in taxpayer subsidies. All while passing the buck on climate change.” Mel Evans, a senior climate campaigner at Greenpeace UK, said the big oil companies knew the danger that their products posed to the climate well before it became common knowledge but pursued profits above the wider interests of the planet. “Why did they continue to promote those products and dispute science they knew to be correct? Why are they still spending hundreds of billions of dollars on making the problem worse, drilling for new oil and gas we can’t possibly afford to burn?” Evans said. “These figures provide the answer. Money is like rocket fuel: burn through enough of it and you can escape the pull of the Earth. But there’s nowhere else to go.” Experts say the decades-long boom is coming to an end as clean energy replaces fossil fuels. Last week Jim Cramer, the influential host of the US investment show Mad Money, said he was “done with fossil fuels” because they had entered a “death knell phase”. Oil companies have been the worst-performing investments in US stock markets over the past decade, after leading major stock market indices in previous decades. The market value of oil and gas companies now makes up only 4% of the S&P500 index, compared with about 28% in the 1980s. “If you go back to 1990 you can really see that oil and gas companies once outperformed the market,” said Kathy Hipple, a financial analyst at the Institute for Energy Economics and Financial Analysis. “But the market looks to the future and there has been a gradual awareness that fossil fuels are not going to be as big a part in the world economy. This doesn’t mean that [oil companies] will go away tomorrow, but the market won’t reward them for the profits of the past.” Last year a Guardian investigation revealed that 20 fossil fuel giants including BP, Shell, Chevron and Exxon were directly linked to more than a third of all greenhouse gas emissions in the modern era. The big four investor-owned corporations were found to be behind more than 10% of all carbon emissions since 1965. The polluters project highlighted how many of the leading fossil fuel corporations had spent billions of pounds on lobbying governments and portraying themselves as environmentally responsible. A study published last year found that the largest five market-listed oil and gas companies – the big four plus Total – spent nearly $200m each year lobbying to delay, control or block policies to tackle climate change. The profit figures were calculated in today’s money by using the annual net income attributable to shareholders for each company while taking inflation rates into account. In absolute terms, without inflation, the combined profits since 1990 were a still considerable $1.6tn, according to Taxpayers for Common Sense. The profits of the world’s most profitable listed oil companies were dwarfed last year by the financial reports of the Saudi state-owned oil firm Aramco. It listed on the Saudi stock exchange after racking up profits of $111.1bn (£84.7bn) in 2018, which was more than double the profits of Apple and five times those reported by Shell. Experts say the environmental impact of fossil fuels was known by industry leaders and politicians, particularly in the US, as far back as the mid-60s. Certainly by 1990 the facts were well known: two years earlier the Nasa scientist James Hansen had raised the alarm about the impact of fossil fuels during a landmark hearing at the US Congress. In 1992, world leaders came together at a summit in Rio to recognise the role carbon emissions were playing and to pledge coordinated action. BP did not respond to a request for comment on the findings. A spokesperson for Shell said: “Just as reliable, affordable energy has benefited us all, all of society has a role to play in tackling climate change. We’re working hard to develop lower-carbon energy options and meet demand for more and cleaner energy.” ExxonMobil said it was helping address “the dual challenge of the world’s growing demand for energy and reducing emissions”. Chevon said it was taking action to address climate change by “lowering the company’s carbon intensity, increasing the use of renewable energy and investing in breakthrough technologies.”"
"
So far, SC24 solar magnetic activity has been in a relative funk. See my post on this very issue from last month.
Leif Svalgaard points out this new paper in AGU from Keating, and kindly placed a copy on his own website for us to examine: Link to Keating-Bz.pdf
The crux of the paper is a forecast, which extends significantly into SC24, even though there is just a small number of observed data points:
Fig. 1. Actual boxcar averages for measured Bz(m) magnitude and the forecast results of applying the McNish- Lincoln technique. Actual data are represented by solid squares, while the calculated results are shown as a curve. The correlation between the two is due to the fact that the McNish-Lincoln method uses actual data when available. The calculated forecast is performed only for the time period after the end of the actual data. This plot shows that Bz(m) reached its minimum average magnitude in mid-2007 and has begun to increase in magnitude. The forecast is that it will continue to increase slowly through the first part of 2008, but will then begin to rapidly increase in magnitude beginning in the latter part of this year, reaching its first peak in late 2009.
There seem to be two schools of thought on the activity level of SC24, those who think it will be very low, and those that think it will be higher than normal.
Dr. Svalgaard goes on record here on this blog in saying: 
I’ve been predicting that SC24 would be the smallest cycle in a century, so it is no surprise that it starts out weak and anemic.
While I’m certainly no solar expert, based on what I’ve seen thus far, I’m inclined to agree. I think that Keating’s prediction will not be realized.
This graph of Ap magnetic index will be updated in a few days, with the uptick this month in SC24 spots, perhaps we’ll also see a corresponding uptick in the Ap Index.
From the data provided by NOAA’s Space Weather Prediction Center (SWPC) you can see just how little magnetic field activity there has been. I’ve graphed it below with the latest available data from October 6th, 2008:

click for a larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b561ad0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterDer Spiegel reports on a paper in Nature written by Hans von Storch and Matthias Zahn claiming that elevated greenhouse gas concentration will lead to fewer North Atlantic storms by the year 2100. Der Spiegel writes:
Instead of 50 to 60, there will only be about half as many Arctic hurricanes, the scientists say.
After every big winter storm, e.g. like Kyrill, we get here in northern Germany, we always hear the media crow about how it is due to global warming. Now the opposite is claimed. We’re a long way from settled science, aren’t we?
The mechanism leading to the Nature paper’s claim is described in the abstract as follows:
This change can be related to changes in the North Atlantic sea surface temperature and mid-troposphere temperature; the latter is found to rise faster than the former so that the resulting stability is increased, hindering the formation or intensification of polar lows.
I can certainly buy that. But then the authors apply bold science.
In the Nature abstract it is written:
Now, in projections for the end of the twenty-first century, we found a significantly lower number of polar lows and a northward shift of their mean genesis region in response to elevated atmospheric greenhouse gas concentration.
The elevated greenhouse gas/lower number of polar effect is quite a hypothesis. The authors are assuming that more CO2 will lead to higher atmospheric temps and thus fewer storms. That’s awfully bold science.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But wait, it gets even bolder, Der Spiegel writes about the scientists:
Using computer models, that also used the climate prognoses of the United Nations, the scientists have played out the development of the northern seas up to the year 2100.
Making a projection for the year 2100 with that methodology? Now that’s really bold. I’m doing all I can to rein in the sarcasm here. The authors then add:
Our results provide a rare example of a climate change effect in which a type of extreme weather is likely to decrease, rather than increase.
I haven’t read the full Nature paper, as it is behind a pay-wall. I just wonder where the authors come up with: “a rare example”. This is probably a case of: whose bread one eats, whose words one speaks, which one has to submit to when dealing with Nature and government funding.
There are actually many examples that warm climates are more beneficial than not. After all, who the hell wants to go back to a little ice age, let alone a big one?
Why not just leave the crap out? I’d have no problem with a paper presenting a couple of if-then hypotheses, like:
1) If temperatures rise, our dynamic models show that there will be fewer storms.
2) If the temperature drops, then there will be more storms.
Leave the global warming faith and religion out of it.
Share this...FacebookTwitter "
"Christiana Figueres is a founder of the Global Optimism group and was head of the UN climate change convention when the Paris agreement was achieved in 2015. Your new book is called The Future We Choose. But isn’t it too late to stop the climate crisis?We are definitely running late. We have delayed appallingly for decades. But science tells us we are still in the nick of time. You say this decade is the most consequential in human history…This is the decade in which, contrary to everything humanity has experienced before, we have everything in our power. We have the capital, the technology, the policies. And we have the scientific knowledge to understand that we have to half our emissions by 2030. So we are facing the most consequential fork in the road. If we continue as now, we are going to be irreparably going down a course of constant destruction, with much human pain and biodiversity loss. Or we can choose to go in the other direction, a path of reconstruction and regeneration, and at least diminish the negative impacts of climate change to something that is manageable. But we can only choose it this decade. Our parents did not have this choice, because they didn’t have the capital, technologies and understanding. And for our children, it will be too late. So this is the decade and we are the generation. Only 11 pages or so of the book describe the terrible consequences of unchecked climate change, while the rest talks about the possibility of a much better world. Why?It’s important for everyone to face the negative consequences that we’re sleepwalking ourselves toward, which is why those 11 pages are there. But equally as important is to spark the imagination and the creativity that comes with understanding that we do have this incredible agency to create something completely different. We wanted to offer both universes to those who, understandably, are paralysed by despair and grief at the loss that is already under way, as well as those who are paralysed by their comfort and lack of understanding of the moment that we’re in. A lot of the book is about the need for a shift in people’s consciousness. Isn’t this rather grandiose or, on the other hand, too vague to make a difference in the real world?Whatever we hold as being possible, and whatever values and principles we live by, determine the actions that we take. Whatever we hold to be near and dear to us is what we’re willing to work toward. And so to shift from doom and gloom to a positive, optimistic, constructive attitude is very important because it is what gets us up in the morning and says “yes, we can do this, we’re going to work together on that”, rather than pulling the blanket over our head and saying “it’s all too difficult”. So that change in attitude inside ourselves is critical. We also have to understand that we can no longer live in a world based on limitless extraction and waste. Rather, we have to change our consciousness to one of regeneration. How can people’s consciousness change in that way?The first thing we have to understand is the consequence of not changing our attitudes. There are very serious existential consequences. Then hopefully we can make a serious, mature decision whether we want to choose something different. One of the 10 actions recommended in the book is to be a citizen and not a consumer. Can you explain that and why it is important?The very concept of being a consumer already points us in the direction of consuming irresponsibly. We have to be able at some point, particularly in developed countries, to get to the point where we say “enough is enough”. Before you make a purchase, or an investment, or any kind of decision that impacts on the planet and on other people, the question should be: “Do I really need this and is this actually conducive to furthering the quality of life on this planet?” Another of the actions you chose is building gender equality. Why?Educating young women and empowering women to come to decision-making tables is the strongest thing that we can do for the climate. When there are more women in boardrooms and in high-level positions in institutions, you get decisions that are wiser and longer term. Of course there are many men that also do this. But there is a tendency for women to be more collaborative, which is the basis of what we need to do, and they tend to think much more long term. [Women] have the first duty of care of our newborn children and hence, biologically, we’re geared towards that stewardship. But it is just plain stupid, frankly, not to use 50% of human potential. We are in such an emergency that we need to deploy 100% of our potential. Tackling the climate emergency requires global cooperation and action by governments and businesses. But what can individuals do?We tend to forget that it is humans who have caused climate change, and we tend to export responsibility to large corporations or governments. The fact is, we all contributed to it. If we all reduce our emissions, collectively we give a signal to the market. Obviously, corporations have their own responsibilities but it’s helpful to have a strong demand from the public. Once you get governments, corporations and the public moving in the same direction towards low carbon, it can grow exponentially [such as with renewable energy and electric cars]. People reducing their emissions – by flying less, eating less meat and using clean energy, for example – is important. But is demanding that politicians tackle the climate emergency and voting accordingly perhaps the most important thing individuals can do?I would say both. If we had 50 years to fiddle with this, then you could choose one or the other. But we are in such an emergency that we can no longer do things sequentially or exclusively. We have to be an “and/also” world. • The Future We Choose: Surviving the Climate Crisis by Christiana Figueres and Tom Rivett-Carnac is published by Manilla Press (£12.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15 • Christiana Figueres and Tom Rivett-Carnac will be in conversation at a Guardian Live event at the Royal Geographical Society, London SW7, on Tuesday 3 March, 7pm"
"Climate change will cause all sorts of problems for humans in the future. It could cause mass migration and conflict as people flee flooded homes or arid farmland, and fight over ever more scarce resources. It’ll mean economic slowdown as industries are hit and societies cough up the money required to adapt to the new world. Climate change will even affect your health. Indeed, some scholars have warned that climate change is the greatest risk to global health. While this is likely to be true beyond the next 30 years if CO2 emissions continue unabated, this kind of argument risks reinforcing the reductionist idea that climate is some sort of universal predictor of health.   The time lag between greenhouse gas emissions and their impact on the environment means that, until at least 2045, poverty will be a much more important health factor than climate change. My commentary, recently published in the Geographical Journal, argues that this is for two reasons.  First, climate change should not be considered a direct cause of poor health in the way specific diseases can be. Even if it were a direct cause, there would be more important threats such as heart and lung disease or cancers. These non-communicable diseases already comprise the leading causes of death globally, and their burden is likely to increase as people age and adopt Western lifestyles. Second, the threat of climate change to health is one of modification: a changed climate will exacerbate and moderate existing health problems by making people more vulnerable. If vulnerability to climate change is addressed through adaptation, the threat will be much reduced, though not averted. Climate change is only one factor among many contributing to population vulnerability, of which poverty is the most important. This can be illustrated by taking a look at two major risks to global public health.  Vector-borne diseases are transmitted between humans or between animals and humans by another living organism, or vector. Vectors include mosquitoes, sandflies, ticks and aquatic snails, and the diseases they transmit include malaria and dengue fever. Even small changes in climate can cause shifts in where these diseases are most prevalent. People living in areas where the vectors may be able to breed in future due to climate change may be under-prepared for this shift if public health systems are ill-equipped and natural immunity is lower in these areas. For example, some research suggests that malaria is likely to penetrate higher altitudes in the East African highlands due to increased temperatures. Transmission of the disease from vectors to humans is reliant on both natural and human influences. For many of these man-made factors, including nutrition and access to education or health care, poverty is the key determinant. The importance of poverty is shown by the eradication of many vector-borne diseases in richer countries with similar climates to other nations where the same diseases are still prevalent. Malaria existed in Europe less than 90 years ago and has now all but disappeared. Dengue fever was eradicated in the US due to investment in public education, mosquito control programmes, piped water and screened windows. Neither Europe nor the US needed changes in climate to rid themselves of these diseases. In the case of infectious diseases the message is clear enough: improving health will be best achieved through reducing poverty. Climate change is likely to affect crop yields due to drought, flooding and rising seas creeping onto farmland. Seafood yields will be hit by warmer water temperatures and ocean acidification. Overall, food is likely to get more expensive. Already more than 1 billion people are estimated to be suffering from a lack of sufficient dietary energy, with one of the most important reasons being that people simply cannot afford to buy food. Purchasing power is very important in determining malnutrition, and food prices have doubled in the past ten years and experienced very high volatility. There was a 60% rise in basic food prices globally in 2008/9 and another 40% increase in 2011/12. These peaks were the combined effect of financial speculation in food and increased energy prices.  In light of this, it is difficult to argue that climate change is the greatest threat to global health based on food security, as poverty is a more important determinant of access to food. Poverty is one of the things that makes people most vulnerable to climate change and it is also key contributor to many health problems. It is imperative that climate change mitigation continues, not least because in the next 30 years it is likely to become a critical health factor, but for now the focus should be on reducing poverty. Not only does this align more closely with conventional public health practices, it will also confer immediate health benefits as well as ensuring people are more resilient to climate change impacts in the future. It is vital we emphasise poverty for the next 30 years. At the current rate of development, global GDP would have to increase 15 times in order for the world’s population to all earn at least $1.25 per day; a process taking more than 100 years. Poverty kills, and just focusing on climate change could mean this message is ignored."
"

Last December the United States agreed at a United Nations meeting in Kyoto, Japan, to reduce its emissions of greenhouse gases by 7 percent below 1990 levels. That reduction, to be achieved mainly by cutting the combustion of fossil fuels, will lower emission levels 41 percent below where they will likely be in the year 2010 if the trend observed since 1990 continues. 



The Kyoto agreement–if fully complied with–would likely reduce the gross domestic product of the United States by 2.3 percent per year. However, according to a climate model of the National Center for Atmospheric Research recently featured in _Science_ , the Kyoto emission‐​control commitments would reduce mean planetary warming by a mere 0.19 degree Celsius over the next 50 years. If the costs of preventing additional warming were to remain constant, the Kyoto Protocol would cost a remarkable 12 percent of GDP per degree of warming prevented annually over a 50‐​year period. 



The Kyoto Protocol will have no discernible effect on global climate–in fact, it is doubtful that the current network of surface thermometers could distinguish a change on the order of .19 degree from normal year‐​to‐​year variations. The Kyoto Protocol will result in no demonstrable climate change but easily demonstrable economic damage. 
"
"

America’s security commitment to Taiwan faces a significant test. China’s growing power presents a challenge to U.S. military superiority, while Taiwan’s investment in its own defense has languished. Adding to the challenge of keeping peace in the Taiwan Strait is the shifting political situation in Taiwan, exemplified by the January 2016 elections in which voters rejected the cross‐​strait rapprochement policies of the Kuomintang (KMT) and turned over control of the presidency and legislature to the Democratic Progressive Party (DPP). The China‐​Taiwan relationship has remained relatively calm, but changes in the U.S.-China balance of power could make the Taiwan Strait a dangerous place once more if the implicit U.S. defense commitment to Taiwan loses credibility.



This paper outlines three broad policy options for the United States: shoring up the defense commitment by restoring military superiority over China; sustaining a minimum level of military advantage over China; or stepping down from the commitment to use military force to maintain Taiwan’s de facto independence. It concludes that the United States should step down from the defense commitment eventually, ideally through an incremental and reciprocal process with China that would draw concessions from Beijing. In the long term, the U.S. security commitment to Taiwan is neither beneficial nor advantageous for the United States. Taiwan will have to take responsibility for its own defense.



Stepping down from the implicit commitment to come to Taiwan’s rescue with military force carries risks, but other options leave the United States worse off in the long term. The likely damage to U.S.-Chinese relations caused by pushing for military superiority in the region outweighs the benefits. Sustaining a minimum level of military advantage is possible, but absent a long‐​term economic slowdown and/​or political changes in China—both of which are beyond U.S. control—maintaining such an advantage in perpetuity will be difficult. Stepping down from the commitment through a long‐​term process would give Taiwan the time it needs to make necessary changes in its defense technology and military strategy. Peace in the Taiwan Strait is an important American interest, but it must be weighed against the difficulty of maintaining credibility and the growing costs of deterrence failure.



The U.S. defense relationship with Taiwan is a risky and costly commitment that has become increasingly difficult to sustain. Barry Posen of the Massachusetts Institute of Technology put it best when he wrote, “The U.S. commitment to Taiwan is simultaneously the most perilous and least strategically necessary commitment that the United States has today.”1 The United States can and should strive for a peaceful resolution of the Taiwan dispute, but through means other than an implicit commitment to use military force to defend the island.



Washington’s approach to keeping the peace in the Taiwan Strait during the latter years of Taiwan’s Lee Teng‐​hui (1988–2000) and most of the Chen Shui‐​bian (2000—2008) administrations was known as “dual deterrence.” Under dual deterrence the United States issued a combination of warnings and reassurances to both China and Taiwan to prevent either from unilaterally changing the status quo.2 America’s overwhelming military advantage over the People’s Liberation Army (PLA) deterred China from using military force, while Taiwan moderated its behavior lest U.S. forces not come to its rescue.3 However, the dual deterrence concept is ill‐​suited to the current military environment in the Taiwan Strait.



Dual deterrence is no longer viable because the modernization of the PLA has improved Beijing’s ability to inflict high costs on U.S. military forces that would come to Taiwan’s aid in the event of a Chinese invasion attempt.4 The deployment of two U.S. Navy aircraft carriers to the waters around Taiwan during the 1995–1996 Taiwan Strait Crisis was a major embarrassment for the PLA, and it has played an important role in driving China’s military modernization.5 Improvements in China’s anti‐​access/​area denial (A2/AD) capabilities have significantly complicated the ability of the United States to defend Taiwan by making it difficult for the U.S. Navy and Air Force to operate in and around the Taiwan Strait.6 According to a recent RAND Corporation study, “a Taiwan [conflict] scenario will be extremely competitive by 2017, with China able to challenge U.S. capabilities in a wide range of areas.”7 This shifting balance of power strains the credibility of the U.S. defense commitment to Taiwan by increasing the costs the United States would have to pay in an armed conflict.



Two additional developments will challenge the cross‐​strait peace. First, the period of rapprochement that has characterized cross‐​strait relations since 2008 has ended. The former Taiwanese president, Ma Ying‐​jeou (2008–2016), championed cross‐​strait cooperation and economic linkages that brought a welcome sense of calm after the tumultuous administrations of Lee and Chen.8 However, the January 2016 landslide victory of the DPP in both presidential and legislative elections revealed popular dissatisfaction with Ma’s policies and a weakening economy.9 President Tsai Ing‐​wen pledged to maintain peace. But her unwillingness to declare support for the “1992 Consensus” (simply stated as “one China, different interpretations”) caused Beijing to suspend communication between the Taiwan Affairs Office and Taipei’s equivalent, the Mainland Affairs Council.10 It is too early to tell how Tsai’s administration and a DPP‐​controlled legislature will affect cross‐​strait relations, but the relatively high level of cooperation the Ma administration promoted is likely over.11



Second, China’s slowing economy adds uncertainty to cross‐​strait relations. China’s GDP growth rate was 6.9 percent during the first nine months of 2015, well below the double‐​digit GDP growth rates of the last couple of decades.12 Sliding growth and the resulting social instability could encourage China’s leaders to behave more aggressively toward Taiwan to bolster domestic legitimacy and ensure regime survival.13 However, a slowing economy could also restrict military spending and encourage Chinese policymakers to avoid big conflicts as they focus on shoring up the economy. At the very least, China’s economic situation is a source of uncertainty that was not present when the United States relied on dual deterrence.



What approach should the United States take in this shifting environment? Generally speaking, there are three options for the United States: it could do more to shore up the defense relationship with Taiwan and restore its military superiority over China; sustain a minimum level of military advantage over China; or step down from the implicit commitment to use military force in defense of Taiwan. This paper explores each of these and concludes that stepping down from the commitment is the best of the three options. The success of dual deterrence should be praised, but American policymakers must begin adjusting to a new state of affairs in the Taiwan Strait.



The U.S. security commitment to Taiwan consists of two pillars established in the Taiwan Relations Act (TRA) of 1979: arms sales and an implicit promise to defend Taiwan with military force should it be attacked. Both are set forth in Section 3 of the TRA, which states, in part, that the United States is permitted to sell Taiwan “defense articles and defense services in such quantity as may be necessary to enable Taiwan to maintain a sufficient self‐​defense capability.”14 Comparatively, the implicit commitment to use force to defend Taiwan is less clear. Section 3, part 3, authorizes the president and Congress to “determine, in accordance with constitutional processes, appropriate action by the United States” in response to “any threat to the security or the social or economic system of the people on Taiwan and any danger to the interests of the United States arising therefrom.”15 Military force is not explicitly mentioned, but it falls within the category of appropriate action that the United States could take.



The imprecise wording of the TRA has served the United States well by creating “strategic ambiguity,” the underpinning of dual deterrence.16 Strategic ambiguity, the open question of whether or not the U.S. military would intervene in a cross‐​strait conflict, had two important effects. First, it gave the United States greater freedom of action in trilateral relations. By not binding itself to one particular position, the United States could better adapt to unpredictable events. Second, strategic ambiguity restricted China and Taiwan’s freedom of action. Upsetting the status quo carried high costs for both sides. The United States could warn Taiwan that no cavalry would come to the rescue if Taiwan provoked China by making moves toward de jure independence.17 Likewise, the high costs that would be inflicted on the PLA by a U.S. intervention prevented Beijing from initiating a conflict.



China’s growing military power has diminished the value of strategic ambiguity by improving Beijing’s ability to inflict high costs on an intervening American force. The mere possibility of American intervention may no longer be enough to deter China if the PLA is better prepared to mitigate the effects.



Further complicating the U.S.-Taiwan defense relationship is the slow but steady erosion of U.S. credibility over the last two decades. This analysis uses the “Current Calculus” theory set forth by Dartmouth professor Daryl G. Press as the basis for assessing U.S. credibility. Press states, “Decisionmakers assess the credibility of their adversaries’ threats by evaluating the balance of power and interests … Future commitments will be credible if—and only if—they are backed up by sufficient strength and connected to weighty interests.”18 From Beijing’s perspective, the U.S. commitment to defend Taiwan is credible if American military power can pose a threat to Chinese forces and the United States has a strong interest in defending Taiwan.



On the subject of interests, Taiwan carries much more importance for China than it does for the United States. Charles Glaser of George Washington University writes “China considers Taiwan a core interest–an essential part of its homeland that it is determined to bring under full sovereign control.”19 Beijing does not appear eager to reunite Taiwan with the mainland by force in the near future, but China’s president Xi Jinping has warned that “political disagreements that exist between the two sides … cannot be passed on from generation to generation.”20 Maintaining Taiwan’s de facto independence may be important for the U.S. position in East Asia, but it does not carry the same significance that China places on reunification.21



Since China enjoys an advantage in the balance of interests, the credibility of the U.S. commitment rests on American military power. According to Press’s model, if the United States can carry out its threat to intervene with relatively low costs, then the threat is credible.22 When the TRA was passed in 1979, the United States enjoyed a clear advantage over a militarily weak China. That is no longer the case. Several recently published assessments of a U.S.-China conflict over Taiwan have sobering conclusions: America’s lead is shrinking, victory is less certain, and the damage inflicted on the U.S. military would be substantial. In China’s Military Power, Roger Cliff of the Atlantic Council writes, “Although China’s leadership could not be confident that an invasion of Taiwan in 2020 would succeed, it is nonetheless possible that it could succeed.… Even a failed attempt, moreover, would likely be extremely costly to the United States and Taiwan.”23 The RAND Corporation reached a similar conclusion: “At a minimum, the U.S. military would have to mount a substantial effort—certainly much more so than in 1996—if it hoped to prevail, and losses to U.S. forces would likely be heavy.”24 It is impossible to determine exactly how many American ships, aircraft, and lives would be lost to defend Taiwan from a PLA attack. But given the improved quality of PLA weapons systems and training exercises, it is safe to assume that the U.S. military would have to cope with losses that it has not experienced in decades.



Of course, it is important to note that high costs do not flow one way. In a war, the United States and Taiwan would make an invasion very costly for China, which reduces the credibility of Beijing’s threats to use force. However, U.S. military superiority in a Taiwan Strait conflict was nearly absolute until very recently. This superiority made victory relatively cheap, which enhanced the credibility of the American commitment.25 Improvements to already formidable Chinese weapons systems, combined with recent reforms that enhance command and control for fighting modern war, continue to ratchet up the costs the United States would have to absorb.26



If the PLA continues to improve at the rate it has done over the last 20 years, the United States could be in the unpleasant position of fighting a very costly conflict over a piece of territory that China has a much stronger interest in controlling than the United States has in keeping independent. Close economic ties between the United States and China (bilateral trade in goods was valued at $598 billion in 2015 in nominal dollars) would likely suffer as well.27 The high costs the United States would face in a conflict over Taiwan undermine U.S. credibility. China’s stronger interests and ability to inflict high costs on the United States could encourage Beijing to take risks that until recently would have been considered unacceptable.



Broadly speaking, the United States has three options for dealing with the diminishing credibility of its implicit commitment to defend Taiwan. In this section I explain what kinds of policies would most likely accompany each option and present favorable arguments for each.



The most straightforward way to bolster American credibility would be to increase the U.S. military presence close to Taiwan and clearly demonstrate the political will to honor the defense commitment. The combination of increased military presence and unequivocal political support would be a clear break from dual deterrence. Instead of directing warnings and reassurances toward both Taiwan and China, the United States would only warn China and only reassure Taiwan.28 The United States would welcome a stronger Taiwan, but U.S. support would not be preconditioned on Taiwan’s willingness to develop its defenses.



The ultimate goal of this policy option would be the establishment of a decisive and durable U.S. military advantage over the PLA. The clearest indicator of the U.S. commitment is military resources. Increasing the survivability of American air power in the area around Taiwan would send a clear signal of support. The American forces currently deployed in Japan would be the first to respond in a Taiwan conflict. Increasing the number of hardened aircraft shelters at U.S. bases in Japan, especially at Kadena Air Base on Okinawa, would protect aircraft from ballistic missile attacks.29 Additionally, the United States would revive the annual arms‐​sale talks with Taiwan that occurred from 1983 until 2001. Advocates for returning to annual talks argue that moving away from scheduled talks resulted in arms sales becoming less frequent.30 Future arms sales would include more advanced equipment that Washington is currently unwilling to sell to Taiwan, such as the F-35 Joint Strike Fighter aircraft and diesel attack submarines.31



Politically, American policymakers would clarify that U.S. military intervention in a Taiwan conflict is guaranteed. They would interpret the TRA as a serious commitment to Taiwan’s security, and, according to Walter Lohman of the Heritage Foundation, “[make] abundantly clear to Beijing the consequences that will ensue from the use of force.”32 The TRA would not be modified in any way that reduces the scope of America’s commitment. Supporters in Congress would regularly issue resolutions that reaffirm support for the TRA, especially the parts related to the defense of Taiwan.33 Strict interpretation of the TRA would be a clear demonstration of American willpower to take a hard line against China.



Public statements by American officials about U.S. intervention would not carry any preconditions or caveats. Such statements would be similar to the one made by President George W. Bush in April 2001 that the United States would do “whatever it takes” to defend Taiwan.34 Bush eventually walked back this statement, but successful implementation of the restore‐​superiority option would require similarly categorical shows of support. Removing preconditions from the commitment would bolster credibility by removing an off ramp the United States could take to avoid intervention. Additionally, Taiwan would not be expected to spend a certain percentage of its GDP on defense to secure U.S. arms sales or intervention.



Finally, the U.S. government would actively support de jure Taiwanese independence. As Weekly Standard editor William Kristol warns, “Opposing independence … might give Beijing reason to believe that the U.S. might not resist China’s use of force against Taiwan or coercive measures designed to bring about a capitulation of sovereignty.”35 However, supporting Taiwanese independence would be risky. In 2005, China passed the Anti‐​Secession Law (ASL) in response to the growing political power of the pro‐​independence movement in Taiwan.36 Article 8 of the ASL states that “non‐​peaceful means and other necessary measures” will be employed if “secessionist forces … cause the fact of Taiwan’s secession from China.”37 The increased American military presence resulting from the restore‐​superiority option would have to be strong enough to prevent China from invoking the ASL.



Advocates of the U.S. military commitment to Taiwan argue that the island’s success as a liberal democracy is linked to the regional security interests of the United States. For example, during his failed campaign for president, Sen. Marco Rubio (R-FL) said that “Taiwan’s continued existence as a vibrant, prosperous democracy in the heart of Asia is crucial to American security interests there and to the continued expansion of liberty and free enterprise in the region.”38 In the U.S. Congress the ideologically driven, “pro‐​democracy” camp of Taiwan supporters is large and influential.39 Proponents of a strong U.S. commitment to Taiwan also argue that Taiwan’s political system is evidence that Chinese culture is compatible with democracy. According to John Lee of the Hudson Institute, “Taiwan terrifies China because the small island represents a magnificent vision of what the mainland could be and what the [Chinese] Communist Party is not. This should be a reason to reaffirm that defending democracy in Taiwan is important to America and the region.”40 Supporters of a strong U.S. defense commitment to Taiwan through restoring America’s military superiority want to send a clear message to Beijing that the security commitment has not been shaken by China’s growing military power.



The second option, sustaining a minimum advantage, would maintain the current U.S. military commitment with some slight modifications. This option is much less resource‐​intensive than the restore‐​superiority option. The United States would maintain its implicit military commitment, but with preconditions that encourage Taiwan to invest more in its own defense. Importantly, the United States would reserve the right not to intervene if Taiwan provoked an armed conflict with China. The overarching themes of this option are balance and moderation. It has taken the United States years of effort to create what appears to be a relatively stable status quo, so, its supporters ask, why risk destabilizing it by significantly altering the U.S.-Taiwan relationship without very good reason?41



Under this option, the United States would improve the military assets for defending Taiwan, but at a much smaller scale than with the restore‐​superiority option. The PLA’s steadily improving capabilities diminish the credibility of the U.S. commitment to Taiwan by raising the costs of conflict. Maintaining a qualitative advantage over the PLA as it continues to develop will enhance the credibility of the U.S. commitment to Taiwan by keeping the costs of war high for the PLA. However, such improvements would be tempered to mitigate the chance of overreaction by Beijing and possible damage to U.S.-China relations.



American arms sales to Taiwan would continue under this policy option. Arms sales create tension in the U.S.-China relationship, but three benefits of arms sales mitigate the costs they create.42 First, arms sales complicate PLA planning and raise the costs of conflict for China. Second, damage done to U.S.-China relations as a result of the arms sales is relatively small. A joint report from the Project 2049 Institute and the U.S.-Taiwan Business Council on China’s reactions to arms sales concludes, “Past behavior indicates that the PRC is unlikely to challenge any fundamental U.S. interests in response to future releases of significant military articles and services to Taiwan.”43 Finally, arms sales demonstrate the commitment to Taiwan’s defense, especially in times of political transition.



Arms sales to Taiwan would also be adjusted to counteract the PLA’s quantitative advantage and operational strengths. Expensive items such as AV-8B Harriers, F-16 fighters, and Perry‐​class frigates would no longer be sold because they are highly vulnerable to Chinese weapons systems.44Instead, arms sales would prioritize cheaper, more numerous precision‐​guided weapons and advanced surveillance assets that would prevent Chinese forces from achieving a quick victory and buy time for the United States to come to Taiwan’s rescue.45 Such weapons systems are, generally speaking, much cheaper and easier to maintain than aircraft and ships. A report from the Center for Strategic and Budgetary Assessments argues that by “forego[ing] further acquisitions of costly, high‐​end air and naval surface combat platforms” Taiwanese policymakers can focus their economic resources on more “cost‐​effective platforms” better suited to Taiwan’s defense.46



The United States would expect Taiwan to make serious defense investments by increasing military spending and developing indigenous weapons systems. Taiwan’s military spending has increased in nominal terms after a precipitous drop in the late 1990s and early 2000s, but since 1999 defense spending has not risen above 3 percent of GDP.47 Taipei’s unwillingness to spend more on defense has upset some officials in Washington. In a November 2015 letter to President Obama calling for a new arms sale to Taiwan, Sen. John McCain (R-AZ) and Sen. Benjamin L. Cardin (D-MD) wrote, “We are increasingly concerned that, absent a change in defense spending, Taiwan’s military will continue to be under‐​resourced and unable to make the investments necessary to maintain a credible deterrent across the strait.”48 Thankfully, Tsai Ing‐​wen and the DPP have made increased defense spending a major policy goal.



The development of Taiwan’s defense industry would provide an additional source of high‐​quality military equipment for the island’s defense. Taiwan has experience designing and manufacturing sea and air defense weapons. James Holmes of the U.S. Naval War College notes, “[In 2010] Taiwanese defense manufacturers secretly designed and started building a dozen stealthy, 500‐​ton fast patrol craft [Tuo Chiang–class] armed with indigenously built, supersonic anti‐​ship missiles.”49 Indigenously produced air defense systems include the Tien Kung (TK) family of missiles, the Indigenous Defense Fighter, and anti‐​aircraft guns.50 Importantly, “Made in Taiwan” is not a byword for poor quality. According to Ian Easton of the Project 2049 Institute, the TK surface‐​to‐​air (SAM) missiles are “comparable to [U.S.-made] Patriot systems in terms of capability,” and the Hsiung Feng III anti‐​ship missile “is more capable than any comparable system fielded by the U.S. Navy in terms of range and speed.”51



Sustaining a minimum advantage would be the easiest of the three policy options for the United States to implement. Inertia is a powerful force. The United States has invested a considerable amount of resources and effort to reach a stable status quo in the Taiwan Strait, creating an “if it isn’t broken, don’t fix it” mentality. Advocates of maintaining the status quo, such as the Center for Strategic and International Studies, argue that it is “critically important to U.S. interests” to deter Chinese coercion of Taiwan, lest instability spread in East Asia.52 In prepared testimony before the House Foreign Affairs Committee, Deputy Assistant Secretary of State Susan Thornton said, “The United States has an abiding interest in cross‐​Strait peace and stability.”53 Congress, historically a strong bastion of support for Taiwan, shows no indication of changing America’s Taiwan policy anytime soon.



Buttressing support for this policy option is the belief that America’s commitment to Taiwan is a bellwether for the U.S. position in East Asia. According to John J. Mearsheimer of the University of Chicago, “America’s commitment to Taiwan is inextricably bound up with U.S. credibility in the region … If the United States were to sever military ties with Taiwan or fail to defend it in a crisis with China, that would surely send a strong signal to America’s other allies in the region that they cannot rely on the United States for protection.”54 Advocates of maintaining the U.S. commitment argue that East Asia would become more dangerous if other allies lose faith in the United States and start building up military capabilities of their own.55 Supporters of the U.S. commitment also contend that backing down on Taiwan would embolden Chinese aggression in other territorial disputes.



The final policy option would do away with America’s commitment to Taiwan’s defense on the grounds that military intervention to preserve the island’s de facto independence has become too costly and dangerous for the United States. Stepping down from the commitment to come to Taiwan’s rescue would be a major change in U.S. policy. However, other factors unrelated to the U.S. commitment would still make the use of force unattractive for Beijing. Taiwan would therefore not be defenseless or subject to imminent Chinese attack if the United States chose this policy option.



Without a U.S. commitment, Taiwan would have to improve its self‐​defense capability to deter an attack by China and fight off the PLA if deterrence failed. Taiwan does face an unfavorable balance of power vis‐​à‐​vis China, but this does not doom Taiwan to military defeat. In fact, research by Ivan Arreguín‐​Toft of Boston University indicates that large, powerful actors (such as China) have lost wars against weaker actors “with increasing frequency over time.”56 However, in order to have the greatest chance of success, the weaker side must have the right military strategy. A head‐​on, symmetric fight with the PLA would likely end in disaster for Taiwan, but Taiwan could successfully deny the PLA from achieving its strategic objectives through the same kind of asymmetric strategy that China uses to make it difficult for the United States to defend Taiwan.57 A military strategy emphasizing mobility, concealment, and area denial would both raise the costs of war for China and be sustainable, given Taiwan’s limited means.



Changing Taiwan’s defense strategy would not be a quick or easy task. The most immediate roadblocks to change are the equipment and mindset of Taiwan’s military. The upper echelons of the military have resisted implementing changes that could improve their ability to fight a war against the modern PLA. For example, James Holmes points out that Taiwan’s navy “[sees] itself as a U.S. Navy in miniature, a force destined to win decisive sea fights and rule the waves.”58 This is a dangerous mindset given the PLA Navy’s dominance in fleet size, strength, and advanced equipment. The Taiwan Marine Corps (TMC) is also ill‐​suited to meeting the threat posed by China. Instead of being a light, agile force, the TMC is “heavy, mechanized, and not particularly mobile,” reflecting “a glaring failure by Taiwan’s defense establishment to recognize the TMC’s essential role in national defense.”59 Overcoming the forces of bureaucratic inertia will be very difficult, but doing so is necessary if Taiwan can no longer count on the United States.



Stepping down from the U.S. defense commitment would likely involve reductions in U.S. arms sales. Reductions in the size, quantity, and frequency of arms sales would likely precede any reductions to the defense commitment because arms sales are a measurable signal of American support for Taiwan. Lyle J. Goldstein of the U.S. Naval War College points out, “Arms sales have for some time taken on a purely symbolic meaning.”60 This implies that the negative effects of reducing arms sales would be relatively small, since China’s extant military advantages are not being offset by U.S. weaponry. Additionally, stopping the arms sales would not have to be instantaneous. The United States could reduce arms sales incrementally to give Taiwan time to improve its self‐​defense capabilities.



One common argument made by opponents of stepping down from the commitment is that it is the only thing preventing China from attacking Taiwan. This argument ignores several important factors that make the use of force unattractive for Beijing. First, China’s reputation and standing in East Asia would be seriously damaged. Other countries in East Asia would harshly criticize China’s use of force, and would likely take steps to defend themselves. For example, countries involved in territorial disputes with Beijing in the South China Sea have responded to Chinese aggressiveness by improving their military power and pushing back politically and diplomatically.61 China’s reputational costs for attacking Taiwan would be very high. Additionally, any military operation against Taiwan would tie up a great deal of resources. Other states could take advantage of a Taiwan‐​focused Beijing to push back against other Chinese territorial claims.



Second, the PLA has problems with both “hardware” (equipment) and “software” (experience) that would restrict its options for using military force against Taiwan.62 The modern PLA has no experience conducting large‐​scale amphibious landings, which are complicated operations that would be very costly to execute against a dug‐​in defender.63 On the hardware side, the PLA still lacks the amphibious‐​lift capabilities and replenishment ships necessary to mount a successful invasion attempt.64 China has made big strides shifting the relative balance of power in the Taiwan Strait, but it still faces significant challenges that will take time to overcome.65 Presently, the PLA is more prepared to push back against American intervention than to initiate an invasion of Taiwan.



How the United States goes about stepping down from its commitment is important. Suddenly abrogating the TRA would be practically impossible given the entrenched support for Taiwan within Congress. The most realistic, feasible approach requires incremental reductions in U.S. support for Taiwan. Examples of such reductions could include setting a cap on the value and/​or quality of military equipment that can be sold to Taiwan, changing the TRA to more narrowly define what constitutes a threat to Taiwan, or requiring Taiwan to spend a certain percentage of its GDP on defense in order to receive U.S. military support.



Incremental reduction would be easier to sell to U.S. policymakers because it buys time for Taiwan to improve its defenses, thus increasing the credibility of the island’s military deterrent. As discussed earlier, Taiwan’s defense industries have proven they can make high‐​quality military equipment that meets the island’s defense needs. Taiwan has the ability to develop a robust and effective military deterrent, but it needs time to overcome existing challenges and address unforeseen obstacles. If the United States were to reduce its commitment incrementally, Taiwan’s political and military leadership would have the time to address such challenges.



Incremental implementation of this policy option would also provide the United States with opportunities to learn about Chinese intentions, based on Beijing’s reaction.66 Stepping down from the defense commitment to Taiwan would be a major accommodation on a core Chinese security interest. American policymakers should demand some sort of reciprocal actions from Beijing that reduce the military threat the PLA poses to Taiwan. In Meeting China Halfway, Lyle J. Goldstein explains how “cooperation spirals” in the U.S.-China relationship can build “trust and confidence … over time through incremental and reciprocal steps that gradually lead to larger and more significant compromises.”67 However, if Washington takes accommodating policy positions and Beijing responds with obstinacy or increased aggression, then American policymakers would likely want to adjust their approach.



Stepping down from the U.S. defense commitment to Taiwan, regardless of how it is implemented, is a controversial policy option that would face significant opposition. However, there is a strong case to be made for the benefits of such a policy. Taiwan’s fate carries much more significance for China than the United States, and American military superiority over China is eroding. Although Taiwan faces serious challenges, it would be capable of maintaining a military deterrent without American support, especially given the other factors that rein in Chinese aggression. A self‐​defense strategy emphasizing asymmetric warfare could raise the costs of military conflict for China to unacceptably high levels. Most important, the risk of armed conflict between the United States and China would be significantly reduced.



Each of the three policy options has problems and shortcomings that would make their implementation difficult and limit their effectiveness. In this section I will discuss the most important flaws of each policy option.



Restoring U.S. military superiority would shore up the credibility of the American commitment to Taiwan at the cost of severe damage to the U.S.-China relationship. China might be deterred from attacking Taiwan, but it would have ample reason to strongly oppose the United States across other issue areas, including the South China Sea, trade issues, and reining in North Korea. Additionally, unequivocal American support would reduce incentives for Taiwan to improve its defenses.



The most important negative consequence of restoring U.S. military superiority is the severe damage that would be done to U.S.-China relations. China and the United States do not see eye‐​to‐​eye on many issues, but this does not make China an outright adversary.68 Chinese cyber espionage against American companies, the rise of alternative development institutions led by Beijing, and island‐​building in the South China Sea are of great concern to policymakers in Washington.69 However, U.S.-Chinese cooperation on other pressing issues, especially environmental concerns and punishing North Korea after its recent nuclear tests, has supported U.S. goals.70 China is certainly not a friend or ally of the United States, but treating it as an enemy that needs to be contained is unwise.71 Restoring U.S. military superiority would set back much of the progress made in U.S.-China relations.



Restoring U.S. military superiority might be a boon to America’s credibility in the short term, but superiority may be fleeting. The growing U.S. military presence in East Asia, a result of the Obama administration’s “pivot” or “rebalance” to the region, has exacerbated the Chinese perception of the United States as a threat.72 Restoring U.S. military superiority will likely support this perception and provide a strong incentive for China to invest even more resources in its military. Additionally, falling behind in the conventional balance of power could prompt China to increase the quantity and quality of its nuclear weapon arsenal.73 If Beijing quickly offsets the advantages of stronger U.S. military support for Taiwan, the United States could end up in a similar position to the one it’s in now, but with a stronger China to deter.



Increasing American support for Taiwan without any preconditions regarding Taiwan’s role in its own defense would be detrimental in the long run. Taiwan and the United States’ other East Asian allies are willing to cheap‐​ride on American security guarantees.74 Taiwan is not disinterested in self‐​defense, but if someone else is shouldering the burden there is less urgency to do more, especially if increasing military spending means reducing social spending. China could exacerbate Taiwan’s “guns vs. butter” dilemma if it restricted economic exchanges (trade, investment, and tourism) with Taiwan as a result of a stronger U.S. posture.



Increasing the American commitment to Taiwan carries significant risks and costs for a benefit that would likely be fleeting. The likely negative consequences of restoring U.S. military superiority would not be worth the benefits. American policymakers should not go down this path.



The biggest weakness of sustaining a minimum U.S. military advantage is that it does not resolve any of the underlying issues in the cross‐​strait dispute, most important of which is the fact that Taiwan matters more to China than it does to the United States. Since the United States cannot equalize the imbalance of stakes vis‐​à‐​vis China, credible deterrence will require the United States to maintain military superiority over a steadily improving PLA. The United States is capable of absorbing these costs in the short run, but the recent history of the U.S.-China military balance suggests that China will be able to narrow the gap eventually.



Maintaining stability in the Taiwan Strait will become more complicated as a result of two trends in cross‐​strait relations and one higher‐​level trend. First, a distinct identity is taking hold in Taiwan; the people living there see themselves as Taiwanese instead of Chinese. Surveys conducted in 2014 showed that “fewer than 4 percent of respondents [in Taiwan] self‐​identified as solely Chinese, with a clear majority (60 percent) self‐​identifying solely as Taiwanese.”75 A unique Taiwanese identity is dangerous to Beijing because it makes China’s ultimate goal of reunification more difficult, especially if the identity issue leads to greater political support for independence. Thankfully, the Taiwanese people have been very pragmatic and have not yet made a significant push for de jure independence.76



Second, if China’s economy continues to slow down Beijing could become more aggressive toward Taiwan. A parade of doom and gloom headlines reveal the weaknesses of China’s economic miracle. The Chinese stock market experienced downturns in August 2015 and January 2016 that affected global financial markets.77 China Labor Bulletin, a Hong Kong‐​based workers’ rights group, recorded more than 2,700 strikes and worker protests throughout China in 2015—more than double the 1,300 recorded the year before.78 In February 2016, Reuters reported that 1.8 million workers in China’s state‐​owned coal and steel companies will be laid off in the coming years.79 This is not to say that China’s economy is in imminent danger of a catastrophic collapse. However, the political instability resulting from economic troubles could create an incentive for Beijing to act aggressively to burnish the Chinese Communist Party’s image at home.80 Exacerbating this risk is the rise of nationalist forces within Chinese society that could push the government into a more aggressive cross‐​strait policy. Such forces played an important role in the government’s heavy‐​handed response to 2014’s Occupy Central protests in Hong Kong.81 Economic problems coupled with aggressive ideology could prompt China to back away from any rapprochement with Taiwan. This could make the task of deterring a Chinese attack harder for the United States.



Third, America’s other security commitments could draw attention and resources away from Taiwan. Keeping pace with the PLA in the Taiwan Strait will require investments in military power that will become more difficult to sustain, barring either a reduction in global commitments or a significant decrease in China’s own economic and military power. The fight against ISIS in the Middle East and North Africa, the Russian threat to Eastern Europe, and Chinese island‐​building in the South China Sea are all vying for the attention of the U.S. military. The military has been able to cope with these contingencies, but there are signs of strain on the force.82 Given America’s current global security posture, it will be difficult for the United States to sustain a minimum advantage over the PLA in perpetuity.



Sustaining a minimum U.S. military advantage is growing more difficult and costly over time as these above trends develop. Fortunately, the costs are likely to increase slowly and could be mitigated by advances in U.S. military technology. However, ultimately the United States will be stuck in the unenviable position of trying to defend Taiwan from a China that has growing military power and a strong interest in prevailing in any dispute.



The two most important potential negative consequences of stepping down from the defense commitment to Taiwan are the reputational and credibility costs to the United States and the worsening of America’s military position in the region. Advocates of maintaining the U.S. commitment also contend that Chinese control over Taiwan would lead to a substantial PLA presence, which would pose a serious threat to American and allied interests. The military dominance that the United States has enjoyed since the end of World War II would be called into question. Advocates of U.S. primacy in East Asia consider such an outcome dangerous and unacceptable.83



Opponents of stepping down from the commitment argue that both China and the United States’ Asian allies will view such a change as a sign of American weakness and unwillingness to live up to other commitments.84 If the United States does not show strong resolve as China grows more powerful, Beijing would take advantage of American weakness to more forcefully pursue objectives that are detrimental to U.S. allies and partners.85 The Brookings Institution’s Richard Bush argues that “[the United States] cannot withdraw from the cross‐​Strait contest altogether because U.S. allies and partners would likely read withdrawal as a sign that the U.S. security commitments to them are no longer dependable.”86 Stepping down from the commitment to Taiwan would have two mutually reinforcing harmful effects: China would grow bolder in threatening U.S. allies and the allies would presume that the United States would not fulfill its commitments as the threat from China grows.



Fears over these negative consequences stem from a popular misconception of credibility in which the past actions of a state are considered indicative of how the state will behave in the future. As noted earlier, academic research indicates that states take other factors into account when making judgements of credibility, but the dogmatic adherence to this misconception among the American policymaking elite makes stepping down from the commitment an uphill battle.87 Formal treaty commitments to states like Japan and South Korea carry more weight than America’s vague commitment to Taiwan, but fears of abandonment will likely weigh heavily on the minds of policymakers in Seoul, Tokyo, and Washington.88 Overturning the assumptions that credibility is bound up in upholding past promises will take a great deal of time and effort.



Ending the U.S. defense commitment to Taiwan could be detrimental to the U.S. military’s broader goals in East Asia. Taiwan lies in the middle of an island chain that runs from Japan to the South China Sea. Control of Taiwan has important strategic implications because of this location. The PLA could use Taiwan as a staging area to more easily project power into the South China Sea, the East China Sea, and the western Pacific.89 Keeping this island chain free of Chinese military bases and friendly to the United States is therefore seen as essential for America’s position in the region. Indeed, Taiwan has loomed large in American military strategy in the region for decades. In 1950 General Douglas MacArthur described Taiwan as “an unsinkable aircraft carrier and submarine tender ideally located to accomplish offensive strategy and at the same time checkmate defensive or counter‐​offensive operations” from the surrounding area.90 If Taiwan becomes the PLA’s ‘unsinkable aircraft carrier,’ it would make U.S. military actions in support of other regional interests more difficult.



Fears over China’s improved military position that would follow seizing control over Taiwan are valid, but there are roadblocks to this outcome that exist independent of the U.S. defense commitment. As mentioned earlier in this analysis, China would face numerous hurdles and negative consequences if it tried to invade Taiwan, given the difficulty of conducting amphibious invasions, the high likelihood of regional backlash, and the materiel and training limitations of the PLA.91 Taiwan could also do more to raise the costs of conflict for China through changes in military technology and warfighting doctrine.92 For example, Taiwan’s fleet of fighter aircraft is costly to maintain and outclassed by PLA fighters and surface‐​to‐​air missile capabilities.93 Reducing the size of Taiwan’s fighter fleet and redirecting funds to build up mobile missile forces that could support ground units fighting against a PLA invasion attempt would improve Taiwan’s ability to resist the PLA and inflict heavy losses on Chinese forces.94 If President Tsai and the DPP can deliver on their promises to increase defense spending and develop Taiwan’s defense industries, Taiwan could be capable of mounting an effective self defense without American intervention in the coming decades.



The United States should step down from the implicit commitment to use military force to preserve Taiwan’s de facto independence. American credibility is slowly eroding as China becomes more powerful, and the commitment will be more costly to maintain for a relatively minor benefit. Broadly speaking, the United States has two options for how it could implement this policy option: it could try to draw concessions from China to get something in return for stepping down from the commitment, or it could unilaterally drop the commitment. In either scenario, Taiwan would have to take on sole responsibility for deterring Chinese military action.



A policy that wins concessions from China would be the more desirable of the two options. Concessions could include resolution of other territorial disputes involving China and American allies or dropping the Chinese threat to use force against Taiwan. This would be characteristic of what Charles Glaser calls a grand bargain, “an agreement in which two actors make concessions across multiple issues to create a fair deal … that would have been impossible in an agreement that dealt with a single issue.”95 Making the end of the U.S. commitment to Taiwan contingent upon Chinese concessions to resolve its other territorial disputes peacefully would benefit both the United States and China.96 The United States would free itself of an increasingly costly and risky commitment to Taiwan’s defense, but only if China compromises in ways that align with U.S. allies’ interests in the South and East China Seas. China would have to limit its objectives in the South and East China Seas, but in return would earn a major policy concession from the United States on a core national interest that has much more importance than the other territorial disputes.



If China proves unwilling to make concessions across multiple issue areas, the United States could still push for concessions on China’s military posture toward Taiwan. Instead of demanding a concession on the South China Sea dispute, U.S. policymakers could press China to take actions that reduce the military threat it poses to Taiwan via an incremental, reciprocal process of concessions.97 Refusing to sell Taiwan any new military equipment would be a good way to initiate a cooperation spiral.



Stopping the sale of new equipment would not significantly reduce the Taiwanese military’s ability to defend itself for three reasons. First, most equipment sold to Taiwan by the United States does not represent the latest in U.S. military technology and is not necessarily superior to new capabilities fielded by the PLA.98 Second, Taiwan’s domestic defense industry is capable of producing new equipment that is well‐​suited to asymmetric defense, although it will take time for Taiwan’s relatively small and underdeveloped defense industry to reach its full potential.99 Finally, stopping the sale of new weapons still gives the United States the latitude to sell spare parts and ammunition for weapons systems that have already been sold. Halting the sale of new types of weapons systems will signal a reduced U.S. commitment to Taiwan’s security that would not be overly disruptive to Taiwan’s self‐​defense.



One of several ways that Beijing might respond to this U.S. concession on arms sales would be to reduce the number of short‐​range ballistic missiles (SRBMs) within firing range of Taiwan. Currently there are more than 1,000 conventionally armed SRBMs (with a maximum range of approximately 500 miles) in the PLA arsenal that could strike Taiwan.100 Improvements in guidance technology have transformed these missiles from inaccurate “terror weapons” that would likely target cities to precision munitions better suited for strikes against military airfields and ports.101Stationing the SRBMs out of range of Taiwan would be a low‐​cost, but symbolically important, action. The missiles are fired from mobile launchers that could be moved back into range of Taiwan. However, the act of moving the missiles out of range would, according to Lyle J. Goldstein, “show goodwill and increasing confidence across the Strait and also between Washington and Beijing.”102 If China agrees to America’s demand to relocate its ballistic missiles, then additional steps could be taken to further reduce the threat China poses to Taiwan.



If China proved unwilling to make any concessions, either in other territorial disputes or in cross‐​strait relations, the United States could still unilaterally withdraw from its military commitment to Taiwan. No demands or conditions would be placed on Chinese behavior. American policymakers are unlikely to accept such a course of action given recent shows of Chinese assertiveness. Charles Glaser explains, “China appears too likely to misinterpret [unilaterally ending the U.S. commitment to defend Taiwan], which could fuel Chinese overconfidence and intensify challenges to U.S. interests.”103 Unilateral withdrawal would reduce the likelihood of U.S.-Chinese armed conflict, but the dearth of other benefits would make the policy difficult for policymakers to implement. Extracting some kind of concession from China, either in cross‐​strait relations or in other territorial disputes, should be a priority.



Finally, stepping down from the commitment to defend Taiwan with military force does not remove America’s interest in keeping the Taiwan Strait free of armed conflict. The United States would retain the ability to punish China in other ways should it attack Taiwan. Diplomatic isolation and economic sanctions may not inflict the same kinds of costs on Beijing as military force, but they are additional costs that would have to be absorbed.104 Additionally, U.S. arms sales are separate from the implicit commitment to defend Taiwan and could continue, albeit in some reduced or modified form.105 Continuing to sell arms to Taiwan while stepping down from the implicit commitment to use military force to defend the island allows the United States to demonstrate support for Taiwan’s defense without taking on the risks associated with direct intervention.106



The United States should no longer provide the military backstop for Taiwan’s de facto independence. The security commitment to Taiwan outlined in the TRA is a product of a different time, when the United States enjoyed clear military advantages over China, and Taiwan could be defended on the cheap. China’s growing military power strains the credibility of the American commitment. Policymakers in Washington could respond to this changing environment by restoring American military superiority, sustaining a minimum military advantage, or stepping down from the commitment. All of these options carry risks and negative consequences, but it is in the best long‐​term interest of the United States to step down from the commitment to Taiwan.



American policymakers must come to terms with the idea that the balance of power has become much more favorable for Beijing since the TRA was adopted in 1979. Defending Taiwan is more difficult now than ever before, and this trend will be very hard to reverse. The most realistic way to reorient U.S. policy is to reach out to China to take incremental, reciprocal steps that slowly bring about the end of America’s commitment. This policy will be very difficult for the United States to implement, but the advantages to U.S.-China relations could be substantial. Changing the U.S.-Taiwan security relationship would greatly reduce the likelihood of armed conflict between the United States and China and could create opportunities for U.S.-China cooperation that are currently beyond reach.



1\. Barry R. Posen, Restraint: A New Foundation for U.S. Grand Strategy (Ithaca, NY: Cornell University Press, 2014), p. 102.



2\. Richard Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” Asia Working Group Paper no. 1, Brookings Institution, December 2015, p. 5.



3\. Andrew Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” in China’s Great Leap Outward: Hard and Soft Dimensions of a Rising Power, ed. Andrew Scobell and Marylena Mantas (New York: The Academy of Political Science, 2014), pp. 130–31.



4\. Invasion is not the only military option available to China. The PLA could also conduct a blockade of Taiwan or conduct decapitation strikes to eliminate Taiwan’s political leadership. This analysis focuses on a Chinese invasion attempt because it is the most severe military option in terms of costs for all sides involved, and it carries the best chance for Beijing to accomplish its ultimate goal of reunifying Taiwan with the mainland via direct military and political control.



5\. For an excellent overview of the 1995–1996 crisis, see: Ted Galen Carpenter, America’s Coming War with China: A Collision Course over Taiwan (New York: Palgrave Macmillan, 2005), pp. 66–70; Robert S. Ross, “The 1995–96 Taiwan Strait Confrontation: Coercion, Credibility, and the Use of Force,” International Security 25, no. 2 (Fall 2000): 87–123. On the role of the crisis on China’s military modernization, see: Michael S. Chase et al., China’s Incomplete Military Transformation: Assessing the Weaknesses of the People’s Liberation Army (PLA) (Santa Monica, CA: RAND Corporation, 2015), p. 14; Andrew J. Nathan and Andrew Scobell, China’s Search for Security (New York: Columbia University Press, 2012), pp. 303–08.



6\. Dean Cheng, “Countering China’s A2/AD Challenge,” The National Interest, September 20, 2013, http://nationalinterest.org/commentary/countering-china%E2%80%99s-a2-ad-challenge-9099?page=show; Henry J. Hendrix, At What Cost a Carrier? Disruptive Defense Papers (Washington: Center for a New American Security, 2013); Ronald O’Rourke, China’s Naval Modernization: Implications for U.S. Navy Capabilities—Background and Issues for Congress (Washington: Congressional Research Service, 2015).



7\. Eric Heginbotham et al., The U.S.-China Military Scorecard: Forces, Geography, and the Evolving Balance of Power 1996–2017 (Santa Monica, CA: RAND Corporation, 2015), p. 330.



8\. Lyle J. Goldstein, Meeting China Halfway: How to Defuse the Emerging U.S.-China Rivalry (Washington: Georgetown University Press, 2015), pp. 52–53.



9\. Tom Phillips, “Taiwan Elects First Female President,” Guardian (London), January 16, 2016, http://​www​.the​guardian​.com/​w​o​r​l​d​/​2​0​1​6​/​j​a​n​/​1​6​/​t​a​i​w​a​n​-​e​l​e​c​t​s​-​f​i​r​s​t​-​f​e​m​a​l​e​-​p​r​e​s​ident.



10\. Javier C. Hernandez, “China Suspends Diplomatic Contact With Taiwan,” New York Times, June 25, 2016, http://​www​.nytimes​.com/​2​0​1​6​/​0​6​/​2​6​/​w​o​r​l​d​/​a​s​i​a​/​c​h​i​n​a​-​s​u​s​p​e​n​d​s​-​d​i​p​l​o​m​a​t​i​c​-​c​o​n​t​a​c​t​-​w​i​t​h​-​t​a​i​w​a​n​.html.



11\. Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” pp. 15–19.



12\. Shannon Tiezzi, “China’s ‘New Normal’ Economy and Social Stability,” The Diplomat, November 24, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​1​1​/​c​h​i​n​a​s​-​n​e​w​-​n​o​r​m​a​l​-​e​c​o​n​o​m​y​-​a​n​d​-​s​o​c​i​a​l​-​s​t​a​b​i​lity/.



13\. Ted Galen Carpenter, “Could China’s Economic Troubles Spark a War?” The National Interest, September 6, 2015, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​c​o​u​l​d​-​c​h​i​n​a​s​-​e​c​o​n​o​m​i​c​-​t​r​o​u​b​l​e​s​-​s​p​a​r​k​-​w​a​r​-​13784.



14\. The full text of the Taiwan Relations Act can be found at American Institute in Taiwan, “Taiwan Relations Act,” January 1, 1979, http://​www​.ait​.org​.tw/​e​n​/​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​-​a​c​t​.html.



15\. Ibid.



16\. Brett V. Benson and Emerson M. S. Niou, “Comprehending Strategic Ambiguity: U.S. Security Commitment to Taiwan,” November 12, 2001, http://people.duke.edu/~niou/teaching/strategic%20ambiguity.pdf; Carpenter, America’s Coming War with China, pp. 7–8; J. Michael Cole, “Time to End U.S. ‘Ambiguity’ on Taiwan,” The Diplomat, July 6, 2012, http://​thediplo​mat​.com/​2​0​1​2​/​0​7​/​t​i​m​e​-​t​o​-​e​n​d​-​u​-​s​-​a​m​b​i​g​u​i​t​y​-​o​n​-​t​a​iwan/; and Michal Thim, “Time for an Improved Taiwan-U.S. Security Relationship,” American Citizens for Taiwan, February 21, 2016,http://​www​.amer​i​canci​t​i​zens​for​t​ai​wan​.org/​t​i​m​e​_​f​o​r​_​a​n​_​i​m​p​r​o​v​e​d​_​t​a​i​w​a​n​_​u​_​s​_​s​e​c​u​r​i​t​y​_​r​e​l​a​t​i​o​n​ship/.



17\. Carpenter, America’s Coming War with China, pp. 84–85; Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” pp. 135–36.



18\. Daryl G. Press, Calculating Credibility: How Leaders Assess Military Threats (Ithaca, NY: Cornell University Press, 2005), p. 3.



19\. Charles L. Glaser, “A U.S.-China Grand Bargain? The Hard Choice between Military Competition and Accommodation,” International Security 39, no. 4 (Spring 2015): 61.



20\. “China’s Xi says Political Solution for Taiwan Can’t Wait Forever,” Reuters, October 6, 2013, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​a​s​i​a​-​a​p​e​c​-​c​h​i​n​a​-​t​a​i​w​a​n​-​i​d​U​S​B​R​E​9​9​5​0​3​Q​2​0​1​31006.



21\. On the asymmetry of interests between China and the United States, see: Charles Glaser, “Will China’s Rise Lead to War? Why Realism Does Not Mean Pessimism,” Foreign Affairs 90, no. 2 (March/​April 2011): 86; Glaser, “A U.S.-China Grand Bargain?” p. 50; Goldstein, Meeting China Halfway, p. 65; John J. Mearsheimer, “Say Goodbye to Taiwan,” The National Interest no. 130 (March/​April 2014): 103; Posen, Restraint, p. 103; and Ross, “The 1995–96 Taiwan Strait Confrontation,” p. 123.



22\. Press, Calculating Credibility, p. 3.



23\. Roger Cliff, China’s Military Power: Assessing Current and Future Capabilities (New York: Cambridge University Press, 2015), p. 221. Emphasis in original quote.



24\. Heginbotham et al., The U.S.-China Military Scorecard, p. 331.



25\. Press, Calculating Credibility, p. 21.



26\. Philip C. Saunders and Joel Wuthnow, China’s Goldwater‐​Nichols? Assessing PLA Organizational Reforms (Washington: National Defense University, April 2016).



27\. The trade value figure from the U.S. Census Bureau represents the sum of U.S. exports to ($116.2 billion) and imports from ($481.9 billion) China. See United States Census, “Trade in Goods with China,” https://​www​.cen​sus​.gov/​f​o​r​e​i​g​n​-​t​r​a​d​e​/​b​a​l​a​n​c​e​/​c​5​7​0​0​.html.



28\. Richard C. Bush, Untying the Knot: Making Peace in the Taiwan Strait (Washington: Brookings Institution Press, 2005), p. 258.



29\. Cliff, China’s Military Power, p. 197.



30\. Michael Mazza, “Taiwanese Hard Power: Between a ROC and a Hard Place,” in A Hard Look at Hard Power: Assessing the Defense Capabilities of Key U.S. Allies and Security Partners, ed. Gary J. Schmitt (Carlisle Barracks, PA: U.S. Army War College Press, 2015), p. 221.



31\. Kent Wang, “Why the U.S. Should Sell Advanced Fighters to Taiwan,” The Diplomat, January 10, 2014, http://​thediplo​mat​.com/​2​0​1​4​/​0​1​/​w​h​y​-​t​h​e​-​u​s​-​s​h​o​u​l​d​-​s​e​l​l​-​a​d​v​a​n​c​e​d​-​f​i​g​h​t​e​r​s​-​t​o​-​t​a​iwan/.



32\. Walter Lohman, “What the United States Owes to Taiwan and Its Interests in Asia,” War on the Rocks, January 27, 2016, http://​waron​the​rocks​.com/​2​0​1​6​/​0​1​/​w​h​a​t​-​t​h​e​-​u​n​i​t​e​d​-​s​t​a​t​e​s​-​o​w​e​s​-​t​o​-​t​a​i​w​a​n​-​a​n​d​-​i​t​s​-​i​n​t​e​r​e​s​t​s​-​i​n​-​asia/.



33\. Riley Walters, “Affirming the Taiwan Relations Act,” The Daily Signal, March 27, 2014, http://​dai​lysig​nal​.com/​2​0​1​4​/​0​3​/​2​7​/​a​f​f​i​r​m​i​n​g​-​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​-act/.



34\. Quoted in Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” p. 131. Also see David E. Sanger, “U.S. Would Defend Taiwan, Bush Says,” _New York Times_, April 26, 2001, http://​www​.nytimes​.com/​2​0​0​1​/​0​4​/​2​6​/​w​o​r​l​d​/​u​s​-​w​o​u​l​d​-​d​e​f​e​n​d​-​t​a​i​w​a​n​-​b​u​s​h​-​s​a​y​s​.​h​t​m​l​?​p​a​g​e​w​a​n​t​e​d=all.



35\. William Kristol, “The Taiwan Relations Act: The Next 25 Years,” in Rethinking “One China,” ed. John J. Tkacik, Jr. (Washington: The Heritage Foundation, 2004), p. 17.



36\. Zhidong Hao, “After the Anti‐​Secession Law: Cross‐​Strait and U.S.-China Relations,” in Challenges to Chinese Foreign Policy: Diplomacy, Globalization, and the Next World Power, ed. Yufan Hao, George Wei, and Lowell Dittmer (Lexington, KY: The University Press of Kentucky, 2009), p. 201.



37\. Full text of the Anti‐​Secession Law can be found at Embassy of the People’s Republic of China in the United States of America, “Anti‐​Secession Law” (full text), March 15, 2005, http://​www​.chi​na​-embassy​.org/​e​n​g​/​z​t​/​9​9​9​9​9​9​9​9​9​/​t​1​8​7​4​0​6.htm. See also, Chunjuan Nancy Wei, “China’s Anti‐​Secession Law and Hu Jintao’s Taiwan Policy,” Yale Journal of International Affairs 5, no. 1 (Winter 2010): 112–27.



38\. “Following Historic China‐​Taiwan Meeting, Rubio Calls for Strengthening U.S.-Taiwan Relations,” press release, Marco Rubio’s official website, November 7, 2015, http://www.rubio.senate.gov/public/index.cfm/press-releases?ID=2ae3bcfd-82f8-4ffe-9580–6b988123c1d0.



39\. Eric Gomez, discussion with staffer for a senior Member of the House Armed Services Committee, October 6, 2015.



40\. John Lee, “Why Does China Fear Taiwan?” The American Interest, November 6, 2015, http://​www​.the​-amer​i​can​-inter​est​.com/​2​0​1​5​/​1​1​/​0​6​/​w​h​y​-​d​o​e​s​-​c​h​i​n​a​-​f​e​a​r​-​t​a​iwan/.



41\. Eric Gomez, conversation with Andrew Scobell, Senior Political Scientist, RAND Corporation, November 23, 2015.



42\. Michael Forsythe, “China Protests Sale of U.S. Arms to Taiwan,” New York Times, December 17, 2015, http://​www​.nytimes​.com/​2​0​1​5​/​1​2​/​1​8​/​w​o​r​l​d​/​a​s​i​a​/​t​a​i​w​a​n​-​a​r​m​s​-​s​a​l​e​s​-​u​s​-​c​h​i​n​a​.html.



43\. US‐​Taiwan Business Council and Project 2049 Institute, Chinese Reactions to Taiwan Arms Sales, ed. Lotta Danielsson (Arlington: US‐​Taiwan Business Council and Project 2049 Institute, 2012), p. 36.



44\. On the subject of AV-8 aircraft, see Wendell Minnick, “Despite Pressures from China, Taiwan Might Procure Harriers,” Defense News, January 16, 2016, http://​www​.defense​news​.com/​s​t​o​r​y​/​d​e​f​e​n​s​e​/​a​i​r​-​s​p​a​c​e​/​s​t​r​i​k​e​/​2​0​1​6​/​0​1​/​1​6​/​d​e​s​p​i​t​e​-​p​r​e​s​s​u​r​e​s​-​c​h​i​n​a​-​t​a​i​w​a​n​-​m​i​g​h​t​-​p​r​o​c​u​r​e​-​h​a​r​r​i​e​r​s​/​7​8​7​3​3284/. On Perry‐​class frigates, see Ankit Panda, “US Finalizes Sale of Perry‐​class Frigates to Taiwan,” The Diplomat, December 20, 2014, http://​thediplo​mat​.com/​2​0​1​4​/​1​2​/​u​s​-​f​i​n​a​l​i​z​e​s​-​s​a​l​e​-​o​f​-​p​e​r​r​y​-​c​l​a​s​s​-​f​r​i​g​a​t​e​s​-​t​o​-​t​a​iwan/. On F-16 aircraft, see: Van Jackson, “Forget F‐​16s for Taiwan: It’s All About A2/AD,” The Diplomat, April 8, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​0​4​/​f​o​r​g​e​t​-​f​-​1​6​s​-​f​o​r​-​t​a​i​w​a​n​-​i​t​s​-​a​l​l​-​a​b​o​u​t​-​a2ad/.



45\. William S. Murray, “Revisiting Taiwan’s Defense Strategy,” Naval War College Review 61, no. 3 (Summer 2008): 13–38; and Jim Thomas et al., Hard ROC 2.0 Taiwan and Deterrence through Protraction (Washington: Center for Strategic and Budgetary Assessments, 2014).



46\. Thomas et al., Hard ROC 2.0, p. 4.



47\. Bonnie Glaser and Anastasia Mark, “Taiwan’s Defense Spending: The Security Consequences of Choosing Butter over Guns,” Asia Maritime Transparency Initiative, March 18, 2015, http://​amti​.csis​.org/​t​a​i​w​a​n​s​-​d​e​f​e​n​s​e​-​s​p​e​n​d​i​n​g​-​t​h​e​-​s​e​c​u​r​i​t​y​-​c​o​n​s​e​q​u​e​n​c​e​s​-​o​f​-​c​h​o​o​s​i​n​g​-​b​u​t​t​e​r​-​o​v​e​r​-​guns/. Also see: Justin Logan and Ted Galen Carpenter, “Taiwan’s Defense Budget: How Taipei’s Free Riding Risks War,” Cato Institute Policy Analysis no. 600, September 13, 2007.



48\. For the full text of the letter, see Taiwan Defense and National Security, “Benjamin L. Cardin and John McCain Letter to President Obama Regarding Arms Sales to Taiwan,” November 19, 2015, http://www.ustaiwandefense.com/benjamin-l-cardin-john-mccain-letter-to-president-obama-regarding-arms-sales-to-taiwan-november-19–2015/.



49\. James Holmes, “Securing Taiwan Starts with Overhauling Its Navy,” The National Interest, February 5, 2016, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​s​e​c​u​r​i​n​g​-​t​a​i​w​a​n​-​s​t​a​r​t​s​-​o​v​e​r​h​a​u​l​i​n​g​-​t​h​e​-​n​a​v​y​-​15122.



50\. Ian Easton, Able Archers: Taiwan Defense Strategy in an Age of Precision Strike (Arlington: Project 2049 Institute, September 2014), pp. 35–37.



51\. Ibid., p. 36 (TK surface‐​to‐​air missile), and p. 64 (HF-3 anti‐​ship missile). It should be noted that the quote comes from a report written in late 2014. In February 2016, the U.S. Navy announced that the Standard Missile‐​6 (SM-6), which is capable of greater range and speed than the HF-3, will be modified for use as an anti‐​ship missile. Sam LaGrone, “SECDEF Carter Confirms Navy Developing Supersonic Anti‐​Ship Missile for Cruisers, Destroyers,” USNI News, February 4, 2016, http://​news​.usni​.org/​2​0​1​6​/​0​2​/​0​4​/​s​e​c​d​e​f​-​c​a​r​t​e​r​-​c​o​n​f​i​r​m​s​-​n​a​v​y​-​d​e​v​e​l​o​p​i​n​g​-​s​u​p​e​r​s​o​n​i​c​-​a​n​t​i​-​s​h​i​p​-​m​i​s​s​i​l​e​-​f​o​r​-​c​r​u​i​s​e​r​s​-​d​e​s​t​r​oyers.



52\. Michael Green et al., Asia‐​Pacific Rebalance 2025: Capabilities, Presence and Partnerships (Washington: Center for Strategic and International Studies, January 2016), p. 94.



53\. Susan Thornton, “Testimony of Susan Thornton,” House Foreign Affairs Committee, Subcommittee on Asia and the Pacific (Washington: House Foreign Affairs Committee, February 11, 2016), p. 4, http://​docs​.house​.gov/​m​e​e​t​i​n​g​s​/​F​A​/​F​A​0​5​/​2​0​1​6​0​2​1​1​/​1​0​4​4​5​7​/​H​H​R​G​-​1​1​4​-​F​A​0​5​-​W​s​t​a​t​e​-​T​h​o​r​n​t​o​n​S​-​2​0​1​6​0​2​1​1.pdf.



54\. Mearsheimer, “Say Goodbye to Taiwan,” p. 35.



55\. Shelley Rigger, “Why Giving Up Taiwan Will Not Help Us with China,” American Enterprise Institute (November 2011), p. 3.



56\. Ivan Arreguín‐​Toft, “How the Weak Win Wars: A Theory of Asymmetric Conflict,” International Security 26, no. 1 (Summer 2001): 96.



57\. Michael J. Lostumbo et al., Air Defense Options for Taiwan: An Assessment of Relative Costs and Operational Benefits (Santa Monica, CA: RAND Corporation, 2016); Murray, “Revisiting Taiwan’s Defense Strategy,” p. 13.



58\. Holmes, “Securing Taiwan Starts with Overhauling Its Navy.”



59\. Grant Newsham and Kerry Gershaneck, “Saving Taiwan’s Marine Corps,” The Diplomat, November 16, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​1​1​/​s​a​v​i​n​g​-​t​h​e​-​t​a​i​w​a​n​-​m​a​r​i​n​e​-​c​orps/.



60\. Goldstein, Meeting China Halfway, p. 65.



61\. Dan De Luce et al., “Why China’s Land Grab Is Backfiring on Beijing,” Foreign Policy, December 7, 2015, http://​for​eign​pol​i​cy​.com/​2​0​1​5​/​1​2​/​0​7​/​w​h​y​-​c​h​i​n​a​s​-​l​a​n​d​-​g​r​a​b​-​i​s​-​b​a​c​k​f​i​r​i​n​g​-​o​n​-​b​e​i​jing/; Prashanth Parameswaran, “Indonesia Plays Up New South China Sea ‘Base’ after China Spat,” The Diplomat, March 28, 2016, http://​thediplo​mat​.com/​2​0​1​6​/​0​3​/​i​n​d​o​n​e​s​i​a​-​p​l​a​y​s​-​u​p​-​n​e​w​-​s​o​u​t​h​-​c​h​i​n​a​-​s​e​a​-​b​a​s​e​-​a​f​t​e​r​-​c​h​i​n​a​-​spat/; and Richard Sisk, “Japan Sends ‘Destroyer’ to South China Sea in Message to China,” Mil​i​tary​.com, April 6, 2016, http://​www​.mil​i​tary​.com/​d​a​i​l​y​-​n​e​w​s​/​2​0​1​6​/​0​4​/​0​8​/​j​a​p​a​n​-​s​e​n​d​s​-​d​e​s​t​r​o​y​e​r​-​t​o​-​s​o​u​t​h​-​c​h​i​n​a​-​s​e​a​-​i​n​-​m​e​s​s​a​g​e​-​t​o​-​c​h​i​n​a​.​h​t​m​l​#​d​i​s​q​u​s​_​t​hread.



62\. Chase et al., China’s Incomplete Military Transformation; and Scott L. Kastner, “Is the Taiwan Strait Still a Flash Point? Rethinking the Prospects for Armed Conflict between China and Taiwan,” International Security 40, no. 3 (Winter 2015/16): 71–74.



63\. David A. Shlapak et al., A Question of Balance: Political Context and Military Aspects of the China‐​Taiwan Dispute (Santa Monica, CA: RAND Corporation, 2009), p. 118.



64\. Chase et al., China’s Incomplete Military Transformation, p. 100.



65\. Recent military reforms could speed up the pace of solving these challenges. See “Xi’s New Model Army,” The Economist, January 16, 2016, http://​www​.econ​o​mist​.com/​n​e​w​s​/​c​h​i​n​a​/​2​1​6​8​8​4​2​4​-​x​i​-​j​i​n​p​i​n​g​-​r​e​f​o​r​m​s​-​c​h​i​n​a​s​-​a​r​m​e​d​-​f​o​r​c​e​s​t​o​-​h​i​s​-​o​w​n​-​a​d​v​a​n​t​a​g​e​-​x​i​s​-​n​e​w​-​m​o​d​e​l​-army; Kor Kian Beng, “A Different PLA with China’s Military Reforms,” Straits Times (Singapore), January 5, 2016, http://​www​.strait​stimes​.com/​a​s​i​a​/​a​-​d​i​f​f​e​r​e​n​t​-​p​l​a​-​w​i​t​h​-​c​h​i​n​a​s​-​m​i​l​i​t​a​r​y​-​r​e​forms; and Mu Chunshan, “The Logic Behind China’s Military Reforms,” The Diplomat, December 5, 2015,http://​thediplo​mat​.com/​2​0​1​5​/​1​2​/​t​h​e​-​l​o​g​i​c​-​b​e​h​i​n​d​-​c​h​i​n​a​s​-​m​i​l​i​t​a​r​y​-​r​e​f​orms/.



66\. Glaser, “A U.S.-China Grand Bargain?” p. 51.



67\. Goldstein, Meeting China Halfway, p. 12.



68\. In a recent Pew survey, 23 percent of U.S. respondents considered China to be an adversary of the United States, while 44 percent considered China to be a “serious problem, but not an adversary.” Pew Research Center, “International Threats, Defense Spending,” May 5, 2016,http://​www​.peo​ple​-press​.org/​2​0​1​6​/​0​5​/​0​5​/​3​-​i​n​t​e​r​n​a​t​i​o​n​a​l​-​t​h​r​e​a​t​s​-​d​e​f​e​n​s​e​-​s​p​e​n​ding/.



69\. On cyber espionage, see: Jon R. Lindsay, “The Impact of China on Cybersecurity: Fiction and Friction,” _International Security_ 39, no. 3 (Winter 2014/15): 7–47; and Ellen Nakashima, “Chinese Breach Data of 4 Million Federal Workers,” _Washington Post_ , June 4, 2015,https://​www​.wash​ing​ton​post​.com/​w​o​r​l​d​/​n​a​t​i​o​n​a​l​-​s​e​c​u​r​i​t​y​/​c​h​i​n​e​s​e​-​h​a​c​k​e​r​s​-​b​r​e​a​c​h​-​f​e​d​e​r​a​l​-​g​o​v​e​r​n​m​e​n​t​s​-​p​e​r​s​o​n​n​e​l​-​o​f​f​i​c​e​/​2​0​1​5​/​0​6​/​0​4​/​8​8​9​c​0​e​5​2​-​0​a​f​7​-​1​1​e​5​-​9​5​f​d​-​d​5​8​0​f​1​c​5​d​4​4​e​_​s​t​o​r​y​.html. On alternative institutions, see: Jane Perlez, “China Creates a World Bank of Its Own, and the U.S. Balks,” _New York Times_ , December 4, 2015, http://​www​.nytimes​.com/​2​0​1​5​/​1​2​/​0​5​/​b​u​s​i​n​e​s​s​/​i​n​t​e​r​n​a​t​i​o​n​a​l​/​c​h​i​n​a​-​c​r​e​a​t​e​s​-​a​n​-​a​s​i​a​n​-​b​a​n​k​-​a​s​-​t​h​e​-​u​s​-​s​t​a​n​d​s​-​a​l​o​o​f​.​h​t​m​l​?_r=0. On the South China Sea, see: Melissa Sim, “U.S., China Cross Swords over South China Sea,” Straits Times (Singapore), February 25, 2016, http://​www​.strait​stimes​.com/​w​o​r​l​d​/​u​n​i​t​e​d​-​s​t​a​t​e​s​/​u​s​-​c​h​i​n​a​-​c​r​o​s​s​-​s​w​o​r​d​s​-​o​v​e​r​-​s​o​u​t​h​-​c​h​i​n​a-sea.



70\. Joshua P. Meltzer, “U.S.-China Joint Presidential Statement on Climate Change: The Road to Paris and Beyond,” Brookings Institution, September 29, 2015, http://​www​.brook​ings​.edu/​b​l​o​g​s​/​p​l​a​n​e​t​p​o​l​i​c​y​/​p​o​s​t​s​/​2​0​1​5​/​0​9​/​2​9​-​u​s​-​c​h​i​n​a​-​s​t​a​t​e​m​e​n​t​-​c​l​i​m​a​t​e​-​c​h​a​n​g​e​-​m​e​ltzer; and Somini Sengupta, “U.S. and China Agree on Proposal for Tougher North Korea Sanctions,” New York Times, February 25, 2016, http://​www​.nytimes​.com/​2​0​1​6​/​0​2​/​2​6​/​w​o​r​l​d​/​a​s​i​a​/​n​o​r​t​h​-​k​o​r​e​a​-​s​a​n​c​t​i​o​n​s​.html.



71\. Posen, Restraint, pp. 93–96.



72\. Exacerbating tensions: Andrew J. Nathan and Andrew Scobell, “How China Sees America: The Sum of Beijing’s Fears,” Foreign Affairs 91, no. 5 (September/​October 2012): 32–47; and Robert S. Ross, “The Problem with the Pivot,” Foreign Affairs 91, no. 6 (November/​December 2012): 70–82. On the subject of Taiwan’s role in the pivot, see: Green et al., Asia‐​Pacific Rebalance 2025, pp. 87–94.



73\. Fiona S. Cunningham and M. Taylor Fravel, “Assuring Assured Retaliation: China’s Nuclear Posture and U.S.-China Strategic Stability,” International Security 40, no.2 (Fall 2015): 16–19; and Gregory Kulacki, China’s Military Calls for Putting Its Nuclear Forces on Alert (Cambridge, MA: Union of Concerned Scientists, January 2016).



74\. Jennifer Lind, “Japan’s Security Evolution,” Cato Institute Policy Analysis no. 788, February 25, 2016; Logan and Carpenter, “Taiwan’s Defense Budget.”



75\. Kastner, “Is the Taiwan Strait Still a Flash Point?” p. 76.



76\. Ibid.



77\. “The Causes and Consequences of China’s Market Crash,” The Economist, August 24, 2015, http://​www​.econ​o​mist​.com/​n​e​w​s​/​b​u​s​i​n​e​s​s​-​a​n​d​-​f​i​n​a​n​c​e​/​2​1​6​6​2​0​9​2​-​c​h​i​n​a​-​s​n​e​e​z​i​n​g​-​r​e​s​t​-​w​o​r​l​d​-​r​i​g​h​t​l​y​-​n​e​r​v​o​u​s​-​c​a​u​s​e​s​-​a​n​d​-​c​o​n​s​e​q​u​e​n​c​e​s​-​c​hinas.



78\. James Griffiths, “China on Strike,” CNN, March 29, 2016, http://​www​.cnn​.com/​2​0​1​6​/​0​3​/​2​8​/​a​s​i​a​/​c​h​i​n​a​-​s​t​r​i​k​e​-​w​o​r​k​e​r​-​p​r​o​t​e​s​t​-​t​r​a​d​e​-​u​n​i​o​n​/​i​n​d​e​x​.html.



79\. Kevin Yao and Meng Meng, “China Expects to Lay Off 1.8 Million Workers in Coal, Steel Sectors,” Reuters, February 29, 2016, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​c​h​i​n​a​-​e​c​o​n​o​m​y​-​e​m​p​l​o​y​m​e​n​t​-​i​d​U​S​K​C​N​0​W205X.



80\. Carpenter, “Could China’s Economic Troubles Spark a War?”



81\. Taisu Zhang, “China’s Coming Ideological Wars,” Foreign Policy, March 1, 2016, http://​for​eign​pol​i​cy​.com/​2​0​1​6​/​0​3​/​0​1​/​c​h​i​n​a​s​-​c​o​m​i​n​g​-​i​d​e​o​l​o​g​i​c​a​l​-​w​a​r​s​-​n​e​w​-​l​e​f​t​-​c​o​n​f​u​c​i​u​s​-​m​a​o-xi/.



82\. David Larter, “Carrier Scramble: CENTCOM, PACOM Face Flattop Gaps This Spring Amid Tensions,” Navy Times, January 7, 2016, http://​www​.navy​times​.com/​s​t​o​r​y​/​m​i​l​i​t​a​r​y​/​2​0​1​6​/​0​1​/​0​7​/​c​a​r​r​i​e​r​-​s​c​r​a​m​b​l​e​-​c​e​n​t​c​o​m​-​p​a​c​o​m​-​f​a​c​e​-​f​l​a​t​t​o​p​-​g​a​p​s​-​s​p​r​i​n​g​-​a​m​i​d​-​t​e​n​s​i​o​n​s​/​7​8​4​2​6140/; and Andrea Shalal, “U.S. Arms Makers Strain to Meet Demand as Mideast Conflicts Rage,” Reuters, December 4, 2015, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​m​i​d​e​a​s​t​-​c​r​i​s​i​s​-​u​s​a​-​a​r​m​s​-​i​n​s​i​g​h​t​-​i​d​U​S​K​B​N​0​T​N​2​D​A​2​0​1​51204.



83\. Dana R. Dillon and John J. Tkacik, Jr., “China and ASEAN: Endangered American Primacy in Southeast Asia,” Heritage Foundation Backgrounder no. 1886, October 19, 2005, http://​www​.her​itage​.org/​r​e​s​e​a​r​c​h​/​r​e​p​o​r​t​s​/​2​0​0​5​/​1​0​/​c​h​i​n​a​-​a​n​d​-​a​s​e​a​n​-​e​n​d​a​n​g​e​r​e​d​-​a​m​e​r​i​c​a​n​-​p​r​i​m​a​c​y​-​i​n​-​s​o​u​t​h​e​a​s​t​-asia.



84\. Ross, “The 1995–96 Taiwan Strait Confrontation,” p. 109.



85\. Nancy Bernkopf Tucker and Bonnie Glaser, “Should the United States Abandon Taiwan?” The Washington Quarterly 34, no. 4 (Fall 2011): 32–33; Mearsheimer, “Say Goodbye to Taiwan”; Peter Navarro, “Is It Time for America to ‘Surrender’ Taiwan?” The National Interest, January 18, 2016, http://www.nationalinterest.org/blog/the-buzz/it-time-america-%E2%80%98surrender%E2%80%99-taiwan-14955; and Daniel Twining, “(Why) Should America Abandon Taiwan?” Foreign Policy, January 10, 2012, http://​for​eign​pol​i​cy​.com/​2​0​1​2​/​0​1​/​1​0​/​w​h​y​-​s​h​o​u​l​d​-​a​m​e​r​i​c​a​-​a​b​a​n​d​o​n​-​t​a​iwan/.



86\. Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” p. 21.



87\. Max Fisher, “The Credibility Trap,” Vox, April 29, 2016, http://​www​.vox​.com/​2​0​1​6​/​4​/​2​9​/​1​1​4​3​1​8​0​8​/​c​r​e​d​i​b​i​l​i​t​y​-​f​o​r​e​i​g​n​-​p​o​l​i​c​y-war; Paul Huth and Bruce Russett, “What Makes Deterrence Work? Cases from 1900 to 1980,” World Politics 36, no. 4 (July 1984): 496–526; Jonathan Mercer Reputation and International Politics (Ithaca, NY: Cornell University Press, 1996), pp. 22–25; and Press, Calculating Credibility, pp. 20–24.



88\. Glaser, “A U.S.-China Grand Bargain?” pp. 77–78.



89\. Bosco, “Taiwan and Strategic Security.”



90\. Quoted in Andrew S. Erickson and Joel Wuthnow, “Why Islands Still Matter in Asia,” The National Interest, February 5, 2016, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​w​h​y​-​i​s​l​a​n​d​s​-​s​t​i​l​l​-​m​a​t​t​e​r​-​a​s​i​a​-​1​5​1​2​1​?​p​a​g​e​=show.



91\. Nathan and Scobell, China’s Search for Security, pp. 307–08.



92\. J. Michael Cole, “How A2/AD Can Defeat China,” The Diplomat, November 12, 2013, http://​thediplo​mat​.com/​2​0​1​3​/​1​1​/​h​o​w​-​a​2​a​d​-​c​a​n​-​d​e​f​e​a​t​-​c​hina/; Eric Gomez, “Taiwan’s Best Option for Deterring China? Anti‐​Access/​Area Denial,” Cato at Liberty (blog), April 7, 2016,http://​www​.cato​.org/​b​l​o​g​/​t​a​i​w​a​n​s​-​b​e​s​t​-​o​p​t​i​o​n​-​d​e​t​e​r​r​i​n​g​-​c​h​i​n​a​-​a​n​t​i​-​a​c​c​e​s​s​a​r​e​a​-​d​enial; Holmes, “Securing Taiwan Starts with Overhauling Its Navy”; and Thomas et al., Hard ROC 2.0.



93\. Lostumbo et al., Air Defense Options for Taiwan, pp. 2–11.



94\. Ibid., pp. 73–89.



95\. Glaser, “A U.S.-China Grand Bargain?” p. 79.



96\. Ibid., pp. 78–83.



97\. Goldstein, _Meeting China Halfway,_ p. 12.



98\. Sam LaGrone, “UPDATED: U.S. Plans Modest $1.83B Taiwan Arms Deal; Little Offensive Power in Proposed Package,” _USNI News,_ December 16, 2015, https://news.usni.org/2015/12/16/breaking-u-s-plans-modest-1–83b-taiwan-arms-deal-little-offensive-power-in-proposed-package.



99\. For Taiwan’s indigenously produced equipment, see Easton, _Able Archers._



100\. Kastner, “Is the Taiwan Strait Still a Flash Point?” p. 70; and Mazza, “Taiwanese Hard Power: Between a ROC and a Hard Place,” p. 202.



101\. Murray, “Revisiting Taiwan’s Defense Strategy,” pp. 17–19; and Thomas et al., Hard ROC 2.0, pp. 12–14.



102\. Goldstein, _Meeting China Halfway,_ p. 63.



103\. Glaser, “A U.S.-China Grand Bargain?” p. 85.



104\. Carpenter, _America’s Coming War with China,_ p. 177.



105\. Eric Gomez, “The U.S.-Taiwan Relationship Needs a Change,” _Cato at Liberty_ (blog), November 30, 2015, http://​www​.cato​.org/​b​l​o​g​/​u​s​-​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​h​i​p​-​n​e​e​d​s​-​c​hange>.



106\. Carpenter, _America’s Coming War with China,_ p. 176.
"
"

On Sunday I posted about the USHCN climate station of record in Tucumcari, NM highlighting its positive points since it has all the hallmarks of a well sited station with a long and uninterrupted record. But something was odd with the temperature record that didn’t quite make sense at first glance.
I also cross posted my report on Climate Audit since I always value additional input from the community there.
I noted that while this station is in fact well sited, and rates a CRN2, it has some oddities with it’s temperature record around the year 2000, something that looked like a step function to me.

Click for larger graph from NASA GISTEMP
Of course, even though this a truly rural station, 3 miles from the outskirts of town, Hansen and GISS apply adjustments to it anyway, which is part of the flawed “nightlights” algorithm incorrectly flagging this station for adjustment. Even though the adjustment makes the present cooler, it still seems misplaced given the station quality and history. Steve McIntyre said it best:
Here we have a rural station where there doesn’t seem to be any reason to adjust the temperature for population growth/UHI. But in this case, Hansen adjusts Tucumcari as though it were a city. Why is he even adjusting Tucumcari at all? (The “reason” is that its lights value removes it from the rural classification and it goes into the adjustment pool.) While Hansen sometimes seemingly cools rural stations in the past, for the GISS dset2 version here, he warms the past of the station (cools the present).
It’s more that this is a case of another unjustified adjustment by the “adjuster in chief” showing once again that the Hansen adjustments do not do what they are supposed to do – and the best that can be hoped from the Hansen adjustment program by users of this dataset is that the adjustments overall end up being pointless and random, rather than pointless and biased.
The adjustment by GISS looks like this:

Ok adjustments aside, the fact that there is a step at 2000 that remained unexplained until commenters on CA started looking at the data themselves. DaleC provided this graph:

Click for original image
Note the arrow that I place at the year 2000. Notice anything?
The annual average minimum temperature has exceeded 45°F and maintained the rise since 2000. For the first time in the station history going back to 1905, the minimum temperature has gone above that 45°F mark and stayed there. Yes there have been some previous brief excursions above 45°F, but none appear to have lasted more than 2 years. Note that average annual maximum temperatures did not increase during the same period.
What could cause that? We can rule out adjustments, since this is GHCN data before Hansen gets his adjuster mitts on it. We can rule out location change or equipment change, since according to NCDC metadata the station has been in the same place since at least 1946 and possibly longer. It still uses mercury max/min thermometers, so there’s no MMTS next to a building or parking lot to blame.
So what is left? Something around the station in the measurement environment that affects the nighttime readings. I recalled seeing this before. And back in early 2007, I had posted a story about a paper from Dr. John Christy of UAH where he studied a number of stations in the San Joaquin Valley in California because they had exhibited this same symptom:
The culprit? Irrigation. See my story and Christy’s press release
Christy remarks: “Another factor is the dry air, something common to all deserts. Water vapor is a powerful greenhouse gas. Desert air lacks water vapor. The air turns cold at night because it doesn’t retain much warmth from the daytime and it can’t trap what little heat might rise from the ground at night.”
Evaporation from irrigated fields adds water vapor to the air — a process that cools summer days but traps heat rising from the damp soil at night.
“If there is anything I’ve learned in Alabama, it is that humidity can make summer nights very warm,” said Christy, a Fresno, Calif., native who has lived in Alabama since 1987.
Once I mentioned this as a possibility to explain the increase in nighttime temperatures, it didn’t take Steve McIntyre long to find some anecdotal evidence that correlated:

http://cahe.nmsu.edu/news/1997/043097_irrigation_tour.html
A few years ago when cattle prices were high, we saw a tremendous increase in the number of irrigated grass acres in Quay County,” said Jeff Bader, Quay County Extension program director. “One reason was our limited water situation for irrigation, and high cattle prices made it look very attractive.”
When cattle prices dropped again, interest in irrigated pastures declined, but now the cattle market is improving, he said. Producers never really lost concern about irrigated pasture because it fits into the management scheme for water conservation so well in Quay County.
“Irrigated pastures fill a niche in this area because of their ability to produce under varying levels of irrigation,” said Rex Kirksey, superintendent of NMSU’s Agricultural Science Center in Tucumcari. “Pastures remain a viable option in many situations where irrigation water is too limited or unpredictable for corn or alfalfa production.”
What is interesting is that the director of the Ag Science Center, Rex Kirksey, is also the person that took these photos for the station survey. There’s quite a large water project in the area, called unsurprisingly, the Tucumcari Project.







Conchas Dam and Lake




Apparently they have quite a problem with water “disappearing” there as outlined in this report:
“The principal problem has been the loss of over half of the District’s surface water supply in the canal and distribution system that carries Canadian River water from Conchas Reservoir to the irrigated farms in the District.”
“The District’s report concluded that seepage losses from the system’s canals become greater, in quantity, each year.”
From that report I obtained a the study area map, and located where the town and the USHCN station is. Unsurprisingly, the USHCN station is situated close to the canals of the water project, and prevailing winds in the area tend to be from south to southwest:

With increased irrigation to pastureland for cattle, and a leaky canal system that loses half it’s volumes and demands ever more water to meet customer deliveries, it seems plausible that the Tucumcari area is becoming more humid, and with the increased humidity, per Christy, increased night time temperatures.
The studies of Roger Pielke Sr. show that land-use changes are an important factor in local and regional climate change. This effects of the changes in agriculture and irrigation on the local measurement of climate in Tucumcari might very well make a good case study.
I had hoped that the automated weather station that sets next to the Stevenson Screen might have humidity data that I could track, but alas it does not.

There is a fairly complete record though of temperature and precipitation at this link from the Western Regional Climate Center.
UPDATE 7/1/08 One of our commenters “AnonyMoose” has brought this well done historical weather and climate report by NMSU superintendent Rex Kirksey to our attention:
http://tucumcarisc.nmsu.edu/documents/rr751.pdf
This merits some further study for the data it contains. I’m committed to other projects today, so if anyone wants to have a go, be my guest and I’ll post it.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e08f0a0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
In comments, Jonn-X wondered:
Dead pixels or new sunspecks (pore-ettes) ?
At first I was pretty sure I was looking at nothing, then I saw the official NOAA bulletin
http://www.swpc.noaa.gov/forecast.html
and the usual phrase, “The visible disk was spotless,” was omitted – typical practice when there’s something there, but too small to be “officially noticed.”
Anybody else see anything?
I do. I know where the dead pixels are, and have labeled them below in the SOHO MDI image. Note that there are two very small sunspecks, possibly soon to be sunspots, emerging on both sides of the equator.

Click for a full sized image
For those that don’t know. The SOHO spacecraft sensor does have some stuck pixels, and these can sometimes be cured in a “bake off” where they heat up the sensor for a few hours.
Our resident official solar physicist, Dr. Leif Svalgarrd will confirm or refute my suspicions on the categorizations of SC23 and SC24 I’m sure. For comparisons, you can also see the SOHO magnetogram.
I’ve included it also below:
UPDATE: The specks are fading, so far no observation agency has assigned a region or counted them that I know of, see the updated SOHO MDI.
SOHO Magnetogram- click for larger image
UPDATED SOHO MDI:

Click for larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c58d268',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterRussian Prime Minister Vladimir Putin arrived Monday on Samoilovsky Island in the western Siberian Arctic to visit a joint Russian-German scientific expedition, Lena-2010.
Lena 2010 is conducting studies on the Russian Arctic permafrost, which is 1.5 km thick at the Samoilovsky Island location and estimated to be 40,000 years old.
The German page of Ria Novosti has a video up (sorry – only in German) which reveals an interesting comment, one not reported in the western media. The video shows Putin visiting the study site, and even helps the scientists bore into the permafrost.
Update: English video click here



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the 1:15 mark of the video, Putin is reported to have asked each scientist the same question:
Is climate change caused by man, or not?
To which the video gives the reply at the 1:24 mark, the reporter says (translated from German):

The scientists, however, could not give an exact answer. The heat wave in Russia resulted from a combination of factors.
Well that’s a long way from meaning “yes”. So much for consensus and the science being done.
When asked about plans in the Arctic, Putin explains at the 1:09 mark:
We are planning to extract natural resources from the ground in Western Siberia. We have to understand the entire eco-system and how it will respond.  
The Russians obviously are getting ready to mine and drill in the region.
Reuters presents an interesting politically correct view of Putin’s visit, and completely ignore the answer the scientists gave to Putin’s question. Read here: http://alertnet.org/thenews/newsdesk/LDE67M11O.htm
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSome of you will recall this pretty face we saw a couple of months ago.
We got to know her here, here and here. The IPCC based solar impact on climate on the “consensus of a single astronomer, who agreed with herself”.
In March, 2008, Dr. Lean made a presentation on global warming, see the following video link:
http://www.youtube.com/watch?v=OOMsQcEN1Bg
It kind of surprises me (and yet it doesn’t) that she doesn’t even mention water vapour as a greenhouse gas. She neglects to mention the PDO as well, looking only at the small timescale El Ninos and La Ninas.
She also completely ignores clouds as a factor effecting the temperature of the earth.
I guess the models are getting simpler and simpler (I mean the climate models).
Okay, it appears this video is not complete, and so I’ll hold back judgement.
Share this...FacebookTwitter "
"Since it was completed 100 years ago, the Panama Canal has been the only shipping route through the land mass of the Americas. Controlled by the US for most of its history, it allows ships to navigate between Pacific and Atlantic oceans without having to sail all the way to the tip of South America, through the infamous Magellan Strait. This makes it one of the world’s most important economic arteries.  But on December 22, work began on a new canal route in the region, in what is one of the most ambitious infrastructure projects of our time. The Interoceanic Grand Canal of Nicaragua will take five years to build over some 278 miles, traversing several major rivers and also Lake Nicaragua, the largest freshwater reserve in Central America. Having been discussed for some 200 years, it is expected to cost $50bn (£32bn), nearly five times the country’s annual GDP.  The project resulted from the Nicaraguan national assembly agreeing a 50-year concession with the Chinese company Hong Kong Nicaragua Development (HKND) – with the potential for a 50-year renewal thereafter. The canal will allow the passage of the world’s largest ships, some of which will be too big for the Panama Canal even after its current expansion project has completed.   The project is supported by Daniel Ortega, the Nicaraguan president. Ortega is the former commander of the Sandinista revolution and head of state in the 1980s, who returned to the helm after the 2006 election and has had a history of difficult relations with the US. His party, the Sandinista National Liberation Front (FSLN), has become an authoritarian political machine that is particularly effective at controlling the population. It is difficult to get a job in government without belonging to the FSLN, and hard to have any power without being close to the Ortega clan.  The president’s son, Laureano Ortega, handled the negotiations with Wang Jing, the chairman/CEO of HKND, on the conditions of the concession. The project is managed directly by the president. The canal budget does not appear in the draft government budget, and the finance minister himself has complained about the lack of information around project costs.  There are also a number of important questions about the project that have never been answered. We know that HKND is a sprawling Beijing-based operation with 15 subsidiaries, but despite strenuous efforts, the Nicaraguan media has so far failed to find out where its leader’s personal fortune comes from. Wang Jing has announced that he has gathered enough investors for the project, but despite several public presentations and insistent questions, the details about project finance remain unknown.  There has been speculation about the links between Wang Jing and the Chinese military. It is seen by some as the main provider of canal project funds, even though the project is officially private and China has made no official statement. The nation’s growing commercial interests in Latin America certainly make it easy to imagine that it might like to have an alternative to the Panama Canal, now run by a Panamanian government agency.  The US has not made any statement about the project either. Different specialists who I have interviewed have variously argued that the US is waiting to see if the project is serious; is too busy elsewhere, notably in the Middle East; and has not taken the measure of the project and its possible consequences. Whatever the case, the silence is surprising.  The environmental lobby has been more forthcoming. One of the most active has been the Centro Alexander von Humboldt, which has released the only environmental impact report to date. It has said that the impact of construction through the lake would be irreversible, affecting the nearly one million people who depend on it for drinking water. Dredging the lake at a depth of more than 30m, displacing millions of tons of sediment, could also radically alter and potentially destroy the biodiversity of the lake. Also controversial is the work that is currently starting on two deep-water ports on either side of the country and a highway to transport equipment and concrete-manufacturing plants. The environmental impact study for these parts of the project, by the English company ERM, is not due to complete until April 2015. The Grand Canal Act meanwhile authorises HKND to expropriate any land in Nicaragua for the needs of the project. It will displace thousands of peasants and indigenous peoples. Yet unlike the Panama project, there was no referendum on whether it should go ahead. In the face of the government’s promises about taking the country “out of poverty” and providing thousands of jobs, farmers’ property rights seem to weigh little. The farmers likely to be affected by land expropriation have been holding demonstrations in numerous localities since last September. They have been told nothing about how and under what conditions this will happen. In El Tule in the western part of the country, farmers have been blocking the Managua-San Carlos highway for several days to prevent the entry of Chinese workers and vehicles of the Nicaraguan army. With the Chinese beginning to install their first settlements, protesters have been complaining about the increasing militarisation that is accompanying the project.  Neither has anything been said about how the country will manage the future economic impact of the canal. I have heard concerns among Nicaraguan economists that the country could become little more than an enclave economy with little in common with the rest of the region. In short, it is long overdue that the spotlight be shone properly on this huge project. The environmental, social and geopolitical implications could cast a shadow over the region for decades to come."
"
Share this...FacebookTwitterThe Science Skeptical Blog here has released some fascinating results from Hans von Storch’s Klimazwiebel blogsite’s survey here. Although the survey is not representative, 578 valid participants from 28 countries took part and interesting results have been produced. Here are the most interesting results:
1. Climate science skeptics have been involved in the topic for a long time. More than 50% of the participants have been following the topic 5 years or more. And skeptics are not born as skeptics. Skeptical Science writes:
More than 75% of the participants started off as neutral or even alarmist. That shows the longer one looks at the climate issue, the more skeptical one becomes with regards to alarmism.
2. Skepticism is deepening. Skeptical Science blog writes:
The last two years have been predominated by the debates on the 2007 IPCC Report, failure in climate negotiations at the international level, and Climategate. These have served to deepen skepticism.
3. Skeptics are shown to be competent. More than 2/3 of the participants have a scientific or technical education.
4. Skeptics like to quote highly competent experts, by a wide margin they like citing  Steven McIntyre and Richard Lindzen.
5. Skeptics characterised three blogs (Realclimate, Skeptical Science, Klimalounge) as alarmist. Meanwhile Klimazwiebel and Lucias Blackboard were viewed as moderate or neutral.
6. Skeptics do read the alarmist blogs mentioned in no.5, and thus are informed of both sides of the issue. Both “neutral” blogs were viewed very positively.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




7. WUWT and CA were the sites skeptics preferred to visit.
8. German skeptics liked visiting EIKE, WUWT and Klimazwiebel the most. [Next time they ought to put NoTricksZone on the ballot, I’d clean all their clocks out. :)].
Now the following are the summary results from the Klimazwiebel website, in case you’re interested:
Q1: The reason for being a skeptic.
2/3 are skeptic because they find that knowledge about the earth’s climate system would be insufficient for legitimating mitigation measures.
Q2: How long engaged/interested in climatic issues?
25% of the respondents became interested after the hot news issue of IPCC 2007. Most layman are no longer than 10 years, and the skeptical scientists are generally engaged for a longer time.
Q.3: Initial opinion upon first contact with climatic issues?
There is a clear warmist (38%)/”neutral” tendency.
Q.4 Which experience had respondents upon having asked their first critical questions?
Two of the six possible answers were clearly on top of the votes:
– The answer was an attempt to promote a political point of view (35%)
– The answer showed limited competence of the other side.
Q.5 How did attendants get to skepticism?
Internet resources was the most ticked choice in this multiple-options question (63%). The hockey stick discussion also represents a major factor. Both of these are clearly less a factor for skeptical climate scientists (Internet 27%); for these scientific publications are an important factor (up to 69%).
Q.6 What is the tendency, related to the past two years?
A vast majority (74%) tends clearly towards skepticism in this time scale. Attendants from web pages as eike-klima-energie.eu and nelson.blogspot as well as oekowatch.org are ticked around 83% (or even 100%).
—————————————————————————————————
Copyright reminder: It is not allowed to reproduce this post without first obtaining permission from No Tricks Zone. Other sites and media may cut and paste max. 25% of the content, and then followed by a link to this site. Let’s all be fair and let credit go where credit is due. Thanks!
Share this...FacebookTwitter "
"A French ski resort has angered ecologists by using a helicopter to move snow from higher up the mountains after exceptionally mild weather left its slopes bare. Officials at Luchon-Superbagnères in the Pyrenees authorised the “exceptional” emergency operation overnight on Friday.  The helicopter spent two hours transporting 50 tonnes of snow to drop on the lower slopes used by beginners and ski schools. Hervé Pounau, the director of the local department council, said the cost of the operation would be recouped many times over by the business that would have been lost to a lack of snow. “It will cost us between €5,000 and €6,000, in the knowledge that over the long term we will get at least 10 times’ return on that investment,” Pounau said in a statement. Keeping the station open safeguarded 50 to 80 jobs, including lift operators, ski school teachers, childminders, ski equipment rental shop staff and restaurant owners, he added. “We’re not going to cover the entire ski station in snow, but without it we would have had to close a huge part of the ski domain, and it’s during the holidays that we have the most activity for beginners and the ski schools,” Pounau said. He admitted it was not “very ecological”, but added: “It’s really exceptional and we won’t be doing it again. This time we didn’t have a choice.” The operation has angered French ecologists. Bastien Ho, the secretary of Europe Écologie Les Verts party, said the snow transfer operation was evidence of an “upside-down world”. “Instead of adapting to global warming we’re going to end up with a double problem: something that costs a lot of energy, that contributes heavily to global warming and that in addition is only for an elite group of people who can afford it. It is the world upside down,” he told French television. The February-March half term holidays in France – known as the “winter sport holidays” – are staggered over four weeks across different regions and are the busiest time of the year for the country’s mountain resorts. Luchon-Superbagnères depends on this period for 60% of its income, but exceptionally mild weather has forced the resort to close all but six of its 28 slopes. It is the first time helicopters have been used to transport snow from higher altitudes to lower resorts in the French Pyrenees, though similar operations have been carried out in the Alps. Local officials said the snow would guarantee that beginners could enjoy the lower slopes and ski instructors could continue with classes for the next two weeks. • This article was amended on 19 February 2020. An earlier version said that the February-March half term holidays in France are staggered over six weeks. This should have said four weeks; while it is correct that three different areas of France take two weeks of holiday each, the weeks overlap. "
"
Share this...FacebookTwitterThe German Freie Welt has a short piece by Fabian Heintel called Lost in Translation. It takes a look at how the sceptical International Climate Science Coalition gets treated by Wikipedia.
The English Wikipedia describes the organisation as:
 …an international association of scientists, economists and energy and policy experts
That sounds respectable and authoritative enough, but too much so for the German language Wikipedia.
In climate dissent-intolerant Germany, sceptics are to be viewed as flake amateurs who have little scientific authority. Here’s how the German Wikipedia describes the International Climate Science Coalition .
…Zusammenschluss von Ökonomen, Politikern und anderen Personen
Translated in English that’s:
association of economists, politicians and other persons. 
Sounds kind of ragtag, doesn’t it?
Here’s a list of the members of the international organisation. You make the call if the German Wikipedia description fits.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEd Caryl likes to research, and a short time ago wrote an essay about phytoplankton, see here. Some readers pointed out a flaw, and so Ed has insisted on posting a correction – as is appropriate in science. Happens to the best of us. (We’re not Penn State or CRU here).
===========================================================================
The Phytoplankton are not Starving
By Ed Caryl
In a comment to the original article, The Phytoplankton are Starving, R. de Hann made the following comment:
 So I have big trouble accepting the loss of plankton for fact.
In regard to the claim of over-fishing and fishing methods (by other reports), which is a serious problem in several places, we see the gap of lost volume filled up with other species very quickly.
We know for example that tuna eat jellyfish and in those waters where the tuna numbers have been reduced the numbers of jellyfish have exploded, compensating for the “loss of mass”.
So one species is quickly replaced by another.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Willis Eschenbach made this comment:
Humans catch and remove about 70 million tonnes of mostly big fish from the ocean annually. But the amount of mass at each trophic level reduces by about 90%. Tuna are four trophic levels up from phytoplankton. This means that for every kilo (pound) of fish that we eat, there are about 10,000 kilos (pounds) of phytoplankton that are supporting that fish.
That means that we are removing something on the order of one ten-thousandth (0.01%) of the nutrients that the phytoplankton that fed those tuna depend on …
However, that’s not all. Much of the phytoplankton goes to feed things that are generally not eaten by humans. As a result, the reduction in phytoplankton nutrients is even smaller than 0.01%. Much of it is never seen by humans in any form.
As a result, your hypothesis (reduction in plankton results from human usage of the required nutrients) fails by a number of orders of magnitude. We simply don’t remove enough nutrients to make a difference.
These comments forced me to re-think my position. I went back to the literature.
The total autotrophic biomass production in the oceans is about 48 billion tons carbon per year. We are currently harvesting about 80 million tons of fish and shellfish per year. The harvest has averaged over 70 million tons per year for the last 30 years, and over 40 million tons per year for the 30 years before that. Over the last 60 years we have harvested about 3.5 billion tons. The total biomass production in the ocean is circulating production. The fish we pull out removes that mass from circulation. So the reduction in phytoplankton nutrients is in fact small. Willis is correct; this doesn’t explain the 40% reduction in phytoplankton. As Willis correctly states, the food web in the oceans has several trophic levels, each about 10% of the level below it.
So thanks to R. de Hann and Willis Eschenbach for their comments.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA Swiss report appearing at www.20min.ch claims that the extraction of groundwater for communities and agriculture is adding massively to global sea level rise.
According to a yet to be published paper by the America Geophysical Union, 2000 cubic km of freshwater are consumed yearly – with 1500 cubic km coming from lakes, ponds, rivers etc., and the remaining 500 cubic kilometers of freshwater being extracted from the ground. According to 20min.ch:
Scientists now have calculated how much of this extracted water returns from where it came: only 3%. The remaining 97% flows into the oceans via evaporation and precipitation.
Of course, the scientists say, this bodes extremely ill for groundwater levels and pose a serious threat to arid regions.
Adding to sea level rise
Moreover the AGU paper claims that the extraction of freshwater is contributing to an annual 0.8 mm rise in sea level, 0r more than 25% of the overall 3.1 mm/yr. It goes without saying that there are uncertainties involved here, as dams act to slow the entry of freshwater into the oceans. But 20min.ch writes:

Scientists are convinced that the relation between groundwater reduction and sea level rise will gain importance. Simply because global water consumption will continue to increase and for that reason groundwater reserves will continue to sink.

Uncertainty always allows alarmists to be convinced of anything they claim. Sceptic Edgar L Gärtner at eigentümlich freicomments in a piece called: Climate Lies: How geo-scientists are abandoning their expertise. Leaky faucets are causing the oceans to overflow, writes:
It can be certain that pumping out groundwater has no measureable impacts on sea levels. But perhaps US scientists are tying it to climate change in order to bring attention to the risks of exhausting groundwater reserves.
Drawing attention to potential groundwater depletion problems is legitimate. But claiming it is leading to sea level rise sounds absurd.
Share this...FacebookTwitter "
"The government could ban the sale of petrol and diesel cars in 2032, three years earlier than previously suggested, the transport secretary has said. A consultation launched last week suggested all cars with internal combustion engines could be banned from 2035 but Grant Shapps told BBC radio on Wednesday the ban could come within 12 years. The ban would happen by 2035 – or even 2032, subject to consultation, he said. The comments will add to the concerns of the car industry, which has criticised the government for “moving the goalposts” with its earlier announcement that the limits could come by 2035, from an initial expectation of a 2040 ban. Mike Hawes, the chief executive of the Society of Motor Manufacturers and Traders (SMMT), said it was a “date without a plan”. Previous proposals also excluded some hybrid cars, which combine internal combustion engines with battery electric power. Hybrids are seen by carmakers as a key way of reducing the carbon footprint of their vehicles, and multiple companies such as Toyota and the Mercedes-Benz owner, Daimler, have bet heavily on hybrids to avoid steep EU fines. Shapps and Andrea Leadsom, the business secretary, will meet the SMMT on Thursday, with the lobby group hoping to persuade the government that the industry will need more help to achieve the targets. Among the main concerns for carmakers are incentives such as grants for electric cars as well as building national infrastructure capable of charging millions of vehicles. During the third quarter of 2019, the Department for Transport recorded 22,596 registrations of ultra-low emission vehicles, which includes electric cars – 39% more than the same period in the previous year. However, that still represented only 3.1% of new car registrations during the period. European manufacturers are launching a wave of electric-only models to try to reduce the emissions of the cars they sell and meet the EU targets. Volkswagen, the world’s largest carmaker by volume, plans to sell 1m electric cars by the end of 2023."
"Amid the many hellish images and stories that have emerged from Australia’s apocalyptic wildfire season, the fate of the country’s wine business will have come some way down the list of most observers’ concerns. That’s only as it should be. When we are talking about the loss of human and animal life and habitats – when we are seemingly witnessing extreme predictions for a distant post-climate emergency future happening right now – fretting about the viability of a branch of the booze business can seem trivial.  As the country counts the costs of the conflagration, however, concerns about the long-term viability of what has become one of its most significant exports have come into focus. According to Wine Australia, about 1% (or 1,500 hectares) of vines were in the heart of the fire-affected zones, with the worst-hit region being the Adelaide Hills. That’s the site of some of Australia’s most exciting cooler-climate wines, and it lost as much as 30% of its vineyards, including Henschke’s renowned Lenswood vineyard, and land and buildings belonging to the cult star producer, Vinteloper. Similar soul-searching has been taking place in California, where October’s Kincade fire burned through Sonoma County, the latest in a series of increasingly devastating wildfires to engulf California wine country. In the past two years, we’ve also seen prime vineyard land in Chile, South Africa, the south of France, Portugal, Spain and Greece go up in flames with more intensity and regularity than before. It’s not just the expensive loss of vines and infrastructure. There are after-effects stored in any grapes that might be salvaged at vintage time, even on vines that were some distance from the frontline. Once they’ve been through the veraison process (when the grapes change colour), the closer the grapes are to being ready for harvest the more they are prone to “smoke taint”, with the absorption of smoke particles leading to flavours akin to a chemical fire in the finished wine. This problem has been the subject of considerable research in Australia and California – although so far, it seems, the only action available is to ameliorate rather than eliminate the aromas. By allowing less contact between juice and skins, by fining and filtering, and by adding distracting flavours such as oak and tannins, you can produce a wine where the effect of the smoke taint is dialled down. None of those options are attractive for winemakers, and many simply take the expensive decision to dump the crop. No wonder they are weighing up whether wildfires have gone beyond occupational hazard to become a threat to their viability. For now, smoke-tainted wines are relatively rare. And, as with many symptoms of the climate emergency, there’s still just about time to act rather than merely accepting that burnt has to be the taste of the new normal. Henschke Giles Pinot Noir Adelaide Hills, Australia 2017 (from £31.80 vinvm.co.uk; ozwines.co.uk)The Henschkes are best known for superlative shiraz from the Eden Valley, but, before it was badly damaged in the 2019 bushfires, their Lenswood vineyard had earned a reputation for luxuriously silky, bright red berry-fruited pinot noir. Vinteloper If Life Gives You Lagrein Langhorne Creek, Australia 2017 (from £14.80, fieldandfawcett.co.uk; thewhiskyexchange.com)Vinteloper’s David Bowley had his entire 30-hectare Adelaide Hills property destroyed by bushfire. Buying a bottle of this thirst-quenchingly tangy red from a plot of the north Italian variety lagrein in Langhorne Creek is a great way to help him stay in business. Casa de Mouraz Branco Dão, Portugal 2017 (£17, highburyvintners.co.uk)The team at Casa de Mouraz, one of the best producers in central Portugal’s Dão region, had only just harvested the grapes for this typically luminous, textured, pithy dry white before wildfire swept through its vineyards and winery in October 2017. Mayacamas Chardonnay Mt Veeder, Napa Valley, California, USA 2017 (£55, robersonwine.com)California has had to get used to wildfires: last year, the Kincade fire in Sonoma; in 2017, fires swept through Napa destroying buildings at Mayacamas Vineyards, but leaving vines, including those that produce one of the world’s great chardonnays, intact. Château de Caraguilhes Corbières, France 2017 (£10.98, Waitrose)In 50 years’ time, will winemakers still be able to produce such spicy, robust, brambly fruited reds as this from southern France’s Corbières, a region that, having already endured fire damage in 2016, lost 300 hectares of vineyard in wildfires in July last year? Vergelegen Premium Chardonnay Stellenbosch, South Africa 2017 (from £10.89, rannochscott.co.uk; ampswinemerchants.co.uk; robertsandspeight.co.uk)Another wildfire survivor: as much as 40% of the 300-year-old Cape estate Vergelegen was damaged in South Africa’s last big conflagration in 2017. The chardonnay from that vintage was not affected: it’s as rich yet balanced as ever. 
This article contains affiliate links, which means we may earn a small commission if a reader clicks through and
makes a purchase. All our journalism is independent and is in no way influenced by any advertiser or commercial initiative.
By clicking on an affiliate link, you accept that third-party cookies will be set.
More information.
"
"
Share this...FacebookTwitterlist Game Indonesia buat Permainan Judi Online! waktu kamu mencari trik judi yg lebih baru. Permainan judi Indonesia pantas dipertimbangkan buat kamu. Berikut yaitu sekian banyak trick kamu sanggup menikmati perjudian di negeri ini bersama sekian banyak trick inovatif.
bersama perkembangan tehnologi ketika ini permainan game online jadi ternama. lantaran dgn main dengan cara online menciptakan kamu lebih nyaman & enteng utk memainkannya. Terutama dalam permainan judi online, permainan game online paling baik yg mampu menghibur kamu. Bahkan permainan judi online serta sanggup berikan kamu pendapatan penambahan.
Dalam permainan judi online ketika ini amat sangat beraneka ragam yg dapat kamu mainkan. Berikut ini ada sekian banyak permainan game judi online yg dapat kamu mainkan :
Bandarq
Ini ialah permainan judi Indonesia yg terkenal & serta dinamakan bersama permainan judi card. dalam permainan bandarq dibutuhkan enam card domino ganda dalam set dua puluh delapan. Dalam biasanya kasus, pemain memakai card mungil namun dikala mereka aus, mereka dibuang. Pemain mesti membayar jumlah yg terus sebelum mulai sejak main-main. Mereka mendapati tiga card domino buat menolong mereka disaat mereka dalam perbaikan.
kamu bebas bertaruh, lipat, naik atau dingin sesudah card kamu dievaluasi. bila tak ada petaruh pada awal mulanya, kamu bakal diizinkan utk bertaruh. tidak cuma itu, apabila permainan cuma melibatkan kamu adalah( cuma satu petaruh), itu berhenti sekaligus. kamu memperoleh pot dalam kasus itu tidak dengan mesti menunjukkan card kamu. Ada dua putaran yg dikenakan batas dalam permainan. Babak ke-2 mempunyai batas yg lebih tinggi.
tidak cuma itu, card ditempatkan berpasangan di mana seluruhnya pasangan ditambahkan. Pasangan paling atas buat tiap-tiap kontes merupakan 9 sedangkan pasangan kedua dianggap sbg yg terakhir. Pemain mengaplikasikan beraneka cara juga matematika buat menang. satu orang pemain diyakini mempunyai qiu sesudah tiga ganda awal.
Poker Online di Indonesia
Poker online kurang dibatasi di Indonesia di bandingkan dgn permainan tradisional. Pemain bisa membawa pertolongan dari web website poker internasional agung utk mencapai maksud mereka. terkecuali itu, mereka sedia di negara-negara dgn yurisdiksi pemerintah Indonesia yg lebih sedikit.
Poker internasional dgn pemain Indonesia tak diizinkan. namun para pemain bebas utk menikmati beragam situasi seperti permainan duit nyata & pertandingan. dgn begitu bermacam permainan dimainkan dengan cara online yg terjangkau.
Dominoqq
cocok pengamatan, perjudian online di negeri ini konsisten meningkat sejauh itu jadi sumber pendapatan bagi tidak sedikit orang. Dominoqq terdiri dari sekian banyak permainan termasuk juga Judi Bola (perjudian bola), permainan tradisional, permainan video poker & sebagainya.
Apa yg mesti kamu tonton dalam Permainan Judi Online?
https://semogaqq.net/
Mengamankan Uang
seluruhnya pemain sesudah membuahkan duit paling tidak sedikit dalam masa terpendek. telah terang bahwa transaksi aman mempermudah pemain buat menjaga duit mereka konsisten aman. pula masalah yg terkait dgn setoran & penarikan dihindari dgn ini.
tiap-tiap pemain mendapati akun pribadi di mana dirinya mampu mengecek penghasilannya. pula disediakan bermacam akun deposito maka para pemain sanggup bertransaksi duit dari akun yg sah mereka.
tapi dalam permainan judi online seperti bandarq, dominoqq & poker kamu butuh mengetahui & pilih web judi online terpercaya buat menopang jalannya permainan yg kamu mainkan diwaktu ini.
trik utk meraih website judi terpercaya kamu mesti menonton profil web tersebut. Atau kamu pun sanggup menyaksikan anggota yg bergabung di dalam website tersebut.
Nah, itulah rekomendasi dari artikel kami berkenaan list game judi online indonesia & trick buat pilih website. mudah-mudahan dgn adanya info ini bisa menopang kamu dalam permainan nantinya. Selamat main-main & mudah-mudahan berhasil!
Share this...FacebookTwitter "
"

Now that Newt Gingrich has vaulted into the lead in the Republican presidential stakes, he’s going to be seeing a lot more of this:





Calling it the “dumbest thing” he has done in recent years isn’t enough. This is not going away.



But, Newt can turn his pas‐​de‐​deux on the loveseat with Nancy into an opportunity to promote a consistent and electable political message.



Because he will be asked about this at every turn, Newt will have the air time and the opportunity to tell the truth on climate change — based upon real science — and to interface it with a limited‐​government philosophy. The two are easy to do.



Start with the obvious. Instead of waffling on the subject, he could just point out that earth’s surface temperature is about 1°C warmer than it was a century ago. There were two periods of warming, the first from about 1910 through 1945, and the second from the mid‐​1970s to the late‐​1990s. They were both roughly the same magnitude. Because the first one was long before we put the majority of fossil carbon into the atmosphere, Newt can say — with scientific authority — that the magnitude of ‘natural’ climate change is likely to be at least as large as what humans have done.



Then Newt should proceed to the future. It’s not the heat, it’s the sensitivity. How much it will warm in the future is a function of how sensitive the atmosphere is to doubling atmospheric carbon dioxide. There are a number of independent arguments now coming together showing that this value may have been overestimated. The reference for the most recent is a November issue of _Science_.



Next, a little refreshing honesty: in 2009, the House of Representatives passed a cap‐​and‐​trade bill (which the Senate did not) commanding that the average american — 38 years from now — be allowed the carbon dioxide emissions of the average citizen in 1867. Prior to its passage, Newt was undeniably for such a system. Newt could call that that the “second dumbest thing” he has done and be finished with it.



After setting the record straight, how about interjecting his own political philosophy? When asked what to do about global warming, he should give the honest answer: if the atmosphere indeed is not as sensitive to carbon dioxide changes as previously thought, the correct response is _nothing._



That’s because “nothing” really means something. Newt should channel his historian. When markets are free, capital supports innovation more efficiently than when they aren’t. Think about the remarkable changes in energy and technology in the last 100 years. Isn’t it rather obvious that the same will occur in the next century, if only we don’t hinder capital development? Newt might even use the catchy saw (first sloganed by Northern Illinois Gas in 1972) _the future belongs to the efficient_ , and that there are impressive market forces that advantage those who produce things efficiently or produce efficient things.



When the doomsayers say Newt is believing in a _Deus ex machine_ to dramatically cut carbon dioxide emissions, he might point out that those were the same folks who, only a very few years ago, told us we were running out of natural gas. Innovation and capital revolutionized drilling and fracturing shale, and we now know we have literally hundreds of years of it under our feet. There are a lot more voters and delegates around the country who will benefit from the shale revolution than there are in ethanol‐​addicted Iowa.



Thus comes Newt’s opportunity to back down his support of corn‐​based ethanol. It’s a loser. When it powers a car, its total life cycle results in more carbon dioxide emissions than simply sticking with gasoline.



Yes, if he ran away from ethanol before the Iowa primary, Romney might win. But Newt would be called courageous and the voters in the rest of the nation will notice.



Finally, Newt could say that he’s sick of the government subsidizing any form of energy or transportation. Sell GM and don’t look back. Stop subsidizing the Volt, solar energy, wind power, gas, coal, all things energy, and let people keep their money and invest. Stop tilting the federal purse at windmills.



Newt can do these things and win. Otherwise, he’s could spend fall of 2012 on that darned couch.
"
"

Since its inception in 1908, the Federal Bureau of Investigation has kept itself busy surveilling those who hold controversial political views — from Christian pacifists in World War I, to Martin Luther King Jr. in the 1950s, to Arab/​Muslim Americans in the 1990s. “American Big Brother,” a new project from the Cato Institute, features an interactive online timeline of these surveillance projects over the past 100 years. “The theme that emerges clearly from the timeline’s episodes is that in many of these cases, federal surveillance and political repression were directed most forcefully at individuals and organizations that challenged the prevailing political paradigm on the issue at hand,” wrote Cato policy analyst Patrick G. Eddington. And in many cases, the individuals and organizations subjected to this warrantless surveillance suffered irreparable damage to their personal and professional lives. The timeline, found at www​.cato​.org/​a​m​e​r​i​c​a​n​-​b​i​g​-​b​r​other, already features dozens of stories of surveillance, and is an ongoing project which will be updated regularly with archival research and new developments in the news.



 **The Battle for Free Expression, Continued**  
 _The Tyranny of Silence_ , the story of how one cartoon ignited a global debate over free speech, was first published two years ago. Since then, the battle for free speech has raged on—from the tragic _Charlie Hebdo_ attacks in Paris, to the fight for free speech on public campuses. Now available for the first time in paperback, Flemming Rose’s book, which The Economist dubbed one of the best books of 2014, recounts his experience publishing cartoons of the prophet Muhammad in Danish newspaper _Jyllands‐​Posten_ in 2005, which quickly exploded into a global controversy known as the “Cartoon Crisis.” Rose bravely defended the decision to print the 12 drawings, even as Muslims around the world protested, Danish embassies came under attack, and newspaper and magazine editors were arrested. Rose tells his gripping personal story of the events that unfolded. “What do you do when suddenly the entire world is on your back?” Rose recalls. The paperback edition includes a new afterword, in which Rose reflects on the _Charlie Hebdo_ attack and the state of free speech in both Europe and America. The United States, he writes, is “afflicted with identity politics and grievance fundamentalism,” while in Europe, “it looks like freedom of speech will be sacrificed on the altar of cultural, religious, and ethnic diversity.”



 **The Economics of Environmentalism**  
More than 10 years after its original publication, the second edition of Richard Stroup’s invaluable _Eco‐​nomics: What Everyone Should Know about Economics and the Environment_ provides a thoroughly updated guide to environmental problems from a free market perspective. As in the first edition, Stroup offers a concise primer of how economic principles shed light on environmental issues, and why so many environmental laws fail. But Stroup also adds new chapters, including a brief overview of the history of environmentalism in the United States, the “constantly changing view of our environment and how to protect it,” and an examination of the most controversial environmental issue of today—climate change. “Although the book is a small one, I have attempted to identify in it the core tenets of free‐​market approaches to environmental protection and to make clear why these approaches are worth serious consideration,” writes Stroup. “The weight of opinion tends to push toward a greater role for government, even though that role is often misused and sometimes has unfortunate consequences. Economics shows us the wisdom of considering a greater role for market solutions.”
"
nan
"

Mayan ruins in Guatemala.

This is an email I recently received from statistician Dr. Richard Mackey who writes:
The following appeared on Gore’s blog of Nov 19, 2008:
Looking Back to Look Forward
Looking  Back to Look Forward November 19, 2008 : 3:04 PM

A new study suggests the  Mayan civilization might have collapsed due to environmental  disasters:
These models suggest that as ecosystems were destroyed  by mismanagement or were transformed by global climatic shifts,  the depletion of agricultural and wild foods eventually contributed to  the failure of the Maya sociopolitical system,’ writes  environmental archaeologist Kitty Emery of the Florida Museum of Natural  History in the current Human Ecology journal. 
As we move  towards solving the climate crisis, we need to remember the consequences to  civilizations that refused to take environmental concerns  seriously.
If you haven’t read already read it, take a look at  Jared Diamond’s book, Collapse.”
This is a most curious  reference.
It means that Gore is advocating the abandonment of the IPCC  doctrine and barracking for the study and understanding of climate  dynamics that ignores totally the IPCC/AWG doctrine and focuses on all  the other variables, especially how climate dynamics are driven  by atmospheric/oceanic oscillations, the natural internal dynamics of  the climate system and the role of the Sun in climate dynamics.
Brian  Fagan in Floods, Famines and Emperors  El Nino and the fate of civilisations   Basic Books 1999, shows that the Maya collapse, whilst having complex  political, sociological, technological and ecological factors, was largely  driven by the natural atmospheric/oceanic oscillations of ENSO and NAO.  The  book is one of three by Brian Fagan, Prof of Anthropology UC Santa Barbara,  that documents how natural climate variations, ultimately driven by solar  activity, have given rise to the catastrophic collapse of civilisations.  The  book has a chapter on the Mayan civilisation which collapsed around 800  to 900 AD.
Here are some quotes from his book:
“The “Classic  Maya collapse” is one of the great controversies of
archaeology, but there is  little doubt that droughts, fuelled in part
by El Nino, played an important  role.”
“The droughts that afflicted the Maya in the eighth and  ninth
centuries resulted from complex, still little understood  atmosphere-
ocean interactions, including El Nino events and major decadal  shifts
in the North Atlantic Oscillation, as well as two or three  decade-long
variations in rainfall over many centuries.”
“Why did the  Maya civilisation suddenly come apart?  Everyone who
studies the Classic Maya  collapse agrees that it was brought on by a
combination of ecological,  political, and sociological factors.”
“When the great droughts of the  eighth and ninth centuries came, Maya
civilisation everywhere was under  increasing stress.”
“The drought was the final straw.”
“The  collapse did not come without turmoil and war.”
Brian Fagan describes  how the ruling class (the kings had divine powers, they were also shamans and  there was a vast aristocracy and their fellow-travellers that the tightly  regulated workers toiled to maintain) encouraged population growth beyond  what the land could carry; how the rulers enforced rigid farming practices  which were supposed to increase food production and the ruler’s incomes but  had the effect of undermining farm productivity and diminishing  the quality of the poor soils of the area.  When there were heavy  rains the soil was washed away.  In times of drought the soil blew  away.
More quotes from Brian Fagan:
“The Maya collapse is a  cautionary tale in the dangers of using
technology and people power to expand  the carrying capacity of
tropical environments.”
“Atmospheric  circulation changes far from the Maya homeland delivered
the coup de grace to  rulers no longer able to control their own
destinies because they had  exhausted their environmental options in an
endless quest for power and  prestige.”
Gore says that we should use our understanding of the Maya  collapse help us solve the climate crisis, noting that “we need to remember  the consequences to civilizations that refused to take  environmental
concerns seriously”.
Given what we know of the Maya  collapse, what is Gore really saying?
He is saying that we should take  all the IPCC/AWG publications and related papers to the tip and bury them  there and put all our efforts into the study and understanding of the reasons  for climate dynamics that address every theory except that of IPCC/AWG  doctrine.
Specifically, we should understand as well as we can how  climate dynamics are driven by atmospheric/oceanic oscillations, the  natural internal dynamics of the climate system and the role of the Sun  in climate dynamics.
In an overview of his work Brian Fagan  concluded:  “The whole course of civilisation … may be seen as a process of  trading up on the scale of vulnerability”.  (Fagan (2004, page  xv)).
We are now, as a global community, very high up on that  scale.
Allow me to quote a little from my Rhodes Fairbridge paper  because of its relevance to Brian Fagan’s work and what Gore is really trying  to say, but can’t quite find the right words.
(My paper is here: http://www.griffith.edu.au/conference/ics2007/pdf/ICS176.pdf ).
“In his many publications (for example, NORTH (2005)), Douglass  North stresses that if the issues with which we are concerned, such  as global warming and the global commons, belong in a world of  continuous change (that is, a non-ergodic world), then we face a set of  problems that become exceedingly complex.  North stresses that our capacity  to deal effectively with uncertainty is essential to our succeeding in  a
non-ergodic world.  History shows that regional effects of  climate change are highly variable and that the pattern of change is  highly variable.  An extremely cold (or hot) year can be followed  by extremely hot (or cold) year.  Warming and cooling will be  beneficial for some regions and catastrophic for others.  Brian Fagan  has documented in detail relationships between the large-scale  and
generally periodic changes in climate and the rise and fall  of civilisations, cultures and societies since the dawn of history.   The thesis to which Rhodes Fairbridge devoted much of his life is that  the
sun, through its relationships with the solar system, is  largely responsible for these changes and that we are now on the cusp of  one of the major changes that feature in the planet’s history.   As
Douglass North showed, the main responsibility of governments  in managing the impact of the potentially catastrophic events that  arise in a non-ergodic world is to mange society’s response to them so as  to
enable the society to adapt as efficiently as possible to them.
Amongst  other things, this would mean being better able to anticipate and manage our  response to climate change, to minimise suffering and maximise benefits and  the efficiency of our adaptation to a climate that is ever-changing –  sometimes catastrophically – but generally predictable within bounds of  uncertainty that statisticians can estimate.  At the very least, this  requires that the scientific community acts on the wise counsel of Rhodes W  Fairbridge and presents governments with advice that has regard to the entire  field of planetary-lunar-solar dynamics, including gravitational  dynamics.
This field has to be understood so that the dynamics of  terrestrial climate can be understood.
References:
North, D. C.,  2005. Understanding the Process of Economic Change
Princeton University  Press.
Fagan, B., 2004.  The Long Summer.  How Climate Changed  Civilization.
Basic Books.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a885ba4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Boris Johnson must put the climate crisis at the top of his government’s agenda if crunch UN talks this year are to be a success, leading international figures have told the Guardian. Alok Sharma was appointed on Thursday as the business secretary and president of Cop26, the UN talks on the climate crisis to be held this November in Glasgow. Some climate experts are concerned that he won’t be able to stand up to governments reluctant to make strong commitments to cut greenhouse gases, while at the same time supporting British businesses struggling in the turmoil of Brexit.  Lord Stern, former World Bank chief economist, said he had confidence in Sharma, having worked with him closely on international development issues. “[Sharma] has worked with developing countries, and has shown he can build up the relationships he needs,” he said. “He has the knowledge and the ability to form the relationships that are needed with developing countries that will be essential to the success of Cop26.” The appointment of Sharma has encouraged civil servants who were concerned that Cop26 had slipped down the government’s agenda. Insiders said the appointment of Sharma – a well-regarded secretary of state for international development – would kickstart the process of international diplomacy needed to make the Cop26 climate conference a success. Fatih Birol, executive director of the International Energy Agency, said: “This is an excellent choice and we at the IEA are looking forward to working together with minister Sharma and his team to support the Cop26 process in order to reach a successful outcome in Glasgow for our international climate goals. We will work together to make sure 2019 is remembered as the year of peak emissions so that the 2020s become the decade that global emissions decline quickly.” Campaigners are concerned, however, that the dual role may get in the way of a successful outcome. John Sauven, executive director of Greenpeace UK, said: “The new business secretary has a herculean task on his hands. Not only has he got to get the UK delivering on its carbon budgets, but he’s also got to get the world behind more ambitious climate action. “At home, he needs to unblock the expansion of on- and offshore wind currently sandwiched between various government departments and backbench MPs, as well as tackling carbon emission from 30m draughty buildings. Globally, he’s got to deal with leaving the EU and signing international trade deals while also cajoling countries to get behind solving the climate emergency. This will only work if Boris Johnson steps in and puts the full weight of No 10 behind it.” Aaron Kiely, climate campaigner at Friends of the Earth, said: “It’s hard to be optimistic about the UK’s presidency of the UN climate conference, when the man given the job is expected to do it as a side project. Leading international talks on the climate crisis is easily the most important role in government right now, yet Alok Sharma is expected to take it on at the same time as being business secretary. If the presidency is being treated as a part time gig then what next? Perhaps the conference venue will be expected to share space with a wedding fair.” He added: “The government really needs to start taking this seriously. At the very least this means giving someone the organisation role as a full-time job, and proper cooperation with the Scottish government.”"
"

ASOS station in Greenville, Alabama, Mac Crenshaw Memorial Airport
Photo by: Howard Wiseman
You’d think either the FAA or NOAA would get this cleaned up before surfcaestations.org volunteers have the opportunity to get such pictures. So, in the spirit of the “FAIL blog” I thought I’d offer this.
To be fair, this looks like a junker placed there rather than a aviation accident. I’ve seen many such aircraft at small airports that become forgotten derelicts like this.
Here is a view sans aircraft.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ab7e661',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Despite rising federal deficits, Congress is set to pass another budget‐​busting spending bill. This time it is a $19 billion package of disaster‐​related subsidies.   
  
  
The _Washington Post_ reports “taxpayer spending on U.S. disaster fund explodes.” It documents increases in disaster spending by the Federal Emergency Management Agency (FEMA). In a typical recent year, “spending on the federal disaster relief fund is almost 10 times higher than it was three decades ago, even after adjusting for inflation.”   
  
  
The story identifies two causes of the spending increases: climate change and population growth in disaster‐​prone areas. But it ignored perhaps the most important cause: increased federal intervention in the sorts of emergencies that used to be handled by the states, as I discuss here.   
  
  
The _Post_ is correct that more Americans are moving into disaster‐​prone areas:   




Many more Americans have moved into harm’s way, with growth exploding in the Gulf Coast region and along the Continental Divide, where tornadoes frequently occur, according to a study on the “expanding bull’s eye effect” by Stephen M. Strader of Villanova University and Walker S. Ashley of Northern Illinois University.   
  
  
Since 1970, 35 million more people and their homes have moved to coastal shoreline “in the direct path of potentially devastating storm surges,” the researchers found, a 40 percent increase.   
  
  
“We’ve put more stuff in the wrong place the wrong way,” said W. Craig Fugate, a former FEMA administrator under President Barack Obama. “We’ve got a lot more stuff — bigger houses, multiple cars, more people — in high‐​hazard areas.”



More people are also living in fire‐​prone areas of California.   
  
  
The _Post_ does not explore an important reason why Americans are moving into these areas: government subsidies. Federal subsidies for flood insurance, flood control structures, beach replenishment, and disaster rebuilding have encouraged development in coastal areas, as I discuss here. Meanwhile, state policies have contributed to building in California’s fire‐​prone areas.   
  
  
American governments are not alone in pursuing policies that increase disaster hazards. A World Bank / United Nations study identified such policies in numerous countries and discussed market‐​based reforms to mitigate risks.   
  
  
In the United States, federalism is supposed to undergird our system of handling disasters, particularly natural disasters. Under the 1988 Stafford Act, the federal government is supposed to get involved in disasters only if they are of “such severity and magnitude that effective response is beyond the capabilities of the state and the affected local governments.”   
  
  
However, presidents and congresses have increasingly ignored this limit. The number of presidential disaster declarations has soared and the costs of disaster bills have increased as politicians shoe‐​horn subsidies unrelated to immediate emergency response into bills.   
  
  
Growing federal intervention is undermining the role of the states and private institutions in handling disasters. This intervention stems from politics not practical benefits. State and local governments and the private sector are better positioned to handle most disaster response. Also, states, cities, and private utilities aid each other during disasters.   
  
  
Rising FEMA spending is not a good metric for measuring the severity of natural disasters striking the United States. Rather, it reflects growing populations living in risky areas and growing disregard for federalism in disaster‐​related response and rebuilding.   



"
"
Newly discovered evidence that polar bears, CO2, climate change, and the sun are intimately connected in ways never envisioned.
No wonder the sun seems to be slowing down.

With apologies to the French, and everybody else for that matter.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bfbf49f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Remember the big, somewhat bulky Mercedes Benz cars of the early 1990s? State-of-the-art at that time, they left their competitors behind on Germany’s autobahn. Mercedes is still here today, but the autobahns are not what they were: Germans fear that two decades of neglect after reunification might have seriously damaged the country’s once superb infrastructure.  Similarly, Germany once possessed one of the world’s most well engineered power systems. Yet in 2014 the country urgently needs new power grids – or “stromautobahnen” (electricity highways) –- to keep supply stable in the face of growing proportions of volatile renewable energy. There are concerns over whether it can keep pace with the phase-out of nuclear power, a decision taken in the wake of the Fukushima disaster three years ago. Nuclear helps to stabilise demand on the grid but eight out of 17 reactors have now been shut down, with the rest due to close by 2022.  Electricity generation from renewables has risen lately to new unprecedented highs, overtaking lignite coal to become the number one single source in the nine months to September 2014. This is part of the government’s policy of “energiewende” – the rapid transformation of the power system towards green energy. The stated target is for renewables to provide 55% to 60% of German electricity by 2035 – among the most ambitious in the world.  The changing German energy mix But there are doubts as to whether it is realistic, and whether it might turn out to be harmful for Germany’s industry. One indicator is that electricity tariffs have almost doubled since 2000, for example. Meanwhile CO2 emissions have not decreased, but actually increased over the last few years because coal-fired power has been on the rise while nuclear wanes.  It is also no secret that Germany’s energy companies are not doing well, as pointed out by the chief executive of French rival EDF recently in a general attack on the state of the German energy sector. Both RWE and E.ON have had to accept huge write-downs in the value of their domestic nuclear and fossil-fuel-powered assets. Is Germany thus in the process of sacrificing the once famous reliability of its power sector for little more than problems? And if so, what might be the way forward? Back to nuclear energy?  The disastrous state of Germany’s repositories for radioactive waste such as Brunsbüttel and Asse in the north are not encouraging. And public support for both renewables and nuclear phase-out is still great. But further afield the German decision to live without nuclear has perplexed many observers. To the great amusement of the participants at a business conference in Berlin in 2010, Vladimir Putin offered the Germans firewood from Siberia to heat the country once its nuclear power stations had been switched off. Clearly the energiewende has not exactly impressed the Russians.  How about others? Japan, which has more reason to back the energiewende than almost anyone, is relaunching its nuclear programme. The United States is happy with its shale gas, and at home in Europe, not many countries have joined the Germans either. On top of this, Günther Oettinger, Germany’s accomplished top official in the European Commission, is no longer responsible for energy. It would seem then that Germany’s energy policy has not many influential supporters left.  There remains the idea to sell German-engineered green tech to the world. Siemens is a world leader in renewable technology, for example. Yet the technical level of difficulty is not necessarily high enough to serve as the basis of a high-tech industry in which the Germans can have a competitive advantage in the long term – it is too easy for other players to fabricate equivalent structures. This was illustrated by the Desertec project to build huge solar parks in the North African desert. It collapsed, where Siemens had already walked away a few months earlier. It wasn’t seen as a big deal for the Germans, since many components for the project may have come from China anyway. If the same thing happened with the production of wind turbines, however, it would be a serious setback to the idea of Germany relying on green tech as a base for future industrial production because it would be undercut by other countries who could make the same products more cheaply.   But most things in life have two sides. The great hope is that German engineering will once more surprise the world with technical solutions for difficult problems in difficult times. The central issue is the power grid. One of the key challenges is to find an electricity storage mechanism that can allow a quick and flexible response to changes in the amount of wind and solar power – currently the availability drops when the wind stops blowing or the sun stops shining.  Another big challenge is persuading consumers to react more flexibly to these ups and downs, most likely by altering the price of power accordingly. Solve these problems and the fact that renewables have no fuel costs would make them a serious alternative to other energy sources.  The power grid and its components, not Putin’s firewood, could become the “wunderwaffe” (“wonder weapon”) able to solve Germany’s energy problems. But these new techniques need to be affordable, otherwise the effect will be negligible and only perpetuate the existing problems.  To end on a positive note, there is one much less debated issue around the country’s nuclear phase-out programme. If Germany sticks with its chosen course, the development of third-generation nuclear plants will be in the hands of countries such as France, Britain and Poland. But German industry is famous for its ability to occupy profitable technological niches. Given that there are many nuclear power stations to be dismantled, and a lot of nuclear waste already rotting in the country, the Germans will have to find solutions to deal with it. If they still are the engineers they used to be, they will. This could become a very lucrative thing to then sell to the rest of the world."
"
Many people have pointed me to this story, I wanted to read about it a bit before posting it.  Almost two years ago, when this blog was in its very first month, I posted this story on the puzzling leveling off of global methane concentrations. FYI Methane has a “global warming potential” (GWP) 23-25 times that of CO2.
CDIAC has an interesting set of graphs on methane, the first of which shows that indeed global concentrations of CH4 through 2004 have leveled off:

This one on latitude -vs- concentration would surely seem to point to anthropogenic sources of CH4:

So here is yet another addition to the puzzle, which seems to point in the opposite direction:
MIT scientists baffled by global warming theory, contradicts scientific data 
From: TG Daily By Rick C. Hodgin
Boston (MA) – Scientists at MIT have recorded a nearly simultaneous world-wide increase in methane levels. This is the first increase in ten years, and what baffles science is that this data contradicts theories stating man is the primary source of increase for this greenhouse gas. It takes about one full year for gases generated in the highly industrial northern hemisphere to cycle through and reach the southern hemisphere. However, since all worldwide levels rose simultaneously throughout the same year, it is now believed this may be part of a natural cycle in mother nature – and not the direct result of man’s contributions.
Methane – powerful greenhouse gas
The two lead authors of a paper published in this week’s Geophysical Review Letters, Matthew Rigby and Ronald Prinn, the TEPCO Professor of Atmospheric Chemistry in MIT’s Department of Earth, Atmospheric and Planetary Science, state that as a result of the increase, several million tons of new methane is present in the atmosphere.

Methane accounts for roughly one-fifth of greenhouse gases in the atmosphere, though its effect is 25x greater than that of carbon dioxide. Its impact on global warming comes from the reflection of the sun’s light back to the Earth (like a greenhouse). Methane is typically broken down in the atmosphere by the free radical hydroxyl (OH), a naturally occuring process. This atmospheric cleanser has been shown to adjust itself up and down periodically, and is believed to account for the lack of increases in methane levels in Earth’s atmosphere over the past ten years despite notable simultaneous increases by man.
More study
Prinn has said, “The next step will be to study [these changes] using a very high-resolution atmospheric circulation model and additional measurements from other networks. The key thing is to better determine the relative roles of increased methane emission versus [an increase] in the rate of removal. Apparently we have a mix of the two, but we want to know how much of each [is responsible for the overall increase].”
The primary concern now is that 2007 is long over. While the collected data from that time period reflects a simultaneous world-wide increase in emissions, observing atmospheric trends now is like observing the healthy horse running through the paddock a year after it overcame some mystery illness. Where does one even begin? And how relevant are any of the data findings at this late date? Looking back over 2007 data as it was captured may prove as ineffective if the data does not support the high resolution details such a study requires.
One thing does seem very clear, however; science is only beginning to get a handle on the big picture of global warming. Findings like these tell us it’s too early to know for sure if man’s impact is affecting things at the political cry of “alarming rates.” We may simply be going through another natural cycle of warmer and colder times – one that’s been observed through a scientific analysis of the Earth to be naturally occuring for hundreds of thousands of years.
Project funding
Rigby and Prinn carried out this study with help from researchers at Commonwealth Scientific and Industrial Research Organization (CSIRO), Georgia Institute of Technology, University of Bristol and Scripps Institution of Oceanography. Methane gas measurements came from the Advanced Global Atmospheric Gases Experiment (AGAGE), which is supported by the National Aeronautics and Space Administration (NASA), and the Australian CSIRO network.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ba818fa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter 
Is paranoia contagious? It seems so in certain circles. Like fear, it begins with one, and then spreads like wildfire.
Yesterday I wrote about Professor Stefan Rahmstorf’s hissing and fitting reaction to a piece written by EIKE, the humble sceptic organisation that had the audacity to bring up one of Rahmstorf’s old, yet embarrassing papers here, which I wrote about here.
Something is really getting to the poor fella. Science is supposed to be for the calm, and not the irrational. Rahmstorf has to learn to get a grip.
It seems whenever someone expresses dissent, which is normal in science, Rahmstorf flies off the handle and lashes out, almost irrationally.
Yesterday, for example, Roger Pielke Jr. wrote a piece here with details on an e-mail exchange between Rahmstorf’s sidekick Michael Mann and journalist Daniel Greenberg. These guys really seem to think the whole world is out to get them.
And recall the paranoia that pervaded throughout the Climategate e-mails involving Jones, Mann and the rest of the cast.
In this post I’m only going to focus primarily on Rahmstorf, as he is the issue here in Germany at the moment. But it fundamentally applies to the rest of his team as well.
Firstly, Rahmstorf is one of the very few “climate scientists” who truly believes that sea levels are about to rise at alarming rates over the next few decades, even refusing to accept the much more moderate projections of the IPCC, for which he is a lead author. Even though there is no data out there to support it, Rahmstorf is sure the atmosphere and seas are out to get us.
Calmer minds have attempted on numerous occasions to alleviate his anxiety by pointing out that scientific data do not support his horror visions, and that there is no need to go sleepless about it. For example sea levels over the last 100 years have risen at their normal rate of about 20 cm per century. Over the last years sea level rise has even slowed down, Read more here: Going Down.
Sea level rise is slowing. TOPEX U. of Colorado


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And when someones tries to explain to him that the Greenland ice cores provide a reasonable temperature reconstruction of the past for the globe, he refuses to believe that too, insisting the fluctuations were local and not global. And if someone shows him that multiple proxies from all over the globe also show that temperatures fluctuated naturally, corresponding with the Greenland ice core, Rahmstorf still refuses to believe it and insists the climate is coming for us.
Proxies from all over the world confirm fluctuations on a global scale.
Whether it’s sea ice data, accumulated cyclone energy, the last two brutal German winters, ocean cycles, satellite data, etc., Rahmstorf refuses to believe the data no matter what. The climate’s after us.
So why does Rahmstorf refuse to believe data and the fellow scientists who deliver them? Here he also believes that these scientists are paid hacks of the big carbon industries, tobacco, and ultra right-wing think tanks.
The lefty Süddeutsche Zeitung newspaper, for example, even reminded Rahmstorf awhile back that EIKE is just small, mailbox sceptic organisation. Are corporations pouring millions and millions and only getting a mailbox operation for their big bucks?
Gravediggers of science
Rahmstorf is acting too paranoid, and this behaviour is now running rampant through much of the warmist side. He even thinks that European corporations are buying up US denier senators. He writes at his piece Headlines From Absurdistan, citing treehugger blog:
Also European companies don’t pinch their pennies when it comes to buying up candidates for the US Senate who deny anthropogenic climate change.
How are regular people reacting to this? Daniel Greenberg wrote to Pielke:
My sympathy to you and anyone else who has to deal with them. They’re gravediggers of science.
It’s called self-destruction.
Relax – the climate is not going to tip and destroy the planet. The data does not show this.
Share this...FacebookTwitter "
"
And the hits just keep on coming…

646,024 for the Month of July, up from 582,079 in June.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d548764',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Thank you Mr. Chairman and Members of the Committee for inviting me to testify before you today. By way of introduction, I am an Associate Professor of Law at Capital University Law School in Columbus, Ohio, where I teach, among other subjects, Election Law. Though I appear today on my own behalf, I am also an Adjunct Scholar of the Cato Institute. I have researched and written extensively, in both academic and popular journals, on the subject of campaign finance.



First, let me say that I appreciate the decision of this Committee to hold hearings on campaign finance reform. Before Congress now are bills to reform campaign finance by placing new limits on speech — the so called “McCain‐​Feingold” bill in the Senate (S25) and “Shays‐​Meehan” bill in the House (H493). Numerous lending constitutional scholars and campaign finance experts have called these bills unconstitutional. Yet one of the name sponsors of the bill on the Senate side just this month derided these concerns on national radio, saying “When [my opponent] starts relying on those Constitutional arguments, I know he doesn’t have much else in his arsenal.” On another occasion, on national television, this senator stated cavalierly that his opponents “may be right that that particular provision [of the bill] is unconstitutional. And that’s why we have a back‐​up provision.”



Meanwhile, the minority leader here in the House was quoted in a national news magazine earlier this month, saying that “freedom of speech” and “a healthy democracy” are “in direct conflict.”



When a member of Congress so casually treats his oath to uphold the constitution; and when the House minority leader suggests that the First Amendment must itself be amended because free speech “is in direct conflict” with democracy, it is both timely and appropriate for this committee to hold hearings.



Before congress attempts to solve the problems of campaign finance with more regulations burdening free speech rights, we should take stock of the fact that the current regulatory system is responsible for many of the evils we see in campaign finance. We do not need to plug “loopholes” in the system. Rather, we should scrap most all of the present system of campaign finance regulation, remembering the admonition of the First Amendment to the Constitution, that Congress shall make no law abridging the rights of free speech.



Before discussing the details of campaign finance, I think it is important to briefly remind ourselves that, for most of this country’s history, the funding of political campaigns has been totally or largely unregulated. During our nation’s first century, the era which produced as presidents Abraham Lincoln, George Washington, Thomas Jefferson, Grover Cleveland, James Knox Polk, and Andrew Jackson, and which saw giants such as Daniel Webster, Henry Clay, John Quincy Adams, and John C. Calhoun serve in Congress, there were literally no laws regulating campaign finance. And today, we often look back on that century as a golden age of politics — one in which memorable debates over such monumental issues as slavery and western expansion were discussed in serious campaigns, one in which people talked and debated these issues, one in which politics was marked by mass rallies and torchlight parades, and one in which voter turnout was considerably higher than it is today.



The federal government did not become involved in campaign finance until this century. If we look back, we find that the arguments made in favor of regulation a century ago were the same that we hear today: that the American people believed Congress to be made up of the “instrumentalities and agents of corporations;” that “corruption” was the norm; that new advertising techniques and technologies‐​in those days mass newspapers, recordings, train travel‐​had created an insatiable demand for political spending that could only be curbed by spending limits; and that we faced a “crisis” of democracy. In response to such complaints, the federal government passed its first campaign finance law in 1907, banning direct corporate contributions to candidates. In 1943, this ban was extended to labor unions. Additionally, congress passed greater disclosure requirements in 1925. However, these disclosure measures were so toothless as to be meaningless. For example, from its enactment in 1925 until its repeal in 1971, there was not a single prosecution under the Federal Corrupt Practices Act. Yet democracy survived, and this period of minimal regulation gave us Presidents Theodore and Franklin Roosevelt, Calvin Coolidge, Harry Truman, and Dwight Eisenhower, Congressional leaders such as Robert Taft, Hubert Humphrey, and Everett Dirksen, and serious debates over such issues as civil rights. For nearly two centuries, our democracy flourished despite, or perhaps even because of, the absence of any meaningful campaign finance regulation.



Not until the 1974 Amendments to the Federal Elections Campaign Act (FECA) did the federal government pass a campaign finance law with any serious enforcement mechanism. And it was also this law which, for the first time, gave us both contribution limits and, as a necessary accessory to those limits, the strange doctrines of independent expenditures and express advocacy. The 1974 Amendments threw of web of regulation, with an accompanying enforcement bureaucracy, the FEC, over American politics.



The stated goals of the 1974 FECA Amendments were to lower the cost of campaigning, reduce the influence of so‐​called “special interests,” open up the political system to change, and “restore confidence in government.” So what has actually happened in the twenty years since the 1974 Amendments took effect? Well, campaign spending has increased by more than 350 percent; PAC contributions have increased by more than 800 percent; House incumbents, who had previously outspent challengers by approximately 1.5 to 1, now outspend challengers by nearly 4 to 1; incumbent reelection rates have risen to record high levels, spurring the demand for term limits; and public confidence in government has fallen to record lows. Clearly, the 1974 FECA Amendments have been a dismal failure. Yet the response of the reformers — notably Common Cause, the interest group most responsible for the 1974 Amendments, and today the number one cheerleader behind the Shays‐​Meehan bill — is to argue that we need more regulation, more limits, and more bureaucracy. Indeed, some now claim that because earlier regulation has failed, we not only need still more regulation, but we need to amend the First Amendment to allow government to regulate political speech and activity in ways the Founders never dreamed of. I would suggest, however, that when an approach has failed so clearly, so dismally, with such negative consequences, over a period of twenty years, it is time to consider a whole new approach. It is time not for more regulation, nor for more efforts at “loophole” plugging, which is the approach taken by Shays‐​Meehan. Rather, it is time to deregulate American politics.



In my opinion large parts of Shays‐​Meehan and its Senate counterpart, McCain‐​Feingold, are unconstitutional. The so‐​called “voluntary” spending limits of the bills are in fact punitive and coercive, and amount to an unconstitutional condition leveled on the Constitutional right to free speech. That portion of McCain‐​Feingold abolishing PACs is unconstitutional, as even its supporters seem to recognize. If one person can spend $1000 on bumper stickers, it is inconceivable that two people cannot join together to spend $1,000 on bumper stickers.



The Senate bill’s limitations on out‐​of‐​district contributions are probably unconstitutional. There is no reason why an out‐​of‐​district contribution is more “corrupting” than an in‐​district contribution. Thus, there is no compelling government interest to justify the ban on speech.



Overall, Shays‐​Meehan and McCain‐​Feingold mark the most serious legislative assault on free speech in over two decades‐​since the 1974 Amendments to FECA. Many of those 1974 reforms were eventually held unconstitutional. The others have stifled free speech and contributed to the current problems. We should not go down that road again.



But where Shays‐​Meehan and McCain‐​Feingold are most at odds with the Constitution and sound policy is in their efforts to silence political groups engaged in issue advocacy. Indeed, this question of “issue advocacy” versus “express advocacy,” which has aroused the ire of those who would regulate political speech, is a prime example of the danger of the FECA’s attempt to regulate politics to produce a desired result. Congressional Quarterly has noted that in recent years, litigation has become a major campaign tactic. Thus we have Republicans filing complaints against the AFL-CIO, and the Democrats filing complaints against the Christian Coalition, U.S. Term Limits, Americans for Tax Reform, and the Christian Action Network, to name just a few recent complaints. In each case, the complaints amount to a blatant effort to silence political advocacy by these groups. In each of these incidents, the groups involved were not engaged in any nefarious activities such as vote fraud or bribery. Rather, their alleged infractions amounted to what might be called the crime of “committing politics.” That is to say, the groups involved were trying to persuade the American people that either their positions were right, or someone else’s were wrong. It is true that incumbent officeholders do not like being attacked for their stands on issues; particularly when they view those attacks as shameless demogogery. However, the robust discussion of issues is wholly in line with both the First Amendment and the American tradition of political participation. That opposing political interests can invoke the powers of a government bureaucracy in an effort to silence these voices is, I suggest, a much more serious blight on our system than the alleged effects of campaign contributions.



The reasons that these complaints are even treated seriously is because of the doctrine of “express advocacy.” This doctrine itself a bizarre outgrowth of efforts by supporters of campaign finance “reform” to limit campaign contributions. These “reformers” seek to prevent individuals and groups from participating in politics through contributions of money. However, under the Supreme Court’s ruling in _Buckley v. Valeo_ , Congress may not, constitutionally, restrict individual or group expenditures that do not “include explicit words of advocacy of election or defeat of a candidate.…” Thus, political speech is free from FEC regulation if it does not expressly advocate the defeat or election of a clearly identified candidate, but is subject to FEC regulation if it crosses that line.



The response of Shays‐​Meehan to this type of political activity, is to define “express advocacy” more broadly, by looking at such factors as timing and context. In fact, however, it is hard to see how these factors make any serious difference. For example, would last year’s AFL-CIO ads have been any more or any less “express advocacy” if aired three months or fifteen months before an election? Would it really matter if the group sponsoring the ads had public positions on some of the issues addressed? Clearly not. The ads would be no more nor less aimed at shaping public opinion, regardless of the added factors that Shays‐​Meehan seeks to consider. Thus, the long and short of an expanded definition of “express advocacy” would be a sharp reduction in political speech, which is precisely what the Supreme Court’s decision in _Buckley_ , protecting issue advocacy, is intended to guard against.



Of course, the FEC’s concern over “express advocacy” and independent expenditures is all part of a larger effort to plug “loopholes” in the disastrous system of contribution and spending limits enacted in 1974. The only reason anyone cares about “express advocacy” is the fear that, absent such a regulation, groups will spend money to try to affect federal elections, and in doing so will exceed the contribution limits of the FECA. I have written and commented at length on the undemocratic and deleterious effects that these limits have had on American politics. _See e.g._ Bradley A. Smith, _Faulty Assumptions and Undemocratic Consequences of Campaign Finance Reform_ , 105 Yale Law Journal 1049 (1996); Bradley A. Smith, Testimony before Committee on Rules and Administration, United States Senate, February 1, 1996; Bradley A. Smith, _Campaign Finance — Deformed_ , Wall Street Journal, Oct. 6, 1995 (copy attached). In short, these limits have entrenched incumbents; burdened grassroots political activity; limited the number and type of candidates; had the perverse effect of increasing the incentives both for campaign contributors to seek influence, rather than electoral success, and for office holders to reward financial patrons; and increased the power of unelected elites, most notably the media. The efforts to drive money from politics have grotesquely distorted our political system. The reason is simple and obvious: efforts to limit political participation not only run afoul of the Constitution, but they are like efforts to stop the flow of a river — one way or another, the water will pass, diverting course as necessary to do so. So long as the federal government spends over $1 trillion each year and regulates virtually every phase of the economy, not to mention many non‐​economic activities, the American people will seek to persuade Americans to elect their favored candidates. In modern society, this political communication and participation requires the expenditure of money.



Fortunately, past efforts to limit political discourse have consistently been struck down by the courts as unconstitutional. Nevertheless, these attempts to stop this legitimate political advocacy by placing a heavy bureaucracy over political campaigns have had, as I mentioned, a variety of negative consequences. Not the least of these is the way in which such regulation stifles true grassroots democracy.



For example, a 1991 study by the Los Angeles Times found that among the most common violators of FECA were “elderly persons… with little grasp of the federal campaign laws.” Even well‐​funded and well‐​organized groups can find their efforts at grassroots advocacy smothered. In one ill‐​founded effort to prevent political advocacy in violation of campaign finance laws, the FEC passed a rule that would have prevented the United States Chamber of Commerce from communicating political endorsements to more than 220,000 of its dues paying members, mainly small businesses whose owners and managers have little time to follow politics and rely on the Chamber of Commerce precisely for such information. Similarly, the regulation in question would have made more than two‐​thirds of the National Rifle Association’s members ineligible to receive the group’s endorsements, as well as over 44,000 dues paying members of the American Medical Association. This regulation was, fortunately, found unconstitutional by the U.S. Court of Appeals for the D.C. Circuit, but only after these groups had had their speech chilled in the 1994 election. Chamber of Commerce v. FEC, 1995 U.S. App. LEXIS 31925 (D.C. Cir. Nov. 14, 1995).



Another recent FEC rule attempted to prevent corporations and other groups from actively engaging in issue‐​oriented advertising during a campaign, on the theory that such advertising would implicate federal elections. Again, this effort to limit the flow of political information had to be struck down by a federal court. Maine Right to Life Committee, Inc. v. FEC (D. Me., Feb. 13, 1996). Simply putting similar measures into a statute will not make them constitutional.



Limitations on “express advocacy” call for precisely the type of judgments that benefit large organizations with the ability to hire a battery of lawyers to advise them through the regulatory process. Efforts to broaden the concept to include advocacy beyond such express words as “elect” or “defeat” would truly burden free speech, especially for smaller, local groups. Political participation, by definition, seeks to influence voter preferences on both issues and candidacies. Any broadening of the term would lead to a murky standard that would significantly burden most all political speech.



Nevertheless, in addition to the unconstitutional provisions of Shays‐​Meehan, we now find offered up a Constitutional Amendment which would authorize Congress to adopt “reasonable regulations,” so long as they do not “interfere with the right of the people to fully debate issues.” This is classic double speak. Our Founding Fathers recognized that government could not be trusted to make such distinctions: The incentives to crush the opposition would be too great. Thus they wisely passed the First Amendment.



Historically, debates on the First Amendment have concerned the extent to which it covers pornography, or hate speech, or commercial speech, or “fighting words,” or treasonous speech. What has always been accepted, across the political spectrum, is that it covers political speech. So let’s be honest about it: what the Amenders really seek is a clause reading “the First Amendment to this Constitution is hereby repealed.”



Efforts to limit “express advocacy,” like, indeed, the rest of the FECA regulatory scheme, are based on the belief that Americans ought not participate in politics. However, it is not a bad thing for Americans to participate in politics — it is a good thing. It is constitutionally protected. And the fact of the matter is that, more than ever in American society, communicating in the political realm requires the expenditure of money. Money is not an evil in politics — it is a source of information to voters. Efforts to regulate the flow of money in politics over the past 20 years have done much more than money ever did to distort the political system and create a public distrust of government. It is now time to try a new approach — that is, it is time to deregulate politics. There is simply no _a priori_ method to say what is fair or not fair — how much groups should be able to spend, or what kind of advocacy they can spend it on. The bureaucracy that has been established to regulate politics is stifling grassroots advocacy and political communication.



After twenty years of campaign finance regulation, it should by now be clear that independent electoral advocacy by citizen groups lies at the core of the First Amendment, and that such advocacy ought to be beyond the permissible scope of government regulation. Political battles should be fought out in forums of public persuasion. It is poor policy to divert such debates to federal courtrooms, with each side attempting to silence its opponents through such arcane concepts as “express advocacy” and “coordinated” or “uncoordinated” expenditures.



Deregulation of campaign finance, not added regulation, is the proper course of action. The FECA $1000 limit on individual campaign contributions should be abolished entirely, or at least raised to a realistic figure, in order to reduce the need for candidates to rely on independent expenditures. (The $1000 limit, in existence since 1974, has never been adjusted for inflation. Had it been, it would be approximately $3500 today. This is the minimum to which the contribution limit should be raised: $5000, $10,000, or complete removal of the cap would be preferable.) All caps on political party giving should be removed. Donations from a party to its own candidates are not “corrupting.” Moreover, since last year’s Supreme Court decision in Colorado Republican Federal Campaign Committee v. Federal Election Commission, 1996 WL 345766 (U.S. 1996), parties may spend unlimited amounts in support of their candidates, but only independently of the candidate’s campaign. Driving a wedge between parties and candidates is poor public policy. Disclosure of political expenditures meets any public need to know the source of financing. However, even here I must counsel caution. Disclosure rules can have a chilling effect on speech and may be constitutionally limited. McIntyre v. Ohio Board of Elections, 1995 WL 227810 (U.S.)(1995). Disclosure rules governing independent expenditures should be limited, therefore, to groups which engage in substantial activity, spending over $50,000 in an election cycle. Electronic filing and mandatory FEC posting of reports on the Internet would help to insure an informed public. These are the type of sensible, constitutional reforms congress should consider‐​not the unconstitutional Shays‐​Meehan bill or the foolish drive to repeal the First Amendment.



In recent years, it has become increasingly difficult to discuss meaningful campaign finance reform. This is because both public and congressional opinion has become trapped in a box. This box is the conscious creation of groups such as Common Cause, which for 25 years have worked tirelessly to convince the American public that the members of this Committee, and indeed all of Congress, are corrupt bribe‐​takers, and that the public itself consists of innocent dupes incapable of making intelligent voting decisions based on the information presented to them. By constantly drawing simplistic correlations to financial support and voting records, and through the conflation of the issue of campaign finance reform with other issues of voter concern, such as lobby reform, negative campaigning, and legislative gridlock, these groups have purposely attempted to create a climate of public opinion in which certain core assumptions are not to be challenged. These core assumptions are that political advocacy must be heavily regulated; political contributions and, ultimately, political spending limited; and all possible “loopholes” plugged. However, the heavy regulatory regime which these “reformers” have placed over campaign activity is, in fact, a major contributing factor to the very problems that have created such public disgust with the campaign finance system and, indeed, Congress in general.



Now is the time to get out of the box. We must not plunge ahead, sacrificing our First Amendment Freedoms. Congress must realize that Shays‐​Meehan style “reforms,” based, as they are, on the erroneous assumption that Americans should not spend money on political affairs, cut off grassroots involvement and decrease the flow of information to voters. The regulatory approach enacted in 1974 has had unintended, negative consequences that have only increased voter cynicism. The House should reject simplistic proposals such as Shays‐​Meehan, or efforts to amend the Constitution to destroy the right to free political speech, and move generally to deregulate political speech. It ought not be a crime to “commit politics” in America.



Thank you.
"
"Our children will enjoy in their homes electrical energy too cheap to meter.    – Lewis L Strauss, chair of the US Atomic Energy Commission, 1954. When Strauss first coined the phrase above he was thinking of hydrogen fusion, a technology that always seems to be a tantalising 30 years away. However even without futuristic new technology the British electricity system may actually approach this mythical situation of energy that is “too cheap to meter”. In the future, the industry’s costs will be determined by the number and size of power plants and turbines that will be needed, rather than the fuel burned in them. This won’t mean an end to electricity bills – but it will mean some major changes in how they are calculated. When the UK’s electricity industry was privatised in the 1990s its power plants had the capacity to generate more than was needed. This was despite the fact many of these plants had already been in service for decades. The national transmission network was also well established. This meant the cost of electricity was largely determined by fuel prices – mainly coal and gas. The hardware used by the industry today has changed little since privatisation – in fact much of it has been in service since the 1960s. On a typical day in November 2014, 37% of the UK’s electricity was produced by 40-year old coal-fired power stations, 31% came from gas power stations, many built during Margaret Thatcher’s “dash for gas”, and 14% from nuclear power stations, all but one of which are scheduled to close in the next ten years. The rest came from renewables and imports from the continent. Over the coming 15 years this will change. To meet national and European targets for CO2 and industrial pollution, existing coal plants will be phased out and gas will be reserved for periods of peak demand. Most electricity will be provided by nuclear and renewables, supplemented during the winter by coal and gas.  This means a big shift from fuel to infrastructure costs – constructing the power stations, turbines, solar panels and associated infrastructure will cost many billions of pounds. Nuclear power’s refuelling and operating costs are relatively low. For wind and solar, the operating costs are effectively zero.  To get anywhere near its emissions targets the UK will have to see a drastic reduction in the amount of gas used for home heating. While scrapping gas boilers in favour of electric heat pumps may reduce emissions, it will also mean the load on the electricity grid will be much greater in winter than in summer.  The problem here is that renewable energy doesn’t operate on the same cycle as us humans – solar panels are much less productive on a gloomy day in December, when electricity demand is high, than on a sunny day in June, when it is low.  The financial incentives for electricity generators are changing. The US fracking boom has reduced natural gas prices in America (but not in Europe as the networks are not connected) and many US electricity generators have therefore switched from coal to gas, leaving a surplus of coal. This has in turn reduced world coal prices at a time when the situation in Ukraine has left gas supplies uncertain. Therefore, if they have the choice, UK generators are burning coal rather than gas.  At periods of low electricity demand, the availability of almost free wind energy is depressing the demand for fossil-fuel generation so it is not difficult to see why the UK’s gas plants are increasingly under-used.   In the UK, generators (the firms who own the power plants) sell their electricity to retailers (the firm named at the top of your bill) through an auction every half hour. The market works on energy prices (£/kWh). This was logical when the investment in power stations had been paid-off years earlier and fuel inputs such as coal or gas were the biggest cost. However, in the approaching situation where marginal energy costs could be almost zero and capacity charges – based on peak usage – are very high, it is difficult to see how an electricity market auction is possible. Bid prices would no longer be related to operating costs and there must be a risk that it would be closer to a game of poker, where the bid is based on an assessment of the competition, rather than on cost. A free market for electricity would be likely to produce extremely high prices in winter, particularly at periods of peak demand, but very low prices at times when the demand can be met entirely by renewable energy. If these energy costs are passed on to the customer, we could see the cost of using an electric kettle to make a cup of tea at 18:30 in January being many pounds, but electricity costing almost nothing during long periods in the summer.   Headlines about a pensioner paying £20 to boil a kettle would be a big challenge for whichever government is in power. However, if peak-time prices are not allowed to reflect shortages there would be no incentive for continued investment in backup fossil fuel generation to be used during these periods. Such generation would be used only rarely, but without it the UK would risk blackouts – even more politically challenging. Reforms to the electricity market are supposed to resolve this problem through introducing regular capacity auctions, where generators can bid to hold otherwise unused plants in readiness over a particular time period. Taken with an agreed “strike price” for renewable and nuclear generators which guarantees a minimum payment per kWh this has the effect of almost eliminating a competitive market for low-carbon electricity generators. As leading energy economist Dieter Helm wrote at the time of electricity privatisation, “the idea that governments could simply retreat from the scene and leave it to competitive markets is an illusion – energy is just too important to the economy and society”.  Perhaps consumers should be charged on their contribution to national peak load, rather than on their energy consumption. Then energy really would become too cheap to meter."
"
Two Stories for you, one about the snow itself, and the other about climate law being debated and passed in the middle of the unusual snow.- Anthony
London has first October snow in over 70 years
From the Guardian
Cold snap causes flight cancellations while a motorway accident kills one driver and causes severe disruption

Parts of south-east England had more than an inch of snow last night while London experienced its first October snowfall in more than 70 years as winter conditions arrived early.
Snow settled on the ground in parts of the capital last night as temperatures dipped below zero. A Met Office spokeswoman said it was London’s first October snow since 1934.
For greater south-east of England it was the first October snow since 1974. High Wycombe in Buckinghamshire had 3cm (1.2 inches). One of the coldest temperatures recorded was -4.1C in Benson, Oxfordshire.
“It is unusual to have snow this early,” the Met spokeswoman said. “In October 2003 sleet and snow was recorded in Northern Ireland, Wales, south-west, north-west and north-east England and the Midlands, but it was mainly over higher ground.”
read the entire story here
How Parliament passed the Climate Bill (in spite of the weather)
By Andrew Orlowski The Register
Posted in Government, 29th October 2008 12:35 GMT

Excerpt: Snow fell as the House of Commons debated Global Warming yesterday – the first October fall in the metropolis since 1922. The Mother of Parliaments was discussing the Mother of All Bills for the last time, in a marathon six hour session.
In order to combat a projected two degree centigrade rise in global temperature, the Climate Change Bill pledges the UK to reduce its carbon dioxide emissions by 80 per cent by 2050. The bill was receiving a third reading, which means both the last chance for both democratic scrutiny and consent.
The bill creates an enormous bureaucratic apparatus for monitoring and reporting, which was expanded at the last minute. Amendments by the Government threw emissions from shipping and aviation into the monitoring program, and also included a revision of the Companies Act (c. 46) “requiring the directors’ report of a company to contain such information as may be specified in the regulations about emissions of greenhouse gases from activities for which the company is responsible” by 2012.
Recently the American media has begun to notice the odd incongruity of saturation media coverage here which insists that global warming is both man-made and urgent, and a British public which increasingly doubts either to be true. 60 per cent of the British population now doubt the influence of humans on climate change, and more people than not think Global Warming won’t be as bad “as people say”.
Read the rest of the story at the Register, here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bd25ee1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

One of the few things that BOTH sides of the Carbon Dioxide and AGW debate seem to be able to agree on is the belief that CO2, as a trace gas, is “well-mixed” in the atmosphere. Keeling’s measurements at Mauna Loa and other locations worldwide rely on this being true, so that “hotspots” aren’t being inadvertently measured.
As support for this, if you do some Google searches for these phrases, you’ll get hundreds of results of the usage together:
CO2 + “well mixed”
“carbon dioxide” + “well mixed”
You’ll find complete opposites using the same “well mixed” phrase, for example:

Gavin Schmidt of Real Climate writes in comment # 162 of this thread on Realclimate.org
“A full doubling of CO2 is 3.7 W/m2, and so by looking at all well-mixed GHGs you get about 70% of the way to a doubling.”
Roger Pielke Sr. writes in April 2008:
“…and thus are not providing quantitatively realistic estimates of how the climate system responds to the increase in atmospheric well mixed greenhouse gases in terms of the water vapor feedback.”
You’ll also find the phrase in use in titles of scientific papers, for example this one published in the AGU:
New Estimates of Radiative Forcing Due to Well Mixed Greenhouse Gases
And you’ll find the phrase used in popular media, such as this article from the BBC:
Carbon dioxide continues its rise
In describing the emasurements of CO2 at Mauna Loa Observatory: “The thin Pacific air is ideal for this research since it is “well-mixed”, meaning that there is no obvious nearby source of pollution, such as a heavy industry, or a natural “sink”, such as forest which would absorb CO2.”
Hmm, “no obvious nearby source of pollution” I suppose the volcanic outgassing nearby doesn’t count as “pollution” since it is natural in origin.
So it seems clear that there is a broad agreement on the use of the term. I suppose you’d call that “scientific consensus”.
So it was with some surprise that I viewed this image from NASA JPL, a global CO2 distribution as measured by satellite:

Note the variations throughout the globe, ranging from highs of 382 PPM to lows around 365 PPM. There is a whole range of data and imagery like this above available here
My question is: how does this global variance translate into the phrase “well-mixed” when used to describe global CO2 distribution? It would seem that if it were truly “well-mixed”, we’d see only minor variances on the order of a couple of PPM. Yet clearly we have significant regional and hemispheric variance.
NASA JPL provides this caption to help understand it:
Although originally designed to measure atmospheric water vapor and temperature profiles for weather forecasting, data from the Atmospheric Infrared Sounder (AIRS) instrument on NASA’s Aqua spacecraft are now also being used by scientists to observe atmospheric carbon dioxide. Scientists from NASA; the National Oceanic and Atmospheric Administration; the European Center for Medium-Range Weather Forecasts; the University of Maryland, Baltimore County; Princeton University, Princeton, New Jersey; and the California Institute of Technology (Caltech), Pasadena, Calif., are using several different methods to measure the concentration of carbon dioxide in the mid-troposphere (about eight kilometers, or five miles, above the surface). The global map of mid-troposphere carbon dioxide above, produced by AIRS Team Leader Dr. Moustafa Chahine at JPL, shows that despite the high degree of mixing that occurs with carbon dioxide, the regional patterns of atmospheric sources and sinks are still apparent in mid-troposphere carbon dioxide concentrations. “This pattern of high carbon dioxide in the Northern Hemisphere (North America, Atlantic Ocean, and Central Asia) is consistent with model predictions,” said Chahine. Climate modelers, such as Dr. Qinbin Li at JPL, and Dr. Yuk Yung at Caltech, are currently using the AIRS data to study the global distribution and transport of carbon dioxide and to improve their models. 
As we’ve found with surface based temperature measurement, it seems the more we look at satellite data, the more we learn that our earth bound assumptions based on surface measurement don’t always hold true.
When measuring the planet, looking at the whole planet at one time seems a better idea than trying to measure thousands of data points at the surface, sorting out noise, doing adjustments to “fix” what is perceived as bias, and assuming the result is accurately representatiive of the globe.
UPDATE: 7/31/08 I got a response from the AIRS team on satellite CO2 measuremenst, see this new posting
We won’t have to rely on ground based CO2 measurements much longer.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9db144bd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGuest writer Ed Caryl recently looked at 9 “rural” stations scattered over the Arctic: from Alaska, to Canada, to Northern Europe western Russia and Siberia, and found Arctic temperatures follow the AMO, and not CO2. Read here A Light In Siberia. It’s important to note that the 9 stations were selected because they appeared to be NOT influenced by man-made heat sources.
First, here’s the AMO going back more than 150+ years. The cycles are clear to see.

Atlantic Multidecadal Oscillation (AMO). Source: http://www.appinsys.com/globalwarming/SixtyYearCycle.htm
North Pole, 17 March 1959. Image from NAVSOURCE
The AMO shows warm periods centering at about 1880, 1940 and 2005, i.e., 60 year cycle. We recall seeing photos of submarines surfacing at the North Pole back in the 1950s, see photo left, meaning it was relatively balmy back then too, as the AMO chart suggests.
Well how do the temperature curves of Ed’s 9 untainted Arctic stations match up with the AMO? The following are the GISS graphs of these 9 stations, each shown individually. Take a look at each of them:





What happens from 1940 – 1980, a time when CO2 was increasing? What happens after 1980? How do these charts match up with the above AMO chart? Fit pretty well? It seems so.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some of the temperature records shown above are shorter and some are longer. But they all show that temperatures between 1940 and 1980 were dropping. Remember that the Arctic is called the canary in the coal mine. When the globe cools or warms, you really see it in the Arctic, so they say.
Next Ed Caryl plotted each of the above graphs on a single chart. Ed calls these “rural” stations isolated because they are not impacted by man-made heat sources like asphalt, light bulbs, etc:
Plot of the 9 ""rural"" (isolated) stations.
And then he normalized the plots and generated an average. He explains how here, scroll down to “The averaging of station data”.  The resulting plot with a linear trend line is shown as follows:
Average of the ""rural"" stations
Sure some hot-shot statisticians out there are going to say you can’t do this, or that, or whatever blah blah blah…but that’s just nitpicking. Attention to tiny detail is a later thing.
Ed’s method suffices for now to generate a good general picture. If the math hotshots out there want to do it with micrometers, no one is stopping them. I doubt the general picture is going to change that much, though.
If you look at the 9 individual plots above and imagine how a composite of all 9 would look like, it would look like Ed’s chart – common sense.
Doesn’t the shape of above curve look eerily similar to the shape of the AMO from 1920 to today? Ed thought so too, and so he superimposed the average of the 9 isolated stations and the AMO:

Gee, do you think Arctic temperatures correlate better with CO2? 
Of course this is only a preliminary analysis that examined only 9 isolated stations scattered over the entire Arctic perimeter. But I suspect that if all stations were thrown in, except the crappy ones equipped with light bulbs and of that sort, you’d end up with similar results.
Could the AMO possibly drive climate? Well, the latest paper authored by Phil Jones and others seem to be hinting at this. Read my post from yesterday https://notrickszone.com/2010/09/24/der-spiegel-the-oceans-influence-greater-than-thought/.
Obvious conclusion: Trace gas Co2 drives the Arctic climate about as much as a sea breeze drives a loaded freight train.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Ed Caryl
In attempting to keep A Light in Siberia as short as possible, the how and why of some points were not included. This led to some comments calling into doubt some of the results. I would like to clarify some of those points.
The choice of baseline period for the surface temperature anomaly map
The 1933 to 1963 baseline for the surface temperature anomaly map was chosen for two major reasons. First, that period includes the peak of the last warm period before the present one. Second, that period was before most of the UHI warming took place for the arctic stations studied, making them show up on the map as red or orange grid-squares. The Arctic and Antarctic stations are highlighted. If you choose the modern warm period, 1979 to 2009, the baseline period includes much of the UHI warming, and the anomalies are much less pronounced.
GHN GISS 1933 - 1963
GISS 1979 - 2009
 
The satellite temperature map shows the arctic warming
Yes, it sure does. The reason is that the bottom of the AMO cycle was just prior to the beginning of the satellite measurements (1979). The arctic has been warming since then. If the satellites had been first launched in 1940 it would be a different picture. In 2050 it will be a different picture. These cycles are 70 years long, the biblical “three-score and ten”. Our main problems with studying climate are that we don’t live long enough to remember more than part of one cycle, and the satellite era has only been 31 years.


The selection criteria for “Urban” versus “Isolated”
“Urban” – Next to or in a town or research station with growth, or change in population over time, or change in heat generated over time.
“Isolated” – A location with no significant population or heat generator changes over time, a stable unchanging environment. An isolated location is one that never had more than one or two buildings, and with always the same size staff, and no adjacent town.
Remember, in the Arctic, in the winter, the environment around all these locations is very cold, bleak, desolate, and unpopulated. A steam-heated town or research station will stand out in the infrared like a bonfire in a desert.
 The averaging of station data
The stations discussed all have data over different time periods, and have average temperatures that are different. Some have gaps in the data. How can these be averaged without distortion?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The data was downloaded from GISS as text files. These were dropped into Excel spreadsheets, and converted into column-delimited files. The monthly data was discarded, as the annual average has already been computed by GISS and is the rightmost column. Data gaps in GISS files are marked by the entries 999.9. These cells were cleared.
After putting all the stations into one spreadsheet with the total year span in the leftmost column, and each station with its own column, aligned with the correct years, each station column was averaged using the SUM of the column divided by the COUNT of the cells in each column with data. Then the average of all the columns was computed. This number is then the average of all the temperatures in all the stations over the whole time period. Call that the “table” average.
The next step was to “normalize” the data for each station by subtracting the “table” average from each column average. This results in a normalization factor for each column. That normalization factor was then subtracted from each value in that column. The normalization factor will be different for each station.

 The rest is the data for all the stations now plot right over each other, in a narrow range, and now can be averaged across the rows in the same manner as the columns were averaged, using SUM divided by the COUNT for each row. Those adjacent years with many stations reporting data get a somewhat smoother plot than those years that have only one or two stations with data, but there is no distortion in the average from some stations being much warmer or cooler than the others.

The R2 question
R-squared value definition:
The R-squared value, also known as the coefficient of determination, is an indicator that ranges in value from 0 to 1 and reveals how closely the estimated values for the trendline correspond to your actual data. A trendline is most reliable when its R-squared value is at or near 1.
The above definition is cut and pasted unedited from the Excel Help files. What it means is that if the data and the trend-line coincide, as above, (if the data plotted is a straight line and the trend is coincident) then the R2 value would be 1.

Our very noisy data, with the AMO sine-wave-like curve superimposed, has a trend line with an extremely low R2 value, because it has a very low correspondence to the data. The trend has very little statistical significance, thus little or no warming is indicated.
Share this...FacebookTwitter "
"

As the list of Democratic presidential candidates grows, so do their promises. So far, the candidates have largely embraced the same policy focus: expanded entitlement spending to guarantee new welfare benefits.



Massachusetts Sen. Elizabeth Warren recently endorsed a universal federal provision of child care. Vermont Sen. Bernie Sanders has long supported “Medicare for All,” and businessman Andrew Yang is perhaps best‐​known for his advocacy of a universal basic income. Meanwhile senators Cory Booker, Kamala Harris, Kirsten Gillibrand and others have all backed the Green New Deal, which promises to address climate change and inequality by providing universal health care and creating millions of jobs.



While reasonable people can disagree on some aspects of these proposals, one fact is uncontroversial: the United States cannot afford them.





Given that the US cannot afford its existing entitlement programs, adopting the massively expensive policies proposed by Democratic candidates would be foolish.



Congressional Budget Office projections of the federal debt make this point compellingly. According to CBO projections, federal debt held by the public, currently at 78% of America’s gross domestic product, or GDP, will approach 100% in the next decade and reach 152% by 2048. Cutting the debt‐​GDP ratio to its 1957–2007 average within 25 years would require policymakers to permanently reduce spending or increase taxes by 3.8% of GDP, which amounts to about $800 billion, or 24% of federal revenue.



Distinguished economists have recently argued that debt may be less costly in the future because of low interest rates. This assumes we can forecast future rates. In reality, estimates of long‐​run interest rates differ widely and are highly uncertain. Rates have stayed low over the last decade due to a combination of factors, such as monetary policy, weak foreign demand and deleveraging in the wake of the 2008 recession. These forces are unlikely to play as important a role in the future.



More importantly, while low interest rates might permit running a long‐​term deficit that is stable relative to GDP, standard forecasts, such as those from CBO, project rising deficits.



Some Democrats have also claimed that federal debt is not a constraint by relying on “Modern Monetary Theory” (MMT), which argues that central banks can issue enough money to fund federal expenditures with little threat of inflation. But the stable inflation of recent years is precisely due to central banks’ intentional pursuit of price stability as the primary objective of monetary policy. If monetary policy were to focus on funding government spending instead, the threat of inflation would increase rapidly.



Several Democrats have also advocated increasing taxes, but the revenue generated would not come close to funding their proposals. Warren has campaigned for two new wealth taxes that are estimated to increase federal revenue by about $210 billion annually. New York Rep. Alexandria Ocasio‐​Cortez supports a 70% marginal tax rate on income over $10 million, which is estimated to generate an additional $20 billion to $70 billion per year, assuming that wealthy Americans are not discouraged from working. Even if the US adopted all three of these new taxes, annual federal revenue would increase by at most $280 billion.



The additional revenue would hardly make a dent in the cost of Democrats’ policy proposals. For example, Yang’s plan for a universal basic income is estimated to cost $3.8 trillion annually, and the Green New Deal would likely cost upwards of $6.6 trillion per year. Revenue from all three tax increases would fund less than 10% of either of these programs, let alone help pay down existing debt.



Supporters claim that although Medicare for All would massively increase government spending, it would be offset by decreases in private spending that leave total health care expenditures unchanged. While this is plausible in the short run, little evidence suggests that public health care spending would grow more slowly than private spending in the future. Public health care spending in the US has grown faster than private spending over the past 30 years, and other countries with publicly‐​funded health care systems have continued to see increases in expenditures as a share of GDP. An aging US population is likely to increase health care costs in the future, regardless of whether those costs are paid by government.



Given that the US cannot afford its existing entitlement programs, adopting the massively expensive policies proposed by Democratic candidates would be foolish. Instead, the federal government should cut entitlement spending and look for cheaper ways to address problems like climate change.



Instead of the Green New Deal, the federal government could adopt a revenue‐​neutral carbon tax to decrease emissions without exacerbating the fiscal imbalance. Economists from across the political spectrum supportcarbon taxation as the most cost‐​effective way to address climate change. And a carbon tax would be most effective if uniformly adopted by other countries, too.



At a minimum, the US should slow the growth of entitlement spending to no more than the average growth of GDP. Reasonable approaches include increasing the age of eligibility for Social Security and Medicare, modifying the indexation of Social Security benefits or tightening eligibility requirements for disability insurance. These reforms would keep entitlement spending affordable, unlike current policy.



Reasonable people can disagree on the benefits of federal welfare programs, the appropriate level of redistribution or optimal cost‐​cutting reforms. But everyone should agree that restoring fiscal sanity in the United States requires significant cuts to federal entitlement spending. The policies advocated by Democratic candidates will only make things worse.
"
"
Share this...FacebookTwitterEuropean particle physics research center CERN located near Geneva, Switzerland has gotten its budget cut by 6% over the next five years. Europe no longer has the money to fund it. 
It was just a question of time. With so much money being pissed away on bogus climate research and save-the-world projects, eventually you run out of funds to pay for real science and research.
How many billions have been poured into researching the non-problem of climate change? The IPCC? Hansen’s GISS surface station folly? Green subsidies? The list is endless.
What benefit have we gotten? I’d say we’ve gotten a lot more damage than benefit, especially if you consider what could have been done with the money instead. Lost opportunities.
Swiss radio has a report (German): CERN budget cut and writes:
The money worries of the European countries is now adversely impacting the European CERN research facility in Geneva. Over the next 5 years 343 million Swiss Francs have to be saved – about 6 percent of the budget.
While the management of CERN say the financial measure is painful, it will be bearable. But the research laboratory’s association of employees has a different opinion: The future of CERN is being put at risk.
Here’s my advice to CERN employees who fear losing their jobs: Call your elected officials and tell them stop wasting so much on bogus climate science.
Share this...FacebookTwitter "
"
More harbinger of the Northern Hemisphere winter to come?







A bulldozer cleans snow on the Sichuan-Tibet  road in Nyingchi, southwest China’s Tibet Autonomous Region Oct. 30, 2008.  (Xinhua Photo)





LHASA, Oct. 30 (Xinhua) — The death toll has risen to seven, and one person  remains missing, as a result of the worst snowstorm on record in Tibet, local  authorities said Thursday. 
 The seven people killed either frozen to death or were crushed by  collapsing buildings. About 144,400 heads of livestock died in the storm, which  also knocked out telecommunications and traffic in parts of Shannan prefecture. 
In Lhunze County, 1,348 people stranded by damaged buildings or blocked roads  had been rescued, the county government said. Rescue operation for the remaining  289 trapped was still underway. 
 The worst-hit county had 36 consecutive hours of snowfall from Sunday,  with an average snow coverage of 1.5 meters. Four people died and one remained  missing in the snowstorm. 
 The rescued people have been moved to other villages, sleeping in schools  or government buildings. 
 A road linking Lhunze to Cuona County reopened on Thursday after 63 hours  of snow clearing efforts of armed policemen and transportation staff. 
 Cuona had been isolated from the outside for three days due to the road  blockage. 
 The Tibet regional civil affairs department has allocated relief  materials such as clothes and tents to the affected areas.
h/t to Dr. Roger Pielke


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b21998f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The consistent message from those who would seek to exploit shale gas is that it has three distinct advantages over existing forms of fossil fuel energy: it is cheap, it has a lower influence on global warming, and it reduces the reliance in foreign imports. In the UK the ability of shale gas to replace substantial amounts of other energy sources is unproven. The International Energy Agency (IEA) says even high levels of shale gas extraction by the middle of the century would still only leave shale on an equal footing with gas, coal and oil. Claims of low energy prices have been rejected by many commentators including the industry itself. Lord Browne, former head of BP and current chair of the leading shale gas company Cuadrilla, contradicted both the prime minister and the chancellor last year by saying that in the UK fracking would not reduce gas prices. This leaves the mitigation of climate change as the central plank of the shale gas argument. Replacing coal and oil with gas would, it is argued, emit lower carbon dioxide volumes per unit of energy which is a major contributor to global warming.  However, a new report published in the journal Nature shows that this is unlikely to be true either. Haewon McJeon from the US Pacific Northwest National Laboratory along with teams from Australia, Italy, Germany and Austria ran five different models to see what would happen if abundant gas was made available on the market. They concluded it would indeed change things, but it would not mitigate climate change. Such cheap, available gas would affect CO2 emissions by between -2% and +11%. This in turn would lead to a slight increase in climate forcing (imposed disturbances in the earth’s energy balance) according to most models, of between -0.3% and +7%. This means that large-scale shale gas extraction will actually mean more CO2 emitted into the atmosphere, and more human-generated climate change. It’s certainly some way off the minimum 80% carbon reduction climate scientists say we need to achieve from our energy by 2050. The researchers explain that an abundance of gas will lead to higher levels of consumption, even when growing populations and wealth are accounted for. If gas is cheap, people will use more of it. This higher consumption assumes that prices of gas will come down, contradicting the industry view in Europe. However, if prices stay the same as oil and coal, the expectation is that impact in global warming will be considerably worse. The team ran a further unlikely scenario where coal would be effectively outlawed to see the effect of new gas reserves on climate change. This produced CO2 emission reductions of up to -6% – still a long way from the required -80%. Unfortunately, this is a best case scenario. One of the many concerns about fracking for shale gas is that it produces high levels of fugitive methane, another greenhouse gas which has over 20 times the warming properties of carbon dioxide. When the team modelled this, the upper influence of climate forcing goes from +7% to +12% by 2050. The unequivocal conclusion of the team is that: Abundant gas does not discernibly reduce climate forcing … and, under high fugitive emission assumptions, three models reported increased climate forcing of more than 5%. This work confirms what the IEA has known for some time. In May 2012 the agency issued a special report on the exploitation of shale gas called Golden Rules for a Golden Age of Gas. Committed readers needed to go past the executive summary and soldier on until the 90th page before reading that if shale gas was fully exploited it would mean global warming will increase to at least 3.5C, well past the comparative safety and current target of 2C. Politicians have thus far been able to resist the tide of academic works that have shown why exploiting a new source of fossil fuel may not be a particularly great idea. This latest report is particularly stark and has a better chance than most to divert evidence-based policy towards a more sustainable direction."
"
Share this...FacebookTwitterAlex Bojanowski at Germany’s online Der Spiegel reports here on a new paper appearing in Nature that shows climate change in the 1970s was caused by ocean cooling. Climate simulation models once indicated that the cooling in the 1970s was due to sun-reflecting sulfur particles, emitted by industry. But now evidence points to the oceans.

I don’t know why this is news for the authors of the paper. Ocean cycles are well-known to all other scientists. The following graphic shows the AMO 60-year cycle, which is now about to head south.
Atlantic Multidecadal Oscillation (AMO). Source: http://www.appinsys.com/globalwarming/SixtyYearCycle.htm
Computer models simulating future climate once predicted that it would soon get warm because of increasing GHG emissions, but, writes Der Spiegel, citing Nature:
Now it turns out that the theory is incomplete. A sudden cooling of the oceans in the northern hemisphere played the decisive role in the drop of air temperatures.
The paper was authored by David W. J. Thompson, John M. Wallace, John J. Kennedy, and Phil D. Jones. The scientists discovered that ocean temperatures in the northern hemisphere dropped an enormous 0.3°C between 1968 and 1972. Der Spiegel writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A huge amount of energy was taken out of the oceans. The scientists said that it was surprising that the cooling was so fast.
 This shows, again, that the climate simulation models used for predicting the future are inadequate. It’s not sure what caused the oceans to cool. But scientists are sure that aerosols were not the cause. Der Spiegel describes a possible scenario how the oceans may have cooled: 
Huge amounts of melt water from Greenland’s glaciers poured into the Atlantic at the end of the 1960s, and formed a cover over the ocean. The melt water cooled the ocean for one thing, and acted to brake the Gulf Stream, which transports warm water from the tropics and delivers it to the north. The result: the air also cools down.
But, as Spiegel reports, that hardly explains why there was also cooling in the north Pacific. Der Spiegel:

 The scientists will have to refine their climate simulations. The new study shows one thing: The influence of the oceans is greater than previously thought.

I’d say that’s a very polite way of saying: Your models have been crap, and it’s back to the drawing board. This time don’t forget to properly take the oceans and every thing else into account. Yes, there’s a quite a bit more to climate than a single trace gas in the atmosphere. Hooray – the warmists are finally beginning to realize it! (Maybe)
Share this...FacebookTwitter "
"Seagrass is one of the most important coastal habitats where young ocean-going fish such as Atlantic cod can grow and develop before setting out on the journey of life. But these critically important habitats, revealed in new research, are being damaged the world over and its not just threatening biodiversity but our food security. Some 30,000 km2 of seagrass (Zostera marina) has disappeared over the past two decades, about 18% of the global area. This is incredibly important. One hectare of seagrass absorbs 1.2 kilogrammes of nutrients each year, equivalent to the treated effluent of 200 people. It can produce 100,000 litres of oxygen per day, can support 80,000 fish and 100m invertebrates – and absorb ten times as much CO2 as a pristine area of Amazon rainforest. Providing shallow-water habitats where young ocean-going fish can grow and develop is one of the key ecosystem services that our coastal seas provide, but unfortunately we largely don’t recognise the value of them in supporting the fishery resources of vast ocean basins. We continue to allow the loss of this coastal habitat to occur throughout the world – in spite of regulations in many nations to protect key habitats and biodiversity. The diminished status of fisheries for species such as the Atlantic cod (Gadus morhua) means we need to move beyond purely managing adult fish stocks and consider the interaction of individual high-value species within the whole seascape and the critical habitats within that.  This is critical as our declining fishery stocks do not align with the needs of the region or the planet as a whole. To meet the needs of the predicted human population of 2050, an additional 75m tons of protein from fish and aquatic invertebrates will be required – this is a 50% increase in current supply.  The Atlantic cod is a species of significant economic and historic importance but is now better known for its catastrophic decline. Apart from overfishing, the causes of this decline and its subsequent lack of recovery remain largely unresolved. The degree to which specific coastal and shallow-water habitats are important for this species still remains unquantified at the scale of the North Atlantic.   There is extensive evidence of the presence of juvenile Atlantic cod in seagrass throughout the North Atlantic. Juvenile cod have been recorded in such high density in seagrass that they average 246 individuals per hectare. This density of juvenile Atlantic cod is higher in seagrass meadows compared to alternative habitats. This includes an incredible dataset from Norway where researchers have sampled juvenile cod in seagrass annually since 1919 and other recent studies observing juvenile cod in seagrass in North Wales by our team at Swansea University using stereo Baited Remote Underwater Video systems and seine nets.  Juvenile Atlantic cod have greater long-term viability after having spent time in seagrass, which improves their chances of reaching maturity. Our new analysis, published in the open access journal, Global Ecology and Conservation, illustrates how juvenile Atlantic cod grow faster in seagrass than in surrounding alternative habitat types and have higher survival rates from predation. Although juvenile Atlantic cod do not always need to use seagrass meadows as juvenile habitats, it appears that they may intentionally select seagrass as a nursery habitat. This data comes from studies throughout the Atlantic including Newfoundland in the west and Sweden in the east. The study, conducted in collaboration with Richard Lilley at Cardiff University, was an extensive meta-analysis of research on the life history of the Atlantic cod resulting in a review and synthesis of its nursery habitat usage. It includes data sources from throughout the region, ranging from Newfoundland, to Norway, to Scotland and to Sweden.  Our work provides strong evidence that seagrass meadows are of significant importance to contributing to Atlantic cod stocks, and our review presents extensive quantitative evidence of the role of seagrass as valuable nursery habitat for Atlantic cod. These findings are of major significance given the continued threats to these systems.  Seagrass meadows are globally important resources that are being threatened by a whole series of issues ranging from climate change and major weather events to boating activity, poor water quality and coastal development. This work clearly illustrates how key habitats in our coastal seas such as seagrass need protecting, not just for biodiversity but for the continued food security provided by our oceans."
"
Share this...FacebookTwitterThe Portuguese sceptic site Ecotretas has a piece today called Not so fine Mr Gore which features a video clip of Al Gore making a speech before a crowd, predominantly businessmen. Gore seems to think that Portugal’s economy is on the right track and is coming out of recession. Far from it. In fact, Portugal’s economic woes are worsened by Gore’s vision for the planet.
So Ecotretas reminds Gore:
Someone should tell Al Gore that Portugal is not coming out of the Great Recession. In fact, Portugal’s economy is getting worse, as can be seen by the high sovereign risk, usually one of the highest 10 in the world in the last months. This is so because major bad economic decisions have been made in the last years, namely in alternative energy.
The total cost for Portuguese consumers and taxpayers is estimated at 700 million euros this year. But costs are growing exponentially.
Just like Spain, we’re going down, while unemployment keeps going up, and the promised green jobs are one of Europe’s lowest. So, Mr. Gore: If some Portuguese told you that “I feel fine”, I can bet he felt like the farmer in your nasty story!
Gore delivers a humorous story to make a point. Probably a lot of scientists feel like the poor farmer when asked: “Is your data, which show man is causing climate change, fine?”
Share this...FacebookTwitter "
"From the 1950s until recently, we thought we had a clear idea of how continents form. Most people will have heard of plate tectonics: moving pieces on the surface of the planet that collide, pull away or slide past one another over millions of years to shape our world.  There are two types of crust that sit on top of these plates: oceanic crust (that beneath our oceans) and continental crust (that beneath our feet). These move across the surface of the Earth at rates of up to 10cm per year. Many are in a state of constant collision with one another.  Continental crust is thicker than oceanic crust. When continents collide, they buckle upwards and sideways to form mountain ranges: the Himalayas, for example. When continental and oceanic regions collide, the oceanic crust slides beneath the continent and gets consumed back into the Earth in a process that geologists call subduction.  In these circumstances, the plate on top is subjected to compressing and stretching forces that can create mountain belts such as the Andes in South America. The sinking ocean plate meanwhile melts and can produce volcanoes at the surface. All of this adds new material to the continent. As the plate beneath pushes its way under the one above, large earthquakes can also be generated, like the one that struck Sumatra in 2004 and caused the Boxing Day tsunami.  For 60 years the orthodoxy has been that these processes gradually form supercontinents, such as Gondwana or Laurasia, where a vast land mass is brought together before slowly breaking up and drifting away in pieces again. This has happened a number of times in cycles since the Earth was formed, collecting and then separating land over and over again. Now we have new information that suggests that the process is more complex than we had thought. When supercontinents break apart, small pieces of so-called “exotic continental crust” sometimes splinter off and get set adrift in newly formed oceanic crust (which is generated in places where continents break up).  When the oceanic crust containing the remnant fragment of continental material collides with another continent, the exotic piece of crust is too thick and buoyant to take part in the usual process of subduction. Instead of sliding beneath, it gets stuck at the margin of the continent.  When the surrounding zones of tectonic collision recede as the large piece of continental crust increases in size, the newly formed crust is forced to wrap itself around the exotic continental fragment. This creates a dramatic bent mountain belt called an orocline. This theory was first published by a group of Australian academics earlier this year, based on predictions from their 3D computer model. But the field evidence to support their findings was limited, so the race was on to demonstrate that this really does happen.  To confuse things further, not all oroclines are necessarily formed in this way: sometimes mountain ranges can bend for other reasons. So the likes of the Texas Orocline in eastern Australia or the Cantabrian Orocline in Iberia would be good places to look for evidence of the new theory. But their existence doesn’t tell us anything by itself.  This is where my team came in. I have spent the best part of 12 years driving around the outback in eastern Australia, digging holes to bury small seismic sensors. These record earthquakes from places like Indonesia, Fiji and Japan, which through a process called seismic tomography has enabled us over time to build up a 3D image of the Earth’s crust in Australia. It is similar to the X-ray-based computerised tomography (CT-scan) that doctors use to construct internal images of parts of the human body. Over the years I planted about 700 of these sensors. The sensors have now enabled us to prove that the theory is correct. Ironically we found what we were looking for, not in any of the world’s known bent mountain ranges but in one of the flattest places on Earth: the Hay plains in western New South Wales, a dry dusty expanse over hundreds of miles.  Hay is the site of an old sea that formed and receded due to variations in sea level, during which sediments were deposited on the eroded bedrock below. Our imaging shows that buried underneath it are the remains of exactly the sort of orocline the theory predicted.  What does this mean for geology? It shows us that continents form in more complex ways than we thought. Scientists will now probably start testing other parts of the Earth’s crust to try and find examples elsewhere, including the oroclines that we can already see. It is very hard to say how widespread these features will turn out to be. Most likely the old version of plate tectonics will still be true in the majority of cases. The discovery may give us new insights into how minerals are formed. I wouldn’t go as far as to say it will help us to find more minerals, but it should add extra sophistication to our predictive framework for saying where and how minerals form.   It will also make us think more about what happens when supercontinents break apart, especially smaller pieces the size of Tasmania or the UK. It could mean that a lot of them end up forming new continents through this sort of process. Previously scientists hadn’t given this much thought. Wherever the new findings take us, it may be the beginning of a new chapter in how the world fits together."
"


A glacial region in Norway (Source:  NRK)
Reposted from the DailyTech
By: Mike Asher


Scandinavian nation reverses trend, mirrors  results in Alaska, elsewhere.
After years of decline, glaciers in Norway are again growing, reports the  Norwegian Water Resources and Energy Directorate (NVE). The actual magnitude of  the growth, which appears to have begun over the last two years, has not yet  been quantified, says NVE Senior Engineer Hallgeir Elvehøy.
The flow rate of many glaciers has also declined. Glacier flow ultimately  acts to reduce accumulation, as the ice moves to lower, warmer  elevations.
The original trend had been fairly rapid decline since the  year 2000.  
The developments were originally reported by the Norwegian  Broadcasting Corporation (NRK).
DailyTech has previously reported on the  growth in Alaskan glaciers, reversing a 250-year trend of loss. Some  glaciers in Canada, California, and New Zealand are also growing, as the result  of both colder temperatures and increased snowfall.
Ed Josberger, a  glaciologist with the U.S. Geological Survey, says the growth is “a bit of an  anomaly”, but not to be unexpected.
Despite the recent growth, most glaciers in the nation are still smaller than  they were in 1982. However, Elvehøy says that the glaciers were even smaller  during the ‘Medieval Warm Period’ of the Viking Era, prior to around the year  1350.
Not all Norwegian glaciers appear to be affected, most notably those in the  Jotenheimen region of Southern Norway.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ac52297',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Over the past 30 years China has become the world’s factory. For the past few weeks, the production line has been shut down by plant closures deemed necessary to halt the spread of coronavirus. Beijing fears there will be both short- and long-term damage from the outbreak. The country is on course for its first quarter of negative growth in decades, while earlier this week China’s ambassador to the World Trade Organization (WTO) called on other countries not to use coronavirus as an excuse to put up trade barriers. The fact that Zhang Xiangchen felt the need to make this appeal speaks volumes. China suspects there will be backdoor protectionism – and it is almost certainly right, because for years countries around the world have needed little encouragement to resort to protectionism. What’s more, the restrictions are not just on the movement of goods. Earlier this month, the US treasury announced curbs on foreign investment to protect critical technology, data and infrastructure from foreign sabotage. Donald Trump’s plans for a wall along the US border with Mexico are emblematic of a toughening up of controls on migration. An era of open markets and open borders – where trade and transnational capital flows rose rapidly as a share of global output – has run its course. The instruments of deglobalisation are being weaponised. Companies are realising that lengthy global supply chains have costs as well as benefits. Coronavirus has brought that home with a vengeance Today, governments are less interested in breaking down barriers and more concerned about safeguarding jobs, preventing intellectual property theft and the risk of cybercrime. Companies are realising that lengthy global supply chains designed to take advantage of low wages in the developing world have costs as well as benefits. Coronavirus has brought that home with a vengeance, and is likely to further encourage the repatriation of production that was offshored in the 1990s and 2000s. Deglobalisation has happened before, notably between 1914 and 1945. It is happening now as a result of geopolitics, economic torpor, rising inequality, the failure to develop new political structures to manage globalisation, and the response to new threats. Every wave of globalisation has required a champion, a hegemonic power confident enough to spread the gospel of free trade and open markets. That role fell to Britain in the late 19th century and the US in the second half of the 20th century. But America’s self-confidence has been punctured by the rise of China as a strategic threat, and the power struggle between the world’s two biggest economies is heating up. Stock markets were jubilant when Washington and Beijing signed a trade deal but – rather like the Molotov-Ribbentrop pact of August 1939 – it is merely a truce of convenience. If Trump wins re-election in November, hostilities will resume. Countries also get defensive when times are tough, as they have been ever since the financial crisis of 2008. The most recent wave of openness occurred in the decade after the collapse of communism, culminating in China becoming a member of the WTO in 2001. This was a time when growth was strong, partly due to a globalisation feedback loop in which cheaper imports pushed down inflation rates and allowed central banks in the west to keep interest rates low and asset prices high. But the financial crisis exposed the weaknesses of a system that was able to operate globally without adequate controls and effective supervision. The resulting slump was deep and the recovery has been long, painful and incomplete. Inevitably, countries have become more cautious. That trend has been amplified because globalisation’s fruits have been enjoyed primarily – though not exclusively – by owners of capital and the better off. Consumers have gained from lower prices, but inequality has risen in every part of the world. In democracies, there is a limit to how long people will put up with the rich getting richer while their living standards are stagnating or barely growing. The hope – always somewhat sketchy – was that an international polity would be developed to match the internationalisation of economics. If capital could organise on a global level, the argument went, then democratic mechanisms could and would be developed too. This simply has not happened. The one multilateral institution created in the past three decades to manage globalisation – the WTO – is in a parlous state. The WTO has two main functions: as a forum where comprehensive trade deals are negotiated, and to provide a court where trade disputes between countries can be settled. It is currently doing neither. The failure to develop a transnational political response to globalisation has meant voters have demanded a response at a level where they have a voice: the nation state. Taking back control is proving to be a compelling rallying cry for the populists of the right such as Trump and the populists of the left such as Bernie Sanders. Neither man has much time for the WTO. Global heating looks certain to add to the deglobalisation pressure. The existential threat posed by the climate emergency is forcing governments, businesses and consumers to ask some questions about the way the global economy works. Is it sensible to ship car parts backwards and forwards across national borders or invest in fossil fuel companies? Is it sustainable to fly in fruit and veg from the other side of the world rather than grow it locally? Most fundamentally of all, are there more important things than economic growth? Globalisation was sold as a way of boosting prosperity for all by making markets bigger and more efficient. For a while the model worked, but when it blew up and caused extensive collateral damage, a backlash was inevitable. Deglobalisation is the result. • Larry Elliott is the Guardian’s economics editor"
"Mass melting of the West Antarctic ice sheet, driven by warmer ocean temperatures, was a major cause of extreme sea level rise more than 100,000 years ago, according to new research. A research team, led by scientists at the University of New South Wales, examined the cause of high sea levels during a period known as the last interglacial, which occurred 129,000-116,000 years ago.  Their study finds that melting of the West Antarctic ice sheet caused a sea-level rise of more than three metres and it took less than 2C of ocean warming for that to occur. The authors say their findings could have “major implications” for the future given the ocean warming and ice melt currently occurring in Antarctica. The study’s lead author Chris Turney is a climate change and earth scientist at UNSW. He said the West Antarctic was particularly vulnerable to ocean warming because it sits mostly on the sea bed, rather than on land. “This has been a big concern and is what the concern is in the present day,” Turney said. “So the question is how much could fall into the ocean and this is where the last interglacial [period] is so important.” The paper says ocean temperatures during the last interglacial were likely up to 2C warmer than they are today and global sea levels were 6-9 metres higher. To trace Antarctica’s potential contribution to this sea-level rise, the scientists travelled to West Antarctica to the Patriot Hills Blue Ice Area, which is on the periphery of the West Antarctic ice sheet. Blue ice areas are created by katabatic winds. When these winds blow over mountains, they remove snow and ice, allowing ancient ice to come to the surface. A lot of Antarctic research involves deep ice core drilling to study years of climate history. In this study, the researchers used what they called “horizontal ice core” analysis, which involved simply walking across the valley towards the mountain. “As you walk towards the mountain, you walk over increasingly older ice,” Turney said. They used some shallow drilling to take ice samples from the surface. Through isotope measurements, they found a gap in the ice sheet record immediately prior to the last interglacial. Turney said this gap coincided with an extreme rise in sea level and suggested a period in which there was no ice accumulating in that valley. “It means that a large part of the west Antarctic almost certainly disappeared in the last interglacial. It melted. It flowed rapidly into the ocean,” he said. He said the research also suggested this mass melting happened quite early during the ocean warming “somewhere between zero and 2C”. Countries have signed on to the Paris agreement which aims to keep global heating below 2C. Turney said the current summer in Australia alone had shown the dangers of a warming world just at 1C. He said the team’s research could be used to focus on which sections of West Antarctica are most vulnerable to the current climate crisis. “What these results suggest, or show, is that when people talk about a 2C warmer world as a good thing, actually what it shows is we don’t want to get close to 2C,” he said."
"

This Sunday, Al Gore will probably win an Academy Award for his global‐​warming documentary _An Inconvenient Truth_ , a riveting work of science fiction.



The main point of the movie is that, unless we do something very serious, very soon about carbon dioxide emissions, much of Greenland’s 630,000 cubic miles of ice is going to fall into the ocean, raising sea levels over twenty feet by the year 2100.



Where’s the scientific support for this claim? Certainly not in the recent Policymaker’s Summary from the United Nations’ much anticipated compendium on climate change. Under the U.N. Intergovernmental Panel on Climate Change’s medium‐​range emission scenario for greenhouse gases, a rise in sea level of between 8 and 17 inches is predicted by 2100. Gore’s film exaggerates the rise by about 2,000 percent.



Even 17 inches is likely to be high, because it assumes that the concentration of methane, an important greenhouse gas, is growing rapidly. Atmospheric methane concentration hasn’t changed appreciably for seven years, and Nobel Laureate Sherwood Rowland recently pronounced the IPCC’s methane emissions scenarios as “quite unlikely.”



Nonetheless, the top end of the U.N.‘s new projection is about 30‐​percent lower than it was in its last report in 2001. “The projections include a contribution due to increased ice flow from Greenland and Antarctica for the rates observed since 1993,” according to the IPCC, “but these flow rates could increase or decrease in the future.”



According to satellite data published in _Science_ in November 2005, Greenland was losing about 25 cubic miles of ice per year. Dividing that by 630,000 yields the annual percentage of ice loss, which, when multiplied by 100, shows that Greenland was shedding ice at 0.4 percent per century.



“Was” is the operative word. In early February, _Science_ published another paper showing that the recent acceleration of Greenland’s ice loss from its huge glaciers has suddenly reversed.



Nowhere in the traditionally refereed scientific literature do we find any support for Gore’s hypothesis. Instead, there’s an unrefereed editorial by NASA climate firebrand James E. Hansen, in the journal _Climate Change_ — edited by Steven Schneider, of Stanford University, who said in 1989 that scientists had to choose “the right balance between being effective and honest” about global warming — and a paper in the Proceedings of the National Academy of Sciences that was only reviewed by one person, chosen by the author, again Dr. Hansen.



These are the sources for the notion that we have only ten years to “do” something immediately to prevent an institutionalized tsunami. And given that Gore only conceived of his movie about two years ago, the real clock must be down to eight years!



It would be nice if my colleagues would actually level with politicians about various “solutions” for climate change. The Kyoto Protocol, if fulfilled by every signatory, would reduce global warming by 0.07 degrees Celsius per half‐​century. That’s too small to measure, because the earth’s temperature varies by more than that from year to year.



The Bingaman‐​Domenici bill in the Senate does less than Kyoto — i.e., less than nothing — for decades, before mandating larger cuts, which themselves will have only a minor effect out past somewhere around 2075. (Imagine, as a thought experiment, if the Senate of 1925 were to dictate our energy policy for today).



Mendacity on global warming is bipartisan. President Bush proposes that we replace 20 percent of our current gasoline consumption with ethanol over the next decade. But it’s well‐​known that even if we turned every kernel of American corn into ethanol, it would displace only 12 percent of our annual gasoline consumption. The effect on global warming, like Kyoto, would be too small to measure, though the U.S. would become the first nation in history to burn up its food supply to please a political mob.



And even if we figured out how to process cellulose into ethanol efficiently, only one‐​third of our greenhouse gas emissions come from transportation. Even the Pollyannish 20‐​percent displacement of gasoline would only reduce our total emissions by 7‐​percent below present levels — resulting in emissions about 20‐​percent higher than Kyoto allows.



And there’s other legislation out there, mandating, variously, emissions reductions of 50, 66, and 80 percent by 2050. How do we get there if we can’t even do Kyoto?



When it comes to global warming, apparently the truth is inconvenient. And it’s not just Gore’s movie that’s fiction. It’s the rhetoric of the Congress and the chief executive, too.
"
"
Share this...FacebookTwitterIt appears that major media outlets took reports based on old observations and passed it on to the public as breaking news last week.
H/T: Reader Dirk H.
Last week a number of major media outlets reported on how a large plume of oil had been spotted in the Gulf of Mexico, insisting that the BP oil spill was not naturally disappearing, as claimed by BP and US officials.
Well, it turns out that last week’s reports were based on oil plume observations made way back in June and junk science. Here are some of the headlines we saw last week in the land of angst, Germany:
Sueddeutsche Zeitung on August 18:
Scientists: 80% Of The Oil Is Still There
Der Spiegel on August 18;
Scientists Attack US Government’s Announcement That Worst Is Over
WDR5 German Radio on August 20:
Large Oil Cloud Beneath The Surface
Die Welt on August 21:
Platform Operator Accuses BP Of Cover-Up  and on August 20: 35 Kilometer Oil Slick
All these reports claimed that BP and officials were premature in calling off the emergency, and that the oil slick was far greater than the public was led to believe. There’s much more oil out there and the problem is still an environmental catastrophe, they all insisted last week.
What did these enviro-bedwetting journalists base their stories on?
They were based on a study by Woods Hole Oceanographic Institute describing an undersea oil cloud observed June 19 to 28. Here’s the Wood Hole press release. These media outlets were too lazy to check out the source.
So is there still lots of oil out there? No.
The Washington Post wrote yesterday on a Study: Petroleum-eating microbes significantly reduced gulf oil plume that according to the newest findings by a team of scientists led by Terry C. Hazen of the Lawrence Berkeley National Laboratory in California, the oil plumes are today practically gone, and on how microbes and bacteria have…



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




…reduced the amount of oil amounts in the undersea “plume” by half about every three days, according to research published online Tuesday by the journal Science.
The Washington Post also mentions how the Woods Hole study was based on observations from June.  The WaPo writes:
In the Woods Hole study, scientists described finding an undersea oil cloud June 23 to 27 similar to the one Hazen and his colleagues found between May 25 and June 2 – which was similar to one found soon after by people from the Montereyâ Bay Aquarium Research Institute.
And so the mystery of the missing oil is explained:
The findings, by a team of scientists led by Terry C. Hazen of the Lawrence Berkeley National Laboratory in California, help explain one of the biggest mysteries a mystery of the disaster: Where has all the oil gone?
‘What we know about the degradation rates fits with what we are seeing in the last three weeks,’ Hazen said. “We’ve gone out to the sites, and we don’t find any oil, but we do find the bacteria.”
The oil is gone. The WaPo report also shows that the feared oxygen depletion catastrophe is also all hype:
One thing that many scientists feared – severe depletion of oxygen as microbes consumed the oil – apparently hasn’t happened. The Woods Hole study published last week found no decrease in oxygen in the oil plume, and the new study found only a slight one.
No plume could be found in the past three weeks, however. The oil that remains appears to be too diluted to be detected.
There it is folks. Another bout of incontinence by the German media. Listening to them last week, you’d think the Gulf of Mexico was sloshing with crude. Bad reporting is a great way to wrongly scare away tourists at a time the Gulf coast needs them the most.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe July Arctic sea ice outlook for September is out. Click here.
Here’s a graphic of the prediction made by 16 different institutes this month, now that they are all 30 days wiser.
Now compare this to the projections made 30 days ago, late June.
Then again, some are incapable of learning anything. Anyway, at least five now concede that we may not even reach last year’s low. Strangely, after July’s slow melt, some have grown even more pessimistic.
Then again, optimism has never been a trait one finds in climate “science”. Who do you think will grab the headlines?
Share this...FacebookTwitter "
"A colleague at the UN’s Food and Agriculture Organization (FAO) tells a terrifying story about the desert locust. In 2005 she visited farmers in Niger as they prepared to harvest their crops. Just hours later, a swarm of locusts swept through the area and destroyed everything. One month later, truckloads of families were forced to leave their homes because they had nothing to eat. A year before that the UN had launched an appeal for $9m (£6.9m) to help Niger and neighbouring countries control the locusts. The response was slow, and six months later the amount required in the appeal had reached $100m. The maths was simple: the locusts were faster than the international response. History is now in danger of repeating itself. But on a much bigger scale. The worst outbreak of desert locusts in decades is currently underway in the Horn of Africa. It is the biggest of its kind in 25 years for Ethiopia and Somalia – and the worst Kenya has seen for 70 years. The impacts of the outbreak in these countries are particularly acute as pastures and crops are being wiped out in communities that were already facing food shortages. As we write, the swarms have just crossed into Uganda and Tanzania, and moved within 50km (31 miles) of South Sudan. Djibouti and Eritrea are also affected. And Oman, Saudi Arabia, Sudan, Yemen, and Pakistan are fighting their own serious infestations. The desert locust is considered the world’s most destructive migratory pest. A single locust can travel 150km and eat its own weight in food – about two grams – each day. A swarm the size of New York City can consume the same amount of food in one day as the total population of New York, New Jersey and Pennsylvania. What we are seeing in East Africa today is unlike anything we’ve seen in a very long time. Its destructive potential is enormous, and it’s taking place in a region where farmers need every gram of food to feed themselves and their families. Most of the countries hardest hit are those where millions of people are already vulnerable or in serious humanitarian need, as they endure the impact of violence, drought, and floods. We have acted quickly to respond to this upsurge. Local and national governments in East Africa are leading the response, and our respective offices are working closely together to keep this outbreak under control. The UN’s Office for the Coordination of Humanitarian Affairs has released $10 million from its Central Emergency Relief Fund to fund a huge scale-up in aerial operations to manage the outbreak. The FAO is urgently seeking $76 million from donors and other organisations to help the affected countries fight the outbreak. The amount required is likely to increase as the locusts spread. But the window to contain this crisis is closing fast. We only have until the beginning of March to bring this infestation under control as that is when the rain and planting season begins. The swarms are highly mobile; the terrain often difficult; the logistical challenges immense. But left unchecked – and with expected additional rains – locust numbers in East Africa could increase 500 times by June. We must act now to avoid a full-blown catastrophe. And we will. At the same time, we need to pay attention to a bigger picture. This is not the first time the Greater Horn of Africa has seen locust upsurges approach this scale, but the current situation is the largest in decades. This is linked to climate change. Warmer seas mean more cyclones, generating the perfect breeding conditions for locusts. Together, we express deep solidarity with the people and communities affected. And we call on the international community to respond with speed and generosity to control the infestation while we still have the chance. • Qu Dongyu is director-general of the UN Food and Agriculture Organization; Mark Lowcock is the UN under-secretary-general for Humanitarian Affairs and Emergency Relief Coordinator."
"Former environment minister Owen Paterson has called for the UK to scrap its climate change targets. In a speech to the Global Warming Policy Foundation, he cited “considerable uncertainty” over the impact of carbon emissions on global warming, a line that was displayed prominently in coverage by the Telegraph and the Daily Mail. Paterson is far from alone: climate change debate has been suffused with appeals to “uncertainty” to delay policy action. Who hasn’t heard politicians or media personalities use uncertainty associated with some aspects of climate change to claim that the science is “not settled”? Over in the US, this sort of thinking pops up quite often in the opinion pages of The Wall Street Journal. Its most recent article, by Professor Judith Curry, concludes that the ostensibly slowed rate of recent warming gives us “more time to find ways to decarbonise the economy affordably.” At first glance, avoiding interference with the global economy may seem advisable when there is uncertainty about the future rate of warming or the severity of its consequences. But delaying action because the facts are presumed to be unreliable reflects a misunderstanding of the science of uncertainty. Simply because a crucial parameter such as the climate system’s sensitivity to greenhouse gas emissions is expressed as a range – for example, that under some emissions scenarios we will experience 2.6°C to 4.8ºC of global warming or 0.3 to 1.7 m of sea level rise by 2100 – does not mean that the underlying science is poorly understood.  We are very confident that temperatures and sea levels will rise by a considerable amount. Perhaps more importantly, just because some aspects of climate change are difficult to predict (will your county experience more intense floods in a warmer world, or will the floods occur down the road?) does not negate our wider understanding of the climate. We can’t yet predict the floods of the future but we do know that precipitation will be more intense because more water will be stored in the atmosphere on a warmer planet. This idea of uncertainty might be embedded deeply within science but is no one’s friend and it should be minimised to the greatest extent possible. It is an impetus to mitigative action rather than a reason for complacency. There are three key aspects of scientific uncertainty surrounding climate change projections that exacerbate rather than ameliorate the risks to our future. First, uncertainty has an asymmetrical effect on many climatic quantities. For example, a quantity known as Earth system sensitivity, which tells us how much the planet warms for each doubling of atmospheric carbon dioxide concentration, has been estimated to be between 1.5°C to 4.5ºC. However, it is highly unlikely, given the well-established understanding of how carbon dioxide absorbs long-wave radiation, that this value can be below 1ºC.  There is a possibility, however, that sensitivity could be higher than 4.5ºC.  For fundamental mathematical reasons, the uncertainty favours greater, rather than smaller, climate impacts than a simple range suggests. Second, the uncertainty in our projections makes adaptation to climate change more expensive and challenging.  Suppose we need to build flood defences for a coastal English town. If we could forecast a 1m sea level rise by 2100 without any uncertainty, the town could confidently build flood barriers 1m higher than they are today.  However, although sea levels are most likely to rise by about 1m, we’re really looking at a range between 0.3m and 1.7m. Therefore, flood defences must be at least 1.7m higher than today – 70cm higher than they could be in the absence of uncertainty. And as uncertainty increases, so does the required height of flood defences for non-negotiable mathematical reasons.  And the problem doesn’t end there, as there is further uncertainty in forecasts of rainfall occurrence, intensity and storm surges.  This could ultimately mandate a 2 to 3m-high flood defence to stay on the safe side, even if the most likely prediction is for only a 1m sea-level rise. Even then, as most uncertainty ranges are for 95% confidence, there is a 5% chance that those walls would still be too low. Maybe a town is willing to accept a 5% chance of a breach, but a nuclear power station cannot to take such risks. Finally, some global warming consequences are associated with deep, so-called systemic uncertainty. For example, the combined impact on coral reefs of warmer oceans, more acidic waters and coastal run-off that becomes more silt-choked from more intense rainfalls is very difficult to predict. But we do know, from decades of study of complex systems, that those deep uncertainties may camouflage particularly grave risks. This is particularly concerning given that more than 2.6 billion people depend on the oceans as their primary source of protein. Similarly, warming of Arctic permafrost could promote the growth of CO2-sequestering plants, the release of warming-accelerating methane, or both. Warm worlds with very high levels of carbon dioxide did exist in the very distant past and these earlier worlds provide some insight into the response of the Earth system; however, we are accelerating into this new world at a rate that is unprecedented in Earth history, creating additional layers of complexity and uncertainty. Increasingly, arguments against climate mitigation are phrased as “I accept that humans are increasing CO2 levels and that this will cause some warming but climate is so complicated we cannot understand what the impacts of that warming will be.” This argument is incorrect – uncertainty does not imply ignorance. Indeed, whatever we don’t know mandates caution. No parent would argue “I accept that if my child kicks lions, this will irritate them, but a range of factors will dictate how the lions respond; therefore I will not stop my child from kicking lions.”  The deeper the uncertainty, the more greenhouse gas emissions should be perceived as a wild and poorly understood gamble. By extension, the only unequivocal tool for minimising climate change uncertainty is to decrease our greenhouse gas emissions."
"
Recently we’ve been discussing products from the AIRS satellite instrument (Atmospheric InfraRed Sounder) onboard the Aqua satellite. There has been quite a bit of interest in this because unlike the satellite temperature record that goes back to 1979, until now we have not had a complementary satellite derived CO2 record. We are about to have one, and much more.

Click image to see a slide show with this graphic in it (PDF)
I wrote to the AIRS team to inquire about when the satellite data on CO2, and other relevant products might be made public. All that has been released so far are occasional snippets of data and imagery, such as the short slide show above.
Here is the response I got from them:
Thank you for your interest in the AIRS CO2 data product.
We are still in the validation phase in developing this new product.
It will be part of the Version 6 data release, but for now those of us
working on it are intensively validating our results using in situ
measurements by aircraft and upward looking fourier transform IR
spectrometers (TCCON network and others).
The AIRS CO2 product is for the mid-troposphere. For quite some time
it was accepted theory that CO2 in the free troposphere is
“well-mixed”, i.e., the difference that might be seen at that altitude
would be a fraction of a part per million (ppmv). Models, which
ingest surface fluxes from known sources, have long predicted a smooth
(small)variation with latitude, with steadily diminishing CO2 as you
move farther South. We have a “two-planet” planet – land in the
Northern Hemisphere and ocean in the Southern Hemisphere. Synoptic
weather in the NH can be seen to control the distribution of CO2 in
the free troposphere. The SH large-scale action is mostly zonal.
Since our results are at variance with what is commonly accepted by he
scientific community, we must work especially hard to validate them.
We have just had a paper accepted by Geophysical Research Letters that
will be published in 6-8 weeks, and are preparing a validation paper.
We have global CO2 retrievals (day and night, over ocean and land, for
clear and cloudy scenes) spanning the time period from Sept 2002 to
the present. Those data will be released as we satisfactorily
validate them.
I suggest you Google “Carbon Tracker” for some interesting maps
generated using model atmospheres and data for CO2 sources. It shows
the CO2 weather in the lowest part of the atmosphere.
The big picture is that CO2 sources and sinks are in the planetary
boundary layer. Global circulation of CO2 occurs in the free
troposphere. Thus, PBL is local whereas free troposphere is
international.
———-
AIRS Team
With the suggestion of using the Google “Carbon tracker”, some readers might look at this response as a “dodge”. I don’t see it that way at all. Why? Because they are actively engaged in proving the instrument by doing a series of aircraft based measurements to validate the data the instrument on the spacecraft is seeing.
For example, read this paper from them:
First Satellite Remote Sounding of the Global Mid-Tropospheric CO2
These graphics show how hard they are working to validate the data from in situ measurements using airborne flask samples sent to a lab spectrometer:

…and the results of the flask sample measurements:

Read more about this here in this paper (PDF)
Also if you read between the lines in their response to me, particularly this paragraph:
Since our results are at variance with what is commonly accepted by he
scientific community, we must work especially hard to validate them.
We have just had a paper accepted by Geophysical Research Letters that
will be published in 6-8 weeks, and are preparing a validation paper.
I’d say that waiting that 6-8 weeks for the paper and supporting data will be well worth it.  The working title of the upcoming paper is: “Satellite Remote Sounding of Mid-Tropospheric  CO2” and the lead author is Moustafa T. Chahine.
Good things come to those who wait.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d78f2bb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From NASA Science News h/t to John-X
Spotless Sun: 2008 is the Blankest Year of the Space Age
Sept. 30, 2008: Astronomers who count sunspots have announced that 2008 is now the “blankest year” of the Space Age.
As                      of Sept. 27, 2008, the sun had been blank, i.e.,                      had no visible sunspots, on 200 days of the year. To find                      a year with more blank suns, you have to go back to 1954,                      three years before the launch of Sputnik, when the sun was                      blank 241 times.
“Sunspot counts are at a 50-year low,” says solar physicist David Hathaway of the NASA Marshall Space Flight Center. “We’re experiencing a deep minimum of the solar cycle.”

Above: A histogram showing the blankest years of the last half-century. The vertical axis is a count of spotless days in each year. The bar for 2008, which was updated on Sept. 27th, is still growing. [Larger images: 50                      years, 100 years]
A spotless day                      looks like this:

The image, taken by the Solar and Heliospheric Observatory (SOHO) on Sept. 27, 2008, shows a solar disk completely unmarked by sunspots. For comparison, a SOHO image taken seven years earlier on Sept. 27, 2001, is peppered with colossal sunspots, all crackling with solar flares: image. The difference is the phase of the 11-year solar cycle. 2001 was a year of solar maximum, with lots of sunspots, solar flares and geomagnetic storms. 2008 is at the cycle’s opposite extreme, solar minimum, a quiet time on the sun.
And                      it is a very quiet time. If solar activity continues as low as it has been, 2008 could rack up a whopping 290 spotless days by the end of December, making it a century-level year in terms of spotlessness.
Hathaway cautions that this development may sound more exciting than it actually is: “While the solar minimum of 2008 is shaping up to be the deepest of the Space Age, it is still unremarkable compared to the long and deep solar minima of the late 19th and early 20th centuries.” Those earlier minima routinely racked up 200 to 300 spotless days per year.
Some                      solar physicists are welcoming the lull.
“This gives us a chance to study the sun without the complications of sunspots,” says Dean Pesnell of the Goddard Space Flight Center. “Right now we have the best instrumentation in history looking at the sun. There is a whole fleet of spacecraft devoted to solar physics–SOHO, Hinode, ACE, STEREO and others. We’re bound to learn new things during this long solar minimum.”
As an example he offers helioseismology: “By monitoring the sun’s vibrating surface, helioseismologists can probe the stellar interior in much the same way geologists use earthquakes to probe inside Earth. With sunspots out of the way, we gain a better view of the sun’s subsurface winds and inner magnetic dynamo.””There is also the matter of solar irradiance,” adds Pesnell. “Researchers are now seeing the dimmest sun in their records. The change is small, just a fraction of a percent, but significant. Questions about effects on climate are natural if the sun continues to dim.”
Pesnell is NASA’s project scientist for the Solar Dynamics Observatory (SDO), a new spacecraft equipped to study both solar irradiance and helioseismic waves. Construction of SDO is complete, he says, and it has passed pre-launch vibration and thermal testing. “We are ready to launch! Solar minimum is a great time to go.”
Coinciding with the string of blank suns is a 50-year record low in solar wind pressure, a recent discovery of the Ulysses spacecraft. (See the Science@NASA story Solar                      Wind Loses Pressure.) The pressure drop began years before the current minimum, so it is unclear how the two phenomena are connected, if at all. This is another mystery for SDO and the others.
Who                      knew the blank sun could be so interesting?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c0c0072',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
It never ceases to amaze me how people think when it comes to the Arctic. Somehow there is this pervasive belief that “if we just go there and document it, we’ll be able to demonstrate how climate change is affecting the arctic”.  This is the second team with such dubious aspirations this year, the first being failed kayaker Lewis Gordon Pugh who spun his dismal and embarrassing failure into an “accomplishment”, and then would not even take valid questions about his false claim of being the person who “kayaked furthest north”.
I have no sympathy for these people. Nature is teaching them hard lessons, let us hope they retain the material. – Anthony

STUCK IN THE ARCTIC FOR THREE WEEKS…AND COUNTING
Posted: 	Friday, September 26, 2008 8:20 AM by Jen Brown
From Peter Alexander, TODAY correspondent
So, here we are. In the Arctic. Day 23. Good times!
Producer Paul Manson and I, along with cameraman Callan Griffiths and soundman Ben Adam, were sent here on assignment to report on climate change and the Arctic for an upcoming broadcast. The primary news peg — and one reason for our visit — is that for only the second time in recorded history the Northwest Passage is ice free, effectively clearing this shortcut between Europe and Asia.
Our intention was to stay on board for 10 days, shooting video and interviews.  Mother Nature, apparently, had other plans. Inclement weather, along with an emergency search and rescue mission, has spoiled all five of our attempts to leave the ship.  Getting stuck in the Arctic is not uncommon; getting stuck five times is like punishment.
Joining the team
We left NYC Sept. 3, joining up with a team of scientists from ArcticNet on board the Canadian Coast Guard icebreaker, Amundsen. (In Canada, the Coast Guard is civilian, not military. It is part of the country’s Department of Oceans and Fisheries.) This particular Coast Guard ship has been dedicated to scientific research and outfitted with all the necessary tools. In a unique partnership, the scientists work side-by-side with the Coast Guard crew. For example, the scientists are testing water samples and sediment samples (from the ocean floor) as well as mapping uncharted territories in this remote part of the world. There are 40 scientists, 40 Coast Guard members and the four of us. By now we’re part of the team, learning to help on deck, in the lab and at dinner.
We boarded the Amundsen Thursday, Sept. 4, in Resolute Bay, a small Inuit village, along the Northwest Passage. The plan was to fly off by helicopter at the northern most civilian community in North America, Grise Fjord, and then begin our long journey home. Freezing rain and harsh weather kept our chopper grounded both Monday and Tuesday. The ship kept going and our chance to get off passed. We continued North with the expedition along the coasts of the Canadian Arctic and Greenland, coming within 900 miles of the North Pole.
Over the next couple weeks, we would make three more attempts to fly to land. Each one failed due to weather. Unbelievably, on Thursday our absolute best chance to get off the ship failed, too. The ship was diverted back north to assist a search and rescue mission, something the crew says has only happened once or twice in the last couple years.  From the beginning, we were warned that the ships primary mission was science. The cost of operating this icebreaker and moving the expedition forward is $50,000 a day. While we’ve been welcomed guests on board, we knew the ship wouldn’t be stopping for us. 
Close quarters
Paul and I have been sharing what would normally be the infirmary on this overloaded ship. To our eye, it’s roughly, 10 by 12 feet. A thin curtain is the only thing separating us — and our dignity. Callan and Ben share a bunk bed in a slighter larger room downstairs.






Soundman Ben Adam, producer Paul Manson, cameraman Callan Griffiths and correspondent Peter Alexander



In our 23 days on the ship we have covered more than 2,500 miles. The ship rocks incessantly and a sonar machine used for ocean floor mapping ticks loudly all day and night. It’s akin to being audibly poked day in and day out. (Callan has lovingly promised to buy each of us a metronome when we get home so that will be able to sleep as comfortably in NYC.)
Since we were done shooting two weeks ago, we’ve been left with a lot of time to fill. Meals have become a priority. It’s often the only way we can keep track of what time and day it is. Thursday is a favorite — breakfast crepes. Speaking of crepes, we’ll remind you this is a French-Canadian ship, and so we’ve been more than well fed. In fact, we’re convinced Fabien, the ships pastry chef — yes, I said pastry chef — is trying to kill us slowly with desserts.
Meals are always heavy and large. (Now, so are some of us.) But fear not, there is a fitness club on board. Let us describe it for you: it’s half the size of our bedroom (read: infirmary), and consists of a treadmill, two bikes and a bench that’s hidden beneath a four-foot ceiling. (Running on a treadmill when the ship is rocking could easily pass as its own Olympic sport.) Not to worry, we’ve now collectively run or biked the length of Greenland six times over. The other hours have been spent staring at the ocean, staring in the abyss and staring at each other — followed by routine games of Scrabble, “what’s for dinner?” and “if you could be any kind of animal, what would you be?”
A once-in-a-lifetime experience
Let’s be clear, although we’ve been mentally ready to leave for a long time now, we have seen and done some extraordinary things, including meeting some inspiring scientists whose dedication to their field reminds us daily why we’re here. We’ve seen polar bears, beluga whales and icebergs the size of floating hotels. Each sighting reminds us how far away we are from home. In addition, we’ve seen sea creatures from far below the ocean’s surface that would rival the characters at the Star Wars bar.
The scenery is both breathtaking and intimidating. We’ve been awed by sights that most people will never see and appreciate that this is a once-in-a-lifetime experience. (Hopefully.)
VIDEO: Peter Alexander and Paul Manson phone home to describe the (mis)adventures


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c3cfdff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This sums the banking issue well.
“Is anyone even paying attention to these Wing Nut AGW people? With 1/2 of America worried about having to eat cat food during their retirement, global warming is the last thing on their mind.”
From “Jeff” in comments


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c2e4a48',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"France is to restrict access to Mont Blanc in the Alps in an effort to halt reckless summit attempts and protect the biodiversity of the mountain and its surroundings. Emmanuel Macron announced the new rules during a visit to Chamonix on Thursday when he visited the famous Mer de Glace (Sea of Ice) glacier, which has shrunk dramatically over the last 20 years.  “What we see with this glacier melting is irrefutable evidence of global warming,” Macron said after visiting the 4.7-mile (7.5km) glacier. He said the fight for biodiversity was urgent and a “fight for our own survival”. “It’s the battle of the century,” Macron said. He announced a new agency, the French Office of Biodiversity (OFB), to monitor and protect the environment. The French government is to establish a protected zone around Mont Blanc and limit the number of people who can access the summit, which at just over 4,800 metres is the highest in western Europe. Climbers will also be asked for proof they have planned their ascent, that they have reserved places at reservations en route and are carrying specified equipment. Macron’s visit came after Jean-Marc Peillex, mayor of Saint Gervais – near Contamines-Montjoie, where a coronavirus outbreak was reported last week – wrote to the Elysée claiming the free-for-all on the mountain was risking lives and damaging the environment. “It is all well and good to worry about the Amazon rainforest, but to ignore what is happening on Mont Blanc and to allow this disrespect to continue is intolerable,” Peillex wrote last September. He said “oddballs” were polluting the mountain, citing a recent attempt by a former British Royal Marine to summit Mont Blanc for charity while carrying a full-sized rowing machine, which he abandoned on the path down. As many as 30,000 people attempt to climb Mont Blanc every year – around 200-300 a day. Some ignore weather and safety warnings and many leave rubbish on the mountain. Peillex continued: “The difficulty is that if we start listing everyone’s madcap schemes then we’ll finish up with an endless list. There will always be someone willing to do something new or original that new rules haven’t imagined. The idea is to be reasonable. What’s this site for? Is it a place for opera or to advertise desserts? Isn’t it better to give it back its original vocation for mountaineering and skiing? If the president supports this kind of thinking we will have won. “Today, 99% of people are shocked by what happens on the summit of Mont Blanc … Let us allow people to go on Mont Blanc, but let us set the rules. When you go to someone’s house, you’re not the one who sets the rules. You don’t put your feet on their table. It’s the same here.” The Mer de Glace has lost more than 65 metres in depth and 300 metres in length since 1996."
"
Share this...FacebookTwitterDi kesempatan kali ini , Agen Poker online Indonesia akan mengemukakan beberapa info berkaitan bagaimanakah cara untuk Tingkatkan Kesempatan Menang Dalam Bermain Poker itu. Info kesempatan ini bisa kalian aplikasikan baik bagi kalian yang masih pemula dalam bermain poker atau kalian yang sudah pakar dalam bermain poker bentuk online itu. Mengenai info itu bisa kalian lihat serta kalian baca seperti berikut ini :
Mengerti basic dari permainan. Permainan apa pun yang kalian mainkan semua tentu mempunyai yang namanya ketentuan serta basic dari permainan tersebut tidak kecuali permainan poker itu. Permainan poker itu biasanya sama juga dengan permainan poker berbentuk online. Karena itu, jika kalian sudah kuasai permainan poker berbentuk konvesional itu tentu saja kuasai permainan poker berbentuk online itu.
Cara Jitu Dalam Bermain Poker Online Indonesia
Cobalah mengingat rutinitas musuh. Ini ialah langkah paling akhir yang dapat kalian kerjakan dalam bermain poker online, yakni dengan memperhatikan gerak musuh kalian seperti rutinitas musuh kalian. Ini bermanfaat untuk kalian dalam memastikan langkah apa yang perlu kalian kerjakan waktu musuh tengah lakukan kesukaannya itu. Satu diantara contoh dari hal itu bisa kalian lihat dari kartu yang sedang digengam oleh pemain itu. Jika kartu yang dipegangnnya itu ialah kartu yang lumayan bagus, tentu raut muka atau ekspresi yang diperlihatkan itu ialah raut muka suka dan lain-lain.
Dana taruhan. Jika kalian sudah lakukan 2 hal seperti yang kami berikan di atas itu langkah paling akhir yang perlu kalian kerjakan ialah mulai mempersiapkan dana yang cukup waktu bermain poker online itu pada bandar yang kalian percayai. Coba untuk mempersiapkan dana yang cukup di luar dari dana keperluan inti atau dana genting sebab ke-2 dana itu sebenarnya penting.
Hindari Berekspresi Berlebihan Dalam Bermain Poker Online Indonesia
Jauhi untuk berekspresi terlalu berlebih. Ini hal yang paling penting dalam bermain Poker Online Indonesia sebab beberapa pemain tentu bisa memeperkirakan kartu apa yang ada ditangan kalian hanya dengan melihat ekpresi kalian. Tentu saja kalian akan terlihat suka waktu tengah memperoleh kartu yang bagus serta begitupun sebaliknya. Untuk menghindari hal yang tidak dapat ditebak, lebih baik kalian menghindari berekspresi terlalu berlebihan saat bermain hingga musuh kalian tidak bisa memprediksi siapa serta kartu apa yang ada ditangan kalian sekarang.
Demikian beberapa info yang dapat kami berikan berkaitan bagaimanakah cara untuk tingkatkan kesempatan dalam bermain poker di situs poker online sah itu. Mudah-mudahan lewat info yang kami berikan itu bisa menolong kalian dalam bermain poker itu hingga potensi kalian dalam bermain poker itu makin baik daripada awalnya. Serta mudah-mudahan lewat info itu dapat juga tingkatkan kesempatan kalian dalam bermain poker itu hingga prosentase kemenangan kalian dalam bermain makin tinggi dibanding awalnya.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterJohn Kerry is a co-sponsor of the latest Cap & Trade bill, which thank God has been put off indefinitely. The bill, should it become law, God forbid, would force all working Americans to pay more for energy and to live more humbly.
But living humbly and paying more taxes to the government applies only to the little guys. For the rich, elite and privileged, like Kerry, they’d continue to fly around in private jets, be chauffeured in limousines and frolic the seas in big yachts. Watch Kerry’s reaction when confronted about skirting Massachusetts taxes by birthing his new 76 foot $7 million yacht in Rhode Island:
[SEE VIDEO]
By berthing the yacht in Rhode Island, Kerry skirts paying nearly $450,000 in sales tax and a yearly $70,000 excise tax bill to his own home state. Look at how pissy he gets when confronted by the media.
Worse, Kerry, who claims to be fighting for American jobs, had the yacht built in New Zealand. The same yacht could have been built at a yard in his own state of Massachusetts. So much for American jobs. And what about environmental friendliness?
What does a 76-foot yacht include?
According to the Boston Herald, Kerry’s yacht has two cabins, a pilot house fitted with a wet bar and cold wine storage.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I wasn’t able to get further specific information on Kerry’s yacht, but looking at other typical 76-foot yachts on the market, we find that they are far from spartan. Take this 76-foot Monte Fino for only $2,450,000 – a real bargain when compared to Kerry’s $7 million cruiser. The Monte Fino includes a 10,000 liter diesel fuel tank, 1500 hp twin engines and is filled with high-tech electronic doo-dads.
God knows what Kerry got for his $7 million.
Kerry spokesman David Wade said Friday the boat is being kept at Newport Shipyard not to evade taxes, but “for long-term maintenance, upkeep and charter purposes.”
John Kerry is married to Theresa Heinz, who is millionaire heiress to the Heinz ketchup fortune, and is a philanthropist and environmentalist.
When you’re rich, you can do things like this. It’s okay. But these rich people should not be making it much harder for the rest of us to make ends meet.
When confronted by the media in the above clip, Kerry defends berthing the floating palace in tax-haven Rhode island, claiming he is paying his taxes. Then he scuttles away – not on a bicycle or in a hybrid car – but in a chauffeured SUV.
“Can I get outa here please!”, he orders his chauffeur.
And let’s not forget Sen. Jeff Greene and 145 ft yacht dragging anchor through coral reef: http://www.miamiherald.com/2010/07/23/1743175/greene-denies-his-anchor-damaged.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterToday I’m coming out a day early and declaring July 2010 as the slowest melting July since the AMSR-E satellite record has been kept. The once ballyhooed “death spiral” is dead.
Reminds me of that line in Tarantino’s cult film Pulp Fiction:
“Who’s Zed?”
“Zed? Zed is dead.”
At the end of June I recall seeing lots of headlines in the newspapers about a record Arctic sea ice melt occurring. Words like “alarming” and “unprecedented” were used liberally. The reports were splashed with pictures of polar bears for added effect.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




One month later the media are completely silent. As the following graphic shows, this July’s Arctic sea ice melt was the slowest since this dataset has been kept.  Click Here.
Here are the numbers for the amount of July-melt in million square kilometers:
Year      6/30 to 7/30
2003           2.25
2004           2.08
2005           2.52
2006           2.11
2007           3.00
2008           2.45
2009           2.81
2010          1.85
It was the first time that July failed to reach 2 million sq. km. Now 2010 is on track to reach last year’s low. So far the Arctic has been cold this summer, one of the coldest summers north of 80°N on record, Click Here.
What’s the forecast?
Meteorologist Joe Bastardi projects a significant Arctic sea ice recovery in the couple of years ahead, flying in the face of predictions made by climate “scientists”. Bastardi’s claim is in line with the latest NOAA seasonal forecasts.
NWS/NCEP forecasts a cold Arctic in the months ahead.
La Nina is strengthening and global temps, dare I say, are beginning a death-spiral of their own.
Share this...FacebookTwitter "
"Delta announced an ambitious plan on Friday to become the first US airline to go carbon neutral, committing $1bn over the next 10 years to mitigate all emissions from its global business. The move by Delta will put pressure on other airlines to follow suit at a time when the UN is warning that airplane emissions of carbon dioxide will triple by 2050.  Carbon dioxide emitted by airlines increased by 32% from 2013 to 2018, according to a study by the International Council on Clean Transportation. A Guardian analysis found long-haul flights generate more carbon emissions than the average person in dozens of countries around the world produces in a year. “There’s no challenge we face that is in greater need of innovation than environmental sustainability, and we know there is no single solution. We are digging deep into the issues, examining every corner of our business, engaging experts, building coalitions, fostering partnerships and driving innovation,” said Ed Bastian, Delta’s chief executive.  Delta outlined a series of efforts it will use to reach its goal: The company will seek to reduce its carbon footprint by decreasing the use of jet fuel and increasing efficiency. Delta will seek to offset its carbon emissions by investing in carbon removal programs in forestry, wetland restoration, grassland conservation, marine and soil capture and other negative emissions technologies. The company will seek coalitions with its employees, suppliers, global partners, customers, industry colleagues, investors and other stakeholders to reduce their carbon footprint. Delta is the latest big company in a heavily polluting industry to announce plans to go carbon neutral. This week energy company BP announced plans to reduce its carbon footprint to net zero by 2050, which has been met with skepticism by climate activists. Peter Miller, carbon offsets expert at the Natural Resources Defense Council, said he was encouraged to see Delta spending money to reduce its carbon footprint but said that there were at present few low-carbon options available at scale for airline companies. “It will be critical that the offsets they purchase are credible and real,” he said. “And that is certainly possible. It’s a whole lot better than doing nothing.”"
"

Mr. Chairman, distinguished members of the subcommittee:



My name is Roger Pilon. I am a senior fellow at the Cato Institute and the director of Cato’s Center for Constitutional Studies.



I want to thank Chairman Hyde of the committee and Chairman Coble of the subcommittee for their invitations to me to testify on the important issue of “Judicial Misconduct and Discipline.” These hearings have been called, I understand, because of a concern that a number of people have expressed about “judicial activism”–the practice by judges of applying to cases before them not the law but principles or values that are no part of the law. Because such a practice is thought by many to constitute judicial misconduct, some in Congress are searching for ways to discipline it.



 **I. Summary**



At the outset, let me summarize my thoughts on this subject, then discuss it in somewhat more detail. There can be no question that judicial activism, as just described, has been a problem in our legal system for some time. The power of the judiciary under our Constitution to declare the law and decide cases under that law is awesome; when abused, that power is too often beyond reach. At the same time, I believe that many of those who have complained most often about judicial activism have overstated and misstated the problem, thus distracting us from the real issue–legislative activism on the part of Congress, which leads to judicial activism.



Overstating the problem. Many of the examples of “judicial activism” that are cited turn out, when examined more closely, not to be cases in which the judge failed to apply the law but applied the law differently, or applied different law, to reach a result different than the result thought correct by the person charging activism. To be sure, there is no bright line between failing to apply the law and wrongly applying the law or applying the wrong law, but when that distinction is drawn, it turns out that there are fewer cases of true judicial activism than at first may appear.



Misstating the problem. More importantly, the problem of “judicial activism” is seriously misstated when it is cast, as it often is, as involving judges overruling the will of the people. In our legal system, judicial review often requires a judge to do just that. In such a case, were the judge to defer to the political will, exercising “judicial restraint” when the law requires active judicial intercession, that restraint would itself be a kind of activism, for it would amount to an “active” failure to apply the law in deference to democratic or majoritarian values. The judge in such circumstances would be shirking his judicial responsibilities every bit as much as if he overrode a legitimate exercise of political will in the name of other values.



Thus, as terms of art, judicial “activism” and “restraint” can be quite confusing and even misleading. What is more, they are often used in ways that camouflage the real issues. What we all want, I assume, is judges who are neither “active” nor “restrained” but “responsible”–responsible to the law. But when the law is unclear or inconsistent, judicial responsibility may be difficult to achieve–and “activism” inevitable. In the end, therefore, our substantive law may be the ultimate source of the problem before us today. That, in fact, is what I will argue shortly. Let me begin, however, with a brief overview of the complaints. [1]



 **II. The Critics of Judicial Activism**



Complaints about “judicial activism,” however formulated, can be found from our inception as a nation. In their modern form, however, they have come largely since the advent of the Warren Court and most often from political conservatives. My fellow panelist today, Professor Lino Graglia, with whom I have debated the issue more than once, has put the complaint starkly:



… the thing to know to fully understand contemporary constitutional law is that, almost without exception, the effect of rulings of unconstitutionality over the past four decades has been to enact the policy preferences of the cultural elite on the far left of the American political spectrum. [2]



“That is exactly right,” comments Judge Robert Bork in his recent best‐​seller, _Slouching Towards Gomorrah_ , “and the question is what, if anything, can be done about it.” [3] I gather that these hearings are a partial answer to that question.



The bitter confirmation battle that followed Judge Bork’s Supreme Court nomination a decade ago had a way of concentrating the issue for many, of course. Still, the issue has been in the air since the 1950s, covering subjects as various as civil rights, apportionment, federalism, speech, religion, abortion, education, criminal law and procedure, and much else. And in each case, the complaints from conservatives have been essentially the same.



Speaking before the Federalist Society’s 10th anniversary lawyers convention last November, for example, Senator Orrin Hatch, chairman of the Senate Judiciary Committee, summarized the issue from his perspective:



What is at stake … is nothing less than our right to democratic self‐​government as opposed to … “Government by Judiciary.” For when we commission judicial activists who distort the Constitution to impose their own values, policy preferences, or visions of what is just or right, we are in effect sacrificing our ability to govern ourselves through the democratic political processes to the whims and preferences of unelected, life‐​tenured platonic guardians. [4]



Judges “must _interpret_ the law, not legislate from the bench,” Senator Hatch continued. “A judicial activist, on the left _or_ the right, is not, in my view, qualified to sit on the federal bench.” [5]



In a similar vein, little more than two months ago Senator John Ashcroft, chairman of the Constitution Subcommittee of the Senate Judiciary Committee, told the Conservative Political Action Conference at its annual meeting that it was time “to take a broader, comprehensive look at the alarming increase in activism on the court.” [6] Asking what we can do to put an end to “judicial tyranny,” Senator Ashcroft called for rejecting “judges who are willing to place private preferences above the people’s will.” [7]



Not to be outdone by the Senate, on March 11 House Majority Whip Tom DeLay told editors and reporters at the _Washington Times_ that “as part of our conservative efforts against judicial activism, we are going after judges” and are “right now” writing articles of impeachment. [8] Those sentiments were echoed two days later by Congressman Bob Barr of this subcommittee when he appeared on CNN’s “Crossfire.” Clearly, perhaps as never before, the issue of judicial activism is on the nation’s agenda. [9]



 **III. Overstating the Problem**



It is not entirely clear just what has brought the judiciary and its methods to the nation’s attention at this point in time. Cynics point to the need for something–some issue–in a drifting Republican Party: “The revolution is in the doldrums. Nobody’s got a plan; nobody’s got a direction.” [10] Others, however, have noted a rising frustration among conservatives over their relative ineffectiveness on the judicial front despite having dominated the judicial selection process since the Nixon years. [11] And still others cite a series of recent cases that have seemed to crystalize complaints about judicial activism: the district judge who stayed the California Civil Rights Initiative (CCRI); [12] the New York judge who suppressed evidence in a drug case, saying the police had no reason to stop the suspects; [13] the decision by the Supreme Court that the Virginia Military Institute had to become coeducational. [14]



Looked at in broad perspective, there can be no question that the drift in American law over the past 40 years and more has been in large part to the left, as that term is ordinarily understood. And a good part of that drift has resulted from court decisions. Yet by no means can all or even most of the drift be attributed to the courts. Moreover, even that part that has resulted from court decisions does not arise entirely or even primarily from “judicial activism”–not unless that idea is stretched to include every decision that conforms to some leftist political agenda.



In fact, when we look at most such decisions closely, we rarely find that the judge or justices “legislated.” To be sure, they often reach results consistent, if not with their “whims,” at least with their “values, policy preferences, or visions of what is just or right.” But those results can usually be tied to some legal anchor, even if it takes some stretch to do so.



Take the recent CCRI decision by U.S. District Court Judge Thelton Henderson, which enjoined enforcement of the initiative shortly after it was passed by some 54 percent of California’s voters. Many critics of the judiciary immediately pointed to the decision as a blatant example of judicial activism. Judge Henderson’s opinion was a stretch, to be sure. But it was not without legal foundation, citing _Hunter v. Erickson_ , 393 U.S. 385 (1969) and _Washington v. Seattle School District No. 1,_ 458 U.S. 457 (1982). Moreover, as we know, the case has taken the normal appellate course; the decision has since been reversed by the U.S. Court of Appeals for the Ninth Circuit; [15] and plaintiffs have just filed a petition for certiorari with the Supreme Court. We are likely to learn from the Court whether the cases Judge Henderson relied upon in fact apply or are still good law. In the meantime, however, we are hard pressed to say that his decision was “lawless,” however strained it may have been.



One could review putative cases of judicial activism almost _ad infinitum_ , of course, but the fact remains that the better part of such cases do not exhibit judicial lawmaking, just better or worse judicial reasoning. It is no small irony, however, that when we do come across a genuine case of blatant judicial activism that cuts the other way, politically, many conservative critics of the judiciary are strangely silent. That was pointed out just last week, for example, by conservative constitutional scholar Bruce Fein in an op‐​ed in the _New York Times_ , citing the current controversy over the decision of an Alabama state judge to defy a long line of Supreme Court rulings on the separation of church and state “by posting a copy of the Ten Commandments in his courtroom and inviting clergy to lead juries in prayer,” [16] even after a state appellate court found the practices unconstitutional.



 **IV. Misstating the Problem**



In the end, therefore, those who are concerned about judges who seem always to be leaning to the left may be better advised to look less to the judicial role in our system–to the practice of judicial review–and more to the reasoning judges employ in performing their roles and, more importantly, to the sources they employ when doing their reasoning. Bad reasoning is just that and should be called that, not called judicial “activism.” But bad law, from which so much bad reasoning proceeds, is another matter. We should hardly be surprised that judges today are thought so often to be engaged in “judicial activism” when they are called upon so often to apply law that is inconsistent, incoherent, and fairly invites them to make all manner of value judgments. In such circumstances, they can hardly be seen to be doing anything but legislate.



We come, then, to what in fact is the crux of the matter. Under our system of law, the role of the judge should be much simpler than it has come to be. The problem, however, does not go back just 40 years, as too many conservatives believe. Rather, its institutional roots are in the New Deal. And its ideological roots are in the Progressive Era, when we stopped thinking of government as a “necessary evil,” as the Founders had conceived of it, and started thinking of government as an engine of good, an instrument for solving all manner of social and economic problems. Standing in the way of carrying out that agenda, of course, was a constitution that established a government of limited, enumerated powers–a constitution that held, more or less, until the New Deal. As we all know, however, when President Roosevelt was unable to get his programs past the Court–there being no authority for them under the Constitution–he threatened to pack the Court with six additional members. Not even Congress would go along with that. Nevertheless, the Court got the message; there was the famous switch in time that saved nine; and by 1938 the Court had essentially turned the Constitution on its head, as New Deal architect Rexford Tugwell would later tell us the administration meant for it to do. [17]



In a nutshell, a document of delegated, enumerated, and thus limited powers became in short order a document of effectively unenumerated powers, limited only by rights that would thereafter be interpreted narrowly by conservatives on the Court and episodically by liberals on the Court. Both sides, in short, would come to ignore our roots in limited government, buying instead into the idea of vast majoritarian power–the only disagreement being over what rights might limit that power and in which circumstances. Indeed, we need look no further than to Judge Bork–no liberal he–to see the new vision stated–and wrongly ascribed to James Madison. The “Madisonian dilemma” that constitutional courts face, Bork tells us, is this:



[America’s] first principle is self‐​government, which means that in wide areas of life majorities are entitled to rule, if they wish, simply because they are majorities. [It’s second principle is] that there are nonetheless _some_ things majorities must not do to minorities, _some_ areas of life in which the individual must be free of majority rule. [18]



That gets the Madisonian vision exactly backward, of course. America’s first _political_ principle may indeed have been self‐​government, but its first _moral_ principle–and the reason the people instituted government at all–was individual liberty, as the Declaration of Independence makes plain for “a candid world” to see.



Indeed, we did not throw off a king only to enable a majority to do what no king would ever dare. Rather, the Founders instituted a plan whereby in “wide areas” individuals would be entitled to be free simply because they were born so entitled, while in “some” areas majorities would be entitled to rule not because they were inherently so entitled but as a practical compromise.



That gets the order right: individual liberty first; self‐​government second, as a means toward securing that liberty–with wide berths to state governments, which were later reined in by the Civil War Amendments. That is why the Constitution enumerated the powers of Congress and the executive, to limit them. And that is why the Bill of Rights concludes with the Ninth and Tenth Amendments: to make clear that Americans begin and end with their rights, enumerated and unenumerated alike, while government proceeds only with the power it is given.



The New Deal changed all that, of course, not by amending the Constitution, the proper method, but by radically reinterpreting it: in particular, by reading the General Welfare and Commerce Clauses not as shields against power, as they were meant to be, but as swords of power; then by turning the Bill of Rights into a document of “fundamental” and “nonfundamental” rights. [19] None of that was found plainly in the Constitution–to the contrary, the entire document tends plainly the other way. Rather, it was invented virtually out of whole cloth, by the New Deal Court, to make way for the New Deal’s political agenda.



Our modern problem of overweening, inconsistent, incoherent statutory law began, then, not with an activist Court–to the contrary–but with an activist Congress and executive branch, bent on expanding government power. In time, however, the problem was abetted by an activist Court–succumbing to pressure from the political branches. But as noted earlier, the Court’s “activism” was not as we think of it today–a search for rights not apparent in the Constitution. Rather, it was activism in finding rationales for power–what conservatives today call deference to the political branches.



It needs to be said again, however, that the New Deal Court’s activism was not entirely without legal foundation. The sources for the Court’s rulings were there, in the Constitution, even if it did take a high degree of creativity, to be charitable, to draw them out, and even if doing so did fly in the face, for the most part, of a century and a half of constitutional jurisprudence that went the other way.



We come, then, to the bottom line in all of this. Law, including constitutional law, is not written in immutable stone. It is to some extent malleable, of necessity, and is given life by those charged with giving it life–the judiciary. In doing their work, however, judges do not work in a vacuum. They work instead in a larger political climate. If we who shape that climate persist in believing that it is proper for government to be addressing our every problem, no matter how trivial or personal, and persist in believing that our Constitution can legitimately be read to authorize that result, then we should not be surprised that the judiciary is dragged along to play its part in the process–today, often, to try to undue the mess that legislatures make of the effort. [20]



Yes, judges today often thwart the majoritarian will–as a vestige, perhaps, of their former principal role. Just as often, however, a judge may see himself as simply a facilitator in the grand enterprise of government. We are coming to the close of what has rightly been called the century of government–more accurately, the century of failed government planning. If we are unhappy with the role the judiciary sometimes plays in this setting, it may be that we need to look first to the material we give judges to work with–the reams of statutory material we have enacted over the course of the century.



The Founders had a simpler vision in mind when they set out to craft our legal order. They left most human affairs to private ordering, not to government planning. That gives the judiciary–and Congress–relatively little to do. Is that not what critics of judicial activism want?



A curriculum vitae is attached. Pursuant to House Rule XI, clause 2(g)(4), neither I nor the Cato Institute receives any federal funds–as a matter of principle.



[1] I have discussed the issues that follow more fully in: “Congress, the Courts, and the Constitution,” _Cato Handbook for Congress_ (105th Congress), ch. 3 (esp. pp. 36–42), (1997); “A Government of Limited Powers,” _Cato Handbook for Congress_ (104th Congress), ch. 3 (1995) (reprinted as “Restoring Constitutional Government,” _Cato’s Letter No. 9_ (1995)); “Rethinking Judicial Restraint,” _Wall Street Journal_ , Feb. 1, 1991, at A10 (op‐​ed); “Constitutional Visions,” _Reason_ , Dec. 1990, at 39–41 (review of Robert Bork’s _The Tempting of America_ ); “Legislative Activism, Judicial Activism, and the Decline of Private Sovereignty,” in _Economic Liberties and the Judiciary_ (J. Dorn & H. Manne eds., 1987); and “On the Foundations of Justice,” 17 _Intercollegiate Rev._ 3 (1981).



` `[2]Lino Graglia, “It’s Not Constitutionalism, It’s Judicial Activism,” 19 _Harvard Journal of Law & Public Policy_, 293, 298 (Winter 1996).



[3]Robert H. Bork, _Slouching Towards Gomorrah_ 114 (1996).



[4]“Remarks of Sen. Orrin Hatch Before the Federalist Society’s 10th Anniversary Lawyers Convention,” Senate Judiciary Committee News Release, Nov. 15, 1996, at 4.



[5]Id., at 5 (original emphasis).



[6]John Ashcroft, “Courting Disaster: Judicial Despotism in the Age of Russell Clark,” March 6, 1997, at 4 (MS available from the office of Senator Ashcroft).



[7]Id., at 3.



[8]Ralph Z. Hallow, “Republicans out to impeach ‘activist’ jurists,” _Washington Times_ , March 12, 1997, at 1. See also Katharine Q. Seelye, “House G.O.P. Begins Listing A Few Judges to Impeach,” _New York Times_ , Mar. 14, 1997, at A24.



[9]This very brief overview barely touches on the vast body of both scholarly and popular literature on the subject, to say nothing of political activism about judicial activism. In this last category, for example, is the Judicial Selection Monitoring Project of the conservative Free Congress Foundation’s Center for Law & Democracy, which on January 27, on behalf of 260 grassroots organizations and 35 radio and television talk show hosts, petitioned President Clinton and members of the Senate to nominate and confirm only those candidates for the federal bench who are committed to judicial restraint.



[10]Michael Kelly, “TRB from Washington: Judge Dread,” _The New Republic_ , Mar. 31, 1997, at 6. See also Laurie Kellman, “Republicans rally ’round judge‐​impeachment idea,” _Washington Times_ , Mar. 13, 1997, at A1: “The plan is aimed in part at reviving Republican morale, which has flagged this year because of Mr. Gingrich’s ethics troubles and the majority’s sparse floor schedule,” at A18.



[11]See, _e.g._ , Terry Eastland, “Deactivate the Courts,” _The American Spectator_ , Mar. 1997, at 60. For a fuller treatment of why conservative efforts to influence the courts have been so unsuccessful, see James F. Simom, _The Center Holds: The Power Struggle Inside the Rehnquist Court_ (1995). For a critique of that book, and the Court itself, see Roger Pilon, “A Court Without a Compass,” 40 _New York Law School Law Review_ 999 (1996).



[12]Coalition for Economic Equity v. Wilson, 946 F. Supp. 1480 (N.D. Cal. 1996).



[13]United States v. Bayless, 913 F. Supp. 232 (S.D.N.Y.), rev’d on rehearing, 921 F. Supp. 211 (S.D.N.Y. 1996).



[14]United States v. Virginia, 116 S. Ct. 2264 (1996).



[15]Coalition for Economic Equity v. Wilson, 1997 U.S. App. LEXIS 6512 (9th Cir.).



[16]Bruce Fein, “Judge Not,” _New York Times_ , May 8, 1997, at A39. Cf. Debbie Kaminer, “Thou Shalt Not Display the Ten Commandments in Court,” _Legal Times_ , May 5, 1997, at 27; Terrence P. Jeffrey, “Governor James at the Courthouse Door,” _Human Events_ , May 9, 1997, at 6.



[17]“To the extent that these [New Deal policies] developed, they were tortured interpretations of a document [ _i.e._ , the Constitution] intended to prevent them.” Rexford G. Tugwell, “A Center Report: Rewriting the Constitution,” _Center Magazine_ , Mar. 1968, at 18, 20.



[18]Robert H. Bork, _The Tempting of America_ 139 (1990)(emphasis added).



[19]I have discussed these issues more fully in Roger Pilon,“Freedom, Responsibility, and the Constitution: On Recovering Our Founding Principles,” 68 _Notre Dame Law Review_ 507 (1993).



[20]Thus, the Court has long been criticized by conservatives for its 1971 decision in Griggs v. Duke Power Co., 401 U.S. 424, which gave rise to the “effects test” in antidiscrimination law and to a host of affirmative action programs. But in interpreting the language of section 703 (h) of the Civil Rights Act of 1964, which authorizes “any professionally developed ability test” that is not “designed, intended, _or used_ to discriminate because of race” (at 433, emphasis by the Court), the Court simply drew upon the ambiguity of “used.” Congress could later have addressed that ambiguity, of course, but it did not. In cases like this, then, responsibility rests ultimately with Congress.
"
"
Share this...FacebookTwitterUpdate 7/30/2010: WUWT debunks this scare: here!
Reading the German online daily news this morning, today’s scare-de-jour is the “shocking” reduction of phytoplankton now underway, all due of course to manmade climate change. The “news” is based on a report just published in Nature by scientists Daniel Boyce and Marlon Lewis of the Dalhousie University (Halifax) and Boris Worm of the German Potsdam Institute for Climate Impact Research (think Schellnhuber and Rahmstorf).
When reading about such stories,  it’s a very good idea to first read the following:
Science Turns Authoritarian
Here’s a sampling of today’s headlines in Germany:
Die Welt PLANKTON REDUCTION PUTS FOOD CHAIN AT RISK
Der Spiegel: FOOD CRISIS IN WORLD’S OCEANS
Focus: GREEN FOOD SUPPLY IN OCEANS SHRINKING
Süddeuschte Zeitung: DISAPPEARING IN THE OCEANS
The cause of the phytoplankton decrease is “warming of the oceans”, Nature reports. Satellite measurements since the end of the 1970s have shown fluctuation in oceanic plankton levels, but have not delivered a clear picture.
That’s why the researchers went back and looked at data of ocean chlorophyll content. The team analysed almost 450,000 measurements from the period 1899 to 2008. The result, according to Die Welt:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In eight of ten oceanic regions, phytoplankton has decreased during the 20th century. Content dropped with increasing sea surface temperature, especially in the tropics and subtopics regions. It is suspected that as a consequence of warming, a more pronounced layering of water occurred.
Süddeutsche Zeitung writes that phytoplankton concentrations in the oceans have declined by two thirds since 1899.
What do I think?
Schellnhuber: Visionary of The Great Transformation
The study involves the Potsdam Institute For Climate Impact Research, directed by Hans Joachim Schellnhuber, visionary of The Great Transformation , and Stefan Rahmstorf, who predicts sea levels will rise 1.7 meters in the next 90 years, but refuses to even bet on a 60 cm rise. This is an institute that is well well-known for activist science and fear-mongering, with the clear agenda of reorganizing how people live. A dangerous social engineering experiment.
I’m sure the findings of this study will turn out to be more plankton-crap.
UPDATE: Beware of science authority. Above I mentioned the Science Turns Authoritarian story, which looked at how often certain authoritarian phrases are used in the media. Click on the following graphic:

Be wary, be very wary, of media claims on climate science.
Share this...FacebookTwitter "
"
From the BBC, a video report so absurd, you wonder if it is an April fools joke. The premise? Noise from excessive ice calving  and cracking due to “climate change” would affect the bear’s hearing. I wonder what agency was gullible enough to provide a grant for this load of rubbish? Like polar bears have never heard ice floes cracking and calving before? Give me a break. Plus, the polar bear they are using for a test subject isn’t in it’s natural environment, it’s at a zoo and who’s to say this bear establishes a credible baseline hearing test? This is just unbelievable stupidity in the guise of bad science. What next? Hearing aids for polar bears? A hat tip to Tony B in the UK for alerting me to this story. – Anthony

How to test a bear’s hearing

Click preview image above for link to video story
Scientists in California are testing the hearing of polar bears to try to find out whether the noises associated with melting Arctic ice could affect their ability to survive.
The BBC’s Peter Bowes goes to SeaWorld in San Diego to meet Charly, a 12-year-old polar bear taking part in the experiment – and his trainer Mike Price.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c1eea0f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe Moving Finger writes; and, having writ,
Moves on: nor all thy Piety nor Wit
Shall lure it back to cancel half a Line,
Nor all thy Tears wash out a Word of it.
– Omar Khayyám
Scratch off the Potsdam Institute For Climate Impact Research from the alarmist list. No kidding!
The European Institute For Climate and Energy has a new piece written by Raimund Leistenschneider that takes a look at two interesting papers dug up from 2003. I wonder if Rahmstorf and Schellnhuber are going to feign amnesia on this. Big hat tip to NTZ reader Ike!
Rahmstorf 2003 paper shows pronounced cooling



The paper by Prof Stefan Rahmstorf confirms that today’s temperatures are actually quite cool compared to temperatures earlier in the Holocene.
In a paper he authored: “Timing of abrupt climate change: A precise clock“, Geophys. Res. Lett.. 30, Nr. 10, 2003, S. 1510, doi:10.1029/2003 GL017115, Ramhstorf examined the Dansgaard-Oeschger (DO events).
These events are rapid climate changes occurring 23 times during the last ice age between 110,000 and 23.000 BP and were reconstructed from the GISP-2-ice cores from Greenland. The following chart is a plot in Rahmstorf 2003 paper showing the temperature over the last 50,000 years.
 The next graphic shows the temperature for the last 50,000 years and the last 9,000 up close below, also derived from the GISP-2-ice cores.

On Rahmstorf’s paper, EIKE writes:
Easy to recognize, at least using the studies done by Rahmstorf, we are living in a comparably cold time today. During the MWP 1000 years ago, when the vikings were farming Greenland, it was 1°C warmer than today. During the Roman Optimum 2000 years ago, when Hannibal crossed the Alps with his elephants in the wintertime, it was even 2°C warmer than today. And during the Holocene climate optimum 3500 years ago it was about 3°C warmer than today. Since about  3200 years ago, there has been a cooling of about 2°C.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Multiple studies confirm that the warming was not a regional phenomena.
Schellnhuber could not discern any warming back in 2003
Meanwhile Prof. Hans Joachim Schellnhuber, Rahmstorf’s boss at the Potsdam Institute, was unable to discern any warming when examining a multitude of worldwide temperature records back in 2002 and 2003.
In a paper published in 2003, using their own studies, the authors concluded there had been no global warming over the last decades. (J.F. Eichner, E. Koscielny-Bunde, A. Bunde, S. Havlin, and H.-J. Schellnhuber: Power-law persistence and trends in the atmosphere, a detailed study of long temperature records, Phys. Rev. E 68 2003),
The temperature records of 95 stations distributed over the globe were studied. In the paper’s summary discussion, Schellnhuber and his colleagues wrote:
In the vast majority of stations we did not see indications for a global warming of the atmosphere.
and
Most of the continental stations where we observed significant trends are large cities where probably the fast urban growth in the last century gave rise to temperature increases.
And la pièce de resistance!
The fact that we found it difficult to discern warming trends at many stations that are not located in rapidly developing urban areas may indicate that the actual increase in global temperature caused by anthropogenic perturbation is less pronounced than estimated in the last IPCC Intergovernmental Panel for Climate Change.
Last I checked, global temperatures have gone nowhere since 2003. So where’s the warming?
—————————————————————————————————
Copyright reminder: It is not allowed to reproduce this post without first obtaining permission from No Tricks Zone. You may cut and paste max. 25% of the content, and then followed by a link to this site. Thanks!
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHow is it that a settled science keeps finding things never expected?
For example, the HIAPER Pole-to-Pole Observations (HIPPO) mission was launched in January 2009 and will make a series of five flights over three years covering more than 24,000 miles to sample the atmosphere in some of the most inaccessible regions of the world. Read HIPPO background here.
The goal of the mission is the first-ever, global, real-time sampling of carbon dioxide and other greenhouse gases across a wide range of altitudes in the atmosphere, from pole-to-pole.
Professor Mark Zondlo of Princeton University has taken measurements of water vapour in the atmosphere, from 14 km high to just above the sea ice, using a vertical cavity surface mini laser hydrometer.
Watch Zondlo video here.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Here are some of Professor Zondlo’s observations so far:
We don’t really know how clouds are formed. Water vapour impacts the climate more than any other gas.
What we are finding is surprising. Large plumes of water vapour exist in areas we never expected to find them.
Learning how this fits into the puzzle is crucial for predicting climate and making smart policy decisions.
What does that mean? It means the climate models used so far were nothing more than junk, thus the same applies for their predictions. They completely neglected the water vapour factor (and who knows what other factors).
Climate forecasting is best left to real forecasters, and not tainted modelers.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe winter has even overwhelmed winter-hardened Sweden. So much so that the military has been called out to assist.
Temperatures as low as -36 degrees Celsius have been recorded in Sweden as snowfalls and storm winds play havoc with transport services.

Transport agency calls for caution after snow (23 Nov 10)
Sweden braces for new winter storm (22 Nov 10)
Rail operators prepared for harsh winter: report (16 Nov 10)

Sweden’s main meteorological agency, SMHI, noted that the winter continued its march south across the country as strong winds from the Baltic Sea brought heavy snowfalls in eastern areas of Svealand and Götaland.
The snow is expected to remain on the ground in many parts of southern Sweden as temperatures are set to remain well below zero.
“There has been a lot of snow overnight,” Lisa Frost at SMHI said.
Kalmar county was obliged to call for military help on Wednesday to aid in the battle against widespread flooding which had caused damage to property in the area.
Read more: http://www.thelocal.se/30396/20101124/ (The Local).
The Independent got half of it right anyway. Maybe not “rare”, but at least “exciting”.
Share this...FacebookTwitter "
"Last month was the hottest January on record over the world’s land and ocean surfaces, with average temperatures exceeding anything in the 141 years of data held by the National Oceanic and Atmospheric Administration.  The record temperatures in January follow an exceptionally warm 2019, which has been ranked as the second hottest year for the planet’s surface since reliable measurements started. The past five years and the past decade are the hottest in 150 years of record-keeping, an indication of the gathering pace of the climate crisis. According to Noaa, the average global land and ocean surface temperature last month was 2.05F (or 1.14C) above the 20th-century average. This measurement marginally surpassed the previous January record, set in 2016. A pulse of unusual warmth was felt across much of Russia, Scandinavia and eastern Canada, where temperatures were an incredible 9F (5C) above average, or higher. The Swedish town of Örebro reached 10.3C, its hottest January temperature since 1858, while Boston experienced its hottest ever January day, at 23C (74F). Meanwhile, the Antarctic has begun February with several temperature spikes. The southern polar continent broke 20C (68F) for the first time in its history on 9 February, following another previous high of 18.3C just three days previously. Scientists called the readings “incredible and abnormal”. Noaa said the four warmest Januaries on record have occurred since 2016, while the 10 warmest Januaries have taken place since 2002. The world’s governments agreed in 2015 to keep the global temperature increase to well below 2C, compared with the pre-industrial era, in order to stave off disastrous flooding, food insecurity, heatwaves and mass displacement of people. However, planet-warming emissions from human activity are not showing any sign of decline, let alone the deep cuts needed to meet the 2C goal and address the climate crisis. According to scientists, the world must halve its emissions by 2030 to stand any chance of avoiding disastrous climate breakdown. • This article was amended on 17 February 2020. A previous version said the average global land and ocean surface temperature in January was 2.5F above the 20th-century average. This has now been corrected to 2.05F."
"
Share this...FacebookTwitterAs an American citizen living in Germany, today is just another regular work day here, but of course I still celebrate Thanksgiving, and do so by having a lavish turkey dinner on Friday evening with friends and family. For the non-Americans who visit this site,  here’s a short version of how Thanksgiving started and became a tradition.
The Pilgrims escape oppression in Europe
Giving thanks and celebrating festivals for successful harvests had existed for centuries, way before the first American Thanksgiving. Giving thanks in America started when the first Pilgrims came to Massachusetts (Plymouth Rock) from England on the Mayflower in 1620. The Pilgrims came to the New World to escape persecution and oppression, particularly from the Church of England, kind of like how climate skeptics are oppressed by the Church of Climatology today.
The Pilgrims land at Plymouth Rock and many starve to death because of climate
The Mayflower with its 102 passengers had been originally bound for Jamestown, Virginia, but Atlantic storms blew the ship north to Massachusetts (Storms back then had natural causes, and were not man made ;). The first winter there was especially harsh, and because they had arrived too late they could not grow crops and they didn’t have fresh food. Half the Pilgrims on the Mayflower died of mal-nutrition and starvation during the first winter alone. But despite the extreme hardship, these newcomers had some luck, as it was the tradition of the local Wampanoag Indians, led by Chief Massasoit, to share food with any visitors.
The Indians teach the Pilgrims adaptation, and not useless mitigation
The following spring, in 1621, the Indians taught them how to grow corn (maize) and introduced cranberries, which were new foods for the new settlers. They also showed them how to grow other crops like beans, pumpkins and squash in the strange soil. The Indians taught the Pilgrims how to hunt and fish as well. Back then, the Indians taught the Pilgrims that it was useless to mitigate climate. Now just imagine if the Pilgrims had resorted to rain dancing and forbidden tree cutting. We can be thankful they were smarter then our political leaders of today.
The fruits of adaptation
After the first harvest had been completed by the new colonists in the autumn of 1621, the colonists had an abundance of food (the wonders of adaptation!). Governor William Bradford therefore proclaimed a day of thanksgiving and prayer. And to thank the Indians for teaching them how to survive in the New World, the Pilgrims of Plymouth Rock invited their Indian friends to their first Thanksgiving. It was a three day celebration to give thanks to God and the leaders of the Wampanoag Indians.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Thanksgiving spreads to other states and declared a national holiday 
After the first Plymouth Thanksgiving, the custom spread to the other colonies. But each region chose its own date. In 1789 George Washington, the first president of the United States, declared November 26 as a day of thanksgiving, but it still was not an official holiday. Thanksgiving Day continued to be celebrated in the United States on different days in different states – until Mrs. Sarah Josepha Hale, editor of Godey’s Lady’s Book, embarked on a campaign. For more than 30 years she wrote letters to the governors and presidents asking them to make Thanksgiving Day a national holiday.
Thanksgiving becomes a national holiday
In 1863, President Lincoln called on Americans to unite “with one heart and one voice” and to celebrate Thanksgiving Day on the last Thursday of November. In 1939 President Franklin D. Roosevelt moved Thanksgiving Day a week earlier to make the Christmas shopping season longer. However, because some states used the new date and others the old one, it was changed again just 2 years later. Now Thanksgiving Day is celebrated on the fourth Thursday in November.
Freedom of Want – by Norman Rockwell (1943)
Why turkey?
The turkey tradition was pushed by Benjamin Franklin, who even wanted to make it the United States national symbol. In the end the bald eagle was selected instead of the wild turkey as the official national symbol. I think most Americans will agree it was the right choice. Finally, the turkey was made famous by Norman Rockwell’s 1943 image of the family Thanksgiving, Freedom of Want, that appeared on the cover of the Saturday Evening Post. The turkey has been the Thanksgiving Day favorite ever since.
Dinner and family
The American tradition of Thanksgiving revolves around an extravagant meal, with turkey at the center.  Thanksgiving dinner also includes corn, cranberry, potatoes, gravy and a variety of pies for dessert – like pumpkin pie or apple pie. It’s tradition to say a special prayer of thanks before the meal. In many homes, family members each mention something they are very thankful for. Thanksgiving is a time for families to come together.
I have very fond memories of Thanksgivings in New England as a boy, especially of coming in out of the cold into a warm house heated by a wood-burning Franklin stove and savouring the aroma of an 18-pound turkey that’s been baking in the oven for five hours. Thanksgiving memories last a lifetime.
Share this...FacebookTwitter "
"

After five days of contentious discussions in Bangkok, governments from nearly 200 countries last week agreed to an agenda for further talks to forge a new United Nations global warming agreement. One sticking point has been developing nations’ insistence that industrialized countries should take the first steps in reducing emissions and should help finance reductions in developing countries. But this represents a serious misreading of the underlying economic situation.



The theory behind the “developed countries should pay” model was articulated by Yvo de Boer, executive secretary of the U.N. Framework Convention on Climate Change: “The problem of climate change … is a result of rich countries’ emissions, not the result of poor countries’ emissions. The historic responsibility of this problem lies with industrial nations.”



Yet although greenhouse gas emissions can be blamed on nations based on the location of emission activities, these emissions are the effluvia of civilization and all its activities. In today’s interconnected world, economic activity in one country helps provide livelihoods and incomes for many inhabitants elsewhere, and vice versa. A substantial portion of economic growth in developing countries is attributable to trade, remittances, tourism and direct investment from industrialized countries.



For example, remittances, mainly from the United States, Britain and the oil‐​rich Gulf states, account for 13% of Bangladesh’s GDP. Absent economic activities that directly or indirectly fuel such contributions to developing countries, U.S. emissions might be lower, but so would jobs and incomes in developing countries like Bangladesh.



These linkages have had hugely positive effects. Greenhouse‐​gas‐​fueled economic activity has enabled today’s rich societies to invest in agricultural, medical and public health research that has raised crop yields and lowered hunger in developing countries; to devise effective medical interventions to address old diseases like tuberculosis, malaria, diarrhea and smallpox and new diseases like AIDS; and to provide aid in times of famine or other natural disasters.



Absent such economic activity, human capital would have been lower worldwide. Consider, for instance, the millions of non‐​Americans who have been cycled through universities in the U.S. who then returned to advance their native countries’ economic and technological development.



Some might argue that one should not take indirect effects of greenhouse‐​gas‐​producing activities into consideration: Only direct effects should be considered. But the notion of assigning responsibility or demanding compensation for climate change is itself based on indirect and inadvertent outcomes. Industrialized countries did not emit greenhouse gas emissions just for fun. There are clearly benefits.



So if the U.S. contribution to global warming, for instance, could be estimated, the next step would be to estimate the net harm caused to, say, Bangladesh. This requires estimating both direct and indirect impacts not just of climate change but all greenhouse gas‐​producing activities on Bangladesh.



This raises some serious questions, including: Had there been no greenhouse gas‐​producing activities in the U.S., what would have been Bangladesh’s GDP and level of human well‐​being? How would that affect life expectancy, which is currently 62 years but was only 35 years in 1945? Would Bangladesh’s hunger and malnutrition rates rise? How many Bangladeshis were saved in the 1960s and 1970s because of food aid from industrialized countries? How much of its increase in agricultural productivity is due to higher CO2 levels, or indirectly due to efforts enabled because the U.S. was wealthy enough to support them? If future agricultural productivity declines due to climate change, how do you subtract past and present benefits from future harms?



Clearly, it’s premature to assign “responsibility” to industrialized countries for net damages to developing countries, since we don’t know whether those damages have, in fact, been incurred. Even if one could assign responsibility for climate change, it does not follow that it would be “fairer” if industrialized nations were to expend resources now on ambitious mitigation measures, based partly on the premise that it would reduce future climate change risks for developing nations. The same resources would, in the short‐ to medium term, provide greater and faster benefits to precisely those nations by reducing existing — and generally larger — climate‐​sensitive risks and vulnerabilities such as hunger, malaria and the threat of cyclones and other extreme events.



The U.N. climatocrats owe it to the people of the developing world to consider these trade‐​offs before they charge ahead with their ambitious new agenda.
"
"This summer has seen mother nature banish us to our bedrooms. It’s been the summer of staying indoors – whether you’re in Australia sheltering from violent storms and avoiding the worst air quality in the world, or abroad, stuck in your hotel room, cruise ship cabin or apartment trying not to infect or be infected with coronavirus. It’s impossible to tell if this mass banishment indoors is just a temporary thing – a strange confluence of events (weather patterns, rapid spread of mysterious virus) – or whether this is the new normal. As the world out there feels increasingly unsafe and unpredictable, are we going to spend more and more time inside? Already as a society we’re showing a preference for the indoors life. From the comfort of our couch we order in UberEats or Deliveroo rather than go out to a restaurant, we stream movies at home rather than go to the cinema, we talk to our friends on group chat rather than meeting up at the pub, we meet people through dating apps rather than on the dancefloors or at bars, and we’ll exercise at home – following a YouTube tutorial such as the highly popular Yoga with Adrienne – rather than attend a class. In overcrowded, hot cities such as Cairo, leaving your apartment for anything from a haircut to a blood test has become such a hassle that a micro economy has sprung up in delivering goods and services. For a few dollars someone will come to your house and wax your bikini line or deliver you the paper and a coffee. One man told the New York Times: “There is no incentive to go outside. Just one hour outside is enough to ruin anyone’s day.” It’s probably a good thing we’re getting more and more habituated to spending time at home, as increasingly we are finding ourselves confined there, but not by choice. Right now tens of millions of people in China are either in self-imposed or forced quarantine, thousands are stuck in their cabins on infected cruise ships, and others have been quarantined in detention centres, air force bases and disused mining camps. In Australia, appalling air quality from the bushfires meant schools kept their students indoors at lunch and recess, and parents were not letting their kids play outside. Friends who are runners have complained they couldn’t exercise outside during the haze, and everything from music festivals to fun runs were cancelled due to fires and poor air quality. Eastern New South Wales got another taste of cabin fever when 550mm  of rain fell across parts of NSW last weekend. Sydney recorded its heaviest rain in three decades and was battered by gale-force winds. On Sunday, the worst day of the storm, I was trapped at a friend’s house in North Bondi. I had stuff to do, people to meet, food to buy!! – but leaving seemed … dangerous. Rain came in over the golf course across the road in vast, thick sheets. I saw no one on the street all day, except for a lone man with a dog. Both appeared to be walking sideways. The pair disappeared over the hill, towards the cliffs, presumably never to be heard from again. On the balcony, the gales knocked over and smashed large terracotta pots, the rain came in sideways – in both directions - colliding then being picked up by the wind and swirled in the air in a mini-tornado. All I could do was sit on the couch and watch it in horror as the ceiling leaked in the bedroom, ruining the carpet, and the wind sounded like someone badly imitating a ghost. Woooooo … woooooooooo … Ordering UberEats was out of the question. It would be cruel, possibly psychotic, to insist that some young person on a pushbike and zero hours contract deliver a burrito while dodging flying debris and falling trees. (Although restaurants will argue that they need to keep making money during bad weather.) Socialising was also cancelled. Instead I kept in touch with friends throughout the day via text (boredom was the main gripe), and passively, through watching each other’s Instagram stories of sodden carpets, scared pets and knocked-up pantry dinners. Expect more of it – these weather “events” are becoming more extreme and frequent across the world. Just this week, Storm Dennis and a few days earlier Ciara hit the UK with “snow showers combine(d) with strong winds”. Up to 10cm of snow was forecast in higher areas, hardly ideal weather to be out and about. People are coping in different ways with being trapped indoors. One man confined in China has run 50km around his living room. “I have not been outside for many days, today I cannot bear sitting down any more!” the amateur marathon runner posted last week. Meanwhile an Australian couple quarantined in their cabin on a cruise ship for 14 days “kept the party flowing by getting a drone to deliver wine (pinot noir) straight to their cabin”. Having been quarantined myself, in 2012 for whooping cough, I can tell you it’s no fun. After being sent home from the doctor and told not to leave the house for two weeks, I had to isolate myself. I couldn’t go into work (apologies Microsoft!), my flatmate had to move in with her girlfriend until the contagion had passed, and I switched my entire life – everything from socialising to grocery shopping – online. If I didn’t die of whooping cough, I thought I would die of boredom. I was going stir crazy sitting inside staring at the walls, not to mention lonely. Little did I know, I had glimpsed the future. Brigid Delaney is a Guardian Australia columnist "
"
Share this...FacebookTwittertutorial main bandarqq Online supaya Menang tidak sedikit , terhadap peluang yg indah ini kami dapat sedikit mengulas info yg berhubungan bersama perjudian online, utk artikel kali ini kita dapat mengupas tuntas seperti apa meraih kemenangan dalam main judi card domino88, nah sebelum masuk ke pembahasan ada sekian banyak perihal yg butuh kamu ketahui terkait dgn permainan judi satu ini. mula-mula merupakan buat menyangkut berapa jumlah card yg dimanfaatkan dalam permainan ini, permainan card domino sendiri ialah salah satu tipe permainan card yg telah lama ada, terkait dgn jumlah card yg dimanfaatkan, permainan ini memakai jumlahnya 52 kombinasi card bersama bulatan 1 pasang & mempunyai nilai yg berlainan.
selanjutnya buat system permainannya sendiri lumayan gampang & simple buat dipelajari, masing-masing buat satu meja mampu terdiri dari 6-8 orang pemain, buat tiap-tiap pemain dapat mendapati pembagian 4 card, pembagian card perdana dilakukan sejalan jarum jam, sebelum menuju pembagian card ke-2 rata-rata pemain bakal disuruh utk pilih call, raise, fold, check maupun all in. seterusnya baru card ke-2 bakal dibagian, nah disini system permainannya yaitu member yg mendapati card dgn nilai yg paling gede sehingga dialah yg dapat jadi pemenangnya, lumayan gampang bukan ? pasti saja pass gampang bila dimainkan oleh para pemain profesional, sedangkan utk kamu sendiri masihlah nampak susah, berikut ini merupakan pembahasan trik menang bemain judi card domino 99 online paling enteng buat pemula. utk meraih kemenangan main judi QQ online, sehingga aspek perdana yg mesti kamu melakukan merupakan percaya bersama insting yg kamu punyai, seluruhnya pemain judi tentunya mempunyai insting buat kemenangan. seandainya kamu telah pass percaya, sehingga kemenangan telah dekat ada didepan mata kamu. main-main judi QQ domino 99 bukan lagi semata-mata cuma bersama memanfaatkan unsur tebakan saja, melainkan ada unsur di mana seluruhnya pemain mesti memang mendalami & percaya dgn ketentuan mereka. buat dikarenakan itu kalau kamu mau menang main-main judi, pahami bersama baik seluruh permainan & pastikan kamu percaya bersama insting yg kamu miliki.
selanjutnya yg berikutnya seandainya mau menang main-main judi domino sehingga kamu mesti memang lah sanggup mendalami bersama baik permainan ini, apalagi buat kombinasi card sendiri. Ada tidak sedikit pemain yg senantiasa terkecoh bersama kombinasi card gede utk perdana kali muncul contohnya saja 9 + 6, & terhadap hasilnya mesti kalah. dgn kemunculan angka 9 + 0. system permainan ini memang lah sedikit mengecoh para pemainnnya, menjadi jangan sampai sempat gemar dulu diwaktu kamu memperoleh kombinasi angka 9 ataupun angka istimewa yang lain. butuh kamu pahami dgn baik bahwa judi pun mempunyai segi kelemahan adalah tiap-tiap kemunculan susah utk kamu tebak. setelah itu jikalau kamu mau menang dalam main-main judi domino 99 sehingga kamu mesti mampu main-main dengan cara sehat, tujuan dari main-main dengan cara sehat ini ialah main-main dgn pikiran yg kalem. Ada tidak sedikit pemain yg kadang main-main bersama trick yg serakah atau nafsu tinggi buat mendapati kemenangan dalam disaat yg langsung, padahal sanggup dikatakan apabila main dgn nafsu yg tinggi cuma bakal mengambil kekalahan dalam ketika yg segera. buat itu konsisten konsentrasi & main dgn trick yg baik.
silakan bergabung dgn kami & temukan kemenangan yg pantas seperti sama seperti pemain judi sejati.
Share this...FacebookTwitter "
"As wind power companies venture into ever-deeper waters, the traditional windmill-style turbine may not be the most suitable solution. It’s time to look at alternatives. Wind turbines traditionally had blades that rotate around a horizontal axis, and this has remained standard in modern wind farms. Conventional horizontal-axis wind turbines (HAWTs) came to dominate the wind energy market, both on land and in shallow waters offshore. But the best wind farm sites are often found in deeper seas, far from wind obstructions, shipping lanes, nimbys and migrating birds. Support structures fixed rigidly to the seabed may not be economically viable in depths beyond 50m, so engineers are instead turning to floating foundations. These are very different conditions in which to generate electricity from wind, which has meant a re-emerging interest in alternative wind turbine configurations. As we report in Philosophical Transactions of the Royal Society adopting a vertical rotation axis is one such alternative. These turbines, which always face the wind, are known as vertical-axis wind turbines (VAWTs). Traditional horizontal-axis turbines are very top-heavy, with the blades and the equipment that generates power necessarily fixed to the top of the tower. In a large offshore turbine this part can weigh several hundred tonnes and be around 100m above sea level. In a vertical-axis turbine the generation systems can be at a much lower height, even right at the base of the tower. This means VAWTs tend to have a much lower centre of gravity. On floating supports this advantage is magnified – just think of how stable you are when seated in a canoe rather than standing on it. Lowering the generators also makes them much more accessible and easier to maintain or replace. For example, with a VAWT workers only need to climb a short distance to access machinery rather than climb a 100m tower that is constantly moving.  In general, bigger wind turbines produce cheaper electricity. This has led to turbines getting ever larger and more powerful, especially offshore where 50m blades are now routine and some are even far bigger. But such horizonally-rotating giants may soon reach their limit. If the axis of the blade is considered, when this blade is horizontal the gravitational force will be perpendicular to it, while when the blade is vertical (upward) the gravitational force will be parallel to the axis and pointing toward the root of the blade. Finally, when the blade is vertical again (downward), the gravitational force will be parallel to the blade axis, but now pointing toward the tip.
Some studies claim this oscillating gravitational load limits the size of horizontal axis turbines. Vertical-axis turbines resolve this problem because as they rotate they experience a constant gravitational force, always in the same direction. Without the stress of holding up 80m metal blades by one end, VAWTs can potentially become much larger. Research has also shown vertical turbines can be placed closer to each other in a wind farm. So for a given area, more VAWTs can be installed, and more electricity generated than if HAWTs were used, thereby reducing the cost of electricity. So now the obvious question arises: why have VAWTs not yet been used if they are so advantageous? The main reason is that it is still a new technology. Although researchers have looked at vertical axes for decades, the technology fell behind due mainly to material and bearing system limitations. Investors still see the known technology as a safe bet and even when looking at deep-sea wind they have been more inclined to support horizontal-axis turbines on floating platforms. However both governments and forward-thinking companies are now investing in the potential of VAWTs for deep sea offshore wind. The Nova project, funded by the UK government, demonstrated vertical-axis offshore was commercially viable. Sandia National Laboratories, backed by the US energy department, and various EU-backed projects  have also supported the technology. Industrial companies such as the British VertAx Wind or Norway’s Gwind are also following suit through the development of different concepts. The French company Nenuphar is pushing ahead its VAWT design, with plans (pictured at the top of this article) to install the first floating wind farm off the coast of Marseilles. With increasing demand for energy we are obliged to explore every possible solution. Vertical-axis wind turbines located in the deep sea have strong potential to be one of these solutions."
"
Share this...FacebookTwitterIt was bound to happen sooner than later. A high level German politician speaking out against dubious climate science. Marie-Luise Dött, German Parliamentarian and a central figure on Angela Merkel’s environmental committee, expressed scepticism on climate change, the Financial Times Deutschland reports here in an article titled: The Climate Revisionists.
Now she is at the receiving end of brimstone and hellfire from all sides, including the media.
Here in Germany, climate skeptics face a level of intolerance not seen here in 65 years.
Last Wednesday, she made comments at a parliamentary forum discussion on the economic  impacts of climate protection held by the FDP Free Democrats, the junior coalition partner of Angela Merkel’s CDU/FDP coalition government.  Fred Singer – “a tobacco lobbyist” – was a guest speaker.
Dött’s comments not only left environmentalists and climate protection activists speechless and gasping for air, but exposed Dött as a climate skeptic. She is reported to have called climate protection a
…replacement religion, and that anyone who dared to express doubt could be branded an outlaw, forced to confess sins, sent to purgatory, or even cast into hell, if being really bad.
Free scientific thinking is a myth here.
Well, the vicious intolerant reactions she is now reaping confirm that her views are accurate, more than ever imagined. Even colleagues from within her own CDU Party piled on:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The next days are going to be very uncomfortable for her.
The intolerance from the opposition came swiftly. Hermann Ott, a spokesman for the German Green Party, blasted Dött and her CDU Party:
The CDU and the FDP Free Democrats are moving outside of the common community when they provide a forum in the German Parliament for the blind theories of climate change deniers.
(Note: denying the Holocaust in Germany is a crime. Ott is de facto calling Dött a criminal of the worst kind).
A member of the SPD was said to be in “shock” and demanded Dött be fired. He added it all confirmed the “real intentions of the coalition government.”
I’m not even going to get into what the media snobs are saying. Noses could not be higher.
Frau Dött not only has revealed herself to be skeptical of climate science, but has exposed Germany’s return to last century’s intolerance.
You can send a message of support here: E-Mail to MdB Marie-Luise Dött. Just fill in your message in the box: “Nachricht” with your name and city (You can ignore the boxes below the “Nachricht” box – they’re optional. 
Share this...FacebookTwitter "
"

Yes, you read the title correctly.
Sometimes I feel like a strange attractor for weather station chaos. Here I am at home tonight minding my own business, in my home office and I have the TV on. JEOPARDY comes on. Alex Trebek announces the categories…and I pay little attention until the last one is announced and he says “National Weather Service”. I practically got whiplash turning to look at the TV.  In 25 years of watching this TV program, that is a category I never expected to see.
Then to explain the category, up pops one of the “clue crew” people standing in front of the NWS office in Upton, New York, in the parking lot.
I didn’t hear a single word she said, because my eyes were transfixed on what was right behind her: a Stevenson Screen and MMTS just a couple of feet from the parking lot with the brick walls of the NWS office right behind it.
WTH!? Then it was gone.
I waited out the first round of JEOPARDY hoping to see more, but the contestants avoided the NWS category. Finally with nothing left they started into it. Then there it was again, the NWS station with visitor parking privileges.
After acing the category (the final answer was supercell) I decided to see if I could find this NWS office in Upton and maybe get a picture. I found that and more.
My first simple Google Image search found it right away, a photo taken during an open house on a Skywarn page:
MMTS and Stevenson Screen, NWS Office Upton NY - Photo: Bergen Skywarn
It did show the proximity to the brick building, but it really didn’t tell the whole story of what I saw in the TV shot. What was funny was that in the JEOPARDY segment, the NWS employees had apparently done some “sprucing up” and had painted the legs of the shelter and the MMTS mount pole a blue color to match the logo color of the NWS emblem over the office door:
Upton NY NWS office looking North - Photo: NWS
I found the above picture and the one below at the NWS Upton web page where they have a “virtual tour” of the facility. Here is another angle from the web page that shows the overall NWS complex, including the NEXRAD Doppler radar tower:
Upton NY NWS office looking Northeast - Photo: NWS
Looking at the style of the automobiles, I’m guessing these photos were taken sometime in the early 90’s when this office was opened. What is interesting about these photos, besides the siting issues with proximity to parking and the building, is the fact that the Stevenson Screen door is facing SOUTH rather than the requisite north. The idea is to keep direct sunlight from hitting the thermometers when readings are taken.
I thought perhaps this station is purely a “figurehead” used for school tours, etc, but then I thought: “Why would they want to show it being done incorrectly?”. I checked the NCDC MMS meta-database to see if the station was active. Oddly I couldn’t find the right station in Upton in the database. Poking around again at the NWS Upton website I found out why: This used to be New York City’s station. It was once on top of the RCA building as I discovered from their virtual tour:



Dec. 28, 1960 to Oct. 24, 1993
RCA/GE Building
30 Rockefeller Center NY, NY
Mezzanine  Level



Your National Weather Service office was located in midtown Manhattan on the mezzanine level of this building until October 25, 1993, when we relocated to our current site. The picture depicts the top of the building where our old radar was located (ball-shaped object). NOAA Weather Radio transmitters are also pictured and still reside atop 30 Rock.  
Once knowing it was the NYC station and not “Upton”, I was able to find it in the NCDC MMS metadatabase and determine that indeed it is an active station. Fortunately it is listed as NOT being part of the climate network, and neither USHCN or GISS uses this station.
From the lat/lon posted there (40.86667 -72.86667 ) I was able to locate the station on Google Earth:
Using the ruler tool - less than 4 meters from parking - Click for larger image
It turns out that the NWS Forecasts Office happens to be on the grounds of the Brookhaven National Laboratory, and the address is at 175 Brookhaven Ave, Upton, NY.
It seems that there is ample room in the grassy area in the rear to place a weather station, rather than putting it up front in the parking lot. A Microsoft Live maps image also shows the proximity issues up front and with the building.
Upton NY NWS office looking west - Click for live interactive image
Of course looking at this photo, it would now seem that the rear of the building might not be the best choice either with that bank of 5 a/c units back there. But it could find a site further away to the rear or perhaps cleared more trees.
Even if this station isn’t in the climate network,  it really does beg the question: why does the NWS blatantly flaunt flout their own 100 foot rule? Further, since this NWS Office is located on the grounds of the Brookhaven National Laboratory, wouldn’t you think they’d want to put their absolute best scientific foot forward?
Even is this station is only used to show school kids what a weather station looks like and how it is operated, why not do it right and show proper placement away from biases, proper door alignment on the screen, and explain why these things are important for proper measurements?
Or, maybe, these things aren’t important to the NWS at all.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ae5fb3f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Of course many of you that live in this weather already know this, but there is an early start to winter this year, not only in the USA, but also in London, where it snowed in October for the first time in over 70 years.
So far, no mention of this broadly distributed U.S. record event in the mainstream media. There are a few individual mentions or record lows in Florida. See this Google News search.
Here, from NOAA’s  National Climatic Data Center (NCDC), is a list of these new or tied records for October 29th, 2008.
I find the -25 below in Alaska interesting, since it bested the old record by 4 degrees.
Here are the 115 new or tied low temperature records:
The table below has been formatting to fit the blog, Here is a direct link to the original data from NCDC



29 October 2008
Record
New (83)
Tied (32)
Previous
Record
Previous
Year
Period
of
Record


CIRCLE HOT SPRINGS, AK
-25.0°F
-21.0°F
2001
44


TONSINA, AK
-17.0°F
-16.0°F
1985
42


CAMP HILL 2 NW, AL
21.0°F
25.0°F
1968
76


HAMILTON 3 S, AL
23.0°F
24.0°F
1968
45


CENTREVILLE 6 SW, AL
26.0°F
28.0°F
2001
32


MUSCLE SHOALS AP, AL (KMSL)
27.0°F
28.0°F
1952
67


GREENVILLE, AL
28.0°F
29.0°F
2001
78


GENEVA #2, AL
29.0°F
29.0°F
2001
32


HIGHLAND HOME, AL
29.0°F
30.0°F
1976
112


HUNTSVILLE INTL AP, AL (KHSV)
30.0°F
30.0°F
2005
50


MONTGOMERY AP ASOS, AL (KMGM)
31.0°F
32.0°F
2001
60


ATMORE, AL
32.0°F
33.0°F
2001
48


MOBILE RGNL AP, AL (KMOB)
32.0°F
36.0°F
1987
60


FAIRHOPE 2 NE, AL
33.0°F
34.0°F
1952
89


CODEN, AL
34.0°F
35.0°F
1957
43


DAUPHIN IS #2, AL
47.0°F
48.0°F
2001
32


BOONEVILLE 3 SSE, AR
28.0°F
29.0°F
1993
30


MURFREESBORO 1 W, AR
29.0°F
29.0°F
1993
33


SPARKMAN, AR
29.0°F
29.0°F
2005
40


FORDYCE, AR
30.0°F
30.0°F
1993
71


ROHWER 2 NNE, AR
31.0°F
32.0°F
1997
47


WEST MEMPHIS, AR
31.0°F
33.0°F
1976
45


BLYTHEVILLE, AR
32.0°F
32.0°F
1939
79


EUDORA, AR
32.0°F
32.0°F
1997
45


PERRY, FL
29.0°F
32.0°F
1987
71


TALLAHASSEE WSO AP, FL (KTLH)
29.0°F
31.0°F
1987
63


GLEN ST MARY 1 W, FL
29.0°F
32.0°F
1957
80


MAYO, FL
30.0°F
30.0°F
1957
57


NICEVILLE, FL
31.0°F
33.0°F
2001
62


JACKSONVILLE INTL AP, FL (KJAX)
33.0°F
39.0°F
1987
60


APALACHICOLA AP, FL (KAAF)
34.0°F
41.0°F
1976
76


PENSACOLA RGNL AP, FL (KPNS)
36.0°F
38.0°F
1968
60


TAMPA WSCMO AP, FL (KTPA)
42.0°F
45.0°F
1963
75


ORLANDO INTL AP, FL (KMCO)
43.0°F
49.0°F
1952
54


DAYTONA BEACH INTL AP, FL (KDAB)
44.0°F
46.0°F
1957
60


KISSIMMEE 2, FL
44.0°F
45.0°F
1968
46


VERO BEACH INTL AP, FL (KVRB)
46.0°F
48.0°F
1943
57


FT MYERS PAGE FLD AP, FL (KFMY)
47.0°F
47.0°F
1910
109


WEST PALM BCH INTL AP, FL (KPBI)
49.0°F
51.0°F
1944
69


MIAMI INTL AP, FL (KMIA)
55.0°F
61.0°F
1968
60


FT LAUDERDALE INTL AP, FL (KFLL)
55.0°F
62.0°F
2006
35


KEY WEST INTL AP, FL (KEYW)
61.0°F
66.0°F
1957
56


NAHUNTA 6 NE, GA
28.0°F
30.0°F
1957
45


PLAINS SW GA EXP STN, GA
30.0°F
30.0°F
2001
52


BLAKELY, GA
31.0°F
34.0°F
1976
95


ALBANY CAA AP, GA
31.0°F
35.0°F
1952
33


BRUNSWICK, GA
39.0°F
40.0°F
1957
90


CASSODAY, KS
24.0°F
24.0°F
1993
46


IOLA 1 W, KS
26.0°F
26.0°F
1980
48


HOMER 3 SSW, LA
27.0°F
33.0°F
2001
55


BASTROP, LA
29.0°F
31.0°F
2005
78


ASHLAND, LA
30.0°F
32.0°F
2005
54


MONROE ULM, LA
30.0°F
32.0°F
2005
31


ALEXANDRIA AP, LA (KESF)
31.0°F
31.0°F
2005
56


MANSFIELD, LA
33.0°F
34.0°F
2005
32


JONESVILLE LOCKS, LA
33.0°F
39.0°F
2005
36


SLIDELL, LA
34.0°F
35.0°F
1957
52


BUNKIE, LA
34.0°F
34.0°F
1957
50


RED RVR RSCH STN, LA
34.0°F
35.0°F
2001
31


RESERVE, LA
35.0°F
35.0°F
1913
101


BOYCE 3 WNW, LA
39.0°F
41.0°F
2001
31


GALENA, MO
22.0°F
25.0°F
1963
43


MT VERNON M U SW CTR, MO
22.0°F
25.0°F
1980
48


BUFFALO 2 N, MO
22.0°F
23.0°F
1980
44


WASOLA, MO
25.0°F
26.0°F
1952
61


HICKORY FLAT, MS
26.0°F
27.0°F
2001
51


OAKLEY EXP STN, MS
27.0°F
28.0°F
2001
37


WINONA 5 E, MS
28.0°F
28.0°F
2001
54


GRENADA 5 NNE, MS
28.0°F
29.0°F
1957
53


MCCOMB AP, MS (KMCB)
31.0°F
34.0°F
1957
60


WIGGINS, MS
32.0°F
34.0°F
1957
52


ROLLING FORK, MS
32.0°F
35.0°F
2005
35


PASCAGOULA 3 NE, MS
33.0°F
33.0°F
1987
71


YAZOO CITY 5 NNE, MS
33.0°F
33.0°F
1963
46


GRANDFATHER MTN, NC
17.0°F
17.0°F
1968
52


SUPERIOR 4E, NE
20.0°F
21.0°F
1991
53


TUSKAHOMA, OK
24.0°F
31.0°F
1973
46


MARIETTA 5SW, OK
25.0°F
26.0°F
1952
67


LINDSAY 2 W, OK
27.0°F
31.0°F
1993
43


KEYSTONE DAM, OK
28.0°F
29.0°F
1980
41


PERRY, OK
28.0°F
28.0°F
1980
89


BROKEN BOW DAM, OK
32.0°F
32.0°F
1973
34


SANDHILL RSCH ELGIN, SC
30.0°F
30.0°F
1976
50


DICKSON, TN
23.0°F
23.0°F
1952
106


AMES PLANTATION, TN
28.0°F
29.0°F
2001
31


JOHNSON CITY, TX
28.0°F
34.0°F
1970
41


GILMER 4 WNW, TX
28.0°F
30.0°F
1952
72


MT VERNON, TX
28.0°F
35.0°F
1973
42


SMITHVILLE, TX
28.0°F
34.0°F
1957
81


WARREN 2 S, TX
29.0°F
33.0°F
1957
32


WEATHERFORD, TX
29.0°F
29.0°F
1913
103


EMORY, TX
29.0°F
35.0°F
1995
42


GREENVILLE KGVL RADIO, TX
30.0°F
30.0°F
1952
103


MADISONVILLE, TX
30.0°F
31.0°F
1955
61


CENTERVILLE, TX
30.0°F
33.0°F
1970
65


KERRVILLE 3 NNE, TX
31.0°F
36.0°F
2006
34


CENTER, TX
31.0°F
31.0°F
1952
65


FOWLERTON, TX
32.0°F
32.0°F
1970
52


HILLSBORO, TX
32.0°F
32.0°F
1913
97


HENDERSON, TX
32.0°F
36.0°F
1973
67


AUSTIN BERGSTROM INTL, TX (KAUS)
33.0°F
37.0°F
1970
35


CLEVELAND, TX
33.0°F
35.0°F
1965
44


HONDO MUNI AP, TX (KHDO)
34.0°F
40.0°F
1993
37


GRAPEVINE DAM, TX
35.0°F
35.0°F
1910
66


LONGVIEW 11 SE, TX
35.0°F
38.0°F
1993
33


LA GRANGE, TX
36.0°F
38.0°F
2005
46


TOWN BLUFF DAM, TX
36.0°F
37.0°F
2001
37


JACKSONVILLE, TX
36.0°F
36.0°F
1970
44


VICTORIA ASOS, TX (KVCT)
37.0°F
40.0°F
1980
53


STILLHOUSE HOLLOW DAM, TX
37.0°F
38.0°F
1970
40


EL CAMPO, TX
38.0°F
39.0°F
1970
36


MATAGORDA 2, TX
40.0°F
40.0°F
1952
78


ARANSAS WR, TX
40.0°F
46.0°F
1980
35


POINT COMFORT, TX
42.0°F
43.0°F
2007
48


RAYMONDVILLE, TX
45.0°F
45.0°F
1970
92




Here are 163 new or tied lowest high temperature records for October 29th, 2008
Here is a direct link to NOAA’s NCDC data for these records:



29 October 2008
Record
New (120)
Tied (48)
Previous
Record
Previous
Year
Period
of
Record


BRIDGEPORT 5 NW, AL
49.0
55.0
2001
44


SAND MT SUBSTN, AL
50.0
50.0
1952
59


MOULTON 2, AL
51.0
53.0
1973
49


TALLADEGA, AL
52.0
55.0
1973
107


CLANTON, AL
52.0
53.0
1910
110


SYLACAUGA 4 NE, AL
52.0
56.0
1997
46


BELLE MINA 2 N, AL
52.0
53.0
1952
57


VERNON, AL
54.0
55.0
1973
49


HAMILTON 3 S, AL
54.0
58.0
1968
45


GREENVILLE, AL
55.0
59.0
2001
78


JASPER, AL
55.0
55.0
1976
45


EVERGREEN, AL
55.0
57.0
1910
83


THORSBY EXP STN, AL
55.0
57.0
1997
50


BREWTON 3 SSE, AL
57.0
60.0
1958
79


CODEN, AL
59.0
59.0
1997
44


MARSHALL, AR
52.0
52.0
1969
54


FT BRAGG 5 N, CA
53.0
53.0
1953
72


FERNANDINA BEACH, FL
64.0
64.0
2001
109


ST PETERSBURG, FL (KSPG)
64.0
64.0
1952
96


GAINESVILLE RGNL AP, FL (KGNV)
64.0
64.0
2007
45


ST AUGUSTINE LH, FL
66.0
69.0
1987
34


KEY WEST INTL AP, FL (KEYW)
71.0
74.0
1987
56


FT LAUDERDALE INTL AP, FL (KFLL)
76.0
78.0
1989
35


ALPHARETTA 4 SSW, GA
49.0
53.0
1959
41


GAINESVILLE, GA
49.0
49.0
1910
103


ALLATOONA DAM 2, GA
50.0
53.0
1953
43


DALLAS 7 NE, GA
51.0
55.0
1976
50


ELBERTON 2 N, GA
51.0
51.0
1910
68


HARTWELL, GA
51.0
53.0
2001
94


TOCCOA, GA
51.0
51.0
1910
105


SILOAM 3 N, GA
56.0
56.0
2003
46


MAUNA LOA SLOPE OBS 39, HI
48.0
48.0
1976
49


NORMAL 4NE, IL
45.0
45.0
1988
31


PERU, IL
46.0
46.0
1988
45


COLUMBIA CITY, IN
39.0
41.0
1968
44


PORTLAND 1 SW, IN
41.0
43.0
1976
30


BLUFFTON 1 N, IN
42.0
44.0
1980
36


NEW CASTLE 4 SSE, IN
42.0
42.0
1968
58


BAXTER, KY
44.0
49.0
1968
56


WEST LIBERTY 3NW, KY
45.0
46.0
1973
56


MT VERNON, KY
45.0
48.0
1980
49


JAMESTOWN WWTP, KY
47.0
48.0
1976
31


MONTICELLO 3 NE, KY
47.0
47.0
1980
52


PAINTSVILLE 1 E, KY
47.0
51.0
2003
30


BRADFORDSVILLE, KY
48.0
48.0
1968
44


BARBOURVILLE, KY
48.0
50.0
1953
54


FROSTBURG 2, MD
37.0
39.0
1976
36


SAVAGE RVR DAM, MD
39.0
41.0
1976
56


EMMITSBURG 2 SE, MD
48.0
48.0
1965
50


CUMBERLAND 2, MD
50.0
50.0
2002
32


IONIA 2 SSW, MI
39.0
42.0
1988
69


LAPEER WWTP, MI
40.0
41.0
2006
56


GROSSE POINTE FARMS, MI
44.0
44.0
2006
57


SHELBINA, MO
48.0
48.0
1980
62


WELDON SPRING NWS, MO
50.0
50.0
1976
42


PORTAGEVILLE, MO
50.0
50.0
1976
41


RIPLEY, MS
50.0
54.0
1968
66


INDEPENDENCE 1 W, MS
51.0
52.0
1976
50


IUKA, MS
51.0
57.0
1997
30


PONTOTOC EXP STN, MS
51.0
54.0
1968
55


HICKORY FLAT, MS
52.0
52.0
1980
51


WINONA 5 E, MS
52.0
54.0
1997
54


HOLLY SPRINGS 4 N, MS
52.0
54.0
1976
46


EUPORA 2 E, MS
53.0
55.0
1976
76


GRENADA 5 NNE, MS
53.0
56.0
1997
53


CALHOUN CITY, MS
53.0
59.0
1980
52


BELZONI, MS
55.0
57.0
1976
76


NORTH WILKESBORO, NC
48.0
52.0
1976
53


YADKINVILLE 6 E, NC
48.0
51.0
2003
50


STATESVILLE 2 NNE, NC
50.0
52.0
2003
101


ALBEMARLE, NC
53.0
55.0
2003
96


CLAYTON WTP, NC
55.0
55.0
2001
47


LEWISTON, NC
55.0
56.0
2005
52


ELIZABETHTOWN 3 SW, NC
56.0
60.0
2005
47


CAPE HATTERAS MITCHELL, NC (KHSE)
56.0
56.0
1976
51


FLEMINGTON 5 NNW, NJ
42.0
45.0
1976
110


NEW BRUNSWICK 3 SE, NJ
43.0
44.0
1976
40


DELHI 2 SE, NY
33.0
35.0
1952
75


BINGHAMTON WSO AP, NY (KBGM)
33.0
33.0
1952
60


WARSAW 6 SW, NY
35.0
35.0
1965
53


BAINBRIDGE 2 E, NY
35.0
39.0
1939
56


NORWICH, NY
36.0
37.0
1925
99


WATERTOWN AP, NY (KART)
37.0
39.0
1962
59


ELMIRA, NY
38.0
38.0
1928
112


PORT JERVIS, NY
40.0
40.0
1952
113


YORKTOWN HTS 1 W, NY
40.0
43.0
1976
43


WEST POINT, NY
42.0
42.0
1952
108


CADIZ, OH
39.0
41.0
1910
102


COSHOCTON AG RSCH STN, OH
40.0
42.0
1980
51


STEUBENVILLE, OH
40.0
41.0
1952
66


NEWARK WTR WKS, OH
42.0
42.0
1952
73


HANNIBAL L&D, OH
42.0
43.0
1976
33


NAPOLEON, OH
42.0
46.0
1980
39


NEW LEXINGTON 2 NW, OH
43.0
43.0
1952
66


WASHINGTON COURT HOUSE, OH
44.0
45.0
1968
81


BRADFORD RGNL AP, PA (KBFD)
31.0
35.0
2002
51


PLEASANT MT 1 W, PA
33.0
35.0
1959
55


DUBOIS FAA AP, PA (KDUJ)
34.0
38.0
1968
41


FRANCIS E WALTER DAM, PA
35.0
39.0
1976
41


WELLSBORO 4 SW, PA
36.0
37.0
1980
74


HAWLEY 1 E, PA
36.0
44.0
1997
82


CHALK HILL 2 ENE, PA
37.0
43.0
1990
31


MATAMORAS, PA
37.0
45.0
1965
42


TOWANDA 1 S, PA
38.0
39.0
1925
114


CONFLUENCE 1 SW DAM, PA
39.0
40.0
1957
62


TIONESTA 2 SE LAKE, PA
40.0
40.0
2001
65


WAYNESBURG 1 E, PA
41.0
44.0
1976
47


STEVENSON DAM, PA
42.0
43.0
2001
39


HAMBURG, PA
43.0
43.0
1907
67


WEST CHESTER 2 NW, PA
44.0
44.0
1976
103


LEWISTOWN, PA
46.0
47.0
1997
66


LONG CREEK, SC
49.0
52.0
1952
54


CHESTER 1 NW, SC
51.0
52.0
1959
76


PICKENS, SC
52.0
54.0
1952
57


SUMTER, SC
54.0
58.0
2001
81


CALHOUN FALLS, SC
54.0
55.0
1925
90


MANNING, SC
56.0
58.0
2001
35


BAMBERG, SC
56.0
57.0
1959
56


ANDREWS, SC
58.0
58.0
2001
37


ALLARDT, TN
43.0
44.0
1968
78


MONTEAGLE, TN
44.0
45.0
1952
68


TAZEWELL, TN
46.0
50.0
1976
42


LIVINGSTON RADIO WLIV, TN
48.0
50.0
1973
43


NEAPOLIS EXP STN, TN
49.0
52.0
1976
31


PORTLAND SEWAGE PLT, TN
50.0
51.0
1976
52


COVINGTON 3 SW, TN
50.0
51.0
1976
109


LINDEN WTP, TN
50.0
53.0
1976
45


SMITHVILLE 2 SE, TN
51.0
54.0
1976
36


SELMER, TN
51.0
54.0
1976
50


PULASKI WWTP, TN
51.0
57.0
2001
50


LEXINGTON, TN
51.0
51.0
1968
41


RIPLEY, TN
51.0
53.0
2002
43


MARTIN U OF T BRANCH E, TN
52.0
52.0
1976
72


CHEATHAM L&D, TN
52.0
54.0
1976
35


BROWNSVILLE, TN
52.0
52.0
1973
101


ATHENS, TN
52.0
52.0
1976
46


WYTHEVILLE 1 S, VA
39.0
41.0
1893
86


ABINGDON 3 S, VA
40.0
52.0
2006
36


BLACKSBURG NWSO, VA
40.0
46.0
1976
54


PULASKI 2 E, VA
40.0
43.0
1968
53


SALTVILLE 1N, VA
40.0
50.0
1968
49


GRUNDY, VA
42.0
47.0
1968
44


STAFFORDSVILLE 3 ENE, VA
42.0
48.0
2001
37


LURAY 5 E, VA
46.0
46.0
1976
66


STERLING RCS, VA
50.0
51.0
2002
31


WEST ALLIS, WI
43.0
44.0
1954
46


SNOWSHOE, WV
24.0
29.0
2005
31


TERRA ALTA #1, WV
31.0
40.0
1967
43


BELINGTON, WV
35.0
41.0
1976
41


ROWLESBURG 1, WV
36.0
40.0
1976
66


SUMMERSVILLE LAKE, WV
37.0
43.0
1976
41


BUCKEYE, WV
37.0
42.0
1968
46


FAIRMONT, WV
39.0
43.0
1952
102


ELKINS RANDOLPH CY AP, WV (KEKN)
39.0
39.0
1952
82


WESTON, WV
39.0
39.0
1925
106


CLARKSBURG 1, WV
39.0
44.0
1934
83


UPPER TRACT, WV
39.0
39.0
1910
38


OAK HILL, WV
40.0
45.0
1976
67


MORGANTOWN L&D, WV
40.0
42.0
1980
62


WEST UNION 2, WV
41.0
45.0
1976
35


MIDDLEBOURNE 3 ESE, WV
41.0
48.0
1980
66


GASSAWAY, WV
41.0
47.0
1952
54


PINEVILLE, WV
42.0
48.0
1976
62


GRANTSVILLE 1 ESE, WV
42.0
48.0
1976
43


BLUESTONE LAKE, WV
42.0
46.0
1976
65


DUNLOW 1 SW, WV
44.0
47.0
1997
36


RIPLEY, WV
44.0
44.0
1988
61


PARKERSBURG, WV
44.0
44.0
1952
82




Here are the 63 snowfall records:
Direct link to NOAA’s NCDC data for snowfall records
HTML clipboard



29 October 2008
Record
New (63)
Tied (0)
Previous
Record
Previous
Year
Period
of
Record


ASHFIELD, MA
1.5 in
0.0 in
2007
30


EAST BRIMFIELD LAKE, MA
0.1 in
0.0 in
2007
46


MC HENRY 2 NW, MD
9.0 in
2.0 in
2006
37


FROSTBURG 2, MD
3.4 in
0.7 in
2006
36


SANDUSKY, MI
0.5 in
Trace
1925
99


MAPLE CITY 1E, MI
0.3 in
Trace
1993
49


MARSHALL, NC
1.0 in
0.2 in
1910
109


GRANDFATHER MTN, NC
0.5 in
Trace
1973
53


MT WASHINGTON, NH (KMWN)
10.1 in
9.5 in
2000
60


POTTERSVILLE 2 NNW, NJ
2.0 in
0.0 in
2007
40


NEW BRUNSWICK 3 SE, NJ
1.5 in
0.0 in
2007
40


FLEMINGTON 5 NNW, NJ
1.0 in
0.8 in
1965
110


HOOKER 12 NNW, NY
19.0 in
3.5 in
1968
97


STILLWATER RSVR, NY
13.0 in
2.0 in
1990
83


TUPPER LAKE SUNMOUNT, NY
13.0 in
2.0 in
1934
109


LOWVILLE, NY
9.0 in
3.0 in
1893
116


PISECO, NY
8.0 in
1.0 in
2006
65


HIGHMARKET, NY
5.2 in
3.0 in
1965
84


NEWCOMB, NY
4.8 in
1.0 in
1965
49


CANTON 4 SE, NY
4.5 in
1.5 in
1962
115


INDIAN LAKE 2SW, NY
3.0 in
1.5 in
2006
109


ROCK HILL 3 SW, NY
2.3 in
0.0 in
2007
45


FRIENDSHIP 7 SW, NY
2.0 in
1.3 in
2006
39


LOCKE 2 W, NY
2.0 in
0.0 in
2007
76


BINGHAMTON WSO AP, NY (KBGM)
0.6 in
0.4 in
1952
60


JAMESTOWN 4 ENE, NY
0.5 in
0.0 in
2007
48


YOUNGSTOWN WSO AP, OH (KYNG)
1.6 in
0.6 in
1952
74


CLEVELAND WSFO AP, OH (KCLE)
0.3 in
Trace
2003
60


RIDGWAY, PA
6.0 in
Trace
1987
115


MEYERSDALE 2 SSW, PA
3.0 in
Trace
2006
45


DUNLO, PA
3.0 in
0.5 in
2006
60


SOMERSET, PA
2.8 in
1.4 in
2006
59


MAHANOY CITY 2 N, PA
2.1 in
0.0 in
2007
36


EBENSBURG SEWAGE PLT, PA
2.0 in
1.0 in
1965
44


KANE 1NNE, PA
2.0 in
1.0 in
1965
114


CONFLUENCE 1 SW DAM, PA
2.0 in
Trace
1965
62


MERCER, PA
2.0 in
Trace
1990
58


GLEN HAZEL 2 NE DAM, PA
2.0 in
1.5 in
2006
66


CHALK HILL 2 ENE, PA
1.2 in
Trace
1987
31


BOSWELL, PA
1.0 in
Trace
1965
48


PORT ALLEGANY, PA
1.0 in
0.5 in
2006
60


TIONESTA 2 SE LAKE, PA
0.8 in
0.5 in
1965
87


SLIPPERY ROCK 1 SSW, PA
0.7 in
Trace
2006
59


FRANCIS E WALTER DAM, PA
0.7 in
Trace
1990
45


PITTSBURGH WSCOM 2 AP, PA (KPIT)
0.6 in
0.4 in
1952
63


BUFFALO MILLS, PA
0.3 in
Trace
1965
84


MATAMORAS, PA
0.3 in
0.0 in
2007
104


MT MANSFIELD, VT
12.0 in
4.0 in
2006
53


ROCHESTER, VT
2.5 in
1.0 in
2000
79


MORRISVILLE 4 SSW, VT
1.4 in
Trace
2007
46


ESSEX JUNCTION 1 N, VT
1.2 in
Trace
2000
36


NEWPORT, VT
1.2 in
1.1 in
2000
78


ST ALBANS RADIO, VT
1.0 in
0.3 in
1992
30


CORINTH, VT
1.0 in
0.0 in
2007
60


SNOWSHOE, WV
8.0 in
1.0 in
1995
33


BAYARD, WV
5.5 in
1.5 in
1952
106


TERRA ALTA #1, WV
5.0 in
1.5 in
2006
60


GLADY 1 N, WV
4.4 in
Trace
2005
35


VALLEY HEAD, WV
3.2 in
2.0 in
1952
70


BELINGTON, WV
1.6 in
Trace
1968
70


BARTOW 1 S, WV
0.5 in
0.1 in
2006
64


ROCK CAVE 2 NE, WV
0.5 in
0.0 in
2007
55


SUTTON LAKE, WV
0.1 in
0.0 in
2007
91





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bbd0a86',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterH/T EIKE
The folly of windfarms and solar panels knows no boundaries. For example nowhere today in the north German area where I live is it possible to drive or take my bike 10 minutes anywhere without seeing a cluster of white behemoths chopping through the landscape (and birds).
Photo source: www.greenpeace.org. Here Greenpeace is major proponent of such let’s-target-and-desecrate-the-landscape projects.
Just days ago, Germany’s leading, renown political daily the Frankfurter Allgemeine Zeitung (FAZ) had a piece on the dark side of alternative energy sources, writing:
This program for ‘rescuing the climate’ is reckless, technocratic and ugly.

The FAZ adds,
Wind and solar parks will change the landscape with no consideration of protecting nature. Soon seeing natural landscape  will be possible only by looking in story books.
In fact, disfiguring the landscape is taking on such proportions and has gotten so out of control that some of the windmills’ former proponents, real environmentalists, are now beginning to sober up and are painfully realising the disaster they’ve helped to create.
The FAZ reports that even the President of the Federal Environmental Department, Jochen Flasbarth, is having second thoughts. He warns that an attempt to supply Germany with 100% renewable energy by 2050 will lead to a:
…huge aggravation of its citzens. The unavoidable consequence of a decentralized supply of renewable energy is that it will lead to an alteration of the landscape in many regions.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Face it, windparks need lots of space and are eyesores. It takes 1000 large 5-MW turbines to replace a single 1000 megawatt coal fired power plant. And because the turbines rarely work at 100% capacity (20% is typical) you need 5 times as many! And when the wind stops blowing, you need a conventional power plant to kick in anyway (200% capacity to ensure 100% supply).
And because land is not in plentiful supply, many projects are slated for offshore. The FAZ writes:
Most people don’t have any idea as to how big the dimensions of these windparks are, and even less an ideas of the ecological damage they cause to marine wildlife during their set-up and threat they pose to shipping vessels later. Germany’s Green party and environmental groups are silent on this topic.
The sprouting of offshore windmills is becoming such a threat that even the WWF warns of the Wild Wild West of the Baltic Sea. Sweden’s The Local writes on wind energy development until 2030:
Wind energy resources are forecast to undergo significant expansion over the period, from a current 400 megawatts to 25,000 megawatts which correspond to a 60-fold increase by 2030. Much of the expansion in wind energy will occur at sea.
Ironically environmentalists always protest loudly when it comes to building a new street, fearing for the life of field mice and worms. Yet they don’t seem to care about the destruction of natural scenery that hundreds of windparks cause and the thousands of migratory birds they kill as they navigate through their beloved machanized killing fields.
Well, how can that be? It gets down to green money. According to the FAZ:
Greenpeace earns money in replacing highly efficient, space-saving coal and nuclear power with extremely expensive, vast space-consuming, ugly and dangerous-to-wildlife, mechanized eyesores that happened to have the ecological seal of approval.
I ask myself: How much longer will it be before the real environmentalists and citizens wake up to this utter folly?
Share this...FacebookTwitter "
"

Al Gore’s histrionics are amusing, but nothing he has said compares to Sheryl Crow’s proposal to restrict how much toilet paper can be used. Perhaps there can be a new monitoring bureaucracy to search our homes. Maybe government agencies can stand guard in public restrooms. The BBC reports on the latest in cutting‐​edge environmentalism: 



Singer Sheryl Crow has said a ban on using too much toilet paper should be introduced to help the environment. … The 45-year-old…has just toured the   
US on a biodiesel‐​powered bus to raise awareness about climate change. …The pair targeted 11 university campuses to persuade students to help combat the world’s environmental problems. … “I have spent the better part of this tour trying to come up with easy ways for us all to become a part of the solution to global warming,” Crow wrote. …“I propose a limitation be put on how many squares of toilet paper can be used in any one sitting.”



Crow’s publicists managed to get the BBC to reference her biodiesel bus, but her environmental bona fides do not stand up under closer scrutiny. Thesmok​ing​gun​.com exposes the demands she makes when going on tour: 



The rock star’s performance contract includes specific day‐​to‐​day instructions on what kind of booze Sheryl needs in her dressing room (TSG has never seen such attention to detail in any other concert rider we’ve posted). … promoters are directed to purchase specific booze depending on what day of the week the concert falls, as the below rider excerpt reveals. Additionally, when the global warming warrior hits the road, her touring entourage (and equipment) travels in three tractor trailers, four buses, and six cars. Now that’s a carbon footprint!
"
"

A new piece of scientific research hit the presses last week. It reported finding more warming in one of the (several) satellite-observed temperature histories of the earth’s lower atmosphere than had been previously reported. As these satellite-measured temperatures were the recent subject of comments made by presidential candidate Ted Cruz, a lot of scrutiny and interest surrounds these new findings—findings which seemed to refute some of Cruz’s assertions.   
  
In researching his story on the new study, the Associated Press’s Seth Borenstein solicited my opinion about them and how they may alter climate change skeptics’ way of thinking about the satellite-observed temperatures—temperature datasets which had previously shown precious little warming over the past nearly two decades.   
  
I was happy to offer my thoughts, and equally happy to see some of them reflected in Seth’s AP story. Given topical and length constraints, understandably, Seth had to be selective.   
  
But I do have a bit more to say about the new research finding besides that it “shows ‘how messy the procedures are in putting the satellite data together.’”   
  
Many of my additional thoughts were included in my broader email response to Seth’s initial inquiry and, with his permission, I am reproducing our correspondence below.   
  
To Seth’s summary of my thoughts, I’d add “but even considering the new findings, the complete collection of satellite- and weather balloon-observed temperature histories of the earth’s atmosphere indicate that climate models are projecting too much warming in this important region.”   
  
Again, my thanks to Seth for reaching out to me in the first place. Here is out question and answer exchange:



Chip,   
  
Seeing that the climate doubter community has hinged so much on RSS and saying there has been no warming post 1997 _ despite NOAA heat records in 1998, 2005, 2010, 2014 and 2015 _ you’ve seen the RSS update that shows there has been warming in the last 18 years. I’m wondering what your thoughts are on it. Will you and those in your community keep using RSS, even if it shows no warming. Add to that the UAH record warming in February. Are satellites now contradicting the climate doubter community?   
  
Thanks,   
Seth



 ****   




Seth,   
  
Thanks for soliciting my opinion.   
  
I can't speak for the climate doubter community, however that is defined.   
  
Personally, my doubts are not that human-caused climate change as a result of greenhouse gas emissions is not occurring and that a temperature rise as a result is not detectable in large spatial averages, but I have doubts that the change is taking place at the rate projected by the collection of climate models and that its effects are currently detectable on most smaller scale climate/weather metrics.   
  
So with that out of the way, I’ll give some opinions as to the new RSS results and their importance to my way of thinking…   
  
First off, as I have tweeted (https://twitter.com/PCKnappenberger/status/705515578325270529), the overall 1979-2014 trend in the RSS v4 MT data is still pretty far beneath the climate model expectations…far enough to continue to indicate a sizable discrepancy that needs further scientific attention.   
  
Second, the trend in the new RSS v4 MT now makes it the mid-tropospheric (MT) dataset (including other satellite based and weather-balloon based) that has the greatest trend over the 1979-2014 period (see the same tweet mentioned above, as well as this one, https://twitter.com/PCKnappenberger/status/705472903458914305 which shows the old and new RSS data in comparison to weather-balloon compilations).   
  
Given these two things, I don’t think it helps settle any questions regarding the temperature behavior of the mid-troposphere.   
  
But what it does do is shed more light on just how messy the procedures are in putting the satellite data together. Decisions, guided by science but not specifically defined by it, occur at many points in the procedure. The new RSS paper, again highlights how sensitive the final results are to those decisions. It is good that we have many different groups involved in assembling both the satellite history and the weather-balloon history. That these different groups provide answers that are pretty close to each other helps not to lower the uncertainty in any single result, but that the general result is not indicative as to what is going on in the MT. The new RSS v4 now lies outside the old envelop of these collective findings. It’ll either prove to move the science in a bit of a different direction, or prove to be an erroneous result. Time will tell.   
  
As to the impact on the “pause,” IMO there was too much being made about the “pause"" in the first place. No serious student of climate science thought that it would last forever. The important thing about it was that it provided a challenge to climate science and prompted enhanced research into natural climate variability, climate sensitivity, and other important aspects of climate science. So that it’s now over comes as no surprise. But, once the El Nino warming subsides, I think we’ll probably see a continuation of the modest (below model mean) rate of warming.   
  
I hope this is useful. If you have any further questions, I’d be more than happy to try to answer them.   
  
-Chip



In addition to Seth’s story for the AP, more reactions about the new satellite-study can be found at Watts Up With That, Climate Etc., and at Roy Spencer’s blog, among others.


"
"The international gas market was once constrained by pipelines and long-term trade deals. Now, it is rapidly globalising. Cross-border trade and investment in gas is growing, and national gas markets are now more connected than ever. This is largely a good thing, but for some countries these changes expose their consumers to the highs and lows of global markets. Take the UK. Colleagues and I recently completed a research project on the country’s global gas challenge. As with everything from oil and wheat to laptops and mobile phones, the UK now sources most of its natural gas abroad. Import dependency – the ratio of gas imports to domestic consumption – hit an all-time high of over 50% in 2013.  This imported gas reaches the UK through pipelines from Norway’s offshore gas fields and the European gas market, and through specially designed ships carrying liquefied natural gas (LNG). The international natural gas trade – and the UK’s gas supply chain – are undergoing a profound globalisation in which LNG’s much greater geographical flexibility versus pipeline gas is playing an important role. Liquefying natural gas makes it cheaper to transport and store. The gas, which is mainly methane, is cooled to below its boiling point of -162˚C so that its volume is reduced 600-fold. A tank of LNG stores roughly as much energy as the same amount of crude oil, making it commercially possible to move gas by either road or ship beyond the limits of the pipeline network.  Though technology to turn natural gas into liquid is not new, major investments have driven a doubling of ocean-borne LNG trade so that it now accounts for a third of all internationally-traded gas. No longer constrained by pipelines, LNG is generating a more geographically complex and globally interconnected gas market. The UK’s growing import dependency has drawn it into this globalising gas trade. The country’s three active terminals – South Hook and Dragon in south Wales, and the Isle of Grain near London – are capable of importing more than two-thirds of annual gas consumption. Building these terminals has opened the UK up to gas imported from far and wide. Physical infrastructure is one thing; whether gas actually shows up is quite another. The volume of liquefied gas arriving in the UK has been a lot less than physical capacity would suggest and highly variable over time.  UK imports of LNG: Contracts concluded between LNG sellers and buyers in the UK allow cargo to be diverted to take advantage of regional differences in gas prices. Differences in price – and the strategies adopted by LNG producers in placing their gas – are important determinants of how much LNG flows to the UK and when.  LNG imports peaked at a third of UK gas consumption in 2011 as the country took delivery of liquefied gas originally intended for the US but displaced by growing shale gas production. Imports quickly fell away, however, in the second half of that year as LNG was diverted to Japan in the wake of the tsunami and the post-Fukushima nuclear shutdown. Things changed when the large price differential between UK/European and Asian gas markets sharply narrowed, in part because of falling oil prices throughout 2014. In the past six months or so, LNG cargoes have begun to arrive in the UK more frequently. The capacity to import more gas makes the UK more resilient. Yet it also introduces new uncertainties and vulnerabilities.  The waxing and waning of liquefied gas imports indicates how the UK functions as a reserve market for global LNG: its physical infrastructure and market liquidity can absorb substantial cargoes, but much of this gas will go elsewhere when more attractive opportunities are available.  The geographical flexibility of LNG has diversified the UK’s supply options but, at the same time, it has also created new dependencies, with more than 90% of the UK’s LNG imports coming from Qatar. A substantial proportion of the UK’s current LNG supply depends, therefore, on the whim of Qatar Petroleum.  “Gas security” requires understanding the UK’s changing position in a globalising gas market: increasingly that means looking beyond Russia and Ukraine to consider the implications of new developments in global LNG."
"

Global warming is increasing Western wildfires! 



At least that’s what lots of news stories said in response to a July 6 Sciencexpress paper by A.L. Westerling of the Scripps Institute of Oceanography and three co‐​authors. 



Like most scientific issues, though, this one is more complicated than the headlines suggest. 



Westerling examined wildfire data between 1970 and 2003 and found that major fires have occurred four times as frequently, on average, since 1986 than they did from 1979 through 1985. But he had one key conclusion: “Whether the changes observed in western hydro‐​climate and wildfire are the result of greenhouse gas‐​induced global warming or only an unusual natural fluctuation, is presently unclear.” 



Why so unclear? In large part, because the science isn’t straightforward, and three decades is a very short period of climate time. 



Snowmelt and temperature are thought to be the driving factors for Western wildfires. The years with high wildfire frequency tend to be those in which snow begins melting earlier than normal, which the authors of the paper said was “not surprising.” But examination of Westerling’s own data shows no significant difference in snowmelt timing between the periods of 1970 to 1985 and 1986 to 2003. 



However, spring and summer regional temperatures have gone up slightly — about 1 degree Celsius — over the same period, which serves to increase evaporation and, thus, flammability. 



Changes of this sort are certainly not unprecedented. Rather than limiting the perspective to 34 years, why not look at the last 1,200? Two years ago, Columbia University scientist Edward Cook and several colleagues reconstructed the West’s drought history back to 800 A.D. They wrote that “compared to earlier megadroughts that are reconstructed to have occurred around A.D. 936, 1034, 1150, and 1253 … the current drought does not stand out as an extreme event, because it has not yet lasted nearly as long.” 



In fact, Cook’s study shows a general decline in Western drought over the last millennium, with the recent era looking pretty much like the long‐​term average. In other words, the West is naturally accustomed to more drought than it has experienced since it was colonized by immigrants. It is also worth noting that the Western population boom began in the early 20th century, the wettest era of the last 1,200 years. 



What are perhaps more interesting are the changes in overall moisture that have accompanied the warming of the U.S. in 20th century. 



“Drought” is a combination of lack of rain and increasing evaporation. The latter is obviously dependent upon temperature, as more surface moisture evaporates into a warmer atmosphere. 



But as the country warmed, precipitation also went up. In fact, it went up far more than evaporation did. So, all else being equal, the U.S. as a whole is a wetter place than it was before the planet’s surface temperature began to rise. 



This doesn’t mitigate the fact that the West is a dry, fire‐​prone region, and will continue to be, if history is any guide. But relating human‐​induced global warming to Western drought is more difficult. 



There are many indices of drought severity, all of which attempt to balance rainfall, evaporation, streamflow and other factors. Perhaps the most often used is the Palmer Drought Severity Index. It has been around for more than half a century, and the National Climatic Data Center, in Asheville, N.C., records it for different regions of the country. 



Most scientists think humans are behind the planetary warming that began in the mid‐​1970s. (Another warming of similar magnitude occurred in the early 20th century, but was entirely natural in origin). But what is the relationship between global warming and drought in the Western U.S.? 



There isn’t any. Statistically speaking, the correlation is zero, which means that as humans have warmed the planet, they haven’t influenced Western drought. This holds whether one starts at the beginning of the Palmer record, in 1895, or the first year of Westerling’s study, 1970. 



Seeing as we have had about a hundred years of global warming, the lack of a clear relationship between earth temperature and Western drought is reassuring, because the relationship between drought and fire is real, even if it is more complicated than people are led to believe.
"
"Boris Johnson’s reshuffle rewarded a number of Brexit-supporting MPs who, on the face of it, appeared odd choices for the roles to which they were appointed. Critics say the government’s choice to lead on the climate emergency has previously seemed a somewhat half-hearted environmentalist, the new attorney general has taken a dim view of the courts and the international development secretary appears to have questioned the value of international aid.  As president of the crunch UN Cop26 climate talks to be hosted by the UK this November, Sharma has a critical role. Yet he showed little interest in the climate emergency before taking over as secretary for international development last July. He has used the term “climate” only six times in parliament, and on only two of those occasions did he have anything substantive to say about the crisis. A Guardian analysis showed he voted only twice in favour of climate protection in 13 votes on the issue. Campaigners are also concerned that his dual role as business secretary will make him beholden to powerful vested interests. However, he did urge the World Bank last October to devote more funding to the climate. The new attorney general has often seemed at odds with the courts. She supported the proroguing of parliament, which was later ruled illegal by the supreme court. In an article last year, she defended the prime minister’s controversial move. She said: “They [remainers] say that proroguing parliament is thwarting democracy … They argue that Boris Johnson is stopping MPs from holding the government to account. That’s hypocritical … What the ‘Outrage Brigade’ has failed to mention during its angry five minutes on air is that parliament was already going to be away on recess for party conferences for three weeks.” In a comment piece on the Conservative Home website last month, she accused the courts of exercising “a form of political power”. She said that while the Human Rights Act was “noble in its intentions”, “the concept of ‘fundamental’ human rights has been stretched beyond recognition”. The new international development secretary has raised questions about the value of international aid. In 2012 – three years before she became MP for Berwick-upon-Tweed – she replied to a tweet from DfID saying: “No one in Africa should go hungry,” by saying: “Nor in the UK. There r kids in NE who have no regular meals due to chaotic parents. Should they go hungry?” A year later, she tweeted about an article by the Conservative donor Michael Ashcroft calling for an end to the guaranteed aid budget of 0.7% of gross national income (GNI), saying: “Interesting article by Lord Ashcroft on the value (or otherwise) of the overseas aid budget,” ending with “#charitybeginsathome”."
"
From The Sun
By VINCE SOODIN
Published: 26 Aug 2008
TWO British schoolgirls cheated death after being stung by a lethal Portuguese  Man O’War.
Paddling Molly Purcell, ten, suffered toxic shock and gasped for breath after  the tentacles of the sea creature — which can kill with a single sting —  wrapped around her arms and legs.
Pal Amelia Walsh, 12, was left with huge welts on her legs after brushing part  of the creature that was draped over a rock.
Medics used freezing water and seawater to flush out the toxins.
Bathers were evacuated after last Friday’s attack at Monmouth Beach, Dorset.
Molly, of Ascot, Berks, said: “I thought I was stung by a bee at first, then  suddenly it felt like my arm was on fire. It got worse and worse until I  couldn’t stop shaking.”
Last night mum Sheenagh said: “Molly is getting better but her arms are still  very swollen.”
Brits were warned last month of seven species of poisonous sea creatures  heading for our shores due to global warming.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d1e6c9d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The new minister for resources, Keith Pitt, says he sees no case to increase the level of taxation on the booming gas industry, despite concerns from experts that large multinationals are avoiding paying tax, and the budget is missing out on valuable revenue. The Queensland National, who replaced Matt Canavan in the portfolio after Canavan resigned to back Barnaby Joyce’s unsuccessful tilt at the party leadership, told Guardian Australia: “I think the taxation levels are reasonable where they are, and it will be steady as she goes.”  Pitt acknowledged there were calls to overhaul the tax regime, with concerns Australians are not being properly compensated for the extraction of the country’s abundant natural resources, but he said “there are also expert views around saying we should increase the GST, or change a whole pile of other policies, but that’s not the government’s position”. The new resources minister also called for: An expansion of Australia’s controversial coal seam gas industry, including the contentious Santos project in north-west New South Wales, arguing the industry had worked to fix problems and “the technology is proven”. “I think we should open up Narrabri and others,” Pitt said. “If we can drive down domestic gas prices, everyone’s lives are made easier, particularly business.” More exploration of carbon capture and storage, even though CCS has not been commercially viable despite years of development. Pitt said there were “real opportunities” with the technology that could be combined with ultra super critical coal plants to reduce emissions. He said he was firmly technology-neutral when it came to power generation, but “coal will continue to be an important part of not only the economy, but what happens in regional areas for a long time to come”. While he was supportive of the coal industry, Pitt declined to express a view about the viability of the Collinsville coal-fired power plant proposal that has split the government. He also declined to say whether the government would indemnify the project against future carbon risk. Scott Morrison has left open the option of his government indemnifying the plant from future carbon risk, and politically connected power baron Trevor St Baker told Guardian Australia no project at that scale could proceed without an indemnity from the commonwealth. But Pitt said those were issues for his colleague, the energy minister, Angus Taylor, and he said he would not be in a position to have an informed view about whether or not Collinsville stacked up until the feasibility study was completed. “That’s why we are doing the study,” he said. “I can’t assess the feasibility of the project until that is completed.” Pitt has previously advocated consideration of nuclear power in Australia, but declined to repeat that view now he was in the resources portfolio. He said it would be up to Taylor if nuclear proceeded in Australia, and he said it would certainly not proceed unless there was bipartisan support. He said he accepted the science of climate change, but declined to express a view about whether global heating was fuelled by human activity. Pitt said there was “no doubt” the climate was changing, but the focus now should be on building resilience, and on technological solutions to lowering emissions. Pitt said he has never held the view that “we should be pouring unrestricted amounts of all sorts of bits and pieces, not just CO2 or greenhouse gases, into the atmosphere”. Pitt’s comments came as Australia’s chief scientist, Alan Finkel, told the National Press Club Australia’s energy sector was already well on the way with the transition to low emissions energy. But he said while renewable technologies were “being scaled up, we need an energy companion today that can react rapidly to changes in solar and wind output”. Finkel said Morrison had nominated gas as the transitional fuel “in the short term”. But he declared the “hero” of the transition would be hydrogen. In parliament, Taylor said the government was committed to rolling out bilateral agreements with the states to guide the energy transition, like the $2bn deal unveiled recently with New South Wales that Canberra says will boost gas supply. But he said for that deal to be replicated, the states “must do the right thing”. “When it comes to those deals, whether it’s NSW or Victoria, the principle is very simple,” Taylor said. “No gas, no cash.”"
"
Share this...FacebookTwitterSnow already!
A cold blast of polar air has moved over central Europe, pushíng temperatures far below normal. Snow is forecast in the Alps at elevations as low as 1400 meters.
The following chart shows the forecast for the next 2 weeks. It shows that summer is finished for much of Europe. In fact, as mentioned above, the lower Alps are expected to get snowfall in the couple of days ahead. Continue reading below.
According to Snowfinder, it’s time to get your skis out. Snow is in the forecast at many ski resorts. At Mürren Schilthorn in Switzerland elev. 2360 meters, snow is forecast for early this week. The same is true for Kitzbühel in Austria at 2000 meters elevation.
Heck, even below 2000 meters snow is in the forecast, for example Steinplatte Waidring in Austria , which is only 1800 meters elevation. Maybe Nature will publish a study attributing it to global warming.
===================================================
Update 1,  August 30: Rain mixed with SNOW was reported on the Brocken summit, elevation 1141 meters, In north Germany this morning on north German NDR radio.
Update 2, August 30: Snow line has dropped to 1400m (below the 5000′ line) in the Alps. Heavy snow is in the forecast. See following chart (h/t: Reader Patagon):
Source: http://www.meteoexploration.com/snow/snowmaps.html
Share this...FacebookTwitter "
"Materials essential for technology products such as electric vehicles, wind turbines or hard disks, known as rare earth elements, aren’t becoming any less rare, or any less crucial. In fact, experts at a major rare earths conference in Milan on October 16 – the European Rare Earths Competency Network (ERECON) – agreed supply shortages will continue for the time being. This isn’t just a matter for tech companies: their gloomy outlook should be of crucial importance for the future of international relations.  We need new sources of such materials, and we need our current industries to become more resilient towards supply disruptions. They will happen and we must be prepared. Rare earth elements are like the salt and pepper of numerous everyday products – the world wouldn’t end without them, but it would be an impoverished place. Some say the worst is over. The supply crunch for rare earth became apparent when prices tripled in 2011, driven by concerns over limited supply and export restrictions imposed by China, the dominant producer at the time. Following this price peak the US, Japan and the EU successfully appealed to the WTO and forced China to open up its export market. Supplies from outside of China such as the Mountain Pass mine in California have also developed and China’s global market share has accordingly gone from 95% down to around 75%, according to conference participants. And geologists around the world don’t stop reassuring us that the reserves are plentiful. So, is this much ado about nothing? Actually, the picture looks less bright than those slowly emerging trends of a more independent supply might suggest. Most countries are at least as vulnerable as in 2011. China is progressively moving “downstream” in the rare earths supply chain. Once, it merely dominated in mining the actual materials, now it wants to create jobs and a high-tech manufacturing industry dependent on these rare earths. It has put measures in place to “encourage” manufacturers to locate their production in China, to “ensure” access to rare earths according to conference participants.  This leaves European producers in a pretty uncomfortable position. They want reliable contracts with downstream industries, which operate closer to the final products. But survival in competitive times is tough, and the firms that actually make cell phones or wind turbines are now locked into investment decisions that will require more rare earth supplies for the foreseeable future. There is a less-known ethical dimension in it. Some 40% of Chinese rare earth extraction is estimated to take place in illegal mines according to an expert who works with Chinese sources. Consumers should start realising that a significant number of fancy devices and cherished renewable energies are in fact based upon a dirty, bloody and illegal extraction process. On top of all that, the gunboat diplomacy that created the momentum for the 2011 supply crunch continues. Territorial disputes in the South and East Chinese Seas have left China on bad terms with several neighbouring countries. Given that historians constantly remind us that the Great War emerged out of a similar constellation in Europe in 1914, one should clearly be worried about what potentially could escalate into a serious catastrophe. The US, France, Germany and Japan have all run large-scale projects to test new recycling options that could help reduce dependency on new rare earths. These sorts of initiatives can yield results quickly. For countries who haven’t put such projects in place (including the UK) it’s high time to catch up. Manufacturers such as Siemens or General Electric who depend on rare earths seem ready and willing to engage. After all it is in their interest to establish supply chains that can withstand a run on these materials. A platform for engagement between government and manufacturers may require policy intervention – some sort of technology strategy board or an international metal recovery covenant. Though recycling and increased efficiency is important such steps won’t be enough by themselves: new supplies will be needed – and this means mining. Sweden has some of the most promising deposits in Europe, so has Greece, among other regions. Local politicians may struggle to get backing for new mining but the public interest in green technologies that rely on a sustainable supply should help. Greenland also has rich rare earth deposits – potentially a quarter of the world’s supply. China is already getting its foot in that door, with 1,500 workers already mining there, but the race is not yet over. The rare earth supply crunch is a short-term urgency with mid-term opportunities. Nobody should blame China for acting in its own interests. Rather, others must collaborate together to reach a solution. Last but not least international efforts to establish due diligence along the value chain should help China to move away from illegal mining and establish a more sustainable and transparent rare earth supply."
"

Some things are sacred to scientists: Facts, data, quantitative analysis, and _Nature_ magazine, long recognized as the world’s most prestigious science periodical. 



Lately, many have begun to wonder if Jayson Blair has a new job as their science editor. On page 616 of the April 8 issue, _Nature_ published an article using a technique that they said, on page 593 _of the same issue,_ was “oversold”, was inappropriately influencing policymakers, and was “misunderstood by those in search of immediate results.” 



The technique is called “regional climate modeling,” which attempts to simulate the effects of global warming over areas the size of, say, the United States. 



As reported by Quirin Schiermeier, scientists at a Lund, Sweden climate conference, “admitted privately that the immediate benefits of regional climate modeling have been oversold in exercises such as the Clinton administration’s US regional climate assessment, which sought to evaluate the impact of climate change on each part of the country.” 



Then, 23 pages later, _Nature_ published an alarming and completely misleading article predicting the melting of the entire Greenland ice cap in 1,000 years, thanks to pernicious human economic activity, i.e., global warming, using a regional climate projection. 



The lower 48 states comprise 2 percent of the globe. Schiermeier reported that the consensus of scientists is that climate models on such a small scale are inappropriate for policy purposes. Greenland covers 0.4 percent of the planet. If the models are no good over the U.S., they’re worse over Greenland. Yet the authors “conclude that the Greenland ice‐​sheet is likely to be eliminated by anthropogenic climate change unless much more substantial emission reductions are made than those envisaged by the IPCC [a United Nations Panel].” 



The Greenland paper, by Jonathan Gregory and two others, was profoundly misleading, offering any climate alarmist an incredible sound bite attributable to our most prestigious science publication. 



The first paragraph states: “The Greenland ice‐​sheet would melt…if the annual average temperature in Greenland increases by more than 3°C [5.4°F]. This could raise global average sea‐​level by 7 meters [23 feet] over a period of 1,000 years or more.” 



Guaranteed, that quote will be on _Hardball_ on May 28, the day that the non‐​science fiction global warming flick, _The Day After Tomorrow_ comes out. It’s ironclad. After all, it’s from _Nature._



And it’s also deceptive. It’s not a warming of 5.4°F that causes the massive meltdown. Instead, it’s an annual warming of an impossible 14°F. Given the way greenhouse warming splits between summer and winter, this implies an outlandish 30°F change in the winter, fueled by a world that would have to be producing carbon dioxide at a rate far beyond anything remotely possible. It is the most extreme scenario in a pack of outlandish future emission scenarios that the U.N. cooked up a few years ago. They actually call them “storylines,” which is appropriate, since they make little sense. 



For example, one of the major storylines assumes that people increasingly favor personal wealth over environmental protection, which is absurd. The richer a nation is, the richer a city is, or the more affluent a neighborhood is, the more it protects its environment. 



How did the first paragraph get by the editors at _Nature?_ Either they weren’t looking or they thought it was OK. Take your pick. 



It’s not the first time, either. Just as scientists “admitted privately” that the models don’t work, so have prestigious environmental journalists told me privately that they are concerned about _Nature’_ s handling of global warming stories, both in terms of increasingly shoddy reviews and timing clearly designed to influence policy. No one has forgotten that in 1996 _Nature_ featured a paper, right before the most important U.N. conference leading to the Kyoto protocol, “proving” that models forecasting disastrous warming were right. The paper was subsequently found to have used data selectively to generate its dire result. 



Note to _Nature:_ Even journalists, normally your friends on global warming, are getting suspicious. 



The Greenland paper is truly an exercise in virtual reality. The threshold for melting is based upon a uniform annual temperature rise. But, every scientist knows that greenhouse effect warming is much greater in winter, when the authors say “no melting takes place.” When they account for this (not reporting how they did so), fully one‐​third of their scenarios fall below the melt threshold. Only when they assume what is patently untrue do almost all the scenarios result in a net melting, and only the most extreme, illogical ones completely melt things. 



This is nothing but tragic, junk science, published by what is (formerly?) the most prestigious science periodical in the world. There’s been a lot of hype‐​much of it from scientists themselves‐​over global warming, but nothing as sacrilegious as this, in such a sacred place. 
"
"Is there any embarrassment in the Ocado boardroom for the part that supine non-executives played in sanctioning an £88m bonus for directors? It would seem not. Andrew Harrison, who chairs the remuneration committee, brazened it out in the annual report, urging shareholders to look at a different number – the 265% increase in the company’s share price over the past two years (and certainly not the £214m loss it delivered in 2019). Tim Steiner, the chief executive, got £54m of the pot, receiving a tidy £58.7m once his salary and other bonuses are taken into account. He’s hit the jackpot twice because the surge in the share price has boosted the value of his personal stake to nearly £300m.  The bonus, even measured by the yardstick of City excesses, is obscene. It’s one of the biggest bonus payouts made by a listed UK company, only bettered by the likes of Jeff Fairburn, the disgraced former chief executive of the housebuilder Persimmon, who pocketed £75m, and Sir Martin Sorrell, who banked £70m from WPP in 2015. Yet we are supposed to see Ocado differently these days. Think Tesla, not Tesco – well, Elon Musk’s potential $50bn pay deal provides a flattering comparison, at least. Today Ocado has successfully reinvented itself as a tech stock selling armies of grocery-picking robots to supermarkets – and says it should have Silicon Valley-style pay deals to keep its indispensable executives “motivated”. But for many people Ocado is still the company that pulls up at their front door with bags full of Waitrose groceries. That will end this autumn when the company will attempt to switch horses to Marks & Spencer – which is paying £750m for half its UK grocery business – without falling from the saddle. So you would think the company would be more concerned about its image. Once beloved of the middle classes, Ocado is facing incoming fire from parents and teachers opposed to its plan to open a distribution hub – complete with diesel fuel pumps – yards from their north London primary school. The company, which has been trialling electric vans in high-density areas, has said it will replace all the diesel vans if it can get a power upgrade at the site. Maybe the best part of the £100m that executives have pocketed would have been better spent on greening its fleet? The annual report shows that Steiner’s pay was more than 2,600 times that of the median employee, who all-in gets £22,500. As part of the preamble, the company highlights that executive pay is “more at risk than wider employee pay due to the use of variable pay” and without the mega bonus the multiple would only have been about 200 times. The icing on the cake was the decision to raise the basic salaries of the four executives who shared the £88m jackpot; this year Steiner will have to drag himself out of bed for a guaranteed £720,000 salary. Which brings us back to Waitrose, part of the employee-owned John Lewis Partnership, where staff in good years are paid the same rate of bonus as top executives. Once Ocado shifts over to M&S, shoppers will have to make a conscious decision to move away from their familiar service to Waitrose’s own website. As a shopper, all you can do is vote with your feet. The US environmentalist and author Bill McKibben famously warned that in the race to tackle the climate crisis, winning slowly is the same as losing. Last week BP only barely found its feet in the starting blocks – again. Almost 23 years ago, BP’s chief executive at the time, John Browne, became the first oil boss to publicly accept responsibility for the looming climate crisis. In a landmark address at Stanford University he promised “substantial, real and measurable” action from the UK oil giant. On the top of his list was a pledge to control BP’s own emissions. But almost a quarter of a century later, the world is still waiting. Browne was in the audience in London on 12 February at a London hotel conference room as his former protege and BP’s new chief executive, Bernard Looney, set out a new target for the oil industry’s climate ambitions. He one-upped Browne’s vow to cut carbon emissions by setting an ambition to reduce and “neutralise” enough greenhouse gases to turn BP into a carbon-neutral company. The 21st-century version of BP’s green turn may have relied more heavily on slick video production and branded graphics, but the pledges share a worrying parallel: neither set out how BP would achieve the grand goals set for the middle of this century. And so, the world has been asked to wait a little longer. Investors must wait until September to learn how Looney plans to make his green vision a reality. The net-zero revolution itself is a 2050 target, meaning another quarter of a century will pass before the company intends to meet it. Looney’s address acknowledged that people would be impatient for detail, and many others would never be satisfied that BP is going far enough, fast enough. On this point, at least, he is wholly correct. He is already on borrowed time. For anyone looking to invest their life savings, the gambling industry is hardly a tantalising prospect. Public unease about the industry, following criticism over its failings in dealing with problem gamblers, has reached boiling point. The Gambling Commission, the UK regulator, has begun flexing its muscles. It has already banned bets on credit cards, and promised to review a proposal by MPs that would see online casino stakes cut to £2, bringing web-based casino games into line with fixed-odds betting terminals. Betting shares duly tanked as investors digested the prospect of decreased revenues. Bookmakers fought tooth and nail to save FOBTs, only to emerge utterly defeated by a ragtag coalition of ex-addicts, MPs and rivals in the casino and amusement arcade business. With that defeat fresh in the mind, betting firms now fear the same treatment will be meted out to their lucrative online operations. The stakes have never been higher. Where once the industry dismissed concerns about problem gambling and sought to discredit its opponents, the newly formed Betting and Gaming Council has preferred the tactic of deflection. What about unauthorised overseas gambling sites, they ask. What about the role that banks and big tech companies can play in reducing harm? Most of all, the BGC points to the voluntary steps taken by the industry to address public problem gambling, including reining in advertising. Such olive branches, of course, were offered only after the industry found itself staring down the barrel of a gun. Now the government is reviewing the Gambling Act and has a once-in-a-generation chance to introduce precise, well-informed legislation. Cutting stakes to £2 would not be a cure-all. Limits may have a role in addressing gambling disorders but so do things like curbing the speed of play and robust affordability checks. Whatever the outcome, the odds are short on the sector emerging smaller and less profitable."
"
Share this...FacebookTwitterWhat is Europe to do? The old continent wants to save the world but it seems no one wants to play along.
Before the summer, there seemed to be some hope the USA could pass a climate rescue bill with President Barack Obama and the Democrat-held Congress. But that hope has all but completely disappeared.
So writes the very green Austrian Der Standard here: Europe Should Focus On China For Climate Protection, Not USA.
With the Democrats getting voted out of office in Congress, the respective concepts for protecting the climate will wind up in the trash can. Europe should quickly search for willing partners.
All together now: Ohhhhh! How dreadful!
But hope is the last to die, and not quite everyone has lost it. Yet, Der Standard sees the writing on the wall:
While there are still a few NGOs who have hope in the good of people and Obama, there’s no one left in the USA who is ready to bet even a single cent on a US climate law could get off the ground.
Jeffrey Sachs, Director of the Earth Institute at Columbia University in New York, and occasionally a columnist for Der Standard says:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There’s nothing left to salvage!
According to Der Standard:
In an interview conducted via Skype with participants of a Climate Workshop in Brussels, Sachs stated that people in the United States have practically zero interest in climate protection. At least for the next two years, but likely it’ll be much longer before any real kind of initiatives on the issue of climate protection can be expected.
Der Standard mopes that the oil disaster in the Gulf of Mexico had no impact on the public’s opinion, and that a wide majority of Americans favor offshore oil drilling. Now come the old conspiracies and paranoia. Der Standard:
The reason is fear of losing jobs and an unprecedented propaganda machine that was put into motion by the oil industry and its allies.
These allies include a number of evil European corporations like Eon,  BP, Bayer, Basf, Arcelor oder GDF-Suez. Now the enviro-malcontents in Europe and at Der Standard say Europe has to shift its focus to China. Der Standard writes that China has become a “high flyer” in wind and solar energy. Partnership there must be enhanced.
Europe has to accept that there is nothing more to gain from the USA, except frustration. It makes much more sense to work on mutual interests with China.
Consider this a major milestone. Cap & Trade is dead for good in the United States. Even Europe sees it.
Share this...FacebookTwitter "
" New South Wales bureaucrats sought urgent advice from major cotton farmers about how recent rainfall might damage their water harvesting infrastructure, in an apparent effort to justify giving them the green light to retain the first rainfall in over a year, rather than letting it flow downstream. On 7 February the government announced it would restrict the harvesting of overland flows throughout the northern Murray-Darling Basin for the first time, because it was “in the public interest”. But within days, the government had lifted the ban for two valleys and part of a third.  Emails obtained by the Guardian show the NSW Department of Planning, Industry and Environment asked irrigator groups for “urgent” advice on and examples of the sort of damage that their members might incur to levees, pumps and regulators if the water was allowed to flow across their land and then down the Barwon River to the Lower Darling. But by the time the emails were sent, the NSW government had already given the large cotton growing areas a three-day exemption from the embargo. That allowed several huge cotton properties between Walgett and Wee Waa, and west of Moree, to take the first flows in years. The Guardian understands the department received numerous representations from the cotton industry after the government declared the embargo. The government rarely uses its public interest powers and so it detailed the reasons for the embargo. These included “to cope with a water shortage”, to address a “threat to public health and safety” caused by critically low town water supplies and to “manage water for environmental purposes”, noting that the river system health was declining rapidly, putting aquatic biota at risk. The cotton properties have extensive water harvesting infrastructure such as levees and pumps, which are used to divert overland flows into massive on-farm storages. There is no information at this stage about how much water was harvested. However, the proliferation of this infrastructure – only some of which is approved – has had a major impact on how much rainfall reaches the rivers in the northern Basin. Some estimates say historical flows have been reduced to 40% or lower. The decision has angered farmers and communities in the lower Darling, where extremely dry conditions have led to mass fish deaths. Less water entering the river means it may not flow downstream and will instead seep into the dry riverbeds. The email was sent at 3.46 pm on 10 February by a bureaucrat involved in flood plain management to the heads of the Gwydir Valley irrigators, the Namoi Valley irrigators and Barwon Darling Irrigators. “URGENT: requesting further details re: infrastructure risks from temporary FPH restrictions,” the subject line read. “Could you please provide some examples of properties in your valleys where landholders have reported risks to infrastructure from not being able to take floodplain harvesting? “Please supply; property name, contact details (if you have them handy) and a quick description of reported at risk infrastructure.” The email was then forwarded by the irrigator bodies to the cotton growers. This appears to have been after the exemption had been granted. A spokesman for the department told the Guardian: “Following concerns during the rain event that the restrictions were exacerbating localised flash flooding, the department decided to take a precautionary approach due to the potential safety and infrastructure risks, and temporarily lifted the embargo in a limited area.” But the independent NSW MP Justin Field said claims that the decision was taken to reduce the risk to infrastructure didn’t pass the pub test. “This infrastructure is designed to divert flood water into on-farm storages,” he said. “The minister has to explain what infrastructure was at risk, on whose request the decision was made, why such a large area was exempted from the embargo and how much water has been diverted,” he said. A spokeswoman for the minister, Melinda Pavey, said she had not played a role. The department said: “We began investigations as soon as practicable, including an aerial review as soon as it could be arranged.” The Natural Resources Access Regulator (NRAR) said its officers had flown from Tamworth to Walgett after the most recent rainfall/flood event. “Of the properties observed it appeared that most had managed the volume of water, with limited infrastructure damage,” a spokeswoman said, noting this was not part of NRAR’s normal compliance activities. “There was limited infrastructure damage to roads, channels, levee banks, and pumps,” she said.  The NSW Department of Primary Industry and previous ministers have come under scrutiny for providing preferential treatment to cotton interests in the past. In the wake of a Four Corners program in 2017 which revealed a tape of senior NSW bureaucrats offering irrigators information so they could attack the Murray-Darling Basin plan, the head of the water division resigned. An independent inquiry referred a number of matters to the NSW Independent Commission against Corruption. Despite two years of investigations there have been no findings to date. The department said it was continuing to monitor river flows. The embargo is now in place for all designated floodplains until 28 February. More rain is expected this weekend."
"

The impact of state business taxation on employment and capital has been heavily debated in both academic and policy circles on both theoretical and empirical grounds. State‐​level business taxation could depress business activity through several channels. Businesses that might otherwise have hired or invested might simply not do so because of the difference between pre‐​tax and after‐​tax profits, or, alternatively, businesses might move their activities to another U.S. state. On the other hand, increased business taxation might not have a negative effect on business activity if businesses can change their activities to use more tax‐​favored production strategies or organizational forms, or if tax revenues are spent on public goods that improve the state’s business climate. As U.S. states face increasing fiscal pressures, the debate over the effects of state tax policy on state‐​level business activity is likely to intensify.



A long empirical literature has studied the geographic location decisions of new firms or establishments as a function of state tax and other characteristics. These studies have used aggregated panel data at the state, county, or industry level to examine the effect of state and local taxes on economic growth, employment, or capital formation. This line of work has faced two main challenges. First, tax policy is not exogenously determined, so that ascribing a causal interpretation to correlations between state tax changes and counts of businesses or employees has been problematic. The primary concern is that state governments might change tax policy in anticipation of changing economic conditions. Some work addresses this question by using county‐​level data to study how state taxation or business climates affect business activity in border counties between states that change policy and those that do not. The second challenge is that the studies have lacked comprehensive micro‐​data at the establishment level, so that the decisions of individual businesses cannot be tracked over time, leaving uncertainty as to whether firms are relocating their businesses to other regions or reducing the scale of their operations.



Our research uses comprehensive and fully disaggregated establishment‐​level data from the U.S. Census Bureau to examine the impact of state business taxation on employment and capital. We focus on firms with establishments in multiple states. To measure an effect of state tax policy on establishment counts, employment, and capital, we begin by exploiting the fact that the corporate tax code only has a direct effect on firms organized as subchapter C corporations, whereas firms organized as S corporations, partnerships, or sole proprietorships (so‐​called pass‐​through entities) are only directly affected by the individual tax code and other business taxes.



This setting allows for separate identification of the effects of the corporate tax code on the activities of C corporations and of the effects of the personal tax code on the activities of pass‐​through entities. Furthermore, the establishment‐​level micro data allow us to disentangle reallocation versus pure economic disincentives of taxation. Specifically, we examine whether firms increase their activities in a given state when taxes increase in the other states in which they are active.



We consider the complete sample of all U.S. establishments from 1977–2011 belonging to firms with at least 100 employees and having operations in at least two states. We find that a one percentage point increase (decrease) in the state corporate tax rate leads to the closing (opening) of 0.03 establishments belonging to firms organized as C corporations in the state. This corresponds to an average change in the number of establishments per C corporation of 0.4 percent. A similar analysis shows that a one percentage point change in the state personal tax rate affects the number of establishments in the state per pass‐​through entity by 0.2–0.3 percent. On the intensive margin, we find similar results. The elasticity of C corporation employment for a given establishment is 0.4 with respect to the state corporate income tax rate, and the elasticity of pass‐​through business employment is 0.2 with respect to the personal income tax rate. These effects are robust to controls for local economic conditions and heterogeneous time trends. 



Opposite effects of around half of these magnitudes are observed in response to tax changes in the other states in which firms operate, so that around half of the baseline effect is offset by reallocation of activity across states. This lends strong support to the view that tax competition across states is economically relevant and is consistent with earlier findings that emphasize the importance in the labor market of shifts in the distribution of employment opportunities across work sites.



We find no empirical correlation in the data between changes in employment at an establishment belonging to a C corporation and the personal tax rate. Similarly we find no empirical correlation between changes in employment at an establishment belonging to a pass‐​through entity and the corporate tax rate. The lack of cross‐​correlation is consistent with the identifying assumption in these regressions that there are not state‐​level trends in general business activity that follow changes in tax policy for reasons unrelated to the tax reforms. This finding also suggests that movement of activity between the corporate and noncorporate sector, while clearly important on the national level over the past several decades, plays a somewhat limited role in shaping the overall economic response to state‐​level tax changes.



Further analysis captures complexities, heterogeneity, and changes in state tax codes regarding apportionment of income in multi‐​state firms. If a company has physical presence in more than one state, the company has to apportion its profits according to each state’s apportionment factor weights for property, payroll, and sales. Furthermore, so‐​called throwback or throw‐​out rules at the state level require companies to apportion profits arising from sales into states where they have no physical presence back to states where they do. 



We show that the response of moving establishments and employees is greatest when those factors have greater apportionment weights. Even if the sales apportionment factor is large, we also find strong effects when throwback or throw‐​out rules are in effect, as these rules mitigate the tax attractiveness of firm moves to high sales‐​apportionment states.



We further examine whether there is evidence of confounding differential trends in C corporations versus pass‐​through entities in the years leading up to tax changes in a subsample of firms affected by states that changed at least one of their tax rates by at least 100 basis points. These large tax changes occurred 161 times during the sample period. Here we find similar directional results of somewhat smaller magnitude. Around half of the effects are felt in the tax year in which the tax rate changed, with the full force being felt in the following year.



Analysis on the subset of the Census data on manufacturing firms allows us to consider the impact of state taxation on capital formation and location. We find that capital shows similar directional patterns to labor in its response to taxation, but that the elasticities are 36 percent smaller. Furthermore, the detailed data on the location of manufacturing firm property allow us to consider the impact of a state policy that raises the actual tax claim on a dollar of total corporate profit by one percentage point, as opposed to increases in the statutory rate by one percentage point. Under this definition, which captures cross‐​sectional firm heterogeneity in the extent to which statutory changes affect the tax burden, we find somewhat larger elasticities of around 0.4 percent for labor and 0.3 percent for capital.



One concern about the analysis might be that the results could be affected by firms that change their organizational form in response to changes in the tax code. Earlier work finds that the share of economic activity by firms in corporate form does in fact respond to the relative taxation of personal to corporate state income. However, our sample in this paper consists only of firms with activity in more than one state, and firms must choose one organizational form that will be applicable to all entities. For these firms, federal tax policy should be far more important for the organizational form decision than the mix of state tax policies they face, a hypothesis we confirm in the data. Limiting the sample to the 92 percent of observations belonging to firms that do not change their organizational form within 5 years of tax changes leaves the results unaffected.



Overall, our findings on the effects of corporate taxation are larger than those found in work that has examined the impact of tax policy at the national level. Some of this difference may be attributable to differences in the measurement of corporate tax rates (average versus marginal), the level of analysis (state versus federal), the identification strategy and the distinction between GDP per capita and the variables we consider. That said, in our analysis, tax competition across states roughly doubles the baseline effects that would be found in the absence of firms’ ability to move across states, and for that reason we would extrapolate that the impact of state policy on state business activity should be about double the impact of federal business taxation on federal business activity.



 **Note**



This research brief is based on Xavier Giroud and Joshua Rauh, “State Taxation and the Reallocation of Business Activity: Evidence from Establishment‐​Level Data,” National Bureau of Economic Research Working Paper no. 21534, September 2015.
"
"

It’s nearly impossible to read major newspapers, magazines, or online publications in recent months without encountering a plethora of articles contending that the United States is turning inward and “going alone,” “abandoning Washington’s global leadership role” or “retreating from the world.” These trends supposedly herald the arrival of a new “isolationism.” The chief villain in all of these worrisome developments is, of course, Donald Trump. There is just one problem with such arguments; they are vastly overstated bordering on utterly absurd.



President Trump is not embracing his supposed inner isolationist. The policy changes that he has adopted regarding both security and international economic issues do not reflect a desire to decrease Washington’s global hegemonic status. Instead, they point to a more unilateral and militaristic approach, but one that still envisions a hyper‐​activist U.S. role.



For instance, it’s certainly not evident that the United States is abandoning its security commitments to dozens of allies and clients. Despite the speculation that erupted in response to Trump’s negative comments about the North Atlantic Treaty Organization (NATO) and other alliances during the 2016 election campaign (and occasionally since then), the substance of U.S. policy has remained largely unchanged. Indeed, NATO has continued to expand its membership with Trump’s blessing—adding Montenegro and planning to add Macedonia.





If you look at his actions and not his words, you won’t find it.



Indeed, Trump’s principal complaint about NATO has always focused on European free‐​riding and the lack of burden‐​sharing, not about rethinking the wisdom of the security commitments to Europe that America undertook in the early days of the Cold War. In that respect, Trump’s emphasis on greater burden‐​sharing within the Alliance is simply a less diplomatic version of the message that previous generations of U.S. officials have tried sending to the allies.



Moreover, Trump’s insistence at the July NATO summit in Brussels that the European nations increase their military budgets and do more for transatlantic defense echoed the comments of President Obama’s Secretary of Defense Chuck Hagel in 2014. Hagel warned his European counterparts that they must step up their commitment to the alliance or watch it become irrelevant. Declining European defense budgets, he emphasized, are “not sustainable. Our alliance can endure only as long as we are willing to fight for it, and invest in it.” Rebalancing NATO’s “burden‐​sharing and capabilities,” Hagel stressed, “is mandatory—not elective.”



Additionally, U.S. military activities along NATO’s eastern flank certainly have not diminished during the Trump administration. Washington has sent forces to participate in a growing number of exercises (war games) along Russia’s western land border—as well as in the Black Sea—to demonstrate the U.S. determination to protect its alliance partners. Trump has even escalated America’s “leadership role” by authorizing the sale of weapons to Ukraine —a very sensitive step that President Obama carefully avoided.



Trump even seems receptive to establishing permanent U.S. military bases in Eastern Europe. During a state visit to Washington in mid‐​September, Poland’s president, Andrzej Duda, promised to provide $2 billion toward construction costs if the United States built a military base in his country. Duda even offered to name the base “Fort Trump.” Trump’s reaction was revealing. Noting that Poland “is willing to make a very major contribution to the United States to come in and have a presence in Poland,” Trump stated that the United States would take Duda’s proposal “very seriously.” _American Conservative_ columnist Daniel Larison notes that while Trump often is accused of wanting to “retreat” from the world, “his willingness to entertain this proposal shows that he doesn’t care about stationing U.S. forces abroad so long as someone else is footing most of the bill.”



U.S. military activism does not seem to have diminished outside the NATO region either. Washington persists in its futile regime‐​change campaign in Syria, and it continues the shameful policy of assisting Saudi Arabia and its Gulf allies pursue their atrocity‐​ridden war in Yemen. Both of those Obama‐​era ventures should have been prime candidates for a policy change if Trump had wished to decrease America’s military activism.



There are no such indications in Europe, the Middle East, or anywhere else. The U.S. Navy’s freedom of navigation patrols in the South China Sea have actually increased in size and frequency under Trump—much to China’s anger . Washington’s diplomatic support for Taiwan also has quietly increased over the past year or so, and National Security Advisor John Bolton is on record suggesting that the United States move some of its troops stationed on Okinawa to Taiwan. The U.S. military presence in Sub‐​Saharan Africa is increasing, both in overall size and the number of host countries.



Those are all extremely strange actions for an administration supposedly flirting with a retreat from the world to be adopting. So, too, is Trump’s push for increases in America’s already bloated military budget, which now exceeds $700 billion—with even higher spending levels on the horizon.



Accusations of a U.S. retreat from the world on non‐​military matters have only slightly greater validity. True, Trump has shown little patience for multilateral arrangements such as the Trans‐​Pacific Partnership, the Paris climate agreement, or the United Nations Human Rights Council that he concluded did not serve America’s national interests. On those issues, the president’s actions demonstrated that his invocation of “America First” was not just rhetoric. However, regarding such matters, as well as the trade disputes with China and North Atlantic Free Trade Agreement partners, the administration’s emphasis is on securing a “better deal” for the United States, not abandoning the entire diplomatic process. One might question the wisdom or effectiveness of that approach, but it is a far cry from so‐​called isolationism.
"
"It is 2050. We have been successful at halving emissions every decade since 2020. We are heading for a world that will be no more than 1.5C warmer by 2100 In most places in the world, the air is moist and fresh, even in cities. It feels a lot like walking through a forest and very likely this is exactly what you are doing. The air is cleaner than it has been since before the Industrial Revolution. We have trees to thank for that. They are everywhere.  It wasn’t the single solution we required, but the proliferation of trees bought us the time we needed to vanquish carbon emissions. When we started, it was purely practical, a tactic to combat climate crisis by relocating the carbon: the trees took carbon dioxide out of the air, released oxygen and put the carbon back where it belongs, in the soil. This, of course, helped to diminish climate crisis, but the benefits were even greater. On every sensory level, the ambient feeling of living on what has again become a green planet has been transformative, especially in cities. Reimagining and restructuring cities was crucial to solving the climate challenge puzzle. But further steps had to be taken, which meant that global rewilding efforts had to reach well beyond the cities. The forest cover worldwide is now 50% and agriculture has evolved to become more tree-based. The result is that many countries are unrecognisable, in a good way. No one seems to miss wide-open plains or monocultures. Now we have shady groves of nut and orchards, timber land interspersed with grazing, parkland areas that spread for miles, new havens for our regenerated population of pollinators. A major part of the shift to net-zero emissions was a focus on electricity; achieving the goal required not only an overhaul of existing infrastructure but also a structural shift. In some ways, breaking up grids and decentralising power proved easy. We no longer burn fossil fuels. Most of our energy now comes from renewable sources such as wind, solar, geothermal and hydro. All homes and buildings produce their own electricity – every available surface is covered with solar paint that contains millions of nanoparticles, which harvest energy from the sunlight, and every windy spot has a wind turbine. If you live on a particularly sunny or windy hill, your house might harvest more energy than it can use, in which case the energy will simply flow back to the smart grid. Because there is no combustion cost, energy is basically free. It is also more abundant and more efficiently used than ever. Homes and buildings all over the world are becoming self-sustaining far beyond their electrical needs. For example, all buildings now collect rainwater and manage their own water use. Renewable sources of electricity make possible localised desalination, which means clean drinking water can now be produced on demand anywhere in the world. We also use it to irrigate hydroponic gardens, flush toilets and shower. Petrol and diesel cars are anachronisms. Most countries banned their manufacture in 2030, but it took another 15 years to get internal combustion engines off the road completely. What’s strange is that it took us so long to realise that the electric motor is simply a better way of powering vehicles. It gives you more torque, more speed when you need it, and the ability to recapture energy when you brake and it requires dramatically less maintenance. We also share cars without thinking twice. In fact, regulating and ensuring the safety of driverless ride sharing were the biggest transportation hurdles for cities to overcome. The goal has been to eliminate private ownership of vehicles by 2050 in major metropolitan areas. We’re not quite there yet, but we’re making progress. We have also reduced land transport needs. Drones organised along aerial corridors are now delivering packages, further reducing the need for vehicles. Thus we are currently narrowing roads, eliminating parking spaces and investing in urban planning projects that make it easier to walk and bike in the city. While we may have successfully reduced carbon emissions, we’re still dealing with the aftereffects of record levels of carbon dioxide in the atmosphere. The long-living greenhouse gases have nowhere to go other than the already-loaded atmosphere, so they are still causing increasingly extreme weather, though it’s less extreme than it would have been had we continued to burn fossil fuels. Glaciers and Arctic ice are still melting and the sea is still rising. Severe droughts and desertification are occurring in the western United States, the Mediterranean and parts of China. Ongoing extreme weather and resource degradation continue to multiply existing disparities in income, public health, food security and water availability. But now governments have recognised climate crisis factors for the threat multipliers that they are. That awareness allows us to predict downstream problems and head them off before they become humanitarian crises. Everyone understands that we are all in this together. A disaster that occurs in one country is likely to occur in another in only a matter of years. It took us a while to realise that if we worked out how to save the Pacific islands from rising sea levels this year, then we might find a way to save Rotterdam in another five years. The zeitgeist has shifted profoundly. How we feel about the world has changed, deeply. And, unexpectedly, so has how we feel about one another. When the alarm bells rang in 2020, thanks in large part to the youth movement, we realised that we suffered from too much consumption, competition, and greedy self-interest. Our commitment to these values and our drive for profit and status had led us to steamroll our environment. As a species, we were out of control and the result was the near-collapse of our world. We emerged from the climate crisis as more mature members of the community of life, capable of not only restoring ecosystems but also of unfolding our dormant potentials of human strength and discernment. Humanity was only ever as doomed as it believed itself to be. Vanquishing that belief was our true legacy. • This is an edited extract from The Future We Choose: Surviving the Climate Crisis by Christiana Figueres and Tom Rivett-Carnac, published by Manilla Press (£12.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15 • Christiana Figueres and Tom Rivett-Carnac will be in conversation at a Guardian Live event at the Royal Geographical Society, London SW7, on Tuesday 3 March, 7pm"
"My wildlife friends and I often talk about what species we would bring back from extinction. I am torn between the dodo and the thylacine, also known as the Tasmanian tiger. This was once a speculative, sci-fi debate but not anymore. Ever since Dolly the sheep was cloned, conservation biologists have muted the idea and the process of de-extinction – bringing back dead species – is coming closer to reality.  De-extinction can be achieved by one of two means: selective breeding or cloning.  In the selective breeding method we try to re-create extinct species such as the aurochs (extinct large cattle from Europe and Asia) by looking for their surviving genes among existing cattle and breeding animals to favour these genes. Then you compare the genome of the resulting animals with that for aurochs until you have what is genetically an auroch.   The second method essentially involves finding the DNA of an extinct species and inserting it into a recipient egg cell and recipient animal – the cloning process. This second process is limited to species that have gone extinct more recently (hundreds of years) because you need to find intact DNA, so I am afraid there will be no Jurassic Park. We would also need to find DNA from several different individuals otherwise we would end up with problems due to inbreeding such as those seen in white tigers. Unsurprisingly the “instant fix” of cloning has received more interest as it would not depend on many generations of captive breeding. A wide range of species have been suggested for cloned de-extinction from the dodo to the woolly mammoth.  Initially, I liked the idea. I’d love to see a dodo in a zoo or even better to see wild woolly mammoths on an ecotourism trip to the steppes of Siberia. But such meddling raises a host of questions. For instance, an African elephant would be the obvious recipient for woolly mammoth DNA. But as mammals learn a considerable part of their behaviour from their parents and peers – are we not just creating an elephant in mammoth’s clothing? It would, therefore, seem our resurrected animals would need some kind of training to survive in the wild, which may not be unlike the survival training reintroduced zoo animals already receive. If a species was successfully reintroduced and its population grew to previous levels it would have a major ecological impact. The animals which have occupied its ecological space may find themselves squeezed out. Governments would, rightly, be very cautious about the reintroduction of such animals. Given the limited money available for wildlife conservation it’s not clear that the expense of bringing back the dodo makes sense. A simple utilitarianism would suggest not; the cost of resurrecting the dodo could be used to save many other living species from extinction. For example, it now appears that a cloning approach may be the only solution to save the northern white rhinoceros from extinction – there are now only five individuals left. However society, thankfully, does not always run according to such utilitarian analyses. So perhaps the dodo will have its day – even if that is just living in a zoo. It may behave like a farmyard chicken but it would still be a powerful symbol for species conservation: I suspect some zoos would be shedding their giant pandas to go into dodos. But what kind of symbol would a living dodo be? It can no longer be the symbol of extinction; the Rubber Dodo Award for people who have contributed most to species extinction would need to be renamed. It would be testimony to how far science has come and how far science can take us.   But this sense of scientific wonder isn’t always helpful. A living dodo would give out the wrong message to society and politicians – we can destroy anything we like and scientists will eventually find a way to fix it. This seems, for example, to be the hope with climate change. As a species I think we need to accept responsibility for what we have done to this planet and not have blind faith that in the future scientists will fix all of our mistakes.  We need to live with our mistakes and learn from them. It is for this reason I am not wishing for de-extinction."
"The Intergovernmental Panel on Climate Change (IPCC) is set to publish the “synthesis report” of its fifth assessment period, drawing on three individual working group reports already published on: the physical science of climate change; climate impacts and adaptation; and mitigation, or how to reduce emissions or enhance the natural uptake of greenhouse gases (GHGs).  The latest report is the first collective assessment of climate change by governments since the 2007 report, published just as the world fell off a financial and economic cliff. It is therefore a vital input to the current round of UN climate negotiations culminating in Paris next year. IPCC reports can be over 1,000 pages long – and no one seriously expects even the most diligent of politicians to read through the whole thing. To make life easier, each report includes a “Summary for Policymakers”. However this summary is subject to line-by-line agreement by representatives of the 195 government members. There is therefore always a risk that the IPCC’s policy messages might not adequately reflect the underlying science. This isn’t just a theoretical possibility. The summary of the working group report on climate mitigation published earlier this year left out an important analysis of country emissions by income grouping, which differs from the standard binary classification – developed and developing – used in the UN climate negotiations.  No doubt some governments were concerned that this income-based analysis might imply the need for greater action on their part. In terms of the physical science, there are perhaps three key headline messages: human influence on the climate system is clear; warming of the climate system is unequivocal; limiting the risks from climate change will require substantial and sustained reductions of GHG emissions. The present pause in the rise of the global mean surface temperature does not mean we do not need to be concerned.  Of the 14 warmest years on record, 13 have occurred this century and 2014 looks likely to be another very warm year globally. Atmospheric concentrations of carbon dioxide are still growing rapidly. On current trends, by the end of the century, the average surface temperature of our planet is as likely as not to have increased by 4°C relative to pre-industrial conditions, with greater increases nearer the poles.  Worryingly, we are fundamentally changing the climate system, raising the likelihood of severe, pervasive, and irreversible impacts on society and the natural systems on which we all depend. Even the modest warming so far (0.85°C since the second half of the 19th century) has had a clear impact. Mitigating our use of fossil fuels lies right at the heart of an effective response to climate change. While a 2°C target remains technically feasible, achieving it will be extremely challenging. The IPCC’s mitigation report compared hundreds of energy modelling scenarios that strongly suggest that to achieve a 2°C target, global GHG emissions would need to be around 40-70% lower than 2010 levels by 2050 and near zero by 2100.  This would require rapid improvements in energy efficiency and at least a tripling of the share of zero- and low‐carbon energy supply by 2050. The longer action is delayed and the lower the availability of key low-carbon technologies, such as carbon capture and storage, the less feasible achieving a 2°C target becomes. Internationally agreed action on climate is difficult to achieve and we are rapidly running out of time to limit global warming to 2°C.  At some point soon it may become impossible in practical terms. The IPCC highlights the fact that climate risk increases with the cumulative level of carbon emissions over time, not just the emissions in any given year. So reversing rising emissions is a first necessary step to limit these catastrophic risks.   An effective response requires action across all major countries, not just the developed world. The international community now needs to build on – and go further than – the 2009 Copenhagen Accord, which was a paradigm shift that stimulated emissions-reduction pledges to 2020 from a wide range of countries – including China and India. National emissions-reduction contributions beyond 2020 are due to be made in early 2015. The major developed economies will need to make serious commitments on finance as well as emissions; the recent EU summit decision on its 2030 target should help. China now emits more CO2 per capita than the EU, while India’s per capita emissions are still less than 40% of the world average – what works for one won’t be necessarily be appropriate for the other, yet action by both is critical.    The 2015 Paris summit is therefore an important moment, but it will not be able – and is not intended – to provide the final answer. The summit will make more progress if it focuses on near-term operational targets, such as achieving a peak in global emissions by a certain date rather than triggering a damaging row over how the small budget of future cumulative CO2 emissions consistent with a 2°C target should be shared among nations.  Aiming to pass “peak emissions” by a certain point would provide a coherent framework for assessing individual national contributions and negotiating the conditions under which they could be improved. It could also allow continued growth of emissions in the poorer developing countries for some time to come, which is essential.  Governments are beset by many other challenges and focused on the short term. The next climate agreement must therefore also build in the flexibility to enhance our response in the light of evolving scientific and political developments."
"
Share this...FacebookTwitterThe European Institute For Climate and Environment (EIKE) in Germany recently had a piece at it’s blogsite here, which I have summarized.
The USA has James Hansen, and England has Phil Jones. Germany now has Prof. Dr. Gerhard Adrian, the new President of the German Weather Service. His mission: to produce a trend to climate catastrophe as quickly as possible. Recently he said:
The average temperature in Germany has risen by in 1.1°C from 1881 to 2009. It could go up another 2 to 4°C by the end of the century.
and:
We’ll have a completely whole new set of extremes to deal with; that’s the threat.
This is the new message from the once respected German Weather Service. Suddenly, doom and gloom are the forecast.
Inconveniently for Dr. Adrian, his own data and earlier statements made by German Weather Institutes seem to contradict his claims.

Firstly, why choose a timescale that starts in 1881 when records go back as much as 300 years in Europe? The following graphics are temperature charts going back more than 200 years for some European cities (visit EIKE for better quality graphics (here).
Here’s the temperature chart for Berlin going back 300 years (same as above in the introduction):
There we don’t see much going on until about 1990. In fact the total trend is 0.08″C rise per century – statistically insignificant. Surely Dr. Adrian is aware of this.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now let’s take a look at the temperature CO2 correlation.
Poor correlation there too. While atmospheric CO2 concentrations climbed from 1750 to 1980, Berlin’s temperature did the opposite. But when one is hired to promote global warming alarmism, then 1881 is a good place to start.
Let’s go back and look at the last 2000 years. Maybe that’ll reveal something more earth-shattering.
All the climate catastrophe talk put out by Hansen, Jones, and now Dr. Gerhard Adrian, simply do not materialize when you take an honest look at the statistics.
As far as weather extremes occurring, here’s what the German Weather Institutes said in the pre-Gerhard-Adrian days, just a couple of years ago. According to the German Weather Service, recently quoting the German Meteorological Society, 3/2002, p. 2:
When it comes to extreme weather events, no significant trend can be observed up to now. Also such events like the flooding of 2002 are part of the norm in our climate.
According to a German Weather Service press conference 24 April 2007, Berlin;
Up to now there has been no increase of extreme events:  Up to now – with the exception of the already mentioned heavy summertime precipitation – there has been no detectable systematic changes or shifts of extreme values.
And again, according to the German Meteorological Society 3/2002, S. 2, on the flood of 2002:
Also such events, like the big flood of 2002, are part of the norm in our climate.
Dr. Adrian ought to listen to his own data.
Share this...FacebookTwitter "
"It may surprise you to know that man’s best friend is not a friend of wildlife. Or that those cute cats from online videos are often mass killers. The dark side of pet ownership is something not usually addressed but in wildlife conservation it has become a burning issue. Last week the Australian government named domestic cats as one of the major threats to its unique wildlife. The problem of domestic cats and dogs killing wildlife is massive.  A recent study in the US found that cats, especially stray cats, kill four billion birds and up to 21 billion mammals annually.  If they were just killing mice and rats, which are invasive species, no-one would be concerned but no – they are also killing native wildlife and could make some species extinct. Cat and dog ownership is a global phenomenon and no country is immune. My research group in Brazil did some studies into why greater rheas, a large flightless bird, were being wiped out in Minas Gerais and we discovered the problem was farmer’s dogs. Cattle pasture makes ideal habitat for these birds. As part of our research we interviewed farmers about the problem and the response was always the same: their neighbour’s dog was to blame for killing this species but never their dog. The problem in regions with such rich biodiversity is that dogs in the countryside might technically be pets but in reality they are feral. And their instinct to hunt, especially if they are not being fed regularly takes over. Dogs hunt wildlife, but they also kill it indirectly by spreading disease. Studies conducted in Brazil have shown that they spread diseases such as canine distemper to wild endangered canines such as maned wolves. In Ethiopia, such diseases are among the most serious threats to the world’s rarest canine, the Ethiopian wolf. Scientists often use camera traps to estimate the numbers of medium-to-large mammals in the wild. I have many colleagues who employ this technology around the world.  And as a lover of wildlife I am always keen to see their photographs but instead of seeing peccaries, deer or even jaguars, the most common photographically captured animal is the domestic “feral” dog. The problem is so pronounced that Matthew E Gompper, professor of Mammalogy at the University of Missouri has edited a book on the subject. Cats appear to kill more wildlife globally than dogs.  This no doubt relates to the culture of people letting their cats roam free. Even well-fed pet cats will go out and hunt and data from a study in the UK estimated that domestic cats kill a total of 92m animals annually and this has an impact on native species conservation. The culture of letting cats and farm dogs roam is difficult to break. In some countries this problem has been dealt with by culling the offending animals. But this does not always work – in Brazil, this is not done because the owners of such animals may kill wildlife in revenge. In countries like New Zealand where the native wildlife never evolved with such predators and is essentially defenceless, culling may be the only solution if flightless parrots such as the kakapo are to be saved from extinction. We could of course sterilise the offending animals, but in many countries around the world this would require the owner´s consent. Plus with dogs the problem is with males – if you castrate them their behaviour changes, they become less aggressive and this may result in the influx of dogs from neighbouring regions. However there is now a single injection that sterilises male dogs and does not change their behaviour. Whenever possible, dog numbers should be kept under control. In more developed countries without large remote wildlife areas the problem can be dealt with by the law.  Dogs are supposed to be under the control of the owner at all times in public.  However, I have unfortunately witnessed people allowing their out-of-control dog to chase wildlife in the UK.  But with education and law enforcement this situation should be a minor issue. Pet cats are another issue. Putting a bell or sonic device on their collar halves their predatory efficiency, however, with experience cats can improve their hunting despite such devices.  There are even cities in Australia that operate cat curfews.  And farmers could be encouraged to control better the cats they keep for vermin control.   We will never completely stop pet carnivores killing wildlife, but through responsible ownership and control programmes we can reduce the number of victims from the billions to the millions: removing cats and dogs from wildlife’s most dangerous threat list."
"

The foreign policy record of the Clinton‐​Gore administration deserves a less than stellar grade. At the end of the Cold War, there was an extraordinary opportunity to build a new relationship with a democratic Russia; restructure U.S. security policy in both Europe and East Asia to reduce America’s burdens and risk exposure; and revisit intractable Cold War‐​era problems, such as the frosty relations with Cuba, Vietnam, and North Korea. The administration’s performance must be judged within the context of such an unprecedented opportunity for constructive change.



The record is acutely disappointing. True, the administration has scored some successes: improving the negotiating climate in Northern Ireland and the Middle East, pushing for permanent normal trade relations with China, and normalizing relations with Vietnam. But the failures greatly outnumber the successes. The administration needlessly meddled in the complex disputes of the Balkans, leaving to its successor two U.S.-led NATO protectorates (Bosnia and Kosovo) and a colossal mess of a nation‐​building commitment with no end in sight. A similar morass is emerging in Colombia as a result of the administration’s prosecution of the drug war.



U.S. policy toward long‐​time adversaries is on autopilot. The rote perpetuation of an economic embargo and occasional bombing attacks against Iraq have devastated the Iraqi people while barely bothering Saddam Hussein. Washington’s policy toward Cuba is equally sterile and cruel.



Worst of all is the growing list of missed opportunities. Instead of integrating a newly democratic Russia into the West, the Clinton administration needlessly antagonized Russia by expanding NATO’s membership and waging war against Moscow’s long‐​time allies in the Balkans. Relations with China have been damaged by an inconsistent, at times nearly incoherent, U.S. policy. Instead of embracing efforts for greater military self‐​reliance on the part of our European allies, the administration has engaged in carping criticism and apparently views such initiatives as a threat to America’s dominant position in the transatlantic relationship. Instead of viewing the end of the Cold War in East Asia as an opportunity to reduce America’s security burdens in that region, the United States insists on keeping 100,000 troops deployed seemingly forever. Administration officials even reacted with ambivalence to the recent summit between North and South Korea and gave highest priority to retaining the U.S. troop presence on the Korean peninsula.



Given the number of botched opportunities, the administration’s record merits a grade of D.
"
"London has long been one of the world’s most congested cities. Before a £5 “congestion charge” was introduced for vehicles entering the city centre, cars would spend a third of their time in peak hours at a complete standstill. The charge, introduced in 2003, was hailed as a triumph of economics as it forced those who contributed to congestion to pay a price that reflected its cost. As economics would suggest, traffic decreased and journeys became quicker. Traffic accidents and related injuries and fatalities weren’t the main target of the congestion charge, but they may have been influenced by it. On the one hand, reduced congestion means fewer cars are in central London with an expectation of fewer accidents. On the other hand, less traffic means higher travel speeds, which generally leads to more accidents – particularly in a dense area such as central London where cars, cyclists and pedestrians share the road.  My research with John Heywood of the University of Wisconsin-Milwaukee and Maria Navarro of Lancaster University is the first major study of the congestion charge’s effect on traffic accidents and their severity. One issue is that we simply don’t know what would have happened to accidents in the congestion zone had there not been a charge. Comparing the number of accidents before and after charge is not enough, as we might simply be observing something that would have happened anyway. It is well known that the number of traffic accidents has been steadily declining across the UK in the past decade.  This means we need to define appropriate comparison or control groups. In this case, we used the most populous 20 cities in Britain, not including London. We contrasted the change in accidents in the congestion charge zone to that change over the same period in these other cities. We use the fact that the downward trend in accidents happens in both London and the comparison group to more accurately identify the influence of the charge.   The congestion charge reduced traffic accidents in central London by 30 a month – an enormous 40% reduction. Accidents that result in individuals being killed or seriously injured also fell, by just under four a month, or 45 a year. This means around 500 people have avoided serious injury or death thanks to the congestion charge. Congestion charge means fewer accidents in London: We next explored any negative “spill-over” from the policy. Did the congestion zone just move traffic to other parts of London, other times of the day, or to non-charged vehicles? If the answer is yes, reduced accidents may be offset by more car crashes and fatalities elsewhere and at other times.  This isn’t the case, however. We examined areas surrounding the congestion charge zone – both a 2km and a 5km radius – and we found that, not only did accidents not increase in these areas, there was actually a large positive spill-over. The charge decreased accidents dramatically in the surrounding areas – about 20 fewer total accidents a month, with 3.5 fewer serious or fatal accidents. Our estimates also show that accidents and injuries were reduced in non-charged times (before 7 am and after 6 pm) and for exempt vehicles (largely bicycles, motorcycles, taxis and buses). The charge meant more people rode bikes into central London – that was the idea. Our research confirmed that one unanticipated effect of the congestion charge was a small initial increase in accidents involving cyclists, roughly 1.5 a month up to 2005. Yet, by the end of 2006 we found this had reversed and that the rise didn’t carry on. It seems likely to us that an initial response to the congestion charge was that more inexperienced cyclists took to the roads. In time, they either gave up or became more worldly. Is the congestion charge set at the right level? Since the initial £5 charge in 2003, it has been raised twice, £8 in 2005 and £10 in 2011. These changes and inflation allow us to see what effect a £1 real charge has on accidents. Our results show that each pound increase in levy reduces accidents by 5 per month. While we can’t say what is the “right” charge, our work suggests it’s not just the fact that the charge exists that matters. It may not be great news for commuters, but a more expensive charge might well reduce traffic accidents further still."
"

Some of the most pressing questions about global economies — whether governments are spending beyond their means, for example, and if so by how much — concern what economists call “fiscal imbalance.” Although this is a concept familiar to economists, it can often be difficult for non‐​economists to decipher. In“Fiscal Imbalance: A Primer” (White Paper), Director of Economic Policy Studies Jeffrey Miron provides a clear introduction to the concept of fiscal imbalance. Fiscal imbalance essentially concerns whether a government can continue forever to make the expenditures necessitated by its existing policies, given the expected revenues under those policies and the government’s debt. This includes its ability to borrow money in the future — which is not infinite. This imbalance, as Miron writes in **“U.S. Fiscal Imbalance over Time: This Time Is Different”** (White Paper), is growing. He projects fiscal imbalance for every year between 1965 and 2014, revealing that the United States has seen a rising fiscal imbalance since the early 1970s. “As of 2014, the fiscal imbalance stands at $117.9 trillion, with few signs of future improvement even if GDP growth accelerates or tax revenues increase relative to historic norms,” he warns. “Thus the only viable way to restore fiscal balance is to scale back mandatory spending policies, particularly on large health care programs such as Medicare, Medicaid, and the Affordable Care Act (ACA).”



 **THE COSTS OF GUN CONTROL**  
Gun control advocates persistently call for measures like universal background checks, a ban on high‐​capacity magazines, or a ban on so‐​called “assault weapons.” But, as associate policy analyst David B. Kopel argues in **“The Costs and Consequences of Gun Control”** (Policy Analysis no. 784), “Such proposals are not likely to stop a deranged person bent on murder.” Kopel examines the actual costs and benefits of these popular gun‐​control measures, demonstrating that they would prove largely ineffective. “Before adding new gun regulations to the legal code, policymakers should remember that several mass murders in the U.S. were prevented because citizens used firearms against the culprit before the police arrived on the scene,” he warns.



 **MURDER AS A THREAT TO FREE SPEECH**  
The brutal Charlie Hebdo killings last year were a shocking act of violence, but unfortunately, not the first violent reactions to speech perceived as blasphemy. As Robert Corn‐​Revere , a partner at Davis Wright Tremaine LLP, writes in **“To Confront the Assassin’s Veto, or to Ratify It?”** (Working Paper no. 36), “This was yet another grim marker in the cross‐​cultural conflict illustrated by events such as the Ayatollah Khomeini’s 1989 fatwah against Salman Rushdie for writing _The Satanic Verses_ , the 2004 murder of filmmaker Theo van Gogh on the streets of Amsterdam for perceived insults to Islam, and the violent reaction to the cartoons of Mohammad published in the Danish newspaper _Jyllands‐​Posten_ in 2005.” Corn-Revere’s paper confronts the question of how the law should deal with these sinister attempts to chill speech.



 **ESAS: EMPOWERING STUDENTS AND FAMILIES**  
“Every child deserves the chance at a great education and the American dream,” Cato policy analyst Jason Bedrick, Goldwater Institute education director Jonathan Butcher, and former Goldwater Institute vice president for litigation Clint Bolick — who has since been appointed to the Arizona Supreme Court — write in **“Taking Credit for Education: How to Fund Education Savings Accounts through Tax Credits”** (Policy Analysis no. 785). In an effort to improve education, several states have passed laws allowing students to receive an Education Savings Account (ESA) which parents can put toward alternative education services. In this analysis, the authors show how legislators can design ESAs that will work in the 40 states with constitutional provisions that prohibit the use of public funds at religious schools. “Tax‐​credit‐​funded ESAs would empower families with more educational options while enhancing accountability and refraining from coercing anyone into financially supporting ideas they oppose,” they write.



 **CHINA AT A CROSSROADS**  
China, as Cato vice president James Dorn puts it, is “at a crossroads.” It has made tremendous progress in recent years by expanding the market and strengthening property rights. But at the same time, its powerful one‐​party state maintains a strong grip on citizens’ private and commercial dealings. “The damage China’s illiberal state has inflicted on the nation is becoming evident as the economy slows, debts mount, and state‐​owned enterprises (SOEs) draw capital away from the more productive private sector,” writes Dorn in **“China’s Challenge: Expanding the Market, Limiting the State”** (Working Paper no. 34). He highlights the importance of renewing interest in China’s ancient culture and writings on topics like freedom and limited government — a legacy which its authoritarian leaders have obscured.



 **THE LUKEWARMING WORLD**  
In **“Climate Models and Climate Reality: A Closer Look at a Lukewarming World”** (Working Paper no. 35), Center for the Study of Science director Pat Michaels and assistant director Chip Knappenberger further the case for the “lukewarmers” — those who believe that the evidence for some human‐​caused climate change is persuasive, but that, contrary to the alarmists, this warming occurs in accordance with the lower end of expectations from mainstream science. They contend that the rate of warming over the past several decades has been so slow it was “completely unexpected” by any of the climate models — “a worrying indication that the current stateof‐ the‐​art climate models are not up to the task of simulating the actual behavior of the earth’s climate.” This consequently throws efforts to implement climate policy based on these models into serious doubt.



 **THE EVOLUTION OF WEAPONRY**  
Technological advances in recent years have led to a bevy of increasingly small, cheap, and sophisticated weapons. “This new diffusion of power has major implications for the conduct of warfare and national strategy,” U.S. National Defense University distinguished research fellow T. X. Hammes argues in “Technologies Converge and Power Diffuses: The Evolution of Small, Smart, and Cheap Weapons” (Policy Analysis no. 786). Hammes delves into the particular challenges posed by various types of emerging technology, like drones, artificial intelligence, and nanoenergetics, or explosives. With such abundant and affordable technology available, the United States may be exposed to much more danger when waging military campaigns in the future. “Increasingly,” he writes, “we will have to ask the question ‘Is the strategic benefit of an intervention worth the cost when the enemy can strike back in and out of theater?’”
"
"
Share this...FacebookTwitterI know what a lot of people are thinking when they look at the Arctic sea ice graph since September 1st – my oh my how has the ice reduced! Indeed just take a look at the numbers themselves:
September 1:     5,332,344 sq km
September 18:   4,813, 594 sq km
That’s a drop in area of over 500,000 sq km. Still, I’m going to say that the ice has grown. You think it’s preposterous, right?

But now take a look at the following chart that compares September 1 ice to September 18 ice. Which would you prefer to be standing on?

These charts are taken from: http://www.ijis.iarc.uaf.edu/cgi-bin/seaice-monitor.cgi
Which ice looks thicker (more concentrated)?
Don’t sweat the ice area statistics. The thickness (er, concentration) is much greater today, and we could even say the volume is likely more.  Arctic temperatures above 80°N have been colder this summer and September. The ice area will rebound quickly, of course. I projected a 5.75 million sq km min. for 2011 a couple weeks back. I’m sticking to it.
Share this...FacebookTwitter "
"
When we last checked in to the Nansen Sea Ice Graphs, it looked like they were heading towards the “normal” line in a hurry. Ice area seems to still be on that trend, while extent seems to be leveling off it’s growth rate. Area appears to be within about 200,000 square kilometers of the 1979-2007 monthly average and still climbing.
Sea Ice Area - red line is current value, shaded area represents 1 standard deviation
Of course the fact that the 2007 data is included in the average line, means the average is a lower than usual target than one might expect. If we compare to ice area over at Cryopshere today, they use a 1979-2000 mean, which is higher.  Still the rebound we are seeing is impressive.
Sea ice extent looks like this:
Sea Ice Extent - red is current value, shaded area is 1 standard deviation
These graphs will automatically update, so check back often.
For those of you wondering, here is the difference between area and extent, as described in the NSIDC FAQ’s page:
What is the difference between sea ice area and extent? Why does NSIDC use extent measurements?
Area and extent are different measures and give scientists slightly different information. Some organizations, including Cryosphere Today, report ice area; NSIDC primarily reports ice extent. Extent is always a larger number than area, and there are pros and cons associated with each method.
A simplified way to think of extent versus area is to imagine a slice of swiss cheese. Extent would be a measure of the edges of the slice of cheese and all of the space inside it. Area would be the measure of where there’s cheese only, not including the holes. That’s why if you compare extent and area in the same time period, extent is always bigger. A more precise explanation of extent versus area gets more complicated.
Extent defines a region as “ice-covered” or “not ice-covered.” For each satellite data cell, the cell is said to either have ice or to have no ice, based on a threshold. The most common threshold (and the one NSIDC uses) is 15 percent, meaning that if the data cell has greater than 15 percent ice concentration, the cell is considered ice covered; less than that and it is said to be ice free. Example: Let’s say you have three 25 kilometer (km) x 25 km (16 miles x 16 miles) grid cells covered by 16% ice, 2% ice, and 90% ice. Two of the three cells would be considered “ice covered,” or 100% ice. Multiply the grid cell area by 100% sea ice and you would get a total extent of 1,250 square km (482 square miles).
Area takes the percentages of sea ice within data cells and adds them up to report how much of the Arctic is covered by ice; area typically uses a threshold of 15%. So in the same example, with three 25 km x 25 km (16 miles x 16 miles) grid cells of 16% ice, 2% ice, and 90% ice, multiply the grid cell area by the percent of sea ice and add it up. You’d have a total area of 675 square km (261 square miles).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b42a7d5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"In the flurry of the holiday season, many people will have missed the government’s verdict on the 2014 badger culls, published on December 18. Farmers’ representatives have branded these recent culls “successful”, and environment secretary Liz Truss claims that they show how culling “can work to reduce disease”, confirming her plan to extend this controversial approach across western England. Cattle farmers have suffered terribly as a result of bovine tuberculosis (TB). Many are desperate, and would welcome a cull of badgers, which research (including my own) has shown to be a source of infection for cattle. Sadly, a closer look at the evidence suggests that the 2014 culls bring little hope of succour. Despite the environment secretary’s optimism, there is so far no evidence that these pilot culls have reduced disease. The government has commissioned research to estimate the impacts of pilot badger culling on cattle TB but no results have been published to date, nor are any benefits anticipated so soon after the start of the annual culls. Culled badgers have not even been tested for TB. Since changes in cattle TB take so long to emerge, in the short-term the government measures culling success in terms of reduced badger numbers. This is an appropriate measure because, perversely, killing too few badgers increases cattle TB rather than reducing it.  In a randomised controlled trial conducted in 1998-2007, cattle TB was consistently elevated where culling reduced indices of badger numbers by 10-35%. By contrast, nearby farms saw gradual reductions in cattle TB where large-scale culling reduced the same indices by 69-73%. To achieve similar benefits (and to avoid increasing cattle TB), the 2013-4 culls were intended to reduce badger numbers by at least 70%. The first two culls, conducted in 2013, clearly failed to achieve this aim. Government scientists, overseen by an independent expert panel, estimated the reduction in numbers by identifying individual badgers from hair entangled in barbed wire traps. They estimated that between 37 and 51% of badgers were killed in the Somerset cull zone, with between 43 and 56% killed in Gloucestershire. For the second year of culling, the government discarded both independent oversight and the hair trapping method which had revealed the first year’s failures. Before the 2014 culls commenced, the government’s planned monitoring methods were so inadequate that I warned “any future claim that the 2014 culls have reduced badger numbers sufficiently to control TB will be completely baseless”. Although ministers and farming representatives do indeed now claim success, the numbers tell a different story. There are no published estimates of the percent reductions achieved by the 2014 culls. Instead, claims of success are based on the number of badgers killed in Somerset, which reached the minimum target required by the culling licence (the Gloucestershire cull spectacularly failed to meet its target, killing just 274 badgers against a target of 615). Yet the Somerset target was derived from the lower bound on the range of possible badger numbers, rather than from the best estimate. If the estimation method was accurate, there would be a 97.5% chance that the true population size was greater than this lower bound, and hence that the target was too low. Despite having met this target, statistically it is still far more likely than not that the 2014 Somerset culls failed to reduce badger numbers by 70% as planned. Simple calculations provide further evidence of ineffective culling in Somerset. Government scientists estimate that, before any culling took place, the Somerset zone contained between 1,876 and 2,584 badgers. The total number of badgers killed (341 last year plus 955 in 2013) comprises just 69% of the lowest estimate. Taking into account the fact that births and immigration would have increased badger numbers between the two culls, the population cannot have been reduced by “at least 70%” if the government’s population estimates were correct. Government documents describe Somerset’s low target as “precautionary”. But from the perspective of disease control – the justification for killing otherwise protected wildlife – it risked worsening cattle TB and was hence the opposite of precautionary. With separate maximum targets in place to avoid killing too many badgers, the only risk reduced by a low target was the risk of a cherished project being branded a failure. Failing to reduce badger populations sufficiently risks exacerbating cattle TB, potentially making a bad situation worse. Farming leaders have managed to press forward with badger culling in the face of scientific consensus, legal challenge, public opinion and a groundswell of protest. In future they may look back on such victories as Pyrrhic: one more such victory might undo the farmers they strive to support."
"Alok Sharma, the new president of the Cop26 climate conference to be held in Glasgow in November, has experience of working closely with developing countries on the climate crisis in his former role as secretary for international development. This may be valuable in helping him forge the “grand coalition” that experts say is needed to break the deadlock on international climate action.  The last round of UN climate talks, in Madrid last December, showed the massive task that Britain will face as host this year in trying to build consensus on the issue. While more than half a million protesters from around the world lined the streets of the Spanish capital, inside the conference centre government officials squinted at semicolons in a dense text on how countries can buy and sell carbon. For almost three decades, world governments have met every year to forge a global response to the climate emergency. Under the 1992 United Nations Framework Convention on Climate Change, every country on earth is treaty-bound to “avoid dangerous climate change”, and find ways to reduce greenhouse gas emissions globally in an equitable way. Cop stands for conference of the parties under the UNFCCC. The UK will host Cop26 this November in Glasgow. In the Paris agreement of 2015, all governments agreed for the first time to limit global heating to no more than 2C above pre-industrial levels, and set out non-binding national targets on greenhouse gases to achieve that. However, these targets are insufficient, and if allowed to stand would lead to an estimated 3C of heating, which scientists say would spell disaster. For that reason, the Cop26 talks in Glasgow are viewed as the last chance for global cooperation on the emergency, with countries expected to come with tough new targets on emissions. The negotiations will be led by environment ministers and civil servants, aided by UN officials. Nearly every country is expected to send a voting representative at the level of environment secretary or equivalent, and the big economies will have extensive delegations. Each of the 196 nations on earth, bar a few failed states, is a signatory to the UNFCCC foundation treaty. The Cops, for all their flaws, are the only forum on the climate crisis in which the opinions and concerns of the poorest country carry equal weight to that of the biggest economies, such as the US and China. Agreement can only come by consensus, which gives Cop decisions global authority. Fiona Harvey Environment correspondent Two weeks of talks produced little more than a tetchy agreement to gather again this year for Cop26 (the 26th conference of the parties), with proposals for strengthening national plans to reduce emissions. Even that, in the context of the disasters that had threatened the talks, was better than some had feared. Since the Paris agreement was signed in 2015, the willingness of governments to tackle the climate crisis has waned. Donald Trump’s election as US president was the biggest factor – he has called climate science a “hoax” and begun the process of withdrawing the US from the agreement. That withdrawal will not take legal effect until 4 November, the day after the next US election. Emboldened by Trump, other countries have also started to backslide. Jair Bolsonaro, the president of Brazil, has embarked on a programme of exploitation of the Amazon, and in Madrid his officials worked hard to scupper any climate deal. They fought over details of an obscure clause of the Paris agreement governing carbon trading, which will now have to be resolved in Glasgow. Other countries were less vocal but no less inimical to progress. Saudi Arabia tried to hold up consensus, and Russia is also hostile to Paris. India, by siding with Brazil on carbon trading, bolstered the wreckers, but in other forums called for more urgent action under Paris. China’s stance was viewed as encouraging by Paris supporters, but it had little new to say. The EU made the boldest announcement, of a European green deal to transform the economy and reach net zero emissions by mid-century, but the details of its commitments are still subject to wrangling by member states. Patricia Espinosa, the UN’s top climate official, showed some frustration in her assessment. “We need to be clear that the conference did not result in agreement on the guidelines for a much-needed carbon market, an essential part of the toolkit to raise ambition. Developed countries have to fully address the calls from developed countries for finance, technology and capacity building, without which they cannot green their economies. High-emitting countries did not send a clear enough signal that they are ready to ramp up ambition.” All of this leaves the UK with a diplomatic mess to sort out. At Cop26, countries are supposed to come forward with new plans for stringent emissions cuts, in line with the science. Time is running out for those new plans to take effect, and without strong signals from governments the required changes will not be made. Arguably, the task facing Sharma is even harder than negotiating the 2015 Paris accord – at least the French government could rely on Barack Obama’s support, and a US-China agreement was fundamental to the success of Paris. In the four years since Paris was signed, while governments have dithered, businesses have carried on investing in fossil fuels and emissions have risen by a further 4%. Climate science, meanwhile, has grown stronger: the Intergovernmental Panel on Climate Change, the body of the world’s leading experts, said in 2018 that catastrophic climate breakdown would become inevitable within this decade unless the world changed course and started to bring global emissions down dramatically. “We need to really get across the point that this is not some minor adjustment that is required,” said Mary Robinson, the chair of the Elders, a campaigning group of senior world figures, and a former UN climate envoy. “The reality is that we need every company, every city, every country to be carbon neutral by 2050. If we can get that, then Cop26 really will be a game-changer.”"
"
Share this...FacebookTwitter
I’ve just read the latest climate horoscope at the Hannoversche Allgemeine Zeitung website, which delivers them almost daily.
The latest one comes from the fortune tellers and scryers at the Massachusetts Institute of Technology, led by psychic Paul O’Gorman, now available at the PNAS here.
The latest horoscope foretells that (later) in the 21st century, summers will be stickier and grittier, and winters will be stormier – this according to visions and images delivered by crystal balls and gazings into MIT scrying pools.
Apparently MIT diviners made contact with the spirits of 1981 to 2000, so writes the HAZ, and felt the unsettling vibes of mystic energy of atmospheres past, and the energy intensity of past climatological storms. MIT’s assortment of sophisticated scrying instruments, made of silicone and crystal, all delivered similar predictions for the 21st century – forebodings all confirmed by their climate tarot punch cards.
The bad vibrations and ill spirits foretell one thing only: doom!
The 21st century
The northern hemispheric middle latitudes will be haunted by severe meteorological storms between the autumn and spring equinoxes, becoming especially intense before and after the winter solstices.
“I see storms and doom!”
For periods surrounding the summer solstices, crushing doldrums will beset northern middle latitude regions. Stagnate atmospheres will cause pollutants, and the evil spirits they harbor, to accumulate in ever higher concentrations above cities, bringing misery to non-believers.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Be forewarned! The degree of misery about to haunt the middle latitudes in the end will depend on the amount of ice surrounding the magnetic North Pole at the fall equinoxes.
The southern hemisphere will be visited by other misfortune, so say the MIT instruments of clairvoyance, and the diviners who gaze into them. There, ruthless storms will occur year-round, from solstice to solstice, from equinox to equinox.
Careful though, as other celestial alignments may impact the fortune tellers’ predictions. These predictions may change as they depend on what parts of the atmosphere are heavily impacted. If the earthly layer of the atmosphere energizes, then other currents and eddies come into play.
In the northern hemisphere, however, the heavenly layers of the atmosphere shall warm, and this will act to calm the air mass energy.
Come back tomorrow for more predictions!
For personal mystic climate fortune-telling and face-palm reading, make an appointment with Paul O’Gorman: pog@mit.edu.
UPDATE: I’ve just received a photo of just one of MIT’s highly confidential fortune telling instruments, with one of the teams of divine scryers who gaze at it:


Share this...FacebookTwitter "
"
Hard lesson about  solar realities for NOAA / NASA
Reposted here: October 30th, 2008
by Warwick Hughes

The real world sunspot data remaining quiet month after month are mocking the  curved red predictions of NOAA  and about to slide underneath. Time for a rethink I reckon NOAA !!
Here  is my clearer chart showing the misfit between NOAA / NASA prediction and  real-world data.

Regular  readers might remember that we started posting articles drawing attention to  contrasting predictions for Solar Cycle 24, way back on 16 December 2006. Scroll to the start  of my solar threads.
Then in March 2007 I posted David Archibald’s pdf article, “The Past and  Future of Climate”. Well worth another read now, I would like to see another  version of David’s Fig 12 showing where we are now in the transition from Cycle  23 to Cycle 24.
Solar Cycle 24  Prediction Issued April 2007 from NOAA / NASA
NOTE from Anthony: We now appear to have a new cycle 24 spot, which you can see here:

See the most current MDI and magnetogram here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b7a42a2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Labor MPs have sought to downplay suggestions of a breakaway pro-coal group within the party, as the Coalition seized on the report to accuse the opposition of divisions over climate policy. Following a report by Channel Ten on Wednesday night that a group of about 20 right-aligned Labor MPs dined regularly to canvass policies in support of coal workers, attention shifted from the Coalition’s internal ructions over climate policy to Labor.  The Labor leader, Anthony Albanese, said he had been unaware of the factional dinner, but downplayed its significance, and said his MPs were united on wanting action on emissions reduction. “That’s what happens in Canberra – people go out and people chat about ideas. There is nothing unusual about this,” he said. “The Labor party is united in our position that climate change is real, that we need to act on lowering our emissions. “Good action on climate change means more jobs, lower emissions and lower energy prices.” The Channel Ten report said that the group of 20 MPs had become known as the “Otis group” in reference to their restaurant of choice in Canberra. The senior right-aligned senator Don Farrell said the group was made up of “good solid Labor people, interested in supporting coal workers”. The Coalition turned on Labor over the report in parliament on Thursday, with the prime minister, Scott Morrison, accusing the opposition of having “no alternative policy” on climate change. “When 20 members of the Labor party gather at the Otis restaurant, what I’m more mystified by is that they can actually find a consistent position of the leader of the opposition that they can actually oppose,” Morrison said. “I’m staggered that they can find any consistency in the leader of the opposition’s policy on emissions or electricity or coal or any of these things, because he has it each way every day. “My advice to those who are meeting down at the Otis regularly is to just wait until tomorrow because he’ll have another policy.” The deputy prime minister, Michael McCormack, who is facing an uprising from Queensland Nationals over his leadership style, used a question on the party’s approach to developing regional Australia to also target Labor over its “rebel group”. He read from a menu from the Otis restaurant to say that the Labor MP for the seat of Hunter, Joel Fitzgibbon, had “egg all over his face” over the report. McCormack faced a spill motion last week, largely driven by disgruntled Nationals MPs who want the Coalition to support the development of a new coal-fired power station in Queensland. Both the Coalition and Labor are grappling with how best to frame climate change policy ahead of the next election, with the parties under pressure to take action without losing political ground in coal-mining seats in Queensland and New South Wales. Nationals MPs are arguing that the Coalition won the election on the back of support from regional Queensland, where the LNP promised support for a feasibility study into a coal-fired power station in Collinsville in the seat of Capricornia. But moderate MPs want the government to adopt a more “ambitious” climate change policy to respond to the concerns in inner city Liberal-held seats, including some who want to sign up to the commitment of a net zero carbon emissions target by 2050. Two moderate Liberal MPs – Trent Zimmerman and Dave Sharma – have also both declared that the federal government should not be in the business of underwriting coal-fired power stations. The minister for emissions reduction, Angus Taylor, told parliament on Thursday that the government was considering “all technologies”, including renewables and coal. “We’ll support all technology that drives down the price of electricity, that keeps the lights on and that brings down emissions,” Taylor said. For Labor, the meeting of right-aligned Labor MPs comes as Albanese indicates the party won’t be rushed into announcing a new climate change policy. While the party’s shadow minister for climate change, Mark Butler, said after the election that the party’s policies for ambitious action are “unshakeable”, some of the party’s MPs have warned against moving too far to the left. On Sunday, the party’s deputy leader, Richard Marles, did not rule out Labor supporting new coal developments, saying coal would remain an important part of the economy for “decades to come”. Labor’s shadow resources minister, Joel Fitzgibbon, who suffered a large swing against him in the coal seat of Hunter, has also been agitating for the party to do more to support the mining industry. But amid the simmering tensions, MPs sought to downplay the Otis group’s significance, with Marles calling the report of the dinner a “total beat up”. “At the end of the day, people were having a dinner,” he told Sky News. “I don’t think it’s a big deal.” The Victorian senator Kimberley Kitching said on Twitter that she was “pro coal-worker” and all workers, but was also “passionate about renewables and the jobs they bring”. “ALP conferences determine policy not dinner parties,” she said."
"

Media Contact: (202) 789‑5200



The Cato Institute announced today the expansion of its Center for the Study of Science. Founded in 2012, the Center for the Study of Science was created to provide market‐​based ideas that could transition policy regarding energy consumption, environmental standards, and other science‐​related issues away from government planners.



Today, the Center is adding scholars to a team that will continue to use rigorous science to answer questions related to environmental regulation. The Center will be especially focused on the debate over climate change.



Patrick J. Michaels, who will continue to direct the center, acknowledges climate change is occurring partly due to human actions. He does not believe, however, that these temperature fluctuations are cause for great alarm.



“Yes — burning fossil fuels to get the energy we need to advance as a global society does create carbon dioxide that recycles warming in the lower atmosphere,” said Michaels. “But despite what some scientists and politicians tell you, life as we know it will not end next week, next month or even in the next 500 years due to a warming planet. Policy makers need to know that there is a respectable group of scientists out there who don’t buy in to the alarmist hype.”



President Obama is pursuing an international agreement on carbon emissions that sidesteps Congressional ratification, an issue he is expected to discuss at a United Nations climate summit in New York next week. Michaels warns the President is “playing fast and loose with the Constitution.”



“We believe that some highly qualified scientists should be taking a more clear‐​eyed look at the data policy makers are using to draw conclusions which have resulted in a regulatory structure that inhibits economic activity and stifles innovation,” said Michaels.



Ross McKitrick, who teaches environmental economics at the University of Guelph, and Terence Kealey, Vice Chancellor of the University of Buckingham and a professor of clinical biochemistry, have been named adjunct scholars at the Center. They join Distinguished Senior Fellow Richard Lindzen, an emeritus professor of meteorology at both MIT and Harvard; Adjunct Scholar Edward J. Calabrese, a professor of environmental health sciences at the University of Massachusetts, specializing in toxicology; and Paul C. “Chip” Knappenberger, assistant director of the Center.



The Center for the Study of Science will seek to provide a credible source for media and members of the public who want a fresh perspective on scientific claims made by government and other research organizations. Research areas will include energy use and taxation; use of government subsidies; global warming; and overall environmental regulation.



“The truth is, counter to what President Obama claimed in 2010, the science on climate change is not settled,” said Cato President and CEO, John Allison. “The time is now to build a critical mass of credible scholars who can engage in the type of debate the public needs to hear in order to make informed decisions.”



Michaels said additional scholars and scientists will be named to the Center for the Study of Science in the coming months.
"
"
AIRS has higher resolution tracking of global CO2 - click for image
I’m going to make a formal post on this later, but I wanted to bring it up for discussion now since many people have been waiting for this paper to be published. For my previous perspectives and replies from authors, see this post here:
An encouraging response on satellite CO2 measurement from the AIRS Team
Hat tip to F Rasmin who writes with a link to the new paper:
Hello Anthony. Is this the awaited paper from the AIRS TEAM? ‘Satellite remote sounding of mid-tropospheric CO2′, published 9 September 2008 at:
http://www.agu.org/journals/gl/gl0817/2008GL035022/ 

REPLY: Yes it is. This was on my list of things to check this week, thanks for the tip! I’ll write it up sas soon as I can read it. In the meantime, feel free to post more comments on it in this thread.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c4bac48',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Indian environmentalist Rajendra Kumar Pachauri, under whose leadership a UN climate change panel shared the 2007 Nobel peace prize, has died after recent heart surgery. He was 79. Pachauri’s death was announced late on Thursday by the Energy and Resources Institute (TERI), a research group he headed until 2016 in New Delhi.  He chaired the Intergovernmental Panel on Climate Change panel from 2002 until he resigned in 2015 after an employee at his research firm accused him of sexual harassment. The IPCC and the former US vice-president Al Gore were awarded the 2007 Nobel for their efforts to expand knowledge about anthropogenic climate change and lay the foundations for counteracting it. Pachauri had undergone surgery in a New Delhi hospital this week. He died at his home on Thursday, the Press Trust of India reported. Pachauri won civilian awards from India’s government in 2001 and 2008. TERI’s chairman, Nitin Desai, hailed Pachauri’s contribution to global sustainable development. “His leadership of the Intergovernmental Panel on Climate Change laid the ground for climate change conversations today,” Desai said. The allegations against Pachauri included claims that he sent suggestive text messages, e-mails and WhatsApp messages harassing a 29-year-old female employee in his organisation. Pachauri had denied the charges and his attorneys claimed his messages were hacked in an attempt to malign him. New Delhi police filed a complaint in court but the trial could not be completed. Professor Jean-Pascal van Ypersele, the IPCC vice-chairman from 2002 to 2015, said coming from a developing country Pachauri should be credited for drawing the attention, long before others, to the importance of finding synergies between climate policies and sustainable developing agenda. “Unfortunately he was sometimes overconfident as when he refused to quickly acknowledge and correct the insignificant error that had been present in an IPCC report. This led to escalated and undue criticism of the organisation he chaired.” Pachauri is survived by his wife, a son and a daughter."
"There is a side to the Ebola crisis that, perhaps understandably, has received little media attention: the threat it poses to our nearest cousins, the great apes of Africa. At this moment in time Ebola is the single greatest threat to the survival of gorillas and chimpanzees. The virus is even more deadly for other great apes as it is for humans, with mortality rates approximately 95% for gorillas and 77% for chimpanzees (Pan troglodytes). Current estimates suggest a third of the world’s gorillas and chimpanzees have died from Ebola since the 1990s. As with humans, these deaths tend to come in epidemics. In 1995, an outbreak is reported to have killed more than 90% of the gorillas in Minkébé Park in northern Gabon. In 2002-2003 a single outbreak of ZEBOV (the Zaire strain of Ebola) in the Democratic Republic of Congo killed an estimated 5,000 Western gorillas (Gorilla gorilla). It’s hard to accurately count such elusive creatures but the WWF estimates there are up to 100,000 left in the wild – so a single Ebola outbreak wiped out a considerable chunk of the world’s gorilla population. There are of course additional factors behind the declining numbers of Africa’s great apes: illegal trading in wildlife and bushmeat, war, deforestation and other infectious diseases. The world’s remaining wild apes are being increasingly forced into isolated pockets of forest, which impedes their ability to forage, breed and to hide from hunters. There is also a growing body of evidence linking deforestation and subsequent changes in climate to the spread of Ebola and other infectious diseases. Back in 2003 an article on the decline of great apes, written by a team led by primatologist Peter Walsh, predicted that: Without aggressive investments in law enforcement, protected area management and Ebola prevention, the next decade will see our closest relatives pushed to the brink of extinction. Sadly, this prediction appears to have come true. Since 2008, the IUCN has listed the Eastern Gorilla (Gorilla beringei) as endangered and the Western Gorillas as critically endangered. If we do not act fast, these may prove to be the last decades in which apes can continue to live in their natural habitat. Unfortunately, there appears to be a lack of political will to implement policies which would bring viable solutions into effect.  We need both short-term solutions to halting the spread of Ebola and long-term ones to prevent future outbreaks. As a short-term strategy, vaccination could prove enormously useful in tackling the Ebola crisis in apes. Unlike for humans, a vaccine for gorillas and apes has been developed which thus far has been proven both safe and effective.  To date though, these trials have not involved “challenging” the vaccinated chimps with the live virus. Across much of Europe, medical research on great apes is either banned or highly restricted because of their cognitive similarity to humans. The question is whether or not we should make an exception in this case. In the long term, conservation efforts aimed at restoring forest habitat could also help curb the spread of the virus, as larger forested areas would reduce the chances of infected animals coming into contact with one another. In tandem with forest regeneration, greater protection for apes from hunters and strict laws to control bushmeat consumption would also be hugely beneficial, both for apes and for humans."
"

Not long ago, a group of Cato scholars entertained the question of whether the intellectual debate for free trade had been won.



There was near consensus that it had — in 1776 with publication of _The Wealth of Nations_. In the 240 years to follow, efforts to poke substantive holes and refute Adam Smith’s treatise failed and, today, nearly all economists agree that free trade, by expanding the size of the market to enable greater specialization and economies of scale, generates more wealth than any system that restricts cross‐​border exchange.



What that Cato confab failed to produce was agreement about whether the question under consideration was even pertinent. After all, how much does it really matter whether the intellectual debate has been won when, in practice, free trade remains stubbornly elusive, and the process of U.S. trade policy formulation is distinctly antiintellectual? Consider trade agreements. At the heart of negotiations that produce these deals rests the fallacy that domestic trade barriers are assets to be dispensed with only if reciprocated, in roughly equal measure, by negotiators on the other side of the table. That’s not Adam Smith. That’s neo‐​mercantilism, which posits that policy should aim to maximize exports and minimize imports. Yet Smith is credited with vanquishing mercantilism, which held sway in his day — and apparently still does today.



If the free trade consensus were truly meaningful, trade negotiations would be unnecessary. If free trade were the rule, trade policy would have a purely domestic orientation and U.S. barriers would be removed without need for negotiation because they would be recognized for what they are: taxes on consumers and businesses that impede the global division of labor and the creation of wealth. Apparently, the intellectual consensus for free trade coexists with an absence of free trade and a persistence of protectionism in practice.



For example, in the United States, there are “Buy American” rules that restrict most government procurement spending to U.S. suppliers, ensuring that taxpayers get the smallest bang for their buck; heavily protected services industries, such as transportation and shipping, that drive up the cost of everything; apparently interminable farm subsidies; quotas and high tariffs on imported sugar; high tariffs on basic consumer products, such as clothing and footwear; energy export restrictions; the market‐​distorting cronyism of the Export‐​Import bank; antidumping duties that strangle downstream industries and tax consumers; regulatory protectionism masquerading as public health and safety precautions; rules of origin and local content requirements that limit trade’s benefits; restrictions on foreign investment, and so on.



If an intellectual consensus for free trade exists, policy doesn’t reflect it and politicians appear to abhor it. If anything, the 2016 presidential election season reveals an American public — pitchforks and scythes in hands — ready to storm the ivory tower.



 **TRADE IS RIPE FOR DEMAGOGUERY**  
To cheering crowds, Donald Trump promises to slap duties on imports from China and Mexico and to use the tax code to punish U.S. companies that outsource parts of their operations abroad. Bernie Sanders vows to tear up NAFTA and other free trade agreements, calling them “a disaster for American workers.” Hillary Clinton, a co‐​architect of the Trans‐​Pacific Partnership trade agreement (TPP), now opposes that deal, while promising to disregard certain U.S. treaty obligations with China. Ted Cruz, projecting the pain of workers who have been displaced by import competition and outward investment (but, apparently, not those displaced by technology, changing consumer tastes, or poor business management), says trade has been “unfair” and pledges to “bring our jobs back from China.”



Scapegoating trade for problems real and imagined is nothing new. Blaming the Japanese, Mexicans, Chinese, and other foreigners for domestic woes ingratiates politicians to excitable elements of the electorate and helps them direct voter anger away from their own records. It has become a kind of quadrennial tradition ever since the NAFTA debate took center stage in the 1992 election.



Throughout the 2012 campaign, Mitt Romney assailed President Obama for failing to label China a “currency manipulator,” and the candidates exchanged accusations about who was more “culpable” for “shipping jobs overseas.” Promising to bring manufacturing jobs back home, Rick Santorum resonated with trade‐​skeptical voters, and even won the Iowa caucus that year. In 2008 Senators Obama and Clinton vied to be seen as the supreme trade‐​rules enforcer, each pledging to force U.S. trade partners back to the table to renegotiate NAFTA and various World Trade Organization agreements to make the terms “fair” for American workers. Demonization of trade was also a major component of John Edwards’s divisive “Two Americas” message that year.



John Kerry tapped into the same vein of public anxiety in 2004, referring to U.S. businesses that outsource call centers to places like India as “Benedict Arnold” companies. Blaming Mexico, Japan, and inside‐​the‐​beltway complicity for U.S. manufacturing decline and the erosion of American power, Pat Buchanan promised to punch back with force. His populist message energized the feisty “Buchanan Brigades” and helped him win the New Hampshire primary in 1996.



Trade‐​bashing became popular during the 1992 election, as books about the United States “trading places” with an ascendant Japan flew off the shelves and Ross Perot warned of the imminence of a “giant sucking sound” coming from south of the border. So campaigning politicians denigrating trade is nothing new. It seems to be inextricably woven into the fabric of our presidential elections. But something seems different this year. The tone is harsher. The digs are coming from across the political and ideological spectra. Two of the candidates — Sanders and Trump — seem genuine in their antipathy and their resolve to act. And their messages resonate especially well with primary election voters, who tend to hail from the extremities of the major parties, where trade and globalization are viewed with the greatest skepticism. But, again, these constituencies and their concerns aren’t particularly new either.



What is new — at least for the first time since NAFTA loomed large 24 years ago — is that a major trade agreement (indeed, the largest preferential trade agreement in U.S. history) is being debated and possibly considered for ratification by the U.S. Congress this year. Trade policy has featured prominently in the public square since January 2015, when the president and the new congressional leadership began their push to secure passage of Trade Promotion Authority (TPA) to facilitate completion and ratification of the TPP and, eventually, the Transatlantic Trade and Investment Partnership.



Although the TPA debate itself was shortlived, with the legislation passing in June of last year, anti‐​trade lobbies such as the Sierra Club, the AFL-CIO, and Public Citizen have been mobilizing for several years in anticipation of an epic battle over the TPP. Their anti‐​trade campaigns, with assertions and slogans evoking fantastical worst‐​case scenarios about the relationship between trade and climate change, trade and cancer rates, and trade and joblessness have played to popular fears, and have succeeded in winning more people to their cause. Protectionist ranks have been augmented by those with other kinds of economic grievances in a way that evokes _New York Times_ columnist Thomas Friedman’s 2001 description of the antiglobalization movement as the “well‐​intentioned but ill‐​informed being led around by the ill‐​intentioned and well‐​informed.” Though they are not necessarily wellinformed, the 2016 presidential candidates are complicit in creating this climate of misinformation.



 **UNSEEN CREATION**  
The case for free trade is not obvious. The benefits of trade are dispersed and accrue over time, while the adjustment costs tend to be concentrated and immediate. To synthesize Schumpeter and Bastiat, the “destruction” caused by trade is “seen,” while the “creation” of its benefits goes “unseen.” We note and lament the effects of the clothing factory that shutters because it couldn’t compete with lower‐​priced imports. The lost factory jobs, the nearby businesses on Main Street that fail, and the blighted landscape are all obvious. What is not so easily noticed is the increased spending power of the divorced mother who has to feed and clothe her three children. Not only can she buy cheaper clothing, but she has more resources to save or spend on other goods and services, which undergirds growth elsewhere in the economy.



Consider Apple. By availing itself of lowskilled, low‐​wage labor in China to produce small plastic components and to assemble its products, Apple may have deprived U.S. workers of the opportunity to perform that low‐​end function in the supply chain. But at the same time, that decision enabled iPods and then iPhones and then iPads to be priced within the budgets of a large swath of consumers. Had all of the components been produced and all of the assembly performed in the United States — as President Obama once requested of Steve Jobs — the higher prices would have prevented those devices from becoming quite so ubiquitous, and the incentives for the emergence of spin‐​off industries, such as apps, accessories, Uber, and AirBnb, would have been muted or absent.



But these kinds of examples don’t lend themselves to the political stump, especially when the campaigns put a premium on simple messages. This is the burden of free traders: Making the unseen seen. It is this asymmetry that explains much of the popular skepticism about trade, as well as the persistence of often repeated fallacies.



 **THE MYTHS**  
One of the most frequently invoked trade myths is the portrayal of trade as a competition between “us” and “them.” Central to this perception is that exports are Team America’s points, imports are the foreign team’s points, and the trade account is the scoreboard. Since that scoreboard shows a deficit, the United States is losing at trade, and it’s losing because the foreign team cheats — too often with impunity. Sound familiar?



This fundamental mercantilist fallacy about the nature of trade has a nationalistic appeal, where America is some monolithic entity best served by policies that strengthen her stature vis‐​à‐​vis some foreign monolith. But trade does not occur between countries. Trade is the culmination of billions of daily transactions pursued by individuals seeking value through exchange.



When we transact at the local supermarket, we seek to maximize the value we obtain by getting the most for our dollars. We strive to “import” more than we “export.” But when it comes to trading across borders or when our individual transactions are aggregated at the national level, we tend to forget these basic principles and accept the fallacy that the goal of trade is to achieve a surplus. But, as Adam Smith put it: “What is prudence in the conduct of every private family can scarce be folly in that of a great kingdom.” Never mind the intellectual consensus: This is common sense.



The benefits of trade come from imports, which deliver more competition, greater variety, lower prices, better quality, and new incentives for innovation. Arguably, opening foreign markets should be an aim of trade policy because larger markets allow for greater specialization and economies of scale, but real free trade requires liberalization at home. The real benefits of trade are measured by the value of imports that can be purchased with a unit of exports — our purchasing power or the so‐​called terms of trade. Trade barriers at home raise the costs and reduce the amount of imports that can be purchased with a unit of exports.



And as a result of globalization — the proliferation of cross‐​border investment and transnational supply chains — trade is more of a collaboration than ever before. Typically, about half of the value of U.S. imports is composed of intermediate goods and capital equipment — the purchases of U.S. producers.



How can imports be viewed as the other team’s points under those circumstances? Who, in fact, are “we” and who are “they”? The claim that the trade deficit means we are losing at trade — “losing billions of dollars every year to China and Mexico,” as Trump characterizes it — is another commonly invoked trade myth, which reflects a fundamental misunderstanding of international economics. By purchasing more goods and services from foreigners than foreigners purchase from Americans — trade deficit scolds claim — U.S. factories, farmers, and service providers are deprived of sales, which reduces domestic output, value added (GDP), and employment. That conclusion relies on the assumption that the dollars sent to foreigners to purchase imports do not make their way back into the U.S. economy. The dollars that go abroad to purchase foreign goods and services (imports) and foreign assets (outward investment) are matched nearly identically by the dollars coming back to the United States to purchase U.S. goods and services (exports) and U.S. assets (inward investment). Any trade deficit (net outflow of dollars) is matched by an investment surplus (net inflow of dollars).



This process helps explain why GDP and the trade deficit rise and fall in tandem, and why 41 consecutive years of trade deficits have had no adverse impact on the economy. The fallacy that trade killed U.S. manufacturing has long been a pretense for protectionism or industrial policy. Trump follows in these footsteps when he writes:





U.S. manufacturing is not only alive, it’s thriving. By all relevant metrics — output, value‐​added, revenues, exports, imports, investment, R&D expenditures — U.S. manufacturing remains a global “powerhouse.” With respect to most of those measures, year after year the sector sets new records. U.S. manufacturing attracts more foreign direct investment (FDI) than any other country’s manufacturing sector. In 2014 the stock of FDI in U.S. manufacturing surpassed $1 trillion, more than double the value of FDI in China’s manufacturing sector (and eight times the value in per capita terms).



If by “rapid deindustrialization” Trump means that manufactured goods account for a smaller share of U.S. output than in the past, he’s right about the statistic, but not the interpretation. Manufacturing’s share of the U.S. economy peaked in 1953 at 28.1 percent, whereas today manufacturing accounts for only 12.1 percent of GDP. But in 1953 U.S. manufacturing value added amounted to $110 billion, as compared to a record $2.1 trillion in 2015 — more than six times the value in real terms.



Bernie Sanders is wary of capitalism and in favor of equality of outcome. He perpetuates another common myth: Trade only benefits multinational corporations and the rich. But nothing could be further from the truth. Just like during the Gilded Age, the tariff remains the mother of the trust. And, like then, free trade should be the progressive position.



Protectionism benefits producers over consumers; it favors big business over small business because the cost of protectionism is relatively small to a bigger company; and, it hurts lower‐​income more than higherincome Americans because the former spend a higher proportion of their resources on imported goods.



The United States has relatively low tariffs on average — less than 2 percent. But tariffs on clothing (18 percent), footwear (14 percent), and food products (10 percent) are especially high. Meanwhile, U.S. antidumping restrictions on steel, lumber, cement, appliances, flooring, nails, and paint elevate the material costs of home building. Imports of life’s basic necessities — food, clothing, and shelter — are subject to some of the highest taxes. Why isn’t that too regressive for a progressive like Sanders?



 **WHAT DOES THE RHETORIC PORTEND?**  
Demagoguing trade has become an election year pastime. But trade issues tend to be of marginal concern to voters in the general election, and history suggests that cooler heads will prevail. Despite the abundance of antitrade rhetoric on the campaign trail, it is difficult to imagine an actual president of the United States supporting policies commensurate with the bluster. Every president since FDR, regardless of political party, has embraced or promoted trade liberalization.



While candidates might rail against unfair trade practices and unlevel playing fields on the stump, they change their tunes after taking the oath. Presidents prioritize broader, national interests over regional and parochial issues, and tend to see merit in projecting global economic leadership. They also view trade policy through the prism of foreign policy, and recognize the contributions that trade makes to economic growth and international stability.



Even if there were a President Trump or President Sanders, rest assured that the Congress still has authority over the nuts and bolts of trade policy. The scope for presidential mischief, such as unilaterally raising tariffs, or suspending or amending the terms of trade agreements, is limited. But it would be more reassuring still if the intellectual consensus for free trade were also the popular consensus.



What matters most is that Americans have realized progressively greater freedom to transact with people in other countries over the years. Many barriers still remain. But when the evidence of the economic benefits of liberalization is weighed against the myths and political aspersions, trade is exonerated on all counts.
"
"The feeding habits of an unusual 200-million-year-old fish have been tested in a ground-breaking study published in Palaeontology. This research is particularly notable as it wasn’t carried out by a leading professor but by Fiann Smithwick, a University of Bristol undergraduate. The study illustrates a change in the science of palaeobiology: what was once a somewhat speculative field has now become analytical, and different models can be tested against our expectations. The fish, Dapedium, is known from fossils found in the Lower Lias rocks near Lyme Regis in Dorset, on England’s south coast. It lived side by side with the great sea reptiles of the Jurassic such as the dolphin-shaped ichthyosaurs, long-necked plesiosaurs, and even some marine crocodilians.  Dapedium was one of a number of these animals first discovered by the pioneering 19th century fossil collector Mary Anning, which fascinated early palaeontologists such as Louis Agass and Henry De la Beche, who created some of the first paleoart based on Anning’s fossils. Dapedium was a deep-bodied fish, shaped like a dinner plate in side view, which could grow to more than half a metre in length. It probably escaped being caught by the larger predators pictured above because it was so thin-bodied it might be hard to see head-on. It had a tiny mouth with jutting front teeth and masses of pebble-shaped teeth further back.  But what were those teeth actually used for? To reconstruct the feeding behaviour of this ancient fish, Smithwick applied a new lever-based mechanical model developed by Mark Westneat at the University of Chicago as a means of quantifying jaw motions and forces in modern teleost fishes. Teleosts have more complex jaws than tetrapods such as ourselves – there is not a single jaw joint, but four, so when it is feeding a teleost can project its mouth forwards in the classic “pout”.  Modelling the exact lengths of the multiple cranks of the jaws, their angles to each other, and likely muscle forces, means scientists have a good idea how all modern fishes use their jaws. Depending on their diet, the jaws operate in different ways to snatch soft prey, crush shells, snip corals, or chew seaweed. Smithwick applied this model to 89 specimens of Dapedium in the Natural History Museum, Bristol City Museum, and the Philpot Museum in Lyme Regis, and measured the positions and lengths of the jaw bones.  He figured out the positions and orientations of jaw muscles and varied these to include all possible models.  His calculations showed Dapedium was a shell crusher. Its jaws moved slowly, but strongly, so it could work on the hard shells of its prey. Other fishes have fast-moving, but weaker jaws, adapted for feeding on speedier fish. In comparison with modern fishes, Dapedium matches closely the modern sea breams.  These fishes are also flat-sided and deep-bodied, and they crush shells in their small mouths, armed with blunt-topped teeth.  The new research takes what we know about the mechanics of modern fish such as the sea bream and predicts the jaw mechanics of fossil fishes within this framework. Speculation is not necessary. Smithwick was funded by a Summer Research Bursary from the Palaeontological Association, and he devised the project himself, learned the numerical techniques, and wrote it up himself.  It’s rare for an undergraduate to be able to do all this and pass the scrutiny of one of the world’s leading scientific journals."
"The EU has given its formal backing to 32 major gas infrastructure projects in a move critics say will lock Europe into burning fossil fuels for generations. MEPs voted to support the European commission’s proposal by 443 votes to 169 on Wednesday, with 36 abstentions, provoking environmental groups to lament Brussels’ “hypocrisy” over the climate emergency.  The total cost of the infrastructure is estimated to be €29bn. Under the EU’s funding programme up to 50% of the costs of each project could be covered by European taxpayers. The consulting firm Artelys has claimed in a report that the majority of the projects, stretching from Ireland to Croatia, are “unnecessary”. Environmental groups said gas firms were enjoying the fruits of heavy lobbying and the sector’s advantageous position within the commission’s decision-making institutions. An industry-led advisory body provides the European commission with forecasts on Europe’s future energy needs. Colin Roche, climate justice coordinator for Friends of the Earth Europe, said: “This climate hypocrisy has to end. Following unprecedented disasters like Australia’s wildfires, history will look unkindly on those who today backed building more fossil fuel pipelines and terminals. “A European green deal is not possible with more fossil fuels, and Europe needs to go fossil-free fast.” The European energy commissioner, Kadri Simson, from Estonia, had asked the parliament to back the proposal on the grounds that three-quarters of the 151 projects were electricity- rather than gas-based.  The parliament had the option of accepting the entire list, the fourth proposed by the commission under its “connecting Europe” programme, or voting it down in its entirety. Simson said: “An objection to the 4th PCI [Projects of Common Interest] list would mean that the 3rd PCI list remains in force – a list with 40% more gas projects than the new list. “As a consequence, key electricity interconnectors and energy transition projects such as the North Sea Wind Power Hub, new smart grid projects and new CO2 network projects would not be eligible for funding under the Connecting Europe Facility.” Simson said she would ensure the next project list “reflects the European green deal priorities”, in reference to the EU’s goal of carbon emission neutrality by 2050. She insisted that being on the new list did not guarantee funding. Ciarán Cuffe, an Irish MEP from the Green party, who voted against the projects, said: “Given the severity of the climate emergency and the little time we have left to avert climate catastrophe, it is wrong to spend more public money on fossil fuel investments. “We are past the time to channel public money into expensive and unnecessary fossil fuel projects when Europe should be investing in energy efficiency and renewables.”"
