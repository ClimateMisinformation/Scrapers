"

It’s nearly impossible to read major newspapers, magazines, or online publications in recent months without encountering a plethora of articles contending that the United States is turning inward and “going alone,” “abandoning Washington’s global leadership role” or “retreating from the world.” These trends supposedly herald the arrival of a new “isolationism.” The chief villain in all of these worrisome developments is, of course, Donald Trump. There is just one problem with such arguments; they are vastly overstated bordering on utterly absurd.



President Trump is not embracing his supposed inner isolationist. The policy changes that he has adopted regarding both security and international economic issues do not reflect a desire to decrease Washington’s global hegemonic status. Instead, they point to a more unilateral and militaristic approach, but one that still envisions a hyper‐​activist U.S. role.



For instance, it’s certainly not evident that the United States is abandoning its security commitments to dozens of allies and clients. Despite the speculation that erupted in response to Trump’s negative comments about the North Atlantic Treaty Organization (NATO) and other alliances during the 2016 election campaign (and occasionally since then), the substance of U.S. policy has remained largely unchanged. Indeed, NATO has continued to expand its membership with Trump’s blessing—adding Montenegro and planning to add Macedonia.





If you look at his actions and not his words, you won’t find it.



Indeed, Trump’s principal complaint about NATO has always focused on European free‐​riding and the lack of burden‐​sharing, not about rethinking the wisdom of the security commitments to Europe that America undertook in the early days of the Cold War. In that respect, Trump’s emphasis on greater burden‐​sharing within the Alliance is simply a less diplomatic version of the message that previous generations of U.S. officials have tried sending to the allies.



Moreover, Trump’s insistence at the July NATO summit in Brussels that the European nations increase their military budgets and do more for transatlantic defense echoed the comments of President Obama’s Secretary of Defense Chuck Hagel in 2014. Hagel warned his European counterparts that they must step up their commitment to the alliance or watch it become irrelevant. Declining European defense budgets, he emphasized, are “not sustainable. Our alliance can endure only as long as we are willing to fight for it, and invest in it.” Rebalancing NATO’s “burden‐​sharing and capabilities,” Hagel stressed, “is mandatory—not elective.”



Additionally, U.S. military activities along NATO’s eastern flank certainly have not diminished during the Trump administration. Washington has sent forces to participate in a growing number of exercises (war games) along Russia’s western land border—as well as in the Black Sea—to demonstrate the U.S. determination to protect its alliance partners. Trump has even escalated America’s “leadership role” by authorizing the sale of weapons to Ukraine —a very sensitive step that President Obama carefully avoided.



Trump even seems receptive to establishing permanent U.S. military bases in Eastern Europe. During a state visit to Washington in mid‐​September, Poland’s president, Andrzej Duda, promised to provide $2 billion toward construction costs if the United States built a military base in his country. Duda even offered to name the base “Fort Trump.” Trump’s reaction was revealing. Noting that Poland “is willing to make a very major contribution to the United States to come in and have a presence in Poland,” Trump stated that the United States would take Duda’s proposal “very seriously.” _American Conservative_ columnist Daniel Larison notes that while Trump often is accused of wanting to “retreat” from the world, “his willingness to entertain this proposal shows that he doesn’t care about stationing U.S. forces abroad so long as someone else is footing most of the bill.”



U.S. military activism does not seem to have diminished outside the NATO region either. Washington persists in its futile regime‐​change campaign in Syria, and it continues the shameful policy of assisting Saudi Arabia and its Gulf allies pursue their atrocity‐​ridden war in Yemen. Both of those Obama‐​era ventures should have been prime candidates for a policy change if Trump had wished to decrease America’s military activism.



There are no such indications in Europe, the Middle East, or anywhere else. The U.S. Navy’s freedom of navigation patrols in the South China Sea have actually increased in size and frequency under Trump—much to China’s anger . Washington’s diplomatic support for Taiwan also has quietly increased over the past year or so, and National Security Advisor John Bolton is on record suggesting that the United States move some of its troops stationed on Okinawa to Taiwan. The U.S. military presence in Sub‐​Saharan Africa is increasing, both in overall size and the number of host countries.



Those are all extremely strange actions for an administration supposedly flirting with a retreat from the world to be adopting. So, too, is Trump’s push for increases in America’s already bloated military budget, which now exceeds $700 billion—with even higher spending levels on the horizon.



Accusations of a U.S. retreat from the world on non‐​military matters have only slightly greater validity. True, Trump has shown little patience for multilateral arrangements such as the Trans‐​Pacific Partnership, the Paris climate agreement, or the United Nations Human Rights Council that he concluded did not serve America’s national interests. On those issues, the president’s actions demonstrated that his invocation of “America First” was not just rhetoric. However, regarding such matters, as well as the trade disputes with China and North Atlantic Free Trade Agreement partners, the administration’s emphasis is on securing a “better deal” for the United States, not abandoning the entire diplomatic process. One might question the wisdom or effectiveness of that approach, but it is a far cry from so‐​called isolationism.
"
"

 **A Review of _Reality and Rhetoric: Studies in the Economics of Development_ , by P.T. Bauer, Cambridge, Mass.: Harvard University Press. 184 pp. $15.00**



Professor Peter T. Bauer, of the London School of Economics, is one of those intellectually heroic figures who has stood fast against the fads and hysteria of his time. While the vast currents of “development economics” inundated us with “overpopulation” theories and “vicious cycle of poverty” doctrines that depicted massive foreign aid as the only salvation of the Third World, Bauer said “No!” loud and clear—but virtually alone.



Despite his scholarly achievements and personal experience in the underdeveloped world, Bauer was long ignored or disparaged as he poked holes in the prevailing orthodoxy. Today, he can no longer be ignored‐​not even in academic and media circles where the prevailing orthodoxy was once treated as the one true faith. Bauer’s message has begun to be heard, not only because of his own perseverance and insights, but also because the repeated failures and massive disasters of “development planning” have finally broken through the smug unanimity that long substituted for evidence or critical analysis.



 _Reality and Rhetoric_ is a compilation of Bauer’s essays over the years on such topics as foreign aid, “planning” versus markets in the Third World, imperialism, and the moralistic pronouncements of the clergy on the international economic order. These essays are written and reasoned in a very straightforward way. It is enough to make you forget that he is an economist.



Bauer’s criticisms of current thinking about Third World nations are both wide and deep. He questions the very concept of “foreign aid” or “the third World.” Whether international transfers of money to the less‐​developed nations are an aid or a hindrance to their economic progress is for Bauer a question rather than a foregone conclusion. His own reading of the evidence is that it has hindered more than it has helped.



The tremendous range of extremely different nations lumped together as “the Third World” likewise makes no sense to him. All that these nations have in common is that they receive “foreign aid.” A few of these recipient nations have even had higher per capita income than some of the donor nations. Most—if not all—of the poorer nations have classes of people who are more affluent than the average Western taxpayer in the donor nations—and it is precisely those affluent people who have the inside track in getting their hands on the foreign aid.



Bauer is not a mere Scrooge who says “Bah! Humbug!” to the poor. On the contrary, his vision of the world accords far more respect to the less‐​developed regions and peoples than does the conventional viewpoint. Bauer denounces the “contempt for ordinary people” that underlies development planning. 



Drawing on his own many years of research and observation, he punctures the idea that Third World people can progress only under the tutelage of foreign experts or their own westernized elite. Evidence to the contrary, he notes, is found in “the large scale capital formation in agriculture by the local people” in West Africa; the fact that over half the acreage planted with rubber trees in southeast Asia was owned by Asians, even before World War II; and large‐​scale international migrations by poor and illiterate people who were nevertheless “well informed about economic conditions in distant and alien countries.” 



Dramatic economic changes over time likewise belie the stereotypical picture of hopelessly stagnant peasants needing foreign “experts” or … So much for the notion that Third World masses cannot think beyond today.



Bauer also recognizes “the reality and importance of group differences” within the population of a given nation, even though this subject “is virtually proscribed in the profession.” Particular segments of the population of very poor and backward nations often have people who are entrepreneurial, hard‐​working, thrifty, and with great initiative and imagination. Far from making use of such people for advancing the economic level of the country, many Third World governments devote great efforts to stifling or even expelling such groups, especially when they are racial or ethnic minorities whose prosperity is envied and resented by others. The Chinese in Southeast Asia, the Indians in East Africa, the Lebanese in West Africa, and the Jews historically in Eastern Europe are only some of the more‐​prominent examples of this very widespread phenomenon.



Early in his career, Bauer was struck by these inter‐​group differences, which were largely ignored by other development economics: “The differences in economics performance and hence in achievement among groups were immediately evident, indeed startling.” Unskilled plantation workers in Malaya, working with primitive implements, nevertheless differed in output by a ratio of two‐​to‐​one as between Chinese and Indian workers, though both were “undedicated coolies.” Differences in other occupations‐​especially entrepreneurial occupations‐​are even greater.



Contrary to the prevailing egalitarian ethic, Bauer declares that “differences in incomes and rates of progress and regions…are not reprehensible. They are inevitable.” Egalitarianism is to Bauer simply the “legitimization of envy.” He rejects “the notion that the well‐​off have prospered at the expense of the poor” and calls it “the most pernicious of all economic misconceptions.” Implicitly, it assumes a zero‐​sum world, in which A gains only at the expense of B, turns attention away from the central issue of how to increase total wealth. Throttling the production of wealth, in the name of equality, is not humanitarianism but moralistic self‐​indulgence. So is guilt. Bauer regards “guilt in the West toward the Third World” as “a feeling which does nothing to assist the ordinary people” of the poorer countries.



If your purpose is to understand economic development in the poorer nations, you cannot get a better brief introduction to the subject than in Reality and Rhetoric. If your purpose is to learn the latest fashions in theories and buzzwords, this is not the place. “Statement of the obvious,” Bauer says, “has become a major task,” in part because “prominent economists have perpetuated the grossest elementary transgressions of fat and logic.” Words like infrastructure and phrases like the vicious cycle of poverty have created reputations and programs, even as they have soared above reality and left disaster after disaster in their wake.



Bauer not only mentions some of these disasters but points out how “foreign aid” subsidizes them. The international aid organizations’ emphasis on “need” in general and short‐​run crisis management in particular means that poor nations that have behaved responsibly, and lived within their means, are far less likely to get money than governments that have spent lavishly, engaged in grandiose social and economic experiments, and run up huge foreign debt without any concern for how—or whether—they would pay it off. Bauer is not afraid to call this “preferential treatment of the incompetent, the improvident or dishonest.”



In its effects on national well‐​being, the difference between responsible and irresponsible government is seen by Bauer as far more important than the sums transferred by international aid organizations. Insofar as these transfers reward counterproductive government policies, the losses they engender may readily exceed any benefits they can purchase. The sums involved in these international transfers are often not very large relative to total national output but are very impressive as a percentage of government discretionary spending. Therefore their effect on government policy may be very large—and very counterproductive—while they directly add relatively little to the available resources of the economy.



In India, for example, foreign aid in 1980 amounted to less than 2 percent of gross national product (GNP), but it was 18 percent as large as the government’s total tax receipts. In Tanzania, foreign aid was 18 percent of GNP and slightly larger than all taxes received by the government.



In short, foreign aid greatly increases the recipient government’s economic leverage in the economy. In addition, international development agencies tend to be biased toward statist policies, both inherently and as a matter of choice. Inherently because it is, after all, governments that receive both financial resources and the advisory personnel provided by the international development agencies. Moreover, “many staff members of the international organizations favour dirigiste policies (state economic planning),” according to Bauer.



“The international aid organizations and their staffs are not disinterested,” Bauer points out, but instead have heavy personal and institutional stakes in a large and growing amount of foreign aid. These aid organizations are politically active and effective in the Western nations. Their version of the world economic picture is constantly fed through the media to the public as the only humane and decent way to see it. They have patronage to offer academics in the form of jobs and consulting arrangements. At the same time, these international bureaucratic empires are dependent on the Third World nations to accept their aid—and often express fears that the aid would be refused if various conditions were attached to ensure responsible behavior by the recipients.



The moral climate generated by Western intellectuals—including the media and the clergy‐​is one of the key ingredients in the political success of this process of draining money from Western taxpayers for the benefit of Third World ruling classes and international bureaucracies. Guilt is one of the factors in this moral climate.



The idea that the poverty of some nations is caused by the affluence of other nations is taken as axiomatic in many quarters. Bauer, however, treats this notion as a hypothesis instead of an axiom and looks at the evidence. He finds that in fact poverty and backwardness are greatest in those Third World nations that have been least touched by Western imperialism, trade, or multinational corporations‐​for example, Ethiopia and Liberia in Africa, and Bhutan, Sikkim, Tibet, and Nepal in Asia.



Far from deferring to the moral authority of politically active clergy, Bauer characterizes their arguments as “immoral because they are incompetent.” He says: “There is profound truth in Pascal’s maxim that working hard to think clearly is the beginning of moral conduct.” Bauer sees these activist clergy as “seeking a new role for themselves in the face of widespread erosion or even the collapse of traditional beliefs.” Their susceptibility to any idea that calls itself “social justice” he regards as symptomatic of a lost religious faith that finds a substitute in secular credulity.



Professor Bauer is no longer alone, though he is still vastly outnumbered by those with a vested interest in the foreign‐​aid status quo. This book will make it harder for them to continue to pull the wool over the eyes of the taxpaying public.
"
"
While sunspots are often cited as the main proxy indicator of solar activity, there is another indicator which I view as equally (if not more) important. The Average Planetary Magnetic index (Ap), the strength of which ties into Svensmark’s cosmic ray theory modulating Earth’s cloud cover. A weaker Ap would mean less cosmic rays are deflected by the solar magnetic field, and so the theory goes, more cosmic rays provide more seed nuclei for clouds in Earth”s atmosphere. More clouds mean a greater albedo and less terrestrial solar radiation, which translates to lower temperatures.
I’ve always likened a sunspot to what happens with a rubber band on a toy balsa wood plane. You keep twisting the propeller beyond the normal tightness to get that extra second of thrust and you see the rubber band start to pop out knots. Those knots are like sunspots bursting out of twisted magnetic field lines.
The Babcock model says that the differential rotation of the Sun (the sun being a viscous fluid, the poles rotate at a slower rate than the equator) winds up the magnetic fields of it’s layers during a solar cycle. The magnetic fields will then eventually tangle up to such a degree that they will eventually cause a magnetic break down and the fields will have to struggle to reorganize themselves by bursting up from the surface layers of the Sun. This will cause magnetic North-South pair boundaries (spots) in the photosphere trapping gaseous material that will cool slightly. Thus, when we see sunspots, we are seeing these areas of magnetic field breakdown.

Sunspots are cross connected eruptions of the magnetic field lines, shown in red above. Sometimes they break, spewing tremendous amounts of gas and particles into space. Solar flares and coronal mass ejections (CME’s) are some examples of this process. Sometimes they snap back like rubber bands. The number of sunspots at solar max is a direct indicator of the activity level of the solar dynamo.
As many of you may recall, a few months ago, I had plotted the Average Geomagnetic Planetary Index (Ap) which is a measure of the solar magnetic field strength but also daily index determined from running averages of eight Ap index values. Call it a common yardstick (or meterstick) for solar magnetic activity.

Click for a larger image
I’ve updated the graph today, to include July 2008 Ap data as you can see below:

Click for a larger image
Source data, NOAA Space Weather Prediction Center:
http://www.swpc.noaa.gov/ftpdir/weekly/RecentIndices.txt
As you can see, the Ap Index has continued along at the low level (slightly above zero) that was established during the drop in October 2005. As of July 2008, we now have 34 months of the Ap hovering around a value between 5 to10, with occasional blips of noise.
Since it is provided in the same dataset, I decided to also plot the smoothed Ap Index.
I also plotted my own 24 month smoothing window plot, shown in magenta.

Click for a larger image
I also decided to update the plot of the 10.7 centimeter band solar radio flux, also a metric of solar activity. It is in the same SWPC dataset file as the Ap Index, in columns 8 and 9. The smoothed 10.7 CM flux value provided by SWPC has also dropped about the same time continues a downward trend.
I also provided my own 24 month wind smoothed value which is plotted in magenta.

Click for a larger image
Note the lower flux values during this solar minimum than the last
We continue to remain in a deep solar minimum, and with the forecasts being modified to push back the real “active” start of Solar cycle 24, it remains anybody’s guess as to when the sun will come out of it’s funk.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d0d54cd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGuest writer Ed Caryl recently looked at 9 “rural” stations scattered over the Arctic: from Alaska, to Canada, to Northern Europe western Russia and Siberia, and found Arctic temperatures follow the AMO, and not CO2. Read here A Light In Siberia. It’s important to note that the 9 stations were selected because they appeared to be NOT influenced by man-made heat sources.
First, here’s the AMO going back more than 150+ years. The cycles are clear to see.

Atlantic Multidecadal Oscillation (AMO). Source: http://www.appinsys.com/globalwarming/SixtyYearCycle.htm
North Pole, 17 March 1959. Image from NAVSOURCE
The AMO shows warm periods centering at about 1880, 1940 and 2005, i.e., 60 year cycle. We recall seeing photos of submarines surfacing at the North Pole back in the 1950s, see photo left, meaning it was relatively balmy back then too, as the AMO chart suggests.
Well how do the temperature curves of Ed’s 9 untainted Arctic stations match up with the AMO? The following are the GISS graphs of these 9 stations, each shown individually. Take a look at each of them:





What happens from 1940 – 1980, a time when CO2 was increasing? What happens after 1980? How do these charts match up with the above AMO chart? Fit pretty well? It seems so.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some of the temperature records shown above are shorter and some are longer. But they all show that temperatures between 1940 and 1980 were dropping. Remember that the Arctic is called the canary in the coal mine. When the globe cools or warms, you really see it in the Arctic, so they say.
Next Ed Caryl plotted each of the above graphs on a single chart. Ed calls these “rural” stations isolated because they are not impacted by man-made heat sources like asphalt, light bulbs, etc:
Plot of the 9 ""rural"" (isolated) stations.
And then he normalized the plots and generated an average. He explains how here, scroll down to “The averaging of station data”.  The resulting plot with a linear trend line is shown as follows:
Average of the ""rural"" stations
Sure some hot-shot statisticians out there are going to say you can’t do this, or that, or whatever blah blah blah…but that’s just nitpicking. Attention to tiny detail is a later thing.
Ed’s method suffices for now to generate a good general picture. If the math hotshots out there want to do it with micrometers, no one is stopping them. I doubt the general picture is going to change that much, though.
If you look at the 9 individual plots above and imagine how a composite of all 9 would look like, it would look like Ed’s chart – common sense.
Doesn’t the shape of above curve look eerily similar to the shape of the AMO from 1920 to today? Ed thought so too, and so he superimposed the average of the 9 isolated stations and the AMO:

Gee, do you think Arctic temperatures correlate better with CO2? 
Of course this is only a preliminary analysis that examined only 9 isolated stations scattered over the entire Arctic perimeter. But I suspect that if all stations were thrown in, except the crappy ones equipped with light bulbs and of that sort, you’d end up with similar results.
Could the AMO possibly drive climate? Well, the latest paper authored by Phil Jones and others seem to be hinting at this. Read my post from yesterday https://notrickszone.com/2010/09/24/der-spiegel-the-oceans-influence-greater-than-thought/.
Obvious conclusion: Trace gas Co2 drives the Arctic climate about as much as a sea breeze drives a loaded freight train.
Share this...FacebookTwitter "
"

The foreign policy record of the Clinton‐​Gore administration deserves a less than stellar grade. At the end of the Cold War, there was an extraordinary opportunity to build a new relationship with a democratic Russia; restructure U.S. security policy in both Europe and East Asia to reduce America’s burdens and risk exposure; and revisit intractable Cold War‐​era problems, such as the frosty relations with Cuba, Vietnam, and North Korea. The administration’s performance must be judged within the context of such an unprecedented opportunity for constructive change.



The record is acutely disappointing. True, the administration has scored some successes: improving the negotiating climate in Northern Ireland and the Middle East, pushing for permanent normal trade relations with China, and normalizing relations with Vietnam. But the failures greatly outnumber the successes. The administration needlessly meddled in the complex disputes of the Balkans, leaving to its successor two U.S.-led NATO protectorates (Bosnia and Kosovo) and a colossal mess of a nation‐​building commitment with no end in sight. A similar morass is emerging in Colombia as a result of the administration’s prosecution of the drug war.



U.S. policy toward long‐​time adversaries is on autopilot. The rote perpetuation of an economic embargo and occasional bombing attacks against Iraq have devastated the Iraqi people while barely bothering Saddam Hussein. Washington’s policy toward Cuba is equally sterile and cruel.



Worst of all is the growing list of missed opportunities. Instead of integrating a newly democratic Russia into the West, the Clinton administration needlessly antagonized Russia by expanding NATO’s membership and waging war against Moscow’s long‐​time allies in the Balkans. Relations with China have been damaged by an inconsistent, at times nearly incoherent, U.S. policy. Instead of embracing efforts for greater military self‐​reliance on the part of our European allies, the administration has engaged in carping criticism and apparently views such initiatives as a threat to America’s dominant position in the transatlantic relationship. Instead of viewing the end of the Cold War in East Asia as an opportunity to reduce America’s security burdens in that region, the United States insists on keeping 100,000 troops deployed seemingly forever. Administration officials even reacted with ambivalence to the recent summit between North and South Korea and gave highest priority to retaining the U.S. troop presence on the Korean peninsula.



Given the number of botched opportunities, the administration’s record merits a grade of D.
"
"
Share this...FacebookTwitterlist Game Indonesia buat Permainan Judi Online! waktu kamu mencari trik judi yg lebih baru. Permainan judi Indonesia pantas dipertimbangkan buat kamu. Berikut yaitu sekian banyak trick kamu sanggup menikmati perjudian di negeri ini bersama sekian banyak trick inovatif.
bersama perkembangan tehnologi ketika ini permainan game online jadi ternama. lantaran dgn main dengan cara online menciptakan kamu lebih nyaman & enteng utk memainkannya. Terutama dalam permainan judi online, permainan game online paling baik yg mampu menghibur kamu. Bahkan permainan judi online serta sanggup berikan kamu pendapatan penambahan.
Dalam permainan judi online ketika ini amat sangat beraneka ragam yg dapat kamu mainkan. Berikut ini ada sekian banyak permainan game judi online yg dapat kamu mainkan :
Bandarq
Ini ialah permainan judi Indonesia yg terkenal & serta dinamakan bersama permainan judi card. dalam permainan bandarq dibutuhkan enam card domino ganda dalam set dua puluh delapan. Dalam biasanya kasus, pemain memakai card mungil namun dikala mereka aus, mereka dibuang. Pemain mesti membayar jumlah yg terus sebelum mulai sejak main-main. Mereka mendapati tiga card domino buat menolong mereka disaat mereka dalam perbaikan.
kamu bebas bertaruh, lipat, naik atau dingin sesudah card kamu dievaluasi. bila tak ada petaruh pada awal mulanya, kamu bakal diizinkan utk bertaruh. tidak cuma itu, apabila permainan cuma melibatkan kamu adalah( cuma satu petaruh), itu berhenti sekaligus. kamu memperoleh pot dalam kasus itu tidak dengan mesti menunjukkan card kamu. Ada dua putaran yg dikenakan batas dalam permainan. Babak ke-2 mempunyai batas yg lebih tinggi.
tidak cuma itu, card ditempatkan berpasangan di mana seluruhnya pasangan ditambahkan. Pasangan paling atas buat tiap-tiap kontes merupakan 9 sedangkan pasangan kedua dianggap sbg yg terakhir. Pemain mengaplikasikan beraneka cara juga matematika buat menang. satu orang pemain diyakini mempunyai qiu sesudah tiga ganda awal.
Poker Online di Indonesia
Poker online kurang dibatasi di Indonesia di bandingkan dgn permainan tradisional. Pemain bisa membawa pertolongan dari web website poker internasional agung utk mencapai maksud mereka. terkecuali itu, mereka sedia di negara-negara dgn yurisdiksi pemerintah Indonesia yg lebih sedikit.
Poker internasional dgn pemain Indonesia tak diizinkan. namun para pemain bebas utk menikmati beragam situasi seperti permainan duit nyata & pertandingan. dgn begitu bermacam permainan dimainkan dengan cara online yg terjangkau.
Dominoqq
cocok pengamatan, perjudian online di negeri ini konsisten meningkat sejauh itu jadi sumber pendapatan bagi tidak sedikit orang. Dominoqq terdiri dari sekian banyak permainan termasuk juga Judi Bola (perjudian bola), permainan tradisional, permainan video poker & sebagainya.
Apa yg mesti kamu tonton dalam Permainan Judi Online?
https://semogaqq.net/
Mengamankan Uang
seluruhnya pemain sesudah membuahkan duit paling tidak sedikit dalam masa terpendek. telah terang bahwa transaksi aman mempermudah pemain buat menjaga duit mereka konsisten aman. pula masalah yg terkait dgn setoran & penarikan dihindari dgn ini.
tiap-tiap pemain mendapati akun pribadi di mana dirinya mampu mengecek penghasilannya. pula disediakan bermacam akun deposito maka para pemain sanggup bertransaksi duit dari akun yg sah mereka.
tapi dalam permainan judi online seperti bandarq, dominoqq & poker kamu butuh mengetahui & pilih web judi online terpercaya buat menopang jalannya permainan yg kamu mainkan diwaktu ini.
trik utk meraih website judi terpercaya kamu mesti menonton profil web tersebut. Atau kamu pun sanggup menyaksikan anggota yg bergabung di dalam website tersebut.
Nah, itulah rekomendasi dari artikel kami berkenaan list game judi online indonesia & trick buat pilih website. mudah-mudahan dgn adanya info ini bisa menopang kamu dalam permainan nantinya. Selamat main-main & mudah-mudahan berhasil!
Share this...FacebookTwitter "
"

The administration continues to hype global warming despite the larger fish sizzling on Washington’s grill. Every month Vice President Gore holds a press conference with a federal climatologist in tow to show that yet another record has been set. But there’s a story behind this story that the 2,000 environmental journalists out there have somehow managed to miss, despite the fact that all the information they need is available from government sources on the Internet. 



The tawdry misuse of history began, as predicted by many of us, as few days (eight) after New Year’s as were required to pull together enough data to spell W-O-L-F. And seven monthly wolves later, it turns out that Gore is really not talking about the globe’s temperature after all, and the science he’s peddling hasn’t even been peer reviewed. 



Scientists suspected something was funny with the first federal pronouncement, in January, that said that temperatures in 1997 were the warmest ever measured. Other temperature histories, such as those of NASA, showed no such thing. Global satellite readings put 1997 smack in the middle of the range for the last two decades. 



It turns out that the new history cited by Gore was developed for political impact. It was developed, according to a paper released on June 9 by the National Climatic Data Center (NCDC), a unit of the Department of Commerce, because “climate change questions involve a body politic and busy elected officials [shouldn’t that be “one busy elected official”?] attuned to rapid information delivery.” Further, “Timely climatic information provided when there is a maximum of interest [read: when it’s hot] may be the best way” to communicate about climate. 



Scientists suspected something was funny with the first federal pronouncement…that said that temperatures in 1997 were the warmest ever measured…Global satellite readings put 1997 smack in the middle of the range for the last two decades. 



This “history” was never peer reviewed. It was sent around via e‐​mail by NCDC, which explained, “Our methodology was not documented in the open refereed literature. This [memorandum] is an attempt to provide that documentation.” 



Sending an e‐​mail to everyone is not quite the same as peer review. 



Nor was this history a record of global temperature after all. Instead, according to NCDC, it is an “index” that combines three different measures, kind of like putting fruit salad in a blender. 



The three measures were land surface temperatures, which by definition are hardly global; sea surface temperatures taken from ships; and data from a network of buoys whose deployment was begun in the mid‐​1980s. The last two measurements are very different from the first, and in order to create the desired fruit salad, NCDC adjusted the sea surface temperature data up by 25 percent after 1982. That certainly might make things appear to be a bit warmer in recent years! 



In point of fact, the sea surface temperature data are increasingly at odds with air temperatures taken over the ocean. No one knows the reason for this, but the air temperatures just happen to match up perfectly with those recorded by NASA’s satellites, which happen to match up perfectly with the Weather Bureau’s (what it was called before it became a “service”) weather balloons. None of those records shows a lick of global warming in the last 20 years. 



Parenthetically, we might note that recent reports about the satellite data being in error are themselves in error. Annual temperature averages taken by weather balloons look exactly like those measured by the satellites. So the satellite cannot be wrong unless, somehow, thermometers in the 1,125,000 weather balloons launched over the last 20 years have been making exactly the same mistakes in temperature measurement as the satellites. 



In order to reassure all the recipients of their e‐​mail that their new blended‐​index approach was a good idea, NCDC observed that the three records jammed together looked an awful lot like NASA scientist James Hansen’s global temperature history. “The match is very good,” they wrote. But the Hansen history does not remove the effect of urban warming, which is known to bias global temperatures by about 0.2 degree. No wonder it’s so hot. 



So NCDC had a choice: Either use sea surface temperature data that disagree with marine air temperatures and data from satellites and weather balloons, or use one of those three mutually agreeable records that all show no warming. Guess which choice they made for “busy elected officials”? 



This isn’t temperature measurement, it’s hot air.
"
"
Share this...FacebookTwitterAs an American citizen living in Germany, today is just another regular work day here, but of course I still celebrate Thanksgiving, and do so by having a lavish turkey dinner on Friday evening with friends and family. For the non-Americans who visit this site,  here’s a short version of how Thanksgiving started and became a tradition.
The Pilgrims escape oppression in Europe
Giving thanks and celebrating festivals for successful harvests had existed for centuries, way before the first American Thanksgiving. Giving thanks in America started when the first Pilgrims came to Massachusetts (Plymouth Rock) from England on the Mayflower in 1620. The Pilgrims came to the New World to escape persecution and oppression, particularly from the Church of England, kind of like how climate skeptics are oppressed by the Church of Climatology today.
The Pilgrims land at Plymouth Rock and many starve to death because of climate
The Mayflower with its 102 passengers had been originally bound for Jamestown, Virginia, but Atlantic storms blew the ship north to Massachusetts (Storms back then had natural causes, and were not man made ;). The first winter there was especially harsh, and because they had arrived too late they could not grow crops and they didn’t have fresh food. Half the Pilgrims on the Mayflower died of mal-nutrition and starvation during the first winter alone. But despite the extreme hardship, these newcomers had some luck, as it was the tradition of the local Wampanoag Indians, led by Chief Massasoit, to share food with any visitors.
The Indians teach the Pilgrims adaptation, and not useless mitigation
The following spring, in 1621, the Indians taught them how to grow corn (maize) and introduced cranberries, which were new foods for the new settlers. They also showed them how to grow other crops like beans, pumpkins and squash in the strange soil. The Indians taught the Pilgrims how to hunt and fish as well. Back then, the Indians taught the Pilgrims that it was useless to mitigate climate. Now just imagine if the Pilgrims had resorted to rain dancing and forbidden tree cutting. We can be thankful they were smarter then our political leaders of today.
The fruits of adaptation
After the first harvest had been completed by the new colonists in the autumn of 1621, the colonists had an abundance of food (the wonders of adaptation!). Governor William Bradford therefore proclaimed a day of thanksgiving and prayer. And to thank the Indians for teaching them how to survive in the New World, the Pilgrims of Plymouth Rock invited their Indian friends to their first Thanksgiving. It was a three day celebration to give thanks to God and the leaders of the Wampanoag Indians.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Thanksgiving spreads to other states and declared a national holiday 
After the first Plymouth Thanksgiving, the custom spread to the other colonies. But each region chose its own date. In 1789 George Washington, the first president of the United States, declared November 26 as a day of thanksgiving, but it still was not an official holiday. Thanksgiving Day continued to be celebrated in the United States on different days in different states – until Mrs. Sarah Josepha Hale, editor of Godey’s Lady’s Book, embarked on a campaign. For more than 30 years she wrote letters to the governors and presidents asking them to make Thanksgiving Day a national holiday.
Thanksgiving becomes a national holiday
In 1863, President Lincoln called on Americans to unite “with one heart and one voice” and to celebrate Thanksgiving Day on the last Thursday of November. In 1939 President Franklin D. Roosevelt moved Thanksgiving Day a week earlier to make the Christmas shopping season longer. However, because some states used the new date and others the old one, it was changed again just 2 years later. Now Thanksgiving Day is celebrated on the fourth Thursday in November.
Freedom of Want – by Norman Rockwell (1943)
Why turkey?
The turkey tradition was pushed by Benjamin Franklin, who even wanted to make it the United States national symbol. In the end the bald eagle was selected instead of the wild turkey as the official national symbol. I think most Americans will agree it was the right choice. Finally, the turkey was made famous by Norman Rockwell’s 1943 image of the family Thanksgiving, Freedom of Want, that appeared on the cover of the Saturday Evening Post. The turkey has been the Thanksgiving Day favorite ever since.
Dinner and family
The American tradition of Thanksgiving revolves around an extravagant meal, with turkey at the center.  Thanksgiving dinner also includes corn, cranberry, potatoes, gravy and a variety of pies for dessert – like pumpkin pie or apple pie. It’s tradition to say a special prayer of thanks before the meal. In many homes, family members each mention something they are very thankful for. Thanksgiving is a time for families to come together.
I have very fond memories of Thanksgivings in New England as a boy, especially of coming in out of the cold into a warm house heated by a wood-burning Franklin stove and savouring the aroma of an 18-pound turkey that’s been baking in the oven for five hours. Thanksgiving memories last a lifetime.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA Swiss report appearing at www.20min.ch claims that the extraction of groundwater for communities and agriculture is adding massively to global sea level rise.
According to a yet to be published paper by the America Geophysical Union, 2000 cubic km of freshwater are consumed yearly – with 1500 cubic km coming from lakes, ponds, rivers etc., and the remaining 500 cubic kilometers of freshwater being extracted from the ground. According to 20min.ch:
Scientists now have calculated how much of this extracted water returns from where it came: only 3%. The remaining 97% flows into the oceans via evaporation and precipitation.
Of course, the scientists say, this bodes extremely ill for groundwater levels and pose a serious threat to arid regions.
Adding to sea level rise
Moreover the AGU paper claims that the extraction of freshwater is contributing to an annual 0.8 mm rise in sea level, 0r more than 25% of the overall 3.1 mm/yr. It goes without saying that there are uncertainties involved here, as dams act to slow the entry of freshwater into the oceans. But 20min.ch writes:

Scientists are convinced that the relation between groundwater reduction and sea level rise will gain importance. Simply because global water consumption will continue to increase and for that reason groundwater reserves will continue to sink.

Uncertainty always allows alarmists to be convinced of anything they claim. Sceptic Edgar L Gärtner at eigentümlich freicomments in a piece called: Climate Lies: How geo-scientists are abandoning their expertise. Leaky faucets are causing the oceans to overflow, writes:
It can be certain that pumping out groundwater has no measureable impacts on sea levels. But perhaps US scientists are tying it to climate change in order to bring attention to the risks of exhausting groundwater reserves.
Drawing attention to potential groundwater depletion problems is legitimate. But claiming it is leading to sea level rise sounds absurd.
Share this...FacebookTwitter "
"

On St. Patrick’s Day, we wear green and celebrate the culture of Ireland. I’ll be down at the pub tomorrow, but I’ll be toasting Ireland’s success at attracting greenbacks — all that investment flowing into the Emerald Isle and the resulting prosperity.



Ireland has boomed in recent years, and it now boasts the fourth highest gross domestic product per capita in the world. In the mid‐​1980s, Ireland was a backwater with an average income level 30 percent below that of the European Union. Today, Irish incomes are 40 percent above the EU average. 



Was this dramatic change the luck of the Irish? Not at all. It resulted from a series of hard‐​headed decisions that shifted Ireland from big government stagnation to free market growth. After years of high inflation, double‐​digit unemployment rates, and soaring government debt that topped 100 percent of GDP, Irish policymakers began to cut spending in the late 1980s in a desperate bid to recover financial stability.



Irish government spending fell from more than 50 percent of GDP in the 1980s to 34 percent by 2005. For Europe that is a triumph of restraint, given that the average size of government across 25 EU countries today is 47 percent of GDP. 



And Ireland has steadily reduced its tax rates. The top individual income tax rate was cut from 65 percent in 1985 to 42 percent today. The capital gains tax rate was cut from 40 to 20 percent in 1999. 



However, the key to Ireland’s success has been its excellent tax climate for business. In 1980, Ireland established a corporate tax rate for manufacturing of just 10 percent. That low rate was subsequently extended to high‐​technology, financial services, and other industries. More recently, Ireland established a flat 12.5 percent tax rate on all corporations — one of the lowest rates in the world, and just one‐​third of the U.S. rate.



Low business tax rates have helped Ireland attract huge inflows of foreign investment. Given the country’s modest size, it boosts a high‐​tech industry second to none. Intel, Dell, and Microsoft are among the island’s biggest exporters. Ireland also hosts booming insurance, banking, money management, and pharmaceutical industries.



The Irish model of rock‐​bottom business taxation has been hugely influential. In recent years, corporate tax rates have been slashed across Europe. According to KPMG, the average rate in the EU has fallen from 38 percent in 1996 to 26 percent in 2006. 



Inspired by the Celtic Tiger, many Eastern European nations have gone one step further and installed both low corporate taxes and simple, flat‐​rate taxes on individuals. According to my colleague Dan Mitchell, there are now 13 nations in the “flat tax club,” including Estonia, Russia, and Slovakia. 



The average corporate and individual rates in the flat tax nations are 19 and 18 percent, respectively, and these countries are growing strongly. Ireland and some of the newer “tiger” economies are putting to rest the notion that luck, natural resources, or other uncontrolled factors are the source of growth.



It’s become fashionable to argue that increased government spending on education is the key to success for countries like Ireland. I’m skeptical. For one thing, booming economies today can attract high‐​skill workers from global labor markets. In Ireland, brain drain has been replaced by brain gain as smart people from across Europe are drawn into the country’s growing industries.



Economic growth is spurred by attracting entrepreneurs and investment capital. Countries do that by establishing the rule of law, stable money, open borders, and low taxes. Let’s call these the “rainbow” factors, since Irish legend says that there is a pot of gold at the end of the rainbow.



Consider Hong Kong, which was once a barren outpost with seemingly few natural advantages. It followed the rainbow and found a pot of gold in just a few short decades.



You may recall that the Irish leprechaun is a sneaky character who tries to hide the pot of gold. Those are the politicians who spend their time trying to undermine the free market. Leprechauns, such as Venezuela’s Hugo Chavez, may be buoyed for short periods by high oil prices or other unique factors. But natural resources are usually pots of fool’s gold because of the bad governance they encourage.



The good news is that with the competition spurred by globalization, the leprechauns are on the defensive As more countries follow the path of the trailbrazing Irish, the relationships between the rainbow factors and growth become ever more clear. 



Now if only we could chase the leprechauns out of this country and cut our corporate tax rate, we’d be enjoying Irish‐​level growth rates by next St. Paddy’s Day.
"
"
Set Phasers on Stun
March 29th, 2009 by Roy W.  Spencer, Ph. D.


I’ve been receiving a steady stream of e-mails asking when our latest work on  feedbacks in the climate system will be published. Since I’ve been trying to fit  the material from three (previously rejected) papers into one unified paper, it  has taken a bit longer than expected…but we are now very close to submission.
We’ve tentatively decided to submit to Journal of Geophysical Research (JGR)  rather than any of the American Meteorological Society (AMS) journals. This is  because it appears that JGR editors are somewhat less concerned about a paper’s  scientific conclusions supporting the policy goals of the IPCC — regulating  greenhouse gas emissions. Indeed, JGR’s instructions to reviewers is to not  reject a paper simply because the reviewer does not agree with the paper’s  scientific conclusions. More on that later.
As those who have been following our work already know, our main conclusion  is that climate sensitivity has been grossly overestimated due to a mix up  between cause and effect when researchers have observed how global cloud cover  varies with temperature.
To use my favorite example, when researchers have observed that global cloud  cover decreases with warming, they have assumed that the warming caused the  cloud cover to dissipate. This would be a positive feedback since such a  response by clouds would let more sunlight in and enhance the  warming.
But what they have ignored is the possibility that causation is actually  working in the opposite direction: That the decrease in cloud cover caused the  warming…not the other way around. And as shown by Spencer and  Braswell (2008 J. Climate), this can mask the true existence of negative  feedback.
All 20 of the IPCC climate models now have positive cloud feedbacks, which  amplify the small about of warming from extra carbon dioxide in the atmosphere.  But if cloud feedbacks in the climate system are negative, then the climate  system does not particularly care how much you drive your SUV. This is an issue  of obvious importance to global warming research. Even the IPCC has admitted  that cloud feedbacks remain the largest source of uncertainty in predicting  global warming.
Significantly, our new work provides a method for identifying which direction  of causation is occurring (forcing or feedback), and for obtaining a more  accurate estimate of feedback in the presence of clouds forcing a temperature  change. The method involves a new way of analyzing graphs of time filtered  satellite observations of the Earth (or even of climate model  output).
Well…at least I thought it was new way of analyzing graphs. It turns  out that we have simply rediscovered a method used in other physical sciences:  phase space analysis.  This methodology was first introduced by Willard Gibbs in 1901.
We found that by connecting successively plotted points in graphs of how the  global average temperature varies over time versus how global average radiative  balance varies over time, one sees two different kinds of structures emerge:  linear striations, which are the result of feedback, and spirals which are the  result of internal radiative forcing by clouds.
But such a methodology is not new. To quote from Wikipedia on the subject of  ‘phase space’:
“Often this succession of plotted points is analogous to the system’s  state evolving over time. In the end, the phase diagram…can easily elucidate  qualities of the system that might not be obvious  otherwise.”
Using a simple climate model we show that these two features that show up in  the graphs are a direct result of the two directions of causation: temperature  causing clouds to change (revealed by ‘feedback stripes’), and clouds causing  temperature to change (revealed by ‘radiative forcing spirals’).
The fact that others have found phase space analysis to be a useful  methodology is a good thing. It should lend some credibility to our  interpretation. Phase space analysis is what has helped us better understand  chaos, along with its Lorenz attractor, strange attractor, etc.
And the fact that we find the exact same structures in the output of  the IPCC climate models means that the modelers can not claim our interpretation  has no physical basis.
And now we can also use some additional buzzwords in the new article…which  seems to help from the standpoint of reviewers thinking you know what you are  talking about. The new paper title is, “Phase Space Analysis of Forcing and  Feedback in Models and Satellite Observations of Climate  Variability”.
It just rolls of the tongue, doesn’t it?
I am confident the work will get published…eventually. But even if it didn’t,  our original  published paper on the issue has laid the groundwork…it would just take  awhile before the research community understands the implications of that  work.
What amazes me is the resistance there has been to ‘thinking out of the box’  when trying to estimate the sensitivity of the climate system. Especially when  it has been considered to be ‘thinking in the box’ by other sciences for over a  century now.
And it is truly unfortunate that the AMS, home of Lorenz’s first  published work on chaos in 1963, has decided that political correctness is  more important than the advancement of science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e976ea2e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSweden’s English language The Local has the following headline today:

Coldest December in Sweden in 110 years
The last few days of the year look to be very cold throughout Sweden, according to a forecast by the Swedish meteorological agency SMHI. 
This means that several parts of Sweden, including the southern region Götaland and eastern Svealand, will have experienced the coldest December in at least 110 years.”
Read the complete article here.
This reality of course flies in the face of what climate models had predicted earlier. The SMHI (Sweden’s Met Office and devout warmist organisation) keeps archives, and so I thought surely there must be something there that had earlier forecast warmer winters for Sweden. I didn’t have to look very long to find it.
First there’s this report dated 16 September 2010 here: New climate projections indicate more extreme weather. Here are just a couple of excerpts (Warning – you might first want to tie your butt to yourself to keep from laughing it off!):
New climate projections for severe weather situations in 100 years also show that truly cold days will virtually disappear.”
And:
The new scenarios show the effects of global warming with more details than before, thanks to more computer power and high geographical resolution.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And:
‘As a whole, the new ensembles are an important foundation for continued climate research. However, they can already be applied to many areas,’ says Grigory Nikulin.”
Does he mean like governments preparing for winters? And finally:
Truly cold weather, such as -10°C in Spain or -30°C in southern Sweden, is unlikely to occur in future.”
How stupid must they feel now? The assertions made above likely stem in part from an SMHI-published report 2 years ago called: Temperature and precipitation changes in Sweden; a wide range of model-based projections for the 21st century.
The report analyzed the climate change signal for Sweden in scenarios for the 21st century in a large number of coupled atmosphere-ocean general circulation models (AOGCMs), used in the AR4 by the IPCC.
At the SMHI Rossby Centre, regional climate models were run under different emission scenarios and driven by a few AOGCMs. They used the results of the runs as a basis in climate change in Sweden. What did they find? (Crap, of course, but read it for yourself):
Projected responses depend on season and geographical region. Largest signals are seen in winter and in northern Sweden, where the mean simulated temperature increase among the AOGCMs (and across the emissions scenarios B1, A1B and A2) is nearly 6°C by the end of the century, and precipitation increases by around 25%. In southern Sweden, corresponding values are around +4°C and +11%.
Okay, it’s still a long way to the end of the 21st century. But as Sweden’s 2010 December-of-the-century shows, the models and calculations seem to have forgotten a few important details. Back to the drawing board!
Share this...FacebookTwitter "
"
This is from the Huffington Post. One can only hope that Kerry will follow through. For a quick primer on Kerry’s grasp of climate science, see this WUWT article: Kerry Blames Tornado Outbreak on Global Warming and a rebuttal Increasing tornadoes or better information gathering? I get a kick out of Kerry’s line “This has to stop”. Okay then, please debate Mr. Will, put a stop to it Mr. Kerry! –  Anthony
John Kerry
U.S. Senator from Massachusetts
Posted February 27, 2009 									| 04:47 PM (EST)
Facts Are Stubborn Things: George Will and Climate Change-
To paraphrase the conservative columnist’s favorite president, “There you go again, George.”
George Will has been one of my favorite intellectual sparring partners for a long time, a favorite more recently because he had the guts to publicly recognize the disaster that was George W. Bush’s presidency.
But in his latest Washington Post column, George and I have a pretty big loud disagreement.
Don’t get me wrong. I’m happy to see Will embracing the idea of recycling, but I’m very troubled that he is recycling errors of fact to challenge the science on global warming.
I’m even more troubled that Will used his February 15th column not only to cast doubt on sound science, but also to denigrate the work of two fine scientists.
Let’s be very clear: Stephen Chu does not make predictions to further an agenda. He does so to inform the public. He is no Cassandra. If his predictions about the effects of our climate crisis are scary, it’s because our climate is scary.
Likewise, John Holdren is a friend of mine and one of the best scientific minds we have in our country. Pulling out one minor prediction that he had some unknown role in formulating nearly three decades ago, as Will did in his February 15th column, and then using that to try to undo his credibility as a scientist may be a fancy debating trick, but it’s just plain wrong when it comes to a debate we can’t afford to see dissolve into reductio ad absurdum hijinx. (A side note: The incident in question occurred in 1980, which, as I recall, was just about the time Ronald Reagan made the claim that approximately 80 percent of our air pollution stems from hydrocarbons released by vegetation and that, consequently, we should “not go overboard in setting and enforcing tough emissions standards from man-made sources.”)
Dragging up long-discredited myths about some non-existent scientific consensus about global cooling from the 1970s does no one any good. Except perhaps a bankrupt flat earth crowd. I hate to review the record and see that someone as smart as George Will has been doing exactly that as far back as 1992. And it’s especially troubling when the very sources that Will cites in his February 15th column draw the exact opposite conclusions and paint very different pictures than Will provides, as the good folks at ThinkProgress and Media Matters for America have demonstrated so thoroughly.
This has to stop. A highly organized, well-funded movement to deny the reality of global climate change has been up and running for a long time, but it doesn’t change the verdict: the problem is real, it’s accelerating, and we have to act. Now. Not years from now.
No matter how the evidence has mounted over two decades — the melting of the arctic ice cap, rising sea levels, extreme weather — the flat earth caucus can’t even see what is on the horizon. In the old Republican Congress they even trotted out the author of Jurassic Park as an expert witness to argue that climate change is fiction. This is Stone Age science, and now that we have the White House and the Congress real science must prevail. It is time to stop debating fiction writers, oil executives and flat-earth politicians, and actually find the way forward on climate change.
This is a fight we can win, a problem we can overcome, but time is not on our side. We can’t waste another second arguing about whether the problem exists when we need to be debating everything from how to deal with the dirtiest forms of coal as the major provider of power in China to how to vastly increase green energy right here at home.
“Facts are stupid things,” Ronald Reagan once said. He was, of course, paraphrasing John Adams, who could have been talking about the science on global change when he said, “Facts are stubborn things.”
Stubborn or stupid — lets have a real debate and lets have it now.
I know George Will well, I respect his intellect and his powers of persuasion — but I’d happily debate him any day on this question so critical to our survival.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97b846cc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Smearing around data or paint - the results are similar
Jeff Id of The Air Vent emailed me today inviting me to repost Ryan O’s latest work on statistical evaluation of the Steig et al “Antarctica is warming” paper ( Nature, Jan 22, 2009) I thought long and hard about the title, especially after reviewing the previous work from Ryan O we posted on WUWT where the paper was dealt a serious blow to “robustness”. After reading this latest statistical analysis, I think it is fair to conclude that the paper’s premise has been falsified.
Ryan O, in his conclusion, is a bit more gracious:
I am perfectly comfortable saying that Steig’s reconstruction is not a faithful representation of Antarctic temperatures over the past 50 years and that ours is closer to the mark.
Not only that, Ryan O did a more complete job of the reconstruction than Steig et al did, he mentions this in comments at The Air Vent:
Steig only used 42 stations to perform his reconstruction.  I used 98, since I included AWS stations.
The AWS stations have their problems, such as periods of warmer temperatures due to being buried in snow, but even when using this data, Ryan O’s analysis still comes out with less warming than the original Steig et al paper
Antarctica as a whole is not warming, the Antarctic peninsula is, which is signficantly removed climatically from the main continent.
Click for a larger image
It is my view that all Steig and Michael Mann have done with their application of RegEm to the station data is to smear the temperature around much like an artist would smear red and white paint on a pallete board to get a new color “pink” and then paint the entire continent with it.
It is a lot like “spin art” you see at the county fair. For example, look (at left) at the different tiles of colored temperature results for Antarctica you can get using Steig’s and Mann’s methodology. The only thing that changes are the starting parameters, the data remains the same, while the RegEm program smears it around based on those starting parameters. In the Steig et al case, PC and regpar were chosen by the authors to be a value of 3. Chosing any different numbers yields an entirely different result.
So the premise of the Steig et al paper paper boils down to an arbitrary choice of values that “looked good”.
I hope that Ryan O will write a rebuttal letter to Nature, and/or publish a paper. It is the only way the Team will back down on this. – Anthony
UPDATE: To further clarify, Ryan O writes in comments:
“Overall, Antarctica has warmed from 1957-2006. There is no debating that point. (However, other than the Peninsula, the warming is not statistically significant. ) 
The important difference is the location of the warming and the magnitude of the warming. Steig’s paper has the warming concentrated on the Ross Ice Shelf – which would lead you to entirely different conclusions than having a minimum on the ice shelf. As far as magnitude goes, the warming for the continent is half of what was reported by Steig (0.12 vs. 0.06 Deg C/Decade).
Additionally, Steig shows whole-continent warming from 1967-2006; this analysis shows that most of the continent has cooled from 1967-2006. Given that the 1940’s were significantly warmer in the Antarctic than 1957 (the 1957-1960 period was unusually cold in the Antarctic), focusing on 1957 can give a somewhat slanted picture of the temperature trends in the continent.”
Ryan O  adds later:  “I should have said that all reconstructions yield a positive trend, though in most cases the trend for the continent is not statistically significant.” 

Verification of the Improved High PC Reconstruction
Posted by Jeff Id on May 28, 2009
There is always something going on around here.
Up until now all the work which has been done on the antarctic reconstruction has been done without statistical verification. We believed that they are better from correlation vs distance plots, the visual comparison to station trends and of course the better approximation of simple area weighted reconstructions using surface station data.
The authors of Steig et al. have not been queried by myself or anyone else that I’m aware of regarding the quality of the higher PC reconstructions. And the team has largely ignored what has been going on over on the Air Vent. This post however demonstrates strongly improved verification statistics which should send chills down their collective backs. 
Ryan was generous in giving credit to others with his wording, he has put together this amazing piece of work himself using bits of code and knowledge gained from the numerous other posts by himself and others on the subject. He’s done a top notch job again, through a Herculean effort in code and debugging.
If you didn’t read Ryan’s other post which led to this work the link is:
Antarctic Coup de Grace
——————————————————————————–

Fig. 1: 1957-2006 trends; our reconstruction (left); Steig reconstruction (right)


HOW DO WE CHOOSE?


In order to choose which version of Antarctica is more likely to represent the real 50-year history, we need to calculate statistics with which to compare the reconstructions. For this post, we will examine r, r^2, R^2, RE, and CE for various conditions, including an analysis of the accuracy of the RegEM imputation. While Steig’s paper did provide verification statistics against the satellite data, the only verification statistics that related to ground data were provided by the restricted 15-predictor reconstruction, where the withheld ground stations were the verification target. We will perform a more comprehensive analysis of performance with respect to both RegEM and the ground data. Additionally, we will compare how our reconstruction performs against Steig’s reconstruction using the same methods used by Steig in his paper, along with a few more comprehensive tests.
To calculate what I would consider a healthy battery of verification statistics, we need to perform several reconstructions. The reason for this is to evaluate how well the method reproduces known data. Unless we know how well we can reproduce things we know, we cannot determine how likely the method is to estimate things we do not know. This requires that we perform a set of reconstructions by withholding certain information. The reconstructions we will perform are:
1. A 13-PC reconstruction using all manned and AWS stations, with ocean stations and Adelaide excluded. This is the main reconstruction.
2. An early calibration reconstruction using AVHRR data from 1982-1994.5. This will allow us to assess how well the method reproduces the withheld AVHRR data.
3. A late calibration reconstruction using AVHRR data from 1994.5-2006. Coupled with the early calibration, this provides comprehensive coverage of the entire satellite period.
4. A 13-PC reconstruction with the AWS stations withheld. The purpose of this reconstruction is to use the AWS stations as a verification target (i.e., see how well the reconstruction estimates the AWS data, and then compare the estimation against the real AWS data).
5. The same set of four reconstructions as above, but using 21 PCs in order to assess the stability of the reconstruction to included PCs.
6. A 3-PC reconstruction using Steig’s station complement to demonstrate replication of his process.
7. A 3-PC reconstruction using the 13-PC reconstruction model frame as input to demonstrate the inability of Steig’s process to properly resolve the geographical locations of the trends and trend magnitudes.
–
Using the above set of reconstructions, we will then calculate the following sets of verification statistics:
–
1. Performance vs. the AVHRR data (early and late calibration reconstructions)
2. Performance vs. the AVHRR data (full reconstruction model frame)
3. Comparison of the spliced and model reconstruction vs. the actual ground station data.
4. Comparison of the restricted (AWS data withheld) reconstruction vs. the actual AWS data.
5. Comparison of the RegEM imputation model frame for the ground stations vs. the actual ground station data.
–
The provided script performs all of the required reconstructions and makes all of the required verification calculations. I will not present them all here (because there are a lot of them). I will present the ones that I feel are the most telling and important. In fact, I have not yet plotted all the different results myself. So for those of you with R, there are plenty of things to plot.
Without further ado, let’s take a look at a few of those things.
Fig. 2: Split reconstruction verification for Steig reconstruction
You may remember the figure above; it represents the split reconstruction verification statistics for Steig’s reconstruction. Note the significant regions of negative CE values (which indicate that a simple average of observed temperatures explains more variance than the reconstruction). Of particular note, the region where Steig reports the highest trend – West Antarctica and the Ross Ice Shelf – shows the worst performance.
Let’s compare to our reconstruction:
Fig. 3: Split reconstruction verification for 13-PC reconstruction

There still are a few areas of negative RE (too small to see in this panel) and some areas of negative CE. However, unlike the Steig reconstruction, ours performs well in most of West Antarctica, the Peninsula, and the Ross Ice Shelf. All values are significantly higher than the Steig reconstruction, and we show much smaller regions with negative values.
As an aside, the r^2 plots are not corrected by the Monte Carlo analysis yet. However, as shown in the previous post concerning Steig’s verification statistics, the maximum r^2 values using AR(8) noise were only 0.019, which produces an indistinguishable change from Fig. 3.
Now that we know that our method provides a more faithful reproduction of the satellite data, it is time to see how faithfully our method reproduces the ground data. A simple way to compare ours against Steig’s is to look at scatterplots of reconstructed anomalies vs. ground station anomalies:
Your browser may not support display of this image.
Fig. 4: 13-PC scatterplot (left); Steig reconstruction (right)

The 13-PC reconstruction shows significantly improved performance in predicting ground temperatures as compared to the Steig reconstruction. This improved performance is also reflected in plots of correlation coefficient:
Fig. 5: Correlation coefficient by geographical location
As noted earlier, the performance in the Peninsula , West Antarctica, and the Ross Ice Shelf are noticeably better for our reconstruction. Examining the plots this way provides a good indication of the geographical performance of the two reconstructions. Another way to look at this – one that allows a bit more precision – is to plot the results as bar plots, sorted by location:
Fig. 6: Correlation coefficients for the 13-PC reconstruction

Fig. 7: Correlation coefficients for the Steig reconstruction

The difference is quite striking.
While a good performance with respect to correlation is nice, this alone does not mean we have a “good” reconstruction. One common problem is over-fitting during the calibration period (where the calibration period is defined as the periods over which actual data is present). This leads to fantastic verification statistics during calibration, but results in poor performance outside of that period.
This is the purpose of the restricted reconstruction, where we withhold all AWS data. We then compare the reconstruction values against the actual AWS data. If our method resulted in overfitting (or is simply a poor method), our verification performance will be correspondingly poor.
Since Steig did not use AWS stations for performing his TIR reconstruction, this allows us to do an apples-to-apples comparison between the two methods. We can use the AWS stations as a verification target for both reconstructions. We can then compare which reconstruction results in better performance from the standpoint of being able to predict the actual AWS data. This is nice because it prevents us from later being accused of holding the reconstructions to different standards.
Note that since all of the AWS data was withheld, RE is undefined. RE uses the calibration period mean, and there is no calibration period for the AWS stations because we did the reconstruction without including any AWS data. We could run a split test like we did with the satellite data, but that would require additional calculations and is an easier test to pass regardless. Besides, the reason we have to run a split test with the satellite data is that we cannot withhold all of the satellite data and still be able to do the reconstruction. With the AWS stations, however, we are not subject to the same restriction.
Fig. 8: Correlation coefficient, verification period, AWS stations withheld

With that, I think we can safely put to bed the possibility that our calibration performance was due to overfitting. The verification performance is quite good, with the exception of one station in West Antarctica (Siple). Some of you may be curious about Siple, so I decided to plot both the original data and the reconstructed data. The problem with Siple is clearly the short record length and strange temperature swings (in excess of 10 degrees), which may indicate problems with the measurements:
Fig. 9: Siple station data

While we should still be curious about Siple, we also would not be unjustified in considering it an outlier given the performance of our reconstruction at the remainder of the station locations.
Leaving Siple for the moment, let’s take a look at how Steig’s reconstruction performs.
Fig. 10: Correlation coefficient, verification period, AWS stations withheld, Steig reconstruction
Not too bad – but not as good as ours. Curiously, Siple does not look like an outlier in Steig’s reconstruction. In its place, however, seems to be the entire Peninsula. Overall, the correlation coefficients for the Steig reconstruction are poorer than ours. This allows us to conclude that our reconstruction more accurately calculated the temperature in the locations where we withheld real data.
Along with correlation coefficient, the other statistic we need to look at is CE. Of the three statistics used by Steig – r, RE, and CE – CE is the most difficult statistic to pass. This is another reason why we are not concerned about lack of RE in this case: RE is an easier test to pass.
Fig. 11: CE, verification period, AWS stations withheld
Your browser may not support display of this image.
Fig. 12: CE, verification period, AWS stations withheld, Steig reconstruction
The difference in performance between the two reconstructions is more apparent in the CE statistic. Steig’s reconstruction demonstrates negligible skill in the Peninsula, while our skill in the Peninsula is much higher. With the exception of Siple, our West Antarctic stations perform comparably. For the rest of the continent, our CE statistics are significantly higher than Steig’s – and we have no negative CE values.
So in a test of which method best reproduces withheld ground station data, our reconstruction shows significantly more skill than Steig’s.
The final set of statistics we will look at is the performance of RegEM. This is important because it will show us how faithful RegEM was to the original data. Steig did not perform any verification similar to this because PTTLS does not return the model frame. Unlike PTTLS, however, our version of RegEM (IPCA) does return the model frame. Since the model frame is accessible, it is incumbent upon us to look at it.
Note:    In order to have a comparison, we will run a Steig-type reconstruction using RegEM IPCA.
There are two key statistics for this: r and R^2. R^2 is called “average explained variance”. It is a similar statistic to RE and CE with the difference being that the original data comes from the calibration period instead of the verification period. In the case of RegEM, all of the original data is technically “calibration period”, which is why we do not calculate RE and CE. Those are verification period statistics.
Let’s look at how RegEM IPCA performed for our reconstruction vs. Steig’s.
Fig. 13: Correlation coefficient between RegEM model frame and actual ground data

As you can see, RegEM performed quite faithfully with respect to the original data. This is a double-edged sword; if RegEM performs too faithfully, you end up with overfitting problems. However, we already checked for overfitting using our restricted reconstruction (with the AWS stations as the verification target).
While we had used regpar settings of 9 (main reconstruction) and 6 (restricted reconstruction), Steig only used a regpar setting of 3. This leads us to question whether that setting was sufficient for RegEM to be able to faithfully represent the original data. The only way to tell is to look, and the next frame shows us that Steig’s performance was significantly less than ours.
Fig. 14: Correlation coefficient between RegEM model frame and actual ground data, Steig reconstruction
The performance using a regpar setting of 3 is noticeably worse, especially in East Antarctica. This would indicate that a setting of 3 does not provide enough degrees of freedom for the imputation to accurately represent the existing data. And if the imputation cannot accurately represent the existing data, then its representation of missing data is correspondingly suspect.
Another point I would like to note is the heavy weighting of Peninsula and open-ocean stations. Steig’s reconstruction relied on a total of 5 stations in West Antarctica, 4 of which are located on the eastern and southern edges of the continent at the Ross Ice Shelf. The resolution of West Antarctic trends based on the ground stations alone is rather poor.
Now that we’ve looked at correlation coefficients, let’s look at a more stringent statistic: average explained variance, or R^2.
Fig. 15: R^2 between RegEM model frame and actual ground data
Using a regpar setting of 9 also provides good R^2 statistics. The Peninsula is still a bit wanting. I checked the R^2 for the 21-PC reconstruction and the numbers were nearly identical. Without increasing the regpar setting and running the risk of overfitting, this seems to be about the limit of the imputation accuracy.
Fig. 16: R^2 between RegEM model frame and actual ground data, Steig reconstruction

Steig’s reconstruction, on the other hand, shows some fairly low values for R^2. The Peninsula is an odd mix of high and low values, West Antarctica and Ross are middling, while East Antarctica is poor overall. This fits with the qualitative observation that the Steig method seemed to spread the Peninsula warming all over the continent, including into East Antarctica – which by most other accounts is cooling slightly, not warming.
CONCLUSION
With the exception of the RegEM verification, all of the verification statistics listed above were performed exactly (split reconstruction) or analogously (restricted 15 predictor reconstruction) by Steig in the Nature paper. In all cases, our reconstruction shows significantly more skill than the Steig reconstruction. So if these are the metrics by which we are to judge this type of reconstruction, ours is objectively superior.
As before, I would qualify this by saying that not all of the errors and uncertainties have been quantified yet, so I’m not comfortable putting a ton of stock into any of these reconstructions. However, I am perfectly comfortable saying that Steig’s reconstruction is not a faithful representation of Antarctic temperatures over the past 50 years and that ours is closer to the mark.
NOTE ON THE SCRIPT
If you want to duplicate all of the figures above, I would recommend letting the entire script run. Be patient; it takes about 20 minutes. While this may seem long, remember that it is performing 11 different reconstructions and calculating a metric butt-ton of verification statistics.
There is a plotting section at the end that has examples of all of the above plots (to make it easier for you to understand how the custom plotting functions work) and it also contains indices and explanations for the reconstructions, variables, and statistics. As always, though, if you have any questions or find a feature that doesn’t work, let me know and I’ll do my best to help.
Lastly, once you get comfortable with the script, you can probably avoid running all the reconstructions. They take up a lot of memory, and if you let all of them run, you’ll have enough room for maybe 2 or 3 more before R refuses to comply. So if you want to play around with the different RegEM variants, numbers of included PCs, and regpar settings, I would recommend getting comfortable with the script and then loading up just the functions. That will give you plenty of memory for 15 or so reconstructions.
As a bonus, I included the reconstruction that takes the output of our reconstruction, uses it for input to the Steig method, and spits out this result:
Fig. 17: Steig reconstruction using the 13-PC reconstruction as input.

The name for the list containing all the information and trends is “r.3.test”.
—————————————————————-
Code is here Recon.R


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95dca824',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest Post by Steven Goddard

The Third Little Show
Mad dogs and Englishmen go out in the midday sun The Japanese don’t care to, the Chinese wouldn’t dare to Hindus and Argentines sleep firmly from twelve till one But Englishmen detest-a siesta
– Noel Coward – 1931
Persistence is the British trait which kept the Shackleton crew alive and helped England withstand the Nazi’s throughout World War II.  It keeps the Catlin Crew going and kept Lewis Pugh relentlessly paddling his kayak over Arctic Ice towards the pole.   And it is the same trait which keeps the UK Met Office forecasting warm summers year after year.  The Met Office forecast 2007 to be the warmest year ever globally, and a hot summer in the UK. 
Instead it turned out to be a cool summer and the rainiest on record in England.  Similar story for summer 2008 and winter 2008-2009 .  Yet in fine British tradition the Met Office remains undaunted –
The coming summer is ‘odds on for a barbecue summer’, according to long-range forecasts. Summer temperatures across the UK are likely to be warmer than average and rainfall near or below average for the three months of summer.
Chief Meteorologist at the Met Office, Ewen McCallum, said: “After two disappointingly-wet summers, the signs are much more promising this year. We can expect times when temperatures will be above 30 °C, something we hardly saw at all last year.”
The last 30C day in London was July 26, 2006 – that was over 1,000 days ago.  But you have to admire their grit and determination to get the global warming message across to the ignorant British population.
“The definition of insanity is doing the same thing over and over and expecting different results.”
– Attributed to Albert Einstein

Darts anyone?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9664a685',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Media Contact: (202) 789‑5200





While the DC Court of Appeals has just ruled in favor of the Obama Administration in rejecting challenges to the Environmental Protection Agency’s rules concerning carbon dioxide emissions from cars and light trucks (the so‐​called “tailpipe emissions standards”), Senior Fellow Patrick J. Michaels believes the larger battle is still to come:



“On June 25, the public comment period for the EPA’s proposed regulations on coal‐​fired power plants ended,” said Michaels. “After thorough review, I found that the report from the U.S Global Change Research Program (USGCRP), which served as the source for the scientific opinions underlying the original endangerment finding in 2010, is unrepresentative of the larger body of scientific research on the topic of anthropogenic climate change and its potential impacts on the United States.”



Michaels, working with a team of experts and scientists, assembled an addendum to the USGCRP report, which they submitted as comments on June 22.



“Our review represents the most comprehensive scientific critique of the EPA Endangerment Finding on coal‐​fired plants ever written, and directly counters their claims on how climate change impacts in the United States, using a much more exhaustive survey of peer‐​reviewed science than the EPA relied upon,” said Michaels. 



Michaels also cautions against relying too much on static reports in rulemaking on climate change.



“No static report can provide long‐​term guidance as to the nature of climate change and its impacts, as this field is constantly evolving under the weight of new scientific findings. Consequently, it is imperative that the EPA reassess the current scientific understanding on at least an annual basis,” said Michaels.



The EPA is expected to finalize regulations regarding emissions from coal‐​fired plants later this year.



###
"
"

Media Contact: (202) 789‑5200



The Cato Institute announced today the expansion of its Center for the Study of Science. Founded in 2012, the Center for the Study of Science was created to provide market‐​based ideas that could transition policy regarding energy consumption, environmental standards, and other science‐​related issues away from government planners.



Today, the Center is adding scholars to a team that will continue to use rigorous science to answer questions related to environmental regulation. The Center will be especially focused on the debate over climate change.



Patrick J. Michaels, who will continue to direct the center, acknowledges climate change is occurring partly due to human actions. He does not believe, however, that these temperature fluctuations are cause for great alarm.



“Yes — burning fossil fuels to get the energy we need to advance as a global society does create carbon dioxide that recycles warming in the lower atmosphere,” said Michaels. “But despite what some scientists and politicians tell you, life as we know it will not end next week, next month or even in the next 500 years due to a warming planet. Policy makers need to know that there is a respectable group of scientists out there who don’t buy in to the alarmist hype.”



President Obama is pursuing an international agreement on carbon emissions that sidesteps Congressional ratification, an issue he is expected to discuss at a United Nations climate summit in New York next week. Michaels warns the President is “playing fast and loose with the Constitution.”



“We believe that some highly qualified scientists should be taking a more clear‐​eyed look at the data policy makers are using to draw conclusions which have resulted in a regulatory structure that inhibits economic activity and stifles innovation,” said Michaels.



Ross McKitrick, who teaches environmental economics at the University of Guelph, and Terence Kealey, Vice Chancellor of the University of Buckingham and a professor of clinical biochemistry, have been named adjunct scholars at the Center. They join Distinguished Senior Fellow Richard Lindzen, an emeritus professor of meteorology at both MIT and Harvard; Adjunct Scholar Edward J. Calabrese, a professor of environmental health sciences at the University of Massachusetts, specializing in toxicology; and Paul C. “Chip” Knappenberger, assistant director of the Center.



The Center for the Study of Science will seek to provide a credible source for media and members of the public who want a fresh perspective on scientific claims made by government and other research organizations. Research areas will include energy use and taxation; use of government subsidies; global warming; and overall environmental regulation.



“The truth is, counter to what President Obama claimed in 2010, the science on climate change is not settled,” said Cato President and CEO, John Allison. “The time is now to build a critical mass of credible scholars who can engage in the type of debate the public needs to hear in order to make informed decisions.”



Michaels said additional scholars and scientists will be named to the Center for the Study of Science in the coming months.
"
"
Share this...FacebookTwitterWe want to be fair, and so I’m obligated to inform readers that Professor Stefan Rahmstorf, has testily replied to EIKE’s report, which I wrote about just 2 days ago. Rahmstorf retorts at his website: Headlines From Absurdistan here.
First, recall that Rahmstorf is that alarmist scientist who projects an oddball sea level rise of 1.4 meters over the next 90 years, something that almost all scientists dismiss as nonsense (Rahmstorf may not be aware that sea level rise has indeed been slowing down; a few years ago the rate was 3.4 mm per year). Read more here.
Sea level rise is slowing. TOPEX U. of Colorado
Rahmstorf appears to be miffed that EIKE has labelled him as a lowly sceptic. Let us look at what he has written in reply.
1. First off, we’re all bought off. In his retort he first whines about all the money that big business is pouring into the sceptic machinery and to US denier politicians. (I haven’t seen a cent of it). Rahmstorf writes:
Also European companies don’t pinch their pennies when it comes to buying up candidates for the US Senate who deny anthropogenic climate change.
2. Next he points out that the focus of his 2003 paper was an analysis of  ice core data from Greenland with respect to the timing of climate changes during the ice age only (Dansgaard-Oeschger events).
3. He then claims that EIKE is confusing local with global temperature fluctuations. Rahmstorf writes:
The Greenland data are mainly characterised by fluctuations in the Atlantic currents (the Dansgaard-Oeschger events), which practically have no impact on the global temperature.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




4. In the Holocene in Greenland, like the northern latitudes overall, there was a cooling trend (it was warmer earlier), which was caused by regional solar radiation due to orbital cycles – not a global phenomena.
5. He counters the sceptic argument that past warmings show that today’s warming is natural using the following analogy:
It is as logical as saying that there cannot be forest arson today because there were completely natural forest fires in the past, too.
Of course there were warmer climates in the past than today (but as far as we know, not in the last 2 million years).
Today is the hottest it’s been in the last 2 million years! That’s quite the claim. Rahmstorf then poses the question:
Doesn’t the internet report [by EIKE] simply confirm the wisdom that the deniers of anthropogenic climate change no longer have any arguments – and so they have to resort to the most absurd possible twisting of the facts?
Finally, Rahmstorf takes one last shot at EIKE:
The source of all this nonsense, by the way, is the EIKE “climate sceptic” lobby group with the cute name of “European Institute for Climate and Energy”. For more information read Süddeutsche or the Spiegel.
Needless to say, Rahmstorf cites two lefty sources that don’t exactly say the nicest things about EIKE, calling it a hack organisation that’s headquartered in a mailbox. Funny though how Rahmstorf takes EIKE so seriously that he feels compelled to reply to the “mailbox operation” in less than 72 hours. Must be quite the mailbox.
Share this...FacebookTwitter "
"

“No matter how much you pay with a carbon levy, virtually nothing is received climatically… No matter the level of domestic action that we take, it will pale in comparison to the rapid expansion of carbon dioxide emissions in other parts of the world.”



How much global warming will result from U.S. emissions over the course of this century, and how much of that could be prevented by a carbon tax? These two questions have the same simple answer— _virtually none_. One or two tenths of a degree a century out with–and without–a carbon tax makes the whole climate debate a peculiar exercise.



The Intergovernmental Panel on Climate Change (IPCC) estimates that the earth’s average temperature will increase somewhere between 1.1°C and 6.4°C over the 21st century, depending on the assumed pathway of anthropogenic emissions (both greenhouse gases and aerosols) and the actual (but unknown) climate sensitivity.



A temperature rise towards the low end of this range is not worth worrying too much about (the ‘lukewarming’ position), while a rise near the higher end of the range is potentially much more problematic (the alarmist position). And while lukewarmers and alarmists stray apart when it comes to the amount of climate change they are expecting, _they are bound together by the fact that there is practically nothing that can be done to change the situation, either way_. Why? They use the same math.



But you won’t hear many alarmists admitting to that fact—if they did, you would never have heard of the terms like “cap‐​and‐​trade” or “carbon tax.” Instead, you’d be much more familiar with words like “planning” and “adaptation.”



 **How Much U.S.-Side Global Warming?**



Lest alarmists protest, let’s work through the numbers to see just how much “global warming” is being caused by U.S. economic activity.



In other words, how much of the IPCC’s projected 1.1°C to 6.4°C of warming will the U.S. be responsible for in the next century? The answer is about 0.08°C of the low end estimate and about 0.35°C of the high end estimate (according to an IPCC‐​like analysis*). Using the IPCC’s mid‐​range scenario, carbon dioxide emissions from the U.S. contribute about 0.19°C of the total 2.96°C global temperature rise.



Yep, that is it. For all the incessant talk as to how the highly consumptive U.S. lifestyle—from SUVs, to air conditioners, to big screen TVs and huge portion sizes—is leading climate catastrophe, the sum total of our contribution to “global warming” this century will amount to the neighborhood of _about 0.2°C_. Not five degrees. Not two degrees. But about _two‐​tenths of a degree Celsius_. And even this number may be on the high side if the climate sensitivity is lower than about 3°C (see here for more on recent findings concerning the climate sensitivity).



So all the U.S. carbon dioxide emissions restriction tactics—EPA regulations, cap and trade schemes, carbon taxes, efficiency programs, guilt‐​inducing ad campaigns, etc.—are aimed at chipping away at this already tiny 0.2°C. Big deal.



When considering any of these options, you have to ask yourself (or your representatives in Congress) how much are you willing to pay—in dollars or inconvenience, or both—to avert some portion of this 0.2°C of global temperature increase and its accompanying inconsequential and impossible to measure climate change?



 **Avertable Climate Change**



No matter how much you pay with a carbon levy, virtually nothing is received climatically.



Consider the effect of the Waxman‐​Markey Climate Bill that was passed by the U.S. House of Representatives back in the summer of 2009. That cap‐​and‐​trade scheme was designed to step down U.S. carbon dioxide emissions ultimately by 83% by the year 2050. This would have taken a monumental effort that was sure to be disruptive in any number of ways.



The net climate result? Instead of 0.19°C of warming coming from the U.S. by the year 2100 (assuming the IPCC mid‐​range scenario), our contribution would have been reduced to 0.08°C—for a net “savings” of about 0.11°C of “global warming”. (See my analysis here.) This amount is of virtually no environmental consequence and was repeatedly cited as one of the reasons that this legislation died in the Senate.



Much the same holds true for the present day fad for a carbon tax. The talk of a carbon tax—or more rightly a carbon _dioxide_ tax—was bolstered recently by superstorm Sandy and its aftermath (widely, but wrongly, blamed on anthropogenic climate change). A tax on carbon dioxide emissions would be felt from the gas station to the grocery store and everywhere in between as virtually every aspect of our modern life benefits from cheap carbon dioxide emitting, fossil‐​fuel produced energy.



A carbon tax has become so trendy that even “no new tax pledge” champion Grover Norquist briefly flirted with it before quickly reconsidering.



And for good reason. For about the only thing that a carbon (dioxide) tax in the U.S. will _not_ do, is produce a detectable mitigation of anthropogenic global warming and any associated effects.



The U.S. Energy Information Agency recently projected the impacts on carbon dioxide emissions in the U.S. out to the year 2035 resulting from a carbon dioxide tax of $15/​per ton emitted (beginning in 2013 and increasing by 5% per year out to 2035) and for a tax of $25/​ton of CO2 (beginning in 2013 and increasing by 5% per year out to 2035). The EIA projections are shown in Figure 1. I have continued the same emissions reductions pathway out from 2035 until the year 2100—admittedly, this is a shot in the dark, but at least is gives us something to work with.









Figure 1. Energy Information Agency estimates for the future course of U.S. carbon dioxide emissions resulting from a $15/​ton tax on carbon dioxide emissions (solid blue line) and a $25/​ton tax on carbon dioxide emissions (solid red line), 2010–2035. I have extended these projections to the year 2100 (dotted lines).



When I substitute these carbon tax pathways for U.S. carbon dioxide emissions for the one already included in the IPCC mid‐​range scenario, I calculate that the amount of “global warming” contributed by the U.S. drops from 0.19°C by the year 2100 to 0.13°C and 0.08°C for the $15/​ton and $25/​ton carbon tax respectively (Figure 2).









Figure 2. Amount of total global warming (red bars) and the U.S. contribution to the total global warming (blue bars) over the 21stcentury under three different scenarios. BAU= business‐​as‐​usual as portrayed by the IPCC A1B mid‐​range emissions scenario; $15/​ton CO2=$15/ton tax on U.S. carbon dioxide emissions as prescribed by the EIA to the year 2035 and extended to 2100; $15/​ton CO2=$15/ton tax on U.S. carbon dioxide emissions as prescribed by the EIA to the year 2035 and extended to 2100.



A global warming “savings” of 0.06°C to 0.11°C across this century is of no scientific consequence, while a tax of carbon dioxide emissions of $15 to $25 per ton is sure to be of significant personal consequence (and, my guess, only very temporary, i.e., until the next election cycle).



 **Conclusion**



Any tax on carbon dioxide is clearly a case of not getting what you pay for. You will pay a lot, and receive nothing in return, or at least nothing that you will ever realize, or that could be proven.



What is working against any form of a carbon tax is that the U.S. plays only a minor role in the future course of global warming driven by anthropogenic activities. The rest of the world—primarily the developing countries like China and India—is where the rubber meets the road for climate change. No matter the level of domestic action that we take, it will pale in comparison to the rapid expansion of carbon dioxide emissions in other parts of the world.



Instead of trying to make an expensive repair a very small and inconsequential leak, I would think that our attention ought to be directed at determining just how big the coming flood may be, and make our plans accordingly.



 **Appendix: Methodological Note**



I have used the Model for the Assessment of Greenhouse‐​gas Induced Climate Change (MAGICC) for my analysis of the effect of U.S. emissions on projected global temperature rise. MAGICC is sort of a climate model simulator that you can run from your desktop (available here). It was developed by scientists at the U.S. National Center for Atmospheric Research.



There are many parameters that can be altered when running MAGICC, including the climate sensitivity (how much warming the model produces from a doubling of CO2 concentration) and the size of the effect produced by aerosols. In all cases, I’ve chosen to use the MAGICC default settings, which represent the middle‐​of‐​the‐​road estimates for these parameter values (e.g., climate sensitivity equals 3.0°C).



I’ve had to make some assumptions about the U.S. emissions pathways as prescribed by the original IPCC scenarios in order to obtain the baseline U.S. emissions (unique to each scenario) to which I could apply the various emissions reduction schedules. The most common IPCC definition of its scenarios describes the future emissions, not from individual countries, but from country groupings. Therefore, I needed to back out the U.S. emissions.



To do so, I identified which country group the U.S. belonged to (the OECD90 group) and then determined the current percentage of the total group emissions that are being contributed by the United States—which turned out to be about 50%. I then assumed that this percentage remained constant over time. In other words, that the U.S. contributed 50% of the OECD90 emissions in 2000 as well as in every year between 2000 and 2100.



Thus, I am able to develop the future emissions pathway of the U.S. from the group pathway defined by the IPCC for each scenario (in this case, the B1, the A1B and the A1FI scenarios). The Waxman‐​Markey and carbon tax reductions were then applied to the projected U.S. emissions pathways, and the new U.S. emissions were then recombined into the OECD90 pathway and into the global emissions total over time.



It is the total global emissions that are entered into MAGICC in order to produce global temperature projections. My results are largely insensitive to minor changes in these assumptions.
"
"
Share this...FacebookTwitter**  BREAKING NEWS!   **  BREAKING NEWS! **  BREAKING NEWS! **
Stunning!
Another huge slab of the Climate-Berlin-Wall has fallen. It’s a climate skeptic jail-break! I imagine the Climate-Politburo members must be quivering and trembling in their bunkers in Potsdam by now.
Big hat-tip to NTZ reader Ike.
A leading German news magazine has decided to depart from the dogma of angst and catastrophe and bring up climate science issues that, up to now, have been strictly taboo here in the Vaterland. Tomorrow FOCUS magazine will come out with its newest issue titled:
Prima Klima! Umdenken:Wieso die globale Erwärmung gut für uns ist. (Best Climate! Change of Thinking: Why global warming is good for us.)
Change of Thinking – yes! And the timing couldn’t be better.
Folks, this is the first time in a long time that a major German news magazine has decided to do a little investigative reporting, instead of relying on the press releases from the Palaces Of Panic like the Potsdam Institute of Climate Impact Research, NOAA, Alfred Wegener Institute, etc., and seriously look into this controversial global issue. Game over comrades!
When the global warming hoax collapses in Germany, then Europe follows right behind it – and then, of course, the rest of the world. Germany is that one domino. This represents a major setback for the warministas. Indeed it would be interesting to know what went on in the FOCUS editorial offices.
Perhaps the normally über-alarmist FOCUS has already gotten tired of the winter and longs for the warmer days. I can’t explain why they are coming out with such an issue – especially during Cancun. Whoa! That’s all I can say.
Here’s what tomorrow’s issue will feature:
78   Warm times are good times. Harvest yields increase, Forests grow, deserts shrink
86   Which impacts of climate change are proven?  Which are not?
90   The “Who is who“ in climate science


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Promo video (in German, see English text below):
http://www.focus.de/magazin/videos/focus-titel-prima-klima_vid_21483.html
The video begins with:
This week in the coming FOCUS: Best Climate. Change in thinking – global warming is good for us. FOCUS editor Dr. Christian Pandler (sp?) researched the current topic and reports on it in the new issue.
Editor Dr. Pandler (a bit paraphrased):
In the new FOCUS issue, we take a look at the question of climate change. This week the world climate conference is taking place in Cancun, where world leaders are going to debate over how to combat warming. Our question: Is global warming actually bad? Does it entail only disadvantages and only catastrophic consequences? Up to now, people have only focused on what will be bad. The question is what could be good? No one has really looked at this. It’s taboo in Germany.
We know from history that warm periods were good periods for us. Cold periods were bad periods. We know that 20,000 years ago Europe was a frozen wasteland where nobody lived. That was a real climate catastrophe. For example we had a warming 10,000 years ago, which led to a greening of the Sahara. Then there was cooling which led it to be parched again. Now it’s warming, and there are lots of signs that show it is greening up again. For the people in Africa, it is absolutely a positive development. If it continues that way, it could once again become green with a variety of wildlife, rivers and lakes and so on. This is a consequence that hardly has been discussed.
We’ve spoken to scientists who are there on site. One researcher in particular has gone there every year for 30 years and photographed how the Sahara is gradually getting greener.”
Go out and reward FOCUS by buying this issue – get an additional copy for a friend too. Thank the editor for having the guts to do this.
In the meantime, my advice to that brave editor at FOCUS: Put on your bullet-proof political vest and find the deepest possible bunker. The greenshirts are sending over the B-52s! Achtung!
This is going to be something to relish.
Ironically this comes out precisely when the science is showing signs that cooling is coming instead, and so FOCUS may be only getting false hopes up. Lol! You just can’t make this stuff up. It makes my day.
Share this...FacebookTwitter "
"
What Lunar Orbiter 1 saw as it looked back at Earth on August 23, 1966. Climate studies of Earth will benefit by a look back in time thanks to decades old view from the Moon. Credit: LOIRP/NASA
From Space.com: Old Moon Images Get Modern Makeover
WOODLANDS, Texas — Think of it as a space age twist to that adage: Something old, something new…something borrowed, something blue.
Back in 1966 and 1967, NASA hurled a series of Lunar Orbiter spacecraft to the moon. Each of the five orbiters were dispatched to map the landscape in high-resolution and assist in charting where best to set down Apollo moonwalkers and open up the lunar surface to expanded human operations.
Imagery gleaned from the Lunar Orbiters over 40 years ago is now getting a 21st century makeover thanks to the Lunar Orbiter Image Recovery Project (LOIRP). 
By gathering the vintage hardware to playback the imagery, and then upgrading it to digital standards, researchers have yielded a strikingly fresh look at the old moon. Furthermore, LOIRP’s efforts may also lead to retrieving and beefing up video from the first human landing on the moon by Apollo 11 astronauts in July 1969.
Digital domain
Dennis Wingo, LOIRP’s team leader, detailed the group’s work in progress during last week’s 40th Lunar and Planetary Science Conference.
Teamed with SpaceRef.com, LOIRP’s saga is one of acquiring the last surviving Ampex FR-900 machinery that can play analog image data from the Lunar Orbiter spacecraft. Wingo noted that the work is backed by NASA’s Exploration Systems Mission Directorate, the space agency’s Innovative Partnership Program, along with private organizations, making it possible to overhaul old equipment, digitally upgrade and clean-up the imagery via software. 
LOIRP is located at NASA’s Ames Research Center at Moffett Field, Calif. There, project members are taking the analog data, converting it into digital form and reconstructing the images.
By moving them into the digital domain, Wingo said, the photos now offer a higher dynamic range and resolution than the original pictures, he added.
“We’re going to be releasing these to the whole world,” Wingo said. 
Use of the refreshed images, contrasted to what NASA’s upcoming Lunar Reconnaissance Orbiter (LRO) mission is slated to produce, has an immediate scientific benefit. That is, what is the frequency of impacts on the Moon’s already substantially crater-pocked surface?
“We’ll be able to get crater counts,” Wingo told SPACE.com. “LRO imagery of the same terrain imaged decades ago will provide a crater count over the last 40 years.” 
Frozen in time
There’s also a more down to Earth output thanks to LOIRP scientists.
They have used a Lunar Orbiter 1 image of the Earth for climate studies, basically a snapshot frozen in time that shows the edge of the Antarctic ice pack on August 23, 1966. 
The team is working with the National Snow and Ice Data Center in Boulder, Colorado to correlate their images of the Earth with old NASA Nimbus 1 and Nimbus 2 spacecraft imagery that flew at about the same time — in the mid-1960s — as the Lunar Orbiter 1. Nimbus satellites were meteorological research and development spacecraft.
Wingo said that the original Nimbus images may have been recorded on an Ampex FR-900 – so by processing the original Nimbus tapes there is a very good chance that they can provide NASA with polar ice pack data from ten years earlier.
Lessons learned
One treasure hunt outing by LOIRP may lead to finding what some term as “lost” Apollo 11 slow scan tapes, Wingo said. 
“We don’t think they are lost. People have been looking for the wrong tapes,” he said, explaining that they were recorded on Ampex FR-900 equipment — not on another type of recorder as previously thought.
Wingo said those Apollo tapes are stored at the Federal Records Center, labeled and ready for a look see.
“We think for the 40th anniversary of Apollo we may be able to get the original slow scan tapes,” Wingo said. If so, the hope is to recover them and give the public a higher-quality, never-before-seen view of human exploration of the Moon.
There is a lesson learned output from LOIRP. 
In the beginning, very few people thought this could be done…but now they have seen the results,” Wingo said.
It is not enough to have 100 year recording medium, Wingo explains. Without the retention of the specific era equipment that images are archived on, it will be impossible for future generations to recover older NASA or other satellite data, he advised.
This is a general issue, not specific to the Lunar Orbiter program. The retention of critical hardware should be a requirement for flight efforts. The original historic Apollo 11 slow scan images have been lost due to inattention to this critical detail, Wingo concluded.
(h/t to Gary Boden)
UPDATE: Dennis Wingo responded in comments, and offers this LA Times story on the real trials and tribulations of this project. 
http://www.latimes.com/news/nationworld/nation/la-na-lunar22-2009mar22,0,931431.story
We owe Mr. Wingo and his team, and especially Nancy Evans, a debt of gratitude for preserving space history against the odds. – Anthony
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96e2e2c7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn my last post I summarized the results of Ed Caryl’s analyses of stations far up in the Arctic, where temperature trends appear to follow the 60-year AMO cycle, and do not correlate at all with CO2.
Now I’ve been made aware that temperatures following the AMO are not exclusive to the Arctic. Blogsite digging in the clay has plotted temperatures from other cities located about the globe and came up with the same AMO sine wave trend, see below:

Source: http://diggingintheclay.wordpress.com/2010/09/01/in-search-of-cooling-trends/
And what follows are more plots from Iceland, Norway and Russia on the left, and from USA on the right (Sorry about the poor quality. Better quality graphics can be seen at diggingintheclay here). Again the AMO wave appears there as well. Moreover, the 1930s and 40s in USA even look a bit warmer than today’s temps.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So poor little trace-gas CO2 just doesn’t get no respect, no matter where it goes. Every corner of the globe is ignoring this pip-squeak, climate-driver wannabe. CO2 is fading from the picture.
If the temperature has followed the AMO sine trend over the last 150 years, then why do we keep seeing hockey blades (nowadays without the stick-part) showing temperatures straight-lining up over the last 100 years or so?
Probably because GISS and others have changed historical data to make them fit their ideologized models – as unbelievable as it may sound. Steve Goddard’s site here explains how GISS has precisely done this with US temperatures by going back and flattening the graph.

The above comparator is from: Steve Goddard’s site. His post is short, but a very worthwhile read.
Before 2000, GISS showed a warmer 1930s and cooler current period. The AMO wave is clear to see.  But today, after having fiddled with the data, GISS now shows a cooler 1930s, a warmer present day and a somewhat straighter line that tries to cloak the AMO effect.
 Now you really know why they call it “man-made global warming”.
Share this...FacebookTwitter "
"

From ETH in Zurich, this interesting essay on the last glacial period has some interesting points to ponder. h/t to Sid Stafford – Anthony

The last glacial period was characterised by strong climatic fluctuations. Scientists have now been able to prove very frequent and rapid climate change, particularly at the end of the Younger Dryas period, around 12,000 years ago. These fluctuations were accompanied by rapid changes in circulation in the oceans and the atmosphere.

Researchers are able to determine when glaciers were stable and when they melted by studying titanium content in glacial lake sediments. (Picture: siyublog/flickr)

Sediment deposits in lakes are the climate archives of the past. An international team of researchers from Norway, Switzerland and Germany have now examined sediments originating from the Younger Dryas period from the Kråkenes Lake in northwest Norway. In the sediments, they found clues that point to a “climate flicker” at the end of the last glacial period, oscillating between colder and warmer phases until the transition to the stable climate of the Holocene, our current interglacial period. The short-term, strong fluctuations of the Younger Dryas would have dwarfed the “extreme weather phenomena” seen today, according to Gerald Haug, professor at the Department for Earth Sciences at ETH Zürich and co-author of the study, which was published online yesterday in “Nature Geoscience”.
Seasonal sediment deposits
Seasonal sediment accumulation, for example, gave scientists clues to these strong climate fluctuations. They can be read in lakes in a similar way to reading rings on trees. In warmer phases and melting glaciers, the accumulation of sediments increases. More clues on the changes in glacier growth were given by the element titanium, which is present in the sediments. Glaciers erode their bedrock, and in doing so concentrate the titanium contained in the sediments they are carrying. The sediments containing titanium are washed into the glacier’s draining lakes in the meltwater. The amount of sediment and the titanium content can therefore allow us to deduce when the glaciers were stable and when they melted. The researchers interpreted the maxims, recurring every 10 years, as phases of strong glacier activity caused by temperature fluctuations and thus as warmer times.
A seemingly self-preserving cycle
The scientists also examined a sediment core from seabed deposits of the same age in the North Atlantic. They reconstructed the original temperature and salt concentration of the water based on microfossils and the oxygen isotope ratio in the sediment. It was shown that the results from the lake sediments corresponded to those from the sea sediments. “The melting of glaciers was caused by the warm Gulf stream advancing into this region,” Gerald Haug explains. This increase in temperature caused the west winds to shift to the north and brought warm air to northern Europe. However, the meltwater draining into the Atlantic lowered the salt concentration and the density of the surface water, changing the convection in the ocean, which in turn allowed new sea ice to form. Subsequently, the Gulf Stream and the west winds were again forced out of the North Atlantic area and the region cooled down once again. These processes were repeated for around 400 years, until the current interglacial period was able to stabilise itself.
The Würm glaciation began around 100,000 years ago and lasted until around 10,000 years ago. In this period, there were strong fluctuations between warm and cold phases, particularly in the North Atlantic area. The Younger Dryas, which ushered in the current interglacial period, is one of the best-known and best-researched abrupt climate changes of that glaciation. It began around 12,900 years ago and at first caused an abrupt temperature drop in the northern hemisphere, as well as a temperature rise of up to 10°C in less than 20 years towards the end, around 11,700 years ago.
Unclear mechanisms
Up until now, there have been several studies which document the glacial conditions during the Younger Dryas period of 1,200 years. However, the mechanisms which caused it, sustained it and finally led to an interglacial period have yet to be fully understood. The researchers believe that further high-resolution studies of this type could give insights into how glacial periods are triggered and how they are brought to an end.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9812b02e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterHere’s a scary clip produced by some green Marxists at the German Federal Office of the Environment, who seem to have taken it upon themselves to tell the rest of us how to live. If this represents the government’s view and the target it has in mind for its citizens, then it’s awfully spooky. (Clip is in German – main points are described below).

It’s back to central planning, housing project living, and being told exactly how to live – all the failed experiments of the past, all combined and packaged as a religion that promises salvation.
Remember how the communists promised paradise for workers? This is the same pipe dream. The problem with all of it is that it is not based on the fundamental laws of economics, and so it will definitely fail. Many of the diagnoses offered in the first half of the clip are not even true, and the remedies proposed thereafter contradict each other.
The makers of this clip are sure that the world’s problems are because we are all behaving badly, and that we have to be made to behave differently – dictated – in a way that suits their world view. The world is threatened, says the film, by population growth and our consumption, and demands a system of redistribution.
Here are some of the main points:
 1. We are all spoiled. We cannot, or simply do not want to, go without the things we use daily. We are using more than what nature is capable of giving. We are using much too much. We are living way beyond our means. 
2. We are addicted to a huge array of products that we believe make our lives easier. We are leading lives of profligacy, which requires gigantic amounts of resources like, water, raw material and energy. The planet will soon be depleted. 
The disgruntled among us have been saying this for at least 2 centuries. Yet, everyday we keep finding more than we need. Things are better today than ever.
3. The guilty parties are North America and Europe. The industrialised countries use more than their fair share. Each German uses 60 tons of material annually, Americans 130 tons. This can no lonager be tolerated. 
This is called prosperity. Obviously a concept that the clip’s makers have an aversion to. Note that most material is not consumed – but used. It doesn’t disappear.
4. By 2030, 20% of the most important raw materials will reach their production limits. 
5. All this consumption is leading to many things, particularly climate change and species extinsction. 
6. Expanding deserts, multiplication od disaeases, wars and resource shortages are threatening future generations.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The clip doesn’t say which materials. And actually deserts are shrinking. They tend to grow when the planet cools.
7. People in industrialised nations are only able to be so wasteful because people in other poor countries have to go without raw materials.
8. Our level of prosperity is not possible for everyone. 
That’s false. It’s people in developing countries that need to learn how to use raw materials. This comes with free markets and education. Any normal person with a 4th grade education or more knows that the planet gets destroyed in undeveloped countries, and protected in developed countries.  The solution is for the rest of the world to live like we do. The clip claims:
9. We have to change our thinking radically. Only through a drastic reduction in our consumption of resources through dematerialisation will we be able to sustainably secure our future. We have to consume less.
10. For example, many of the things that we seldom use can be borrowed among friends. Things like the electric drill, lawnmower. We have to redesign our tools so that they have longer lifetimes, or can be repaired, easily dismantled and to recycle.
This is so stupid – economic ignorance now in full display. Imagine if we designed a long life copy machine to last 20 years. It would be obselete in three years, and so it would wind up consuming too much energy and operating too slowly during the remaining 17 years. This film was made by technical and economic illiterates.  The clip then suggests we need to:
11. Use public laundermats, buy second hand clothes, ride bicycles and have fun at “sustanaible parties”. 
WTF? Yeah right, When I was young I worked years with the target of getting away from having to do that. This is back to Soviet living. And the clip says we ought to think about using kooky ideas like:
12. Wind-powered ships, wind-powered buildings.
13. Powering everything with renewables.
Get ready to pay a fortune for an unreliable and primitive source of energy. Indeed get ready to fork it all over. The film then has the temerity to say:
14. Today’s prices do not reflect the ecology. That has to change.
15. Taxes on resources have to go up!
Yes, they want to take everything away from us. Everybody has to get along with much less – except for the state, of course. For them it’s more! more! more! Now we know what all this is about – state power. Power and wealth to the state, and not the individual. This clip is an example of the propaganda, one that despises humanity, that we are getting in Germany for our tax euros.
Folks, you’ve got to start talking to your local and regional politicians and business leaders about this. The higher-ups are all drugged up with their “let’s-take-over-and-save-the-planet” fantasies. It has to start from the botrtom up, like the tea party candidates.  
It’s not the people that “need to drastically change their thinking”. It’s the intoxicated politicians that do.
 
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe winter has even overwhelmed winter-hardened Sweden. So much so that the military has been called out to assist.
Temperatures as low as -36 degrees Celsius have been recorded in Sweden as snowfalls and storm winds play havoc with transport services.

Transport agency calls for caution after snow (23 Nov 10)
Sweden braces for new winter storm (22 Nov 10)
Rail operators prepared for harsh winter: report (16 Nov 10)

Sweden’s main meteorological agency, SMHI, noted that the winter continued its march south across the country as strong winds from the Baltic Sea brought heavy snowfalls in eastern areas of Svealand and Götaland.
The snow is expected to remain on the ground in many parts of southern Sweden as temperatures are set to remain well below zero.
“There has been a lot of snow overnight,” Lisa Frost at SMHI said.
Kalmar county was obliged to call for military help on Wednesday to aid in the battle against widespread flooding which had caused damage to property in the area.
Read more: http://www.thelocal.se/30396/20101124/ (The Local).
The Independent got half of it right anyway. Maybe not “rare”, but at least “exciting”.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAnd attention morons, drones, and other like-minded believers in superstition and ritual behaviour.
At 8:30 p.m CET tomorrow evening it is Earth Hour. It’s that time again to participate in yet another useless collective ritual of madness, by switching off the lights along with all the other drones, in a bid to get the climate gods to bestow nice weather upon us (and even stop bush fires – to and solve all the other world’s problems. Let’s call it: Be Like North Korea Hour.
Here’s one example drone-controller spokesperson, treating Australians like a bunch three-year-olds, asking them to take part in this ridiculous embarrassment of a modern rain-dance:

Now that we are all done throwing up, let’s continue.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany, parents tell their toddlers to finish their food on the plate, otherwise it’ll rain tomorrow. Well now it’s live in the dark, or else it’ll rain a lot, or not enough, or whatever. “Let’s all turn off our lights altogether now so that we can have a better future. Aren’t we all just good little boys and girls! Hooooray!”
Yes, it’s time to call the doctor. Personally, they can ram this retardation where it’s permanently dark.
I guess some creatures and criminals of the night will especially like this. Women stay home (and leave the lights on outside and inside).
Hungary International Airport to switch off tarmac lights!
And get a load of this craziness: A bunch of moron officials at the Budapest Hungary International Airport are going to play around with the tarmac lights! Talk about making the world a safer place to live. Read here.
Of course, we can counter all this madness. For example I’m going to upgrade my light bulbs.
This is better. Osram 200-watt incandescent bulb. (Photo credit: C. Kemme)
The 500-watt flood lights work well too.  Do your part to light up the world. Make your neighbourhood safer tomorrow. Light it up!
Further reading:
Celebrating ignorance
Celebrate Human Achievement Hour
It’s Earth Century in North Korea
Make it look like Christmas
Share this...FacebookTwitter "
"

A new piece of scientific research hit the presses last week. It reported finding more warming in one of the (several) satellite-observed temperature histories of the earth’s lower atmosphere than had been previously reported. As these satellite-measured temperatures were the recent subject of comments made by presidential candidate Ted Cruz, a lot of scrutiny and interest surrounds these new findings—findings which seemed to refute some of Cruz’s assertions.   
  
In researching his story on the new study, the Associated Press’s Seth Borenstein solicited my opinion about them and how they may alter climate change skeptics’ way of thinking about the satellite-observed temperatures—temperature datasets which had previously shown precious little warming over the past nearly two decades.   
  
I was happy to offer my thoughts, and equally happy to see some of them reflected in Seth’s AP story. Given topical and length constraints, understandably, Seth had to be selective.   
  
But I do have a bit more to say about the new research finding besides that it “shows ‘how messy the procedures are in putting the satellite data together.’”   
  
Many of my additional thoughts were included in my broader email response to Seth’s initial inquiry and, with his permission, I am reproducing our correspondence below.   
  
To Seth’s summary of my thoughts, I’d add “but even considering the new findings, the complete collection of satellite- and weather balloon-observed temperature histories of the earth’s atmosphere indicate that climate models are projecting too much warming in this important region.”   
  
Again, my thanks to Seth for reaching out to me in the first place. Here is out question and answer exchange:



Chip,   
  
Seeing that the climate doubter community has hinged so much on RSS and saying there has been no warming post 1997 _ despite NOAA heat records in 1998, 2005, 2010, 2014 and 2015 _ you’ve seen the RSS update that shows there has been warming in the last 18 years. I’m wondering what your thoughts are on it. Will you and those in your community keep using RSS, even if it shows no warming. Add to that the UAH record warming in February. Are satellites now contradicting the climate doubter community?   
  
Thanks,   
Seth



 ****   




Seth,   
  
Thanks for soliciting my opinion.   
  
I can't speak for the climate doubter community, however that is defined.   
  
Personally, my doubts are not that human-caused climate change as a result of greenhouse gas emissions is not occurring and that a temperature rise as a result is not detectable in large spatial averages, but I have doubts that the change is taking place at the rate projected by the collection of climate models and that its effects are currently detectable on most smaller scale climate/weather metrics.   
  
So with that out of the way, I’ll give some opinions as to the new RSS results and their importance to my way of thinking…   
  
First off, as I have tweeted (https://twitter.com/PCKnappenberger/status/705515578325270529), the overall 1979-2014 trend in the RSS v4 MT data is still pretty far beneath the climate model expectations…far enough to continue to indicate a sizable discrepancy that needs further scientific attention.   
  
Second, the trend in the new RSS v4 MT now makes it the mid-tropospheric (MT) dataset (including other satellite based and weather-balloon based) that has the greatest trend over the 1979-2014 period (see the same tweet mentioned above, as well as this one, https://twitter.com/PCKnappenberger/status/705472903458914305 which shows the old and new RSS data in comparison to weather-balloon compilations).   
  
Given these two things, I don’t think it helps settle any questions regarding the temperature behavior of the mid-troposphere.   
  
But what it does do is shed more light on just how messy the procedures are in putting the satellite data together. Decisions, guided by science but not specifically defined by it, occur at many points in the procedure. The new RSS paper, again highlights how sensitive the final results are to those decisions. It is good that we have many different groups involved in assembling both the satellite history and the weather-balloon history. That these different groups provide answers that are pretty close to each other helps not to lower the uncertainty in any single result, but that the general result is not indicative as to what is going on in the MT. The new RSS v4 now lies outside the old envelop of these collective findings. It’ll either prove to move the science in a bit of a different direction, or prove to be an erroneous result. Time will tell.   
  
As to the impact on the “pause,” IMO there was too much being made about the “pause"" in the first place. No serious student of climate science thought that it would last forever. The important thing about it was that it provided a challenge to climate science and prompted enhanced research into natural climate variability, climate sensitivity, and other important aspects of climate science. So that it’s now over comes as no surprise. But, once the El Nino warming subsides, I think we’ll probably see a continuation of the modest (below model mean) rate of warming.   
  
I hope this is useful. If you have any further questions, I’d be more than happy to try to answer them.   
  
-Chip



In addition to Seth’s story for the AP, more reactions about the new satellite-study can be found at Watts Up With That, Climate Etc., and at Roy Spencer’s blog, among others.


"
"
No we aren’t talking pianos, but Grand Solar Minimums. Today a new milestone was reached. As you can see below, we’ve been leading up to it for a few years.
Above: plot of Cycle 23 to 24 sunspot numbers in an 11 year window 
(Update: based on comments, I’ve updated the graph above to show the 2004 solar max by sliding the view window to the left a bit compared to the previous graph. – Anthony)
A typical solar minimum lasts 485 days, based on an average of the last 10 solar minima. As of today we are at 638 spotless days in the current minimum. Also as of today, May 27th, 2009, there were no sunspots on 120 of this year’s (2009) 147 days to date (82%).
Paul Stanko writes:
Our spotless day count just reached 638.
What is so special about 638?  We  just overtook the original solar cycle, #1, so now the only cycles above  this are: cycles of the Maunder minimum, cycles 5 to 7 (Dalton  minimum), and cycles 10 + 12 to 15 (unnamed minimum).
Since the last  one is unnamed, I’ve nicknamed it the “Baby Grand Minimum”, in much  the same way that you can have a baby grand piano. We would now seem to  have reached the same stature for this minimum.  It will be interesting  to see just how much longer deep minimum goes on.
Of course it depends on what data you look at. Solar Influences Data Center and NOAA differ by a few days. As WUWT readers may recall, last year in August, the SIDC reversed an initial count that would have led to the first spotless month since 1913:
Sunspeck counts after all, debate rages…Sun DOES NOT have first spotless calendar month since June 1913
NOAA did not count the sunspot, so at the end of the month, one agency said “spotless month” and the other did not.
From Spaceweather.com in an April 1st 2009 article:
The mother of all spotless runs was of course the Maunder Minimum. This was a period from October 15, 1661 to August 2, 1671.
It totaled 3579 consecutive spotless days. That puts our current run at 17.5% of that of the Maunder Minimum.
By the standard of spotless days, the ongoing solar minimum is the deepest in a century: NASA report. In 2008, no sunspots were observed on 266 of the year’s 366 days (73%). To find a year with more blank suns, you have to go all the way back to 1913, which had 311 spotless days (85%):

The lack of sunspots in 2008, made it a century-level year in terms of solar quiet. Remarkably, sunspot counts for 2009 have dropped even lower.
We do indeed live in interesting times.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96016f4d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSome of us may be wondering whatever happened to the dana who we all love and miss so much. Well, Lubos Motl at the Reference Frame has a nice little update on the adventures of dana:H/t: Mindert Eiting
Why Dana 1981 Hasn’t Proved…
By Lubos Motl
Dana1981 is a 30-year-old Prius driver and the owner of several other alternative vehicles who has mistakingly received a bachelor degree in astrophysics and a master degree in physics, so he or she became a self-described environmental scientist who is “passionate” about the climate hysteria.
Clearly, such people shouldn’t be admitted as college students because they’re incapable of rational thinking. The presence of people like him dramatically cripples the intellectual atmospheres at the world’s universities…”
Read more!
PS: My advice to you dana is: I wouldn’t mess with Lubos, as he would certainly do physics circles and orbits around you. Dana, you’ve only proved one thing, and you may realize what that is when you get older.
Share this...FacebookTwitter "
"

Of course many of you that live in this weather already know this, but there is an early start to winter this year, not only in the USA, but also in London, where it snowed in October for the first time in over 70 years.
So far, no mention of this broadly distributed U.S. record event in the mainstream media. There are a few individual mentions or record lows in Florida. See this Google News search.
Here, from NOAA’s  National Climatic Data Center (NCDC), is a list of these new or tied records for October 29th, 2008.
I find the -25 below in Alaska interesting, since it bested the old record by 4 degrees.
Here are the 115 new or tied low temperature records:
The table below has been formatting to fit the blog, Here is a direct link to the original data from NCDC



29 October 2008
Record
New (83)
Tied (32)
Previous
Record
Previous
Year
Period
of
Record


CIRCLE HOT SPRINGS, AK
-25.0°F
-21.0°F
2001
44


TONSINA, AK
-17.0°F
-16.0°F
1985
42


CAMP HILL 2 NW, AL
21.0°F
25.0°F
1968
76


HAMILTON 3 S, AL
23.0°F
24.0°F
1968
45


CENTREVILLE 6 SW, AL
26.0°F
28.0°F
2001
32


MUSCLE SHOALS AP, AL (KMSL)
27.0°F
28.0°F
1952
67


GREENVILLE, AL
28.0°F
29.0°F
2001
78


GENEVA #2, AL
29.0°F
29.0°F
2001
32


HIGHLAND HOME, AL
29.0°F
30.0°F
1976
112


HUNTSVILLE INTL AP, AL (KHSV)
30.0°F
30.0°F
2005
50


MONTGOMERY AP ASOS, AL (KMGM)
31.0°F
32.0°F
2001
60


ATMORE, AL
32.0°F
33.0°F
2001
48


MOBILE RGNL AP, AL (KMOB)
32.0°F
36.0°F
1987
60


FAIRHOPE 2 NE, AL
33.0°F
34.0°F
1952
89


CODEN, AL
34.0°F
35.0°F
1957
43


DAUPHIN IS #2, AL
47.0°F
48.0°F
2001
32


BOONEVILLE 3 SSE, AR
28.0°F
29.0°F
1993
30


MURFREESBORO 1 W, AR
29.0°F
29.0°F
1993
33


SPARKMAN, AR
29.0°F
29.0°F
2005
40


FORDYCE, AR
30.0°F
30.0°F
1993
71


ROHWER 2 NNE, AR
31.0°F
32.0°F
1997
47


WEST MEMPHIS, AR
31.0°F
33.0°F
1976
45


BLYTHEVILLE, AR
32.0°F
32.0°F
1939
79


EUDORA, AR
32.0°F
32.0°F
1997
45


PERRY, FL
29.0°F
32.0°F
1987
71


TALLAHASSEE WSO AP, FL (KTLH)
29.0°F
31.0°F
1987
63


GLEN ST MARY 1 W, FL
29.0°F
32.0°F
1957
80


MAYO, FL
30.0°F
30.0°F
1957
57


NICEVILLE, FL
31.0°F
33.0°F
2001
62


JACKSONVILLE INTL AP, FL (KJAX)
33.0°F
39.0°F
1987
60


APALACHICOLA AP, FL (KAAF)
34.0°F
41.0°F
1976
76


PENSACOLA RGNL AP, FL (KPNS)
36.0°F
38.0°F
1968
60


TAMPA WSCMO AP, FL (KTPA)
42.0°F
45.0°F
1963
75


ORLANDO INTL AP, FL (KMCO)
43.0°F
49.0°F
1952
54


DAYTONA BEACH INTL AP, FL (KDAB)
44.0°F
46.0°F
1957
60


KISSIMMEE 2, FL
44.0°F
45.0°F
1968
46


VERO BEACH INTL AP, FL (KVRB)
46.0°F
48.0°F
1943
57


FT MYERS PAGE FLD AP, FL (KFMY)
47.0°F
47.0°F
1910
109


WEST PALM BCH INTL AP, FL (KPBI)
49.0°F
51.0°F
1944
69


MIAMI INTL AP, FL (KMIA)
55.0°F
61.0°F
1968
60


FT LAUDERDALE INTL AP, FL (KFLL)
55.0°F
62.0°F
2006
35


KEY WEST INTL AP, FL (KEYW)
61.0°F
66.0°F
1957
56


NAHUNTA 6 NE, GA
28.0°F
30.0°F
1957
45


PLAINS SW GA EXP STN, GA
30.0°F
30.0°F
2001
52


BLAKELY, GA
31.0°F
34.0°F
1976
95


ALBANY CAA AP, GA
31.0°F
35.0°F
1952
33


BRUNSWICK, GA
39.0°F
40.0°F
1957
90


CASSODAY, KS
24.0°F
24.0°F
1993
46


IOLA 1 W, KS
26.0°F
26.0°F
1980
48


HOMER 3 SSW, LA
27.0°F
33.0°F
2001
55


BASTROP, LA
29.0°F
31.0°F
2005
78


ASHLAND, LA
30.0°F
32.0°F
2005
54


MONROE ULM, LA
30.0°F
32.0°F
2005
31


ALEXANDRIA AP, LA (KESF)
31.0°F
31.0°F
2005
56


MANSFIELD, LA
33.0°F
34.0°F
2005
32


JONESVILLE LOCKS, LA
33.0°F
39.0°F
2005
36


SLIDELL, LA
34.0°F
35.0°F
1957
52


BUNKIE, LA
34.0°F
34.0°F
1957
50


RED RVR RSCH STN, LA
34.0°F
35.0°F
2001
31


RESERVE, LA
35.0°F
35.0°F
1913
101


BOYCE 3 WNW, LA
39.0°F
41.0°F
2001
31


GALENA, MO
22.0°F
25.0°F
1963
43


MT VERNON M U SW CTR, MO
22.0°F
25.0°F
1980
48


BUFFALO 2 N, MO
22.0°F
23.0°F
1980
44


WASOLA, MO
25.0°F
26.0°F
1952
61


HICKORY FLAT, MS
26.0°F
27.0°F
2001
51


OAKLEY EXP STN, MS
27.0°F
28.0°F
2001
37


WINONA 5 E, MS
28.0°F
28.0°F
2001
54


GRENADA 5 NNE, MS
28.0°F
29.0°F
1957
53


MCCOMB AP, MS (KMCB)
31.0°F
34.0°F
1957
60


WIGGINS, MS
32.0°F
34.0°F
1957
52


ROLLING FORK, MS
32.0°F
35.0°F
2005
35


PASCAGOULA 3 NE, MS
33.0°F
33.0°F
1987
71


YAZOO CITY 5 NNE, MS
33.0°F
33.0°F
1963
46


GRANDFATHER MTN, NC
17.0°F
17.0°F
1968
52


SUPERIOR 4E, NE
20.0°F
21.0°F
1991
53


TUSKAHOMA, OK
24.0°F
31.0°F
1973
46


MARIETTA 5SW, OK
25.0°F
26.0°F
1952
67


LINDSAY 2 W, OK
27.0°F
31.0°F
1993
43


KEYSTONE DAM, OK
28.0°F
29.0°F
1980
41


PERRY, OK
28.0°F
28.0°F
1980
89


BROKEN BOW DAM, OK
32.0°F
32.0°F
1973
34


SANDHILL RSCH ELGIN, SC
30.0°F
30.0°F
1976
50


DICKSON, TN
23.0°F
23.0°F
1952
106


AMES PLANTATION, TN
28.0°F
29.0°F
2001
31


JOHNSON CITY, TX
28.0°F
34.0°F
1970
41


GILMER 4 WNW, TX
28.0°F
30.0°F
1952
72


MT VERNON, TX
28.0°F
35.0°F
1973
42


SMITHVILLE, TX
28.0°F
34.0°F
1957
81


WARREN 2 S, TX
29.0°F
33.0°F
1957
32


WEATHERFORD, TX
29.0°F
29.0°F
1913
103


EMORY, TX
29.0°F
35.0°F
1995
42


GREENVILLE KGVL RADIO, TX
30.0°F
30.0°F
1952
103


MADISONVILLE, TX
30.0°F
31.0°F
1955
61


CENTERVILLE, TX
30.0°F
33.0°F
1970
65


KERRVILLE 3 NNE, TX
31.0°F
36.0°F
2006
34


CENTER, TX
31.0°F
31.0°F
1952
65


FOWLERTON, TX
32.0°F
32.0°F
1970
52


HILLSBORO, TX
32.0°F
32.0°F
1913
97


HENDERSON, TX
32.0°F
36.0°F
1973
67


AUSTIN BERGSTROM INTL, TX (KAUS)
33.0°F
37.0°F
1970
35


CLEVELAND, TX
33.0°F
35.0°F
1965
44


HONDO MUNI AP, TX (KHDO)
34.0°F
40.0°F
1993
37


GRAPEVINE DAM, TX
35.0°F
35.0°F
1910
66


LONGVIEW 11 SE, TX
35.0°F
38.0°F
1993
33


LA GRANGE, TX
36.0°F
38.0°F
2005
46


TOWN BLUFF DAM, TX
36.0°F
37.0°F
2001
37


JACKSONVILLE, TX
36.0°F
36.0°F
1970
44


VICTORIA ASOS, TX (KVCT)
37.0°F
40.0°F
1980
53


STILLHOUSE HOLLOW DAM, TX
37.0°F
38.0°F
1970
40


EL CAMPO, TX
38.0°F
39.0°F
1970
36


MATAGORDA 2, TX
40.0°F
40.0°F
1952
78


ARANSAS WR, TX
40.0°F
46.0°F
1980
35


POINT COMFORT, TX
42.0°F
43.0°F
2007
48


RAYMONDVILLE, TX
45.0°F
45.0°F
1970
92




Here are 163 new or tied lowest high temperature records for October 29th, 2008
Here is a direct link to NOAA’s NCDC data for these records:



29 October 2008
Record
New (120)
Tied (48)
Previous
Record
Previous
Year
Period
of
Record


BRIDGEPORT 5 NW, AL
49.0
55.0
2001
44


SAND MT SUBSTN, AL
50.0
50.0
1952
59


MOULTON 2, AL
51.0
53.0
1973
49


TALLADEGA, AL
52.0
55.0
1973
107


CLANTON, AL
52.0
53.0
1910
110


SYLACAUGA 4 NE, AL
52.0
56.0
1997
46


BELLE MINA 2 N, AL
52.0
53.0
1952
57


VERNON, AL
54.0
55.0
1973
49


HAMILTON 3 S, AL
54.0
58.0
1968
45


GREENVILLE, AL
55.0
59.0
2001
78


JASPER, AL
55.0
55.0
1976
45


EVERGREEN, AL
55.0
57.0
1910
83


THORSBY EXP STN, AL
55.0
57.0
1997
50


BREWTON 3 SSE, AL
57.0
60.0
1958
79


CODEN, AL
59.0
59.0
1997
44


MARSHALL, AR
52.0
52.0
1969
54


FT BRAGG 5 N, CA
53.0
53.0
1953
72


FERNANDINA BEACH, FL
64.0
64.0
2001
109


ST PETERSBURG, FL (KSPG)
64.0
64.0
1952
96


GAINESVILLE RGNL AP, FL (KGNV)
64.0
64.0
2007
45


ST AUGUSTINE LH, FL
66.0
69.0
1987
34


KEY WEST INTL AP, FL (KEYW)
71.0
74.0
1987
56


FT LAUDERDALE INTL AP, FL (KFLL)
76.0
78.0
1989
35


ALPHARETTA 4 SSW, GA
49.0
53.0
1959
41


GAINESVILLE, GA
49.0
49.0
1910
103


ALLATOONA DAM 2, GA
50.0
53.0
1953
43


DALLAS 7 NE, GA
51.0
55.0
1976
50


ELBERTON 2 N, GA
51.0
51.0
1910
68


HARTWELL, GA
51.0
53.0
2001
94


TOCCOA, GA
51.0
51.0
1910
105


SILOAM 3 N, GA
56.0
56.0
2003
46


MAUNA LOA SLOPE OBS 39, HI
48.0
48.0
1976
49


NORMAL 4NE, IL
45.0
45.0
1988
31


PERU, IL
46.0
46.0
1988
45


COLUMBIA CITY, IN
39.0
41.0
1968
44


PORTLAND 1 SW, IN
41.0
43.0
1976
30


BLUFFTON 1 N, IN
42.0
44.0
1980
36


NEW CASTLE 4 SSE, IN
42.0
42.0
1968
58


BAXTER, KY
44.0
49.0
1968
56


WEST LIBERTY 3NW, KY
45.0
46.0
1973
56


MT VERNON, KY
45.0
48.0
1980
49


JAMESTOWN WWTP, KY
47.0
48.0
1976
31


MONTICELLO 3 NE, KY
47.0
47.0
1980
52


PAINTSVILLE 1 E, KY
47.0
51.0
2003
30


BRADFORDSVILLE, KY
48.0
48.0
1968
44


BARBOURVILLE, KY
48.0
50.0
1953
54


FROSTBURG 2, MD
37.0
39.0
1976
36


SAVAGE RVR DAM, MD
39.0
41.0
1976
56


EMMITSBURG 2 SE, MD
48.0
48.0
1965
50


CUMBERLAND 2, MD
50.0
50.0
2002
32


IONIA 2 SSW, MI
39.0
42.0
1988
69


LAPEER WWTP, MI
40.0
41.0
2006
56


GROSSE POINTE FARMS, MI
44.0
44.0
2006
57


SHELBINA, MO
48.0
48.0
1980
62


WELDON SPRING NWS, MO
50.0
50.0
1976
42


PORTAGEVILLE, MO
50.0
50.0
1976
41


RIPLEY, MS
50.0
54.0
1968
66


INDEPENDENCE 1 W, MS
51.0
52.0
1976
50


IUKA, MS
51.0
57.0
1997
30


PONTOTOC EXP STN, MS
51.0
54.0
1968
55


HICKORY FLAT, MS
52.0
52.0
1980
51


WINONA 5 E, MS
52.0
54.0
1997
54


HOLLY SPRINGS 4 N, MS
52.0
54.0
1976
46


EUPORA 2 E, MS
53.0
55.0
1976
76


GRENADA 5 NNE, MS
53.0
56.0
1997
53


CALHOUN CITY, MS
53.0
59.0
1980
52


BELZONI, MS
55.0
57.0
1976
76


NORTH WILKESBORO, NC
48.0
52.0
1976
53


YADKINVILLE 6 E, NC
48.0
51.0
2003
50


STATESVILLE 2 NNE, NC
50.0
52.0
2003
101


ALBEMARLE, NC
53.0
55.0
2003
96


CLAYTON WTP, NC
55.0
55.0
2001
47


LEWISTON, NC
55.0
56.0
2005
52


ELIZABETHTOWN 3 SW, NC
56.0
60.0
2005
47


CAPE HATTERAS MITCHELL, NC (KHSE)
56.0
56.0
1976
51


FLEMINGTON 5 NNW, NJ
42.0
45.0
1976
110


NEW BRUNSWICK 3 SE, NJ
43.0
44.0
1976
40


DELHI 2 SE, NY
33.0
35.0
1952
75


BINGHAMTON WSO AP, NY (KBGM)
33.0
33.0
1952
60


WARSAW 6 SW, NY
35.0
35.0
1965
53


BAINBRIDGE 2 E, NY
35.0
39.0
1939
56


NORWICH, NY
36.0
37.0
1925
99


WATERTOWN AP, NY (KART)
37.0
39.0
1962
59


ELMIRA, NY
38.0
38.0
1928
112


PORT JERVIS, NY
40.0
40.0
1952
113


YORKTOWN HTS 1 W, NY
40.0
43.0
1976
43


WEST POINT, NY
42.0
42.0
1952
108


CADIZ, OH
39.0
41.0
1910
102


COSHOCTON AG RSCH STN, OH
40.0
42.0
1980
51


STEUBENVILLE, OH
40.0
41.0
1952
66


NEWARK WTR WKS, OH
42.0
42.0
1952
73


HANNIBAL L&D, OH
42.0
43.0
1976
33


NAPOLEON, OH
42.0
46.0
1980
39


NEW LEXINGTON 2 NW, OH
43.0
43.0
1952
66


WASHINGTON COURT HOUSE, OH
44.0
45.0
1968
81


BRADFORD RGNL AP, PA (KBFD)
31.0
35.0
2002
51


PLEASANT MT 1 W, PA
33.0
35.0
1959
55


DUBOIS FAA AP, PA (KDUJ)
34.0
38.0
1968
41


FRANCIS E WALTER DAM, PA
35.0
39.0
1976
41


WELLSBORO 4 SW, PA
36.0
37.0
1980
74


HAWLEY 1 E, PA
36.0
44.0
1997
82


CHALK HILL 2 ENE, PA
37.0
43.0
1990
31


MATAMORAS, PA
37.0
45.0
1965
42


TOWANDA 1 S, PA
38.0
39.0
1925
114


CONFLUENCE 1 SW DAM, PA
39.0
40.0
1957
62


TIONESTA 2 SE LAKE, PA
40.0
40.0
2001
65


WAYNESBURG 1 E, PA
41.0
44.0
1976
47


STEVENSON DAM, PA
42.0
43.0
2001
39


HAMBURG, PA
43.0
43.0
1907
67


WEST CHESTER 2 NW, PA
44.0
44.0
1976
103


LEWISTOWN, PA
46.0
47.0
1997
66


LONG CREEK, SC
49.0
52.0
1952
54


CHESTER 1 NW, SC
51.0
52.0
1959
76


PICKENS, SC
52.0
54.0
1952
57


SUMTER, SC
54.0
58.0
2001
81


CALHOUN FALLS, SC
54.0
55.0
1925
90


MANNING, SC
56.0
58.0
2001
35


BAMBERG, SC
56.0
57.0
1959
56


ANDREWS, SC
58.0
58.0
2001
37


ALLARDT, TN
43.0
44.0
1968
78


MONTEAGLE, TN
44.0
45.0
1952
68


TAZEWELL, TN
46.0
50.0
1976
42


LIVINGSTON RADIO WLIV, TN
48.0
50.0
1973
43


NEAPOLIS EXP STN, TN
49.0
52.0
1976
31


PORTLAND SEWAGE PLT, TN
50.0
51.0
1976
52


COVINGTON 3 SW, TN
50.0
51.0
1976
109


LINDEN WTP, TN
50.0
53.0
1976
45


SMITHVILLE 2 SE, TN
51.0
54.0
1976
36


SELMER, TN
51.0
54.0
1976
50


PULASKI WWTP, TN
51.0
57.0
2001
50


LEXINGTON, TN
51.0
51.0
1968
41


RIPLEY, TN
51.0
53.0
2002
43


MARTIN U OF T BRANCH E, TN
52.0
52.0
1976
72


CHEATHAM L&D, TN
52.0
54.0
1976
35


BROWNSVILLE, TN
52.0
52.0
1973
101


ATHENS, TN
52.0
52.0
1976
46


WYTHEVILLE 1 S, VA
39.0
41.0
1893
86


ABINGDON 3 S, VA
40.0
52.0
2006
36


BLACKSBURG NWSO, VA
40.0
46.0
1976
54


PULASKI 2 E, VA
40.0
43.0
1968
53


SALTVILLE 1N, VA
40.0
50.0
1968
49


GRUNDY, VA
42.0
47.0
1968
44


STAFFORDSVILLE 3 ENE, VA
42.0
48.0
2001
37


LURAY 5 E, VA
46.0
46.0
1976
66


STERLING RCS, VA
50.0
51.0
2002
31


WEST ALLIS, WI
43.0
44.0
1954
46


SNOWSHOE, WV
24.0
29.0
2005
31


TERRA ALTA #1, WV
31.0
40.0
1967
43


BELINGTON, WV
35.0
41.0
1976
41


ROWLESBURG 1, WV
36.0
40.0
1976
66


SUMMERSVILLE LAKE, WV
37.0
43.0
1976
41


BUCKEYE, WV
37.0
42.0
1968
46


FAIRMONT, WV
39.0
43.0
1952
102


ELKINS RANDOLPH CY AP, WV (KEKN)
39.0
39.0
1952
82


WESTON, WV
39.0
39.0
1925
106


CLARKSBURG 1, WV
39.0
44.0
1934
83


UPPER TRACT, WV
39.0
39.0
1910
38


OAK HILL, WV
40.0
45.0
1976
67


MORGANTOWN L&D, WV
40.0
42.0
1980
62


WEST UNION 2, WV
41.0
45.0
1976
35


MIDDLEBOURNE 3 ESE, WV
41.0
48.0
1980
66


GASSAWAY, WV
41.0
47.0
1952
54


PINEVILLE, WV
42.0
48.0
1976
62


GRANTSVILLE 1 ESE, WV
42.0
48.0
1976
43


BLUESTONE LAKE, WV
42.0
46.0
1976
65


DUNLOW 1 SW, WV
44.0
47.0
1997
36


RIPLEY, WV
44.0
44.0
1988
61


PARKERSBURG, WV
44.0
44.0
1952
82




Here are the 63 snowfall records:
Direct link to NOAA’s NCDC data for snowfall records
HTML clipboard



29 October 2008
Record
New (63)
Tied (0)
Previous
Record
Previous
Year
Period
of
Record


ASHFIELD, MA
1.5 in
0.0 in
2007
30


EAST BRIMFIELD LAKE, MA
0.1 in
0.0 in
2007
46


MC HENRY 2 NW, MD
9.0 in
2.0 in
2006
37


FROSTBURG 2, MD
3.4 in
0.7 in
2006
36


SANDUSKY, MI
0.5 in
Trace
1925
99


MAPLE CITY 1E, MI
0.3 in
Trace
1993
49


MARSHALL, NC
1.0 in
0.2 in
1910
109


GRANDFATHER MTN, NC
0.5 in
Trace
1973
53


MT WASHINGTON, NH (KMWN)
10.1 in
9.5 in
2000
60


POTTERSVILLE 2 NNW, NJ
2.0 in
0.0 in
2007
40


NEW BRUNSWICK 3 SE, NJ
1.5 in
0.0 in
2007
40


FLEMINGTON 5 NNW, NJ
1.0 in
0.8 in
1965
110


HOOKER 12 NNW, NY
19.0 in
3.5 in
1968
97


STILLWATER RSVR, NY
13.0 in
2.0 in
1990
83


TUPPER LAKE SUNMOUNT, NY
13.0 in
2.0 in
1934
109


LOWVILLE, NY
9.0 in
3.0 in
1893
116


PISECO, NY
8.0 in
1.0 in
2006
65


HIGHMARKET, NY
5.2 in
3.0 in
1965
84


NEWCOMB, NY
4.8 in
1.0 in
1965
49


CANTON 4 SE, NY
4.5 in
1.5 in
1962
115


INDIAN LAKE 2SW, NY
3.0 in
1.5 in
2006
109


ROCK HILL 3 SW, NY
2.3 in
0.0 in
2007
45


FRIENDSHIP 7 SW, NY
2.0 in
1.3 in
2006
39


LOCKE 2 W, NY
2.0 in
0.0 in
2007
76


BINGHAMTON WSO AP, NY (KBGM)
0.6 in
0.4 in
1952
60


JAMESTOWN 4 ENE, NY
0.5 in
0.0 in
2007
48


YOUNGSTOWN WSO AP, OH (KYNG)
1.6 in
0.6 in
1952
74


CLEVELAND WSFO AP, OH (KCLE)
0.3 in
Trace
2003
60


RIDGWAY, PA
6.0 in
Trace
1987
115


MEYERSDALE 2 SSW, PA
3.0 in
Trace
2006
45


DUNLO, PA
3.0 in
0.5 in
2006
60


SOMERSET, PA
2.8 in
1.4 in
2006
59


MAHANOY CITY 2 N, PA
2.1 in
0.0 in
2007
36


EBENSBURG SEWAGE PLT, PA
2.0 in
1.0 in
1965
44


KANE 1NNE, PA
2.0 in
1.0 in
1965
114


CONFLUENCE 1 SW DAM, PA
2.0 in
Trace
1965
62


MERCER, PA
2.0 in
Trace
1990
58


GLEN HAZEL 2 NE DAM, PA
2.0 in
1.5 in
2006
66


CHALK HILL 2 ENE, PA
1.2 in
Trace
1987
31


BOSWELL, PA
1.0 in
Trace
1965
48


PORT ALLEGANY, PA
1.0 in
0.5 in
2006
60


TIONESTA 2 SE LAKE, PA
0.8 in
0.5 in
1965
87


SLIPPERY ROCK 1 SSW, PA
0.7 in
Trace
2006
59


FRANCIS E WALTER DAM, PA
0.7 in
Trace
1990
45


PITTSBURGH WSCOM 2 AP, PA (KPIT)
0.6 in
0.4 in
1952
63


BUFFALO MILLS, PA
0.3 in
Trace
1965
84


MATAMORAS, PA
0.3 in
0.0 in
2007
104


MT MANSFIELD, VT
12.0 in
4.0 in
2006
53


ROCHESTER, VT
2.5 in
1.0 in
2000
79


MORRISVILLE 4 SSW, VT
1.4 in
Trace
2007
46


ESSEX JUNCTION 1 N, VT
1.2 in
Trace
2000
36


NEWPORT, VT
1.2 in
1.1 in
2000
78


ST ALBANS RADIO, VT
1.0 in
0.3 in
1992
30


CORINTH, VT
1.0 in
0.0 in
2007
60


SNOWSHOE, WV
8.0 in
1.0 in
1995
33


BAYARD, WV
5.5 in
1.5 in
1952
106


TERRA ALTA #1, WV
5.0 in
1.5 in
2006
60


GLADY 1 N, WV
4.4 in
Trace
2005
35


VALLEY HEAD, WV
3.2 in
2.0 in
1952
70


BELINGTON, WV
1.6 in
Trace
1968
70


BARTOW 1 S, WV
0.5 in
0.1 in
2006
64


ROCK CAVE 2 NE, WV
0.5 in
0.0 in
2007
55


SUTTON LAKE, WV
0.1 in
0.0 in
2007
91





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bbd0a86',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Two Stories for you, one about the snow itself, and the other about climate law being debated and passed in the middle of the unusual snow.- Anthony
London has first October snow in over 70 years
From the Guardian
Cold snap causes flight cancellations while a motorway accident kills one driver and causes severe disruption

Parts of south-east England had more than an inch of snow last night while London experienced its first October snowfall in more than 70 years as winter conditions arrived early.
Snow settled on the ground in parts of the capital last night as temperatures dipped below zero. A Met Office spokeswoman said it was London’s first October snow since 1934.
For greater south-east of England it was the first October snow since 1974. High Wycombe in Buckinghamshire had 3cm (1.2 inches). One of the coldest temperatures recorded was -4.1C in Benson, Oxfordshire.
“It is unusual to have snow this early,” the Met spokeswoman said. “In October 2003 sleet and snow was recorded in Northern Ireland, Wales, south-west, north-west and north-east England and the Midlands, but it was mainly over higher ground.”
read the entire story here
How Parliament passed the Climate Bill (in spite of the weather)
By Andrew Orlowski The Register
Posted in Government, 29th October 2008 12:35 GMT

Excerpt: Snow fell as the House of Commons debated Global Warming yesterday – the first October fall in the metropolis since 1922. The Mother of Parliaments was discussing the Mother of All Bills for the last time, in a marathon six hour session.
In order to combat a projected two degree centigrade rise in global temperature, the Climate Change Bill pledges the UK to reduce its carbon dioxide emissions by 80 per cent by 2050. The bill was receiving a third reading, which means both the last chance for both democratic scrutiny and consent.
The bill creates an enormous bureaucratic apparatus for monitoring and reporting, which was expanded at the last minute. Amendments by the Government threw emissions from shipping and aviation into the monitoring program, and also included a revision of the Companies Act (c. 46) “requiring the directors’ report of a company to contain such information as may be specified in the regulations about emissions of greenhouse gases from activities for which the company is responsible” by 2012.
Recently the American media has begun to notice the odd incongruity of saturation media coverage here which insists that global warming is both man-made and urgent, and a British public which increasingly doubts either to be true. 60 per cent of the British population now doubt the influence of humans on climate change, and more people than not think Global Warming won’t be as bad “as people say”.
Read the rest of the story at the Register, here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bd25ee1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterToday I’m coming out a day early and declaring July 2010 as the slowest melting July since the AMSR-E satellite record has been kept. The once ballyhooed “death spiral” is dead.
Reminds me of that line in Tarantino’s cult film Pulp Fiction:
“Who’s Zed?”
“Zed? Zed is dead.”
At the end of June I recall seeing lots of headlines in the newspapers about a record Arctic sea ice melt occurring. Words like “alarming” and “unprecedented” were used liberally. The reports were splashed with pictures of polar bears for added effect.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




One month later the media are completely silent. As the following graphic shows, this July’s Arctic sea ice melt was the slowest since this dataset has been kept.  Click Here.
Here are the numbers for the amount of July-melt in million square kilometers:
Year      6/30 to 7/30
2003           2.25
2004           2.08
2005           2.52
2006           2.11
2007           3.00
2008           2.45
2009           2.81
2010          1.85
It was the first time that July failed to reach 2 million sq. km. Now 2010 is on track to reach last year’s low. So far the Arctic has been cold this summer, one of the coldest summers north of 80°N on record, Click Here.
What’s the forecast?
Meteorologist Joe Bastardi projects a significant Arctic sea ice recovery in the couple of years ahead, flying in the face of predictions made by climate “scientists”. Bastardi’s claim is in line with the latest NOAA seasonal forecasts.
NWS/NCEP forecasts a cold Arctic in the months ahead.
La Nina is strengthening and global temps, dare I say, are beginning a death-spiral of their own.
Share this...FacebookTwitter "
"
Share this...FacebookTwitter25 Years Ago
Challenger 1986
The story of NASA’s (2nd) worst disaster. There was a “consensus” to launch.
 




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->







They will never be forgotten.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterLast Saturday polar bear poster child Knut died unexpectedly, much to the horror of viewers and fans internationally. Knut was only 4 years old. Polar bears normally live to be more than 30.
Of course, everyone is asking why. What was the cause of death?
Media reports say Knut had some sort of brain ailment. Activists are blaming it all on the Berlin Zoo.
German tabloid BILD claims that offcial cause of death was drowning. But Wolfgang Röhl at German blog achgut.com doesn’t buy it. He writes:
Please – since when do polar bears drown? No, there has got to be something else behind all of this. Professor Stefan Rahmstorf of Potsdam Institute for Climate Impact Research, you’re on!”
Yes, the “experts” can clear up all of this – with an authoritive peer-reviewed paper, of course.  The consensus is there after all; the science is done. Everybody today knows what every (bad) thing is caused by.
Share this...FacebookTwitter "
"

Despite rising federal deficits, Congress is set to pass another budget‐​busting spending bill. This time it is a $19 billion package of disaster‐​related subsidies.   
  
  
The _Washington Post_ reports “taxpayer spending on U.S. disaster fund explodes.” It documents increases in disaster spending by the Federal Emergency Management Agency (FEMA). In a typical recent year, “spending on the federal disaster relief fund is almost 10 times higher than it was three decades ago, even after adjusting for inflation.”   
  
  
The story identifies two causes of the spending increases: climate change and population growth in disaster‐​prone areas. But it ignored perhaps the most important cause: increased federal intervention in the sorts of emergencies that used to be handled by the states, as I discuss here.   
  
  
The _Post_ is correct that more Americans are moving into disaster‐​prone areas:   




Many more Americans have moved into harm’s way, with growth exploding in the Gulf Coast region and along the Continental Divide, where tornadoes frequently occur, according to a study on the “expanding bull’s eye effect” by Stephen M. Strader of Villanova University and Walker S. Ashley of Northern Illinois University.   
  
  
Since 1970, 35 million more people and their homes have moved to coastal shoreline “in the direct path of potentially devastating storm surges,” the researchers found, a 40 percent increase.   
  
  
“We’ve put more stuff in the wrong place the wrong way,” said W. Craig Fugate, a former FEMA administrator under President Barack Obama. “We’ve got a lot more stuff — bigger houses, multiple cars, more people — in high‐​hazard areas.”



More people are also living in fire‐​prone areas of California.   
  
  
The _Post_ does not explore an important reason why Americans are moving into these areas: government subsidies. Federal subsidies for flood insurance, flood control structures, beach replenishment, and disaster rebuilding have encouraged development in coastal areas, as I discuss here. Meanwhile, state policies have contributed to building in California’s fire‐​prone areas.   
  
  
American governments are not alone in pursuing policies that increase disaster hazards. A World Bank / United Nations study identified such policies in numerous countries and discussed market‐​based reforms to mitigate risks.   
  
  
In the United States, federalism is supposed to undergird our system of handling disasters, particularly natural disasters. Under the 1988 Stafford Act, the federal government is supposed to get involved in disasters only if they are of “such severity and magnitude that effective response is beyond the capabilities of the state and the affected local governments.”   
  
  
However, presidents and congresses have increasingly ignored this limit. The number of presidential disaster declarations has soared and the costs of disaster bills have increased as politicians shoe‐​horn subsidies unrelated to immediate emergency response into bills.   
  
  
Growing federal intervention is undermining the role of the states and private institutions in handling disasters. This intervention stems from politics not practical benefits. State and local governments and the private sector are better positioned to handle most disaster response. Also, states, cities, and private utilities aid each other during disasters.   
  
  
Rising FEMA spending is not a good metric for measuring the severity of natural disasters striking the United States. Rather, it reflects growing populations living in risky areas and growing disregard for federalism in disaster‐​related response and rebuilding.   



"
"

Spurred by tax competition, the flat tax revolution continues to generate positive results. Albania will have a 10 percent flat tax beginning in January 2008. The corporate rate also will be 10 percent, as will the payroll tax. The latter reform is particularly interesting since many of the flat tax nations in Eastern Europe retain punnitive payroll tax rates — a policy that undermines the pro‐​growth and pro‐​employment effects of the flat tax. The _Southest European Times_ reports: 



In a bid to promote growth and improve the business climate, the administration of Albanian Prime Minister Sali Berisha plans a major overhaul of the tax system. The biggest change is a switch to a flat tax. “As of January 1st, 2008, Albania will have implemented the 10% flat tax system, one of the lowest in Europe,” Berisha told a business community meeting in late March. Corporate taxes, currently at 20%, are to be slashed in half. Social security contributions from businesses will likewise be capped at 10%. The government and other supporters of the reform say it will widen the taxable base and simplify tax administration, while also making Albania an easier place to invest. According to Finance Minister Ridvan Bode, the changes will lead to a more streamlined fiscal system. “The flat tax helps eliminate the potential arbitrage between corporate tax, dividend taxes and the income tax,” he says. VAT and other taxes will also be gradually reduced in order to woo investors, the minister added. …In the past, the IMF has been wary of plans to reduce taxes in Albania. This time, however, it seems more receptive — provided the overhaul is combined with more effective revenue collection. “We will negotiate with the Albanian government about the tax reduction, depending on the tax collection,” IMF representative Ann Margaret Westin told the press.
"
"
I’m sure we’ll see other dead people talking about climate change soon. Jerry Garcia perhaps? Albert Einstein, John Lennon, why maybe even John Wayne could be utilized. I can see him now, “Listen up Pilgrim…climate change is gonna kill you unless you get off your sorry butt and do something about it!”.
We all know these long dead people had opinions about climate change, but they just never had a chance to express them before they died. Right?
Anything to “save the planet”, including putting words in dead people’s mouths. – Anthony
h/t to Jeff Alberts
Greenpeace Resurrects JFK for Global Warming Ad Campaign
Web video depicts dead president warning climate change ‘threatens our very existence,’ claims ‘technology and renewable energy offers the last remaining hope.’
By   Jeff Poor
Business & Media Institute
10/30/2008 1:31:03 PM
There’s something a little creepy about historical figures being brought back to life to promote climate change alarmism, but the over-the-top environmentalists at Greenpeace have no qualms with using it as a tactic.
A video posted on Greenpeace’s YouTube site portrays former President John F. Kennedy, who was assassinated Nov. 22, 1963, in Dallas, making a plea for environmental activism to save the planet from the perils of global warming.
“When man first walked upon the moon, it defined a generation,” Kennedy is depicted saying. “As this new millennium dawns, we face a greater challenge – climate change threatens our very existence. What further disasters will convince world leaders that the existing technology and renewable energy offers the last remaining hope for sustainable future?”
The ad is part of a Greenpeace campaign labeled “Energy [R]evolution” that sets greenhouse gas goals far in excess of the Kyoto treaty. The ad promotes the eradication of coal-fired plants, using more expensive unproven sources of energy, blames industrialized nations for the plight of poor nations, and calls for a radical overhaul of the European auto industry.
Watch the video:

Other aspects of the campaign include an energy policy based on “equity and fairness,” reduction of greenhouse gas emission by “up to 30 percent below 1990 levels,” “strict mandatory efficiency standards” for home and office appliances and the phasing out of nuclear power, which has been deemed as a greenhouse gas-free energy solution to the climate change issue by some.
The ad makes an emotional plea by linking climate change disaster scene after disaster scene, including a post-Hurricane Katrina shot from Reynoir St. in Biloxi, Miss, followed by shots of children, calling for an “energy revolution.”
“Hollow words and spineless resolution have failed,” the voice continued. “Now is the time for an energy revolution. Will we look into the eyes of our children and tell them that we had the opportunity but lacked the courage? Will we look into the eyes of our children and tell them that we had the technology, but lacked the vision? Or, will we look into the eyes of our children and tell them that we faced our challenge and that we fought – we fought for the energy revolution?”
Kennedy is considered by some historians to be a great orator. However, the choice to use his likeness in the ad is curious because, although he is credited for laying some the foundation of modern federal environmental policy, it was his Republican rival and successor, President Richard Nixon, who made the Environmental Protection Agency a reality in 1970.
This isn’t the first time environmental messages have been linked to historical images. Al Gore’s “We Can Solve It” campaign had one commercial spot that began with video from the D-Day invasion of Normandy and included clips from the moon landing and the civil rights movement. Time magazine doctored the famous Iwo Jima photograph by Joe Rosenthal of the Marines raising the American flag and replaced the flag with a tree to push the “war on global warming.”
The use of deceased celebrities in advertising has been considered controversial by some. A recent DirectTV ad starring Craig T. Nelson as the father in the 1982 movie “Poltergeist,” shows the daughter – played by Heather O’Rourke – reciting the movie’s memorable line, “Theeeyyy’re heeerrre” However, O’Rourke died in 1988 and some critics claimed that crossed the line.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b67d40d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Not long ago, a group of Cato scholars entertained the question of whether the intellectual debate for free trade had been won.



There was near consensus that it had — in 1776 with publication of _The Wealth of Nations_. In the 240 years to follow, efforts to poke substantive holes and refute Adam Smith’s treatise failed and, today, nearly all economists agree that free trade, by expanding the size of the market to enable greater specialization and economies of scale, generates more wealth than any system that restricts cross‐​border exchange.



What that Cato confab failed to produce was agreement about whether the question under consideration was even pertinent. After all, how much does it really matter whether the intellectual debate has been won when, in practice, free trade remains stubbornly elusive, and the process of U.S. trade policy formulation is distinctly antiintellectual? Consider trade agreements. At the heart of negotiations that produce these deals rests the fallacy that domestic trade barriers are assets to be dispensed with only if reciprocated, in roughly equal measure, by negotiators on the other side of the table. That’s not Adam Smith. That’s neo‐​mercantilism, which posits that policy should aim to maximize exports and minimize imports. Yet Smith is credited with vanquishing mercantilism, which held sway in his day — and apparently still does today.



If the free trade consensus were truly meaningful, trade negotiations would be unnecessary. If free trade were the rule, trade policy would have a purely domestic orientation and U.S. barriers would be removed without need for negotiation because they would be recognized for what they are: taxes on consumers and businesses that impede the global division of labor and the creation of wealth. Apparently, the intellectual consensus for free trade coexists with an absence of free trade and a persistence of protectionism in practice.



For example, in the United States, there are “Buy American” rules that restrict most government procurement spending to U.S. suppliers, ensuring that taxpayers get the smallest bang for their buck; heavily protected services industries, such as transportation and shipping, that drive up the cost of everything; apparently interminable farm subsidies; quotas and high tariffs on imported sugar; high tariffs on basic consumer products, such as clothing and footwear; energy export restrictions; the market‐​distorting cronyism of the Export‐​Import bank; antidumping duties that strangle downstream industries and tax consumers; regulatory protectionism masquerading as public health and safety precautions; rules of origin and local content requirements that limit trade’s benefits; restrictions on foreign investment, and so on.



If an intellectual consensus for free trade exists, policy doesn’t reflect it and politicians appear to abhor it. If anything, the 2016 presidential election season reveals an American public — pitchforks and scythes in hands — ready to storm the ivory tower.



 **TRADE IS RIPE FOR DEMAGOGUERY**  
To cheering crowds, Donald Trump promises to slap duties on imports from China and Mexico and to use the tax code to punish U.S. companies that outsource parts of their operations abroad. Bernie Sanders vows to tear up NAFTA and other free trade agreements, calling them “a disaster for American workers.” Hillary Clinton, a co‐​architect of the Trans‐​Pacific Partnership trade agreement (TPP), now opposes that deal, while promising to disregard certain U.S. treaty obligations with China. Ted Cruz, projecting the pain of workers who have been displaced by import competition and outward investment (but, apparently, not those displaced by technology, changing consumer tastes, or poor business management), says trade has been “unfair” and pledges to “bring our jobs back from China.”



Scapegoating trade for problems real and imagined is nothing new. Blaming the Japanese, Mexicans, Chinese, and other foreigners for domestic woes ingratiates politicians to excitable elements of the electorate and helps them direct voter anger away from their own records. It has become a kind of quadrennial tradition ever since the NAFTA debate took center stage in the 1992 election.



Throughout the 2012 campaign, Mitt Romney assailed President Obama for failing to label China a “currency manipulator,” and the candidates exchanged accusations about who was more “culpable” for “shipping jobs overseas.” Promising to bring manufacturing jobs back home, Rick Santorum resonated with trade‐​skeptical voters, and even won the Iowa caucus that year. In 2008 Senators Obama and Clinton vied to be seen as the supreme trade‐​rules enforcer, each pledging to force U.S. trade partners back to the table to renegotiate NAFTA and various World Trade Organization agreements to make the terms “fair” for American workers. Demonization of trade was also a major component of John Edwards’s divisive “Two Americas” message that year.



John Kerry tapped into the same vein of public anxiety in 2004, referring to U.S. businesses that outsource call centers to places like India as “Benedict Arnold” companies. Blaming Mexico, Japan, and inside‐​the‐​beltway complicity for U.S. manufacturing decline and the erosion of American power, Pat Buchanan promised to punch back with force. His populist message energized the feisty “Buchanan Brigades” and helped him win the New Hampshire primary in 1996.



Trade‐​bashing became popular during the 1992 election, as books about the United States “trading places” with an ascendant Japan flew off the shelves and Ross Perot warned of the imminence of a “giant sucking sound” coming from south of the border. So campaigning politicians denigrating trade is nothing new. It seems to be inextricably woven into the fabric of our presidential elections. But something seems different this year. The tone is harsher. The digs are coming from across the political and ideological spectra. Two of the candidates — Sanders and Trump — seem genuine in their antipathy and their resolve to act. And their messages resonate especially well with primary election voters, who tend to hail from the extremities of the major parties, where trade and globalization are viewed with the greatest skepticism. But, again, these constituencies and their concerns aren’t particularly new either.



What is new — at least for the first time since NAFTA loomed large 24 years ago — is that a major trade agreement (indeed, the largest preferential trade agreement in U.S. history) is being debated and possibly considered for ratification by the U.S. Congress this year. Trade policy has featured prominently in the public square since January 2015, when the president and the new congressional leadership began their push to secure passage of Trade Promotion Authority (TPA) to facilitate completion and ratification of the TPP and, eventually, the Transatlantic Trade and Investment Partnership.



Although the TPA debate itself was shortlived, with the legislation passing in June of last year, anti‐​trade lobbies such as the Sierra Club, the AFL-CIO, and Public Citizen have been mobilizing for several years in anticipation of an epic battle over the TPP. Their anti‐​trade campaigns, with assertions and slogans evoking fantastical worst‐​case scenarios about the relationship between trade and climate change, trade and cancer rates, and trade and joblessness have played to popular fears, and have succeeded in winning more people to their cause. Protectionist ranks have been augmented by those with other kinds of economic grievances in a way that evokes _New York Times_ columnist Thomas Friedman’s 2001 description of the antiglobalization movement as the “well‐​intentioned but ill‐​informed being led around by the ill‐​intentioned and well‐​informed.” Though they are not necessarily wellinformed, the 2016 presidential candidates are complicit in creating this climate of misinformation.



 **UNSEEN CREATION**  
The case for free trade is not obvious. The benefits of trade are dispersed and accrue over time, while the adjustment costs tend to be concentrated and immediate. To synthesize Schumpeter and Bastiat, the “destruction” caused by trade is “seen,” while the “creation” of its benefits goes “unseen.” We note and lament the effects of the clothing factory that shutters because it couldn’t compete with lower‐​priced imports. The lost factory jobs, the nearby businesses on Main Street that fail, and the blighted landscape are all obvious. What is not so easily noticed is the increased spending power of the divorced mother who has to feed and clothe her three children. Not only can she buy cheaper clothing, but she has more resources to save or spend on other goods and services, which undergirds growth elsewhere in the economy.



Consider Apple. By availing itself of lowskilled, low‐​wage labor in China to produce small plastic components and to assemble its products, Apple may have deprived U.S. workers of the opportunity to perform that low‐​end function in the supply chain. But at the same time, that decision enabled iPods and then iPhones and then iPads to be priced within the budgets of a large swath of consumers. Had all of the components been produced and all of the assembly performed in the United States — as President Obama once requested of Steve Jobs — the higher prices would have prevented those devices from becoming quite so ubiquitous, and the incentives for the emergence of spin‐​off industries, such as apps, accessories, Uber, and AirBnb, would have been muted or absent.



But these kinds of examples don’t lend themselves to the political stump, especially when the campaigns put a premium on simple messages. This is the burden of free traders: Making the unseen seen. It is this asymmetry that explains much of the popular skepticism about trade, as well as the persistence of often repeated fallacies.



 **THE MYTHS**  
One of the most frequently invoked trade myths is the portrayal of trade as a competition between “us” and “them.” Central to this perception is that exports are Team America’s points, imports are the foreign team’s points, and the trade account is the scoreboard. Since that scoreboard shows a deficit, the United States is losing at trade, and it’s losing because the foreign team cheats — too often with impunity. Sound familiar?



This fundamental mercantilist fallacy about the nature of trade has a nationalistic appeal, where America is some monolithic entity best served by policies that strengthen her stature vis‐​à‐​vis some foreign monolith. But trade does not occur between countries. Trade is the culmination of billions of daily transactions pursued by individuals seeking value through exchange.



When we transact at the local supermarket, we seek to maximize the value we obtain by getting the most for our dollars. We strive to “import” more than we “export.” But when it comes to trading across borders or when our individual transactions are aggregated at the national level, we tend to forget these basic principles and accept the fallacy that the goal of trade is to achieve a surplus. But, as Adam Smith put it: “What is prudence in the conduct of every private family can scarce be folly in that of a great kingdom.” Never mind the intellectual consensus: This is common sense.



The benefits of trade come from imports, which deliver more competition, greater variety, lower prices, better quality, and new incentives for innovation. Arguably, opening foreign markets should be an aim of trade policy because larger markets allow for greater specialization and economies of scale, but real free trade requires liberalization at home. The real benefits of trade are measured by the value of imports that can be purchased with a unit of exports — our purchasing power or the so‐​called terms of trade. Trade barriers at home raise the costs and reduce the amount of imports that can be purchased with a unit of exports.



And as a result of globalization — the proliferation of cross‐​border investment and transnational supply chains — trade is more of a collaboration than ever before. Typically, about half of the value of U.S. imports is composed of intermediate goods and capital equipment — the purchases of U.S. producers.



How can imports be viewed as the other team’s points under those circumstances? Who, in fact, are “we” and who are “they”? The claim that the trade deficit means we are losing at trade — “losing billions of dollars every year to China and Mexico,” as Trump characterizes it — is another commonly invoked trade myth, which reflects a fundamental misunderstanding of international economics. By purchasing more goods and services from foreigners than foreigners purchase from Americans — trade deficit scolds claim — U.S. factories, farmers, and service providers are deprived of sales, which reduces domestic output, value added (GDP), and employment. That conclusion relies on the assumption that the dollars sent to foreigners to purchase imports do not make their way back into the U.S. economy. The dollars that go abroad to purchase foreign goods and services (imports) and foreign assets (outward investment) are matched nearly identically by the dollars coming back to the United States to purchase U.S. goods and services (exports) and U.S. assets (inward investment). Any trade deficit (net outflow of dollars) is matched by an investment surplus (net inflow of dollars).



This process helps explain why GDP and the trade deficit rise and fall in tandem, and why 41 consecutive years of trade deficits have had no adverse impact on the economy. The fallacy that trade killed U.S. manufacturing has long been a pretense for protectionism or industrial policy. Trump follows in these footsteps when he writes:





U.S. manufacturing is not only alive, it’s thriving. By all relevant metrics — output, value‐​added, revenues, exports, imports, investment, R&D expenditures — U.S. manufacturing remains a global “powerhouse.” With respect to most of those measures, year after year the sector sets new records. U.S. manufacturing attracts more foreign direct investment (FDI) than any other country’s manufacturing sector. In 2014 the stock of FDI in U.S. manufacturing surpassed $1 trillion, more than double the value of FDI in China’s manufacturing sector (and eight times the value in per capita terms).



If by “rapid deindustrialization” Trump means that manufactured goods account for a smaller share of U.S. output than in the past, he’s right about the statistic, but not the interpretation. Manufacturing’s share of the U.S. economy peaked in 1953 at 28.1 percent, whereas today manufacturing accounts for only 12.1 percent of GDP. But in 1953 U.S. manufacturing value added amounted to $110 billion, as compared to a record $2.1 trillion in 2015 — more than six times the value in real terms.



Bernie Sanders is wary of capitalism and in favor of equality of outcome. He perpetuates another common myth: Trade only benefits multinational corporations and the rich. But nothing could be further from the truth. Just like during the Gilded Age, the tariff remains the mother of the trust. And, like then, free trade should be the progressive position.



Protectionism benefits producers over consumers; it favors big business over small business because the cost of protectionism is relatively small to a bigger company; and, it hurts lower‐​income more than higherincome Americans because the former spend a higher proportion of their resources on imported goods.



The United States has relatively low tariffs on average — less than 2 percent. But tariffs on clothing (18 percent), footwear (14 percent), and food products (10 percent) are especially high. Meanwhile, U.S. antidumping restrictions on steel, lumber, cement, appliances, flooring, nails, and paint elevate the material costs of home building. Imports of life’s basic necessities — food, clothing, and shelter — are subject to some of the highest taxes. Why isn’t that too regressive for a progressive like Sanders?



 **WHAT DOES THE RHETORIC PORTEND?**  
Demagoguing trade has become an election year pastime. But trade issues tend to be of marginal concern to voters in the general election, and history suggests that cooler heads will prevail. Despite the abundance of antitrade rhetoric on the campaign trail, it is difficult to imagine an actual president of the United States supporting policies commensurate with the bluster. Every president since FDR, regardless of political party, has embraced or promoted trade liberalization.



While candidates might rail against unfair trade practices and unlevel playing fields on the stump, they change their tunes after taking the oath. Presidents prioritize broader, national interests over regional and parochial issues, and tend to see merit in projecting global economic leadership. They also view trade policy through the prism of foreign policy, and recognize the contributions that trade makes to economic growth and international stability.



Even if there were a President Trump or President Sanders, rest assured that the Congress still has authority over the nuts and bolts of trade policy. The scope for presidential mischief, such as unilaterally raising tariffs, or suspending or amending the terms of trade agreements, is limited. But it would be more reassuring still if the intellectual consensus for free trade were also the popular consensus.



What matters most is that Americans have realized progressively greater freedom to transact with people in other countries over the years. Many barriers still remain. But when the evidence of the economic benefits of liberalization is weighed against the myths and political aspersions, trade is exonerated on all counts.
"
"

The impact of state business taxation on employment and capital has been heavily debated in both academic and policy circles on both theoretical and empirical grounds. State‐​level business taxation could depress business activity through several channels. Businesses that might otherwise have hired or invested might simply not do so because of the difference between pre‐​tax and after‐​tax profits, or, alternatively, businesses might move their activities to another U.S. state. On the other hand, increased business taxation might not have a negative effect on business activity if businesses can change their activities to use more tax‐​favored production strategies or organizational forms, or if tax revenues are spent on public goods that improve the state’s business climate. As U.S. states face increasing fiscal pressures, the debate over the effects of state tax policy on state‐​level business activity is likely to intensify.



A long empirical literature has studied the geographic location decisions of new firms or establishments as a function of state tax and other characteristics. These studies have used aggregated panel data at the state, county, or industry level to examine the effect of state and local taxes on economic growth, employment, or capital formation. This line of work has faced two main challenges. First, tax policy is not exogenously determined, so that ascribing a causal interpretation to correlations between state tax changes and counts of businesses or employees has been problematic. The primary concern is that state governments might change tax policy in anticipation of changing economic conditions. Some work addresses this question by using county‐​level data to study how state taxation or business climates affect business activity in border counties between states that change policy and those that do not. The second challenge is that the studies have lacked comprehensive micro‐​data at the establishment level, so that the decisions of individual businesses cannot be tracked over time, leaving uncertainty as to whether firms are relocating their businesses to other regions or reducing the scale of their operations.



Our research uses comprehensive and fully disaggregated establishment‐​level data from the U.S. Census Bureau to examine the impact of state business taxation on employment and capital. We focus on firms with establishments in multiple states. To measure an effect of state tax policy on establishment counts, employment, and capital, we begin by exploiting the fact that the corporate tax code only has a direct effect on firms organized as subchapter C corporations, whereas firms organized as S corporations, partnerships, or sole proprietorships (so‐​called pass‐​through entities) are only directly affected by the individual tax code and other business taxes.



This setting allows for separate identification of the effects of the corporate tax code on the activities of C corporations and of the effects of the personal tax code on the activities of pass‐​through entities. Furthermore, the establishment‐​level micro data allow us to disentangle reallocation versus pure economic disincentives of taxation. Specifically, we examine whether firms increase their activities in a given state when taxes increase in the other states in which they are active.



We consider the complete sample of all U.S. establishments from 1977–2011 belonging to firms with at least 100 employees and having operations in at least two states. We find that a one percentage point increase (decrease) in the state corporate tax rate leads to the closing (opening) of 0.03 establishments belonging to firms organized as C corporations in the state. This corresponds to an average change in the number of establishments per C corporation of 0.4 percent. A similar analysis shows that a one percentage point change in the state personal tax rate affects the number of establishments in the state per pass‐​through entity by 0.2–0.3 percent. On the intensive margin, we find similar results. The elasticity of C corporation employment for a given establishment is 0.4 with respect to the state corporate income tax rate, and the elasticity of pass‐​through business employment is 0.2 with respect to the personal income tax rate. These effects are robust to controls for local economic conditions and heterogeneous time trends. 



Opposite effects of around half of these magnitudes are observed in response to tax changes in the other states in which firms operate, so that around half of the baseline effect is offset by reallocation of activity across states. This lends strong support to the view that tax competition across states is economically relevant and is consistent with earlier findings that emphasize the importance in the labor market of shifts in the distribution of employment opportunities across work sites.



We find no empirical correlation in the data between changes in employment at an establishment belonging to a C corporation and the personal tax rate. Similarly we find no empirical correlation between changes in employment at an establishment belonging to a pass‐​through entity and the corporate tax rate. The lack of cross‐​correlation is consistent with the identifying assumption in these regressions that there are not state‐​level trends in general business activity that follow changes in tax policy for reasons unrelated to the tax reforms. This finding also suggests that movement of activity between the corporate and noncorporate sector, while clearly important on the national level over the past several decades, plays a somewhat limited role in shaping the overall economic response to state‐​level tax changes.



Further analysis captures complexities, heterogeneity, and changes in state tax codes regarding apportionment of income in multi‐​state firms. If a company has physical presence in more than one state, the company has to apportion its profits according to each state’s apportionment factor weights for property, payroll, and sales. Furthermore, so‐​called throwback or throw‐​out rules at the state level require companies to apportion profits arising from sales into states where they have no physical presence back to states where they do. 



We show that the response of moving establishments and employees is greatest when those factors have greater apportionment weights. Even if the sales apportionment factor is large, we also find strong effects when throwback or throw‐​out rules are in effect, as these rules mitigate the tax attractiveness of firm moves to high sales‐​apportionment states.



We further examine whether there is evidence of confounding differential trends in C corporations versus pass‐​through entities in the years leading up to tax changes in a subsample of firms affected by states that changed at least one of their tax rates by at least 100 basis points. These large tax changes occurred 161 times during the sample period. Here we find similar directional results of somewhat smaller magnitude. Around half of the effects are felt in the tax year in which the tax rate changed, with the full force being felt in the following year.



Analysis on the subset of the Census data on manufacturing firms allows us to consider the impact of state taxation on capital formation and location. We find that capital shows similar directional patterns to labor in its response to taxation, but that the elasticities are 36 percent smaller. Furthermore, the detailed data on the location of manufacturing firm property allow us to consider the impact of a state policy that raises the actual tax claim on a dollar of total corporate profit by one percentage point, as opposed to increases in the statutory rate by one percentage point. Under this definition, which captures cross‐​sectional firm heterogeneity in the extent to which statutory changes affect the tax burden, we find somewhat larger elasticities of around 0.4 percent for labor and 0.3 percent for capital.



One concern about the analysis might be that the results could be affected by firms that change their organizational form in response to changes in the tax code. Earlier work finds that the share of economic activity by firms in corporate form does in fact respond to the relative taxation of personal to corporate state income. However, our sample in this paper consists only of firms with activity in more than one state, and firms must choose one organizational form that will be applicable to all entities. For these firms, federal tax policy should be far more important for the organizational form decision than the mix of state tax policies they face, a hypothesis we confirm in the data. Limiting the sample to the 92 percent of observations belonging to firms that do not change their organizational form within 5 years of tax changes leaves the results unaffected.



Overall, our findings on the effects of corporate taxation are larger than those found in work that has examined the impact of tax policy at the national level. Some of this difference may be attributable to differences in the measurement of corporate tax rates (average versus marginal), the level of analysis (state versus federal), the identification strategy and the distinction between GDP per capita and the variables we consider. That said, in our analysis, tax competition across states roughly doubles the baseline effects that would be found in the absence of firms’ ability to move across states, and for that reason we would extrapolate that the impact of state policy on state business activity should be about double the impact of federal business taxation on federal business activity.



 **Note**



This research brief is based on Xavier Giroud and Joshua Rauh, “State Taxation and the Reallocation of Business Activity: Evidence from Establishment‐​Level Data,” National Bureau of Economic Research Working Paper no. 21534, September 2015.
"
"

During the 2017 presidential campaign, then‐​candidate Donald Trump was open about his hostility toward Iran and his disdain for the Obama administration’s diplomacy with that country. Since January, the Trump administration has been engaged in an Iran policy review. News reports and leaks suggest the review is highly likely to recommend a more confrontational approach to Iran, whether within the framework of the Iranian nuclear deal or by withdrawing from it. This paper examines the costs of four confrontational policy approaches to Iran: sanctions, regional hostilities, “regime change from within,” and direct military action.



Increased economic sanctions are unlikely to succeed in producing policy change in the absence of a clear goal or multinational support. Indeed, sanctions on Iran are likely to meet with strong opposition from U.S. allies in Europe and Asia, who continue to support the nuclear deal. The second policy we examine — challenging Iranian proxies and influence throughout the Middle East — is likewise problematic. There is little coherent, effective opposition to Iran in the region, and this approach increases the risks of blowback to U.S. forces in the region, pulling the United States deeper into regional conflicts.



The third option, so‐​called regime change from within, is a strategy that relies on sanctions and on backing for internal Iranian opposition movements to push for the overthrow of the regime in Tehran. This approach is not feasible: regime change — whether covert or overt — rarely succeeds in producing a stable, friendly, democratic regime. The lack of any good candidates for U.S. support inside Iran compounds this problem. The final policy alternative we explore is direct military action against Iranian nuclear or military facilities. Such attacks are unlikely to produce positive outcomes, while creating the risk of substantial escalation. Worse, attacking Iran after the successful signing of the nuclear deal will only add to global suspicions that the United States engages in regime change without provocation and that it cannot be trusted to uphold its commitments.



We suggest an alternative strategy for the Trump administration: engagement. This approach would see America continue to uphold the nuclear deal and seek continued engagement with Iran on issues of mutual interest. Engagement offers a far better chance than confrontation and isolation to improve Iran’s foreign policy behavior and empower moderate groups inside Iran in the long term.



In July 2015, the P5+1—the United States, United Kingdom, France, Russia, China, and Germany—reached a diplomatic agreement with Iran to roll back and significantly limit the Iranian nuclear program in exchange for the lifting of economic sanctions. The Joint Comprehensive Plan of Action (JCPOA) was the result of years of meticulous diplomatic negotiations and represented an historic compromise between two long‐​standing adversaries, the United States and Iran. At the time, the Obama administration presented the agreement as a strict nonproliferation agreement that would extend Iran’s so‐​called breakout time—the time it would take Iran to “sprint” to the creation of a useable nuclear weapon—from a few months to a year or longer. Many also hoped that the JCPOA could help to reduce bilateral tensions and quiet calls for U.S. military action against Iran for the foreseeable future. The unexpected election of Donald Trump in 2016 dashed these hopes. With renewed tensions and open debate within the Trump administration as it conducts a “comprehensive review of our Iran policy,” the future of the JCPOA and of U.S.-Iranian relations is uncertain. 1 There are certainly many options for the Trump administration if it wishes to take a more confrontational approach to Iran, four of which are examined in this paper. Yet each is difficult, costly, and carries far higher risks than continuing a policy of engagement.



The JCPOA has been successful, placing strong restrictions on Iran’s ability to engage in even peaceful nuclear development. Iran removed 98 percent of its stockpile of enriched uranium, dismantled two‐​thirds of its uranium enrichment centrifuges, disassembled the core of its heavy water reactor (a potential source of weapons‐​grade plutonium), and converted two major enrichment sites into peaceful research facilities. In addition, Iran agreed to engage in uranium enrichment exclusively at a single facility—the Natanz complex—and to produce only low‐​enriched uranium for 10 years. Because uranium must be enriched to 90 percent for use in a nuclear weapon, Iran’s agreement to restrict enrichment to 3.67 percent constitutes a significant barrier to weapons development. Iran also agreed to limit its stockpile of low‐​enriched uranium to 300 kilograms for 15 years, making it extremely difficult to covertly enrich excess material. 2



To ensure compliance with the JCPOA’s restrictions, Iran agreed to submit what remained of its nuclear program to what Georgetown University’s Ariane Tabatabai describes as “the most intrusive inspections regime ever voluntarily agreed to by any party.” 3 International monitors perform daily inspections of all of Iran’s declared facilities, with some facilities subject to 24‐​hour video surveillance. As critics note, these inspections and many of the deal’s other restrictions eventually expire, phased out over the next 10 to 25 years. 4 As part of the deal, however, Iran rejoined the Nuclear Nonproliferation Treaty (NPT) and ratified its Additional Protocol, a provision that mandates inspections of Iran’s civilian nuclear facilities. In doing so, Iran made a commitment to never become a nuclear weapons state and agreed to monitoring under the NPT indefinitely, far beyond the life of the JCPOA.



Indeed, more than two years after the adoption of the JCPOA, Iran is in full compliance with the deal. Though there has been some debate about the interpretation of certain issues—Iranian missile testing and the extent of U.S. sanctions relief—the deal continues to be implemented by both sides. As of this writing, the International Atomic Energy Agency (IAEA) has reported eight times that Iran is meeting its obligations under the deal. 5 Even the Trump administration, despite public denigration of the agreement, has formally certified that Iran is fulfilling its JCPOA commitments. In exchange, economic sanctions related to Iran’s nuclear program have been lifted, including United Nations and European Union sanctions on Iran’s energy sector and a variety of U.S. secondary sanctions related to Iran’s financial and energy sectors. 6 In addition, Iran has regained access to wealth stored in offshore banks previously interdicted by sanctions. 7



Nonetheless, the change in presidential administration has altered the political climate surrounding the nuclear deal in Washington, D.C. There have been prominent calls from both within the Trump administration and outside it to kill the JCPOA. As a candidate, Donald Trump himself repeatedly boasted that his “number‐​one priority is to dismantle the disastrous deal with Iran,” which he described in his typical hyperbole as “the worst deal ever negotiated.” 8 The recertification process (required every 90 days) has become increasingly politicized as a result: in July 2017, some advisers persuaded the president to refuse certification of Iran’s compliance with the JCPOA, only for other advisers to succeed in persuading him, at the last minute, to accept the IAEA’s conclusions and certify compliance. 9 Trump told journalists following the episode that he intends not to repeat the incident, reportedly informing White House staff that “he wants to be in a place to decertify 90 days from now and it’s their job to put him there.” 10 As David S. Cohen, former deputy director of the Central Intelligence Agency (CIA), notes, President Trump’s “reported demand for intelligence to support his policy preference to withdraw from the Iran nuclear deal risks politicizing intelligence analysis, with potentially grave consequences.” 11



Calls to end the deal have also come from outside the administration. In July, Sens. Tom Cotton (R-AR), Ted Cruz (R-TX), David Perdue (R-GA), and Marco Rubio (R-FL) wrote a letter to Secretary of State Rex Tillerson to “urge that you not certify … that Iran is complying with the terms of the [JCPOA].” 12 John Bolton, United Nations ambassador under George W. Bush and an early candidate to be Trump’s secretary of state, called for bombing Iran’s nuclear facilities months before the JCPOA was signed. 13 In July 2017, he wrote, “withdrawing from the JCPOA as soon as possible should be the highest priority.” 14



Opponents of the deal have little factual basis for their arguments: the IAEA has repeatedly found Iran in compliance with the deal’s restrictions, and the Joint Commission of the JCPOA has not identified any violations. 15 Instead, opponents typically argue that Iran is violating the “spirit” of the deal, pointing to Iran’s ballistic missile tests or its support for violent groups throughout the Middle East. 16 Yet the JCPOA was narrowly written specifically to exclude non‐​nuclear questions; it was never intended to solve all problems in the U.S.-Iranian relationship. Ironically, if any JCPOA signatory is in violation of the deal, it may be the United States. 17 At the G-20 summit in July, President Trump reportedly urged fellow world leaders to stop doing business with Iran, an action that violates the American commitment under the JCPOA to “refrain from any policy specifically intended to directly and adversely affect the normalization of trade and economic relations with Iran.” 18



President Trump appears determined to undermine the JCPOA. The administration is considering using the deal’s “snap inspections” provision—which allows inspectors to demand access to undeclared sites in Iran reasonably suspected of illicit enrichment activity—to make Iran appear noncompliant. 19 In the absence of any clear evidence of illicit enrichment activity, Iran would likely decline the Trump administration’s demand to inspect undeclared military sites, allowing the White House to portray Iran as violating the deal. As Mark Fitzpatrick, executive director of the International Institute for Strategic Studies, notes, this approach is “the route that White House political operatives suggest as a way to meet President Trump’s pre‐​determination not to again certify that Iran is in compliance, even when the facts clearly say otherwise.” 20 This approach also plainly misuses the relevant provisions of the JCPOA: as Daryl Kimball, director of the Arms Control Association put it, the Iran deal’s “special access provisions were designed to detect and deter cheating, not to enable [a] false pretext for unraveling the agreement.” 21 The administration appears to be simply “seeking trumped up reasons to sink [the] Iran deal.” 22



The Trump administration’s approach to Iran approximates the Bush administration’s approach to Iraq in the lead up to the 2003 invasion. Fitzpatrick compares the two situations, noting that “unfounded assumptions, false claims, and ideologically‐​tinged judgements are driving a confrontational approach that could well lead to another war in the Middle East.” 23 As in the case of Iraq, the risk exists for politicization of intelligence findings. As Steve Andreasen and Steve Simon, both former members of the National Security Council, describe in a recent op‐​ed in the _New York Times_ : “It’s a good bet that [administration officials] will cherry‐​pick facts to give the president what he wants: an excuse to scuttle the Iran deal.” 24



President Trump’s commitment to a harder line against Iran—independent of the nuclear deal—is obvious, though the Trump White House’s vicious internal power struggles suggest clear differences inside the administration on the best approach. In June, for example, the _New York Times_ reported that the administration was ramping up a covert action program against Iran, and that “Mr. Trump has appointed to the National Security Council hawks eager to contain Iran and push regime change, the groundwork for which would most likely be laid through CIA covert action.” 25 Yet Trump’s National Security Adviser H. R. McMaster fired the council’s former senior director for intelligence, Ezra Cohen‐​Watnick, in August. Cohen‐​Watnick had previously expressed to administration officials “that he wants to use American spies to help oust the Iranian government.” 26 Along with Derek Harvey, who was the administration’s top Middle East official on the National Security Council, Cohen‐​Watnick had also advocated broadening U.S. involvement in the Syrian civil war as a means of pushing back against Iran. McMaster likewise fired Harvey in July 2017.



Prominent Iran hawks remain in the administration, and some go well beyond arguing for abrogating the JCPOA to make the case for a regime change policy toward Iran. In June, Tillerson testified before the House Foreign Relations Committee that the administration intended to “work toward support of those elements inside of Iran that would lead to a peaceful transition of that government,” 27 though other high‐​level administration officials have denied this is current policy. 28 While he was a member of Congress in 2016, Trump’s current CIA director, Mike Pompeo, publicly called for the United States to “change Iranian behavior, and, ultimately, the Iranian regime.” 29 Senator Tom Cotton (R-AR)—known to be close to the Trump administration—likewise has stated that “the policy of the United States should be regime change in Iran.” 30 Defense Secretary James Mattis as recently as June described Iran as “the most destabilizing influence in the Middle East.” 31



Outside the federal government, other hawkish voices have also made forceful calls for regime change. Soon after Trump was inaugurated, the well‐​connected conservative think‐​tank Foundation for the Defense of Democracies (FDD) submitted a memo to Trump’s National Security Council that argued for “coerced democratization” in Iran, a euphemism for regime change. 32 John Bolton said in a speech in July, “The behavior and the objectives of the regime are not going to change, and therefore the only solution is to change the regime itself.” 33



The debate on Iran in Washington today includes many options, some—though not all—of which begin with killing the JCPOA. Deliberately scuttling the JCPOA would have negative ramifications. The international community and Iran, recognizing U.S. intransigence, could conceivably continue to uphold the nuclear deal without the United States, isolating the United States from allies and handicapping its pursuit of unrelated diplomatic initiatives, notably the question of North Korea’s nuclear program. Alternatively, U.S. termination of the JCPOA could motivate Iran to unburden itself from the deal’s restrictions, expel international monitors, and begin once again to pursue a nuclear weapons capability in earnest. Either possibility puts the United States in a weaker, more dangerous position. Given the momentum in Washington behind pursuing a more hostile approach toward Iran, this policy analysis will explore the likely costs and consequences of four different approaches to confronting Iran, whether as alternatives to the JCPOA or supplementary to it.



The first approach we assess is applying economic pressure in the form of ratcheting up sanctions on Iran, including those the international community agreed to lift under the JCPOA. The second approach looks at the options for challenging Iranian influence in the Middle East, particularly its proxies in Iraq and Syria. The third approach considers the viability of what is called “regime change from within,” where the United States would support internal opposition groups in an effort to undermine or overthrow the government in Tehran. The fourth and final approach we evaluate is military action against Iran, most likely in the form of limited airstrikes against Iranian nuclear or other military facilities. We conclude by proposing a fifth strategy for the Trump administration: uphold U.S. commitments under the JCPOA, refrain from adding new sanctions, and engage with Tehran where U.S. and Iranian interests overlap. There is no silver bullet that can solve the problems in the U.S.-Iranian relationship, but continued engagement carries lower costs and a higher chance of success than any of the other approaches examined here.



Opponents of the JCPOA frequently argue that they could negotiate a better deal through the aggressive use of U.S. sanctions. These sanctions would be extraterritorially applied, forcing European companies to adhere to U.S. law, in theory making Iran willing to concede more of its nuclear program or to make other security and governance concessions. For example, former Connecticut senator Joe Lieberman proposed in December that President Trump “designate the entire Iranian Revolutionary Guards Corps as a foreign terrorist organization … support legislation in Congress punishing sectors of the Iranian economy … propose measures to curb Iranian access to U.S. dollars … and then to walk away, with cause, from the JCPOA.” 34 Such arguments are not restricted only to those who wish to abrogate the JCPOA. Various authors argue that while there are no grounds to “tear up” the deal, the president and Congress should nonetheless seek to impose new sanctions on Iran related to its regional activities and support for the Assad regime in Syria.



Indeed, Congress has already acted in this regard, passing an extensive sanctions bill in July 2017, including North Korean, Russian, and new Iranian sanctions. The bill, “Countering America’s Adversaries through Sanctions Act,” targets a number of new individuals and entities—particularly in relation to Iran’s ballistic missile program—and includes an arms embargo and several new reporting requirements. 35 Congress made last minute changes to the bill to ensure that it did not technically violate the JCPOA, 36 yet as Senator Bernie Sanders (I-VT) pointed out when justifying his vote against the bill: “I believe that these new sanctions could endanger the very important nuclear agreement that was signed between the United States, its partners, and Iran in 2015. That is not a risk worth taking.” 37 Sanders is correct; new sanctions on Iran for its missile programs and human rights abuses raise tensions within the framework of the JCPOA while adhering to the narrowest possible definition of its terms. In response to the new sanctions bill and the threat of further sanctions, Iranian leaders voted to increase the state’s military budget and threatened to restart the nuclear program, highlighting the escalatory potential of new sanctions. 38



Opponents of the JCPOA support the imposition of new sanctions, particularly the designation of the Islamic Revolutionary Guard Corps (IRGC) and IRGC‐​associated businesses, often with little regard for whether new sanctions could torpedo the deal or worsen relations. Council on Foreign Relations Senior Fellow Ray Takeyh has repeatedly said renewed sanctions are the first step in a broader strategy of pressure on Iran, arguing that “we must return to the days of warning off commerce and segregating Iran from global financial institutions. Designating the Revolutionary Guards as a terrorist organization and reimposing financial sanctions could go a long way toward crippling Iran’s economy.” 39 Likewise, the editors of the conservative _National Review_ advised the Trump White House to abrogate the deal through sanctions: “Better to declare an end to this diplomatic farce … and establish a robust sanctions regime that might actually force Tehran to change its ways.” 40



The central problem with this option—whether as a replacement for the JCPOA or in addition to it—is the utter lack of international support. Though often overlooked, the JCPOA is in reality a multinational arms control agreement, negotiated by the P5+1, the five permanent members of the United Nations Security Council, plus Germany. The other parties to the deal have been unequivocal in affirming that Iran is indeed abiding by its commitments under the deal. On August 3, a spokeswoman for European Union foreign policy chief Federica Mogherini told a press conference: “So far, we consider that all parties have been implementing their commitments under the deal.” 41 Sergei Lavrov, Russian foreign minister, likewise confirmed Iran’s compliance and questioned the Trump administration’s motives, saying in August that the Trump administration “continue[s] calling these agreements wrong and erroneous, and it’s a pity that such a successful treaty is now somewhat being cast into doubt.” 42



European support for the deal is strong. As Carl Bildt, former prime minister of Sweden, noted in an opinion piece in August, canceling the deal would be a nonstarter in Europe: “Europe would certainly not go along with this, for one because it would risk undercutting the elaborate inspections systems that the agreement depends on. But primarily because Europe has seen that the deal actually works … and Europe has absolutely zero appetite for a new cascade of conflicts in a region on its doorstep.” 43 As a result, European leaders are also keen to prevent the imposition of further non‐​nuclear U.S. sanctions that could potentially undermine the deal. Indeed, on July 11, Mogherini told reporters: “The nuclear deal doesn’t belong to one country; it belongs to the international community. We have the responsibility to make sure that this continues to be implemented.” 44



It is unlikely that any additional U.S. sanctions would be successful without multinational support. The United States has long had an extensive array of sanctions focused on Iran, including on weapons procurement and development, U.S.-Iranian trade, and terrorist financing. Yet the long‐​term effect of these sanctions on the Iranian economy was relatively minimal prior to 2005. Technology sanctions have undoubtedly been successful in slowing progress on nuclear and missile‐​related projects but have done little to impact Iran’s import and development of conventional weapons. 45



Two changes in the mid‐​2000s substantially increased the efficacy of sanctions on Iran. First, the Treasury department aggressively pursued a strategy of outreach, lobbying (and threatening) foreign banks to ensure that U.S. sanctions would be adhered to extraterritorially. Second, the European Union decided in 2012 to embargo Iranian oil exports. This decision was motivated by increasing concerns over Iran’s nuclear program, even though it was politically and economically costly for the Europeans. In 2010 alone, Iran’s exports to the EU totaled $19 billion, 90 percent of which were energy related. 46 By March 2013, Iran’s oil exports had dropped from 2.5 million barrels per day to 1 million barrels per day, resulting in an Iranian budget deficit of $28 billion that year. 47 While U.S. sanctions alone were relatively ineffectual, these punitive economic costs helped to drive Iran to the negotiating table.



Proponents of increased sanctions therefore typically advocate for more assertive enforcement of secondary sanctions penalties against European and Asian companies. A recent report from the Washington Institute for Near East Policy, for example, called for the United States to step up the extraterritorial enforcement of existing sanctions on terror financing and IRGC‐​affiliated companies, arguing that enforcement and public warnings could discourage European companies from re‐​entering the Iranian market. As Stuart Levey, at the time undersecretary for terrorism and financial intelligence, described the use of extraterritorial sanctions prior to the JCPOA: “Those who are tempted to deal with targeted high‐​risk actors are put on notice: if they continue this relationship, they may be next.” 48 Yet the decision to sanction Iran was costly for European companies. A number of companies, most notably French energy company Total, which signed a $5 billion investment deal with Iran and with China’s National Petroleum in July to develop the South Pars gas field, have begun to re‐​enter the market following the successful conclusion of the JCPOA. 49 In the absence of any concrete evidence of Iranian cheating on the deal, European and Asian governments are likely to push back strongly against new U.S. barriers to trade and investment in Iran, and on the excessive extraterritorial application of existing sanctions.



Another problem with sanctions is that they are rarely successful in producing policy change. Indeed, though targeted sanctions may impose costs on the targeted regime, it is less clear that these costs actually produce policy change. 50 Proponents of increased sanctions point to high profile successes like the JCPOA, while skeptics point to the many cases, from Syria to Zimbabwe, where sanctions have failed to produce policy change. More broadly, academic studies have repeatedly shown sanctions to be ineffective in achieving policy change. As Arne Tostensen and Beate Bull note in the journal _World_ _Politics_ , “The voluminous literature that has accumulated over the years tends to conclude that sanctions are rarely effective, even though exceptions have been documented.” 51 In one of the earliest broad‐​based studies of comprehensive sanctions, for example, researchers found an average sanctions success rate of only 34 percent. 52 Even the research on more recent “smart sanctions,” which are presumed to be more effective thanks to their “targeted” nature, shows that they are also largely ineffective. A wide‐​ranging study of United Nations targeted sanctions found them to be effective in only 10–20 percent of cases, 53 while another survey of post‐​9/​11 U.S. sanctions found them to be effective in only 36 percent of cases. 54



Policy change is especially unlikely when sanctions do not have clear, attainable goals or when the issue is of prime national security importance to the target state. 55 Sanctions focused on economic issues such as trade often seem to be qualitatively different than those focused on security. 56 When University of Chicago’s Robert Pape examined sanctions as an alternative to the use of force, he found they had only been successful in around 5 percent of national security–related cases. 57 Sanctions also tend to fail when they are unilateral; as the Washington Institute’s Katherine Bauer notes, even with the power of U.S. extraterritorial sanctions, “there are limits to U.S. jurisdiction and the ability to compel foreign compliance.” 58 Further sanctions on Iran thus fall into a worst‐​case scenario: security‐​focused sanctions with no clear goals other than securing “a better deal” or weakening the Iranian regime. In the absence of strong support from European or other Security Council nations, there is very little chance that further sanctions will compel Iran’s leaders to capitulate.



An alternative option is a deliberate strategy of challenging Iranian proxies throughout the Middle East. That option would not necessarily require the Trump administration to abrogate the JCPOA. Indeed, as Brookings Institution Senior Fellow Daniel Byman recently noted in congressional testimony: “Because the JCPOA … has put Iran’s nuclear program on the back burner, there is an opportunity to focus on Iran’s support for militant groups and other problems Iran causes in the region.” 59 This approach runs counter to Washington’s current regional strategy: though there are arenas where the United States is engaged in hostilities with Iranian‐​associated proxies—such as U.S. support for the Saudi‐​led campaign in Yemen—America’s anti‐​ISIS campaign typically means that it is de facto fighting on the same side as Hezbollah and other Shi’a militias. The most moderate alternative proposals call for U.S. support for regional allies, such as military and diplomatic support for a peace settlement in Yemen designed to split the Houthi rebels from Tehran’s limited support. 60 Other options include increased maritime presence to help disrupt Iranian arms shipments. 61 Still others call for building the capacity of regional actors: one recent report from the Center for a New American Security suggests maintaining U.S. influence in Iraq and increasing U.S. logistical support for the conflict in Yemen, in hopes of marginalizing Iranian influence in those conflicts. 62



However, there are also a variety of more aggressive proposals. Two senior former administration officials on the National Security Council, Derek Harvey and Ezra Cohen‐​Watnick, were reportedly in favor of direct U.S. military action against Iranian proxies in Syria. 63 Escalating clashes between U.S. troops and militias in southern Syria in recent months, including U.S. airstrikes on several militias, suggest that such clashes will happen even in the absence of a formal policy change. Several recent policy papers also make the argument for a more formalized anti‐​Iran strategy in Syria, often using proxies to challenge Iranian‐​allied groups. The Washington Institute’s Nader Udowski, for example, argued in June 2017 for “a new U.S. policy, the chief component of which should be a strategy targeting Iran’s Quds force and its Shi’a militias.” 64 Similarly, Max Peck of the Foundation for Defense of Democracies has argued that the Trump administration should seek to codify in law that the United States seeks the overthrow of the Assad regime in Syria, and “increase the costs of Iran’s engagement by maintaining the pressure on Assad … through its support for the armed opposition.” 65



Perhaps the most bellicose option is actively increasing U.S. participation in the war in Syria and Iraq. A report from the Institute for the Study of War (ISW) called for the United States to “seize and secure a base in southeastern Syria … create a de facto safe zone … then recruit, train, equip, and partner with local Sunni Arab anti‐​ISIS forces.” The report called for American troops to “fight alongside” these forces. 66 The goals would include not only “defeating al Qaeda, as well as ISIS,” but also “expelling Iranian military forces and most of Iran’s proxy forces from Syria.” This strategy extends to Iraq: as a follow‐​on report argued, America should also “take urgent measures to strengthen Iraqi Prime Minister Abadi,” and work to minimize Iranian influence in Iraq. 67 Though the extent of American military involvement varies widely across these proposals, they all share a common theme: direct or indirect military action against Iranian proxies in Syria, Iraq, Yemen, and elsewhere.



The central problem with this approach is that there is no coherent anti‐​Iranian axis in the Middle East to rely upon in a campaign to challenge Iranian influence in the region. Indeed, observers have often described the region using sectarian narratives—portraying conservative Sunni states in conflict with Iran’s more revolutionary Shi’a axis—that are largely exaggerated.



For example, despite Saudi efforts to form a united regional front against Iran, the conflicts of the Arab Spring have frequently seen the states of the Gulf Cooperation Council (GCC) act against each other’s interests. 68 In Syria, the conflict between Saudi and Qatari proxies helped to radicalize and doom the anti‐​Assad opposition, while a Qatari‐​Emirati rivalry fueled the Libyan conflict. Today’s GCC crisis only serves to highlight this problem: though clearly motivated by a desire to rein in Qatar’s independent foreign policy, the Saudi and Emirati embargo has in reality driven Qatar closer to Iran and Turkey, undermining a common GCC front. 69



Other regional attempts to form anti‐​Iranian movements have likewise failed. A widely‐​publicized Saudi Arabian attempt in December 2015 to create an Islamic Military Alliance to fight terrorism—which pointedly included no Shi’a majority states—has largely failed to develop since that time. 70 Nor is there any guarantee that regional partners will actually promote U.S. interests if the United States increases its support; the actions of allies in the region have all too often served to destabilize and worsen conflicts in Syria, Yemen, and elsewhere, rather than improve them.



Indeed, the lack of a solid anti‐​Iran coalition among existing U.S. partners—capable of achieving America’s often expansive foreign policy goals—is a key reason why the most extreme options for regional confrontation with Iran often involve fabricating an effective anti‐​Iranian bloc from whole cloth, whether that is the creation of a “credible and moderate Syrian opposition,” a regional “multinational Joint Task Force with Arab partners targeted at countering … the IRGC,” or “a new Syrian Sunni Arab partner … to conduct population‐​centric counterinsurgency.” 71 Each of these options is likely to fail. Previous U.S. efforts to create regional coalitions to fight terror groups have been largely unsuccessful. The 2014 collapse of the Iraqi army in the face of ISIS advances is also a salutary lesson; years of training commitments and substantial blood and treasure on the part of the U.S. military were not enough to overcome deeper societal problems like corruption. 72 Without coherent, effective local proxies, and given the major political differences that divide U.S. regional allies, any attempt to build an anti‐​Iranian force or coalition in the region is likely to falter.



A strategy of regional pushback against Iran is also likely to pull the United States more deeply into a variety of regional conflicts and increase the risks of blowback to U.S. troops in the region. The United States is already heavily overcommitted in the Middle East, with tens of thousands of troops engaged in conflicts in Iraq, Syria, Afghanistan, Libya, and Yemen, and stationed at permanent bases elsewhere throughout the region. Indeed, despite the Obama administration’s attempts to draw down American commitments to Middle Eastern conflicts, the number of troops engaged in fighting Middle East conflicts has been increasing again since 2014. 73 A stepped‐​up campaign against Iranian proxies throughout the region will require further troop increases, both in direct combat roles and to train and support local forces.



It is these troops who will bear the brunt of any Iranian military response to this strategy. Several hundred U.S. troops were killed by Iranian‐​associated groups in Iraq during the post‐​invasion occupation, a number likely to rise in any new conflict with these groups. 74 And while Hezbollah has been largely occupied in recent years with fighting on behalf of the Assad regime, if faced with a concerted campaign against it by U.S.-allied forces, it is likely to respond with the kind of asymmetric attacks that have characterized their long‐​running conflict with Israel. 75 Indeed, one potential response to a concerted attack on Iranian proxies throughout the region is retributive attacks on Israel; during the 2006 war, Hezbollah enjoyed substantial success against Israeli forces, disabling a number of tanks and even an Israeli warship. 76 The potential for Iranian retaliation against U.S. troops, regional partners, or shipping in the region suggests that a strategy of regional confrontation with Iran will not make the region safer or more stable, but will instead introduce additional conflict and uncertainty.



Another possible option for dealing with Iran is an explicit U.S. policy of regime change. This is not a new idea; for decades, hawks in Washington have called for regime change in Tehran. Justifications have ranged from the 1979 hostage crisis to Iran’s nuclear program in the mid‐​2000s to the anti‐​regime protests known as the Green Revolution after 2009. 77 Yet the failure of U.S. regime change campaigns in both Iraq and Libya to produce a stable, democratic state has led most proponents of regime change to back away from overt military options and instead suggest that the Trump administration pursue “coerced democratization” or “regime change from within.” In this approach, the United States would pressure the Iranian regime and simultaneously back groups that oppose it—whether the exiled extremist National Council of Resistance of Iran (NCRI), pro‐​democracy Green Revolution factions, or ethnic minorities within Iran—a strategy advocates often compare to Reagan’s support for civil society groups in the Soviet Union. As Reuel Gerecht and Ray Takeyh argue in a _Washington Post_ op‐​ed: “Today, the Islamist regime resembles the Soviet Union of the 1970s … if Washington were serious about doing to Iran what it helped to do to the U.S.S.R., it would seek to weaken the theocracy by pressing it on all fronts.” 78



Another proponent of “coerced democratization,” the Foundation for Defense of Democracies’ Mark Dubowitz, urged President Trump to “go on the offensive against the Iranian regime” by “weakening the Iranian regime’s finances” through “massive economic sanctions,” while also “undermin[ing] Iran’s rulers by strengthening pro‐​democracy forces” inside Iran. 79 This option appears to be gaining traction in the Trump administration’s ongoing Iran policy review and has received public support from Tillerson. CIA Director Mike Pompeo also favored such an approach during his time in Congress. Yet there are important reasons to doubt that such a strategy would actually yield constructive results in Iran or benefit U.S. national interests.



Regime change often fails, particularly when it is covert. According to one study of covert regime change operations by the United States during the Cold War, such efforts succeeded only one‐​third of the time. 80 Indeed, as an administration official said in August, “With Iran, they are looking at regime change but coming up empty. There are no good plans, no decapitation strikes possible.” 81 Arming or funding for local insurgencies also rarely succeeds; a leaked CIA report commissioned in 2012 found that most past attempts to covertly arm insurgencies had minimal impact on long‐​term outcomes and often backfired. 82



Even when successful in unseating one government and establishing another in its place, foreign‐​imposed regime change “generally does not improve relations between interveners and targets. Rather, it often makes them worse,” according to Georgetown University’s Alexander B. Downes and Boston College’s Lindsay A. O’Rourke. 83 Changing the leadership of a state typically fails to alter that country’s perception of its interests, and foreign‐​imposed regimes tend to diverge from the preferences of the intervener as they begin to face domestic political pressures. Contrary to the depiction of many regime change advocates, the Iranian regime enjoys substantial public support, and the population would not welcome a U.S.-imposed government. Any new regime that tried to implement policies that reflect U.S. interests instead of Iranian interests would “attract the ire of domestic actors,” leading to an unstable government viewed as illegitimate by the population. 84



Research shows that “when a country overthrows another’s government, it increases the likelihood of civil wars and usually doesn’t establish a democracy.” 85 The recent experiences of the United States in Iraq, Afghanistan, and Libya only confirm this finding. Sixteen years of U.S. military presence have done little to stabilize war‐​torn Afghanistan. 86 The war in Iraq essentially destroyed the Iraqi state, killing hundreds of thousands of Iraqis and displacing millions more. More than 4,400 U.S. troops were killed in combat, and more than 30,000 were wounded, with direct costs estimated to exceed $2 trillion and indirect costs as high as $4 trillion. 87 A widespread insurgency and civil war led to the rise of the Islamic State, prompting further U.S. intervention to fight against the group. In Libya, the U.S. choice to overthrow the regime of Muammar Gaddafi on humanitarian grounds resulted in a lengthy civil war and the deaths of more Libyans than would likely have perished without the intervention. 88 The likelihood of successful regime change and a subsequent stable, democratic state in Iran are vanishingly small.



Though regime change proponents highlight a variety of groups inside Iran as potential candidates for U.S. support, none are truly viable. The exiled opposition group Mujahideen‐​e‐​Khalq (MEK) (or its political wing, the NCRI) is one such example. The MEK began in the 1960s and 1970s as a paramilitary Marxist‐​Islamic resistance group opposed to the former Shah of Iran, the authoritarian ruler put in power following a 1953 coup sponsored by the United States and Great Britain. The group allied with Saddam Hussein during the 1980s Iran‐​Iraq War, and analysts widely agree that it is an undemocratic group that has no popular support inside Iran. 89 Indeed, the MEK has largely tried to win external support for its agenda of regime change in Iran. Until 2012, it was even designated a terrorist organization by the U.S. State Department and had lobbied hard over the years to win support from prominent current and former U.S. officials to have that designation removed. 90 It has won primarily the support of those who favor a hardline approach to Iran, such as former CIA directors James Woolsey and Porter Goss, former New York City Mayor Rudolph Giuliani, former governors Howard Dean and Ed Rendell, former U.N. Ambassador John Bolton, and former House Speaker and close Trump confidant Newt Gingrich. Yet in the absence of popular support outside certain Washington circles, backing the group in a bid to overthrow the Iranian regime would likely fail. 91



Regime change advocates also suggest supporting the so‐​called Green Movement that emerged amid the protests over the contested Iranian presidential elections in 2009. Unfortunately, according to Ariane Tabatabai and Madison Schramm, the Green Movement “essentially faded away a few months after the elections” and “was never a cohesive faction.” 92 Green Movement leaders Mir Hossein Mousavi and Mehdi Karroubi remain under house arrest in Iran today, and have made clear that their goal was to dispute the 2009 election results, not to overthrow the government. In fact, the best hope for the Green Movement is to avoid association with the United States; whatever popular support it continues to have would quickly evaporate with any whiff of U.S.-backing for regime change. As Michael Axworthy of the University of Exeter writes, “Given the long history of foreign meddling in the country (the CIA‐​inspired coup that removed Prime Minister Mohammad Mosaddeq in 1953 is just one example), any suspicion of foreign backing is political poison in Iran.” 93



The third option—seeking to stoke discontent among Iran’s minority populations—is similarly infeasible. Iran’s ethnic minorities include Kurds (10 percent), Baluchis (2 percent), Arabs (2 percent), and Azeri Turks (16 percent). 94 But Iran is not a country beset by ethnic, cultural, and religious cleavages in the way the former Yugoslavia was. Neighboring Iraq, with its mix of Shia, Sunni, and Kurds, was a comparatively disjointed state held together by a powerful centralized dictatorship. Iran is very different. Any strategy that seeks to foment political upheaval in Iran via these various minority groups ignores the fundamental cohesion that characterizes Iran as a national unit. 95 If anything, such an approach would be more likely to bolster Iranian nationalism than to subvert it. As Vali Nasr, dean of the Johns Hopkins School of Advanced International Studies and an Iranian‐​American, told the _New Yorker_ in 2008, “Iran is an old country—like France and Germany—and its citizens are just as nationalistic. The U.S. is overestimating ethnic tension in Iran … working with the minorities will backfire, and alienate the majority of the population.” 96



Direct military action against Iran is the least likely of the options being considered under the Trump administration’s policy review. Indeed, the focus on nonmilitary options among Iran hawks is likely a response to the widespread distaste among the American public for engaging in another open‐​ended regime change war in the Middle East. Yet some have argued that the Trump administration should “rebuild military leverage over Iran,” including “contingency plans to neutralize Iran’s nuclear facilities,” engage in regional military exercises, and direct the U.S. navy to “fully and responsibly utilize rules of engagement to defend themselves and the Persian Gulf against rising Iranian harassment.” 97



There are various contingencies in which U.S. policymakers may face a decision on the use of military force against Iran, whether it is a purposeful strike against Iran’s nuclear facilities in the wake of U.S. withdrawal from the JCPOA, or a more gradual escalation following military confrontations in Syria, the Gulf, or elsewhere. As the Trump administration considers these options, however, it would do well to remember that the lack of good military options was the key reason behind the Bush and Obama administrations’ decision to pursue diplomacy with Tehran in the first place.



The United States should only undertake military action against another state if its core security interests are threatened. Yet there is no plausible near‐​term scenario in which Iran poses a direct threat to the U.S. homeland. Nor do Iranian actions in the Middle East pose a significant threat to U.S. interests in the region. Taking military action against Iran to thwart the purported threat of its nuclear program would harken back to the preventive war doctrine adopted by the Bush administration after the September 11th terrorist attacks and codified in the 2002 National Security Strategy. 98 Though proponents of military action often describe such action as “preemptive,” one RAND report notes that “generations of scholars and policymakers have defined preemption more restrictively,” limiting it to cases of imminent threat. 99 This is a crucial difference; as the authors highlight, international law holds that truly preemptive attacks are an acceptable use of force in self‐​defense, while preventive attacks are not. As the historian and former Kennedy administration adviser Arthur Schlesinger Jr. put it when criticizing the Bush administration’s case for war against Iraq, this doctrine of preventive war “is alarmingly similar to the policy that imperial Japan employed at Pearl Harbor, on a date which, as an earlier American president said it would, lives in infamy. Franklin D. Roosevelt was right, but today it is we Americans who live in infamy.” 100 With no imminent threat from Iran, there is no legal justification for direct military action.



At the very least, the Trump administration is constitutionally obligated to seek approval from Congress for any military action against Iran. Trump himself may disagree. He previously declined to seek or secure congressional authority for his missile strike on a Syrian military base controlled by the Assad regime in April 2017 and has repeatedly made public statements arguing that military action should be kept secret to preserve the tactical advantage of a surprise attack. If Trump does seek congressional approval for military strikes on Iran, he is likely to face strong opposition from many Democratic members of Congress and at least some Republicans. Senator Chris Murphy (D-CT) argued in February that “Trump and his most radical advisers are begging for war with Iran. This would be a disaster of epic scale, perhaps eclipsing the nightmare of the Iraq war.” 101 Congressional Democrats, already concerned about the administration’s domestic policy proposals, are unlikely to cut him a blank check on Iran.



Even small‐​scale military attacks on Iran—whether targeted strikes on nuclear facilities or clashes with Iranian forces in the Gulf or elsewhere—are likely to lead to escalation. In March 2012, the Pentagon held a classified war simulation “to assess the repercussions” of an Israeli attack on Iran’s nuclear facilities. The results showed that such a targeted strike would provoke immediate Iranian retaliation against U.S. military bases and naval assets in the region, drawing the United States into “a wider regional war.” 102 General James Mattis, now Trump’s secretary of defense, was then head of Central Command and supervised the war game. The _New York Times_ reported that Mattis told aides a strike “would be likely to have dire consequences across the region and for U.S. forces there.” Following a similar war game in 2004, retired Air Force Colonel Sam Gardiner concluded, “There is no military solution for the issues of Iran.” 103



It is not clear that a narrow or targeted strike is even possible. To strike Iran’s nuclear facilities, the United States would also need to bomb Iran’s air defense systems and command and control facilities, which itself carries risks of escalation. Writing in 2006, retired General Thomas McInerney suggested one such plan for attacking Iran’s nuclear facilities, requiring a massive commitment of 700 aircraft, 500 cruise missiles, and 28,000 bunker‐​buster bombs in the initial 36–48 hours. 104 Moreover, airstrikes of this kind, to accomplish any long‐​term objective, could not be limited to a single one‐​off mission. As explained in a 2012 study by the Iran Project, a nongovernmental organization founded to improve official contacts between the American and Iranian governments, for targeted strikes to “fulfill the stated objective of ensuring that Iran never acquires a nuclear bomb, the United States would need to conduct a significantly expanded air and sea war over a prolonged period of time, likely several years.” 105



Under bombardment from the world’s most dominant military superpower and uncertain of U.S. intentions, Iran would be likely to engage in retaliatory strikes against U.S. bases and military assets in Iraq, Syria, Bahrain, Qatar, and the United Arab Emirates. Iran’s Shahab‐​3 intermediate range ballistic missile can hit targets up to 2,000 kilometers away, while its Soumar cruise missile can potentially hit targets up to 2,500 kilometers away, meaning all U.S. forward‐​deployed bases in the Middle East and at least some bases in Europe are within range for conventional retaliation. 106 Likewise, the potential for asymmetric retaliation should not be underestimated. As Afshon Ostavar of the Naval Postgraduate School notes, “While Iran’s neighbors have poured billions of dollars into conventional weaponry, Iran has invested in comparatively cheap proxy forces that have proven effective in numerous theaters.” 107 Proxy groups such as Hezbollah or even Iran’s Quds force, a special unit of the IRGC, could engage in terrorist attacks against U.S. forces or allies in the region.



Anything beyond a limited military strike would have even more dire and counterproductive consequences. Taking military action to topple the Iranian regime, for example, would require a massive, lengthy, and costly military commitment. America’s experience in Iraq should be instructive in this context: Bush administration officials and their allies in the think‐​tank community and news media made bold predictions about the ease with which America would win the war, that Iraq would be reborn as a functioning democracy, and that the costs to the United States in lives and dollars would be minimal. These predictions proved wrong. In addition to bolstering Iran’s strategic position, the war helped to destabilize the region and to exacerbate America’s terrorism problem. A 2006 National Intelligence Estimate concluded that “the American invasion and occupation of Iraq … helped spawn a new generation of Islamic radicalism.” 108 The war had “become the ‘cause celèbre’ for jihadists, breeding a deep resentment of U.S. involvement in the Muslim world and cultivating supporters for the global jihadist movement.” 109



A large‐​scale ground war in Iran would be immensely damaging. Comparisons to Iraq are illuminating. The U.S. invasion was initially successful against a relatively ineffectual Iraqi military with approximately 389,000 men under arms. But U.S. forces have struggled in the years since to control territory, build a functioning Iraqi state, and deal with mass insurgency among the population of around 37 million. In comparison, Iran has a larger (about 523,000 active duty) and more effective military, a bigger population (80.3 million), and territory more than three times the size of Iraq. 110 A study by the Iran Project concluded: “If the United States decided to seek a more ambitious objective, such as regime change in Iran or undermining Iran’s influence in the region, then an even greater commitment of force would be required to occupy all or part of the country.… Given Iran’s large size and population, and the strength of Iranian nationalism, we estimate that the occupation of Iran would require a commitment of resources and personnel” greater than the costs of the wars in Afghanistan and Iraq combined. 111



A direct military attack on Iran, whatever the specific goals, is likely to be counterproductive in terms of nuclear nonproliferation. Military action short of regime change cannot eliminate Iran’s nuclear program or the knowledge behind its existence. 112 Given U.S. interventions in recent years, even targeted strikes may be seen by Tehran as a precursor to more intensive military action that must be deterred. A 2010 Defense Intelligence Agency study concluded that the main goal of Iran’s military strategy is regime survival, with a key focus on deterrence. 113 As Kenneth Pollack, a former CIA and National Security Council analyst, noted in 2006: “The Iraq example coupled with the North Korea example probably is part of the motivation for some in Iran to get a nuclear weapon.” 114 The 2011 U.S. intervention in Libya only intensifies this dilemma for Iran; Muammar Gaddafi voluntarily gave up his nascent nuclear program before being removed by a joint American‐​European intervention. Thus, while targeted strikes could delay Iran’s ability to develop nuclear weapons by destroying infrastructure, they would probably incentivize Iran to redouble its enrichment efforts under the conviction that only a nuclear deterrent can ensure its future survival.



This logic also implies broader strategic costs to an attack on Iran: it would exacerbate the problem of nuclear proliferation more generally. As the current Director of National Intelligence Dan Coats recently acknowledged at the Aspen Security Forum, U.S. actions against Saddam Hussein’s Iraq and Muammar Gadhafi’s Libya have made it clear to other states, like North Korea, that a nuclear deterrent may be the best way to ensure regime survival in the context of a war‐​prone United States. 115 North Korea itself confirmed this logic, releasing a statement after a 2016 nuclear test arguing that “the Saddam Hussein regime in Iraq and the Gaddafi regime in Libya could not escape the fate of destruction after … giving up nuclear programs of their own accord.” 116 As Nobel laureate Thomas Schelling has famously pointed out, American nonproliferation policies are ironically a prime driver of nuclear proliferation. 117 If, after successfully negotiating a nuclear deal, the United States then engages in an aggressive war against Iran despite Tehran’s full compliance with the JCPOA, other potential proliferators would have no reason at all to believe that the United States can be trusted to negotiate in good faith.



Though the Trump administration’s Iran policy review appears predestined to produce a more belligerent approach towards Iran, each of the options explored in this paper has significant flaws. Indeed, each option is unlikely to achieve its stated objectives, while at the same time creating an unacceptably high risk of exacerbating the very problems the Trump administration seeks to resolve. At a fundamental level, a more assertive U.S. policy towards Iran—whatever the details—will inevitably intensify Iranian fears about the country’s national security, worsening the very behaviors that the United States seeks to forestall. Even adopting one of these more hostile approaches to Iran while nominally upholding the JCPOA presents greater problems than embracing the nuclear deal and using it as a vehicle for further engagement designed to temper Iranian behavior.



As this paper highlights, it is doubtful that ratcheting up economic sanctions will alter Iranian policies in a more constructive direction, especially in the absence of international cooperation. Likewise, by pushing back harder against Iranian influence throughout the Middle East, the United States would incur substantial long‐​term costs in exchange for negligible gains in regional security. Moreover, a more aggressive approach could lead to unintended military escalation. Supporting internal opposition groups to pressure the regime or foment domestic upheaval is a hopeless strategy, given Iran’s domestic political realities and America’s long history of failed regime change endeavors. Finally, direct military action would have little public support, no legal basis, and most likely produce profoundly negative consequences for regional security and American interests.



Such actions would effectively return U.S.-Iranian relations to the cycle of enmity in which they were trapped prior to the negotiation of the JCPOA, with the nuclear issue dominating as a justification for continued hostility. Indeed, prior to the JCPOA, American allies in the region, particularly Saudi Arabia and Israel, often used the issue of Iran’s nuclear program to steer American policy toward Iran in a more confrontational direction. In private conservations with U.S. officials early in the Obama administration, then‐​king Abdullah bin Abdulaziz al‐​Saud pushed U.S. military action against the Iranian regime. 118 From 2010 to 2012, there were reports that Israel was close to initiating military strikes against Iranian nuclear facilities, knowing it would likely trigger U.S. involvement. Israeli Prime Minister Benjamin Netanyahu’s cabinet officials reportedly blocked him from taking this step. 119



Maintaining and strengthening the JCPOA will help to minimize the future potential for such pressure. Though he fought hard to subvert the JCPOA, for example, Netanyahu has been relatively silent since its adoption. Carmi Gillon, former head of the Israeli security agency Shabak, wrote in July that, thanks to the JCPOA “the threat of an Iranian nuclear weapon is more remote than it has been in decades.” Gillon added, “the majority of my colleagues in the Israeli military and intelligence communities supported the deal once it was reached, [and] many of those who had major reservations now acknowledge that it has had a positive impact on Israel’s security and must be fully maintained by the United States and the other signatory nations.” 120



If the United States is to avoid returning to high levels of tension and conflict in the U.S.-Iranian relationship, it must avoid the more belligerent options explored in this paper. The alternative—the option most likely to produce a positive outcome for all parties—is to uphold the JCPOA, carefully enforce its terms and conditions, and build on it to further engage Iran where its interests overlap with the United States. Pursuing greater diplomacy and engagement with Iran is, ironically, low‐​hanging fruit at this time. Iranian President Hassan Rouhani, who in his first term helped shepherd the JCPOA to fruition, won reelection this year by a wide margin, receiving 57 percent of the vote (compared to 38.5 percent for his chief opponent). 121 The idea of greater engagement with the West was a key component of Rouhani’s electoral platform; both centrists like Rouhani and reformers like former President Mohammed Khatami have argued in favor of what they describe as “JCPOA 2.0,” a series of internal policy compromises that will allow Iran to continue to engage with the West and begin to reintegrate into the global economy. 122



The key to reaping the benefits of a more conciliatory approach is recognizing that Iran is not a unitary actor. Iranian politics, though not fully democratic, are dynamic and competitive, and include various factions, from conservative hardliners to moderate reformists. The nuclear deal is widely popular in Iran, but antagonism from the Trump administration will bolster the prominence of Iranian hardliners who felt Tehran capitulated too much in the negotiations and who use fears of U.S. duplicity to undermine the idea of constructive engagement with Washington. 123 Similarly, perceptions that the United States is failing to live up to its side of the bargain—or is taking new steps that may undermine Iranian security—weaken political support for pragmatic reformists who see value in making concessions to the West in exchange for sanctions relief and integration with the outside world. Ultimately, unlike the more aggressive policy options explored in this paper, further engagement with Iran when possible will strengthen Iran’s more moderate political factions and weaken hardliners, providing a more hopeful future for U.S.-Iranian relations.
"
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




Using a simple, publically-available, climate model emulator called MAGICC that was in part developed through support of the EPA, we ran the numbers as to how much future temperature rise would be averted by a complete adoption and adherence to the EPA’s new carbon dioxide restrictions*.   
  
The answer? Less than two one-hundredths of a degree Celsius by the year 2100.   
  
0.018°C to be exact.   
  
We’re not even sure how to put such a small number into practical terms, because, basically, the number is so small as to be undetectable.   
  
Which, no doubt, is why it’s not included in the EPA Fact Sheet.   
  
It is not too small, however, that it shouldn’t play a huge role in every and all discussions of the new regulations.   
  
*********   
  
* Details and Additional Information about our Calculation   
  
We have used the Model for the Assessment of Greenhouse-gas Induced Climate Change (MAGICC)—a simple climate model emulator that was, in part, developed through support of the EPA—to examine the climate impact of proposed regulations.   
  
MAGICC version 6 is available as an on-line tool.   
  
We analyzed the climate impact of the new EPA regulations by modifying future emissions scenarios that have been established by the United Nation’s Intergovernmental Panel on Climate Change (IPCC), to reflect the new EPA proposed emissions targets.   
  
Specifically, the three IPCC scenarios we examined were the Representative Concentration Pathways (RCPs) named RCP4.5, RCP 6.0 and RCP8.5. RCP4.5 is a low-end emissions pathway, RCP6.0 is more middle of the road, and RCP8.5 is a high-end pathway.   
  
The emissions prescriptions in the RCPs are not broken down on a country by country basis, but rather are defined for country groupings. The U.S. is included in the OECD90 group.   
  
To establish the U.S. emissions pathway within each RPC, we made the following assumptions:   
  
1) U.S. carbon dioxide emissions make up 50 percent of the OECD90 carbon dioxide emissions.   
  
2) Carbon dioxide emissions from electrical power production make up 40 percent of the total U.S. carbon dioxide emissions.   
  
Figure 1 shows the carbon dioxide emissions pathways of the original RCPs along with our determination within each of the contribution from U.S. electricity production.   








_Figure 1. Carbon dioxide emissions pathways defined in, or derived from, the original set of Representative Concentration pathways (RCPs), for the global total carbon dioxide emissions as well as for the carbon dioxide emissions attributable to U.S. electricity production._



As you can pretty quickly tell, the projected contribution of U.S. carbon dioxide emissions from electricity production to the total global carbon dioxide emissions is vanishingly small.   
  
The new EPA regulations apply to the lower three lines in Figure 1.   
  
To examine the impact of the EPA proposal, we replace the emissions attributable to U.S. power plants in the original RCPs with targets defined in the new EPA regulations. We determined those targets to be (according to the EPA’s Regulatory Impacts Analysis accompanying the regulation), 0.4864 GtC in 2020 and 0.4653 GtC in 2030. Thereafter, the U.S. power plant emissions were held constant at the 2030 levels until they fell below those levels in the original RCP prescriptions (specifically, that occurred in 2060 in RPC4.5, 2100 in RCP6.0, and sometime after 2150 in RCP8.5).   
  
We then used MAGICC to calculate the rise in global temperature projected to occur between now and the year 2100 when with the original RCPs as well as with the RCPs modified to reflect the EPA proposed regulations (we used the MAGICC default value for the earth’s equilibrium climate sensitivity (3.0°C)).   
  
The output from the six MAGICC runs is depicted as Figure 2.   




_  


![Media Name: gsr_061114_fig2.jpg](/sites/cato.org/files/styles/pubs/public/wp-content/uploads/gsr_061114_fig2.jpg?itok=R2UJJXni)

_





_Figure 2. Global average surface temperature anomalies, 2000-2100, as projected by MAGICC run with the original RCPs as well as with the set of RCPs modified to reflect the EPA 30% emissions reductions from U.S power plants._



In case you can’t tell the impact by looking at Figure 2 (since the lines are basically on top of one another), we’ve summarized the numbers in Table 1.   






  
  
In Table 2, we quantify the amount of projected temperature rise that is averted by the new EPA regulations.   






  
  
The rise in projected future temperature rise that is averted by the proposed EPA restrictions of carbon dioxide emissions from existing power plants is less than 0.02°C between now and the end of the century assuming the IPCC’s middle-of-the-road future emissions scenario.   
  
While the proposed EPA plan seeks only to reduce carbon dioxide emissions, in practice, the goal is to reduce the burning of coal. Reducing the burning of coal will have co-impacts such as reducing other climatically active trace gases and particulate matter (or its precursors). We did not model the effects of changes in these co-species as sensitivity tests using MAGICC indicate the collective changes in these co-emissions are quite small and largely cancel each other out.


"
"
And the hits just keep on coming…

646,024 for the Month of July, up from 582,079 in June.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d548764',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

from the Calgary Herald: Canadian mini-satellite may solve carbon puzzle (h/t to WUWT reader “Freezedried”)
Tom Spears Canwestnews Service
Friday, February 27, 2009
While NASA lost a $285-million US satellite this week, a Canadian microsatellite that does the same job is chugging along happily in orbit –at 1/1,000th the cost.
The 30-centimetre-long University of Toronto satellite is searching for the “missing” carbon dioxide–the vast amount of Earth’s main greenhouse gas that somehow vanishes each year.
That’s what NASA’s OCO(orbiting carbon observatory) satellite would have done, if it had survived launch on Tuesday. The big difference: Canada built and launched its tiny version for $300,000.
The OCO launched but failed to reach orbit. (see WUWT story here)
The CanX-2 micro satellite, shown slightly smaller than actual size (10 x 10 x 34 cm) 
Details on the hardware are here
Meanwhile, the U of T’s CanX-2 is cruising 700 kilometres above Earth “and functioning really well,” after some glitches that followed its launch last April, said Ben Quine, the director of space engineering at York University–which made an instrument aboard the tiny CanX. Its job, like OCO’s, is to find Earth’s missing greenhouse gas.
“The measurement principle is almost exactly the same as the one for the OCO,”he said. “It’s very sad when you lose a spacecraft, but it also means that we are the only people in orbit with one-kilometre resolution on the ground.”
That means York’s Argus instrument can look at details below. A Japanese satellite does the same job, but can’t look at features less than 10 kilometres wide.
The problem is that where carbon dioxide comes from, and where it is sucked out of the atmosphere, remains poorly understood.
“Clearly, if we’re going to do something about climate change, we need to understand where CO2 is produced and particularly where it’s absorbed.That’s much less clear,” Quine said.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97edcb62',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterHere’s one of my favorites – by Joe Walsh.
I’ve added the lyrics to the song – see below.
Some warmists tried to tell us yesterday that climate scientists lead humble, low-paid lives. Some probably do, but the ones we are familiar with don’t live bad at all. This is for them.

 
Warming’s been good to me so far
I have a data center
Know its high price
Don’t really work there
Man is that nice


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I sponge off NASA
Fudge behind the wall
I have the government
Pay for it all
They say I’m crooked but it’s not on my dime
I’m just hiding the clues at the scene of the crime
Warming’s been good to me so far
My Teraflop
Runs a model and drive
I lost my data
Now I contrive
I have a big lab
Adjust in the back
I delete emails
In case I get hacked
I’m forging new records
My pols can’t wait
They send me money
And tell me I’m great
So I got me an offer
No cold records on the wall
Temperature keeps climbing
Doubt it will fall
Lucky I’m sane after all I’ve been through
(Everybody sing) It’s warm (He’s cool)
I can’t screw up but sometimes I still do
Warming’s been good to me so far
I fly to conferences
Sometimes I see Gore
It’s hard to believe
No one listens no more
It’s tough to cover
This fudging and shame
Everybody’s suspicious
Data’s the same
They say I’m crooked but it works every time
(Everybody sing) Oh yeah (Oh yeah)
I keep adjusting guess I’ll never know why
Warming’s been good to me so far…
Share this...FacebookTwitter "
"

Guest Post by Steven Goddard



Recreational cycling during the summer of 2007

The UK Climate Impacts Programme (UKCIP) is a government funded organization with the following scientifically neutral mission statement on their home page “The UK Climate Impacts Programme (UKCIP) helps organisations to adapt to inevitable climate change. While it’s essential to reduce future greenhouse gas emissions, the effects of past emissions will continue to be felt for decades.“
On their headline messages page they have a list of global warming predictions and supporting evidence.  In this article we will examine some of their claims and evidence.
Claim: Summers will continue to get hotter and drier…

 Evidence: Total summer precipitation has decreased in most parts of the UK, typically by between 10 and 40% since 1961.

According to the UK Met Office, the summer of 2007 was the wettest summer on record.  Summer, 2008 was the wettest on record in Northern Ireland, and broke many local rainfall records in England.  The last hot day in London (30C or 86F) was on July 27, 2006.  London is normally one of the UK’s warmest locations in summer, and it has been 915 days since London has seen any “hot” weather.

Claim: Winters will continue to get milder and wetter…

 Evidence: Average winter temperature for all regions of the UK has risen by up to 0.7 °C since 1914..

The Met office reported last month: “Temperatures from the Met Office have revealed that the UK has had the coldest start to winter in over 30 years.” 
This month, the Met Office reported: “The British Isles has experienced almost a fortnight of freezing conditions. Temperatures as low as -9 °C have been fairly common throughout southern areas of the UK, with temperatures struggling to rise above freezing in some places.“
This winter has not only been unusually cold, but it has also been unusually dry in the UK.

Recreational boating during the winter of 2008-2009
Claim: Some weather extremes will become more common, others less common…

Evidence: The average duration of summer heatwaves has increased in all regions of the UK by between 4 and 16 days since 1961.
Evidence: The average duration of winter cold snaps has decreased in all regions of the UK by between 6 and 12 days since 1961.
Evidence: There has been a trend towards heavier winter precipitation for most parts of the UK since 1961.







As mentioned above, there have been no hot days in the UK for nearly three years.  The current winter has been one of the coldest and driest in recent memory.




Claim: Sea level will continue to rise…

Evidence: Global average sea level rose by between 10 and 20 cm during the twentieth century.
Evidence: The temperature of UK coastal waters has increased by between 0.2 and 0.6 °C per decade since 1985.


It is somewhat surprising that a scientific organisation would use this information in support of global warming.  Sea level has been rising nearly continuously since the end of the last ice age, 15,000 years ago.  The average sea rise rate has been about 80cm/century, 4X-8X higher than UKCIP’s reported current levels.

From: http://www.globalwarmingart.com/wiki/Image:Post-Glacial_Sea_Level_png
Additionally, there has been little change in sea level rise rates over the last 100 years.

From: http://www.globalwarmingart.com/images/thumb/0/0f/Recent_Sea_Level_Rise.png/700px-Recent_Sea_Level_Rise.png
Regarding their discussion of UK sea temperatures since 1985, there hasn’t been much glacial activity in the UK over the last 25 years and it is unlikely that UK ice sheet melt is adding much to sea level.  Their reported UK SST changes are more likely due to ocean circulation patterns like the AMO.  Current SST anomaly maps show ocean temperatures around the UK near or below normal.  And according to the University Of Colorado, global sea level has scarcely risen since 2005.

From: http://sealevel.colorado.edu/current/sl_noib_global_sm.jpg
One might think that taxpayer funded organisations like UKCIP would be required to keep their public statements a bit more up to date and accurate.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99384947',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWe were told that the mild winters we experienced in Europe were due to global warming. Now, suddenly, we are getting hit with yet another nasty cold winter.
Why? Guest writer Juraj Vanovcan presents his observations and interesting evidence that it has nothing to do with CO2. He presents what I think is an astonishing finding near the end.
===============================================================
Predicting The European Climate From The CET Record – Lesson Learned
By guest writer Juraj Vanovcan
This post was inspired by the article Negative AO bringing cold winters back to Europe.
Recalling the summers and winters of the early 1980s, it becomes obvious to me that it is the prevalence of air circulation that determines if a season is warm or cold. The very mild winter of 2006/2007 in Central Europe was characterized by a sustained flow of warm Atlantic air over the European continent, while the cold and snowy 2005/2006 winter received a lot of Arctic air entering the mid-latitudes from the North.
Air circulation is governed by pressure differences and basically this is what North Atlantic Oscillation is all about.
 
Fig 1 Example of positive NAO (Source: JISAO webpage).
I compared the NAO index with the European long-term climate record. Checking the Central European Record (CET) shown, one sees there is an obvious correlation between NAO and winter temperatures. The dark blue line is CET, orange/light blue is NAO.

Fig 2 North Atlantic Oscillation index compared with CET winter record, 1860-2010 (CET graph source: http://climate4you.com/ ).
As observed above, the NAO oscillates in an 80-year long sine wave cycle. The first period with mild winters happened in the 1920s, which of course we do not remember. The second positive phase began in 70s and mild winters in Central Europe become frequent since late 80s. It also seems that the current period with prevalent NAO-positive years has ended; the recent string of cold winters in North-Western Europe suggests this as well.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On the other hand, CET summers do not show correlation with NAO in the early part of 20th century. Comparing the CET summer anomalies with AMO index (detrended North Atlantic SST) however gives reasonable correlation again. Summers in the period 1930-1950 were often as warm as the recent ones, and the extremely hot and dry summer of 1946 in Central Europe with its catastrophic impact on crops has since never been repeated. However, warm summers in this period were combined with cold winters, like those of 1939/40/41.
Fig 3 Atlantic Multidecadal oscillation index compared with CET summer record, 1860-2010
It is worth noting that while the NAO peaked circa 20 years before the warm AMO phase centered on 1940, their warm phases were much closer to each the other in the later part of the century. It means that while Europe experienced cold summers and mild winters in 1920s and warm summers and cold winters in the 1940s, the last 20 years saw both warm summers as mild winters. This is also probably the reason why 2000-2010 decade is slightly warmer than 1940-1950.
Based on the observed SST and OHC record for North Atlantic, it seems however that AMO had peaked around 2005 and it is now heading down. This time, both NAO and AMO being in their negative phases will mean miserable summers and cold winters. Such shift in temperature trend is already being observed in the whole northern extratropics record.

Fig 4 Northern hemisphere north of 30N, HadCRUT3 data
IPCC attributed the post-1975 warming phase almost solely to anthropogenic reasons, namely to increased “greenhouse effect” caused by increase of CO2 molecules from 3.5 to 4 per other 10,000 molecules in the atmosphere.
We can conclude, however, that at least for Europe, observed warming is fully explainable by natural variations, two of which – AMO and NAO – had their positive phases overlaid during the last 20 years. Neither CO2, aerosols nor greenhouse effect theory are needed.
For those still seeking the anthropogenic signature in recent warming, here is a comparison of 1890-1920 warming trend compared against 1980-2010 warming trend in the winter CET record. The running 10-year mean is strikingly similar, following even minor dips and upticks.
Fig 5 CET winter record with late century warming superimposed on early century warming 
Extrapolations into the future may be tricky, especially when pulling some 20-year trend into year 2100, which seems to be a favorite practice in modern climatology. Observing the European climate record of the early part of 20th century and understanding its causes gives us much more predictive skill when forecasting climate for the next decades. All climate indicators today point to cooling, and not only in the European region.
Juraj Vanovcan (juraj.vanovcan@gmail.com)
26 November 2010
Share this...FacebookTwitter "
"
More harbinger of the Northern Hemisphere winter to come?







A bulldozer cleans snow on the Sichuan-Tibet  road in Nyingchi, southwest China’s Tibet Autonomous Region Oct. 30, 2008.  (Xinhua Photo)





LHASA, Oct. 30 (Xinhua) — The death toll has risen to seven, and one person  remains missing, as a result of the worst snowstorm on record in Tibet, local  authorities said Thursday. 
 The seven people killed either frozen to death or were crushed by  collapsing buildings. About 144,400 heads of livestock died in the storm, which  also knocked out telecommunications and traffic in parts of Shannan prefecture. 
In Lhunze County, 1,348 people stranded by damaged buildings or blocked roads  had been rescued, the county government said. Rescue operation for the remaining  289 trapped was still underway. 
 The worst-hit county had 36 consecutive hours of snowfall from Sunday,  with an average snow coverage of 1.5 meters. Four people died and one remained  missing in the snowstorm. 
 The rescued people have been moved to other villages, sleeping in schools  or government buildings. 
 A road linking Lhunze to Cuona County reopened on Thursday after 63 hours  of snow clearing efforts of armed policemen and transportation staff. 
 Cuona had been isolated from the outside for three days due to the road  blockage. 
 The Tibet regional civil affairs department has allocated relief  materials such as clothes and tents to the affected areas.
h/t to Dr. Roger Pielke


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b21998f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
And the hits just keep on coming…1,318,794 page views for January according to WordPress.

Thanks everybody!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e987edbe3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Today at 00 GMT (5PM PST) a new month started. Every time a new month of statistics starts being logged by WordPress for Watts Up With That, I say to myself, “there’s no way I’ll get this sort of traffic again”. And yet, again I’m surprised that WUWT not only met last months stats, but significantly exceeded them.
Thank you again, loyal readers.

Click for full sized image
It was one year ago that I moved from the Typepad blog to WordPress, and as you can see from above, the growth has been steady, except for one month, April. which had a slight dip.
For September 2008 the total was 846,193 page views, up from 667,215 page views in August 2008.
But there is a caveat, I think the real numbers are just shy of 800,000, because on the weekend of 09/20 and 09/21 I got quite a bit of unexpected traffic that I’m not sure is real or not. During that time, we got a lot of Spam on one particular older entry comparing UAH, RSS, HadCRUT, and GISS, but not anywhere near the numbers specific to that post, shown below:
Blog Stats Increase due to DOS “something”

Saturday 09/20     23,486
Sunday   09/21 25,319
Monday  09/22       1,006
Total: 49,811
You can read about it here in this entry
I checked with WordPress support, twice, and they assured me that the numbers are real, saying:
Hi,
Our stats expert has had a look and found no evidence of a DoS or anything untoward.  He says “the most plausible reason is an email newsletter featuring the URL, or else some other non-browser app loading the URL such as a feed reader. I have not been able to find any evidence of of a DDOS attempt or other “foul play.”
Separately, I’ve checked our security logs and see no other signs of activity that would normally indicate a blog under attack.
In short: we’re quite sure the traffic is genuine and doesn’t correspond with an attack of any kind.
Kind regards,
Alex
WordPress Support
Even so, I’m unconvinced. I got not one single comment added on that posting during the onslaught of traffic, almost 50,000 page views, which tells me the numbers aren’t real, no matter what WordPress support says.
Therefore I have decided to take the step of publishing an “adjusted” set of numbers this month. The difference is that instead of inflating the numbers, such as GISTEMP and USHCN adjustments do, I’m reducing them to what I consider a truly representative value for the month.
Raw WUWT September numbers:           846,193 page views
WUWT Spam Uncertainty numbers:           -49,811 page views (from 09/20 to 09/22)
Final Adjusted WUWT September numbers:    796,382 page views
Still, not too shabby.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bea60b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Japan’s boffins: Global warming isn’t man-made
Climate science is ‘ancient astrology’, claims report
By Andrew Orlowski The Register UK (h/t) from WUWT reader Ric Werme
UPDATE: One of the panelists (Dr. Itoh) weighs in here at WUWT, see below.
Exclusive Japanese scientists have made a dramatic break with the UN and Western-backed hypothesis of climate change in a new report from its Energy Commission.
Three of the five researchers disagree with the UN’s IPCC view that recent warming is primarily the consequence of man-made industrial emissions of greenhouse gases. Remarkably, the subtle and nuanced language typical in such reports has been set aside.
One of the five contributors compares computer climate modelling to ancient astrology. Others castigate the paucity of the US ground temperature data set used to support the hypothesis, and declare that the unambiguous warming trend from the mid-part of the 20th Century has ceased.
The report by Japan Society of Energy and Resources (JSER) is astonishing rebuke to international pressure, and a vote of confidence in Japan’s native marine and astronomical research. Publicly-funded science in the West uniformly backs the hypothesis that industrial influence is primarily responsible for climate change, although fissures have appeared recently. Only one of the five top Japanese scientists commissioned here concurs with the man-made global warming hypothesis.
JSER is the academic society representing scientists from the energy and resource fields, and acts as a government advisory panel. The report appeared last month but has received curiously little attention. So The Register commissioned a translation of the document – the first to appear in the West in any form. Below you’ll find some of the key findings – but first, a summary.
Summary
Three of the five leading scientists contend that recent climate change is driven by natural cycles, not human industrial activity, as political activists argue.
Kanya Kusano is Program Director and Group Leader for the Earth Simulator at the Japan Agency for Marine-Earth Science & Technology (JAMSTEC). He focuses on the immaturity of simulation work cited in support of the theory of anthropogenic climate change. Using undiplomatic language, Kusano compares them to ancient astrology. After listing many faults, and the IPCC’s own conclusion that natural causes of climate are poorly understood, Kusano concludes:
“[The IPCC’s] conclusion that from now on atmospheric temperatures are likely to show a continuous, monotonous increase, should be perceived as an unprovable hypothesis,” he writes.
Shunichi Akasofu, head of the International Arctic Research Center in Alaska, has expressed criticism of the theory before. Akasofu uses historical data to challenge the claim that very recent temperatures represent an anomaly:
“We should be cautious, IPCC’s theory that atmospheric temperature has risen since 2000 in correspondence with CO2 is nothing but a hypothesis. ”
Akasofu calls the post-2000 warming trend hypothetical. His harshest words are reserved for advocates who give conjecture the authority of fact.
“Before anyone noticed, this hypothesis has been substituted for truth… The opinion that great disaster will really happen must be broken.”
Next page: (at the Register)  Key Passages Translated

UPDATE: From Kiminori Itoh, Prof., Yokohama National University.
Hi everybody!
I am one of the five who participated to the article in the JSER journal, which may have seemed to you as a mystery from Japan. At first, I thank you for picking up our activity in Japan. I am a regular reader of several climate blog sites, and had been making some contributions mainly to Climate Science of Prof. Pielke. Actually, the information I gave in the article largely owes the invaluable information shown at this site WUWT as well as Climate Science and Climate Audit. Thus, I felt I should explain a bit about the article of JSER because, unfortunately, it is written in Japanese although it has partly been translated into English. 
Some readers of WUWT might remember my name; I had written a guest blog in Climate Science several months ago, when Roger kindly suggested me to introduce my new book “Lies and Traps in Global Warming Affairs.” Yes, I am regarded as one of the most hard-core AGW skeptics in Japan, although I myself regard me as a realist in this issue.
The article of JSER has been composed of discussions between the five contributors, made through e-mail for several months, and was organized by Prof. Yoshida of Kyoto University (an editor of the JSER journal). Our purpose was to invoke healthy discussions on the global warming issue in Japan. The JSER journal was selected as a platform for this discussion just because Prof. Yoshida has a personal interest in this issue and he is an editor of the journal. 
Thus, it is not correct if one thinks that the discussion represents the opinion of the journal’s editors or of the society JSER. In fact, none of the five contributors belong to the JSER, and Prof. Yoshida kept his attitude neutral in the article.
All the contributors are well-established researchers in different fields and each has characteristic personal opinions on the AGW issue. Only one (Dr. Emori, National Institute of Environmental Sciences, Japan) represents IPCC. Other members are more or less skeptical of the conclusions of IPCC. For instance, as translated into English, Dr. Kusano made a severe critique on climate models; he himself is a cloud-modeler, so that his critique seems plausible. Prof. Akasofu is well known as an aurora physicist, Prof. Maruyama is famous for his ideas in geophysics, and I myself have sufficient academic record in environmental physical chemistry (more than 160 peer review papers).
We know that our try this time is small one, and its impact has a limitation especially due to language problem. Nevertheless, we believe that the discussion was useful and informative for everyone interested in the controversies associated with the AGW issue. In March, another article will come also in the JSER journal because the discussion received much interest from the readers of the journal. 
Any comments and opinions are welcome and very helpful for us. 
Thank you again.
Based on Dr. Itohs comments, I’ve amended the headline to be more reflective of his first hand account on the report. – Anthony



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e986925e8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
For Europe, the huge welcoming applause Palin got after being introduced just before her speech was loud thunder accompanied by tectonic tremors. The signs for catastrophic (political) climate change are clearer than ever.

UPDATED 8/29 12.15 p.m.This blog is supposed to focus on climate and energy, and not politics. But the future of both issues are tangled up in politics, and so it is unavoidable.
Make no mistake about it. The ruling elitists in Europe and Germany are about as far from America’s foundational principles as they have ever been. Looking at the German media headlines yesterday, Beck’s rally was labelled as a gathering of “ultra-conservatives”, “religious right”, “God-fearing, gun-toting Americans”-patriotic folks in T-shirts and caps who chant “USA! USA!” and “God bless America!”. It’s just so unsophisticated – so say the euro-elitists through their noses. Europe’s elite media are marginalising and villainizing American conservatism. Call it the Cold War of the 21st Century. That’s the reality.
The euro-media of course would never broadcast the speeches, fearing their listeners might draw the “wrong conclusions”. So the media select the pictures they think their viewers ought to see and deliver pre-packaged thoughts and commentary to tell them how to think. It’s the responsible way of informing citizens.
Europe’s spite for original American ideals is deep.
Should someone like Palin take the Presidency in 2012, then prepare for headlines from Europe that normally would be reserved for history’s worst dictators. It’ll be “freeze it, personalise it and polarize it” in über-doses. In secular environmental Europe, things like Christianity, God, freedom, small government, life, capitalism are extreme, and thus talk about such things must be reigned in and controlled by the heavy hand of the state and media.
America itself is deeply divided, and it’s going to stay that way. And there’s no question which side Europe’s elites in media and government align themselves with. It’s the believers of the USA’s Constitution, i.e. conservatism, against the rest of the world. It really is that precarious. Those who believe in the founding principles of America have very few allies left in the world.
The differences between the American right and European left (the American left is the European left) are now far beyond reconciliation. The two sides could not be more polarised. What’s the way out? What can be done when reconciliation has no chance? The American right, the believers of its Constitutional principles, are alone on the globe. Do you compromise? Do you try to change the mind of the rest of the world? Conflict and uprising seem to be pre-programmed.
Although American conservatism appears to be making a comeback now, the reality is that it is completely surrounded on all fronts, almost hopelessly outnumbered, with very few political weapons. The NGOs, the UN, the elite media, academia, Hollywood, Russia, China, Europe, the Third World, the “environmentalists”, the billionaires, the special interests, the powerful US Left and all dictators are arrayed against American conservatism. People are to be ruled by governments, and not vice versa.
As President, who could someone like Palin possibly find as an ally? Other than Poland, the Czech Republic, Israel and Georgia, you’d be hard pressed to name one that’s influential. I’m not saying Palin should not be elected for that reason, I’m just telling you what the world is like for the last bastion of American ideals.
The odds are very long.
Share this...FacebookTwitter "
"
Newly discovered evidence that polar bears, CO2, climate change, and the sun are intimately connected in ways never envisioned.
No wonder the sun seems to be slowing down.

With apologies to the French, and everybody else for that matter.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bfbf49f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the grand scheme of things, the Keystone XL pipeline is of little significance to anything tangible — including gas prices, jobs, and, yes, the environment. The price of gasoline is for the most part determined by global forces in the oil market, of which the Keystone XL oil will be but a drop.



The pipeline’s job‐​creation potential is largely ephemeral; although the construction of the pipeline will create tens of thousands of jobs, the operation of it thereafter is expected to create fewer than 100 permanent jobs. And least significant of all is its impact on climate change. If it were to operate at full capacity for the next 85 years, the consumption of oil delivered by the pipeline would lead to global warming of less than a hundredth of a degree — an amount that is scientifically undetectable and environmentally inconsequential.





The fight over the Keystone XL pipeline is, and always has been, nothing but a symbol.



The fight over the Keystone XL pipeline is, and always has been, nothing but a symbol — of dedication to environmentalism, for the Left; of resistance to excessive government interference, for the Right. Huge amounts of time and money have been spent — or, more accurately, wasted — arguing fruitlessly that it is something more concrete. Practically speaking, its implications are tiny.



This is not true, however, of the litany of carbon‐​dioxide‐​limiting regulations that President Obama has imposed through the EPA. These onerous regulations try to force a reduction in demand through increasing the price of energy derived from fossil fuels (that is, coal, oil, natural gas). They will infiltrate the daily lives of each American, making everything more expensive and potentially threatening the reliability of America’s energy supply. And for what?



As far as the environment is concerned, the EPA’s meddling will have no demonstrable effect. Even a complete cessation of all greenhouse‐​gas emissions from the U.S., starting now and lasting forever, would avoid only a fraction of a degree of global temperature rise. At the local level, where people interact with the climate, natural variability would swamp any effect that U.S. emission reductions may have on the daily weather.



And for all the talk of an increasing intensity and frequency of extreme weather events — hurricanes, tornadoes, droughts, floods, heat waves, cold snaps, and any other manner of unpleasant weather — there is little actual scientific research that either identifies much of a trend, or unequivocally links such weather events to climate change, much less traces a direct link to carbon‐​dioxide emissions.



So, when it comes to challenging Obama on climate policy, congressional Republicans should set their sights on issues with tangible and wide‐​ranging impacts, and be willing to trade off largely symbolic projects like the Keystone XL pipeline.



Keystone XL has little to offer besides a moral victory. Limiting the economic damage that the EPA regulations may inflict, on the other hand, will be a benefit to all Americans, for years to come.
"
"

The busted forecast for last week’s East Coast snowstorm points to a very troubling aspect of modern life: We now believe the output of computers more than we trust our own eyeballs. 



At noon on January 25, the most sophisticated weather‐​forecasting model in human history predicted a total snowfall for Washington, D.C., of somewhat less than an inch in the succeeding 36 hours. All the human forecasters I know went along. 



The computer model, named for the Greek letter eta (pronounced “ay‐​ta”), which describes its mathematical coordinate system, forecast a storm far out to sea, sparing the I-95 megalopolis that stretches from Richmond to Boston. Instead, eta confined its significant snow to a sliver of southeastern Virginia, before shoving everything north, east and offshore. 



Fear in the forecasting community was palpable as minute‐​by‐​minute updates from our national radar network painted an army of green and yellow monsters marching north and west toward our nation’s capital. By 9 p.m. they were closer than most of the Confederates ever got, and still the forecast was for a minor dustup. It wasn’t until eta, whose major update cycle is 12 hours, was run again, that forecasters decided disaster was at hand. 



Why didn’t we believe our eyes when the model was clearly busting? The truth is, as models become more sophisticated, forecasters are increasingly reluctant to abandon them, even in the face of contrary evidence. But, “the computer eta my forecast” is an insufficient excuse. 



Would that this were the case merely for the 48‐​hour forecast. Unfortunately, it appears that the same pathology has infected the 48‐​year projection. Just like weather forecast models, our climate simulations have become increasingly sophisticated in recent years. And just like the situation with the recent snow, there has been incontrovertible and advancing evidence, this time over the course of the last two decades, that climate simulations are making a disastrous error, and it has taken forever for forecasters to acknowledge it. 



Every computer model predicts that the entire troposphere, or roughly the bottom 40,000 feet of the atmosphere, should be warming rapidly. Most models even predict that much of the warming accelerates with height. 



But, for over 20 years now, we have two independent measures of temperature — satellites and weather balloons — that show no net warming at all from 5,000 feet skyward. In 1996, we had 17 years of satellite data, and yet the “Policymakers Summary” (the only part that gets read) of the U. N. Intergovernmental Panel on Climate Change report contained not one mention of the word “satellite.” That is the report usually cited as the “consensus of scientists.” 



If that was the 1996 consensus of my profession, it was largely a consensus of ostriches, not scientists. How long could we continue to sweep under the computer the fact that somehow we got 88 percent of the atmosphere wrong? 



Not very long. Here is what the National Research Council now says about our ability to model climatic behavior: “It is clear,” they recently wrote, “that reconciling the discrepancy … is not simply a matter of deciding which [climate model] is correct.… In the long term it will require major advances in our ability to interpret and model [atmospheric behavior].” In other words, our models don’t work. Here, for once, eyeballs have triumphed over the computer. 



Most people use weather forecasts to plan the future. The Kyoto Protocol on climate change was a response to a climate forecast. This onerous document would require the United States to drastically reduce the energy use that has propelled us on our gee-doesn’t-your-401 k -look‐​great way for the last two decades. Everyone (except, maybe, President Clinton, judging from his State of the Union Address) believes it will cost us a fortune. 



When it finally became clear that Washington was going to get buried in the last snowstorm, the federal government shut itself down. Now that we have admitted that our climate forecasts for the next century aren’t worth much at all, can we please shut down the Kyoto Protocol on climate change?
"
"

Mr. Chairman, distinguished members of the subcommittee:



My name is Roger Pilon. I am a senior fellow at the Cato Institute and the director of Cato’s Center for Constitutional Studies.



I want to thank Chairman Hyde of the committee and Chairman Coble of the subcommittee for their invitations to me to testify on the important issue of “Judicial Misconduct and Discipline.” These hearings have been called, I understand, because of a concern that a number of people have expressed about “judicial activism”–the practice by judges of applying to cases before them not the law but principles or values that are no part of the law. Because such a practice is thought by many to constitute judicial misconduct, some in Congress are searching for ways to discipline it.



 **I. Summary**



At the outset, let me summarize my thoughts on this subject, then discuss it in somewhat more detail. There can be no question that judicial activism, as just described, has been a problem in our legal system for some time. The power of the judiciary under our Constitution to declare the law and decide cases under that law is awesome; when abused, that power is too often beyond reach. At the same time, I believe that many of those who have complained most often about judicial activism have overstated and misstated the problem, thus distracting us from the real issue–legislative activism on the part of Congress, which leads to judicial activism.



Overstating the problem. Many of the examples of “judicial activism” that are cited turn out, when examined more closely, not to be cases in which the judge failed to apply the law but applied the law differently, or applied different law, to reach a result different than the result thought correct by the person charging activism. To be sure, there is no bright line between failing to apply the law and wrongly applying the law or applying the wrong law, but when that distinction is drawn, it turns out that there are fewer cases of true judicial activism than at first may appear.



Misstating the problem. More importantly, the problem of “judicial activism” is seriously misstated when it is cast, as it often is, as involving judges overruling the will of the people. In our legal system, judicial review often requires a judge to do just that. In such a case, were the judge to defer to the political will, exercising “judicial restraint” when the law requires active judicial intercession, that restraint would itself be a kind of activism, for it would amount to an “active” failure to apply the law in deference to democratic or majoritarian values. The judge in such circumstances would be shirking his judicial responsibilities every bit as much as if he overrode a legitimate exercise of political will in the name of other values.



Thus, as terms of art, judicial “activism” and “restraint” can be quite confusing and even misleading. What is more, they are often used in ways that camouflage the real issues. What we all want, I assume, is judges who are neither “active” nor “restrained” but “responsible”–responsible to the law. But when the law is unclear or inconsistent, judicial responsibility may be difficult to achieve–and “activism” inevitable. In the end, therefore, our substantive law may be the ultimate source of the problem before us today. That, in fact, is what I will argue shortly. Let me begin, however, with a brief overview of the complaints. [1]



 **II. The Critics of Judicial Activism**



Complaints about “judicial activism,” however formulated, can be found from our inception as a nation. In their modern form, however, they have come largely since the advent of the Warren Court and most often from political conservatives. My fellow panelist today, Professor Lino Graglia, with whom I have debated the issue more than once, has put the complaint starkly:



… the thing to know to fully understand contemporary constitutional law is that, almost without exception, the effect of rulings of unconstitutionality over the past four decades has been to enact the policy preferences of the cultural elite on the far left of the American political spectrum. [2]



“That is exactly right,” comments Judge Robert Bork in his recent best‐​seller, _Slouching Towards Gomorrah_ , “and the question is what, if anything, can be done about it.” [3] I gather that these hearings are a partial answer to that question.



The bitter confirmation battle that followed Judge Bork’s Supreme Court nomination a decade ago had a way of concentrating the issue for many, of course. Still, the issue has been in the air since the 1950s, covering subjects as various as civil rights, apportionment, federalism, speech, religion, abortion, education, criminal law and procedure, and much else. And in each case, the complaints from conservatives have been essentially the same.



Speaking before the Federalist Society’s 10th anniversary lawyers convention last November, for example, Senator Orrin Hatch, chairman of the Senate Judiciary Committee, summarized the issue from his perspective:



What is at stake … is nothing less than our right to democratic self‐​government as opposed to … “Government by Judiciary.” For when we commission judicial activists who distort the Constitution to impose their own values, policy preferences, or visions of what is just or right, we are in effect sacrificing our ability to govern ourselves through the democratic political processes to the whims and preferences of unelected, life‐​tenured platonic guardians. [4]



Judges “must _interpret_ the law, not legislate from the bench,” Senator Hatch continued. “A judicial activist, on the left _or_ the right, is not, in my view, qualified to sit on the federal bench.” [5]



In a similar vein, little more than two months ago Senator John Ashcroft, chairman of the Constitution Subcommittee of the Senate Judiciary Committee, told the Conservative Political Action Conference at its annual meeting that it was time “to take a broader, comprehensive look at the alarming increase in activism on the court.” [6] Asking what we can do to put an end to “judicial tyranny,” Senator Ashcroft called for rejecting “judges who are willing to place private preferences above the people’s will.” [7]



Not to be outdone by the Senate, on March 11 House Majority Whip Tom DeLay told editors and reporters at the _Washington Times_ that “as part of our conservative efforts against judicial activism, we are going after judges” and are “right now” writing articles of impeachment. [8] Those sentiments were echoed two days later by Congressman Bob Barr of this subcommittee when he appeared on CNN’s “Crossfire.” Clearly, perhaps as never before, the issue of judicial activism is on the nation’s agenda. [9]



 **III. Overstating the Problem**



It is not entirely clear just what has brought the judiciary and its methods to the nation’s attention at this point in time. Cynics point to the need for something–some issue–in a drifting Republican Party: “The revolution is in the doldrums. Nobody’s got a plan; nobody’s got a direction.” [10] Others, however, have noted a rising frustration among conservatives over their relative ineffectiveness on the judicial front despite having dominated the judicial selection process since the Nixon years. [11] And still others cite a series of recent cases that have seemed to crystalize complaints about judicial activism: the district judge who stayed the California Civil Rights Initiative (CCRI); [12] the New York judge who suppressed evidence in a drug case, saying the police had no reason to stop the suspects; [13] the decision by the Supreme Court that the Virginia Military Institute had to become coeducational. [14]



Looked at in broad perspective, there can be no question that the drift in American law over the past 40 years and more has been in large part to the left, as that term is ordinarily understood. And a good part of that drift has resulted from court decisions. Yet by no means can all or even most of the drift be attributed to the courts. Moreover, even that part that has resulted from court decisions does not arise entirely or even primarily from “judicial activism”–not unless that idea is stretched to include every decision that conforms to some leftist political agenda.



In fact, when we look at most such decisions closely, we rarely find that the judge or justices “legislated.” To be sure, they often reach results consistent, if not with their “whims,” at least with their “values, policy preferences, or visions of what is just or right.” But those results can usually be tied to some legal anchor, even if it takes some stretch to do so.



Take the recent CCRI decision by U.S. District Court Judge Thelton Henderson, which enjoined enforcement of the initiative shortly after it was passed by some 54 percent of California’s voters. Many critics of the judiciary immediately pointed to the decision as a blatant example of judicial activism. Judge Henderson’s opinion was a stretch, to be sure. But it was not without legal foundation, citing _Hunter v. Erickson_ , 393 U.S. 385 (1969) and _Washington v. Seattle School District No. 1,_ 458 U.S. 457 (1982). Moreover, as we know, the case has taken the normal appellate course; the decision has since been reversed by the U.S. Court of Appeals for the Ninth Circuit; [15] and plaintiffs have just filed a petition for certiorari with the Supreme Court. We are likely to learn from the Court whether the cases Judge Henderson relied upon in fact apply or are still good law. In the meantime, however, we are hard pressed to say that his decision was “lawless,” however strained it may have been.



One could review putative cases of judicial activism almost _ad infinitum_ , of course, but the fact remains that the better part of such cases do not exhibit judicial lawmaking, just better or worse judicial reasoning. It is no small irony, however, that when we do come across a genuine case of blatant judicial activism that cuts the other way, politically, many conservative critics of the judiciary are strangely silent. That was pointed out just last week, for example, by conservative constitutional scholar Bruce Fein in an op‐​ed in the _New York Times_ , citing the current controversy over the decision of an Alabama state judge to defy a long line of Supreme Court rulings on the separation of church and state “by posting a copy of the Ten Commandments in his courtroom and inviting clergy to lead juries in prayer,” [16] even after a state appellate court found the practices unconstitutional.



 **IV. Misstating the Problem**



In the end, therefore, those who are concerned about judges who seem always to be leaning to the left may be better advised to look less to the judicial role in our system–to the practice of judicial review–and more to the reasoning judges employ in performing their roles and, more importantly, to the sources they employ when doing their reasoning. Bad reasoning is just that and should be called that, not called judicial “activism.” But bad law, from which so much bad reasoning proceeds, is another matter. We should hardly be surprised that judges today are thought so often to be engaged in “judicial activism” when they are called upon so often to apply law that is inconsistent, incoherent, and fairly invites them to make all manner of value judgments. In such circumstances, they can hardly be seen to be doing anything but legislate.



We come, then, to what in fact is the crux of the matter. Under our system of law, the role of the judge should be much simpler than it has come to be. The problem, however, does not go back just 40 years, as too many conservatives believe. Rather, its institutional roots are in the New Deal. And its ideological roots are in the Progressive Era, when we stopped thinking of government as a “necessary evil,” as the Founders had conceived of it, and started thinking of government as an engine of good, an instrument for solving all manner of social and economic problems. Standing in the way of carrying out that agenda, of course, was a constitution that established a government of limited, enumerated powers–a constitution that held, more or less, until the New Deal. As we all know, however, when President Roosevelt was unable to get his programs past the Court–there being no authority for them under the Constitution–he threatened to pack the Court with six additional members. Not even Congress would go along with that. Nevertheless, the Court got the message; there was the famous switch in time that saved nine; and by 1938 the Court had essentially turned the Constitution on its head, as New Deal architect Rexford Tugwell would later tell us the administration meant for it to do. [17]



In a nutshell, a document of delegated, enumerated, and thus limited powers became in short order a document of effectively unenumerated powers, limited only by rights that would thereafter be interpreted narrowly by conservatives on the Court and episodically by liberals on the Court. Both sides, in short, would come to ignore our roots in limited government, buying instead into the idea of vast majoritarian power–the only disagreement being over what rights might limit that power and in which circumstances. Indeed, we need look no further than to Judge Bork–no liberal he–to see the new vision stated–and wrongly ascribed to James Madison. The “Madisonian dilemma” that constitutional courts face, Bork tells us, is this:



[America’s] first principle is self‐​government, which means that in wide areas of life majorities are entitled to rule, if they wish, simply because they are majorities. [It’s second principle is] that there are nonetheless _some_ things majorities must not do to minorities, _some_ areas of life in which the individual must be free of majority rule. [18]



That gets the Madisonian vision exactly backward, of course. America’s first _political_ principle may indeed have been self‐​government, but its first _moral_ principle–and the reason the people instituted government at all–was individual liberty, as the Declaration of Independence makes plain for “a candid world” to see.



Indeed, we did not throw off a king only to enable a majority to do what no king would ever dare. Rather, the Founders instituted a plan whereby in “wide areas” individuals would be entitled to be free simply because they were born so entitled, while in “some” areas majorities would be entitled to rule not because they were inherently so entitled but as a practical compromise.



That gets the order right: individual liberty first; self‐​government second, as a means toward securing that liberty–with wide berths to state governments, which were later reined in by the Civil War Amendments. That is why the Constitution enumerated the powers of Congress and the executive, to limit them. And that is why the Bill of Rights concludes with the Ninth and Tenth Amendments: to make clear that Americans begin and end with their rights, enumerated and unenumerated alike, while government proceeds only with the power it is given.



The New Deal changed all that, of course, not by amending the Constitution, the proper method, but by radically reinterpreting it: in particular, by reading the General Welfare and Commerce Clauses not as shields against power, as they were meant to be, but as swords of power; then by turning the Bill of Rights into a document of “fundamental” and “nonfundamental” rights. [19] None of that was found plainly in the Constitution–to the contrary, the entire document tends plainly the other way. Rather, it was invented virtually out of whole cloth, by the New Deal Court, to make way for the New Deal’s political agenda.



Our modern problem of overweening, inconsistent, incoherent statutory law began, then, not with an activist Court–to the contrary–but with an activist Congress and executive branch, bent on expanding government power. In time, however, the problem was abetted by an activist Court–succumbing to pressure from the political branches. But as noted earlier, the Court’s “activism” was not as we think of it today–a search for rights not apparent in the Constitution. Rather, it was activism in finding rationales for power–what conservatives today call deference to the political branches.



It needs to be said again, however, that the New Deal Court’s activism was not entirely without legal foundation. The sources for the Court’s rulings were there, in the Constitution, even if it did take a high degree of creativity, to be charitable, to draw them out, and even if doing so did fly in the face, for the most part, of a century and a half of constitutional jurisprudence that went the other way.



We come, then, to the bottom line in all of this. Law, including constitutional law, is not written in immutable stone. It is to some extent malleable, of necessity, and is given life by those charged with giving it life–the judiciary. In doing their work, however, judges do not work in a vacuum. They work instead in a larger political climate. If we who shape that climate persist in believing that it is proper for government to be addressing our every problem, no matter how trivial or personal, and persist in believing that our Constitution can legitimately be read to authorize that result, then we should not be surprised that the judiciary is dragged along to play its part in the process–today, often, to try to undue the mess that legislatures make of the effort. [20]



Yes, judges today often thwart the majoritarian will–as a vestige, perhaps, of their former principal role. Just as often, however, a judge may see himself as simply a facilitator in the grand enterprise of government. We are coming to the close of what has rightly been called the century of government–more accurately, the century of failed government planning. If we are unhappy with the role the judiciary sometimes plays in this setting, it may be that we need to look first to the material we give judges to work with–the reams of statutory material we have enacted over the course of the century.



The Founders had a simpler vision in mind when they set out to craft our legal order. They left most human affairs to private ordering, not to government planning. That gives the judiciary–and Congress–relatively little to do. Is that not what critics of judicial activism want?



A curriculum vitae is attached. Pursuant to House Rule XI, clause 2(g)(4), neither I nor the Cato Institute receives any federal funds–as a matter of principle.



[1] I have discussed the issues that follow more fully in: “Congress, the Courts, and the Constitution,” _Cato Handbook for Congress_ (105th Congress), ch. 3 (esp. pp. 36–42), (1997); “A Government of Limited Powers,” _Cato Handbook for Congress_ (104th Congress), ch. 3 (1995) (reprinted as “Restoring Constitutional Government,” _Cato’s Letter No. 9_ (1995)); “Rethinking Judicial Restraint,” _Wall Street Journal_ , Feb. 1, 1991, at A10 (op‐​ed); “Constitutional Visions,” _Reason_ , Dec. 1990, at 39–41 (review of Robert Bork’s _The Tempting of America_ ); “Legislative Activism, Judicial Activism, and the Decline of Private Sovereignty,” in _Economic Liberties and the Judiciary_ (J. Dorn & H. Manne eds., 1987); and “On the Foundations of Justice,” 17 _Intercollegiate Rev._ 3 (1981).



` `[2]Lino Graglia, “It’s Not Constitutionalism, It’s Judicial Activism,” 19 _Harvard Journal of Law & Public Policy_, 293, 298 (Winter 1996).



[3]Robert H. Bork, _Slouching Towards Gomorrah_ 114 (1996).



[4]“Remarks of Sen. Orrin Hatch Before the Federalist Society’s 10th Anniversary Lawyers Convention,” Senate Judiciary Committee News Release, Nov. 15, 1996, at 4.



[5]Id., at 5 (original emphasis).



[6]John Ashcroft, “Courting Disaster: Judicial Despotism in the Age of Russell Clark,” March 6, 1997, at 4 (MS available from the office of Senator Ashcroft).



[7]Id., at 3.



[8]Ralph Z. Hallow, “Republicans out to impeach ‘activist’ jurists,” _Washington Times_ , March 12, 1997, at 1. See also Katharine Q. Seelye, “House G.O.P. Begins Listing A Few Judges to Impeach,” _New York Times_ , Mar. 14, 1997, at A24.



[9]This very brief overview barely touches on the vast body of both scholarly and popular literature on the subject, to say nothing of political activism about judicial activism. In this last category, for example, is the Judicial Selection Monitoring Project of the conservative Free Congress Foundation’s Center for Law & Democracy, which on January 27, on behalf of 260 grassroots organizations and 35 radio and television talk show hosts, petitioned President Clinton and members of the Senate to nominate and confirm only those candidates for the federal bench who are committed to judicial restraint.



[10]Michael Kelly, “TRB from Washington: Judge Dread,” _The New Republic_ , Mar. 31, 1997, at 6. See also Laurie Kellman, “Republicans rally ’round judge‐​impeachment idea,” _Washington Times_ , Mar. 13, 1997, at A1: “The plan is aimed in part at reviving Republican morale, which has flagged this year because of Mr. Gingrich’s ethics troubles and the majority’s sparse floor schedule,” at A18.



[11]See, _e.g._ , Terry Eastland, “Deactivate the Courts,” _The American Spectator_ , Mar. 1997, at 60. For a fuller treatment of why conservative efforts to influence the courts have been so unsuccessful, see James F. Simom, _The Center Holds: The Power Struggle Inside the Rehnquist Court_ (1995). For a critique of that book, and the Court itself, see Roger Pilon, “A Court Without a Compass,” 40 _New York Law School Law Review_ 999 (1996).



[12]Coalition for Economic Equity v. Wilson, 946 F. Supp. 1480 (N.D. Cal. 1996).



[13]United States v. Bayless, 913 F. Supp. 232 (S.D.N.Y.), rev’d on rehearing, 921 F. Supp. 211 (S.D.N.Y. 1996).



[14]United States v. Virginia, 116 S. Ct. 2264 (1996).



[15]Coalition for Economic Equity v. Wilson, 1997 U.S. App. LEXIS 6512 (9th Cir.).



[16]Bruce Fein, “Judge Not,” _New York Times_ , May 8, 1997, at A39. Cf. Debbie Kaminer, “Thou Shalt Not Display the Ten Commandments in Court,” _Legal Times_ , May 5, 1997, at 27; Terrence P. Jeffrey, “Governor James at the Courthouse Door,” _Human Events_ , May 9, 1997, at 6.



[17]“To the extent that these [New Deal policies] developed, they were tortured interpretations of a document [ _i.e._ , the Constitution] intended to prevent them.” Rexford G. Tugwell, “A Center Report: Rewriting the Constitution,” _Center Magazine_ , Mar. 1968, at 18, 20.



[18]Robert H. Bork, _The Tempting of America_ 139 (1990)(emphasis added).



[19]I have discussed these issues more fully in Roger Pilon,“Freedom, Responsibility, and the Constitution: On Recovering Our Founding Principles,” 68 _Notre Dame Law Review_ 507 (1993).



[20]Thus, the Court has long been criticized by conservatives for its 1971 decision in Griggs v. Duke Power Co., 401 U.S. 424, which gave rise to the “effects test” in antidiscrimination law and to a host of affirmative action programs. But in interpreting the language of section 703 (h) of the Civil Rights Act of 1964, which authorizes “any professionally developed ability test” that is not “designed, intended, _or used_ to discriminate because of race” (at 433, emphasis by the Court), the Court simply drew upon the ambiguity of “used.” Congress could later have addressed that ambiguity, of course, but it did not. In cases like this, then, responsibility rests ultimately with Congress.
"
"
Share this...FacebookTwitterMeteorologist Karsten Brandt of donnerwetter.de projects more cold winters ahead. (Photo: Donnerwetter.de)
So forget the Met Office and PIK, who have proven themselves to be quite the laughing stocks. All that good money flowing into these institutions, and such rubbish coming out.
Meteorologist Karsten Brandt of German weather service company donnerwetter.de provides us with an outlook for 2011 and the next 10 years ahead. Read here (in German).
More cold winters over the next 10 years
Lately we’ve been hearing a lot about the North Atlantic Oscillation (NAO) going negative, which appears to be linked to quiet solar activity. (On the other hand we’ve been hearing from AGW true believers that the negative AO is caused by sea ice changes, which are caused by Arctic warming, which is caused by man-made trace CO2 emissions).
Clearly though, the NAO plays an important role in Europe’s weather. Knowing what the NAO will do for the next 10 years allows you to make a predictions for Europe’s climate ahead. Karsten Brandt writes:
It is even very probable that we will not only experience a very cold winter, but also in the coming 10 years every second winter will be too cold. Only 2 of 10 will be mild.
Dr Brandt is not some lone guy out there making this kind of blasphemous prediction. A large number of meteorologists and climatologists are projecting the same, e.g. like Accuweather’s Joe Bastardi and warmist climatologist Mojib Latif. Even the Potsdam Institute for Climate Impact Research is joining the chorus, but claiming the cold is due to warming.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




2011 in Germany about 1°C cooler 
Over the short-term Dr Brandt says that making a reasonable forecast for the next 2 months is not that hard, and that one can generate a pretty good idea of how the rest of the year will develop. Last year much of western Europe had a cooler year. What ‘s the outlook for Germany for 2011? Brandt writes:
Compared to the last 20 years, 2011 will turn out with a high probability to be too cold. Instead of 11°C in the west, or 10°C in the north and east, we expect only 8-10 °C, i.e. 1°C colder than the last years. Especially the cold start in the year and the suspected once again colder end of the year will pull the year’s temperature down.
Hopefully municipalities will procure the necessary wintertime snow-clearing equipment and stockpile sufficient amounts of sand and road salt so that next winter citizens will not have to endure the horrific traffic conditions that they are now experiencing over much of Europe. And you home centers ought to think about ordering snow shovels next fall.
But don’t despair, Brandt says the chances for a warm spring and a warm summer month aren’t bad.
Finally, here’s what the National Weather Service is showing:

Share this...FacebookTwitter "
"

On April 7, I wrote about global warming “hotheads,” who dominate the science profession for a lot of very sound economic, incentive‐​based reasons.



We don’t shell out multimillion‐​dollar grants to people who say something isn’t a problem. Recipients of this largess peer‐​review each other’s papers. There’s a lot of incentive to give a bad review to a manuscript downplaying the issue and to give a great one to the paper describing an upcoming apocalypse.



Anyone who disagrees with this should spend some time reading the “climategate” e‐​mails purloined from the Climate Research Unit at the University of East Anglia in November 2009.



Transport yourself from the biased world of peer‐​reviewed science to the biased world of amateur climatology on the Internet. They really aren’t very different; they’re just symmetrically opposite.



In the Internet world where the flatliners live, there’s no such thing as global warming. Where the hotheads reside, it’s everywhere and all the time. Some flatliners even doubt the whole notion of the greenhouse effect — the recycling of infrared radiation by water vapor and carbon dioxide (and a few other things) — that keeps the lower atmosphere about 60 degrees warmer than it would otherwise be.



Both groups are delusional. Hotheads are _convinced, mind you_ that the Koch Brothers are behind the flatliners, and flatliners are _convinced, mind you_ that there’s a hothead conspiracy coordinated by George Soros. Both view each other as murderers of the world: Hotheads will kill the economy, while flatliners will destroy civilization.



Here is the flatliner Holy Picture:





This is the global surface temperature departure from the 1961–1990 average, also from the CRU at East Anglia. (Despite all the climategate hubbub, this is still the reference standard in the business). It’s obviously flat, giving rise to the flatliner mantra: “No warming in 14 years — the same time in which the greatest increases in atmospheric carbon dioxide occurred.” Or, put in more stark terms, “Global warming is a commie plot.”



The flatline argument happens to be popular because it is occurring now, and few people outside of Aspie climate guys like me can remember the weather more than a year or two back.



Here’s a little secret about global warming. The central tendency of computer models using input that pretty much mimics the observed changes in carbon dioxide is to produce a constant (not an increasing) rate of warming.



There are many reasons for this. The response of surface temperature to an increment of carbon dioxide is logarithmic. So for every part per million (ppm) increase, a little less warming is generated. But the actual increase in its atmospheric concentration is a low exponent. When we started monitoring it at Mauna Loa in 1957, CO2 was growing at about 0.75 ppm annually. Now it’s growing at around 2 ppm.



The addition of a logarithmic response to an exponential increase in the cause of something can in fact add up to a straight line, which is very obvious in a suite of the temperature projections made by our friends at the U.N.



There are two warming periods in our recent history. One, in the early 20th century, could not have been caused by carbon dioxide, because we simply hadn’t put very much in the air back then. The second one, which begins in the mid‐​1970s, is much more suspicious because it has been accompanied by a cooling of the stratosphere and is accentuated at high latitudes in the Northern Hemisphere and in the winter, which is what one would expect from increasing CO2.



Here’s the East Anglia history since then, with a straight line fit to the data:





The fit of a constant trend to the overall data is striking, despite the fact that indeed there is no net warming in the last 14 years. In fact, fitting any simple curve to the data does no better than the straight line.



So much for the flatliners. They have lived in fortuitous times. Stay tuned for an analysis of the lukewarmers.
"
"

As part of a yearly tradition, the Cato Institute and Heritage Foundation co-host a debate in which interns of both think tanks debate whether conservatism or libertarianism is a better ideology. Following this year’s debate, the Cato Institute conducted a post-debate survey of attendees to ask who they thought won the debate and what they believe about a variety of public policy and social issues.   
  
The survey finds that millennial conservative and libertarian attendees agree on matters of free speech and religious liberty, the size and scope of government, regulation, health care and what to do about climate change. However, striking differences emerge between the two groups particularly on matters of immigration, the temporary Muslim travel ban, gender pronouns and bathrooms, government’s response to opioid addiction, the death penalty, religious values in government, domestic surveillance, foreign policy, as well as evaluations of the Trump administration.   
  
_Full LvCDebate Attendee Survey results foundhere_   
  
**Priority Differences and Similarities**   
  
Examining conservative and libertarian millennial attendees’ issue priorities offers a quick overview of their similarities and differences. The survey asked attendees how concerned they are about 21 different issues:   








As the chart shows above, conservative millennials are more concerned about morality in society, abortion, terrorism, national security, drug use, and immigration. Libertarian millennials are more concerned about government domestic surveillance, the criminal justice system, and trade. Top priorities shared by both groups include the size and scope of government, free speech, government spending and debt, the economy, and taxes. Notably, libertarians and conservatives share their lowest priority: few are concerned about income inequality.   




**Voting in 2016**   
  
Although 88% of conservative millennial attendees identify as Republican (and 100% do if you include independent leaners), only 51% voted for Donald Trump in 2016. Nevertheless, this is considerably higher than the 20% of libertarian millennial attendees who voted for Trump. Among both sets of Trump voters, fully 7 in 10 said their vote was _against_ Hillary Clinton rather than a vote _for_ Trump. Thus, President Trump received few enthusiastic votes among this group of politically engaged millennial conservatives and libertarians.   
  
While a majority of conservative attendees ultimately voted for Trump, a majority (55%) of libertarian attendees voted for Libertarian candidate Gary Johnson instead. Few voted for Democratic candidate Hillary Clinton (3%). In fact, more said they did not vote (19%) than voted for Clinton.   




**Party Identification**   
  
Conservative millennials overwhelmingly (88%) identify as Republicans, while most libertarian attendees identify with the Libertarian Party (42%) or as politically independent (36%).   




However, after asking independents and libertarians if they lean toward a political party, 100% of the conservative millennial attendees leaned with the Republican Party. A majority (58%) of libertarian millennial attendees did as well, while a third said they are truly independent, and 6% identified as Democrats.   
  
**Evaluations of Trump and Key Political Figures**   
  
Although few of the libertarian and conservative millennial attendees were enthusiastic supporters of Trump during the election, 64% of conservative attendees approve of Trump’s job performance. In stark contrast, 80% of libertarian attendees disapprove. Nevertheless, conservative approval is “soft” with only 12% “strongly” approving. Libertarians are more ardently opposed, with 53% who “strongly disapprove” of President Trump.   




Major differences also emerge in evaluations of key political figures. Notably, while 8 in 10 conservative millennial attendees have a favorable opinion of Attorney General Jeff Sessions, 8 in 10 libertarian attendees have an unfavorable view of him. Libertarian aversion likely stems from disagreements about the criminal justice system, policing, and drug policy. Conservative attendees also have favorable views of Kellyanne Conway (62%), former campaign manager and now Counselor to President Trump, while libertarian attendees do not (22%). Conversely, libertarian millennial attendees have positive views of former Gov. Gary Johnson (61%), while conservatives do not (18%). Conservative attendees are also about twice as likely as libertarians to have positive views of Senators Ted Cruz (88% vs 50%) and Marco Rubio (91% vs 48%).   
  
Libertarian and conservative millennial attendees come together in their shared favorable views of Senator Rand Paul, Education Secretary Betsy Devos (likely due to their shared support of school choice), and writer George Will. They also share unfavorable views of Steve Bannon, Milo Yiannopoulos, and Ann Coulter, figures more closely associated with the “alt-right.”   




**Where Do They Get their News?**   
  
Conservative and libertarian millennial attendees share similar news consumption habits, sharing five of their top six news outlets: the Wall Street Journal (85%, 86%), the New York Times (66%, 75%), the Washington Post (68%, 72%), CNN (50%, 48%) and National Review (76%, 61%). However, conservatives are about 30 points more likely to regularly watch Fox (74% vs 45%) and 15 points more likely to read the Federalist (51% vs 36%). Conversely, libertarians are 53 points more likely to read _Reason_ (24% vs 77%).   




**Culture Wars and Transgender Issues**   
  
Conservatives and libertarian millennials are starkly at odds when it comes to what pronouns to use when referring to transgender people. When referring to a transgender person, 68% of libertarian millennial attendees choose to use the person’s preferred gender pronouns. In contrast, 67% of conservative millennial attendees say they use the pronouns corresponding with the transgender person’s biological sex.   




This difference extends to bathroom access as well. Eight in 10 conservative millennials say transgender people should be required to use the restroom corresponding with their biological sex. Conversely, 70% of libertarian millennials say transgender people should be allowed to use the restroom of the gender they identify with.   
  
Most conservatives (81%) disagree that we as a society need to do more to ensure LGBT people feel fully accepted. Meanwhile, libertarians are split, with 50% believing more should be done for LGBT people to feel accepted and 47% agreeing with conservatives that no more needs to be done.   




Despite these differences, conservatives and libertarians agree on religious liberty: 99% of conservative and 87% of libertarian respondents believe that businesses should be allowed to refuse service to same-sex weddings.   
  
**Immigration**   
  
Conservative and libertarian millennials diverge dramatically on questions of illegal _and legal_ immigration. While 79% of libertarians support increasing the number of immigrants allowed into the country each year, only 20% of conservatives agree. Instead, most conservatives would prefer to decrease the number (35%) or keep it the same (45%).   




When it comes to handling illegal immigration, a majority (52%) of conservative attendees support deporting illegal immigrants and 24% wish to bar them from citizenship. In contrast, 70% of libertarian attendees want to allow illegal immigrants to stay in the US and be able to eventually apply for citizenship.   




Finally, 69% of conservatives favor building a wall along the Mexican border, but 90% of libertarians oppose. However, conservatives are less intensely in favor of the wall than libertarians are opposed to it: only 14% of conservatives “strongly favor” while 67% of libertarians “strongly oppose” its construction.   




When it comes to passing a temporary ban on Muslims immigrating to the United States, conservative attendees are evenly divided, while 86% of libertarian attendees are opposed.   




**Opioid Epidemic**   
  
While most likely share concerns about overuse of prescription painkillers, conservative and libertarian attendees starkly disagree about what government should do about it. Eight in 10 conservatives agree “government needs to do more” to combat prescription painkiller addiction, but 8 in 10 libertarians disagree that government should take on this role.   




**Organ Donations**   
  
It is currently illegal to buy or sell human organs. However, 92% of libertarian attendees believe such a market should be legal; 8% agree with the status quo. Conservatives are ardently opposed with 72% who think such a market should remain illegal, while 28% would favor legalization.   
  
**Foreign Policy and National Security**   
  
Conservative and libertarian millennials sharply disagree on questions of foreign policy. Nearly 90% of conservative attendees support either increasing (38%) or maintaining (51%) our military presence around the world, while 84% of libertarians support decreasing this presence. Moreover, 92% of libertarian respondents support cutting defense spending to help balance the federal budget, while 72% of conservatives oppose such cuts.   




Conservative and libertarian attendees also make different trade-offs between national security and privacy. Seven in 10 conservatives say they’d be willing to give up some personal freedom and privacy for the sake of national security. In contrast, 9 in 10 libertarians say they would not be willing to give up more freedom and privacy for security.   
  
In line with such priorities, 60% of conservative attendees approve of government collection of domestic telephone and Internet data, while 92% of libertarians disapprove of this collection.   
  
**Health Care**   
  
On matters of health care, conservatives and libertarians are often aligned—particularly when it comes to repealing the Affordable Care Act/Obamacare (99%, 95%). However, similar to what’s playing out at the Congressional level, libertarians and conservatives disagree about how to improve the health care system in the country. Three-fourths (76%) of libertarians say the health care system “needs to be completely rebuilt.” However, conservatives are evenly divided with 49% agreeing that the system should be rebuilt, but 46% who think the system “needs major reform but doesn’t need to be completely rebuilt.” Less than 5% of either group think the current system works well and only needs minor changes.   




**Climate Change**   
  
Libertarian and conservative millennial attendees disagree about the causes of climate change but they agree on a solution. Conservatives are more likely to believe climate change is a natural phenomenon, with 54% believing increases in Earth’s temperature are either mostly or entirely due to natural causes. Meanwhile, 62% of libertarians think climate change comes partially, mostly, or entirely from human activity. Despite these different underlying beliefs, 99% of conservative and libertarian attendees agree that technological innovation in the free market will better solve climate change than government regulation.   




**Criminal Justice**   
  
Majorities of conservatives and libertarians reach consensus on several criminal justice issues. Both agree that police departments using military weapons and drones are not necessary for law enforcement purposes. But libertarians (92%) agree more than conservatives (61%). Both groups also favor eliminating mandatory minimum prison sentences for people convicted of selling drugs (64% conservative, 84% libertarian).   
  
However, conservative and libertarian attendees diverge on the death penalty: a majority (56%) of conservatives favor it while a majority (75%) of libertarians oppose it. Respondents also diverge in their perception of racial equality before the law, with 54% of conservatives saying that African Americans and other minorities “receive equal treatment with whites” in the criminal justice system and 73% of libertarians believing that minorities do not receive equal treatment.   




**Free Markets and the Welfare State**   
  
Despite the variety of policy differences between libertarian and conservative attendees outlined above, the two groups largely agree about economic issues, the benefits of free markets, and trade.   
  
For instance, 100% of both groups say they favor a smaller government providing fewer services and low taxes. Nearly 100% of both oppose raising taxes on wealthy households and also agree that regulation too often does more harm than good. Eight in 10 conservatives and nearly 100% of libertarians believe free trade must be allowed even if domestic industries are hurt by foreign competition.   
  
**Religion**   
  
Conservative and libertarian millennials have different ideas about the role of religion in society. An overwhelming majority (83%) of libertarian attendees say religious values should _not_ play a more important role in government. But, 62% of conservative attendees disagree and think such values should play a more important role. Conservatives also believe that it’s important for kids to be brought up with religious values. Libertarians are divided, but tend to disagree (56%).   




Much of this contrast may stem from differences in religious affiliation and habits. While 95% of conservative respondents have a religious preference, 38% of libertarians describe themselves as “non-religious.” Moreover, conservative millennial attendees are twice as likely as libertarians to attend church weekly or monthly (80% vs 41%). A majority (58%) of libertarian respondents either never or rarely attend a religious service.   




**Who Won the Intern Debate?**   
  
Who won the intern debate depends on whom you ask. Among conservative millennial attendees: 53% said the conservative team won and 44% said the libertarian team won. Among libertarian millennial attendees, 94% said the libertarians won while 5% said the conservatives won. Among the moderates, liberals and progressives in the audience, 83% felt the libertarian team won and 12% thought the conservatives won.   
  
**Implications**   
  
This survey provides a useful snapshot of young politically engaged conservatives and libertarians who are interested enough in politics and public policy to intern in Washington or attend an event for Washington interns. Thus, this data offers an idea of the direction young activists may take public policy as they age and the cleavages that may animate policy debates into the future.   
  
_Full LvCDebate Attendee Survey results foundhere_   
  
_David Kemp contributed to this report._


"
"
Recently we’ve been discussing products from the AIRS satellite instrument (Atmospheric InfraRed Sounder) onboard the Aqua satellite. There has been quite a bit of interest in this because unlike the satellite temperature record that goes back to 1979, until now we have not had a complementary satellite derived CO2 record. We are about to have one, and much more.

Click image to see a slide show with this graphic in it (PDF)
I wrote to the AIRS team to inquire about when the satellite data on CO2, and other relevant products might be made public. All that has been released so far are occasional snippets of data and imagery, such as the short slide show above.
Here is the response I got from them:
Thank you for your interest in the AIRS CO2 data product.
We are still in the validation phase in developing this new product.
It will be part of the Version 6 data release, but for now those of us
working on it are intensively validating our results using in situ
measurements by aircraft and upward looking fourier transform IR
spectrometers (TCCON network and others).
The AIRS CO2 product is for the mid-troposphere. For quite some time
it was accepted theory that CO2 in the free troposphere is
“well-mixed”, i.e., the difference that might be seen at that altitude
would be a fraction of a part per million (ppmv). Models, which
ingest surface fluxes from known sources, have long predicted a smooth
(small)variation with latitude, with steadily diminishing CO2 as you
move farther South. We have a “two-planet” planet – land in the
Northern Hemisphere and ocean in the Southern Hemisphere. Synoptic
weather in the NH can be seen to control the distribution of CO2 in
the free troposphere. The SH large-scale action is mostly zonal.
Since our results are at variance with what is commonly accepted by he
scientific community, we must work especially hard to validate them.
We have just had a paper accepted by Geophysical Research Letters that
will be published in 6-8 weeks, and are preparing a validation paper.
We have global CO2 retrievals (day and night, over ocean and land, for
clear and cloudy scenes) spanning the time period from Sept 2002 to
the present. Those data will be released as we satisfactorily
validate them.
I suggest you Google “Carbon Tracker” for some interesting maps
generated using model atmospheres and data for CO2 sources. It shows
the CO2 weather in the lowest part of the atmosphere.
The big picture is that CO2 sources and sinks are in the planetary
boundary layer. Global circulation of CO2 occurs in the free
troposphere. Thus, PBL is local whereas free troposphere is
international.
———-
AIRS Team
With the suggestion of using the Google “Carbon tracker”, some readers might look at this response as a “dodge”. I don’t see it that way at all. Why? Because they are actively engaged in proving the instrument by doing a series of aircraft based measurements to validate the data the instrument on the spacecraft is seeing.
For example, read this paper from them:
First Satellite Remote Sounding of the Global Mid-Tropospheric CO2
These graphics show how hard they are working to validate the data from in situ measurements using airborne flask samples sent to a lab spectrometer:

…and the results of the flask sample measurements:

Read more about this here in this paper (PDF)
Also if you read between the lines in their response to me, particularly this paragraph:
Since our results are at variance with what is commonly accepted by he
scientific community, we must work especially hard to validate them.
We have just had a paper accepted by Geophysical Research Letters that
will be published in 6-8 weeks, and are preparing a validation paper.
I’d say that waiting that 6-8 weeks for the paper and supporting data will be well worth it.  The working title of the upcoming paper is: “Satellite Remote Sounding of Mid-Tropospheric  CO2” and the lead author is Moustafa T. Chahine.
Good things come to those who wait.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d78f2bb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

State officials in Ohio have been complaining recently about potential revenue losses from the growth of untaxed Internet shopping. “We figure we’re losing over $200 million annually from direct marketing, catalog and Internet sales,” says Clare Long, Ohio’s deputy tax commissioner. Across the nation, departments like Long’s have been fighting for years to force out‐​of‐​state mail order companies to collect sales taxes. Now electronic commerce is in the cross hairs. What state officials propose is — you guessed it — more taxes. 



The officials are worried because an estimated 27 million U.S. households (a number that’s getting bigger every day) now use the Internet regularly. In the officials’ eyes, that’s too many consumers who might make purchases without the government’s getting a cut. Buyers are supposed to pay a “use tax” in lieu of a sales tax on all out‐​of‐​state purchases, but few volunteer. 



At first glance, the proposal sounds reasonable: why not tax identical items the same regardless of how they’re purchased? From the tax collector’s perspective, that makes sense. But in the real world, there are several reasons why allowing states to tax out‐​of‐​state electronic commerce is bad policy. 



First, there is no immediate danger of large revenue losses for traditional retailers, and by extension, for state tax authorities. Because they cater to a customer’s desire for a hands‐​on experience, local stores don’t charge for shipping and offer immediate gratification and so will probably always dominate retailing. What’s more, shopping is for many people a pleasurable social experience that cannot be duplicated online. Thus, Internet sales won’t destroy “real” retailers, just as catalog sales haven’t. 



National data support that conclusion. In an era of almost no inflation, state budgets grew by 5 percent in fiscal year 1997 and by more than 6 percent in fiscal year 1998. The last fiscal year ended with about $21 billion more in tax collections than originally anticipated. It appears that states will enjoy a sizable revenue windfall this year as well. If electronic commerce is undermining state revenues, it’s an undetectable trend. Electronic commerce certainly hasn’t slowed the flood of surplus money pouring into Columbus — expected to be around $400 million this year. 



Second, it’s not fair to force out‐​of‐​state firms to act as tax collectors when they don’t benefit from state services. When an Ohio business collects sales taxes, there is a clear linkage between the taxes paid, the services provided, and legislative representation. After all, local firms benefit from police and fire protection, roads and waste collection and other state services, so it is proper that they help cover those costs. And local firms can make their voice heard directly through lobbying and membership in groups like the Chamber of Commerce. 



Remote sellers, on the other hand, don’t enjoy any of those advantages. If the state wants more of the taxpayers’ money, it should collect it itself and not try to push the burden onto out‐​of‐​state businesses. 



Finally, differentiated tax rates create healthy competition that helps keep local rates under control. For example, some residents of Manhattan drive to Delaware to avoid sales taxes — an option that has undoubtedly curbed the profligate fiscal habits of New York politicians. Electronic commerce similarly guards against excessive taxation. When sales tax rates get too high, it’s important that Ohioans have a shopping alternative. 



The idea that government won’t find some way to keep the tax dollars flowing is laughable. So let’s be honest about what’s going on here: allowing states to tax out‐​of‐​state electronic commerce would be the equivalent of a tax increase. States would fatten already overflowing coffers without ever having to bring the issue to a vote at home. That’s a dream scenario for state legislators but a nightmare for taxpayers. 



If states are concerned about local retailers, they can effectively address the issue by moving tax rates downward. Minnesota policymakers have raised one such interesting possibility, proposing to eliminate the sales tax on certain products that are easily acquired online. Specifically targeted are intangible goods that can be downloaded, such as software, music and books. 



Sure, limiting states’ taxing authority can lead to unequal taxation. But such limitations are a crucial component of American federalism. Absent those restraints, confiscatory tax rates — which are the true injustice — would get worse. To improve its business climate Ohio should cut taxes, not scheme to collect more.
"
"

Either global warming is the greatest crisis ever to confront humankind, or it is a lefty plot completely manufactured by scientists and politicians in pursuit of research funding and control over our lives. That’s about the way it plays out in the media, on blogs and in conversations on the Metro. Anyone out front on this issue is either an apocalyptic or a denier, virtuous or vile.



Similarly, one camp maintains that temperatures are rising dramatically with unspeakable portents, while the other thinks what has happened is entirely a result of undefined internal oscillations in the earth‐​sun climate system, and that there is virtually no human component to climate change. This group is especially fond of the lack of statistically significant surface warming since 1995. Since 1997, temperatures really flatlined.



There’s a third way, which suffers from the problem that it is subtle, neither black nor white, and doesn’t do well in sound bites. It’s a “lukewarm” synthesis, arguing indeed that humans have something to do with the rise in surface temperature measured since the mid‐​1970s, but that it is hardly the end of the world as we know it. This view claims to accommodate the seemingly odd behavior of temperature in the last 15 years.



Each of these positions — let’s call them hothead, flatline, and lukewarm — are testable against observed history and theory. To keep some interest in this occasionally boring topic, I’m going to examine them sequentially, starting with the hotheads.



The hothead argument is that we have already set the planet on the road to climate calamity, and that we must promptly reduce the atmospheric concentration of dreaded carbon dioxide‐ the main global warming emission — to levels seen decades ago.



Before we started torching carbon stored in forests and then carbon stored underground as coal and oil, the carbon dioxide concentration of our atmosphere was about 280 parts per million (ppm). It’s now around 390, and headed for a nominal doubling to 600ppm between 2070 and 2090 if the world continues its current rate of development and does not find an effective (meaning neither solar nor wind) and politically acceptable (meaning not nuclear, at least for now) alternative for hydrocarbon fuels.



The high priest of the hotheads is NASA’s James Hansen, who preaches that, unless we dial back to 350ppm, we will lose, within a hundred years’ time, the vast majority of Greenland’s ice, which will raise sea levels about 20 feet. Hansen has testified that he thinks this could happen within a hundred years.



The hothead theory is that the ice on that gigantic island is much less stable than previously thought, and that with a tad more warming, lakes will form in the summer, drain thousands of feet down to the bedrock, and lubricate the flow to the ocean. It quickly melts, submerging a lot of Florida and Manhattan. The Washington Monument becomes an island.



The reason that glaciers flow to begin with is because the bottom is liquid. It’s quite unclear that simply adding more water will have much effect. Recent studies indicate that when the lakes drain suddenly into the ice, the acceleration of flow is not sustained. But that’s today; what about in the future?



One way to project the future with confidence is to look to history, when it was warmer. Danish colonists established a series of weather stations on the Greenland coast, with reliable records that go back over 225 years. They unequivocally show that — from 1920 through 1960 — there was substantially more warming than has been observed in recent decades. If hothead theory is correct, there should have been a detectable jump in sea level during that period, but there was none.



Further, there is very strong evidence that the integrated warming — that’s temperature times time — was much greater for _millennia_ after the end of the recent ice age around 10,800 years ago. Assuming that humans will find something better to power the world with than carbon dioxide‐​emitting fossil fuels in the next one or two hundred years, that total warming back then was greater or equal to what we are likely toinflict on Greenland.



In those millennia — which are only the blink of a geologist’s eye ago — trees used to grow where there is now only barren tundra. When they died, they were preserved in the acidic bogginess, so we can tell exactly when they were alive with carbon dating. It’s very clear that the forest in Eurasia used to extend all the way to the Arctic Ocean during that warm period.



Plant ecologists know that the northern limit of the forest is determined by the mean July temperature. The dead trees tell us it was as much as 13 degrees F warmer than the 20th century average.



The author of that work, Glen MacDonald of UCLA, has noted that the only way to get that region so warm is with a massive influx of Gulf Stream water from the Atlantic. The only “gate” for that is the channel between Greenland and Scandinavia, which means that Greenland (at least the eastern half) would have been pretty balmy compared to today.



And Greenland still retained the lion’s share of its ice cap.



Even so, the Arctic Ocean was likely to have been largely ice‐​free during the summer during much this time — from 6,000 to 8,000 years ago — as noted by theUniversity of Stockholm’s Martin Jacobsson in a 2010 edition of the scientific journal _Quaternary Science Reviews_. The Geological Survey of Norway foundsomething similarin 2008. Not only did Greenland’s ice survive — so did the polar bear.



So it appears that the ice that the hotheads skate on is pretty thin. Are the flatliners doing any better? We’ll have a look in Part 2.
"
"
Share this...FacebookTwitterH/T EIKE
The folly of windfarms and solar panels knows no boundaries. For example nowhere today in the north German area where I live is it possible to drive or take my bike 10 minutes anywhere without seeing a cluster of white behemoths chopping through the landscape (and birds).
Photo source: www.greenpeace.org. Here Greenpeace is major proponent of such let’s-target-and-desecrate-the-landscape projects.
Just days ago, Germany’s leading, renown political daily the Frankfurter Allgemeine Zeitung (FAZ) had a piece on the dark side of alternative energy sources, writing:
This program for ‘rescuing the climate’ is reckless, technocratic and ugly.

The FAZ adds,
Wind and solar parks will change the landscape with no consideration of protecting nature. Soon seeing natural landscape  will be possible only by looking in story books.
In fact, disfiguring the landscape is taking on such proportions and has gotten so out of control that some of the windmills’ former proponents, real environmentalists, are now beginning to sober up and are painfully realising the disaster they’ve helped to create.
The FAZ reports that even the President of the Federal Environmental Department, Jochen Flasbarth, is having second thoughts. He warns that an attempt to supply Germany with 100% renewable energy by 2050 will lead to a:
…huge aggravation of its citzens. The unavoidable consequence of a decentralized supply of renewable energy is that it will lead to an alteration of the landscape in many regions.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Face it, windparks need lots of space and are eyesores. It takes 1000 large 5-MW turbines to replace a single 1000 megawatt coal fired power plant. And because the turbines rarely work at 100% capacity (20% is typical) you need 5 times as many! And when the wind stops blowing, you need a conventional power plant to kick in anyway (200% capacity to ensure 100% supply).
And because land is not in plentiful supply, many projects are slated for offshore. The FAZ writes:
Most people don’t have any idea as to how big the dimensions of these windparks are, and even less an ideas of the ecological damage they cause to marine wildlife during their set-up and threat they pose to shipping vessels later. Germany’s Green party and environmental groups are silent on this topic.
The sprouting of offshore windmills is becoming such a threat that even the WWF warns of the Wild Wild West of the Baltic Sea. Sweden’s The Local writes on wind energy development until 2030:
Wind energy resources are forecast to undergo significant expansion over the period, from a current 400 megawatts to 25,000 megawatts which correspond to a 60-fold increase by 2030. Much of the expansion in wind energy will occur at sea.
Ironically environmentalists always protest loudly when it comes to building a new street, fearing for the life of field mice and worms. Yet they don’t seem to care about the destruction of natural scenery that hundreds of windparks cause and the thousands of migratory birds they kill as they navigate through their beloved machanized killing fields.
Well, how can that be? It gets down to green money. According to the FAZ:
Greenpeace earns money in replacing highly efficient, space-saving coal and nuclear power with extremely expensive, vast space-consuming, ugly and dangerous-to-wildlife, mechanized eyesores that happened to have the ecological seal of approval.
I ask myself: How much longer will it be before the real environmentalists and citizens wake up to this utter folly?
Share this...FacebookTwitter "
"

As we celebrate our 40th year, I’d like to recap some of Cato’s accomplishments and tell you where we’re heading.



Cato has been a vigorous proponent of entitlement restraints, the right to bear arms, marriage equality, fundamental tax reform, downsizing government, property rights, drug legalization, school choice, free trade, immigration liberalization, criminal justice reform, and term limits. We have energetically resisted corporate welfare, campaign finance restrictions, government constraints on the Internet, global warming excesses, overarching executive power, infringements on civil liberties, the administrative state, imperial wars, dubious foreign entanglements, and unnecessary government secrecy.



Cato was the first to address Social Security’s financial problems and offer a private‐​investment alternative. José Piñera, co‐​chairman of Cato’s Project on Social Security Choice, was the architect of privatization in Chile. We’re still fighting for entitlement reform here, where the fiscal implications for Social Security, Medicare, and Medicaid are dismal.



On the health care front, Cato’s efforts yielded Health Savings Accounts — a significant step toward free market health care. And we led the battle against Obamacare. The 2015 Supreme Court challenge was crafted by Cato’s director of health policy studies, Michael Cannon, who demonstrated Obamacare’s flawed structure and legal infirmities.



Our Center for Constitutional Studies, under Roger Pilon’s innovative leadership, has been a forceful advocate for a textual interpretation of the Constitution and a principled judicial engagement to bind the legislative and executive branches with the chains of the Constitution. Pilon and Ilya Shapiro, editor of the peerless _Cato Supreme Court Review_ , compiled an amazing record of amicus briefs, with the Court favoring the party we supported in an overwhelming percentage of cases.



In 2014, we expanded our Center for the Study of Science, which challenges the pseudoscientific claims of climate‐​change alarmists. That same year, Cato’s Center for Monetary and Financial Alternatives got under way — aggressively responding to the threat of an undisciplined central bank and fiat money.



We recognize, of course, that reaching a young audience is essential. Our Libertarian Leadership Project will enable us to dramatically expand our online contact with young, tech‐​savvy friends of liberty — complementing our intern program and Cato University. We’ve also fostered high‐​quality education through the work of the late Andrew Coulson, who directed Cato’s Center for Educational Freedom. Coulson produced a three‐​part documentary that embraces freemarket reforms to make schools more innovative and responsive. _School Inc. — A Personal Journey with Andrew Coulson_ is now available on PBS stations across the country.



Persuasion is key to our mission; and David Boaz’s latest book, _The Libertarian Mind_ , is the perfect messenger — a scholarly but readable work that popularizes and legitimizes libertarianism. Indeed, Cato books are in every major college library and have sold tens of thousands of copies. Cato scholars also deliver hundreds of college lectures annually, presenting the libertarian alternative to the next generation.



In the economic arena, our experts analyze tax reform and budget proposals — unraveling them so they’re digestible. Cato’s “Fiscal Report Card on the Nation’s Governors” is widely quoted, and our Herbert A. Stiefel Center for Trade Policy Studies shaped the debates over Trade Promotion Authority and the Trans‐​Pacific Partnership.



Cato has also been a prominent defender of civil liberties — especially those related to government surveillance and privacy. Meanwhile, our Project on Criminal Justice has steered public opinion against police militarization and the drug war.



In addition, we’re emphasizing the moral and philosophical arguments for liberty. Online courses are available from our Lib​er​tar​i​an​ism​.org website. They’re aimed at young people but accessible to a large and growing audience, as is our CatoAudio app for iOS devices, which contains our daily podcast, archived policy forums, _Classics of Liberty_ , the monthly CatoAudio magazine, and lots more.



A big focus of our 40th year has been finding ways to keep getting better and even more impactful. There is scope to be more connected and effective on Capitol Hill and in states, and in the academy, too, by hosting more visiting scholars. We also have great opportunities to expand the audience of our existing scholarship and research through the expanded use of technology and a broader array of content. We plan to strengthen our efforts to nurture young talent and the development of more organizations and initiatives across the liberty movement.



In short, Cato is an independent, nonpartisan source of intellectual ammunition to the public, government, educators, and the media. Ideas do matter. That’s the reason Cato is indispensable. As our 40th year draws to a close, we reaffirm our enduring commitment to the cause of human freedom.
"
"

The Dayton Peace Agreement, signed in Paris on Dec. 14, 1995, formally ended the civil war in Bosnia. Three years later, Dayton’s goal of creating a unified, multiethnic Bosnian state remains as elusive as ever. But that should have been expected. According to University of Chicago political scientist John Mearsheimer, “History records no instance where ethnic groups have agreed to share power in a democracy after a large‐​scale civil war. The democratic power‐​sharing that Dayton envisions has no precedent.” 



That’s not to say that Dayton hasn’t led to any success. The fighting has stopped, and so far more than 3,600 pieces of heavy weaponry have been removed. But those few successes reveal Dayton for what it really is: a complicated cease‐​fire, not a solution to Bosnia’s long‐​term problems. The country still is deeply fractured, divided into two semiautonomous “entities” separated by a monstrosity called the Inter‐​Entity Boundary Line. One entity, the Serb Republic, now almost is entirely Serb. The other, the Muslim‐​Croat Federation, is made up of rival enclaves that maintain a tense coexistence. Nearly 90 percent of the Serbs who lived in the Muslim‐​Croat Federation before 1992 were expelled or have left. 



The prospect for ethnic reintegration is not promising. By the end of 1997, only 19 percent of Bosnia’s 2.3 million refugees and displaced persons had returned home. Moreover, the total number of returnees in 1998 is expected to be only 11 percent of that of 1997. Even more telling, during the last three years only 55,000 Bosnians have returned to areas where they would be in the minority. At the same time, 80,000 Bosnians have moved from areas where they were in the minority to areas where they would be in the majority. That means there are 25,000 fewer Bosnians living in integrated communities today than when Dayton was signed three years ago. Moreover, 85 percent of Bosnians recently polled still say they will not vote for a candidate from another ethnic group. 



Nevertheless, the Clinton administration insists that the Dayton agreement will not be adapted to the reality that has existed in Bosnia for three years — de facto ethnic separation. That unwillingness to rethink Dayton is ill‐​conceived. 



But Bosnia’s costs are higher than many realize. It and other noncombat operations around the world have been diminishing U.S. national security by creating an operations tempo that undercuts U.S. military readiness. 



First, it ensures that billions of taxpayer dollars will be spent in vain trying to superimpose an imaginary Bosnia (united) over the real Bosnia (divided). The United States already is paying about half of the costs of the Bosnia peacekeeping operation, which includes 6,900 U.S. combat troops in Bosnia, plus 3,100 support personnel in Croatia, Hungary and Italy. By the end of fiscal year 1999, Washington will have spent $10.64 billion on the mission. 



But Bosnia’s costs are higher than many realize. It and other noncombat operations around the world have been diminishing U.S. national security by creating an operations tempo that undercuts U.S. military readiness. In fact, during the last decade, the U.S. Army has been used in 29 significant overseas operations, compared with 10 during the preceding 40 years. The strain of that pace has shown up in negative trend lines across all military services in a number of readiness categories. 



For example, to relieve the European‐​based units that have carried out most of the Bosnia mission so far, peacekeeping duties have been shifted to Ft. Hood’s First Cavalry Division, one of the premier U.S.-based combat divisions. As one staff member of the House National Security Committee observed: “The Army is disassembling one of its most ready, most fearsome war‐​fighting divisions. The action shows how the requirements of Bosnia are detracting from the military’s ability to do high‐​intensity conflicts.” 



Bosnia and other overseas operations also have caused the U.S. Air Force’s readiness to slip. The units that fly over Bosnia and the Persian Gulf have priority for plane rotation, support equipment and pilots. As a result, fighter squadrons based in the United States are at their lowest readiness level in years. In 1992, 86 percent of U.S.-based fighter jets were designated “mission capable.” In 1998 only 75 percent were. 



Even more worrisome, there is mounting evidence that peacekeeping and other noncombat operations have adversely affected retention of soldiers, sailors and pilots. The Pentagon reports that first‐​term soldiers assigned to peacekeeping in Bosnia generally reenlisted at the same rate as their counterparts stationed elsewhere in Europe last year. But soldiers stationed in Bosnia were offered a tax‐​exempt reenlistment bonus, which artificially inflated their retention rate. The gap in retention rates for midcareer soldiers stationed in Bosnia was more noticeable. They reenlisted at a rate 6.1 percent lower than that of their counterparts stationed elsewhere in Europe. 



Or take the Air Force. Since 1996, it has performed hundreds of peacekeeping missions in 11 countries, with the Bosnia operation being one of the largest. These mundane and repetitive missions have affected pilot morale negatively because there is no compelling national interest to keep them motivated. 



“We’re not really fighting the country’s wars; we’re just acting like the world’s policeman,” explains one pilot who is a veteran of both Bosnia and Saudi Arabia. This year, nearly 45 percent of eligible Air Force pilots did not renew their service contracts, up dramatically from 14 percent in 1994. Such an anemic retention rate cannot long be sustained without compromising U.S. military readiness. Last year the Air Force had 45 fewer pilots than needed. This year the number has grown to 700, and it’s expected to reach 2,000 by 2002. 



There also is increasing evidence that peacekeeping operations — as distinguished from actual national defense — deter prospective recruits from joining the military. And a strong economy, with plenty of private‐​sector jobs, has made it even tougher for the military to find recruits to replenish its shrinking ranks. 



For fiscal year 1998, both the Navy and the Air Force failed to meet their recruiting goals. The Army was more successful, but only because its recruiting target was lowered significantly. The Navy fell short of its annual recruitment target by 13 percent, and it recently was reported that the Navy has 18,022 fewer sailors at sea than it needs. 



The recruitment problem likely will worsen with the current demographic downturn in the prime recruiting pool: males between the ages of 18 and 21 who are physically fit high‐​school graduates and who scored in the upper half of the military’s standardized entry examination. That population currently consists of 15 percent fewer people than it did in the mid‐​1980s. 



Washington’s unwillingness to revise Dayton also may be paralyzing Bosnian reconstruction. In election after election Bosnian voters cast ballots for nationalist candidates to counterbalance the perceived political power of their ethnic rivals who, in turn, vote for nationalist candidates for the same reason. Such circular logic is built into Dayton because the agreement requires three ethnic groups, each of which fears the political ambitions of the others, to operate under the fiction of a unified state. The political obstructionism and stalemates brought on by upholding that fiction have crippled Bosnia’s efforts to emerge from a communist economy. In fact, three years and $4.35 billion in reconstruction aid later, Bosnia has yet to privatize any part of its economy. 



Ironically, because so much property in Bosnia still is government owned, NATO troops may be paying as much as $40 million a year to rent deployment and storage space from government‐​owned companies in Bosnia. That money is then pocketed by the nationalist party that happens to exercise control over the local or regional government and its institutions. What is puzzling about these payments is the obvious contradiction. NATO allies effectively are subsidizing the very nationalist political parties that Western officials consider the principal obstacles to peace in Bosnia. As the top Western diplomat in charge of implementing the Dayton agreement, Carlos Westendorp has asked, “How can they pay money to these people when we are supposed to be here promoting democracy?” 



A more prudent and viable U.S. policy now would be to convene a “Dayton II” conference that recognizes the reality that has existed on the ground since 1995: a three‐​way partition. That would allow Bosnian Croats, Muslims and Serbs to escape the current atmosphere of perpetual political confrontation and nationalist rancor and concentrate on rebuilding normal lives. The conference could be organized by the European Union, the Organization for Security and Cooperation in Europe, or the Dayton agreement’s Peace Implementation Council to work out the details of formalizing Bosnia’s divisions and to update Dayton’s arms‐​control, demilitarization and human‐​rights provisions accordingly. 



On the military side, arrangements could be made to replace NATO’s current 32,000-strong Stabilization Force with a European Force, or EFOR, to oversee the transition. The EFOR operation could be conducted with Western European Union troops with, perhaps, a prominent eventual role for the Southeast European Brigade, a new regional security initiative being developed by seven NATO and non‐​NATO countries in or near the Balkans. With a few exceptions — such as providing logistics support, cargo airlift and sealift, and space‐​based communications and intelligence — U.S. forces could be extracted from Bosnia before the Dayton agreement’s fourth anniversary in December 1999. 



No doubt critics will point out that this would allow separatism to prevail over multicultural cosmopolitanism. Ideally, the people of Bosnia should enjoy equal rights regardless of religion or ethnic background, and it is tragic that they have refused to uphold that principle. But a multiethnic Bosnia prospering in a climate of liberal toleration is not a realistic expectation; there is simply too much enmity and suspicion on all sides. Sometimes even an ugly divorce is preferable to preserving a futile and destructive marriage — especially when the union is forced. Most important, a negotiated partition is the last, best chance to create a relatively stable environment that will allow for the timely departure of U.S. forces from Bosnia.
"
"

And yet to play out, let’s also not forget Al Gore’s 2008 prediction: “Entire north polar ice cap will be gone in 5 years”
-Anthony
By Dennis  Avery  in the Canada Free Press
“2008 will be the hottest year in a  century:” The Old Farmers’ Almanac, September 11, 2008, Hurricanes, Arctic  Ice, Coral, Drinking water, Aspen skiing
We’re now well into the earth’s third straight harsher  winter-but in late 2007 it was still hard to forget 22 straight years of global  warming from 1976-1998. So the Old Farmer’s Almanac predicted 2008 would be the  hottest year in the last 100.
But  sunspots had been predicting major cooling since 2000, and global temperatures  turned downward in early 2007. The sunspots have had a 79 percent correlation  with the earth’s thermometers since 1860. Today’s temperatures are about on a  par with 1940. For 2008, the Almanac hired a new climatologist, Joe D’Aleo, who  says the declining sunspots and the cool phase of the Pacific Ocean predict  25-30 years of cooler temperatures for the planet.
“You could potentially sail, kayak or even  swim to the North Pole by the end of the summer. Climate scientists say that the  Arctic ice . . . is currently on track to melt sometime in 2008.” Ted  Alvarez, Backpacker Magazine Blogs, June, 2008.
Soon after this prediction, a huge Russian icebreaker got  trapped in the thick ice of the Northwest Passage for a full week. The Arctic  ice hadn’t melted in 2007, it got blown
into warmer southern  waters. Now it’s back. (Reference)
Remember too the Arctic has its own 70-year climate cycle.  Polish climatologist Rajmund Przbylak says “the highest temperatures since the  beginning of instrumental observation occurred clearly in the 1930s” based on  more than 40 Arctic temperature stations.
(This uneducated prediction may have been the catalyst for Lewis Pugh and his absurd kayak stunt that failed miserably – Anthony)
“Australia’s Cities Will Run Out of Drinking  Water Due to Global Warming.”
Tim  Flannery was named Australia’s Man of the Year in 2007-for predicting that  Australian cities will run out of water. He predicted Perth would become the  “first 21st century ghost city,’ and that Sydney would be out of water by 2007.  Today however, Australia’s city reservoirs are amply filled. Andrew Bolt of the  Melbourne Herald-Sun reminds us Australia is truly a land of long droughts and  flooding rains.
“Hurricane Effects Will Only Get Worse.” Live Science,  September 19, 2008.
So wrote the on-line  tech website Live Science, but the number of Atlantic hurricanes 2006-2008 has  been 22 percent below average, with insured losses more than 50 percent below  average. The British Navy recorded more than twice as many major land-falling  Caribbean hurricanes in the last part of the Little Ice Age (1700-1850) as  during the much-warmer last half of the 20th century.
“Corals will become increasingly rare on reef  systems.” Dr. Hans Hoegh-Guldberg, head of Queensland University (Australia)  marine studies.
In 2006, Dr.  Hoegh-Guldberg warned that high temperatures might kill 30-40 percent of the  coral on the Great Barrier Reef “within a month.” In 2007, he said global  warming temperatures were bleaching [potentially killing] the reef.
But, in 2008, the Global Coral Reef  Monitoring Network said climate change had not damaged the “well-managed” reef  in the four years since its last report. Veteran diver Ben Cropp said that in 50  years he’d seen no heat damage to the reef at all. “The only change I’ve seen  has been the result of over-fishing, pollution, too many tourists or people  dropping anchors on the reef,” he said.
No More Skiing? “Climate Change and  Aspen,” Aspen, CO city-funded study, June, 2007.
Aspen’s study predicted global warming would change the climate  to resemble hot, dry Amarillo, Texas. But in 2008, European ski resorts opened a  month early, after Switzerland recorded more October snow than ever before.  Would-be skiers in Aspen had lots of winter snow-but a chill factor of 18 below  zero F. kept them at their fireplaces instead of on the slopes.
*Sources:
Predictions  of 25-30 year cooling due to Pacific Decadal Oscillation:  Scafetta and West,  2006, “Phenomenological Solar Signature in 400 Years of Reconstructed Northern  Hemisphere Temperature Record,” Geophysical Research Letters.
Arctic Warmer in the 1930s:  R. Przybylak,  2000, “Temporal and Spatial Variation of Surface Air Temperature over the Period  of Instrumental Observation in the Arctic,” International Journal of Climatology  20.
British Navy records of Caribbean  hurricanes 1700-1850:  J.B. Elsner et al., 2000, “Spatial Variations in Major  U.S. Hurricane Activity,” Journal of Climate 13.
Predictions of coral loss:  Hoegh-Guldberg et al., Science, Vol.  318, 2007. Status of Coral Reefs of the World 2008, issued by the Global Coral  Reef Monitoring Network, Nov., 2008.
Aspen climate change study:  Climate Change and Aspen: An  Assessment of Potential Impacts and Responses, Aspen Global Change Institute,  June, 2007.
(1) Reader  Feedback | Click  here to get Canada Free Press in your email
Dennis T. Avery, is a senior fellow with  the Hudson Institute in Washington.  Dennis is the Director for Global Food  Issues ([url=http://www.cgfi.org]http://www.cgfi.org[/url]).  He was formerly a senior analyst for the Department of State. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a091411',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest post by Steve Goddard
There is a considerable amount of misinformation propagated about the greenhouse effect by people from both sides of the debate.  The basic concepts are straightforward, as explained here.
The greenhouse effect is real.  If there were no greenhouse gases in the atmosphere, earth would be a cold place.   Compare Mars versus Venus – Mars has minimal greenhouse gas molecules in its’ atmosphere due to low atmospheric pressure, and is cold.  By contrast, Venus has a lot of greenhouse gas molecules in its’ atmosphere, and is very hot.  Temperature increases as greenhouse gas concentration increases.  These are undisputed facts.
Heat is not “trapped” by greenhouse gases.  The earth’s heat balance is maintained, as required by the laws of thermodynamics.

outgoing radiation = incoming radiation – changes in oceanic heat content
The image below from AER Research explains the radiative balance.

http://www.aer.com/scienceResearch/rc/rc.html
About 30% of the incoming shortwave radiation (SW) is reflected by clouds and from the earth’s surface.  20% is absorbed by clouds and re-emitted back into space as longwave (LW) radiation.  The other 50% reaches the earth’s surface and warms us.  All of that 50% eventually makes it back out into space as LW radiation, through intermediate processes of convection, conduction or radiation.  As greenhouse gas concentration increases, the total number of collisions with GHG molecules increases.  This makes it more difficult for LW radiation to escape.  In order to maintain equilibrium, the temperature has to increase.  Higher temperatures mean higher energies, which in turn increase the frequency of emission events.  Thus the incoming/outgoing balance is maintained.
It has been known for a long time that even a short column of air contains enough CO2 to saturate LW absorption.  This has been misinterpreted by some skeptics to mean that adding more CO2 will not increase the temperature.  That is simply not true, as higher GHG densities force the temperature up.  There is no dispute about this in the scientific community. See the graph below:

Click for larger image
As Dr. Hansen has correctly argued, increases in atmospheric temperature cause the ocean to warm up.  Thus changes the oceanic heat content become the short term imbalance in the incoming/outgoing equilibrium equation, which is not shown in the AER diagram.
The image below shows GHG absorption by altitude and wavenumber.  As you can see, there is a strong absorption band of CO2 at 600/cm.  That is what makes CO2 an important greenhouse gas.

http://www.aer.com/scienceResearch/rc/m-proj/lbl_clrt_mls.html

The important greenhouse gases are: H2O, CO2, O3, N2O, CO and CH4.  The reason why the desert can get very cold at night is because of a lack of water vapor.  The same is true for Antarctica.  The extreme cold in Antarctica is due to high albedo and a lack of water vapor and clouds in the atmosphere, which results in almost all of the incoming radiation returning immediately to space.
An earth with no CO2 would be very cold.  The first few tens of PPM produce a strong warming effect, and increases after that are incremental.  It is widely agreed that a doubling of CO2 will increase atmospheric temperatures by about 1.2C, before feedbacks.  So the debate is not about the greenhouse effect, it is about the feedbacks.
Suppose that the amount of reflected SW from clouds increases from 20% to 21%?  That would cause a significant cooling effect.  Thus the ability of GCM models to model future temperatures is largely dependent on the ability to model future clouds.  Cloud modeling is acknowledged to be currently one of the weakest links in the GCMs.  Given the sensitivity to clouds, it is perhaps surprising that some high profile climate scientists are willing to claim that 6C+ temperature rises are established science.
So the bottom line is that the greenhouse effect is real.  Increasing CO2 will increase temperatures.  If you want to make a knowledgeable argument, learn about the feedbacks.  That is where the disagreement lies.
“Lisa, in this  house we obey the laws of thermodynamics“
– Homer Simpson


Addenddum:

The GHG/stoplight analogy

Suppose that you have to drop your child at school at 8:00 and have to be  at work at 8:30.  There are 10 stoplights between the school and the office.   Your electric car has a fixed maximum speed of 30MPH.  It takes exactly 30  minutes to drive there.

If the city adds another stoplight (analogous to more CO2) the only way you  can make it to work on time is to run traffic lights and/or get the city to make  the traffic lights more efficient at moving cars (analogous to higher  temperature.)  The radiative balance has to be maintained in the atmosphere, so  the outgoing radiation has a fixed amount of time to escape, regardless of how  many GHG molecules it encounters.   Otherwise, Homer and your boss will be very  angry at you for violating the laws of thermodynamics.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9826bf2a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe chances are real, and both are threatening the planet Earth. Apophis and Yellowstone have been appearing in the media lately. The chances of a catastrophic event occurring in your lifetime are higher than you may think.
Illustration of an asteroid impacting the earth. (Source Wikipedia)
Apophis
Russian astronomers are predicting that the asteroid Apophis could collide with the planet earth on April 13, 2036, writes the online Voice of Russia.
Apophis’s length was earlier estimated to be 450 metres, but a better estimate based on spectroscopic observations at NASA’s Infrared Telescope Facility in Hawaii puts it at 350 metres. That’s still a big rock to be hit by.
‘Apophis will approach Earth at a distance of 37,000 – 38,000 kilometers on April 13, 2029. Its likely collision with Earth may occur on April 13, 2036,’ Professor Leonid Sokolov of the St. Petersburg State University said.”
According to Wikipedia, NASA has estimated the energy that Apophis would release if it struck Earth as the equivalent of 510 megatons on TNT. By comparison the impacts of the Tunguska event is estimated to be in the 3–10 megaton range. The 1883 eruption of Krakatoa was the equivalent of roughly 200 megatons, and the Chicxulub impact, believed by many to be a significant factor in the extinction of the dinosaurs, has been estimated to have released about as much energy as 100 million megatons. 
The bad news is that an impact by Apophis would destroy an area of thousands of square kilometres, and seriously disrupt the climate for a few years. The good news is that it would be unlikely to have long-lasting global effects. Also the chances of Apophis actually striking the earth are still remote.
Yellowstone
The other potential natural catastrophe is the Yellowstone super-volcano, reports National Geographic here. Yellowstone’s caldera covers a 40 by 60 kilometer swath of Wyoming, is an ancient crater formed after the last big blast, some 640,000 years ago. The magnitude of an eruption estimated by scientists would be 1000 times more powerful than the 1980 Mount St. Helens eruption of 1980, and would lead to dire consequences for the globe.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




See: “When Yellowstone Explodes“ in National Geographic magazine. Here’s how the last one looked:
Scientists calculate that the pillar of ash from the Yellowstone explosion rose some 100,000 feet, leaving a layer of debris across the West all the way to the Gulf of Mexico. Pyroclastic flows—dense, lethal fogs of ash, rocks, and gas, superheated to 1,470 degrees Fahrenheit—rolled across the landscape in towering gray clouds. The clouds filled entire valleys with hundreds of feet of material so hot and heavy that it welded itself like asphalt across the once verdant landscape.”
The Yellowstone crater today is rising at a record speed, forced up by a huge magma reservoir that is thought to be about 10 km below the surface, see below. It has risen 25 centimeters since 2004. NatGeo writes that roughly 3,000 earthquakes occur in Yellowstone each year.
But between December 26, 2008, and January 8, 2009, there were about 900 earthquakes, and the rate of rise then slowed for a time. Scientists believe the earthquakes may help to release pressure on the magma reservoir below the surface by allowing fluids to escape, and thus relieve some pressure.
Yellowstone super volcano. (Source: Wikipedia)
Yellowstone erupted 3 times in the last 2 .1 million years. The last eruption was about 640,000 years ago. German online FOCUS magazine writes:
 The Yellowstone volcano is considered to be dangerous because on a geological timescale, it is due to erupt.”
Worrisome odds
Yellowstone is not the only super-volcano threatening the planet. A FOCUS map shows 6 others. Although the chances are small that any one in particular will erupt soon – maybe 1 in a 1000, the odds increase to worrisome levels when all the catastrophe possibilities get factored in. If one identifies 10 potential catastrophic events, each with the odds of occurrence being 1 in 1000, then it means the odds of one happening reduce to 1 in a 100. That starts to get worrisome. It means there’s a pretty good chance one catastrophe will occur in the next 100 years.
People who were born just recently have a pretty good chance of witnessing such an event in their lifetime. And the longer the planet goes without a catastrophe occurring, the greater the chances become.
Share this...FacebookTwitter "
"

I am rather concerned about my elderly father‐​in‐​law, who lives in northern Virginia. I just visited him, as Washington’s temperatures bubbled into the high 90s. On his television, the summer’s first heat‐​related fatalities were being reported. I noticed that his house seemed unusually warm, and I went over to the window to turn on his air conditioner. “Don’t bother,” he told me, “it’s not working.” Without air conditioning, my father‐​in‐​law and millions of elderly citizens just like him are at grave risk in this weather. After I write this column, I’m calling Sears and having a new machine delivered pronto. 



“This weather” has been in the news for over a week now. The last week in July is, statistically speaking, normally the hottest all year in the eastern United States. Although the news may be all atwitter about the temperatures, they’re actually pretty much what you’d expect on sunny days at the end of July. 



The average July high temperature along most of the East Coast from New York on down is around 90 degrees, with the southern portion a bit above and the northern a bit below. Sometimes a sea breeze gets into the Big Apple, but Philly, Baltimore, Washington, and the like are far enough inland that they simply bake. 



Given that the last week of July is the warmest in the month, temperatures in the lower 90s should be the rule, not the exception. But these “normal” values are composed of 30‐​year averages. Some days that form that average were sunny, some were cloudy, some had rain and some were in‐​between. It stands to reason that a bright sunny day is going to be warmer than the average–so that 95 degrees is pretty “normal” as long as it doesn’t cloud up. 



This is the threshold temperature at which elderly deaths begin to take off. And, true to form, we’ve seen the usual spate of stories trying to conflate this mortality, this summer’s temperatures and global warming caused by pernicious economic activity. 



Let’s get one thing straight. There is no warming trend in U.S. summer temperatures over the last 80 years. It did warm a bit from 1900 to 1930, but that change surely wasn’t because of a greenhouse effect; we hadn’t put much new carbon dioxide in the air by then. Further, current planetary temperatures measured by satellites and weather balloons are considerably below their average for the last two decades. 



In addition, heat‐​related mortality is going down. In 1995, Chicago saw several hundred deaths in a July heat wave. But there were 885 heat‐​related deaths in the Second City in 1955. Want to see true carnage? Go back to 1900, when 10,000 Americans perished in the heat. (The globe was one degree cooler then!) 



What’s the difference here? Two words: air conditioning. 



Air conditioners use more electricity than any other home appliance. On a hot day, they create such demand for electricity that, sometimes, the power fails. After this, the county coroner isn’t far around the corner. In fact, it was a power failure that magnified the 1995 Chicago tragedy. Normally in a heat wave, the poorer South Side experiences more deaths than the North Side. But a power outage in the affluent side of town resulted in a pretty equal distribution of fatalities across income classes. 



In this summer’s heat, Mayor Richard Daley has been exhorting citizens who feel they cannot afford to run their air conditioners to take advantage of a federal program designed to subsidize payments in just that eventuality. Somehow I do not believe that every 80 year old has gotten this message and fear that some will die today. 



Which brings us back to global warming. It should be self‐​evident that the very technology that enhances the greenhouse effect–the production of electricity–is what saves our lives in the heat of a normal summer. Thousands more would die, as did in 1900, without air conditioning in a world where the enhanced greenhouse effect and dreaded global warming did not exist. 



The risk of power failure can be averted by installing new generation capacity. But every time a new power plant is proposed, someone squawks “global warming.” When lack of power causes an outage on a hot day, that well‐​intended protest becomes a lethal weapon. 



Therefore, it is somewhat ironic that all proposals to fight global warming drastically raise the price of energy and power. The Kyoto Protocol on climate change requires us to reduce our emissions of greenhouse gases (read: use of energy) by 30 to 45 percent by 2008 compared with where we would be if we just went on as we are. If the price of electricity more than doubles (a likely scenario according to most experts), how many more of our elderly will hesitate to turn on the air conditioner until it is too late? The Kyoto Protocol is a killer.
"
"

Even yellow journalists know it’s a good idea to use the refereed scientific literature as the basis for science stories, so it was disconcerting to see a bona fide green journalist like the Washington Post’s Joby Warrick give a great deal of ink to a nonrefereed speech — not even a paper — delivered in San Francisco by federal climatologist Jonathan Overpeck. 



The reason for all the fuss soon became obvious. At the December meeting of the American Geophysical Union, Overpeck said that the so‐​called Medieval Warm Period was local, not global. In other words, the warming that was so substantial that it allowed the Vikings to colonize Greenland and North America was not created by a general planetary warming. That implies that the cooling that followed — known as the Little Ice Age — was similarly non‐​global; otherwise the Warm Period would have shown up, in comparison with temperatures in succeeding centuries as, well, global warming. 



Overpeck’s speech prompted handsprings of joy from our greener friends. Now, instead of saying that the decade of the 1990s (and, in particular, 1998) is the warmest in 600 years (which goes back to the beginning of the putative Warm Period), they can say it’s the warmest in 1,200 years. This story will be in print on or about January 4, 1999, and allows them to declare that the warm terror is here and higher taxes are needed pronto to stop the burning of fossil fuels. 



Others might say, “big deal, sure am glad that I haven’t spent a lick on heating oil and it’s almost Christmas. Think I’ll go and buy some stuff for the missus.” 



Like Tip O’Neill’s politics, climate is local. 



Still others may correctly deduce that Overpeck has created a big problem for those who warn of impending apocalypse. If he is right (a large IF), then regional climate naturally varies tremendously, whether or not the globe warms. In other words, climate changes so dramatic that they promoted the Viking exploration are simply the way of things. And ditto for their flipside — large regional coolings like the Little Ice Age. That event sent the Rhone Glacier in the Alps some 5,000 feet further downslope than it is today and prompted winter carnivals on the frozen Thames. 



Poignant testimony to the social consequences of this regional swing can be found in Kalaallit Nunaat (the politically correct term for Greenland these days), where masonry churches, once built in pastures, are now encased in ice. While KN’s climate clearly changed in ways that were tremendously important to society at the time of the Vikings, that apparently had nothing to do with global warming or cooling. 



Instead, Overpeck says, those changes occurred as purely internal oscillations of the climate system, with no external global change. If we accept that notion, what does it really mean? 



It means that large, regional climate changes have occurred and will occur whether or not the planet warms. That is the kind of change people and plants care about, because no one can sense the global temperature. Like Tip O’Neill’s politics, climate is local. So those who would seek to impose costs on society to prevent climate change had better demonstrate that warming the planet will make large regional excursions more, not less, likely. 



Recently, I explored this notion in a paper in the refereed journal Climate Research. Relying upon historical data (and explicitly ignoring computer models of climate because of their patent unreality), I found that temperature variability between seasons and between years has significantly declined in the second half of this century. And there have been a few warm years in that period, too. 



So when I looked at the variability as a function of the planet’s annual temperature, I found that the cool years were more variable and the warmer ones less. Conclusion? Warming the planet decreases variability on a year‐​to‐​year scale. Cooling the planet makes things more variable. 



That’s pretty good evidence that what human beings are doing to the climate makes things more predictable and equable than before. 



Want more? When the carbon dioxide concentration of the atmosphere was at its highest level since animals first appeared, the biggest animals in history roamed the earth: dinosaurs. Those beasts required a tremendous amount of vegetation to reach their enormous size. Carnivores, like T. Rex, were supported by the massive herbivores. How many tons of vegetation were ultimately required to feed them, considering it had to pass through huge lunks like Apatosaurus (that’s Brontosaurus to you intellectual dinosaurs)? The toasty earth had to have been greener than casino felt. 



What’s more, when the dinos were around, the climate was so stable that they were cold blooded! They’d probably still be here today, except for the fact that they went extinct when the earth got clobbered by a small asteroid. The asteroid raised a huge cloud of dust and killed them with global cooling, which made the climate more variable, resulting in an undependable food supply. 



Our greener friends might become extinct too, if they tout Overpeck’s findings as good news for their side.
"
"

Kalamazoo State Hospital
In an effort to add to the surfacestation.org survey coverage, I’ve been looking at a number of stations from the aerial vantage points available in Google Earth and Microsoft Live Maps. I particularly look for the stations that have Stevenson Screens, as those are the most visible and easy to spot from these online resources.
I was disappointed to learn though that the USHCN station at Kalamazoo State Hospital (a state psychiatric hospital) has been closed. It was probably due to recent construction of new wards as seen here:
Click for a live interactive aerial view
While I was scouring online image databases looking for a surviving photo that would show the placement of the Stevenson Screen, I stumbled across this photo on Flickr taken from afar and this strange comment about it:

Flickr caption reads: I took this photo from well away from the grounds of the State Hospital, so as to avoid violating state law.
“Apparently, it is illegal to take photographs on the grounds of the state hospital as a protection to those who are patients there.  I received a lengthy explanation of this law from the Michigan State Police officer who was on patrol at the hospital the morning I visited.  He advised me that the hospital office has quite a collection of confiscated cameras.”
I did visit another state mental hospital in Napa, CA and found this placement of the MMTS:

You can see a full set of pictures, at the surfacestations.org image database.
So WUWT readers, here is the challenge against very unlikely odds:
Find a surviving photo of the USHCN weather station at Kalamazoo State Hospital. 
Good hunting.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a64e2ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
AIRS has higher resolution tracking of global CO2 - click for image
I’m going to make a formal post on this later, but I wanted to bring it up for discussion now since many people have been waiting for this paper to be published. For my previous perspectives and replies from authors, see this post here:
An encouraging response on satellite CO2 measurement from the AIRS Team
Hat tip to F Rasmin who writes with a link to the new paper:
Hello Anthony. Is this the awaited paper from the AIRS TEAM? ‘Satellite remote sounding of mid-tropospheric CO2′, published 9 September 2008 at:
http://www.agu.org/journals/gl/gl0817/2008GL035022/ 

REPLY: Yes it is. This was on my list of things to check this week, thanks for the tip! I’ll write it up sas soon as I can read it. In the meantime, feel free to post more comments on it in this thread.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c4bac48',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIt appears that major media outlets took reports based on old observations and passed it on to the public as breaking news last week.
H/T: Reader Dirk H.
Last week a number of major media outlets reported on how a large plume of oil had been spotted in the Gulf of Mexico, insisting that the BP oil spill was not naturally disappearing, as claimed by BP and US officials.
Well, it turns out that last week’s reports were based on oil plume observations made way back in June and junk science. Here are some of the headlines we saw last week in the land of angst, Germany:
Sueddeutsche Zeitung on August 18:
Scientists: 80% Of The Oil Is Still There
Der Spiegel on August 18;
Scientists Attack US Government’s Announcement That Worst Is Over
WDR5 German Radio on August 20:
Large Oil Cloud Beneath The Surface
Die Welt on August 21:
Platform Operator Accuses BP Of Cover-Up  and on August 20: 35 Kilometer Oil Slick
All these reports claimed that BP and officials were premature in calling off the emergency, and that the oil slick was far greater than the public was led to believe. There’s much more oil out there and the problem is still an environmental catastrophe, they all insisted last week.
What did these enviro-bedwetting journalists base their stories on?
They were based on a study by Woods Hole Oceanographic Institute describing an undersea oil cloud observed June 19 to 28. Here’s the Wood Hole press release. These media outlets were too lazy to check out the source.
So is there still lots of oil out there? No.
The Washington Post wrote yesterday on a Study: Petroleum-eating microbes significantly reduced gulf oil plume that according to the newest findings by a team of scientists led by Terry C. Hazen of the Lawrence Berkeley National Laboratory in California, the oil plumes are today practically gone, and on how microbes and bacteria have…



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




…reduced the amount of oil amounts in the undersea “plume” by half about every three days, according to research published online Tuesday by the journal Science.
The Washington Post also mentions how the Woods Hole study was based on observations from June.  The WaPo writes:
In the Woods Hole study, scientists described finding an undersea oil cloud June 23 to 27 similar to the one Hazen and his colleagues found between May 25 and June 2 – which was similar to one found soon after by people from the Montereyâ Bay Aquarium Research Institute.
And so the mystery of the missing oil is explained:
The findings, by a team of scientists led by Terry C. Hazen of the Lawrence Berkeley National Laboratory in California, help explain one of the biggest mysteries a mystery of the disaster: Where has all the oil gone?
‘What we know about the degradation rates fits with what we are seeing in the last three weeks,’ Hazen said. “We’ve gone out to the sites, and we don’t find any oil, but we do find the bacteria.”
The oil is gone. The WaPo report also shows that the feared oxygen depletion catastrophe is also all hype:
One thing that many scientists feared – severe depletion of oxygen as microbes consumed the oil – apparently hasn’t happened. The Woods Hole study published last week found no decrease in oxygen in the oil plume, and the new study found only a slight one.
No plume could be found in the past three weeks, however. The oil that remains appears to be too diluted to be detected.
There it is folks. Another bout of incontinence by the German media. Listening to them last week, you’d think the Gulf of Mexico was sloshing with crude. Bad reporting is a great way to wrongly scare away tourists at a time the Gulf coast needs them the most.
Share this...FacebookTwitter "
"

The recent Earth Summit, the United Nations Conference on Environment and Development (UNCED), in Rio de Janeiro was to have established an ambitious environmental agenda for the 21st century. The conference addressed virtually every field of human endeavor: energy, industry, water and land use, agriculture, human health, and transportation, among others. An international congregation of tens of thousands of bureaucrats, environmentalists, technocrats, planners, and others arrived in Rio to guide humanity on its course toward the brave new world defined by Agenda 21, the Rio Declaration, the Conventions on Climate Change and Biological Diversity, and the Forest Principles.



The principal theme for UNCED was “sustainable development,” a concept popularized by the report of the World Commission on Environment and Development. That report, entitled Our Common Future, was presented to the United Nations in 1987 by commission chairman Gro Harlem Brundtland, the Labor prime minister of Norway.



Our Common Future was compiled in response to a 1983 request of the UN General Assembly for “a global agenda for change.” Heavily influenced by former West German chancellor Willy Brandt’s North‐​South Report, the commission called for restructuring mankind’s future so that economic growth would be “based on policies that sustain but expand the environmental resource base.”[1]



“Sustainable development” means different things to different people. Its definition is intentionally vague to increase the possibility of compromise on thorny issues on which reasonable people may differ. To those inclined toward balancing economic and environmental goals–as are the authors of this study–“development” implies economic growth, and “sustainable” implies full consideration of environmental factors. It is becoming abundantly clear, however, that to others the term implies virtually no additional economic development.[2] The latter position is based on the argument–made with great emotion but insufficient facts and analysis–that the current path of development is clearly unsustainable because the planet is about to choke on humanity’s wastes and there is not enough land to meet everyone’s demands. Even though that faction, inherently suspicious of technological change and economic growth, was unable to obtain all that it wanted, it is today much closer to its goal as laid out explicitly in the early drafts of the Agenda 21 documents.



The draft Agenda 21 documents specified various measures that nations “should” undertake. Ultimately, the conference shied away from that heavy‐​handed approach and decreed those measures to be optional. If acted upon faithfully, many of those measures would impose national planning on virtually every economic sector, regulate almost all human activity, and subordinate all social and economic goals to environmental goals. Thus, while calling for a massive transfer of wealth and technology from developed to developing nations, the recommended measures would deny all nations the means of creating more technology and wealth by putting in place legal and institutional frameworks that would be incompatible with either technological progress or economic growth. Proponents of those measures, in part because of their refusal to balance environmental and socioeconomic goals, would needlessly reduce agricultural productivity in the name of sustainability–even as they lament that the world has too many people to feed and that agriculture uses too much land and denudes forests, thereby threatening species and reducing biodiversity. Such measures, in addition to being poor social and economic policy, would be counterproductive; they would cause more environmental harm than good, and they might even bring about the very catastrophe environmentalists strive to avert.



In particular, the Convention on Biological Diversity, which the United States rightly refused to sign, does not even address the single most important reason for the loss of biological diversity and deforestation: the loss of habitat and land conversion to meet fundamental human needs for food, clothing, and shelter. Moveover, the convention would reduce the incentives to research and develop the very technologies that would–if anything could–help meet the competing demands made on land by human beings and other species.



Using as an example the historical increase in U.S. agricultural productivity, this study will show that–but for technological progress–all our forestlands and croplands, including those that would have been only marginally productive, would have had to have been plowed to produce the quantities of food we produce today. The accompanying wholesale destruction of forests and other natural habitats and the reduction of biodiversity would have resulted in environmental problems that would have dwarfed those we face today and matched environmentalists’ worst nightmares.



The only way to feed, clothe, and shelter the greater world population that the future will inevitably bring–while limiting deforestation and loss of biodiversity and carbon dioxide sinks–is to increase, in an environmentally sound manner, the productivity of all activities that use land.[3] Such increases are possible only within a legal, economic, and institutional framework that relies on free markets, fosters decentralized decisionmaking, respects individual property rights, and rewards entrepreneurship. Such an approach is the best hope for a world facing severe pressure on its land base, yet it has been noticeably absent from recent, much‐​publicized strategies that purport to lead us to sustainable development, conserve biological diversity, and combat global deforestation.
"
"
Share this...FacebookTwitterThis is how ZDF German Public television envisioned how life in the year 2000 could appear back in 1972. Hat-tip Michael Miersch here.
Part 1 (Youtube video): Living and going to work (
The first part shows a man named Mr B living in a futuristic, electronic apartment. In the year 2000 people have to work only 25 hours per week, and can live with artificial hearts. Breakfast is prepared automatically and that all food is free of poisons because it is organically produced, free of pesticides and chemicals.
There are no printed newspapers – people get their news from a “printer” twice a day. Everyone lives in huge high rise apartment buildings with 2000 units, each equipped with international satellite TV that can be watched 24 hours per day. Shopping is done by radio-teleshopping; purchases are booked direct from his bank account.
Mr B communicates with friends using a “TV telephones” (smoking is still politically correct).
People don’t visit each other anymore – they converse via TV screen. Sociologists warn of the isolation of man by technology.
Mr B. does not use a conveying sidewalk to go the short distance to the train station when he goes to work, he walks. The air is now clean again because pollution was banned in 1990. Some even called for the death penalty for polluters. He works in another city 80 km away. No problem though, the jet-engine powered 500 km/hr commuter monorail train needs only 15 minutes. At the station he rents an electric city-car, which are readily available at all transportation hubs.
Part 2: At work in the year 2000, and the environmental hell of 1972.
The electric cars are automatically navigated. At work a massive network of people-conveyors take him through the huge maze of buildings. He works at a databank center that sells data to customers. All data is stored at a massive data storage centers and systems. For example, the databanks deliver critical data to politicians almost instantly so that they can always make the right decisions.
Everything is automated, and so Mr B has lots of time on his hands at work – no stress. That’s the way it is for millions of highly skilled workers like him, who only need to sit around and monitor the automated systems. As he sits around, he thinks about what he’ll do when he retires at the age of 50.
Retirement in the year 2000 is a problem too, as people have yet to figure out how spend all that free time on their hands. This is how ZDF imagined life could be in the year 2000 back in 1972.
At the 6 min mark, the show returns back to the reality of 1972. Here ZDF bemoans that 14 million cars jam the streets of West Germany. They pollute the air and threaten to choke the citizens in a sea of metal, exhust and noise. Cars are a symbol of freedom, but in reality they condemn the people to being stuck in traffic jams. The car – it kills 17,000 and injures 500,000 every year. Millions of tons of sulfur dioxide, carbon monoxide and lead are blown yearly into the atmosphere.
Air pollution in urban areas is already at the allowable limits. The SST pollutes the entire stratosphere and creates extreme noise – all to save 3 hrs of flight time. ZDF says we’ve poisoned the biosphere and food chain with our pesticides. Industry has polluted the water so much that clean water will be a luxury product in just a few years. Germany’s Lake Constanz will soon look like Lake Erie. It will take 50 to 100 years before Lake Erie returns to a natural condition.
Part 3: Man is destroying the planet – we have precious little time to save it


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Part 3 looks at life and the environment as it was in 1972. The film starts with:
Industrialization brought prosperity to many, but a threat to all. Industrialization favors the concentration of capital and assets in the hands of a few.
The clip then describes the growing gap between the haves and have-nots. Up to 800,000people (from 60 million) live in poverty. The film says that by1980, half a million students will jam into the universities, where only half will be able find housing, and classroom will be overfilled, leading to riots and civil unrest.
ZDF complained that the field of medicine is outdated, and Germany is falling behind. 30% of all hospital beds were made in the 1920s.
ZDF then asks a series of pessimistic questions. Will the political system be able to quickly enough make the decisions necessary to bring the country forward to meet the challenges of the year 2000?  How will the family survive? How will the workplace change? Automation threatens to turn workers into mere monitors.
For millions there isn’t going to be any work.”
Another problem is that the economy produces more food then what is needed. Europe destroys the surplus of food while people starve in other places on the globe. ZDF then juxtaposes this with military spending, which amounts to 600 billion German Marks annually. “For the first time in history, man is capable of destroying the planet.
The only choice is either we live together, or die together.”
ZDF then complains that technology is advancing too quickly; we can’t keep up and that we don’t really know what we’re getting ourselves into. Since WWII, millions of tons of concrete have been transformed into living units. Now everywhere the landscape is littered with high rise apartment buildings. No one thought about the impact on man and society. No research is being done to see where all this is taking us. The ZDF clip ends with a quote:
‘The capability of man to thoughtlessly destroy the environment is practically without limits,’ says an American scientist. Poiliticans face the challenge of stopping the destruction before it becomes too late. For that we have very little time.”
End
———————————————————————————-
Today we see that life is much better and different than what ZDF predicted in 1972. Lake Erie is also clean again. Sulphur dioxide and carbon monoxide have long since been replaced by life-giving CO2 as the big threat to humanity and nature. The dire prophesies never came true – even though there was a consensus among the “experts”.
HAPPY NEW YEAR EVERYBODY!
Share this...FacebookTwitter "
"
Share this...FacebookTwitter!!! UPDATE: Read the Europol Press Release here !!!
– Estimated 5 billion euros in damage for European taxpayers
– Massive fraud involving criminal networks / Middle East
Hat tip Reader Dirk H.
Here’s more proof that trading of CO2 emission certificates is fraught with fraud and attracts seedy criminal organizations – all costing the consumers and taxpayers billions.
Worse yet, it has spread out of control and appears that the authorities can’t keep up.
The Austrian online Kleine Zeitung here reports that Europol have raided an elaborate CO2 emissions scam in Italy and have arrested more than 100 persons.
The Kleine Zeitung writes: “The damage runs in the billions of euros”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Europol, the Italian tax authorities, directed by the Milan Prosecutor’s Office, have raided 150 companies in Italy. The fraud involves evasion of value added tax with CO2 emission certificates. More than 100 have been arrested and are suspected of being involved in organised crime.
The Kleiner Zeitung reports that the Italian Electric Utilities trading markets had earlier halted entire trading with emissions certificates “because a high number of abnormal transactions”. The loss in tax revenue just from VAT (MTIC (Missing Trader IntraCommunity Fraud) alone is estimated to be 500 million euros, the online Kleine Zeitung writes.
The fraud is widespread
According to reports, it’s been known since June of last year that criminal organizations have been using CO2-emissions trading for defrauding governments of value added tax.
This is not the first time that police raids of this scale have taken place. It’s the latest in a series of raids that have been carried out all over Europe this year, all involving the trading of CO2 emission certificates. It seems the authorities just can’t keep up with the multitudes of swindlers out there.
Norway, Switzerland and the EU countries Belgium, Czech Republic, Denmark, Latvia, the Netherlands, Slovak Republic and Portugal are all among the countries trying to identify the network of criminals behind this massive fraud – a fraud with links to criminal networks operating outside the EU and in other continents, like the Middle East.
2500 investigators – trying to identify. That’s means they haven’t yet. That’s a lot of fraud. The fraud has spread from science to finance. Expect a meltdown – sooner than later.
UPDATE 2: Recall this Danish fraud: http://climaterealists.com/index.php?id=6790
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAlex Bojanowski at Germany’s online Der Spiegel reports here on a new paper appearing in Nature that shows climate change in the 1970s was caused by ocean cooling. Climate simulation models once indicated that the cooling in the 1970s was due to sun-reflecting sulfur particles, emitted by industry. But now evidence points to the oceans.

I don’t know why this is news for the authors of the paper. Ocean cycles are well-known to all other scientists. The following graphic shows the AMO 60-year cycle, which is now about to head south.
Atlantic Multidecadal Oscillation (AMO). Source: http://www.appinsys.com/globalwarming/SixtyYearCycle.htm
Computer models simulating future climate once predicted that it would soon get warm because of increasing GHG emissions, but, writes Der Spiegel, citing Nature:
Now it turns out that the theory is incomplete. A sudden cooling of the oceans in the northern hemisphere played the decisive role in the drop of air temperatures.
The paper was authored by David W. J. Thompson, John M. Wallace, John J. Kennedy, and Phil D. Jones. The scientists discovered that ocean temperatures in the northern hemisphere dropped an enormous 0.3°C between 1968 and 1972. Der Spiegel writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A huge amount of energy was taken out of the oceans. The scientists said that it was surprising that the cooling was so fast.
 This shows, again, that the climate simulation models used for predicting the future are inadequate. It’s not sure what caused the oceans to cool. But scientists are sure that aerosols were not the cause. Der Spiegel describes a possible scenario how the oceans may have cooled: 
Huge amounts of melt water from Greenland’s glaciers poured into the Atlantic at the end of the 1960s, and formed a cover over the ocean. The melt water cooled the ocean for one thing, and acted to brake the Gulf Stream, which transports warm water from the tropics and delivers it to the north. The result: the air also cools down.
But, as Spiegel reports, that hardly explains why there was also cooling in the north Pacific. Der Spiegel:

 The scientists will have to refine their climate simulations. The new study shows one thing: The influence of the oceans is greater than previously thought.

I’d say that’s a very polite way of saying: Your models have been crap, and it’s back to the drawing board. This time don’t forget to properly take the oceans and every thing else into account. Yes, there’s a quite a bit more to climate than a single trace gas in the atmosphere. Hooray – the warmists are finally beginning to realize it! (Maybe)
Share this...FacebookTwitter "
"

WOW look at the SIZE of that seal! (photo added by Anthony, not NYT)
Bearing Up
By SARAH PALIN
Published: January 5, 2008,
Juneau, Alaska
ABOUT the closest most Americans will ever get to a polar bear are those cute, cuddly animated images that smiled at us while dancing around, pitching soft drinks on TV and movie screens this holiday season.
This is unfortunate, because polar bears are magnificent animals, not cartoon characters. They are worthy of our utmost efforts to protect them and their Arctic habitat. But adding polar bears to the nation’s list of endangered species, as some are now proposing, should not be part of those efforts.
To help ensure that polar bears are around for centuries to come, Alaska (about a fifth of the world’s 25,000 polar bears roam in and around the state) has conducted research and worked closely with the federal government to protect them. We have a ban on most hunting — only Alaska Native subsistence families can hunt polar bears — and measures to protect denning areas and prevent harassment of the bears. We are also participating in international efforts aimed at preserving polar bear populations worldwide.
This month, the secretary of the interior is expected to rule on whether polar bears should be listed under the Endangered Species Act. I strongly believe that adding them to the list is the wrong move at this time. My decision is based on a comprehensive review by state wildlife officials of scientific information from a broad range of climate, ice and polar bear experts.
The Center for Biological Diversity, an environmental group, has argued that global warming and the reduction of polar ice severely threatens the bears’ habitat and their existence. In fact, there is insufficient evidence that polar bears are in danger of becoming extinct within the foreseeable future — the trigger for protection under the Endangered Species Act. And there is no evidence that polar bears are being mismanaged through existing international agreements and the federal Marine Mammal Protection Act.
The state takes very seriously its job of protecting polar bears and their habitat and is well aware of the problems caused by climate change. But we know our efforts will take more than protecting what we have — we must also learn what we don’t know. That’s why state biologists are studying the health of polar bear populations and their habitat.
As a result of these efforts, polar bears are more numerous now than they were 40 years ago. The polar bear population in the southern Beaufort Sea off Alaska’s North Slope has been relatively stable for 20 years, according to a federal analysis.
We’re not against protecting plants and animals under the Endangered Species Act. Alaska has supported listings of other species, like the Aleutian Canada goose. The law worked as it should — under its protection the population of the geese rebounded so much that they were taken off the list of endangered and threatened species in 2001.
Listing the goose — then taking it off — was based on science. The possible listing of a healthy species like the polar bear would be based on uncertain modeling of possible effects. This is simply not justified.
What is justified is worldwide concern over the proven effects of climate change.
The Center for Biological Diversity, which petitioned for the polar bear to be protected, wants the listing to force the government to either stop or severely limit any public or private action that produces, or even allows, the production of greenhouse gases. But the Endangered Species Act is not the correct tool to address climate change — the act itself actually prohibits any consideration of broader issues.
Such limits should be adopted through an open process in which environmental issues are weighed against economic and social needs, and where scientists debate and present information that policy makers need to make the best decisions.
Americans should become involved in the issue of climate change by offering suggestions for constructive action to their state governments. But listing the polar bear as threatened is the wrong way to get to the right answer.

Sarah Palin, a Republican, is the governor of Alaska.
h/t to L Nettles


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d4159eb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

This was an eventful week for two government institutions, the Supreme Court and Senate. More than a year after Justice Antonin Scalia’s death, the high court will on Monday finally return to a full complement of nine justices. But the confirmation of the newest justice, Neil Gorsuch, happened only after the Senate decided, on a party‐​line vote, to exercise the “nuclear option” and remove filibusters for Supreme Court nominations.



These developments sound like a really big deal, but they were easily predictable given our toxic political climate and won’t actually change the operation of either institution. But here are five takeaways for our post‐​nuclear‐​option world:





By filibustering the milquetoast Gorsuch despite the high probability and repeatedly expressed intention of the Republicans to go nuclear, the Democrats have destroyed any leverage they had over the next nominee.



 **1.** The Supreme Court. The court effectively returns to the status quo before Scalia’s death. No two justices are the same, but Gorsuch could have been expected to vote the same as Scalia on all the hot‐​button cases that broke down 5–4, and also on the cases (especially in criminal procedure) that joined the court’s left and right against the middle. As it turns out, Scalia’s absence only changed the result in a handful of cases and the court has largely succeeded in avoiding 4–4 splits. Adding a ninth justice will, however, make it marginally easier to get the four votes needed to “grant cert” (have a case accepted for review), especially on potentially controversial issues.



 **2.** The Senate. The exercise of the “nuclear option” returns Senate procedures to what they were 15 years ago. The filibuster was simply not employed for partisan purposes against a nominee who had majority support before Harry Reid started filibustering George W. Bush’s lower‐​court nominees in 2003. (Infamously, the Senate denied Miguel Estrada an up‐​or‐​down vote seven times to prevent Bush from later having the opportunity to elevate the first Hispanic justice.) Reid used the “nuclear option” to eliminate that sort of filibuster a decade later, so perhaps this week’s action should be called “thermonuclear.” A Senate majority will still be able to stall a nomination made by a president of the opposing party—we could see more Merrick Garlands—but a Senate minority will lack that power.



 **3.** The next nominee. By filibustering the milquetoast Gorsuch despite the high probability and repeatedly expressed intention of the Republicans to go nuclear, the Democrats have destroyed any leverage they had over the next nominee. Should there be another vacancy under President Trump while the GOP controls the Senate, there will be zero incentive for the President to moderate his choice. It’s not at all clear that Republican senators such as Susan Collins of Maine, Lisa Murkowski of Alaska, Lindsey Graham of South Carolina and other “institutionalists” would’ve gone along with a “nuclear option” to replace Justice Ruth Bader Ginsburg with a nominee more controversial than Gorsuch. But now they won’t face that dilemma.



 **4.** Our political culture. Given the highly charged battle we’ve seen — only three Democrats, from states Trump won bigly (Indiana, North Dakota, West Virginia), voted for Gorsuch, and just one more, fellow Coloradan Michael Bennet, voted against a filibuster — too many people will now think of the justices in partisan terms. That’s too bad, but not a surprise when contrasting methods of constitutional and statutory interpretation largely track party politics. Relatedly, confirmation hearings will continue to be Kabuki theater, educational to some about various legal doctrines but not illuminating anything of the nominee’s judicial philosophy. On the other hand, perhaps nominees will occasionally feel free to express themselves, knowing that they don’t need any of the minority party’s votes.



 **5.** Neil Gorsuch. You may not agree with him on every case, but his opinions will be well‐​reasoned and clearly written. Gorsuch’s mentor, Justice Byron White, liked to say that each new justice makes for a new court, and I look forward to the breath of fresh air, intellectual rigor, collegiality, and constitutional seriousness that Justice Gorsuch will bring. Neil Gorsuch will serve with distinction.
"
"
In comments, Jonn-X wondered:
Dead pixels or new sunspecks (pore-ettes) ?
At first I was pretty sure I was looking at nothing, then I saw the official NOAA bulletin
http://www.swpc.noaa.gov/forecast.html
and the usual phrase, “The visible disk was spotless,” was omitted – typical practice when there’s something there, but too small to be “officially noticed.”
Anybody else see anything?
I do. I know where the dead pixels are, and have labeled them below in the SOHO MDI image. Note that there are two very small sunspecks, possibly soon to be sunspots, emerging on both sides of the equator.

Click for a full sized image
For those that don’t know. The SOHO spacecraft sensor does have some stuck pixels, and these can sometimes be cured in a “bake off” where they heat up the sensor for a few hours.
Our resident official solar physicist, Dr. Leif Svalgarrd will confirm or refute my suspicions on the categorizations of SC23 and SC24 I’m sure. For comparisons, you can also see the SOHO magnetogram.
I’ve included it also below:
UPDATE: The specks are fading, so far no observation agency has assigned a region or counted them that I know of, see the updated SOHO MDI.
SOHO Magnetogram- click for larger image
UPDATED SOHO MDI:

Click for larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c58d268',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
It has been an interesting couple of days. Today yet another scientist has come forward with a press release saying that not only did their audit of IPCC forecasting procedures and found that they “violated 72 scientific principles of forecasting”, but that “The models were not intended as forecasting models and they have not been validated for that purpose.” This organization should know, they certify forecasters for many disciplines and in conjunction with John Hopkins University if Washington, DC, offer a Certificate of Forecasting Practice. The story below originally appeared in the blog of Australian Dr. Jennifer Marohasy. It is reprinted below, with with some pictures and links added for WUWT readers. – Anthony
 
J. Scott Armstrong, founder of the International Journal of Forecasting
Guest post by Jennifer Marohasy
YESTERDAY, a former chief at NASA, Dr John S. Theon, slammed the computer models used to determine future climate claiming they are not scientific in part because the modellers have “resisted making their work transparent so that it can be replicated independently by other scientists”. [1]
Today, a founder of the International Journal of Forecasting, Journal of Forecasting, International Institute of Forecasters, and International Symposium on Forecasting, and the author of Long-range Forecasting (1978, 1985), the Principles of Forecasting Handbook, and over 70 papers on forecasting, Dr J. Scott Armstrong, tabled a statement declaring that the forecasting process used by the Intergovernmental Panel on Climate Change (IPCC) lacks a scientific basis. [2]
What these two authorities, Drs Theon and Armstrong, are independently and explicitly stating is that the computer models underpinning the work of many scientific institutions concerned with global warming, including Australia’s CSIRO, are fundamentally flawed.
In today’s statement, made with economist Kesten Green, Dr Armstrong provides the following eight reasons as to why the current IPCC computer models lack a scientific basis:
1. No scientific forecasts of the changes in the Earth’s climate. 
Currently, the only forecasts are those based on the opinions of some scientists. Computer modeling was used to create scenarios (i.e., stories) to represent the scientists’ opinions about what might happen. The models were not intended as forecasting models (Trenberth 2007) and they have not been validated for that purpose. Since the publication of our paper, no one has provided evidence to refute our claim that there are no scientific forecasts to support global warming.
We conducted an audit of the procedures described in the IPCC report and found that they clearly violated 72 scientific principles of forecasting (Green and Armstrong 2008). (No justification was provided for any of these violations.) For important forecasts, we can see no reason why any principle should be violated. We draw analogies to flying an aircraft or building a bridge or performing heart surgery—given the potential cost of errors, it is not permissible to violate principles.
2. Improper peer review process. 
To our knowledge, papers claiming to forecast global warming have not been subject to peer review by experts in scientific forecasting.
3. Complexity and uncertainty of climate render expert opinions invalid for forecasting. 
Expert opinions are an inappropriate forecasting method in situations that involve high complexity and high uncertainty. This conclusion is based on over eight decades of research. Armstrong (1978) provided a review of the evidence and this was supported by Tetlock’s (2005) study that involved 82,361 forecasts by 284 experts over two decades.
Long-term climate changes are highly complex due to the many factors that affect climate and to their interactions. Uncertainty about long-term climate changes is high due to a lack of good knowledge about such things as:
a) causes of climate change,
b) direction, lag time, and effect size of causal factors related to climate change,
c) effects of changing temperatures, and
d) costs and benefits of alternative actions to deal with climate changes (e.g., CO2 markets).
Given these conditions, expert opinions are not appropriate for long-term climate predictions.
4. Forecasts are needed for the effects of climate change. 
Even if it were possible to forecast climate changes, it would still be necessary to forecast the effects of climate changes. In other words, in what ways might the effects be beneficial or harmful? Here again, we have been unable to find any scientific forecasts—as opposed to speculation—despite our appeals for such studies.
We addressed this issue with respect to studies involving the possible classification of polar bears as threatened or endangered (Armstrong, Green, and Soon 2008). In our audits of two key papers to support the polar bear listing, 41 principles were clearly violated by the authors of one paper and 61 by the authors of the other. It is not proper from a scientific or from a practical viewpoint to violate any principles. Again, there was no sign that the forecasters realized that they were making mistakes.
5. Forecasts are needed of the costs and benefits of alternative actions that might be taken to combat climate change. 
Assuming that climate change could be accurately forecast, it would be necessary to forecast the costs and benefits of actions taken to reduce harmful effects, and to compare the net benefit with other feasible policies including taking no action. Here again we have been unable to find any scientific forecasts despite our appeals for such studies.
6.  To justify using a climate forecasting model, one would need to test it against a relevant naïve model. 
We used the Forecasting Method Selection Tree to help determine which method is most appropriate for forecasting long-term climate change. A copy of the Tree is attached as Appendix 1. It is drawn from comparative empirical studies from all areas of forecasting. It suggests that extrapolation is appropriate, and we chose a naïve (no change) model as an appropriate benchmark. A forecasting model should not be used unless it can be shown to provide forecasts that are more accurate than those from this naïve model, as it would otherwise increase error. In Green, Armstrong and Soon (2008), we show that the mean absolute error of 108 naïve forecasts for 50 years in the future was 0.24°C.
7. The climate system is stable. 
To assess stability, we examined the errors from naïve forecasts for up to 100 years into the future. Using the U.K. Met Office Hadley Centre’s data, we started with 1850 and used that year’s average temperature as our forecast for the next 100 years. We then calculated the errors for each forecast horizon from 1 to 100. We repeated the process using the average temperature in 1851 as our naïve forecast for the next 100 years, and so on. This “successive updating” continued until year 2006, when we forecasted a single year ahead. This provided 157 one-year-ahead forecasts, 156 two-year-ahead and so on to 58 100-year-ahead forecasts.
We then examined how many forecasts were further than 0.5°C from the observed value. Fewer than 13% of forecasts of up to 65-years-ahead had absolute errors larger than 0.5°C. For longer horizons, fewer than 33% had absolute errors larger than 0.5°C. Given the remarkable stability of global mean temperature, it is unlikely that there would be any practical benefits from a forecasting method that provided more accurate forecasts.
8.  Be conservative and avoid the precautionary principle. 
One of the primary scientific principles in forecasting is to be conservative in the darkness of uncertainty. This principle also argues for the use of the naive no-change extrapolation. Some have argued for the precautionary principle as a way to be conservative. It is a political, not a scientific principle. As we explain in our essay in Appendix 2, it is actually an anti-scientific principle in that it attempts to make decisions without using rational analyses. Instead, cost/benefit analyses are appropriate given the available evidence which suggests that temperature is just as likely to go up as down. However, these analyses should be supported by scientific forecasts.
The reach of these models is extraordinary, for example, the CSIRO models are currently being used in Australia to determine water allocations for farmers and to justify the need for an Emissions Trading Scheme (ETS) – the most far-reaching of possible economic interventions.   Yet, according to Dr Armstrong, these same models violate 72 scientific principles.
********************
1. Marc Morano, James Hansen’s Former NASA Supervisor Declares Himself a Skeptic, January 27,2009. http://epw.senate.gov/public/index.cfm?FuseAction=Minority.Blogs&ContentRecord_id=1a5e6e32-802a-23ad-40ed-ecd53cd3d320
2. “Analysis of the U.S. Environmental Protection Agency’s Advanced Notice of Proposed Rulemaking for Greenhouse Gases”, Drs. J. Scott Armstrong and Kesten C. Green a statement prepared for US Senator Inhofe for an analysis of the US EPA’s proposed policies for greenhouse gases.  http://theclimatebet.com

Sponsored IT training links:
Get guaranteed success in 312-50 exam in first try using incredible 642-374 dumps and other 310-200 training resources prepared by experts.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98f2bcc7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn the wake of the massive earthquake and tsunami that devastated Japan, a tsunami of hysteria has swept the German Green Party to a stunning victory in elections in the state of Baden-Württemberg.The Greens also piled on in the Rhineland Palatinate elections as well. One thing is clear:  Angela Merkel’s coalition government with the FDP liberal democrats were punished for their horrendous handling of energy policy in Germany.
One-year ago Merkel and her government approved operating lifetime extensions of nuclear power plants in Germany, giving 8 older reactors the seal approval, certifying they were safe and reliable. That, they were. Indeed German reactors and their management are among the best and safest in the world.
But then came Fukushima, followed by the tsunami of panic generated by an irresponsible media, anti-nuclear activists and opportunistic Greens, who fanned the flames of hysteria with vigor.
But rather than standing up and putting her full faith in German nuclear reactor technology and management, Angela Merkel panicked, blinked and turned her back on the industry, ordering the shutdown of the 8 nuclear reactors that were built before 1981. By turning her back on the nuclear industry, she in the end will have turned her back on the country. The days of cheap, competitive and reliable energy are coming to an end in Germany.
Turning her back on the nuclear industry also made her government’s earlier approval of the reactors appear like a farce. Angela Merkel only succeeded in showing where her priorities are – political survival. Her stance with regards to the Libya war was also pure political calculation.
Now it’s time to pay the piper. Angela Merkel’s government is now collapsing faster than Japan did under it’s 9.0 March 11 earthquake. Rarely does anyone witness ineptitude of this magnitude. She deserved to fail. All of this now clears the way for the SPD socialists and environmental Greens to sweep into power in 2013 in the national elections.
This all clears the way for the German renewable energy experiment, as the reds and the greens are eager to go full throttle with the renewable energy madness, which in the end will make Germany vulnerable and dependent on its neighours. Thanks in part to Merkel, the way is clear to embark on something that has never been tried: an experiment to see if it is possible to power an industrialised and developed country without nuclear and carbon-based energy.
Share this...FacebookTwitter "
"

TUCSON, July 31 (UPI) — Scientists  confirmed Thursday that water, considered an essential building block of life,  does indeed exist on the planet Mars.An analysis of a soil sample collected by the Phoenix lander  detected traces of water, which exists as ice just below the red soil on the  Martian surface.

“We’ve seen evidence for this water ice before in  observations by the Mars Odyssey orbiter and in disappearing chunks observed by  Phoenix last month,” scientist William Boynton said in a  written statement released by NASA and the Jet Propulsion Lab, “but this is the  first time Martian water has been touched and tasted.”
Boynton is lead scientist for the Thermal and Evolved-Gas  Analyzer team based at the University of Arizona.
Details of the composition of the water were not immediately  released. The sample came from a 2-inch deep trench carefully carved by the  lander’s robotic arm.
The presence of water is one of more dramatic discoveries  made by the Phoenix since it touched down on Mars near the pole May 24. NASA  announced it had secured funding to extend the Phoenix mission through Sept. 30.
More here: http://phoenix.lpl.arizona.edu/



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d675474',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I’m pleased to announce that the www.surfacestations.org project has reached a major milestone, with 67% of the 1221 USHCN network now surveyed.
819 of 1221 stations have been examined in the USHCN network. Of the 819, 807 have been assigned a site quality rating. In some of those cases we’ve found the stations closed, or we are waiting for supplemental information to enable assigning a rating.
The  Google Earth map below shows current coverage. We are in sight of the goal. However there are still some holes, especially in south Texas, Alabama, Idaho, Arkansas, Missouri and Illinois.
See this Google Earth generated image. The circles with question marks are stations left to be surveyed.

Click for a larger image
A Google Earth USHCN Station Rating Map (KML file used to generate the above image) is available – download  here
You can download the Google Earth application for free from this link
Sincere thanks to Gary Boden for this  contribution! This is a very useful tool to help locate stations as hi resolution lat/lon values and descriptions are available from each map icon. Of course, Google Earth will also plot driving directions too.
I’m hoping to reach a minimum of 75% before I start doing data analysis. I want to find more rural stations, with the hope of finding more of the better sited stations since the lions share is comprised of CRN3-5 stations.  I’m hoping those of you that live near some of these “holes” can help. if you can, please leave a comment below and I’ll help you locate stations. You’ll also need to visit the website www.surfacestations.org and register as a volunteer. It’s free and easy.
Here is what the current rating breakdown looks like:

click for a larger image
For those unfamiliar with the rating system, it is identical to the one used by NOAA/NCDC to select sites for their new Climate Refernece Network (CRN) They drew this rating scheme from a paper published by Michel Leroy, of MeteoFrance, that he devised for their meteorological network. Here are the details:
Climate Reference Network Rating Guide – adopted  from NCDC Climate Reference Network Handbook, 2002, specifications  for siting (section 2.2.1) of NOAA’s new Climate Reference  Network:
Class 1 (CRN1)- Flat and horizontal ground surrounded by a  clear surface with a slope below 1/3 (<19deg). Grass/low vegetation ground  cover <10 centimeters high. Sensors located at least 100 meters from  artificial heating or reflecting surfaces, such as buildings, concrete surfaces,  and parking lots. Far from large bodies of water, except if it is representative  of the area, and then located at least 100 meters away. No shading when the sun  elevation >3 degrees.
Class 2 (CRN2) – Same as Class 1 with the  following differences. Surrounding Vegetation <25 centimeters. No artificial  heating sources within 30m. No shading for a sun elevation  >5deg.
Class 3 (CRN3) (error >=1C) – Same as Class 2, except no  artificial heating sources within 10 meters.
Class 4 (CRN4) (error >=  2C) – Artificial heating sources <10 meters.
Class 5 (CRN5) (error  >= 5C) – Temperature sensor located next to/above an artificial heating  source, such a building, roof top, parking lot, or concrete surface.”
Here is how the survey status breaks down by state. States highlighted have less than 50% coverage and are in the need of the most help from volunteers.




State
Number of Stations
Survey Report Done
Percent Reported


Alabama
15
8
53%


Arizona
26
21
81%


Arkansas
15
7
47%


California
54
54
100%


Colorado
25
17
68%


Connecticut
4
4
100%


Delaware
5
4
80%


Florida
22
21
95%


Georgia
23
20
87%


Idaho
26
17
65%


Illinois
36
13
36%


Indiana
36
33
92%


Iowa
23
13
57%


Kansas
32
27
84%


Kentucky
13
7
54%


Louisiana
18
17
94%


Maine
12
10
83%


Maryland
17
9
53%


Massachusetts
12
12
100%


Michigan
24
19
79%


Minnesota
33
30
91%


Mississippi
32
25
78%


Missouri
25
11
44%


Montana
44
27
61%


Nebraska
45
27
60%


Nevada
13
13
100%


New Hampshire
5
4
80%


New Jersey
12
8
67%


New Mexico
28
17
61%


New York
59
28
47%


North Carolina
29
26
90%


North Dakota
24
15
63%


Ohio
26
15
58%


Oklahoma
45
36
80%


Oregon
41
28
68%


Pennsylvania
24
11
46%


Rhode Island
3
3
100%


South Carolina
29
20
69%


South Dakota
24
11
46%


Tennessee
15
12
80%


Texas
48
24
50%


Utah
40
24
60%


Vermont
7
6
86%


Virginia
19
7
37%


Washington
44
35
80%


West Virginia
13
6
46%


Wisconsin
22
13
59%


Wyoming
33
26
79%



















For those that wish to help here is what you need to do:
1. Visit  www.surfacestations.org and register as a volunteer. It’s free and easy.
2. Look over the the How To Guide for surveying a station. All you need is a digital camera, and optionally a portable GPS, but it is not mandatory. A GPS that can get you to a lat/lon you enter is helpful though.
3. Find a station that is unsurveyed by using either the Google Earth KML file download above, or by looking for stations with no entries yet in the Surfacestation image gallery database 
When you decide on stations to survey, drop a comment here to make sure we don’t get duplication of effort.

4. Locate the details on station that you want to survey. The KML file has popup ballons for each station that gives details, and you can get lat/lon from doing a right click and “properties” for a station in Google Earth.Google Earth can give you driving directions. Note that lat/lon values are not alway accurate. I’ve seen them spot on, and sometimes they are as much as a 1/2 mile off., but they’ll generally get you close.
You can also visit the NCDC MMS database here:  http://mi3.ncdc.noaa.gov/mi3qry/login.cfm and use the “guest login” button. Then do a search for the station name and match up with the city and the USHCN station # ID  in the Google Earth KML file balloon. Getting that USHCN ID# right is crucial, as some towns may have 2 or three COOP stations which are not part of the USHCN network. Once you find the right station, click on the link. Be sure to note iuf it says “current” or not.
Another clue to make sure you have the right station in the NCDC database is the “station type” field which will say something like “COOP-A, COOP, LAND SURFACE, A, A” If there is no “A” in the description, then it is not a climate station.
Also check the “Location tab” in the NCDC database, which will say something like like “fire station” or “sewage treatment plant”…you maye have to look down a few entries from the top. Once you have that, some Google web searches will often help you narrow down a likely street address if the Google Earth imagery doesn’t help you visualize the location.
The “Equpiment tab” is also useful, since it will tell you what to look for. Here is a photo link that has most of the usual components of a climate station hat will help you get an idea.
5. If you determine that the station is located at a private residence, you’ll need help locating the observer. For that you need to find the observer name. Thankfully these exist on the NCDC database also, as a signature on many of the B91 forms the observers send in. To find B91 forms with observer names, go to this url:
http://www7.ncdc.noaa.gov/IPS/coop/coop.html
Then narrow down the state and station name in the web form, and click through to see what B91 forms are available, if you don’t see any within the last 6 -12 months, chances are the station is closed (a growing problem).
Download one and you may see an observer name at the lower right. A web lookup for the name and address may lead you there. Most private observers are interested and helpful. Just be sure that you advise them that you only want to get photos of their station and immediate surroundings (6 photos minimum: NSEW at about 20-30 feet, and two overall wide shots showing the station in relation to it’s surrounding) and that you are not going to reveal their names, addresses or phone numbers in any way, or any other private info.
6. Plan your trip. If you have trouble, or need help locating a station, drop a  comment here.
7. Set your camera for 3.1 megapixels (2048×1536) for best results. Or use a photo editor program later to shrink the images to that size if you use a higher resolution. High resolution is good for long distance shots, such as are sometimes required when the station is at a fenced public facility like a water plant. You can then later crop out areas of the hi-res image. It’s like having an extra zoom level. All images should be 2 megabytes or less in size for uploading.
8. Fill out the station survey form (available here ) as best you can, making notes about the station. be sure to save it as a Adobe Acrobat PDF file, which is what is need to upload into the database. A free print to PDF application is available here at www.primopdf.com should you need one.
9. Navigate to the empty folder for the station you surveyed at the Surfacestation image gallery database and click on “add a photo” or “add items” on the left menu. Don’t try to do them all at once, as you may get a time out if your connection speed is slow. Doing 4 at a time really works well.  Here at this link is what a completed survey looks like after uploading.
10. Drop us note at info { at } surfacestations dot org to let us know! Or if you need help.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e997cc081',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThat’s what one of Germany’s leading national dailies, DIE WELT,  writes here at it’s online site.Who knows! Maybe parts of the German media are beginning to see the the big block-letter writing on the wall and are now slowly taking baby-steps towards acknowledging the claims and science behind catastrophic global warming are not all what they are cracked up to be.
Maybe the far-fetched, cockamamie explanations on why all the cold is caused by warming has led the less zealous among the media to reconsider the science. Maybe all the phony predictions that keep turning out to be wrong are finally raising suspicions.
DIE WELT writes (emphasis added):
Forecasts made by many climate scientists, shortly before the year ended, prophesizing that 2010 would be the hottest on record have – once again – proven to be false.
According to CRU data, 2010 was in third place behind 1998 and 2005, and almost exactly tied with the year 2003. Certainly: The last decade was the warmest since records started being kept 130 years ago, but during the decade the warming – at least for the time being – stopped.”
 Colder winters are forecast for the future
The 4th cold winter in a row is also causing some people to wake up from their global warming trance and prompting them to ask questions. DIE WELT writes:
We are now experiencing the 4th cold winter in a row.”
And adds:
The climate scientists who are most loudly warning of global warming now say Central Europe must expect colder winters.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The reason for this, writes DIE WELT, is because of a change in the NAO, which in the 1980s and 1990s supplied Europe with mild winters with winds from the Atlantic keeping the temperature on the mild side. DIE WELT also writes that Europeans ought to get used to long cold winters because that’s pretty much what is now forecast for the years ahead.
Global warming causing cold winters – just another theory
And Die Welt seems a bit annoyed by all the changing science, and so explains that the colder winters caused by global warming, all triggered by a lack of sea ice in the Arctic, is just a theory and reminds us that:
During the especially mild winter of 2006/2007 a completely different story was told: Namely that climate change would be more noticeable in milder winter months and less so as hot summer periods and that this one particular mild season would be typical for winters in the future.”
and then adds later in their report:
The very climate scientists who are warning of global warming now say Central Europe must expect colder winters.”
Thanks, DIE WELT, for reminding readers of that. PIK; GISS and Hadley scientists would like to have everyone forget all those now embarrassing computer-model-based forecasts. It’s encouraging to see journalists start wondering about the new theories that keep popping up when things turn out differently.
Spring 2011 forecast to be cold
Finally, DIE WELT tells us that this spring in Central Europe is not going to be warm either, quoting meteorologists. Unfortunately it has been a long brutal winter for much of Europe. And with the end of February upon us, many Europeans are really getting itchy for spring and some real warming. Unfortunately, it’s going to take (quite) awhile longer, so reports DIE WELT.
But all that has changed, and this winter looks like it is about to get a long extension. Although temperatures are forecast to get above freezing by the weekend, meteorologists are forecasting that March and April will remain on the cool side.
How much cold is it going to take to make the rest of us doubt the bogus warming?
Share this...FacebookTwitter "
"

We are in the process of putting the final touches on our Public Comment concerning the Department of Energy’s use of the social cost of carbon in its recent rulemaking regulating the energy efficiency of microwave ovens. (We’ll make our Comment available as soon as we file it.) The social cost of carbon (SCC) is the government’s idea of how much future damage will be caused by each ton of carbon dioxide emitted today.   
  
Our DOE Comment focuses entirely on the new science concerning the equilibrium climate sensitivity, that is, how much the earth’s average surface temperature will increase from a doubling of the atmospheric carbon dioxide content. We argue that had the new science indicating a lower equilibrium climate sensitivity been properly incorporated into the determination of the SCC used by the DOE, it would have had a significant impact on the cost/benefit analysis used to justify the new regulation. The “benefits” of requiring lower energy consumption from microwave ovens would have been reduced.   
  
But, as we have discussed previously, the new, lower estimate of the equilibrium climate sensitivity is just one of several key variables to which the SCC is very sensitive.   
  
Another is whether or not the social cost of carbon used in U.S. government cost/benefit analyses of proposed rules and regulations should reflect an estimate of global or domestic costs. Currently, in a departure from its own guidelines, the government uses the global SCC in determining the benefits accrued by reducing domestic carbon dioxide reductions. Unsurprisingly, the global SCC is many times higher than the domestic SCC.   
  
And another variable is the “discount rate” used in the SCC calculation. The discount rate reflects how much we are willing to pay now to reduce the costs of projected carbon-related damages in the future. The lower the discount rate, the more we must pay now and thus the higher the current SCC seems to be. In another departure from its own guidelines, the government’s calculation uses an especially low discount rate, resulting in a high SCC and thus more “benefits” from regulations reducing carbon emissions.   




There was an insightful column in the _New York Times_ earlier this week by Eduardo Porter that is one of the clearest explanations we have read on the effects and rationale of the choice of the discount rate when determining the social cost of carbon. He draws a distinction between what he terms a “moralist” approach (which prefers a lower discount rate) and a “business” approach (which requires a higher discount rate):   




The discrepancy between the estimates of the value of climate damage stem from radically different views on how much weight the people of the present should give to damages caused by the climate in the distant future.   
  
The estimate of [the SCC of] $65 a ton is inspired by a moral stance: if warming will impose a cost of 1 percent of the world’s income in the future, we should spend about 1 percent of our income to prevent it—or perhaps somewhat less to account for the trend that people 100 years from now are likely to be much richer than people today.   
  
By contrast, $13.50 a ton comes from the business world. Essentially, it requires that spending to prevent climate change should yield at least the same rate of return, in terms of reduced damages from warming, as any other capital investment.   
  
The two outlooks lead to entirely different decisions. The government’s rendition of the moral approach implies that it is worth making every investment to reduce carbon emissions that has a rate of return of at least 2.5 percent, in terms of avoided damages. Business logic suggests that no investment should be made if the return—after taxes—is less than 5 percent.



Ultimately, Porter thinks that the business logic will win the day. He astutely points out:   




Multiple challenges compete for the world’s resources, from economic development and ending poverty to eradicating AIDS and malaria. The climate is not the world’s only priority. Even if we were to agree that improving the well-being of future generations is worth an enormous investment, there might be better things to invest in than reducing greenhouse gas emissions.



We are happy to see more critical attention being paid to the social cost of carbon, as it represents a huge, but little-known back door that is silently swinging open for costly government regulation of carbon emissions. Economics and science argue that door should be barred and locked.


"
"
From NOAA News, Susan Solomon predicts the future with certainty. In other news, on the same day Caterpillar, Sprint, Texas Instruments, and Home Depot announce massive layoff plans to the tune of 50,000 people,  unemployed climate modelers get a government bailout today courtesy of our new president to the tune of 140 million dollars. That should be just enough to pay the electric power bill for the new supercomputer I’m sure NOAA will just “have to have” now to keep up with the new toy for the Brits at Hadley. (h/t to Ed Scott for the NOAA pr)
New Study Shows Climate Change Largely Irreversible
January 26, 2009
A new scientific study led by the National Oceanic and Atmospheric Administration reaches a powerful conclusion about the climate change caused by future increases of carbon dioxide:  to a large extent, there’s no going back.
The pioneering study, led by NOAA senior scientist Susan Solomon, shows how changes in surface temperature, rainfall, and sea level are largely irreversible for more than 1,000 years after carbon dioxide (CO2) emissions are completely stopped. The findings appear during the week of January 26 in the Proceedings of the National Academy of Sciences.
“Our study convinced us that current choices regarding carbon dioxide emissions will have legacies that will irreversibly change the planet,” said Solomon, who is based at NOAA’s  Earth System Research Laboratory in Boulder, Colo.
“It has long been known that some of the carbon dioxide emitted by human activities stays in the atmosphere for thousands of years,” Solomon said. “But the new study advances the understanding of how this affects the climate system.”
The study examines the consequences of  allowing CO2 to build up to several different peak levels beyond present-day concentrations of 385 parts per million and then completely halting the emissions after the peak. The authors found that the scientific evidence is strong enough to quantify some irreversible climate impacts, including rainfall changes in certain key regions, and global sea level rise. 
If CO2 is allowed to peak at 450-600 parts per million, the results would include persistent decreases in dry-season rainfall that are comparable to the 1930s North American Dust Bowl in zones including southern Europe, northern Africa, southwestern North America, southern Africa and western Australia.
The study notes that decreases in rainfall that last not just for a few decades but over centuries are expected to have a range of impacts that differ by region. Such regional impacts include decreasing human water supplies, increased fire frequency, ecosystem change and expanded deserts. Dry-season wheat and maize agriculture in regions of rain-fed farming, such as Africa, would also be affected.
Climate impacts were less severe at lower peak levels. But at all levels added carbon dioxide and its climate effects linger because of the ocean.
“In the long run, both carbon dioxide loss and heat transfer depend on the same physics of deep-ocean mixing. The two work against each other to keep temperatures almost constant for more than a thousand years, and that makes carbon dioxide unique among the major climate gases,” said Solomon.
The scientists emphasize that  increases in CO2 that occur in this century “lock in” sea level rise that would slowly follow in the next 1,000 years. Considering just the expansion of warming ocean waters—without melting glaciers and polar ice sheets—the authors find that the irreversible global average sea level rise by the year 3000 would be at least 1.3–3.2 feet (0.4–1.0 meter) if CO2 peaks at 600 parts per million, and double that amount  if CO2 peaks at 1,000 parts per million.
“Additional contributions to sea level rise from the melting of glaciers and polar ice sheets are too uncertain to quantify in the same way,” said Solomon. “They could be even larger but we just don’t have the same level of knowledge about those terms. We presented the minimum sea level rise that we can expect from well-understood physics, and we were surprised that it was so large.”
Rising sea levels would cause “…irreversible commitments to future changes in the geography of the Earth, since many coastal and island features would ultimately become submerged,” the authors write.
Geoengineering to remove carbon dioxide from the atmosphere was not considered in the study. “Ideas about taking the carbon dioxide away after the world puts it in have been proposed, but right now those are very speculative,” said Solomon.
The authors relied on measurements as well as many different models to support the understanding of their results. They focused on drying of particular regions and on thermal expansion of the ocean because observations suggest that humans are contributing to changes that have already been measured.
Besides Solomon, the study’s authors are Gian-Kasper Plattner and Reto Knutti of ETH Zurich, Switzerland, and Pierre Friedlingstein of Institut Pierre Simon Laplace, Gif-Sur-Yvette, France.
NOAA understands and predicts changes in the Earth’s environment, from the depths of the ocean to the surface of the sun, and conserves and manages our coastal and marine resources.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e994d102c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Hard lesson about  solar realities for NOAA / NASA
Reposted here: October 30th, 2008
by Warwick Hughes

The real world sunspot data remaining quiet month after month are mocking the  curved red predictions of NOAA  and about to slide underneath. Time for a rethink I reckon NOAA !!
Here  is my clearer chart showing the misfit between NOAA / NASA prediction and  real-world data.

Regular  readers might remember that we started posting articles drawing attention to  contrasting predictions for Solar Cycle 24, way back on 16 December 2006. Scroll to the start  of my solar threads.
Then in March 2007 I posted David Archibald’s pdf article, “The Past and  Future of Climate”. Well worth another read now, I would like to see another  version of David’s Fig 12 showing where we are now in the transition from Cycle  23 to Cycle 24.
Solar Cycle 24  Prediction Issued April 2007 from NOAA / NASA
NOTE from Anthony: We now appear to have a new cycle 24 spot, which you can see here:

See the most current MDI and magnetogram here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b7a42a2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Mike Ronanye writes:
SWPC has just made a change in their solar cycle predictions in the middle of the month without any preannouncement. Both Sunspot and F10.7cm predictions were altered significantly. 



See the following links:
http://www.swpc.noaa.gov/SolarCycle/
The off-cycle update is in this week’s PDF report which contains the altered graphics:
http://www.swpc.noaa.gov/weekly/pdf/prf1747.pdf
You can see the last monthly summary here which I have been complaining reporting about, here:
http://www.swpc.noaa.gov/weekly/pdf/prf1745.pdf
This should have been the January 2009 summary but SWPC recycled the December 2008 summary.
I looked for but was unable to find any press releases. Please search for any additional information and post it here. If you downloaded any SWPC data or graphics hold on to it. I will be updating my SWPC Sunspot animation.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9859b2b9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterEd Caryl likes to research, and a short time ago wrote an essay about phytoplankton, see here. Some readers pointed out a flaw, and so Ed has insisted on posting a correction – as is appropriate in science. Happens to the best of us. (We’re not Penn State or CRU here).
===========================================================================
The Phytoplankton are not Starving
By Ed Caryl
In a comment to the original article, The Phytoplankton are Starving, R. de Hann made the following comment:
 So I have big trouble accepting the loss of plankton for fact.
In regard to the claim of over-fishing and fishing methods (by other reports), which is a serious problem in several places, we see the gap of lost volume filled up with other species very quickly.
We know for example that tuna eat jellyfish and in those waters where the tuna numbers have been reduced the numbers of jellyfish have exploded, compensating for the “loss of mass”.
So one species is quickly replaced by another.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Willis Eschenbach made this comment:
Humans catch and remove about 70 million tonnes of mostly big fish from the ocean annually. But the amount of mass at each trophic level reduces by about 90%. Tuna are four trophic levels up from phytoplankton. This means that for every kilo (pound) of fish that we eat, there are about 10,000 kilos (pounds) of phytoplankton that are supporting that fish.
That means that we are removing something on the order of one ten-thousandth (0.01%) of the nutrients that the phytoplankton that fed those tuna depend on …
However, that’s not all. Much of the phytoplankton goes to feed things that are generally not eaten by humans. As a result, the reduction in phytoplankton nutrients is even smaller than 0.01%. Much of it is never seen by humans in any form.
As a result, your hypothesis (reduction in plankton results from human usage of the required nutrients) fails by a number of orders of magnitude. We simply don’t remove enough nutrients to make a difference.
These comments forced me to re-think my position. I went back to the literature.
The total autotrophic biomass production in the oceans is about 48 billion tons carbon per year. We are currently harvesting about 80 million tons of fish and shellfish per year. The harvest has averaged over 70 million tons per year for the last 30 years, and over 40 million tons per year for the 30 years before that. Over the last 60 years we have harvested about 3.5 billion tons. The total biomass production in the ocean is circulating production. The fish we pull out removes that mass from circulation. So the reduction in phytoplankton nutrients is in fact small. Willis is correct; this doesn’t explain the 40% reduction in phytoplankton. As Willis correctly states, the food web in the oceans has several trophic levels, each about 10% of the level below it.
So thanks to R. de Hann and Willis Eschenbach for their comments.
Share this...FacebookTwitter "
"

What a wonderful year, 1998! Global temperatures reached their highest value recorded in all three available records — surface, satellite, and weather balloon. Sayers of doom had pronounced dire and immediate consequences — so for once it was possible to check their models of misery against what actually happened when it really got warm. 



**El Niño vs. Greenhouse Warming**



Judging from the conflation of El Niño and human‐​induced global warming, you might think the two were one and the same, or maybe even, as Vice President Al Gore intimated, that one caused the other. 



Like many of his reaches, there was a bit of truth in the stretch, but only a bit. Global warming didn’t cause El Niño in any appreciable sense, but the two were related: It was a very good El Niño year, and it was a very, very warm year. 



El Niño is natural. Just because scientists discover something, or because we, as taxpayers, shell out tens of millions of dollars to research something, does not mean that something new has happened. Chemicals existed before chemists, DNA existed before its discovery won a Nobel Prize, and El Niño ebbed and flowed long before the first climatologist was born. 





No one knows why the trades suddenly slow or even reverse, piling warm water up against South America. Reid Bryson, the modern founder of the very true notion that climate does change in ways that are important to people, believes this reversal is mainly an effect of some other large‐​scale physical fluctuation. During an El Niño, a large portion of the tropical Pacific is much warmer than average — as much as 8oC (14.4oF) — and this heat eventually disperses through the atmosphere. 



Heat is energy, and an El Niño shows up both as warming and as motion. Its reach extends into the tropical Atlantic, where it suppresses — yes, suppresses — hurricanes. Rain, often absent for years, falls in the ultradeserts of Peru and Argentina. And the global temperature warms. 



When the trade winds return, the cold upwelling reappears. This is La Niña. It stands to reason that the more the cold water is suppressed, the greater the amount that eventually bubbles up, so a big El Niño warm spike may mean a big temperature fall in the months thereafter. 





We’re totally confident that 1998’s big warming spike was a result of El Niño, and not dreaded “global warming” — that is, a human product. We know because the stratosphere tells us so. 



The human version of global warming is caused by increasing amounts of “greenhouse” gases in the lower atmosphere. These compounds absorb the heating radiation that results from the sun’s warming of the earth’s surface, and reemit that radiation either downward, resulting in additional atmospheric warming, or out to space. If these compounds weren’t there, the radiation would pass directly outward. 





So what we should see from the increasing greenhouse effect is a lowering trend for stratospheric temperature. And El Niño should temporarily stop that trend, at least for a year or so. Figure 3 shows that 1998 was indeed one of the warmest years in the stratosphere in the last two decades and is testimony to El Niño as the cause. 



**Greenorama**



We were besieged with news reports about how El Niño (and, by not‐​so‐​subtle extension, global warming) would cause terrible agricultural disasters. Who can forget the miles of CNN footage showing tractors mired in the Georgia mud, or network reels of browned corn in Texas? 



Well, some folks did poorly, and some folks did well. That happens every year. About the best way we know of to settle the overall score is in the wheat, corn, and soybean markets. When there’s a big supply, the price goes down. Demand fluctuates some too, but a perpetually increasing population has a way of ensuring more mouths to feed. 



By late 1998, the price of U.S. wheat stood, after adjusting for inflation, at its lowest level since reliable records began in 1915. Fluctuations in America’s massive supply of agricultural products, more than anything, dictate the global price. 





Many agricultural economists and a few climatologists have made careers of studying the influence of global weather patterns on crop yields. Moisture at planting time and in the winter before harvest is the major determinant — by far — of winter wheat yield. Winter wheat is planted in the early fall, requires moisture to germinate, and then, when spring springs, is really poised to take advantage of wet soil. In addition, yields are positively influenced by above‐​normal winter temperatures. 



Undoubtedly, the climate of 1998 led to the record yields. But there was another factor as well: Increased carbon dioxide in the air increases yields and makes crops much more efficient in their use of moisture. As Sylvan Wittwer, former head of the Board on Agriculture of the U.S. National Research Council wrote,“Overall, it has been conservatively estimated that global crop productivity has risen by approximately 2.5 to 10 percent, and possibly as high as 14 percent from the current increase in atmospheric CO2 over pre‐​industrial levels.” 



**We’ve Seen Fire and We’ve Seen Rain**



Two other prominent newsmakers this year included the spate of overland fires in Florida during the early summer, and Mitch, a real son‐​of‐​a‐​gun of a flood, but really not much of a hurricane by the time it hit land. 





That didn’t stop everyone from blaming all this on El Niño and global warming, but the fact is that, in general, there is no relationship between summer dryness in Florida and the existence of an El Niño during the previous winter. That’s because El Niño makes it rain during the winter greening season. In the summer, there’s precious little documentation that El Niño does anything at all to Florida weather. Of course, we could blame Florida’s high temperatures this year on global warming, but that would mean ignoring the fact that changing the greenhouse effect warms up the coldest air masses a lot more than it heats up the warmer tropical ones. 





That was inflammatory nonsense. A 1974 hurricane named Fifi, which was also a Category 2 at landfall, took much the same track and killed 7,500. Janet and Edith had much more powerful winds and wreaked tremendous havoc. Perhaps the most interesting comparative aspect with Mitch is that a tropical storm named Claudette, in 1979, also produced 50 inches of rain and resulted in nine deaths (that’s about 9,990 fewer than Mitch caused) when it hit Texas. Perhaps infrastructure and poverty, not global warming, created the tragedy named Mitch. Maybe, just maybe, allowing us to save our money for investment in developing nations like Honduras and El Salvador is a better idea than taking it away in an attempt to stop something that would happen anyway. 



El Niño and hurricanes do share at least one common trait. They have been around for a long time and the biota of the world, thanks to the nature of evolution, likely take advantage of them. In California, rains of the magnitude that associate with El Niño are required to make the desert bloom. Just any old storm isn’t enough, even though the ground gets wet. In that environment, many seeds have to be scarred by the motion of overland movement of water before they’ll even germinate. 





Long before banging the climate‐​disaster gong became the key to career advancement, federal climatologist George Cry calculated the percentage of normal rainfall that comes from all tropical cyclones, including tropical depressions, storms, and hurricanes in the eastern United States. Figure 7 shows the result for September. 



In most of the areas with high values, American agriculture has adopted a double‐​crop system that plants one early, fast‐​maturing crop, and then replaces it with an October harvest crop, mainly soybeans. Late August and September rain can be very important determinants of final yield. It’s pretty clear that years in which amounts are below normal because of lack of tropical cyclones are those in which yields are in jeopardy. 



People adapt to their climatic environment. The biota of the world take advantage of change, and so does our agriculture: One of the biggest El Niños in recent centuries produced a glut of food. That’s the lesson of 1998. 



But the climate hype of 1998 also has portents. If this past year is any guide, when global warming becomes a major focus of the Y2K presidential campaign, the amount of distortion, exaggeration, scare stories and fear‐​mongering we’re sure to witness will be a real climate disaster. 



**References:**



Cry, G.W., 1967. Effects of tropical cyclone rainfall on the distribution of precipitation over the eastern and southern United States. ESSA Professional Paper, 1, U.S. Dept. of Commerce. Washington, D.C. 



Michaels, P.J., 1979. Atmospheric anomalies and crop yields in North America. Ph.D. Dissertation. University of Wisconsin. 



Wittwer, S., 1995. Food Climate and Carbon Dioxide. CRC Press, Boca Raton, Fla., pp. 236.
"
"

Opinion studies indicate that many Americans are terrified of flying. But there’s good news from the Federal Aviation Administration for those nervous Nellies. Last year there were 14 million commercial airline flights carrying 615 million passengers. How many deaths? A big fat zero. The chances of dying by electrocuting yourself in the bathtub, being crushed to death by the airbag in your car or being hit on the head by a meteorite were about the same as by taking a ride on an airplane. 



America has become a nation of worrywarts. We worry about everything from global warming to the amount of fat in our breakfast cereal to the radiation emanating from our computers. A recent poll featured in USA Today confirms that we are concerned about the strangest things. More than half of Americans say that successful cloning is one of their biggest dreads for the 21st century. What is completely inexplicable is that 55 percent of Americans cited “technology” as one of their worries. How’s that for unfounded paranoia? Technology saves lives, it doesn’t cost them. 



Maybe our greatest risk in the modern age is that of worrying ourselves to death. The risks we now face in our daily lives are minuscule compared with the risks of earlier times. In fact, the world is a much safer place today than it was at any time in history. Many of the things we fret about today, such as flying on a commercial airline, are not dangerous at all. If you don’t believe me, consider the following statistics: 



  
  




  
  




  
  




What else do you worry about? War? Floods and tornadoes? AIDS? Nuclear accidents? Acts of terrorism? Sudden Infant Death Syndrome? Contaminated air and water? Heart attack? Believe it or not, the death rate from every single one of those menaces is down–in most cases way down. In the last 20 years modern medicine has made giant strides in treating and preventing heart disease. It remains a major killer only because, as life expectancies increase, we become more prone to this degenerative disease. 



Worrywarts have shifted their phobias to obscure and distant threats posed by things like global climate change, alien invasions, cloning and secondhand smoke. Most of the threats that were really serious and frightening earlier in this century are no longer problems at all. My parents often told gruesome tales of growing up in the 1930s when polio was still a dreaded killer. Everyone knew someone who had been paralyzed and confined to an iron lung or a wheelchair. The Centers for Disease Control tells us that there was not a single reported case of polio in the United States in 1997. 



One of the leading causes of death in America today is poverty. The best way to continue to reduce the risk of injury and early death is to promote economic growth and rising living standards through free‐​market capitalism. Paradoxically, many of the safety, health and environmental regulations that come out of the Environmental Protection Agency and the Occupational Safety and Health Administration do not reduce risk. They are so cost‐​ineffective that they make us poorer and thereby put people at greater risk of death than if we had never issued those regulations at all. Smart regulations save lives. Dumb regulations cost lives. 



Now, if after reading this you are still nervous about the sky falling and the world (or your own world) coming to an end, my advice is to board an airplane. These days, that’s probably the safest place you can be.
"
"
A Guest Post By Steve Goddard
In my most recent article in The Register, and also posted here on WUWT, I incorrectly speculated  that NSIDC graphs appeared to show less growth in Arctic ice  extent than had actually occurred.  My calculations were based on counting ice  pixels from Cryosphere Today maps.  Since then, I have had further discussions  with Dr. Walt Meier at NSIDC and William Chapman at Cryosphere Today, to try to  understand the source of the problem.   Dr. Meier has confirmed that counting  pixels provides a “good rough estimate” and that NSIDC teaches pixel counting to  CU students as a way to estimate ice extent.  William Chapman has confirmed that  the projection used in CT maps is very close to what it appears and to what I  had assumed it to be.  It is an astronaut’s view from about 5,000 miles above  the north pole.
What I have learned
In 2008, CT and NSIDC maps show excellent agreement – as can be seen  in this video which overlays an  August 14 NSIDC map on top of the August 14 CT map.  The borders of ice extent  are nearly identical in the two maps. (The videos show overlays of the two maps.)

The discrepancy occurred in August, 2007, when agreement between  NSIDC and CT was not so good.  The equivalent video from August 15, 2007  shows that the CT map was missing a significant amount of low concentration ice  on the Canada/Alaska side.  I have since confirmed from AMSR maps and NASA  satellite photos that the NSIDC map is probably more accurate than the CT  map.
The reason that CT provides their side-by-side image viewer is apparently to encourage visitors to make a visual comparison of two dates,  which is exactly what myself and others here did when we observed the  discrepancy vs. NSIDC graphs.  The human brain is quite good at making estimates  of relative areas from images, and pixel counting is nothing more than a way to  quantify what has already been observed.  Since writing The Register piece I  have made adjustments to the CT pixel counts for map distortion, and as I  expected that makes the discrepancy slightly larger.

Because CT maps showed less ice in 2007, the increase in 2008 ice extent appears to be much greater. There is little doubt now  that the NSIDC reported ice growth is absolutely correct.  But  wasn’t the ice supposed to shrink this year due to an excess of  “thin first-year ice?”  In May, NSIDC’s mean forecast (based on previous year’s melt) was that Arctic ice extent would be 13% lower  than last year.  (NSIDC has more recently posted on their web site some reasons  why they believe the May estimates didn’t work out.)

Click for a larger image
The next graph shows the NSIDC May  forecast superimposed on the AMSR graph of what has actually happened this year.  Ice extent has increased rather than decreased, and is not  tremendously lower than most other years this decade.   Note that the AMSR and  NSIDC graphs bottom out at 2Mkm2, not zero, which creates the visual impression  that ice extent is lower than it actually is.

Click for a larger image
Looking at the NSIDC map below, several things are apparent.
Firstly, the widely  publicised news stories predicting a possible “ice free  Arctic,” an “ice free North  Pole,” and a record  low extent aren’t likely to happen. (See this WUWT post with a quote from Dr. Meier) The North Pole is nearly as far away  from ice free water as in any other year.  In order to get to ice free water  from the pole, one would have to travel over 500 miles of ice to near Svalbard.   Lewis Pugh’s kayak trip, as reported by the BBC August 30 – “Swimmer aims to kayak to N  Pole” is going to be an impossible one.  That critical error didn’t stop the  BBC from highlighting it on the front page of their web site this weekend.
Secondly, while Arctic ice is well below the 30 year mean,  it is above expectations and nowhere near gone.  NSIDC graphs show Arctic ice  extent at greater than 70% of normal – hardly a six sigma event.  As of August  1, NSIDC was even considering a possible return to normal extent this summer.  If not for a few weeks of stormy Siberian  weather, the map (and story) would be quite different today.

There has been lots of press publicity  for predictions of an ice free Arctic by  the year 2013 or within the next  ten years.  Looking at the CT ice area graph below, that would clearly  require some major non-linear changes to the 30 year trend.

Click for a larger image
So how do we know if the trend is linear,  exponential or sinuous? As seen below, many long-term GISS Arctic temperature  records (except for stations close to the Bering Strait) show the last 30 years  as being the warming leg of a possibly cyclical pattern – with current  temperatures no warmer than 70 years ago.  Most stations close to the Bering  Strait show a sharp upwards shift of several degrees (corresponding to the PDO  shift) in 1976, and relatively flat temperatures since.  University of Alaska data shows that on average, state temperatures have been nearly flat since 1977.  In  fact, parts of Alaska are coming off one of their coldest  summers on record – possibly corresponding to another shift of the PDO to  pre-1976 conditions.
Is it possible that the 30 year  satellite record coincidentally represents only one leg of a waveform?   Greenland temperature records would hint at that.  If you examine only one leg  of a waveform, you will absolutely come to the wrong conclusion about the long  term behaviour – just as some did during the 1970s ice age panic.

Click for a larger image – original source image from NASA GISS

We know from official US Weather Bureau  records that it was possible to sail as far as 81N latitude in ice-free water, during a  similar warm period in 1922.  That is about as close to the pole as you can sail  now.  Temperatures around Spitzbergen, Norway warmed a remarkable 12C during a  few years prior to 1922.  From the November, 1922 Weather Bureau report – “He says that he first noted wanner conditions in 1915, that since that  time it has steadily gotten warmer, and that to-day the Arctic of  that region is not recognizable as the same region of 1865 to 1917.“



Is there cause for concern?   Perhaps.  But unfortunately much of the press coverage has been little more than  science fiction so far.  How do we separate the science from the fiction?  Dr.  Meier has graciously agreed to answer that question (and others) in my next  article.
One thing we can state with a degree of  certainty, is that there likely will be more multi-year ice in 2009 than there  was in 2008.  This is  because the 2008 melt season is ending with more ice area  than 2007.  Barring asteroid impact, after the winter freeze there will be (by  definition) more multi-year ice than what we started with this year.  Any ice  which survives the summer will be classified as multi-year ice in 2009.  If next  year is cool like this year, is it unreasonable to hypothesise that ice extent  will again increase?  Or are we on a non-linear trend which will lead to  ice-free summers and a collapsing Greenland ice sheet?  Hopefully Dr. Meier can  help sort this out for us.







			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cc3e7b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

It seems that ever since 1776, European elites have been grousing about the symbols of American culture that are “forced” upon them. First it was the ideas of our revolution, such as liberty and democracy. Today it’s Hollywood movies, EuroDisney and — horror! — McDonald’s hamburgers. So perhaps it’s no surprise that some European Union politicians seem positively gleeful at the prospect of exporting to the United States that great staple of European life: ubiquitous taxation. 



At issue is who will collect the value‐​added tax (VAT) when a European consumer downloads a digital product or service from a U.S.-based company. The VAT — which can add as much as 25 percent to the cost of a product — is usually charged at the point of entry on “tangible” products shipped from the United States. But since products such as software and music can be delivered directly over the Internet, there aren’t any packages for EU tax collectors to inspect at the border. Thus, when French President Jacques Chirac downloads a game called, say, “Jerry Lewis’ Championship Boxing,” he’s responsible for paying the VAT himself. There’s no practical way to track Chirac’s purchase, however, so the odds of his paying the tax voluntarily are essentially zero. 



Ever alert to the scourge of untaxed commerce, EU politicians have hit upon a solution to their problem: have the Americans harvest the loot! 



That beggar‐​thy‐​neighbor approach to tax collection has been endorsed by the European Commission, which proposed on June 7 that U.S. companies be required to collect a VAT on all sales of digital products to Europe. If the proposal becomes law — which could happen later this year — all U.S. business will have to register with the tax agency of at least one EU member country, ascertain the location and tax status of each and every customer, transmit tax payments electronically to the relevant tax authority, and submit to audits and “due diligence examinations” to make sure no one is cheating. In return for acting as Europe’s tax collectors, American businesses would receive half‐​price coupons for EuroDisney (just kidding, they wouldn’t get anything). 



But as the history‐​minded among us have noted, America once fought a war to escape burdensome taxes imposed from across the Atlantic. In the days before the Internet, America’s political leaders uniformly denounced “taxation without representation” as an intolerable evil. 



Even given today’s less‐​principled political climate, it’s difficult to see what Washington has to gain by going along with Europe’s scheme. Under the current rules, European consumers have an incentive to shop VAT‐​free from U.S. companies, which also makes the United States an attractive market for e‐​commerce investors. European companies rightly complain about that disparity, but too bad: let Europe solve its own tax problems. If political leaders there weren’t reflexively opposed to tax cuts, they could simply exempt digital products and services from the VAT. 



Enforcement will be difficult without Washington’s help, but Europe’s tax collectors are determined to try. U.S. businesses with a branch office in Europe, for example, could probably be forced to comply. Another option being considered is “blacking out” the Web sites companies that refuse to register for VAT collection. But consider that despite having highly centralized systems of Internet service provision, authoritarian governments (such as China) have been unable to control access to dissident Web sites. The Internet is simply too massive and decentralized to police effectively. 



That enforcement problem is why Europe is calling for increased “international collaboration” on tax collection, meaning that the IRS would monitor U.S. companies’ compliance with EU tax law. And there’s no guarantee that Congress won’t play along: the digital‐​VAT already has allies among U.S. state and local politicians who would love to see broader taxation of the Internet. After all, if American businesses can be compelled to collect taxes on their sales to foreign consumers, surely those businesses can be told to collect taxes on sales of products destined for other states? Two wrongs, the politicians hope, will in this case make a right. 



Ordinary Americans, as they pause this summer to consider the reasons our ancestors took up arms against the British 224 years ago, aren’t going to buy it, and Congress should take heed. “Taxation without representation” remains an intolerable evil that neither time nor technology has diminished.
"
"
When we last checked in to the Nansen Sea Ice Graphs, it looked like they were heading towards the “normal” line in a hurry. Ice area seems to still be on that trend, while extent seems to be leveling off it’s growth rate. Area appears to be within about 200,000 square kilometers of the 1979-2007 monthly average and still climbing.
Sea Ice Area - red line is current value, shaded area represents 1 standard deviation
Of course the fact that the 2007 data is included in the average line, means the average is a lower than usual target than one might expect. If we compare to ice area over at Cryopshere today, they use a 1979-2000 mean, which is higher.  Still the rebound we are seeing is impressive.
Sea ice extent looks like this:
Sea Ice Extent - red is current value, shaded area is 1 standard deviation
These graphs will automatically update, so check back often.
For those of you wondering, here is the difference between area and extent, as described in the NSIDC FAQ’s page:
What is the difference between sea ice area and extent? Why does NSIDC use extent measurements?
Area and extent are different measures and give scientists slightly different information. Some organizations, including Cryosphere Today, report ice area; NSIDC primarily reports ice extent. Extent is always a larger number than area, and there are pros and cons associated with each method.
A simplified way to think of extent versus area is to imagine a slice of swiss cheese. Extent would be a measure of the edges of the slice of cheese and all of the space inside it. Area would be the measure of where there’s cheese only, not including the holes. That’s why if you compare extent and area in the same time period, extent is always bigger. A more precise explanation of extent versus area gets more complicated.
Extent defines a region as “ice-covered” or “not ice-covered.” For each satellite data cell, the cell is said to either have ice or to have no ice, based on a threshold. The most common threshold (and the one NSIDC uses) is 15 percent, meaning that if the data cell has greater than 15 percent ice concentration, the cell is considered ice covered; less than that and it is said to be ice free. Example: Let’s say you have three 25 kilometer (km) x 25 km (16 miles x 16 miles) grid cells covered by 16% ice, 2% ice, and 90% ice. Two of the three cells would be considered “ice covered,” or 100% ice. Multiply the grid cell area by 100% sea ice and you would get a total extent of 1,250 square km (482 square miles).
Area takes the percentages of sea ice within data cells and adds them up to report how much of the Arctic is covered by ice; area typically uses a threshold of 15%. So in the same example, with three 25 km x 25 km (16 miles x 16 miles) grid cells of 16% ice, 2% ice, and 90% ice, multiply the grid cell area by the percent of sea ice and add it up. You’d have a total area of 675 square km (261 square miles).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b42a7d5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The lead story on the June 29 MSNBC News was that there were terrible floods in the United States and–interspersed in the middle of the story–that global warming is going to be even worse than we thought. This was one‐​sided, emotional science reporting even worse than I thought possible. 



The story was rooted in a recent study by Tom Wigley, introduced as “a respected climatologist.” Wigley’s study was financed by the Pew Foundation, which is running a multi‐​million‐​dollar campaign to hype global warming. 



Wigley says that sulfate aerosols will be legislated out of existence faster than previously thought. He champions the theory that sulfates reflect away the sun’s radiation, conveniently explaining why the planet has warmed so little despite the claims of warming doomsayers and their computers. 



Without sulfate aerosols, computer models indicate our hemisphere should have already warmed about 2.3 degrees Celsius as a result of the greenhouse effect. The observed warming this century is a scant 0.65 degrees. If the sulfate hypothesis fails, the argument devolves into what the “skeptics” have said for decades: the earth simply isn’t going to warm all that much. 



Having held a doctorate in climatology for two decades, I feel confident in saying that every one of my colleagues who has expressed an opinion to me dislikes Wigley, mainly because he seems arrogantly dismissive of some facts when they get in the way of his theories. He actively discourages the airing of points of view that conflict with his. 



In October 1994, at a global warming meeting called by Rep. John Dingell (D‐​Mich.), Wigley was confronted with the reality that satellites had found no warming. He merely waved his hands and said, “Oh come now, that’s just the satellite data.” Oh come now, Tom, it’s just the only global measure of temperature that exists! 



The sulfate aerosol theory is politically correct, because some explanation is needed for why the early climate models flopped so badly. They served as the basis for the United Nations climate treaty, recently modified in Kyoto to force the United States to reduce its greenhouse gas emissions by as much as 45 percent in 8.5 years. Even though the treaty would devastate the U.S. economy, Wigley thinks it is not enough, saying we need “nine more Kyotos.” 



Needless to say, Tom is very big with the folks like the Pew Foundation and the United Nations, both of which seem to care not a whit about the destruction of the American economic miracle as long as dreaded global warming is banished. 



He co‐​authored a famous 1996 paper that showed that, from 1963 through 1987, the behavior of the atmosphere did in fact increasingly resemble that of one in which greenhouse warming was being counteracted by sulfate cooling. But the data available for his study actually began in 1957 and ended in 1995. When all of the data on the critical region of the atmosphere was looked at, there was no change whatsoever! Wigley has never given a satisfactory explanation of why he ignored all the data. Should he write a response to this paper, I reserve the right to reply. 



Wigley does not like to be confronted with this in public. Resources for the Future, a Washington outfit big in the global warming game, recently held a forum featuring Wigley but none of his critics. When pressed, Wigley said that his critics did not belong on the stage with him because they generally did not publish their work in peer‐​reviewed literature. Hogwash. Wigley’s critics are among the most published in the business. 



Arizona State’s Robert Balling may be the most prolific living climatologist. Only a handful of papers have mathematically searched for predicted greenhouse signals–and several are mine. The five most prominent critics have published nearly a thousand articles, mostly in peer‐​reviewed journals. Critics include a member of the National Academy of Sciences, the former head of the American Physical Society, heads of major research laboratories and former presidents of other scientific and professional societies. The fact that they even appear in the literature at all, given the thousands of people who lose some of the $2.1 billion that we spend each year on global change research if it all goes kerblooey, is testimony to the cogency of their arguments. Maybe that’s why Wigley doesn’t want any opposition. 



Sulfates don’t do a good job of explaining the failure of the models noted above. NASA scientist James Hansen, who essentially ignited the greenhouse issue 11 years ago with his flamboyant congressional testimony, has become very skeptical about sulfates. University of Washington scientist Peter Hobbs found that sulfates off the East Coast are overwhelmed in their own plume by black carbon particles that absorb radiation and cancel sulfate cooling. And throughout the eastern United States, where sulfates have been in decline for the last 30 years, the temperature hasn’t budged during the entire century. 



Any or all of these observations could have been offered by “respected climatologists” if MSNBC had bothered to do a little legwork. The real “story behind the story” is why they didn’t.
"
"**Back to the future. Next week England will return to the tier system, where the country is carved up into different categories.**
Areas where there are higher levels of coronavirus will be under tighter restrictions. Parts of the country where it is less prevalent will have looser limits.
It is the same model that didn't do enough to slow the winter surge of the disease last time. But second time round, it's different in some important ways.
First off, the strictest regime, tier three, will overall be tougher than the last time round.
Swathes of the country are still going to stuck under a pretty restrictive system. (You can read all about exactly how the tiers will work here.)
Second, gyms and businesses that carry out personal care like hairdressers will stay open everywhere. Stricter yes, but way short of a rerun of the spring lockdown.
Third, the decision on which set of instructions a region will have to follow will be taken by the government and its scientists centrally.
For all of the warm words about the importance of local leaders, and local authorities, there is no desire in central government to repeat the very messy political process of trying to get regional buy-in when decisions are being made about moving in and out of the different categories.
That might be a more straightforward way of making the choices. But it could make it harder to get local support for measures being taken.
Although it's worth noting the chancellor has already changed his mind about extending national financial support which was such a bone of contention last time.
And in theory, areas could move more quickly in and out of the different tiers too - with possible switches every fortnight.
First time round the government struggled to communicate how the system would work cleanly, but this is going to be the way things are run until March so ministers want to get it right.
But the political test of how bumpy it could be won't come until Thursday when ministers reveal the answer to the question every MP and member of the public wants to know - what will the rules be in the place they live.
The Tory backbenches and the opposition this time are more demanding of the government in terms of explaining its decision-making.
But the extent of the potential push back will depend to a large extent on just how much of the country is asked to keep coping largely behind closed doors.
Downing Street has repeatedly made it clear that more of England will be in the toughest tier than in the first phase of this system, but won't be drawn yet beyond that.
The prime minister's wi-fi might have held up through his press conference on Monday, unlike the House of Commons earlier (although the PM protested the problems were on the Commons' end) but it's not clear yet how his arguments for the revised tier system will hold up too.
And don't forget the relative success of the new system depends in large part on all of us - how the public responds to the new system of rules and regulations, and how patient we are all willing to be.
P.s. Remember all four parts of the UK are following different approaches right now. Northern Ireland is about to tighten up further. Wales has not long left its circuit break lockdown, and Scotland has a new system of five tiers.
That patchwork is partly why it is 'possible, but not definite', according to government insiders tonight that the four parts of the country will agree how to manage Christmas on Tuesday.
The four administrations have been trying to figure out a way of allowing a bit more flexibility around families spending time with each other at Christmas.
But there are considerations around how long it should be, how to manage transport, how many households it should include, even what really constitutes a household.
There doesn't seem any doubt that an agreement will be reached. It seems (for once perhaps?) the delay is because of the complexities not a conflict.
Conversations continue, and there should be a deal of some sort before too long."
"
It never ceases to amaze me how people think when it comes to the Arctic. Somehow there is this pervasive belief that “if we just go there and document it, we’ll be able to demonstrate how climate change is affecting the arctic”.  This is the second team with such dubious aspirations this year, the first being failed kayaker Lewis Gordon Pugh who spun his dismal and embarrassing failure into an “accomplishment”, and then would not even take valid questions about his false claim of being the person who “kayaked furthest north”.
I have no sympathy for these people. Nature is teaching them hard lessons, let us hope they retain the material. – Anthony

STUCK IN THE ARCTIC FOR THREE WEEKS…AND COUNTING
Posted: 	Friday, September 26, 2008 8:20 AM by Jen Brown
From Peter Alexander, TODAY correspondent
So, here we are. In the Arctic. Day 23. Good times!
Producer Paul Manson and I, along with cameraman Callan Griffiths and soundman Ben Adam, were sent here on assignment to report on climate change and the Arctic for an upcoming broadcast. The primary news peg — and one reason for our visit — is that for only the second time in recorded history the Northwest Passage is ice free, effectively clearing this shortcut between Europe and Asia.
Our intention was to stay on board for 10 days, shooting video and interviews.  Mother Nature, apparently, had other plans. Inclement weather, along with an emergency search and rescue mission, has spoiled all five of our attempts to leave the ship.  Getting stuck in the Arctic is not uncommon; getting stuck five times is like punishment.
Joining the team
We left NYC Sept. 3, joining up with a team of scientists from ArcticNet on board the Canadian Coast Guard icebreaker, Amundsen. (In Canada, the Coast Guard is civilian, not military. It is part of the country’s Department of Oceans and Fisheries.) This particular Coast Guard ship has been dedicated to scientific research and outfitted with all the necessary tools. In a unique partnership, the scientists work side-by-side with the Coast Guard crew. For example, the scientists are testing water samples and sediment samples (from the ocean floor) as well as mapping uncharted territories in this remote part of the world. There are 40 scientists, 40 Coast Guard members and the four of us. By now we’re part of the team, learning to help on deck, in the lab and at dinner.
We boarded the Amundsen Thursday, Sept. 4, in Resolute Bay, a small Inuit village, along the Northwest Passage. The plan was to fly off by helicopter at the northern most civilian community in North America, Grise Fjord, and then begin our long journey home. Freezing rain and harsh weather kept our chopper grounded both Monday and Tuesday. The ship kept going and our chance to get off passed. We continued North with the expedition along the coasts of the Canadian Arctic and Greenland, coming within 900 miles of the North Pole.
Over the next couple weeks, we would make three more attempts to fly to land. Each one failed due to weather. Unbelievably, on Thursday our absolute best chance to get off the ship failed, too. The ship was diverted back north to assist a search and rescue mission, something the crew says has only happened once or twice in the last couple years.  From the beginning, we were warned that the ships primary mission was science. The cost of operating this icebreaker and moving the expedition forward is $50,000 a day. While we’ve been welcomed guests on board, we knew the ship wouldn’t be stopping for us. 
Close quarters
Paul and I have been sharing what would normally be the infirmary on this overloaded ship. To our eye, it’s roughly, 10 by 12 feet. A thin curtain is the only thing separating us — and our dignity. Callan and Ben share a bunk bed in a slighter larger room downstairs.






Soundman Ben Adam, producer Paul Manson, cameraman Callan Griffiths and correspondent Peter Alexander



In our 23 days on the ship we have covered more than 2,500 miles. The ship rocks incessantly and a sonar machine used for ocean floor mapping ticks loudly all day and night. It’s akin to being audibly poked day in and day out. (Callan has lovingly promised to buy each of us a metronome when we get home so that will be able to sleep as comfortably in NYC.)
Since we were done shooting two weeks ago, we’ve been left with a lot of time to fill. Meals have become a priority. It’s often the only way we can keep track of what time and day it is. Thursday is a favorite — breakfast crepes. Speaking of crepes, we’ll remind you this is a French-Canadian ship, and so we’ve been more than well fed. In fact, we’re convinced Fabien, the ships pastry chef — yes, I said pastry chef — is trying to kill us slowly with desserts.
Meals are always heavy and large. (Now, so are some of us.) But fear not, there is a fitness club on board. Let us describe it for you: it’s half the size of our bedroom (read: infirmary), and consists of a treadmill, two bikes and a bench that’s hidden beneath a four-foot ceiling. (Running on a treadmill when the ship is rocking could easily pass as its own Olympic sport.) Not to worry, we’ve now collectively run or biked the length of Greenland six times over. The other hours have been spent staring at the ocean, staring in the abyss and staring at each other — followed by routine games of Scrabble, “what’s for dinner?” and “if you could be any kind of animal, what would you be?”
A once-in-a-lifetime experience
Let’s be clear, although we’ve been mentally ready to leave for a long time now, we have seen and done some extraordinary things, including meeting some inspiring scientists whose dedication to their field reminds us daily why we’re here. We’ve seen polar bears, beluga whales and icebergs the size of floating hotels. Each sighting reminds us how far away we are from home. In addition, we’ve seen sea creatures from far below the ocean’s surface that would rival the characters at the Star Wars bar.
The scenery is both breathtaking and intimidating. We’ve been awed by sights that most people will never see and appreciate that this is a once-in-a-lifetime experience. (Hopefully.)
VIDEO: Peter Alexander and Paul Manson phone home to describe the (mis)adventures


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c3cfdff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Given the thousands of comments made here weekly, I’ve decided to add a new feature to WUWT: Quote of the Week. It will be posted on Sundays.

A commenter on WUWT summed up Earth Hour in a succinct  way:
I will be thinking about the 1.8 billion people on Earth who have no  access to electricity, and how insane they must think we are.
From commenter “007” on the WUWT Poll: What are you going to do for “Earth Hour”? thread.
Anyone that wants to submit a better feature logo that the simple one I cobbled together above is certainly welcome to do so. – Anthony
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e978cbff6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest Post by Steven Goddard

Over the last year or so I have been taking an informal survey of a key news metric – Google news searches for the term “global warming.”  A year ago, the ratio of alarmist/skeptical articles was close to 100/1.  About six months ago, the ratio was 90/10, Two months ago it was 80/20, and today it hit 50/50 for the first time – including the lead skeptical story “A Cooling Trend Toward Global Warming“.  One thing that has changed is the rise of blogs written by informed citizens, complemented by the demise of corporate newspapers which make money from keeping people continually alarmed about one thing or another.
Congratulations to Anthony and all the readers for being a big part of this.  Democracy in it’s purest form – hope and change we can all believe in.
The top two items from Google news “global warming” search today.  The distribution of all stories through the first few search pages was similar in makeup as seen below:


The Tech Herald
A Cooling Trend Toward Global Warming
The New American – ‎1 hour ago‎
With the election of a president who is solidly in the global–warming-alarmist camp – and with many high-level appointees who are bona fide climate-change …
Global warming and climate change: facts and hype Examiner.com
UN global warming stand criticized Delta Farm Press
UN Con on Global Warming Nearly Foiled NewsMax.com
Opposing Views – Atlanta Journal Constitution
all 36 news articles »

New York Times
House Democrats release draft energy, climate bill
New York Times – ‎8 hours ago‎
By DARREN SAMUELSOHN AND BEN GEMAN, Greenwire Democratic leaders of the House Energy and Commerce Committee today unveiled a 648-page draft global warming …
House Democrats unveil sweeping plan to reshape energy in America MiamiHerald.com
Waxman’s clean energy draft includes cap-and-trade proposals Oil & Gas Journal
US lawmakers present draft bill on ‘clean energy’ AFP
iBerkshires.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96f29872',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAlthough the Potsdam Institute For Climate Impact Research (PIK) gives the impression that it is a climate research facility, it also appears to have become an institute for formulating novel economic policy.
H/t: reader Ike
The PIK, commissioned by the German Federal Ministry for the Environment, Nature Conservation and Nuclear Safety, has produced and released a NEW SYNTHESIS REPORT that claims Europe can revitalize its economy by tackling the climate challenge, namely by raising the European climate target for emissions reductions from 20% to 30%. The report is titled:

A New Growth Path for Europe – Generating Prosperity and Jobs in the Low Carbon Economy”
Tipping point to prosperity at 30%
The PIK seems to be claiming there is an economic tipping point to prosperity at 30% emissions reduction. The onlineDie Zeit writes on the new PIK report:
Europe should reduce its greenhouse gas emissions by 30% instead of only 20% by 2020, which is the current plan. This is how the continent could overcome its economic stagnation.
If they stick to the 20% target, ‘then it would be like someone stuck in a hole who is digging deeper’.”
Based on climate-economic models!
The PIK claims a 30% reduction by 2020 would lead to higher growth and increased employment. These projections are based on “new model results”. (PIK models have an incurable habit of producing exactly what the PIK wants to see).
In the coming decade, Europe will need to accept the challenge of increasing economic growth while reducing both unemployment and greenhouse gas emissions. New model results show that these three goals can actually reinforce one another.”
Yet, there must be something terribly wrong with their models because a slew of European governments have been recently forced to do just the exact opposite, due real-life economics, and scale back subsidies to money-losing  green energy sources – especially solar.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But the PIK has never been deterred by the harsh truths of reality, and claims:
Clear policies associated with a decisive move to a 30% target can be doubly beneficial for the climate and the EU economy.”
Their “new model results” also say their new plan would:
• increase the growth rate of the European economy by up to 0.6% per year.
• create up to 6 million additional jobs Europe-wide.
• boost European investments from 18% to up to 22% of GDP.
• increase European GDP by up to $842 billion (2004 dollars).
• increase GDP by up to 6% both in the old (EU15) and new (EU12) member states.”
For the first time in the academic modelling field
The above projections all sound so rosy. So just exactly what kind of brilliant mastermind plan did the PIK use to produce such rosy projections? (Hang on to your chair!):
For the first time in the academic climate modeling field, the present study has taken a state-of-the-art model of climate economics and enhanced it along those lines. The enhanced model includes:
• the fact that investments depend on subjective expectations, not on correct previsions of whatever future possibilities may arise.
• the fact that higher investments trigger higher learning-by-doing, thereby reducing unit costs.
• the resulting existence of different possible equilibria with different growth paths.
The new simulations show that 30% is achievable and can be economically beneficial by shifting the European Economy into a new, more advantageous equilibrium – a path of low-carbon growth.”
I don’t know about you, but I’d be a little wary of the “first-time in the academic climate modeling field” point, especially in a field as complex as economics. Thinking that these things work without glitches after just one tune-up sets a new standard in naiveness.
Apparently I’m not the only one who’s palm over face on this. Even Germany’s greenie Environment Minister Norbert Röttgen is not touching this with a 10-foot pole. According to Die Zeit:
Even Environment Minister Norbert Röttgen has placed little emphasis on the presentation of the new study: Instead of travellingng to Brussels himself, he sent his secretary Katherina Reiche.”
Like the old promises of government central planning, PIK’s plan is a roadmap to a disaster.

Share this...FacebookTwitter "
"

America’s security commitment to Taiwan faces a significant test. China’s growing power presents a challenge to U.S. military superiority, while Taiwan’s investment in its own defense has languished. Adding to the challenge of keeping peace in the Taiwan Strait is the shifting political situation in Taiwan, exemplified by the January 2016 elections in which voters rejected the cross‐​strait rapprochement policies of the Kuomintang (KMT) and turned over control of the presidency and legislature to the Democratic Progressive Party (DPP). The China‐​Taiwan relationship has remained relatively calm, but changes in the U.S.-China balance of power could make the Taiwan Strait a dangerous place once more if the implicit U.S. defense commitment to Taiwan loses credibility.



This paper outlines three broad policy options for the United States: shoring up the defense commitment by restoring military superiority over China; sustaining a minimum level of military advantage over China; or stepping down from the commitment to use military force to maintain Taiwan’s de facto independence. It concludes that the United States should step down from the defense commitment eventually, ideally through an incremental and reciprocal process with China that would draw concessions from Beijing. In the long term, the U.S. security commitment to Taiwan is neither beneficial nor advantageous for the United States. Taiwan will have to take responsibility for its own defense.



Stepping down from the implicit commitment to come to Taiwan’s rescue with military force carries risks, but other options leave the United States worse off in the long term. The likely damage to U.S.-Chinese relations caused by pushing for military superiority in the region outweighs the benefits. Sustaining a minimum level of military advantage is possible, but absent a long‐​term economic slowdown and/​or political changes in China—both of which are beyond U.S. control—maintaining such an advantage in perpetuity will be difficult. Stepping down from the commitment through a long‐​term process would give Taiwan the time it needs to make necessary changes in its defense technology and military strategy. Peace in the Taiwan Strait is an important American interest, but it must be weighed against the difficulty of maintaining credibility and the growing costs of deterrence failure.



The U.S. defense relationship with Taiwan is a risky and costly commitment that has become increasingly difficult to sustain. Barry Posen of the Massachusetts Institute of Technology put it best when he wrote, “The U.S. commitment to Taiwan is simultaneously the most perilous and least strategically necessary commitment that the United States has today.”1 The United States can and should strive for a peaceful resolution of the Taiwan dispute, but through means other than an implicit commitment to use military force to defend the island.



Washington’s approach to keeping the peace in the Taiwan Strait during the latter years of Taiwan’s Lee Teng‐​hui (1988–2000) and most of the Chen Shui‐​bian (2000—2008) administrations was known as “dual deterrence.” Under dual deterrence the United States issued a combination of warnings and reassurances to both China and Taiwan to prevent either from unilaterally changing the status quo.2 America’s overwhelming military advantage over the People’s Liberation Army (PLA) deterred China from using military force, while Taiwan moderated its behavior lest U.S. forces not come to its rescue.3 However, the dual deterrence concept is ill‐​suited to the current military environment in the Taiwan Strait.



Dual deterrence is no longer viable because the modernization of the PLA has improved Beijing’s ability to inflict high costs on U.S. military forces that would come to Taiwan’s aid in the event of a Chinese invasion attempt.4 The deployment of two U.S. Navy aircraft carriers to the waters around Taiwan during the 1995–1996 Taiwan Strait Crisis was a major embarrassment for the PLA, and it has played an important role in driving China’s military modernization.5 Improvements in China’s anti‐​access/​area denial (A2/AD) capabilities have significantly complicated the ability of the United States to defend Taiwan by making it difficult for the U.S. Navy and Air Force to operate in and around the Taiwan Strait.6 According to a recent RAND Corporation study, “a Taiwan [conflict] scenario will be extremely competitive by 2017, with China able to challenge U.S. capabilities in a wide range of areas.”7 This shifting balance of power strains the credibility of the U.S. defense commitment to Taiwan by increasing the costs the United States would have to pay in an armed conflict.



Two additional developments will challenge the cross‐​strait peace. First, the period of rapprochement that has characterized cross‐​strait relations since 2008 has ended. The former Taiwanese president, Ma Ying‐​jeou (2008–2016), championed cross‐​strait cooperation and economic linkages that brought a welcome sense of calm after the tumultuous administrations of Lee and Chen.8 However, the January 2016 landslide victory of the DPP in both presidential and legislative elections revealed popular dissatisfaction with Ma’s policies and a weakening economy.9 President Tsai Ing‐​wen pledged to maintain peace. But her unwillingness to declare support for the “1992 Consensus” (simply stated as “one China, different interpretations”) caused Beijing to suspend communication between the Taiwan Affairs Office and Taipei’s equivalent, the Mainland Affairs Council.10 It is too early to tell how Tsai’s administration and a DPP‐​controlled legislature will affect cross‐​strait relations, but the relatively high level of cooperation the Ma administration promoted is likely over.11



Second, China’s slowing economy adds uncertainty to cross‐​strait relations. China’s GDP growth rate was 6.9 percent during the first nine months of 2015, well below the double‐​digit GDP growth rates of the last couple of decades.12 Sliding growth and the resulting social instability could encourage China’s leaders to behave more aggressively toward Taiwan to bolster domestic legitimacy and ensure regime survival.13 However, a slowing economy could also restrict military spending and encourage Chinese policymakers to avoid big conflicts as they focus on shoring up the economy. At the very least, China’s economic situation is a source of uncertainty that was not present when the United States relied on dual deterrence.



What approach should the United States take in this shifting environment? Generally speaking, there are three options for the United States: it could do more to shore up the defense relationship with Taiwan and restore its military superiority over China; sustain a minimum level of military advantage over China; or step down from the implicit commitment to use military force in defense of Taiwan. This paper explores each of these and concludes that stepping down from the commitment is the best of the three options. The success of dual deterrence should be praised, but American policymakers must begin adjusting to a new state of affairs in the Taiwan Strait.



The U.S. security commitment to Taiwan consists of two pillars established in the Taiwan Relations Act (TRA) of 1979: arms sales and an implicit promise to defend Taiwan with military force should it be attacked. Both are set forth in Section 3 of the TRA, which states, in part, that the United States is permitted to sell Taiwan “defense articles and defense services in such quantity as may be necessary to enable Taiwan to maintain a sufficient self‐​defense capability.”14 Comparatively, the implicit commitment to use force to defend Taiwan is less clear. Section 3, part 3, authorizes the president and Congress to “determine, in accordance with constitutional processes, appropriate action by the United States” in response to “any threat to the security or the social or economic system of the people on Taiwan and any danger to the interests of the United States arising therefrom.”15 Military force is not explicitly mentioned, but it falls within the category of appropriate action that the United States could take.



The imprecise wording of the TRA has served the United States well by creating “strategic ambiguity,” the underpinning of dual deterrence.16 Strategic ambiguity, the open question of whether or not the U.S. military would intervene in a cross‐​strait conflict, had two important effects. First, it gave the United States greater freedom of action in trilateral relations. By not binding itself to one particular position, the United States could better adapt to unpredictable events. Second, strategic ambiguity restricted China and Taiwan’s freedom of action. Upsetting the status quo carried high costs for both sides. The United States could warn Taiwan that no cavalry would come to the rescue if Taiwan provoked China by making moves toward de jure independence.17 Likewise, the high costs that would be inflicted on the PLA by a U.S. intervention prevented Beijing from initiating a conflict.



China’s growing military power has diminished the value of strategic ambiguity by improving Beijing’s ability to inflict high costs on an intervening American force. The mere possibility of American intervention may no longer be enough to deter China if the PLA is better prepared to mitigate the effects.



Further complicating the U.S.-Taiwan defense relationship is the slow but steady erosion of U.S. credibility over the last two decades. This analysis uses the “Current Calculus” theory set forth by Dartmouth professor Daryl G. Press as the basis for assessing U.S. credibility. Press states, “Decisionmakers assess the credibility of their adversaries’ threats by evaluating the balance of power and interests … Future commitments will be credible if—and only if—they are backed up by sufficient strength and connected to weighty interests.”18 From Beijing’s perspective, the U.S. commitment to defend Taiwan is credible if American military power can pose a threat to Chinese forces and the United States has a strong interest in defending Taiwan.



On the subject of interests, Taiwan carries much more importance for China than it does for the United States. Charles Glaser of George Washington University writes “China considers Taiwan a core interest–an essential part of its homeland that it is determined to bring under full sovereign control.”19 Beijing does not appear eager to reunite Taiwan with the mainland by force in the near future, but China’s president Xi Jinping has warned that “political disagreements that exist between the two sides … cannot be passed on from generation to generation.”20 Maintaining Taiwan’s de facto independence may be important for the U.S. position in East Asia, but it does not carry the same significance that China places on reunification.21



Since China enjoys an advantage in the balance of interests, the credibility of the U.S. commitment rests on American military power. According to Press’s model, if the United States can carry out its threat to intervene with relatively low costs, then the threat is credible.22 When the TRA was passed in 1979, the United States enjoyed a clear advantage over a militarily weak China. That is no longer the case. Several recently published assessments of a U.S.-China conflict over Taiwan have sobering conclusions: America’s lead is shrinking, victory is less certain, and the damage inflicted on the U.S. military would be substantial. In China’s Military Power, Roger Cliff of the Atlantic Council writes, “Although China’s leadership could not be confident that an invasion of Taiwan in 2020 would succeed, it is nonetheless possible that it could succeed.… Even a failed attempt, moreover, would likely be extremely costly to the United States and Taiwan.”23 The RAND Corporation reached a similar conclusion: “At a minimum, the U.S. military would have to mount a substantial effort—certainly much more so than in 1996—if it hoped to prevail, and losses to U.S. forces would likely be heavy.”24 It is impossible to determine exactly how many American ships, aircraft, and lives would be lost to defend Taiwan from a PLA attack. But given the improved quality of PLA weapons systems and training exercises, it is safe to assume that the U.S. military would have to cope with losses that it has not experienced in decades.



Of course, it is important to note that high costs do not flow one way. In a war, the United States and Taiwan would make an invasion very costly for China, which reduces the credibility of Beijing’s threats to use force. However, U.S. military superiority in a Taiwan Strait conflict was nearly absolute until very recently. This superiority made victory relatively cheap, which enhanced the credibility of the American commitment.25 Improvements to already formidable Chinese weapons systems, combined with recent reforms that enhance command and control for fighting modern war, continue to ratchet up the costs the United States would have to absorb.26



If the PLA continues to improve at the rate it has done over the last 20 years, the United States could be in the unpleasant position of fighting a very costly conflict over a piece of territory that China has a much stronger interest in controlling than the United States has in keeping independent. Close economic ties between the United States and China (bilateral trade in goods was valued at $598 billion in 2015 in nominal dollars) would likely suffer as well.27 The high costs the United States would face in a conflict over Taiwan undermine U.S. credibility. China’s stronger interests and ability to inflict high costs on the United States could encourage Beijing to take risks that until recently would have been considered unacceptable.



Broadly speaking, the United States has three options for dealing with the diminishing credibility of its implicit commitment to defend Taiwan. In this section I explain what kinds of policies would most likely accompany each option and present favorable arguments for each.



The most straightforward way to bolster American credibility would be to increase the U.S. military presence close to Taiwan and clearly demonstrate the political will to honor the defense commitment. The combination of increased military presence and unequivocal political support would be a clear break from dual deterrence. Instead of directing warnings and reassurances toward both Taiwan and China, the United States would only warn China and only reassure Taiwan.28 The United States would welcome a stronger Taiwan, but U.S. support would not be preconditioned on Taiwan’s willingness to develop its defenses.



The ultimate goal of this policy option would be the establishment of a decisive and durable U.S. military advantage over the PLA. The clearest indicator of the U.S. commitment is military resources. Increasing the survivability of American air power in the area around Taiwan would send a clear signal of support. The American forces currently deployed in Japan would be the first to respond in a Taiwan conflict. Increasing the number of hardened aircraft shelters at U.S. bases in Japan, especially at Kadena Air Base on Okinawa, would protect aircraft from ballistic missile attacks.29 Additionally, the United States would revive the annual arms‐​sale talks with Taiwan that occurred from 1983 until 2001. Advocates for returning to annual talks argue that moving away from scheduled talks resulted in arms sales becoming less frequent.30 Future arms sales would include more advanced equipment that Washington is currently unwilling to sell to Taiwan, such as the F-35 Joint Strike Fighter aircraft and diesel attack submarines.31



Politically, American policymakers would clarify that U.S. military intervention in a Taiwan conflict is guaranteed. They would interpret the TRA as a serious commitment to Taiwan’s security, and, according to Walter Lohman of the Heritage Foundation, “[make] abundantly clear to Beijing the consequences that will ensue from the use of force.”32 The TRA would not be modified in any way that reduces the scope of America’s commitment. Supporters in Congress would regularly issue resolutions that reaffirm support for the TRA, especially the parts related to the defense of Taiwan.33 Strict interpretation of the TRA would be a clear demonstration of American willpower to take a hard line against China.



Public statements by American officials about U.S. intervention would not carry any preconditions or caveats. Such statements would be similar to the one made by President George W. Bush in April 2001 that the United States would do “whatever it takes” to defend Taiwan.34 Bush eventually walked back this statement, but successful implementation of the restore‐​superiority option would require similarly categorical shows of support. Removing preconditions from the commitment would bolster credibility by removing an off ramp the United States could take to avoid intervention. Additionally, Taiwan would not be expected to spend a certain percentage of its GDP on defense to secure U.S. arms sales or intervention.



Finally, the U.S. government would actively support de jure Taiwanese independence. As Weekly Standard editor William Kristol warns, “Opposing independence … might give Beijing reason to believe that the U.S. might not resist China’s use of force against Taiwan or coercive measures designed to bring about a capitulation of sovereignty.”35 However, supporting Taiwanese independence would be risky. In 2005, China passed the Anti‐​Secession Law (ASL) in response to the growing political power of the pro‐​independence movement in Taiwan.36 Article 8 of the ASL states that “non‐​peaceful means and other necessary measures” will be employed if “secessionist forces … cause the fact of Taiwan’s secession from China.”37 The increased American military presence resulting from the restore‐​superiority option would have to be strong enough to prevent China from invoking the ASL.



Advocates of the U.S. military commitment to Taiwan argue that the island’s success as a liberal democracy is linked to the regional security interests of the United States. For example, during his failed campaign for president, Sen. Marco Rubio (R-FL) said that “Taiwan’s continued existence as a vibrant, prosperous democracy in the heart of Asia is crucial to American security interests there and to the continued expansion of liberty and free enterprise in the region.”38 In the U.S. Congress the ideologically driven, “pro‐​democracy” camp of Taiwan supporters is large and influential.39 Proponents of a strong U.S. commitment to Taiwan also argue that Taiwan’s political system is evidence that Chinese culture is compatible with democracy. According to John Lee of the Hudson Institute, “Taiwan terrifies China because the small island represents a magnificent vision of what the mainland could be and what the [Chinese] Communist Party is not. This should be a reason to reaffirm that defending democracy in Taiwan is important to America and the region.”40 Supporters of a strong U.S. defense commitment to Taiwan through restoring America’s military superiority want to send a clear message to Beijing that the security commitment has not been shaken by China’s growing military power.



The second option, sustaining a minimum advantage, would maintain the current U.S. military commitment with some slight modifications. This option is much less resource‐​intensive than the restore‐​superiority option. The United States would maintain its implicit military commitment, but with preconditions that encourage Taiwan to invest more in its own defense. Importantly, the United States would reserve the right not to intervene if Taiwan provoked an armed conflict with China. The overarching themes of this option are balance and moderation. It has taken the United States years of effort to create what appears to be a relatively stable status quo, so, its supporters ask, why risk destabilizing it by significantly altering the U.S.-Taiwan relationship without very good reason?41



Under this option, the United States would improve the military assets for defending Taiwan, but at a much smaller scale than with the restore‐​superiority option. The PLA’s steadily improving capabilities diminish the credibility of the U.S. commitment to Taiwan by raising the costs of conflict. Maintaining a qualitative advantage over the PLA as it continues to develop will enhance the credibility of the U.S. commitment to Taiwan by keeping the costs of war high for the PLA. However, such improvements would be tempered to mitigate the chance of overreaction by Beijing and possible damage to U.S.-China relations.



American arms sales to Taiwan would continue under this policy option. Arms sales create tension in the U.S.-China relationship, but three benefits of arms sales mitigate the costs they create.42 First, arms sales complicate PLA planning and raise the costs of conflict for China. Second, damage done to U.S.-China relations as a result of the arms sales is relatively small. A joint report from the Project 2049 Institute and the U.S.-Taiwan Business Council on China’s reactions to arms sales concludes, “Past behavior indicates that the PRC is unlikely to challenge any fundamental U.S. interests in response to future releases of significant military articles and services to Taiwan.”43 Finally, arms sales demonstrate the commitment to Taiwan’s defense, especially in times of political transition.



Arms sales to Taiwan would also be adjusted to counteract the PLA’s quantitative advantage and operational strengths. Expensive items such as AV-8B Harriers, F-16 fighters, and Perry‐​class frigates would no longer be sold because they are highly vulnerable to Chinese weapons systems.44Instead, arms sales would prioritize cheaper, more numerous precision‐​guided weapons and advanced surveillance assets that would prevent Chinese forces from achieving a quick victory and buy time for the United States to come to Taiwan’s rescue.45 Such weapons systems are, generally speaking, much cheaper and easier to maintain than aircraft and ships. A report from the Center for Strategic and Budgetary Assessments argues that by “forego[ing] further acquisitions of costly, high‐​end air and naval surface combat platforms” Taiwanese policymakers can focus their economic resources on more “cost‐​effective platforms” better suited to Taiwan’s defense.46



The United States would expect Taiwan to make serious defense investments by increasing military spending and developing indigenous weapons systems. Taiwan’s military spending has increased in nominal terms after a precipitous drop in the late 1990s and early 2000s, but since 1999 defense spending has not risen above 3 percent of GDP.47 Taipei’s unwillingness to spend more on defense has upset some officials in Washington. In a November 2015 letter to President Obama calling for a new arms sale to Taiwan, Sen. John McCain (R-AZ) and Sen. Benjamin L. Cardin (D-MD) wrote, “We are increasingly concerned that, absent a change in defense spending, Taiwan’s military will continue to be under‐​resourced and unable to make the investments necessary to maintain a credible deterrent across the strait.”48 Thankfully, Tsai Ing‐​wen and the DPP have made increased defense spending a major policy goal.



The development of Taiwan’s defense industry would provide an additional source of high‐​quality military equipment for the island’s defense. Taiwan has experience designing and manufacturing sea and air defense weapons. James Holmes of the U.S. Naval War College notes, “[In 2010] Taiwanese defense manufacturers secretly designed and started building a dozen stealthy, 500‐​ton fast patrol craft [Tuo Chiang–class] armed with indigenously built, supersonic anti‐​ship missiles.”49 Indigenously produced air defense systems include the Tien Kung (TK) family of missiles, the Indigenous Defense Fighter, and anti‐​aircraft guns.50 Importantly, “Made in Taiwan” is not a byword for poor quality. According to Ian Easton of the Project 2049 Institute, the TK surface‐​to‐​air (SAM) missiles are “comparable to [U.S.-made] Patriot systems in terms of capability,” and the Hsiung Feng III anti‐​ship missile “is more capable than any comparable system fielded by the U.S. Navy in terms of range and speed.”51



Sustaining a minimum advantage would be the easiest of the three policy options for the United States to implement. Inertia is a powerful force. The United States has invested a considerable amount of resources and effort to reach a stable status quo in the Taiwan Strait, creating an “if it isn’t broken, don’t fix it” mentality. Advocates of maintaining the status quo, such as the Center for Strategic and International Studies, argue that it is “critically important to U.S. interests” to deter Chinese coercion of Taiwan, lest instability spread in East Asia.52 In prepared testimony before the House Foreign Affairs Committee, Deputy Assistant Secretary of State Susan Thornton said, “The United States has an abiding interest in cross‐​Strait peace and stability.”53 Congress, historically a strong bastion of support for Taiwan, shows no indication of changing America’s Taiwan policy anytime soon.



Buttressing support for this policy option is the belief that America’s commitment to Taiwan is a bellwether for the U.S. position in East Asia. According to John J. Mearsheimer of the University of Chicago, “America’s commitment to Taiwan is inextricably bound up with U.S. credibility in the region … If the United States were to sever military ties with Taiwan or fail to defend it in a crisis with China, that would surely send a strong signal to America’s other allies in the region that they cannot rely on the United States for protection.”54 Advocates of maintaining the U.S. commitment argue that East Asia would become more dangerous if other allies lose faith in the United States and start building up military capabilities of their own.55 Supporters of the U.S. commitment also contend that backing down on Taiwan would embolden Chinese aggression in other territorial disputes.



The final policy option would do away with America’s commitment to Taiwan’s defense on the grounds that military intervention to preserve the island’s de facto independence has become too costly and dangerous for the United States. Stepping down from the commitment to come to Taiwan’s rescue would be a major change in U.S. policy. However, other factors unrelated to the U.S. commitment would still make the use of force unattractive for Beijing. Taiwan would therefore not be defenseless or subject to imminent Chinese attack if the United States chose this policy option.



Without a U.S. commitment, Taiwan would have to improve its self‐​defense capability to deter an attack by China and fight off the PLA if deterrence failed. Taiwan does face an unfavorable balance of power vis‐​à‐​vis China, but this does not doom Taiwan to military defeat. In fact, research by Ivan Arreguín‐​Toft of Boston University indicates that large, powerful actors (such as China) have lost wars against weaker actors “with increasing frequency over time.”56 However, in order to have the greatest chance of success, the weaker side must have the right military strategy. A head‐​on, symmetric fight with the PLA would likely end in disaster for Taiwan, but Taiwan could successfully deny the PLA from achieving its strategic objectives through the same kind of asymmetric strategy that China uses to make it difficult for the United States to defend Taiwan.57 A military strategy emphasizing mobility, concealment, and area denial would both raise the costs of war for China and be sustainable, given Taiwan’s limited means.



Changing Taiwan’s defense strategy would not be a quick or easy task. The most immediate roadblocks to change are the equipment and mindset of Taiwan’s military. The upper echelons of the military have resisted implementing changes that could improve their ability to fight a war against the modern PLA. For example, James Holmes points out that Taiwan’s navy “[sees] itself as a U.S. Navy in miniature, a force destined to win decisive sea fights and rule the waves.”58 This is a dangerous mindset given the PLA Navy’s dominance in fleet size, strength, and advanced equipment. The Taiwan Marine Corps (TMC) is also ill‐​suited to meeting the threat posed by China. Instead of being a light, agile force, the TMC is “heavy, mechanized, and not particularly mobile,” reflecting “a glaring failure by Taiwan’s defense establishment to recognize the TMC’s essential role in national defense.”59 Overcoming the forces of bureaucratic inertia will be very difficult, but doing so is necessary if Taiwan can no longer count on the United States.



Stepping down from the U.S. defense commitment would likely involve reductions in U.S. arms sales. Reductions in the size, quantity, and frequency of arms sales would likely precede any reductions to the defense commitment because arms sales are a measurable signal of American support for Taiwan. Lyle J. Goldstein of the U.S. Naval War College points out, “Arms sales have for some time taken on a purely symbolic meaning.”60 This implies that the negative effects of reducing arms sales would be relatively small, since China’s extant military advantages are not being offset by U.S. weaponry. Additionally, stopping the arms sales would not have to be instantaneous. The United States could reduce arms sales incrementally to give Taiwan time to improve its self‐​defense capabilities.



One common argument made by opponents of stepping down from the commitment is that it is the only thing preventing China from attacking Taiwan. This argument ignores several important factors that make the use of force unattractive for Beijing. First, China’s reputation and standing in East Asia would be seriously damaged. Other countries in East Asia would harshly criticize China’s use of force, and would likely take steps to defend themselves. For example, countries involved in territorial disputes with Beijing in the South China Sea have responded to Chinese aggressiveness by improving their military power and pushing back politically and diplomatically.61 China’s reputational costs for attacking Taiwan would be very high. Additionally, any military operation against Taiwan would tie up a great deal of resources. Other states could take advantage of a Taiwan‐​focused Beijing to push back against other Chinese territorial claims.



Second, the PLA has problems with both “hardware” (equipment) and “software” (experience) that would restrict its options for using military force against Taiwan.62 The modern PLA has no experience conducting large‐​scale amphibious landings, which are complicated operations that would be very costly to execute against a dug‐​in defender.63 On the hardware side, the PLA still lacks the amphibious‐​lift capabilities and replenishment ships necessary to mount a successful invasion attempt.64 China has made big strides shifting the relative balance of power in the Taiwan Strait, but it still faces significant challenges that will take time to overcome.65 Presently, the PLA is more prepared to push back against American intervention than to initiate an invasion of Taiwan.



How the United States goes about stepping down from its commitment is important. Suddenly abrogating the TRA would be practically impossible given the entrenched support for Taiwan within Congress. The most realistic, feasible approach requires incremental reductions in U.S. support for Taiwan. Examples of such reductions could include setting a cap on the value and/​or quality of military equipment that can be sold to Taiwan, changing the TRA to more narrowly define what constitutes a threat to Taiwan, or requiring Taiwan to spend a certain percentage of its GDP on defense in order to receive U.S. military support.



Incremental reduction would be easier to sell to U.S. policymakers because it buys time for Taiwan to improve its defenses, thus increasing the credibility of the island’s military deterrent. As discussed earlier, Taiwan’s defense industries have proven they can make high‐​quality military equipment that meets the island’s defense needs. Taiwan has the ability to develop a robust and effective military deterrent, but it needs time to overcome existing challenges and address unforeseen obstacles. If the United States were to reduce its commitment incrementally, Taiwan’s political and military leadership would have the time to address such challenges.



Incremental implementation of this policy option would also provide the United States with opportunities to learn about Chinese intentions, based on Beijing’s reaction.66 Stepping down from the defense commitment to Taiwan would be a major accommodation on a core Chinese security interest. American policymakers should demand some sort of reciprocal actions from Beijing that reduce the military threat the PLA poses to Taiwan. In Meeting China Halfway, Lyle J. Goldstein explains how “cooperation spirals” in the U.S.-China relationship can build “trust and confidence … over time through incremental and reciprocal steps that gradually lead to larger and more significant compromises.”67 However, if Washington takes accommodating policy positions and Beijing responds with obstinacy or increased aggression, then American policymakers would likely want to adjust their approach.



Stepping down from the U.S. defense commitment to Taiwan, regardless of how it is implemented, is a controversial policy option that would face significant opposition. However, there is a strong case to be made for the benefits of such a policy. Taiwan’s fate carries much more significance for China than the United States, and American military superiority over China is eroding. Although Taiwan faces serious challenges, it would be capable of maintaining a military deterrent without American support, especially given the other factors that rein in Chinese aggression. A self‐​defense strategy emphasizing asymmetric warfare could raise the costs of military conflict for China to unacceptably high levels. Most important, the risk of armed conflict between the United States and China would be significantly reduced.



Each of the three policy options has problems and shortcomings that would make their implementation difficult and limit their effectiveness. In this section I will discuss the most important flaws of each policy option.



Restoring U.S. military superiority would shore up the credibility of the American commitment to Taiwan at the cost of severe damage to the U.S.-China relationship. China might be deterred from attacking Taiwan, but it would have ample reason to strongly oppose the United States across other issue areas, including the South China Sea, trade issues, and reining in North Korea. Additionally, unequivocal American support would reduce incentives for Taiwan to improve its defenses.



The most important negative consequence of restoring U.S. military superiority is the severe damage that would be done to U.S.-China relations. China and the United States do not see eye‐​to‐​eye on many issues, but this does not make China an outright adversary.68 Chinese cyber espionage against American companies, the rise of alternative development institutions led by Beijing, and island‐​building in the South China Sea are of great concern to policymakers in Washington.69 However, U.S.-Chinese cooperation on other pressing issues, especially environmental concerns and punishing North Korea after its recent nuclear tests, has supported U.S. goals.70 China is certainly not a friend or ally of the United States, but treating it as an enemy that needs to be contained is unwise.71 Restoring U.S. military superiority would set back much of the progress made in U.S.-China relations.



Restoring U.S. military superiority might be a boon to America’s credibility in the short term, but superiority may be fleeting. The growing U.S. military presence in East Asia, a result of the Obama administration’s “pivot” or “rebalance” to the region, has exacerbated the Chinese perception of the United States as a threat.72 Restoring U.S. military superiority will likely support this perception and provide a strong incentive for China to invest even more resources in its military. Additionally, falling behind in the conventional balance of power could prompt China to increase the quantity and quality of its nuclear weapon arsenal.73 If Beijing quickly offsets the advantages of stronger U.S. military support for Taiwan, the United States could end up in a similar position to the one it’s in now, but with a stronger China to deter.



Increasing American support for Taiwan without any preconditions regarding Taiwan’s role in its own defense would be detrimental in the long run. Taiwan and the United States’ other East Asian allies are willing to cheap‐​ride on American security guarantees.74 Taiwan is not disinterested in self‐​defense, but if someone else is shouldering the burden there is less urgency to do more, especially if increasing military spending means reducing social spending. China could exacerbate Taiwan’s “guns vs. butter” dilemma if it restricted economic exchanges (trade, investment, and tourism) with Taiwan as a result of a stronger U.S. posture.



Increasing the American commitment to Taiwan carries significant risks and costs for a benefit that would likely be fleeting. The likely negative consequences of restoring U.S. military superiority would not be worth the benefits. American policymakers should not go down this path.



The biggest weakness of sustaining a minimum U.S. military advantage is that it does not resolve any of the underlying issues in the cross‐​strait dispute, most important of which is the fact that Taiwan matters more to China than it does to the United States. Since the United States cannot equalize the imbalance of stakes vis‐​à‐​vis China, credible deterrence will require the United States to maintain military superiority over a steadily improving PLA. The United States is capable of absorbing these costs in the short run, but the recent history of the U.S.-China military balance suggests that China will be able to narrow the gap eventually.



Maintaining stability in the Taiwan Strait will become more complicated as a result of two trends in cross‐​strait relations and one higher‐​level trend. First, a distinct identity is taking hold in Taiwan; the people living there see themselves as Taiwanese instead of Chinese. Surveys conducted in 2014 showed that “fewer than 4 percent of respondents [in Taiwan] self‐​identified as solely Chinese, with a clear majority (60 percent) self‐​identifying solely as Taiwanese.”75 A unique Taiwanese identity is dangerous to Beijing because it makes China’s ultimate goal of reunification more difficult, especially if the identity issue leads to greater political support for independence. Thankfully, the Taiwanese people have been very pragmatic and have not yet made a significant push for de jure independence.76



Second, if China’s economy continues to slow down Beijing could become more aggressive toward Taiwan. A parade of doom and gloom headlines reveal the weaknesses of China’s economic miracle. The Chinese stock market experienced downturns in August 2015 and January 2016 that affected global financial markets.77 China Labor Bulletin, a Hong Kong‐​based workers’ rights group, recorded more than 2,700 strikes and worker protests throughout China in 2015—more than double the 1,300 recorded the year before.78 In February 2016, Reuters reported that 1.8 million workers in China’s state‐​owned coal and steel companies will be laid off in the coming years.79 This is not to say that China’s economy is in imminent danger of a catastrophic collapse. However, the political instability resulting from economic troubles could create an incentive for Beijing to act aggressively to burnish the Chinese Communist Party’s image at home.80 Exacerbating this risk is the rise of nationalist forces within Chinese society that could push the government into a more aggressive cross‐​strait policy. Such forces played an important role in the government’s heavy‐​handed response to 2014’s Occupy Central protests in Hong Kong.81 Economic problems coupled with aggressive ideology could prompt China to back away from any rapprochement with Taiwan. This could make the task of deterring a Chinese attack harder for the United States.



Third, America’s other security commitments could draw attention and resources away from Taiwan. Keeping pace with the PLA in the Taiwan Strait will require investments in military power that will become more difficult to sustain, barring either a reduction in global commitments or a significant decrease in China’s own economic and military power. The fight against ISIS in the Middle East and North Africa, the Russian threat to Eastern Europe, and Chinese island‐​building in the South China Sea are all vying for the attention of the U.S. military. The military has been able to cope with these contingencies, but there are signs of strain on the force.82 Given America’s current global security posture, it will be difficult for the United States to sustain a minimum advantage over the PLA in perpetuity.



Sustaining a minimum U.S. military advantage is growing more difficult and costly over time as these above trends develop. Fortunately, the costs are likely to increase slowly and could be mitigated by advances in U.S. military technology. However, ultimately the United States will be stuck in the unenviable position of trying to defend Taiwan from a China that has growing military power and a strong interest in prevailing in any dispute.



The two most important potential negative consequences of stepping down from the defense commitment to Taiwan are the reputational and credibility costs to the United States and the worsening of America’s military position in the region. Advocates of maintaining the U.S. commitment also contend that Chinese control over Taiwan would lead to a substantial PLA presence, which would pose a serious threat to American and allied interests. The military dominance that the United States has enjoyed since the end of World War II would be called into question. Advocates of U.S. primacy in East Asia consider such an outcome dangerous and unacceptable.83



Opponents of stepping down from the commitment argue that both China and the United States’ Asian allies will view such a change as a sign of American weakness and unwillingness to live up to other commitments.84 If the United States does not show strong resolve as China grows more powerful, Beijing would take advantage of American weakness to more forcefully pursue objectives that are detrimental to U.S. allies and partners.85 The Brookings Institution’s Richard Bush argues that “[the United States] cannot withdraw from the cross‐​Strait contest altogether because U.S. allies and partners would likely read withdrawal as a sign that the U.S. security commitments to them are no longer dependable.”86 Stepping down from the commitment to Taiwan would have two mutually reinforcing harmful effects: China would grow bolder in threatening U.S. allies and the allies would presume that the United States would not fulfill its commitments as the threat from China grows.



Fears over these negative consequences stem from a popular misconception of credibility in which the past actions of a state are considered indicative of how the state will behave in the future. As noted earlier, academic research indicates that states take other factors into account when making judgements of credibility, but the dogmatic adherence to this misconception among the American policymaking elite makes stepping down from the commitment an uphill battle.87 Formal treaty commitments to states like Japan and South Korea carry more weight than America’s vague commitment to Taiwan, but fears of abandonment will likely weigh heavily on the minds of policymakers in Seoul, Tokyo, and Washington.88 Overturning the assumptions that credibility is bound up in upholding past promises will take a great deal of time and effort.



Ending the U.S. defense commitment to Taiwan could be detrimental to the U.S. military’s broader goals in East Asia. Taiwan lies in the middle of an island chain that runs from Japan to the South China Sea. Control of Taiwan has important strategic implications because of this location. The PLA could use Taiwan as a staging area to more easily project power into the South China Sea, the East China Sea, and the western Pacific.89 Keeping this island chain free of Chinese military bases and friendly to the United States is therefore seen as essential for America’s position in the region. Indeed, Taiwan has loomed large in American military strategy in the region for decades. In 1950 General Douglas MacArthur described Taiwan as “an unsinkable aircraft carrier and submarine tender ideally located to accomplish offensive strategy and at the same time checkmate defensive or counter‐​offensive operations” from the surrounding area.90 If Taiwan becomes the PLA’s ‘unsinkable aircraft carrier,’ it would make U.S. military actions in support of other regional interests more difficult.



Fears over China’s improved military position that would follow seizing control over Taiwan are valid, but there are roadblocks to this outcome that exist independent of the U.S. defense commitment. As mentioned earlier in this analysis, China would face numerous hurdles and negative consequences if it tried to invade Taiwan, given the difficulty of conducting amphibious invasions, the high likelihood of regional backlash, and the materiel and training limitations of the PLA.91 Taiwan could also do more to raise the costs of conflict for China through changes in military technology and warfighting doctrine.92 For example, Taiwan’s fleet of fighter aircraft is costly to maintain and outclassed by PLA fighters and surface‐​to‐​air missile capabilities.93 Reducing the size of Taiwan’s fighter fleet and redirecting funds to build up mobile missile forces that could support ground units fighting against a PLA invasion attempt would improve Taiwan’s ability to resist the PLA and inflict heavy losses on Chinese forces.94 If President Tsai and the DPP can deliver on their promises to increase defense spending and develop Taiwan’s defense industries, Taiwan could be capable of mounting an effective self defense without American intervention in the coming decades.



The United States should step down from the implicit commitment to use military force to preserve Taiwan’s de facto independence. American credibility is slowly eroding as China becomes more powerful, and the commitment will be more costly to maintain for a relatively minor benefit. Broadly speaking, the United States has two options for how it could implement this policy option: it could try to draw concessions from China to get something in return for stepping down from the commitment, or it could unilaterally drop the commitment. In either scenario, Taiwan would have to take on sole responsibility for deterring Chinese military action.



A policy that wins concessions from China would be the more desirable of the two options. Concessions could include resolution of other territorial disputes involving China and American allies or dropping the Chinese threat to use force against Taiwan. This would be characteristic of what Charles Glaser calls a grand bargain, “an agreement in which two actors make concessions across multiple issues to create a fair deal … that would have been impossible in an agreement that dealt with a single issue.”95 Making the end of the U.S. commitment to Taiwan contingent upon Chinese concessions to resolve its other territorial disputes peacefully would benefit both the United States and China.96 The United States would free itself of an increasingly costly and risky commitment to Taiwan’s defense, but only if China compromises in ways that align with U.S. allies’ interests in the South and East China Seas. China would have to limit its objectives in the South and East China Seas, but in return would earn a major policy concession from the United States on a core national interest that has much more importance than the other territorial disputes.



If China proves unwilling to make concessions across multiple issue areas, the United States could still push for concessions on China’s military posture toward Taiwan. Instead of demanding a concession on the South China Sea dispute, U.S. policymakers could press China to take actions that reduce the military threat it poses to Taiwan via an incremental, reciprocal process of concessions.97 Refusing to sell Taiwan any new military equipment would be a good way to initiate a cooperation spiral.



Stopping the sale of new equipment would not significantly reduce the Taiwanese military’s ability to defend itself for three reasons. First, most equipment sold to Taiwan by the United States does not represent the latest in U.S. military technology and is not necessarily superior to new capabilities fielded by the PLA.98 Second, Taiwan’s domestic defense industry is capable of producing new equipment that is well‐​suited to asymmetric defense, although it will take time for Taiwan’s relatively small and underdeveloped defense industry to reach its full potential.99 Finally, stopping the sale of new weapons still gives the United States the latitude to sell spare parts and ammunition for weapons systems that have already been sold. Halting the sale of new types of weapons systems will signal a reduced U.S. commitment to Taiwan’s security that would not be overly disruptive to Taiwan’s self‐​defense.



One of several ways that Beijing might respond to this U.S. concession on arms sales would be to reduce the number of short‐​range ballistic missiles (SRBMs) within firing range of Taiwan. Currently there are more than 1,000 conventionally armed SRBMs (with a maximum range of approximately 500 miles) in the PLA arsenal that could strike Taiwan.100 Improvements in guidance technology have transformed these missiles from inaccurate “terror weapons” that would likely target cities to precision munitions better suited for strikes against military airfields and ports.101Stationing the SRBMs out of range of Taiwan would be a low‐​cost, but symbolically important, action. The missiles are fired from mobile launchers that could be moved back into range of Taiwan. However, the act of moving the missiles out of range would, according to Lyle J. Goldstein, “show goodwill and increasing confidence across the Strait and also between Washington and Beijing.”102 If China agrees to America’s demand to relocate its ballistic missiles, then additional steps could be taken to further reduce the threat China poses to Taiwan.



If China proved unwilling to make any concessions, either in other territorial disputes or in cross‐​strait relations, the United States could still unilaterally withdraw from its military commitment to Taiwan. No demands or conditions would be placed on Chinese behavior. American policymakers are unlikely to accept such a course of action given recent shows of Chinese assertiveness. Charles Glaser explains, “China appears too likely to misinterpret [unilaterally ending the U.S. commitment to defend Taiwan], which could fuel Chinese overconfidence and intensify challenges to U.S. interests.”103 Unilateral withdrawal would reduce the likelihood of U.S.-Chinese armed conflict, but the dearth of other benefits would make the policy difficult for policymakers to implement. Extracting some kind of concession from China, either in cross‐​strait relations or in other territorial disputes, should be a priority.



Finally, stepping down from the commitment to defend Taiwan with military force does not remove America’s interest in keeping the Taiwan Strait free of armed conflict. The United States would retain the ability to punish China in other ways should it attack Taiwan. Diplomatic isolation and economic sanctions may not inflict the same kinds of costs on Beijing as military force, but they are additional costs that would have to be absorbed.104 Additionally, U.S. arms sales are separate from the implicit commitment to defend Taiwan and could continue, albeit in some reduced or modified form.105 Continuing to sell arms to Taiwan while stepping down from the implicit commitment to use military force to defend the island allows the United States to demonstrate support for Taiwan’s defense without taking on the risks associated with direct intervention.106



The United States should no longer provide the military backstop for Taiwan’s de facto independence. The security commitment to Taiwan outlined in the TRA is a product of a different time, when the United States enjoyed clear military advantages over China, and Taiwan could be defended on the cheap. China’s growing military power strains the credibility of the American commitment. Policymakers in Washington could respond to this changing environment by restoring American military superiority, sustaining a minimum military advantage, or stepping down from the commitment. All of these options carry risks and negative consequences, but it is in the best long‐​term interest of the United States to step down from the commitment to Taiwan.



American policymakers must come to terms with the idea that the balance of power has become much more favorable for Beijing since the TRA was adopted in 1979. Defending Taiwan is more difficult now than ever before, and this trend will be very hard to reverse. The most realistic way to reorient U.S. policy is to reach out to China to take incremental, reciprocal steps that slowly bring about the end of America’s commitment. This policy will be very difficult for the United States to implement, but the advantages to U.S.-China relations could be substantial. Changing the U.S.-Taiwan security relationship would greatly reduce the likelihood of armed conflict between the United States and China and could create opportunities for U.S.-China cooperation that are currently beyond reach.



1\. Barry R. Posen, Restraint: A New Foundation for U.S. Grand Strategy (Ithaca, NY: Cornell University Press, 2014), p. 102.



2\. Richard Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” Asia Working Group Paper no. 1, Brookings Institution, December 2015, p. 5.



3\. Andrew Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” in China’s Great Leap Outward: Hard and Soft Dimensions of a Rising Power, ed. Andrew Scobell and Marylena Mantas (New York: The Academy of Political Science, 2014), pp. 130–31.



4\. Invasion is not the only military option available to China. The PLA could also conduct a blockade of Taiwan or conduct decapitation strikes to eliminate Taiwan’s political leadership. This analysis focuses on a Chinese invasion attempt because it is the most severe military option in terms of costs for all sides involved, and it carries the best chance for Beijing to accomplish its ultimate goal of reunifying Taiwan with the mainland via direct military and political control.



5\. For an excellent overview of the 1995–1996 crisis, see: Ted Galen Carpenter, America’s Coming War with China: A Collision Course over Taiwan (New York: Palgrave Macmillan, 2005), pp. 66–70; Robert S. Ross, “The 1995–96 Taiwan Strait Confrontation: Coercion, Credibility, and the Use of Force,” International Security 25, no. 2 (Fall 2000): 87–123. On the role of the crisis on China’s military modernization, see: Michael S. Chase et al., China’s Incomplete Military Transformation: Assessing the Weaknesses of the People’s Liberation Army (PLA) (Santa Monica, CA: RAND Corporation, 2015), p. 14; Andrew J. Nathan and Andrew Scobell, China’s Search for Security (New York: Columbia University Press, 2012), pp. 303–08.



6\. Dean Cheng, “Countering China’s A2/AD Challenge,” The National Interest, September 20, 2013, http://nationalinterest.org/commentary/countering-china%E2%80%99s-a2-ad-challenge-9099?page=show; Henry J. Hendrix, At What Cost a Carrier? Disruptive Defense Papers (Washington: Center for a New American Security, 2013); Ronald O’Rourke, China’s Naval Modernization: Implications for U.S. Navy Capabilities—Background and Issues for Congress (Washington: Congressional Research Service, 2015).



7\. Eric Heginbotham et al., The U.S.-China Military Scorecard: Forces, Geography, and the Evolving Balance of Power 1996–2017 (Santa Monica, CA: RAND Corporation, 2015), p. 330.



8\. Lyle J. Goldstein, Meeting China Halfway: How to Defuse the Emerging U.S.-China Rivalry (Washington: Georgetown University Press, 2015), pp. 52–53.



9\. Tom Phillips, “Taiwan Elects First Female President,” Guardian (London), January 16, 2016, http://​www​.the​guardian​.com/​w​o​r​l​d​/​2​0​1​6​/​j​a​n​/​1​6​/​t​a​i​w​a​n​-​e​l​e​c​t​s​-​f​i​r​s​t​-​f​e​m​a​l​e​-​p​r​e​s​ident.



10\. Javier C. Hernandez, “China Suspends Diplomatic Contact With Taiwan,” New York Times, June 25, 2016, http://​www​.nytimes​.com/​2​0​1​6​/​0​6​/​2​6​/​w​o​r​l​d​/​a​s​i​a​/​c​h​i​n​a​-​s​u​s​p​e​n​d​s​-​d​i​p​l​o​m​a​t​i​c​-​c​o​n​t​a​c​t​-​w​i​t​h​-​t​a​i​w​a​n​.html.



11\. Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” pp. 15–19.



12\. Shannon Tiezzi, “China’s ‘New Normal’ Economy and Social Stability,” The Diplomat, November 24, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​1​1​/​c​h​i​n​a​s​-​n​e​w​-​n​o​r​m​a​l​-​e​c​o​n​o​m​y​-​a​n​d​-​s​o​c​i​a​l​-​s​t​a​b​i​lity/.



13\. Ted Galen Carpenter, “Could China’s Economic Troubles Spark a War?” The National Interest, September 6, 2015, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​c​o​u​l​d​-​c​h​i​n​a​s​-​e​c​o​n​o​m​i​c​-​t​r​o​u​b​l​e​s​-​s​p​a​r​k​-​w​a​r​-​13784.



14\. The full text of the Taiwan Relations Act can be found at American Institute in Taiwan, “Taiwan Relations Act,” January 1, 1979, http://​www​.ait​.org​.tw/​e​n​/​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​-​a​c​t​.html.



15\. Ibid.



16\. Brett V. Benson and Emerson M. S. Niou, “Comprehending Strategic Ambiguity: U.S. Security Commitment to Taiwan,” November 12, 2001, http://people.duke.edu/~niou/teaching/strategic%20ambiguity.pdf; Carpenter, America’s Coming War with China, pp. 7–8; J. Michael Cole, “Time to End U.S. ‘Ambiguity’ on Taiwan,” The Diplomat, July 6, 2012, http://​thediplo​mat​.com/​2​0​1​2​/​0​7​/​t​i​m​e​-​t​o​-​e​n​d​-​u​-​s​-​a​m​b​i​g​u​i​t​y​-​o​n​-​t​a​iwan/; and Michal Thim, “Time for an Improved Taiwan-U.S. Security Relationship,” American Citizens for Taiwan, February 21, 2016,http://​www​.amer​i​canci​t​i​zens​for​t​ai​wan​.org/​t​i​m​e​_​f​o​r​_​a​n​_​i​m​p​r​o​v​e​d​_​t​a​i​w​a​n​_​u​_​s​_​s​e​c​u​r​i​t​y​_​r​e​l​a​t​i​o​n​ship/.



17\. Carpenter, America’s Coming War with China, pp. 84–85; Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” pp. 135–36.



18\. Daryl G. Press, Calculating Credibility: How Leaders Assess Military Threats (Ithaca, NY: Cornell University Press, 2005), p. 3.



19\. Charles L. Glaser, “A U.S.-China Grand Bargain? The Hard Choice between Military Competition and Accommodation,” International Security 39, no. 4 (Spring 2015): 61.



20\. “China’s Xi says Political Solution for Taiwan Can’t Wait Forever,” Reuters, October 6, 2013, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​a​s​i​a​-​a​p​e​c​-​c​h​i​n​a​-​t​a​i​w​a​n​-​i​d​U​S​B​R​E​9​9​5​0​3​Q​2​0​1​31006.



21\. On the asymmetry of interests between China and the United States, see: Charles Glaser, “Will China’s Rise Lead to War? Why Realism Does Not Mean Pessimism,” Foreign Affairs 90, no. 2 (March/​April 2011): 86; Glaser, “A U.S.-China Grand Bargain?” p. 50; Goldstein, Meeting China Halfway, p. 65; John J. Mearsheimer, “Say Goodbye to Taiwan,” The National Interest no. 130 (March/​April 2014): 103; Posen, Restraint, p. 103; and Ross, “The 1995–96 Taiwan Strait Confrontation,” p. 123.



22\. Press, Calculating Credibility, p. 3.



23\. Roger Cliff, China’s Military Power: Assessing Current and Future Capabilities (New York: Cambridge University Press, 2015), p. 221. Emphasis in original quote.



24\. Heginbotham et al., The U.S.-China Military Scorecard, p. 331.



25\. Press, Calculating Credibility, p. 21.



26\. Philip C. Saunders and Joel Wuthnow, China’s Goldwater‐​Nichols? Assessing PLA Organizational Reforms (Washington: National Defense University, April 2016).



27\. The trade value figure from the U.S. Census Bureau represents the sum of U.S. exports to ($116.2 billion) and imports from ($481.9 billion) China. See United States Census, “Trade in Goods with China,” https://​www​.cen​sus​.gov/​f​o​r​e​i​g​n​-​t​r​a​d​e​/​b​a​l​a​n​c​e​/​c​5​7​0​0​.html.



28\. Richard C. Bush, Untying the Knot: Making Peace in the Taiwan Strait (Washington: Brookings Institution Press, 2005), p. 258.



29\. Cliff, China’s Military Power, p. 197.



30\. Michael Mazza, “Taiwanese Hard Power: Between a ROC and a Hard Place,” in A Hard Look at Hard Power: Assessing the Defense Capabilities of Key U.S. Allies and Security Partners, ed. Gary J. Schmitt (Carlisle Barracks, PA: U.S. Army War College Press, 2015), p. 221.



31\. Kent Wang, “Why the U.S. Should Sell Advanced Fighters to Taiwan,” The Diplomat, January 10, 2014, http://​thediplo​mat​.com/​2​0​1​4​/​0​1​/​w​h​y​-​t​h​e​-​u​s​-​s​h​o​u​l​d​-​s​e​l​l​-​a​d​v​a​n​c​e​d​-​f​i​g​h​t​e​r​s​-​t​o​-​t​a​iwan/.



32\. Walter Lohman, “What the United States Owes to Taiwan and Its Interests in Asia,” War on the Rocks, January 27, 2016, http://​waron​the​rocks​.com/​2​0​1​6​/​0​1​/​w​h​a​t​-​t​h​e​-​u​n​i​t​e​d​-​s​t​a​t​e​s​-​o​w​e​s​-​t​o​-​t​a​i​w​a​n​-​a​n​d​-​i​t​s​-​i​n​t​e​r​e​s​t​s​-​i​n​-​asia/.



33\. Riley Walters, “Affirming the Taiwan Relations Act,” The Daily Signal, March 27, 2014, http://​dai​lysig​nal​.com/​2​0​1​4​/​0​3​/​2​7​/​a​f​f​i​r​m​i​n​g​-​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​-act/.



34\. Quoted in Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” p. 131. Also see David E. Sanger, “U.S. Would Defend Taiwan, Bush Says,” _New York Times_, April 26, 2001, http://​www​.nytimes​.com/​2​0​0​1​/​0​4​/​2​6​/​w​o​r​l​d​/​u​s​-​w​o​u​l​d​-​d​e​f​e​n​d​-​t​a​i​w​a​n​-​b​u​s​h​-​s​a​y​s​.​h​t​m​l​?​p​a​g​e​w​a​n​t​e​d=all.



35\. William Kristol, “The Taiwan Relations Act: The Next 25 Years,” in Rethinking “One China,” ed. John J. Tkacik, Jr. (Washington: The Heritage Foundation, 2004), p. 17.



36\. Zhidong Hao, “After the Anti‐​Secession Law: Cross‐​Strait and U.S.-China Relations,” in Challenges to Chinese Foreign Policy: Diplomacy, Globalization, and the Next World Power, ed. Yufan Hao, George Wei, and Lowell Dittmer (Lexington, KY: The University Press of Kentucky, 2009), p. 201.



37\. Full text of the Anti‐​Secession Law can be found at Embassy of the People’s Republic of China in the United States of America, “Anti‐​Secession Law” (full text), March 15, 2005, http://​www​.chi​na​-embassy​.org/​e​n​g​/​z​t​/​9​9​9​9​9​9​9​9​9​/​t​1​8​7​4​0​6.htm. See also, Chunjuan Nancy Wei, “China’s Anti‐​Secession Law and Hu Jintao’s Taiwan Policy,” Yale Journal of International Affairs 5, no. 1 (Winter 2010): 112–27.



38\. “Following Historic China‐​Taiwan Meeting, Rubio Calls for Strengthening U.S.-Taiwan Relations,” press release, Marco Rubio’s official website, November 7, 2015, http://www.rubio.senate.gov/public/index.cfm/press-releases?ID=2ae3bcfd-82f8-4ffe-9580–6b988123c1d0.



39\. Eric Gomez, discussion with staffer for a senior Member of the House Armed Services Committee, October 6, 2015.



40\. John Lee, “Why Does China Fear Taiwan?” The American Interest, November 6, 2015, http://​www​.the​-amer​i​can​-inter​est​.com/​2​0​1​5​/​1​1​/​0​6​/​w​h​y​-​d​o​e​s​-​c​h​i​n​a​-​f​e​a​r​-​t​a​iwan/.



41\. Eric Gomez, conversation with Andrew Scobell, Senior Political Scientist, RAND Corporation, November 23, 2015.



42\. Michael Forsythe, “China Protests Sale of U.S. Arms to Taiwan,” New York Times, December 17, 2015, http://​www​.nytimes​.com/​2​0​1​5​/​1​2​/​1​8​/​w​o​r​l​d​/​a​s​i​a​/​t​a​i​w​a​n​-​a​r​m​s​-​s​a​l​e​s​-​u​s​-​c​h​i​n​a​.html.



43\. US‐​Taiwan Business Council and Project 2049 Institute, Chinese Reactions to Taiwan Arms Sales, ed. Lotta Danielsson (Arlington: US‐​Taiwan Business Council and Project 2049 Institute, 2012), p. 36.



44\. On the subject of AV-8 aircraft, see Wendell Minnick, “Despite Pressures from China, Taiwan Might Procure Harriers,” Defense News, January 16, 2016, http://​www​.defense​news​.com/​s​t​o​r​y​/​d​e​f​e​n​s​e​/​a​i​r​-​s​p​a​c​e​/​s​t​r​i​k​e​/​2​0​1​6​/​0​1​/​1​6​/​d​e​s​p​i​t​e​-​p​r​e​s​s​u​r​e​s​-​c​h​i​n​a​-​t​a​i​w​a​n​-​m​i​g​h​t​-​p​r​o​c​u​r​e​-​h​a​r​r​i​e​r​s​/​7​8​7​3​3284/. On Perry‐​class frigates, see Ankit Panda, “US Finalizes Sale of Perry‐​class Frigates to Taiwan,” The Diplomat, December 20, 2014, http://​thediplo​mat​.com/​2​0​1​4​/​1​2​/​u​s​-​f​i​n​a​l​i​z​e​s​-​s​a​l​e​-​o​f​-​p​e​r​r​y​-​c​l​a​s​s​-​f​r​i​g​a​t​e​s​-​t​o​-​t​a​iwan/. On F-16 aircraft, see: Van Jackson, “Forget F‐​16s for Taiwan: It’s All About A2/AD,” The Diplomat, April 8, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​0​4​/​f​o​r​g​e​t​-​f​-​1​6​s​-​f​o​r​-​t​a​i​w​a​n​-​i​t​s​-​a​l​l​-​a​b​o​u​t​-​a2ad/.



45\. William S. Murray, “Revisiting Taiwan’s Defense Strategy,” Naval War College Review 61, no. 3 (Summer 2008): 13–38; and Jim Thomas et al., Hard ROC 2.0 Taiwan and Deterrence through Protraction (Washington: Center for Strategic and Budgetary Assessments, 2014).



46\. Thomas et al., Hard ROC 2.0, p. 4.



47\. Bonnie Glaser and Anastasia Mark, “Taiwan’s Defense Spending: The Security Consequences of Choosing Butter over Guns,” Asia Maritime Transparency Initiative, March 18, 2015, http://​amti​.csis​.org/​t​a​i​w​a​n​s​-​d​e​f​e​n​s​e​-​s​p​e​n​d​i​n​g​-​t​h​e​-​s​e​c​u​r​i​t​y​-​c​o​n​s​e​q​u​e​n​c​e​s​-​o​f​-​c​h​o​o​s​i​n​g​-​b​u​t​t​e​r​-​o​v​e​r​-​guns/. Also see: Justin Logan and Ted Galen Carpenter, “Taiwan’s Defense Budget: How Taipei’s Free Riding Risks War,” Cato Institute Policy Analysis no. 600, September 13, 2007.



48\. For the full text of the letter, see Taiwan Defense and National Security, “Benjamin L. Cardin and John McCain Letter to President Obama Regarding Arms Sales to Taiwan,” November 19, 2015, http://www.ustaiwandefense.com/benjamin-l-cardin-john-mccain-letter-to-president-obama-regarding-arms-sales-to-taiwan-november-19–2015/.



49\. James Holmes, “Securing Taiwan Starts with Overhauling Its Navy,” The National Interest, February 5, 2016, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​s​e​c​u​r​i​n​g​-​t​a​i​w​a​n​-​s​t​a​r​t​s​-​o​v​e​r​h​a​u​l​i​n​g​-​t​h​e​-​n​a​v​y​-​15122.



50\. Ian Easton, Able Archers: Taiwan Defense Strategy in an Age of Precision Strike (Arlington: Project 2049 Institute, September 2014), pp. 35–37.



51\. Ibid., p. 36 (TK surface‐​to‐​air missile), and p. 64 (HF-3 anti‐​ship missile). It should be noted that the quote comes from a report written in late 2014. In February 2016, the U.S. Navy announced that the Standard Missile‐​6 (SM-6), which is capable of greater range and speed than the HF-3, will be modified for use as an anti‐​ship missile. Sam LaGrone, “SECDEF Carter Confirms Navy Developing Supersonic Anti‐​Ship Missile for Cruisers, Destroyers,” USNI News, February 4, 2016, http://​news​.usni​.org/​2​0​1​6​/​0​2​/​0​4​/​s​e​c​d​e​f​-​c​a​r​t​e​r​-​c​o​n​f​i​r​m​s​-​n​a​v​y​-​d​e​v​e​l​o​p​i​n​g​-​s​u​p​e​r​s​o​n​i​c​-​a​n​t​i​-​s​h​i​p​-​m​i​s​s​i​l​e​-​f​o​r​-​c​r​u​i​s​e​r​s​-​d​e​s​t​r​oyers.



52\. Michael Green et al., Asia‐​Pacific Rebalance 2025: Capabilities, Presence and Partnerships (Washington: Center for Strategic and International Studies, January 2016), p. 94.



53\. Susan Thornton, “Testimony of Susan Thornton,” House Foreign Affairs Committee, Subcommittee on Asia and the Pacific (Washington: House Foreign Affairs Committee, February 11, 2016), p. 4, http://​docs​.house​.gov/​m​e​e​t​i​n​g​s​/​F​A​/​F​A​0​5​/​2​0​1​6​0​2​1​1​/​1​0​4​4​5​7​/​H​H​R​G​-​1​1​4​-​F​A​0​5​-​W​s​t​a​t​e​-​T​h​o​r​n​t​o​n​S​-​2​0​1​6​0​2​1​1.pdf.



54\. Mearsheimer, “Say Goodbye to Taiwan,” p. 35.



55\. Shelley Rigger, “Why Giving Up Taiwan Will Not Help Us with China,” American Enterprise Institute (November 2011), p. 3.



56\. Ivan Arreguín‐​Toft, “How the Weak Win Wars: A Theory of Asymmetric Conflict,” International Security 26, no. 1 (Summer 2001): 96.



57\. Michael J. Lostumbo et al., Air Defense Options for Taiwan: An Assessment of Relative Costs and Operational Benefits (Santa Monica, CA: RAND Corporation, 2016); Murray, “Revisiting Taiwan’s Defense Strategy,” p. 13.



58\. Holmes, “Securing Taiwan Starts with Overhauling Its Navy.”



59\. Grant Newsham and Kerry Gershaneck, “Saving Taiwan’s Marine Corps,” The Diplomat, November 16, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​1​1​/​s​a​v​i​n​g​-​t​h​e​-​t​a​i​w​a​n​-​m​a​r​i​n​e​-​c​orps/.



60\. Goldstein, Meeting China Halfway, p. 65.



61\. Dan De Luce et al., “Why China’s Land Grab Is Backfiring on Beijing,” Foreign Policy, December 7, 2015, http://​for​eign​pol​i​cy​.com/​2​0​1​5​/​1​2​/​0​7​/​w​h​y​-​c​h​i​n​a​s​-​l​a​n​d​-​g​r​a​b​-​i​s​-​b​a​c​k​f​i​r​i​n​g​-​o​n​-​b​e​i​jing/; Prashanth Parameswaran, “Indonesia Plays Up New South China Sea ‘Base’ after China Spat,” The Diplomat, March 28, 2016, http://​thediplo​mat​.com/​2​0​1​6​/​0​3​/​i​n​d​o​n​e​s​i​a​-​p​l​a​y​s​-​u​p​-​n​e​w​-​s​o​u​t​h​-​c​h​i​n​a​-​s​e​a​-​b​a​s​e​-​a​f​t​e​r​-​c​h​i​n​a​-​spat/; and Richard Sisk, “Japan Sends ‘Destroyer’ to South China Sea in Message to China,” Mil​i​tary​.com, April 6, 2016, http://​www​.mil​i​tary​.com/​d​a​i​l​y​-​n​e​w​s​/​2​0​1​6​/​0​4​/​0​8​/​j​a​p​a​n​-​s​e​n​d​s​-​d​e​s​t​r​o​y​e​r​-​t​o​-​s​o​u​t​h​-​c​h​i​n​a​-​s​e​a​-​i​n​-​m​e​s​s​a​g​e​-​t​o​-​c​h​i​n​a​.​h​t​m​l​#​d​i​s​q​u​s​_​t​hread.



62\. Chase et al., China’s Incomplete Military Transformation; and Scott L. Kastner, “Is the Taiwan Strait Still a Flash Point? Rethinking the Prospects for Armed Conflict between China and Taiwan,” International Security 40, no. 3 (Winter 2015/16): 71–74.



63\. David A. Shlapak et al., A Question of Balance: Political Context and Military Aspects of the China‐​Taiwan Dispute (Santa Monica, CA: RAND Corporation, 2009), p. 118.



64\. Chase et al., China’s Incomplete Military Transformation, p. 100.



65\. Recent military reforms could speed up the pace of solving these challenges. See “Xi’s New Model Army,” The Economist, January 16, 2016, http://​www​.econ​o​mist​.com/​n​e​w​s​/​c​h​i​n​a​/​2​1​6​8​8​4​2​4​-​x​i​-​j​i​n​p​i​n​g​-​r​e​f​o​r​m​s​-​c​h​i​n​a​s​-​a​r​m​e​d​-​f​o​r​c​e​s​t​o​-​h​i​s​-​o​w​n​-​a​d​v​a​n​t​a​g​e​-​x​i​s​-​n​e​w​-​m​o​d​e​l​-army; Kor Kian Beng, “A Different PLA with China’s Military Reforms,” Straits Times (Singapore), January 5, 2016, http://​www​.strait​stimes​.com/​a​s​i​a​/​a​-​d​i​f​f​e​r​e​n​t​-​p​l​a​-​w​i​t​h​-​c​h​i​n​a​s​-​m​i​l​i​t​a​r​y​-​r​e​forms; and Mu Chunshan, “The Logic Behind China’s Military Reforms,” The Diplomat, December 5, 2015,http://​thediplo​mat​.com/​2​0​1​5​/​1​2​/​t​h​e​-​l​o​g​i​c​-​b​e​h​i​n​d​-​c​h​i​n​a​s​-​m​i​l​i​t​a​r​y​-​r​e​f​orms/.



66\. Glaser, “A U.S.-China Grand Bargain?” p. 51.



67\. Goldstein, Meeting China Halfway, p. 12.



68\. In a recent Pew survey, 23 percent of U.S. respondents considered China to be an adversary of the United States, while 44 percent considered China to be a “serious problem, but not an adversary.” Pew Research Center, “International Threats, Defense Spending,” May 5, 2016,http://​www​.peo​ple​-press​.org/​2​0​1​6​/​0​5​/​0​5​/​3​-​i​n​t​e​r​n​a​t​i​o​n​a​l​-​t​h​r​e​a​t​s​-​d​e​f​e​n​s​e​-​s​p​e​n​ding/.



69\. On cyber espionage, see: Jon R. Lindsay, “The Impact of China on Cybersecurity: Fiction and Friction,” _International Security_ 39, no. 3 (Winter 2014/15): 7–47; and Ellen Nakashima, “Chinese Breach Data of 4 Million Federal Workers,” _Washington Post_ , June 4, 2015,https://​www​.wash​ing​ton​post​.com/​w​o​r​l​d​/​n​a​t​i​o​n​a​l​-​s​e​c​u​r​i​t​y​/​c​h​i​n​e​s​e​-​h​a​c​k​e​r​s​-​b​r​e​a​c​h​-​f​e​d​e​r​a​l​-​g​o​v​e​r​n​m​e​n​t​s​-​p​e​r​s​o​n​n​e​l​-​o​f​f​i​c​e​/​2​0​1​5​/​0​6​/​0​4​/​8​8​9​c​0​e​5​2​-​0​a​f​7​-​1​1​e​5​-​9​5​f​d​-​d​5​8​0​f​1​c​5​d​4​4​e​_​s​t​o​r​y​.html. On alternative institutions, see: Jane Perlez, “China Creates a World Bank of Its Own, and the U.S. Balks,” _New York Times_ , December 4, 2015, http://​www​.nytimes​.com/​2​0​1​5​/​1​2​/​0​5​/​b​u​s​i​n​e​s​s​/​i​n​t​e​r​n​a​t​i​o​n​a​l​/​c​h​i​n​a​-​c​r​e​a​t​e​s​-​a​n​-​a​s​i​a​n​-​b​a​n​k​-​a​s​-​t​h​e​-​u​s​-​s​t​a​n​d​s​-​a​l​o​o​f​.​h​t​m​l​?_r=0. On the South China Sea, see: Melissa Sim, “U.S., China Cross Swords over South China Sea,” Straits Times (Singapore), February 25, 2016, http://​www​.strait​stimes​.com/​w​o​r​l​d​/​u​n​i​t​e​d​-​s​t​a​t​e​s​/​u​s​-​c​h​i​n​a​-​c​r​o​s​s​-​s​w​o​r​d​s​-​o​v​e​r​-​s​o​u​t​h​-​c​h​i​n​a-sea.



70\. Joshua P. Meltzer, “U.S.-China Joint Presidential Statement on Climate Change: The Road to Paris and Beyond,” Brookings Institution, September 29, 2015, http://​www​.brook​ings​.edu/​b​l​o​g​s​/​p​l​a​n​e​t​p​o​l​i​c​y​/​p​o​s​t​s​/​2​0​1​5​/​0​9​/​2​9​-​u​s​-​c​h​i​n​a​-​s​t​a​t​e​m​e​n​t​-​c​l​i​m​a​t​e​-​c​h​a​n​g​e​-​m​e​ltzer; and Somini Sengupta, “U.S. and China Agree on Proposal for Tougher North Korea Sanctions,” New York Times, February 25, 2016, http://​www​.nytimes​.com/​2​0​1​6​/​0​2​/​2​6​/​w​o​r​l​d​/​a​s​i​a​/​n​o​r​t​h​-​k​o​r​e​a​-​s​a​n​c​t​i​o​n​s​.html.



71\. Posen, Restraint, pp. 93–96.



72\. Exacerbating tensions: Andrew J. Nathan and Andrew Scobell, “How China Sees America: The Sum of Beijing’s Fears,” Foreign Affairs 91, no. 5 (September/​October 2012): 32–47; and Robert S. Ross, “The Problem with the Pivot,” Foreign Affairs 91, no. 6 (November/​December 2012): 70–82. On the subject of Taiwan’s role in the pivot, see: Green et al., Asia‐​Pacific Rebalance 2025, pp. 87–94.



73\. Fiona S. Cunningham and M. Taylor Fravel, “Assuring Assured Retaliation: China’s Nuclear Posture and U.S.-China Strategic Stability,” International Security 40, no.2 (Fall 2015): 16–19; and Gregory Kulacki, China’s Military Calls for Putting Its Nuclear Forces on Alert (Cambridge, MA: Union of Concerned Scientists, January 2016).



74\. Jennifer Lind, “Japan’s Security Evolution,” Cato Institute Policy Analysis no. 788, February 25, 2016; Logan and Carpenter, “Taiwan’s Defense Budget.”



75\. Kastner, “Is the Taiwan Strait Still a Flash Point?” p. 76.



76\. Ibid.



77\. “The Causes and Consequences of China’s Market Crash,” The Economist, August 24, 2015, http://​www​.econ​o​mist​.com/​n​e​w​s​/​b​u​s​i​n​e​s​s​-​a​n​d​-​f​i​n​a​n​c​e​/​2​1​6​6​2​0​9​2​-​c​h​i​n​a​-​s​n​e​e​z​i​n​g​-​r​e​s​t​-​w​o​r​l​d​-​r​i​g​h​t​l​y​-​n​e​r​v​o​u​s​-​c​a​u​s​e​s​-​a​n​d​-​c​o​n​s​e​q​u​e​n​c​e​s​-​c​hinas.



78\. James Griffiths, “China on Strike,” CNN, March 29, 2016, http://​www​.cnn​.com/​2​0​1​6​/​0​3​/​2​8​/​a​s​i​a​/​c​h​i​n​a​-​s​t​r​i​k​e​-​w​o​r​k​e​r​-​p​r​o​t​e​s​t​-​t​r​a​d​e​-​u​n​i​o​n​/​i​n​d​e​x​.html.



79\. Kevin Yao and Meng Meng, “China Expects to Lay Off 1.8 Million Workers in Coal, Steel Sectors,” Reuters, February 29, 2016, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​c​h​i​n​a​-​e​c​o​n​o​m​y​-​e​m​p​l​o​y​m​e​n​t​-​i​d​U​S​K​C​N​0​W205X.



80\. Carpenter, “Could China’s Economic Troubles Spark a War?”



81\. Taisu Zhang, “China’s Coming Ideological Wars,” Foreign Policy, March 1, 2016, http://​for​eign​pol​i​cy​.com/​2​0​1​6​/​0​3​/​0​1​/​c​h​i​n​a​s​-​c​o​m​i​n​g​-​i​d​e​o​l​o​g​i​c​a​l​-​w​a​r​s​-​n​e​w​-​l​e​f​t​-​c​o​n​f​u​c​i​u​s​-​m​a​o-xi/.



82\. David Larter, “Carrier Scramble: CENTCOM, PACOM Face Flattop Gaps This Spring Amid Tensions,” Navy Times, January 7, 2016, http://​www​.navy​times​.com/​s​t​o​r​y​/​m​i​l​i​t​a​r​y​/​2​0​1​6​/​0​1​/​0​7​/​c​a​r​r​i​e​r​-​s​c​r​a​m​b​l​e​-​c​e​n​t​c​o​m​-​p​a​c​o​m​-​f​a​c​e​-​f​l​a​t​t​o​p​-​g​a​p​s​-​s​p​r​i​n​g​-​a​m​i​d​-​t​e​n​s​i​o​n​s​/​7​8​4​2​6140/; and Andrea Shalal, “U.S. Arms Makers Strain to Meet Demand as Mideast Conflicts Rage,” Reuters, December 4, 2015, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​m​i​d​e​a​s​t​-​c​r​i​s​i​s​-​u​s​a​-​a​r​m​s​-​i​n​s​i​g​h​t​-​i​d​U​S​K​B​N​0​T​N​2​D​A​2​0​1​51204.



83\. Dana R. Dillon and John J. Tkacik, Jr., “China and ASEAN: Endangered American Primacy in Southeast Asia,” Heritage Foundation Backgrounder no. 1886, October 19, 2005, http://​www​.her​itage​.org/​r​e​s​e​a​r​c​h​/​r​e​p​o​r​t​s​/​2​0​0​5​/​1​0​/​c​h​i​n​a​-​a​n​d​-​a​s​e​a​n​-​e​n​d​a​n​g​e​r​e​d​-​a​m​e​r​i​c​a​n​-​p​r​i​m​a​c​y​-​i​n​-​s​o​u​t​h​e​a​s​t​-asia.



84\. Ross, “The 1995–96 Taiwan Strait Confrontation,” p. 109.



85\. Nancy Bernkopf Tucker and Bonnie Glaser, “Should the United States Abandon Taiwan?” The Washington Quarterly 34, no. 4 (Fall 2011): 32–33; Mearsheimer, “Say Goodbye to Taiwan”; Peter Navarro, “Is It Time for America to ‘Surrender’ Taiwan?” The National Interest, January 18, 2016, http://www.nationalinterest.org/blog/the-buzz/it-time-america-%E2%80%98surrender%E2%80%99-taiwan-14955; and Daniel Twining, “(Why) Should America Abandon Taiwan?” Foreign Policy, January 10, 2012, http://​for​eign​pol​i​cy​.com/​2​0​1​2​/​0​1​/​1​0​/​w​h​y​-​s​h​o​u​l​d​-​a​m​e​r​i​c​a​-​a​b​a​n​d​o​n​-​t​a​iwan/.



86\. Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” p. 21.



87\. Max Fisher, “The Credibility Trap,” Vox, April 29, 2016, http://​www​.vox​.com/​2​0​1​6​/​4​/​2​9​/​1​1​4​3​1​8​0​8​/​c​r​e​d​i​b​i​l​i​t​y​-​f​o​r​e​i​g​n​-​p​o​l​i​c​y-war; Paul Huth and Bruce Russett, “What Makes Deterrence Work? Cases from 1900 to 1980,” World Politics 36, no. 4 (July 1984): 496–526; Jonathan Mercer Reputation and International Politics (Ithaca, NY: Cornell University Press, 1996), pp. 22–25; and Press, Calculating Credibility, pp. 20–24.



88\. Glaser, “A U.S.-China Grand Bargain?” pp. 77–78.



89\. Bosco, “Taiwan and Strategic Security.”



90\. Quoted in Andrew S. Erickson and Joel Wuthnow, “Why Islands Still Matter in Asia,” The National Interest, February 5, 2016, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​w​h​y​-​i​s​l​a​n​d​s​-​s​t​i​l​l​-​m​a​t​t​e​r​-​a​s​i​a​-​1​5​1​2​1​?​p​a​g​e​=show.



91\. Nathan and Scobell, China’s Search for Security, pp. 307–08.



92\. J. Michael Cole, “How A2/AD Can Defeat China,” The Diplomat, November 12, 2013, http://​thediplo​mat​.com/​2​0​1​3​/​1​1​/​h​o​w​-​a​2​a​d​-​c​a​n​-​d​e​f​e​a​t​-​c​hina/; Eric Gomez, “Taiwan’s Best Option for Deterring China? Anti‐​Access/​Area Denial,” Cato at Liberty (blog), April 7, 2016,http://​www​.cato​.org/​b​l​o​g​/​t​a​i​w​a​n​s​-​b​e​s​t​-​o​p​t​i​o​n​-​d​e​t​e​r​r​i​n​g​-​c​h​i​n​a​-​a​n​t​i​-​a​c​c​e​s​s​a​r​e​a​-​d​enial; Holmes, “Securing Taiwan Starts with Overhauling Its Navy”; and Thomas et al., Hard ROC 2.0.



93\. Lostumbo et al., Air Defense Options for Taiwan, pp. 2–11.



94\. Ibid., pp. 73–89.



95\. Glaser, “A U.S.-China Grand Bargain?” p. 79.



96\. Ibid., pp. 78–83.



97\. Goldstein, _Meeting China Halfway,_ p. 12.



98\. Sam LaGrone, “UPDATED: U.S. Plans Modest $1.83B Taiwan Arms Deal; Little Offensive Power in Proposed Package,” _USNI News,_ December 16, 2015, https://news.usni.org/2015/12/16/breaking-u-s-plans-modest-1–83b-taiwan-arms-deal-little-offensive-power-in-proposed-package.



99\. For Taiwan’s indigenously produced equipment, see Easton, _Able Archers._



100\. Kastner, “Is the Taiwan Strait Still a Flash Point?” p. 70; and Mazza, “Taiwanese Hard Power: Between a ROC and a Hard Place,” p. 202.



101\. Murray, “Revisiting Taiwan’s Defense Strategy,” pp. 17–19; and Thomas et al., Hard ROC 2.0, pp. 12–14.



102\. Goldstein, _Meeting China Halfway,_ p. 63.



103\. Glaser, “A U.S.-China Grand Bargain?” p. 85.



104\. Carpenter, _America’s Coming War with China,_ p. 177.



105\. Eric Gomez, “The U.S.-Taiwan Relationship Needs a Change,” _Cato at Liberty_ (blog), November 30, 2015, http://​www​.cato​.org/​b​l​o​g​/​u​s​-​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​h​i​p​-​n​e​e​d​s​-​c​hange>.



106\. Carpenter, _America’s Coming War with China,_ p. 176.
"
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.   
  
_The Current Wisdom_ only comments on science appearing in the refereed, peer-reviewed literature, or that has been peer-screened prior to presentation at a scientific congress.   
  
**History to Repeat: Greenland’s Ice to Survive, United Nations to Continue Holiday Party**   
  
This year’s installment of the United Nations’ annual climate summit (technically known as the 16th meeting of the Conference of the Parties to the Framework Convention on Climate Change) has come and gone in Cancun. Nothing substantial came of it policy-wise; just the usual attempts by the developing world to shake down our already shaky economy in the name of climate change. News-wise probably the biggest story was that during the conference, Cancun broke an all time daily low temperature record. Last year’s confab in Copenhagen was pelted by snowstorms and subsumed in miserable cold. President Obama attended, failed to forge any meaningful agreement, and fled back to beat a rare Washington blizzard. He lost.   
  
But surely as every holiday season now includes one of these enormous jamborees, dire climate stories appeared daily. Polar bear cubs are endangered! Glaciers are melting!!   
  
Or so beat the largely overhyped drums, based upon this or that press release from Greenpeace or the World Wildlife Fund.   
  
And, of course, no one bothered to mention a blockbuster paper appearing in _Nature_ the day before the end of the Cancun confab, which reassures us that Greenland’s ice cap and glaciers are a lot more stable than alarmists would have us believe. That would include Al Gore, fond of his lurid maps showing the melting all of Greenland’s ice submerging Florida.   
  
Ain’t gonna happen.   




The disaster scenario goes like this: Summer temperatures in Greenland are warming, leading to increased melting and the formation of ephemeral lakes on the ice surface. This water eventually finds a crevasse and then a way down thousands of feet to the bottom of a glacier, where it lubricates the underlying surface, accelerating the seaward march of the ice. Increase the temperature even more and massive amounts deposit into the ocean by the year 2100, catastrophically raising sea levels.   
  
According to Christian Schoof of the University of British Columbia (UBC), “The conventional view has been that meltwater permeates the ice from the surface and pools under the base of the ice sheet….This water then serves as a lubricant between the glacier and the earth underneath it….”   
  
And, according to Schoof, that’s just not the way things work. A UBC press release about his _Nature_ article noted that he found that “a steady meltwater supply from gradual warming may in fact slow down the glacier flow, while sudden water input could cause glaciers to speed up and spread.”   
  
Indeed, Schoof finds that sudden water inputs, such as would occur with heavy rain, are responsible for glacial accelerations, but these last only one or a few days.   
  
The bottom line? A warming _climate_ has very little to do with accelerating ice flow, but _weather_ events do.   
  
How important is this? According to University of Leeds Professor Andrew Shepherd, who studies glaciers via satellite, “This study provides an elegant solution to one of the two key ice sheet instability problems” noted by the United Nations in their last (2007) climate compendium. “It turns out that, contrary to popular belief, Greenland ice sheet flow might not be accelerated by increased melting after all,” he added.   
  
I’m not so sure that those who hold the “popular belief” can explain why Greenland’s ice didn’t melt away thousands of years ago. For millennia, after the end of the last ice age (approximately 11,000 years ago) strong evidence indicates that the Eurasian arctic averaged nearly 13°F warmer in July than it is now.   
  
That’s because there are trees buried and preserved in the acidic Siberian tundra, and they can be carbon dated. Where there is no forest today—because it’s too cold in summer—there were trees, all the way to the Arctic Ocean and even on some of the remote Arctic islands that are bare today. And, back then, thanks to the remnants of continental ice, the Arctic Ocean was smaller and the North American and Eurasian landmasses extended further north.   
  
That work was by Glen MacDonald, from UCLA’s Geography Department. In his landmark 2000 paper in _Quaternary Research_ , he noted that the only way that the Arctic could become so warm is for there to be a massive incursion of warm water from the Atlantic Ocean. The only “gate” through which that can flow is the Greenland Strait, between Greenland and Scandinavia.   
  
So, Greenland had to have been warmer for several millennia, too.   
  
Now let’s do a little math to see if the “popular belief” about Greenland ever had any basis in reality.   
  
In 2009 University of Copenhagen’s B. M. Vinther and 13 coauthors published the definitive history of Greenland climate back to the ice age, studying ice cores taken over the entire landmass. An exceedingly conservative interpretation of their results is that Greenland was 1.5°C (2.7°F) warmer for the period from 5,000-9000 years ago, which is also the warm period in Eurasia that MacDonald detected. The integrated warming is given by multiplying the time (4,000 years) by the warming (1.5°), and works out (in Celsius) to 6,000 “degree-years.”   
  
Now let’s assume that our dreaded emissions of carbon dioxide spike the temperature there some 4°C. Since we cannot burn fossil fuel forever, let’s put this in over 200 years. That’s a pretty liberal estimate given that the temperature there still hasn’t exceeded values seen before in the 20th century. Anyway, we get 800 (4 x 200) degree-years.   
  
If the ice didn’t come tumbling off Greenland after 6,000 degree-years, how is it going to do so after only 800? The integrated warming of Greenland in the post-ice-age warming (referred to as the “climatic optimum” in textbooks published prior to global warming hysteria) is over seven _times_ what humans can accomplish in 200 years. Why do we even worry about this?   
  
So we can all sleep a bit better. Florida will survive. And, we can also rest assured that the UN will continue its outrageous holiday parties, accomplishing nothing, but living large. Next year’s is in Durban, South Africa, yet another remote warm spot hours of Jet-A away.   
  
References:   
  
MacDonald, G. M., et al., 2000. Holocene treeline history and climatic change across Northern Eurasia. _Quaternary Research_ **53** , 302-311.   
  
Schoof, C., 2010. Ice-sheet acceleration driven by melt supply variability. _Nature_ **468,** 803-805.   
  
Vinther, B.M., et al., 2009. Holocene thinning of the Greenland ice sheet. _Nature_ **461** , 385-388.


"
"
Al Gore Leaves The Light On For Ya
From Nashvillepost.com
By Kleinheider
The “312” is his address – 312 Lynnwood Blvd. Nashville
Even during Earth Hour. President of the Tennessee Center For Policy Research Drew Johnson takes a Saturday drive by Al Gore’s during the time most environmentalists went dark:
I pulled up to Al’s house, located in the posh Belle Meade section of Nashville, at 8:48pm – right in the middle of Earth Hour. I found that the main spotlights that usually illuminate his 9,000 square foot mansion were dark, but several of the lights inside the house were on.
In fact, most of the windows were lit by the familiar blue-ish hue indicating that floor lamps and ceiling fixtures were off, but TV screens and computer monitors were hard at work. (In other words, his house looked the way most houses look about 1:45am when their inhabitants are distractedly watching “Cheaters” or “Chelsea Lately” reruns.)
The kicker, though, were the dozen or so floodlights grandly highlighting several trees and illuminating the driveway entrance of Gore’s mansion.
I [kid] you not, my friends, the savior of the environment couldn’t be bothered to turn off the gaudy lights that show off his goofy trees.
More here
Here’s a look at Al Gores Nashville mansion:
Gore's Mansion in Nashville 

Vice President Al Gore has purchased this home, in Nashville’s exclusive Belle Meade section, for a reported USD2.3 million. The deed for the Colonial-style home, which sits on 2.09 acres of some of the city’s most expensive land, was signed on June 17, 2002. Gore and his wife, Tipper, will keep other homes in Tennessee and Virginia. It was published February 28, 2007 that research group in Tennessee, where the former vice president lives, claims that Mr Gore’s 20-room, eight-bathroom home in Nashville consumes more electricity in a month than the average American household uses in a year.
Photo and description Source: Daylife
You can see it here on Google Maps
From an aerial view looking south you can see what could be a handful of solar panels, though the orientation is puzzling if that is what they are. Update: in comments it it pointed out that they may also be skylights, which seems more probable. So it appears there are no solar panels on Mr. Gore’s home. Note the SUV fleet.
From Microsoft Live Earth - click image for an interactive view
Here is a view looking east:
From Microsoft Live Earth - click image for an interactive view
UPDATE: The photos above don’t show solar panels, however an alert commenter found this photo showing the placement on the one flat section of roofing shown in the aerial views above:
Solar panels are seen on the roof of the home of former Vice President Al Gore in Nashville, Tenn. , Thursday, June 7, 2007. Gore, the environmental activist stung by criticism over his house's energy efficiency, said Friday that renovations are nearly complete to make it a model ""green"" home. Earlier this year, a conservative group criticized Gore, citing electric bills that were far more than the typical Nashville home. Utility records showed the Gore family paid an average monthly electric bill of about $1,200 last year for its 10,000-square-foot home. Source: AP
The 34 panels look to be between 200 and 250 watts each, for a total capacity at full sun of 6.8 to 8.5 kilowatts for the system.They will provide an offset, but will not fully replace energy consumption there. Given the 10,000 sq foot size and the pool, this is an undersized installation for the home. Some ground based panels would have helped.
– Anthony

Sponsored IT training links:
We offer guaranteed success in OG0-093 exam using latest 1z0-007 dumps and 70-272 sample tests



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e977d2c01',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

If you’re free Friday morning, you might want to hop on over to the Russell Senate Office Building to learn about the amazing, inexplicable, short‐​sighted market bias against straw‐​bale buildings and the need for the feds to do something about it. The Environmental & Energy Study Institute, the sponsor of this event, 



Invites you to learn how the ‘new but old’ method of straw‐​bale construction can help address some of our most serious national policy challenges, such as record energy prices and unemployment, inadequate supply of affordable housing, the threat of climate change, and pressing needs in transportation and infrastructure funding. The modern building industry places heavy demands on the energy and transportation sectors. Straw is a locally‐​sourced, widely available, and renewable resource that builders, architects, engineers, and home owners are turning into affordable, safe, durable, and energy‐​efficient buildings in many climates. The following presenters will discuss the benefits of using this American invention, the regulatory barriers and institutional biases against straw‐​bale construction, and the role of the federal government in resolving these issues.



And that parable about the three little pigs? A PR smear spun by “Big Brick” no doubt.
"
"
Share this...FacebookTwitterIn my last post I wrote about China’s aggressive, yet legitimate, expansion of its energy supply. I wonder what the western climate hand-wringers think about the following graphic? Will they ask China about this in Cancun?
My bet is that they’re going to ignore it and focus instead on USA’s and Europe’s emissions.
What I find particularly entertaining is that even if the USA did cut its CO2 emissions by 2o% by 2030 (to about 4.7 million tons annually), Chinese growth would wipe out the savings in just 1o years (2020) or less.
Germany wants to put its entire country through economic hardship to reduce its CO2 emissions by 30% by 2020. That’s a drop of about 250 million tonnes. China’s growth would wipe out the alleged benefit of that in just a matter of months.
And if small countries like Canada, Australia,  Denmark or New Zealand all make their cuts, China’s emissions growth will offset their reductions in a matter of days or hours.
Share this...FacebookTwitter "
"

Nearly three‐​fourths (71%) of Americans believe that political correctness has done more to silence important discussions our society needs to have. A little more than a quarter (28%) instead believe that political correctness has done more to help people avoid offending others.



The consequences are personal—58% of Americans believe the political climate today prevents them from saying things they believe. Democrats are unique, however, in that a slim majority (53%) do not feel the need to self‐​censor. Conversely, strong majorities of Republicans (73%) and independents (58%) say they keep some political beliefs to themselves.





Most Americans (59%) think people should be allowed to express unpopular opinions in public, even those deeply offensive to other people. Forty percent (40%) think government should prevent hate speech in public. Nonetheless, an overwhelming majority (79%) agree that it is “morally unacceptable” to engage in hate speech against racial or religious groups. Thus, the public appears to distinguish between _allowing_ offensive speech and _endorsing_ it.





Despite this, the survey also found Americans willing to censor, regulate, or punish a wide variety of speech and expression they personally find offensive:



An overwhelming majority (82%) of Americans agree that it would be difficult to ban hate speech because people can’t agree what speech is hateful and offensive. Indeed, when presented with specific statements and ideas, Americans can’t agree on what speech is hateful, offensive, or simply a political opinion:





African Americans and Hispanics are more likely than white Americans to believe:



However, black, Hispanic, and white Americans agree that free speech ensures the truth will ultimately prevail (68%, 70%, 66%). Majorities also agree that it would be difficult to ban hate speech since people can’t agree on what hate speech is (59%, 77%, 87%).





Two‐​thirds (66%) of Americans say colleges and universities aren’t doing enough to teach young Americans today about the value of free speech. When asked which is more important, 65% say colleges should “expose students to all types of viewpoints, even if they are offensive or biased against certain groups.” About a third (34%) say colleges should “prohibit offensive speech that is biased against certain groups.”



But Americans are conflicted. Despite their desire for viewpoint diversity, a slim majority (53%) also agree that “colleges have an obligation to protect students from offensive speech and ideas that could create a difficult learning environment.” This share rises to 66% among Democrats, but 57% of Republicans disagree.





More than three‐​fourths (76%) of Americans say that recent campus protests and cancellations of controversial speakers are part of a “broader pattern” of how college students deal with offensive ideas. About a quarter (22%) think these protests and shutdowns are simply isolated incidents.



However, when asked about specific speakers, about half of Americans with college experience think a wide variety should not be allowed to speak at their college:



Excluding a speaker who would disrespect police, Democrats are about 15 to 30 points more likely than Republicans to say each of these speakers should not be allowed to speak.





Two‐​thirds (65%) say colleges need to discipline students who disrupt invited speakers and prevent them from speaking. However, the public is divided on how: 46% want to give students a warning, 31% want the incident noted on the student’s academic record, 22% want students to pay a fine, 20% want students suspended, 19% favor arresting students, and 13% want students fully expelled.  
Democrats take a softer while Republicans take a harder approach to handling disruptive college protestors. Nearly two‐​thirds (64%) of Democrats say colleges should listen to and address the students’ concerns, compared to 36% of Republicans. Conversely, Republicans are two to six times as likely as Democrats to support some sort of punishment for the students, such as suspending or expelling them (47% vs. 15%), noting the incident on the students’ records (41% vs. 22%), or having police arrest the students (32% vs. 7%).





Most people support the heckler’s veto. A majority (58%) say colleges should cancel controversial speakers if administrators believe the students will stage a violent protest otherwise. Democrats and Republicans again disagree: Democrats say universities should cancel the speaker (74%) and Republicans say they should not cancel the speaker (54%) if the students threaten violence. 



The survey finds that many microaggressions that colleges and universities advise faculty and students to avoid aren’t considered offensive by most African Americans and Latinos. The percentage who say these microaggressions are _not offensive_ are as follows:



The one microaggression that African Americans (68%) agree is offensive is telling a racial minority, “you are a credit to your race.” Latinos are evenly divided.





A majority (66%) of Americans have heard of safe spaces, but half or less are familiar with other social justice terms and phrases popular on college campuses today, including: cultural appropriation (50%), trigger warnings (49%), “check your privilege” (48%), microaggressions (43%), and “mansplaining” (41%).



In contrast, strong majorities of current college students and graduate students are familiar with all of these words and phrases: safe spaces (86%), cultural appropriation (76%), trigger warnings (75%), “check your privilege” (77%), microaggressions (66%), and “mansplaining” (69%).



Nearly two‐​thirds (65%) of the public say colleges shouldn’t advise students about offensive Halloween costumes and should instead let students work it out on their own. A third (33%) think it is the responsibility of the university to advise students not to wear costumes that stereotype racial or ethnic groups at off‐​campus parties.



A majority of African Americans (56%) believe universities should intervene and advise against offensive costumes. Conversely, a strong majority (71%) of white Americans and a majority of Latinos (56%) believe that college students should discuss offensive Halloween costumes among themselves without administrator involvement.





Only 20% of current college and graduate students believe their college or university faculty has a balanced mix of political views. A plurality (39%) say most college and university professors are liberal, 27% believe most are politically moderate, and 12% believe most are conservative.



Democratic and Republican students see their college campuses differently. A majority (59%) of Republican college students believe that most faculty members are liberal. In contrast, only 35% of Democratic college students agree most professors are liberal.





A slim majority (51%) of Americans oppose, while nearly as many (48%) support, the idea of a confidential reporting system at colleges and universities in which students could report people who make offensive comments about a person’s race, gender, sexual orientation, age, or disability status.



This “bias reporting system,” as it’s often referred to, is highly popular among current students. More than two‐​thirds (68%) of college students and graduate students support it, while less than a third oppose (30%).



Americans tend to oppose firing people for their beliefs or expression. However, Democrats and Republicans differ on what beliefs or expressive acts they believe are fireable offenses:





A majority of Republicans (63%) agree with President Trump that journalists today are an “enemy of the American people.” Conversely, most Americans (64%), as well as 89% of Democrats and 61% of independents, do not view journalists as the enemy.





Despite this, Republicans (63%) agree with most Americans (70%), including Democrats (76%) and independents (71%), that government should not have the power to stop news stories even if officials say they are biased or inaccurate.



Most Americans believe many major news outlets have a liberal bias, including the _New York Times_ (52%), CNN (50%), and MSNBC (59%).1 Fox News, on the other hand, is perceived to have a conservative bias (56%). Americans are divided about whether CBS is balanced (42%) or has a liberal bias (40%). Local news stations are a rare trusted source. A majority (54%) say their local TV station provides balanced news coverage without bias.





Majorities of Democrats believe most major news organizations are balanced in their reporting, including CBS (72%), CNN (55%), the _New York Times_ (55%), as well as their local news station (67%). A plurality (44%) also believe the _Wall Street Journal_ is balanced. The two exceptions are that a plurality (47%) believe MSNBC has a liberal tilt and a strong majority (71%) say Fox has a conservative bias.



Republicans, on the other hand, see things differently. Overwhelming majorities believe liberal bias colors reporting at the _New York Times_ (80%), CNN (81%), CBS (73%), and MSNBC (80%). A plurality also feel the _Wall Street Journal_ (48%) has a liberal bias. One exception is that a plurality (44%) believe Fox News has a conservative bias, while 41% believe it provides unbiased reporting.





The public distinguishes between a business serving people versus weddings:



Few support punishing businesses who refuse service to same‐​sex weddings. Two‐​thirds (66%) say nothing should happen to a bakery who refuses to bake a cake for a same‐​sex wedding. A fifth (20%) would boycott the bakery. Another 22% think government should sanction the bakery in some way, such as by fining the bakery (12%), requiring an apology (10%), issuing a warning (8%), taking away their business license (6%), or sending the baker to jail (1%).





Nearly two‐​thirds (61%) of Hillary Clinton’s voters agree that it’s “hard” to be friends with Donald Trump’s voters. However, only 34% of Trump’s voters feel the same way about Clinton’s. Instead, nearly two‐​thirds (64%) of Trump voters don’t think it’s difficult to be friends with Clinton voters.





Most Americans (59%) say people should be allowed to express unpopular opinions in public, even those that are deeply offensive to other people. A substantial minority (40%), however, say government should prevent people from engaging in hate speech against certain groups in public.





Racial minorities support government banning public hate speech, including 56% of African Americans and 58% of Latinos. Conversely, a majority of white Americans (66%) oppose banning hate speech. 





While solid majorities of Republicans (72%) and independents (60%) oppose government banning hate speech, Democrats stand out with a slim majority in support (52%). However, African American and Latino Democrats largely drive these numbers. A majority (55%) of white Democrats say government should allow public hate speech, but majorities of black Democrats (59%) and Hispanic Democrats (65%) say it should prevent such speech in public.



Thus, the Democratic Party is divided on matters of free speech. White Democrats are more likely to oppose government regulations on speech while black and Hispanic Democrats are more likely to support it.



Current college and graduate students diverge from Americans who have already graduated from college. About half (49%) of current students say government should ban hate speech while the same proportion (49%) say it should not. In contrast, among college _graduates,_ 64% say hate speech should be legal and a third (36%) say it should not.



Using a political typology to identify ideological groups,2 we find that Libertarians (82%) are the most opposed to hate speech laws, followed by Conservatives (75%) and a slim majority (53%) of Liberals. However, nearly two‐​thirds of Populists (64%) say government should prevent hate speech in public.



Altogether, Hispanic and black Americans, Democrats, women, Populists, and college students are most supportive of the government prohibiting public hate speech. Whites, Republicans, independents, men, and Libertarians are most opposed.



Although most Americans say government should not prevent people from engaging in public hate speech, most think hate speech is morally unacceptable. Nearly 8 in 10 (79%) say that it is “morally unacceptable” to “say things that might be offensive to racial or religious groups.”



This indicates that Americans make a distinction between allowing speech and endorsing that speech. Most think that speech that is offensive or insulting toward minority groups should be legally permitted, but that it is still wrong.



More than 8 in 10 Americans (82%) believe that it’s hard to ban hate speech “because people can’t agree what speech is hateful.” Seventeen percent (17%) disagree.



As a later section will show, Americans are sharply at odds over what speech they would personally define as hateful, offensive, or neither. For instance, a majority of Democrats (52%) believe saying that transgender people have a mental disorder is hate speech. Only 17% of Republicans agree. On the other hand, 42% of Republicans believe it’s hateful to say that the police are racist, while only 19% of Democrats agree.



Majorities across partisan groups, demographic groups, college students, and non‐​college students alike agree that hate speech is hard to define and thus may be hard to regulate.



Most Americans (75%) are aware that making racist statements in public is legal under the First Amendment. However, a substantial minority—24%—think hate speech is currently prohibited by law.



Unsurprisingly, those with less education are more likely to think that hate speech is currently illegal. About a third (32%) of those with high school degrees or less think hate speech is illegal, compared to 19% of college graduates and 13% of those with post‐​graduate degrees.



When asked if Americans might favor banning hate speech against particular groups of people, Americans still oppose such laws. There is, however, relatively more support for banning offensive and insulting speech against African Americans (46%). After that, about 4 in 10 would support banning offensive speech about Jewish Americans (41%), immigrants (40%), armed service members (40%), Hispanics (39%), Muslims (37%), the police (37%), gays, lesbians, and transgender people (36%), and Christians (35%). About a third (32%) would support banning insulting speech about white people.



Interestingly, Democrats favor hate speech protections for some groups more than others. Majorities of Democrats support making it illegal to say offensive or insulting things in public about African Americans (61%) and Jewish Americans (53%). Compared to Republicans, Democrats tend to be more supportive of hate speech laws across the board. Nearly half support hate speech laws for immigrants (49%), gays, lesbians and transgender people (48%), Latinos (46%), and Muslims (45%). About 4 in 10 support such laws for military members (42%), Christians (39%), the police (38%), and a third (33%) support such laws for white Americans.





In contrast, majorities of Republicans tend to more consistently oppose hate speech laws for all the groups included on the survey, with about 3 in 10 in support. However, Republicans are relatively more likely to support banning hate speech against military service members (36%) and the police (36%) but less likely to support such laws for Muslims (25%) and LGBT people (24%).



Racial minority groups are more likely than whites to support hate speech laws for groups across the board, but particularly members of their own racial/​ethnic group. Nevertheless, blacks and Hispanics are more supportive than white Americans of laws banning offensive speech about white Americans as well.



African Americans are most likely to favor a law that bans hate speech against African Americans (62%). Fewer support banning hate speech against Hispanic (53%) and white (41%) Americans.



Latinos are most likely to favor a law that bans hate speech against Latinos (65%). A majority (59%) also favor making offensive speech against African Americans illegal and 47% favor banning hate speech against white Americans.



Whites are comparatively less likely to support banning hate speech against particular racial/​ethnic groups. Nevertheless, whites are most likely to favor a law that bans hate speech against black Americans (39%). A little more than a quarter support banning offensive speech about Latino (28%) and white (26%) Americans.



Hispanic (51%) and black (40%) Americans are also more likely than white Americans (32%) to support making it illegal to say offensive or disrespectful things about the police. This is surprising given that surveys have long shown that African Americans and Latinos view the police more negatively.3 The data reveal that both groups tend to more consistently support laws that restrict offensive public speech about any group, not just some groups.



As one 26‐​year‐​old Hispanic female further explained, “we are all human beings, we must all respect each other equally.” Similarly, a 31‐​year‐​old black male in the survey explained that he supported hate speech laws not only for African Americans “but it should be for everybody because it will stop the hate.”



Women are more likely than men to support hate speech laws for different racial, religious, and other groups—particularly for African Americans.



A majority of women (57%) favor a law that would make it illegal to say offensive or insulting things about African Americans in public while 43% oppose. In contrast, only 36% of men would similarly favor this law while 64% would oppose it.



Majorities of women oppose similar laws for other groups. However, they are about 15 points more likely than men to favor banning hate speech against immigrants (46% vs. 33%), gays, lesbians, and transgender people (45% vs. 27%), the police (45% vs. 28%), Hispanics (45% vs. 30%), Muslims (44% vs. 30%), Jewish people (45% vs. 35%), and Christians (43% vs. 27%).



Besides slurs and biological racism, Americans are strikingly at odds over what speech and ideas constitute hate.4





First, majorities agree that calling a racial minority a racial slur (61%), saying one race is genetically superior to another (57%), or calling gays and lesbians vulgar names (56%) is not just offensive—but is hate speech. Interestingly a majority do not think calling a woman a vulgar name is hateful (43%), but most would say it’s offensive (51%). Less than half believe it’s hateful to say that all white people are racist (40%), transgender people have a mental disorder (35%), America is an evil country (34%), homosexuality is a sin (28%), the police are racist (27%), or illegal immigrants should be deported (24%). Less than a fifth believe it’s hateful to say Islam is taking over Europe (18%) or that women should not fight in military combat roles (15%).



Liberals and conservatives significantly diverge over what speech they define as hateful, offensive, or simply an opinion. (See Appendix B).



Majorities of Americans agree with liberals that slurs and biological racism are hateful. However, majorities do not agree with liberals that it’s hateful to say “transgender people have a mental disorder” (35% of all Americans vs. 59% of liberals) or to call women a vulgar name (43% vs. 54%).



Strikingly, majorities of conservatives don’t think any of these ideas are “hateful” although most consider them “offensive” or hateful. In fact, conservatives are about 40 points less likely than liberals to think that saying transgender people have a mental disorder (17% vs. 59%) or saying racial slurs (43% vs. 81%) are hateful. While strong majorities of conservatives agree these are at least _offensive_ or hateful, they are less likely to equate these phrases and ideas with hate specifically.



Liberals are also more likely than conservatives to view a variety of political opinions and speech as either _offensive_ or hateful.





Liberals are more than 40 points more likely than conservatives to think it is offensive or hateful for a person to say that homosexuality is a sin (90% vs. 47%), women shouldn’t fight in military combat roles (87% vs. 47%), illegal immigrants should be deported (80% vs. 36%), or Islam is taking over Europe (79% vs. 33%). Not even a majority of conservatives find these statements to be offensive or hateful.



Notice that two of these, women fighting in combat roles and deporting illegal immigrants, are _policy positions_ that a substantial number of Americans hold. Yet, to merely express these as political positions would also be viewed as highly offensive to a large share of the population.



Furthermore, President Trump has explicitly advocated for deporting illegal immigrants during his 2016 presidential campaign.5 Thus, a large share of Americans not only disagree with his policy position but also find it highly offensive if not hateful. 



Majorities of conservatives did not find any of the statements included on the survey hateful. However, they were _more likely_ than liberals to find several statements hateful. First, conservatives are about twice as likely as liberals to think it’s hateful to say the police are racist (39% vs. 17%). Second, conservatives are somewhat more likely to believe it’s hateful to say that America is an evil country (39% vs. 29%). Third, conservatives are somewhat more likely than liberals to think it’s hateful to say that all white people are racist (44% vs. 35%).



Most Americans (68%) do not think it’s morally acceptable to use physical violence against Nazis, while 32% say it is morally acceptable.6



However, strong liberals stand out with a slim majority (51%) who say it’s moral to punch Nazis in the face. Only 21% of strong conservatives agree. The survey found liberals were more likely to consider upsetting and controversial ideas “hateful” rather than simply “offensive.” This may help partially explain why staunch liberals are more comfortable than the average American with using violence against Nazis.



Strong liberals’ approval of Nazi‐​punching is not representative of Democrats as a whole. A majority (56%) of Democrats believe it is not morally acceptable to punch a Nazi. Thus, tolerance of violence as a response to offensive speech and ideas is found primarily on the far Left of the Democratic Party.





Approval for punching Nazis also varies with age and race. Millennials (42%) are nearly twice as likely as people over 55 (24%) to say violence is morally justified. African Americans (45%) are also 17 points more likely than whites (28%) and 10 points more likely than Latinos (35%) to say punching Nazis is morally acceptable. Nevertheless, majorities of each of these groups say physical force is not justified, even against a Nazi.



Nearly two‐​thirds (64%) of Americans oppose a law that would make it illegal to deny that the Holocaust happened. About a third (35%) would support banning Holocaust denial. These results put Americans at odds with a number of European countries that have outlawed denying the historicity of the Holocaust.7



Support for banning Holocaust denial varies with ideology. A plurality (50%) of strong liberals support such a law, followed by 43% of liberals, 33% of moderates, 30% of conservatives, and 26% of strong conservatives.



A slim majority (54%) of Americans oppose a law that would ban making sexually explicit statements in public, while 45% would oppose.



Although majorities of Democrats (52%), Republicans (55%), and independents (57%) all oppose such a law, certain demographics would support it.



Women (54%) are more than 20 points more likely than men (36%) to support banning sexually explicit public statements. Hispanics (55%) and African Americans (50%) are also somewhat more likely than white Americans (41%) to support such a ban.



Church attendance also predicts support for banning sexually explicit public statements. A slim majority (52%) of regular churchgoers support such a law, but support declines as church attendance declines. A majority (55%) of those who seldom attend church and nearly two‐​thirds (63%) of those who never attend oppose a ban.



Libertarians (67%) and Liberals (64%) are most opposed to banning sexually explicit language in public. 8 Conservatives also marginally oppose (54%). But Populists stand out, with a majority (57%) who say we should outlaw explicit statements. (See Appendix A).



Americans oppose legal restrictions on hate speech, Holocaust denial, and sexually explicit public statements. However, nearly two‐​thirds (62%) would support a law making it illegal to call for violent protests. A little more than a third (37%) would oppose this law.



Outlawing public calls to violently protest is not controversial. Solid majorities of partisans and demographic groups support prohibiting this type of public speech.



Nearly 6 in 10 liberals (59%) favor a law that would require people to refer to transgender persons by their preferred gender pronouns, not their biological sex. This is in sharp contrast to what Americans overall support. Nearly two‐​thirds (62%) oppose a law requiring people use certain pronouns for transgender people while 37% would support it. Moderates (60%) and conservatives (82%) are highly opposed to such laws, including 59% of conservatives who _strongly_ oppose.





These results are relevant to the cities and states that are moving to fine or jail businesses and landlords who refuse to use transgender people’s preferred pronouns. For instance, California enacted a new law that punishes long‐​term nursing home care staff who refuse to use a resident’s preferred name or pronouns.9 Or in New York City, new regulatory guidance subjects landlords and businesses to fines for refusing to use transgender employees’, customers’, or tenants’ preferred pronouns.10 Americans overall, however, do not support these laws.



While Democrats are more supportive of censorship when it comes to hate speech, Republicans disdain criticizing patriotic symbols like the American flag.



A majority (53%) of Republicans favor stripping a person of their U.S. citizenship if they burn the American flag, while 47% would oppose. These results fit with President Trump’s tweets soon after his presidential election victory in which he called for a “loss of citizenship” to punish flag burning.11





While aligned with Trump, Republicans are out of step with the mainstream: 61% of Americans don’t think we should strip people of their citizenship for flag burning. Thirty‐​nine percent (39%) think revoking a person’s citizenship is a reasonable response to flag burning.



A strong majority of Democrats (71%) and independents (61%) oppose such a proposal. Nevertheless, a non‐​insignificant minority of Democrats (28%) and independents (38%) support stripping citizenship from a flag burner.



Latinos align most with Republicans on this issue: 49% agree flag burners should have their citizenship revoked. Latinos are 22 points more likely than African Americans (27%) and 10 points more likely than white Americans (39%) to support such a policy.



Support for revoking citizenship steadily declines with education. While nearly half (48%) of those with high school degrees or less agree with President Trump, only 29% of college graduates and 20% of post‐​grads agree.



Although the Supreme Court has ruled that flag burning is protected speech under the First Amendment, a majority (58%) of Americans still favor a law banning it while 42% oppose.



Majorities of Republicans (72%) and independents (60%) also favor making it illegal to burn or desecrate the flag. Democrats stand out, with a slim majority (53%) who oppose a flag burning ban.



Hispanic Americans are most in favor (63%) of a ban on flag burning, followed by white Americans (58%). African Americans are divided, with 50% in favor and 49% opposed. Women are also more likely than men to support such a ban (65% vs. 50%).



Libertarians (56%) and Liberals (62%) stand out in opposition to a flag burning ban. 12 In contrast, nearly three‐​fourths of Conservatives (74%) and Populists (69%) support it. (See Appendix A).



In this section, the survey report investigates the public’s assumptions about how free speech operates. We find that Americans believe free speech has both benefits and costs. First, nearly two‐​thirds (67%) think that “freedom of speech ensures the truth will ultimately win out” and 58% say free speech does more to protect minority viewpoints. But also, most believe that speech can turn violent: 53% say hate speech _is_ an act of violence and even more say that hate speech _leads to_ violence against minority groups (70%). Ultimately, a majority (56%) think it’s possible to both ban hate speech and still protect free speech.



There are wide racial and partisan divides over how people think free speech operates. Democrats, African Americans, and Latinos are more likely than Republicans and white Americans to believe that hate speech is violent and allows majority views to crowd out minority viewpoints, that supporting a racist’s free speech right is as bad as being a racist, that people who offend others with their ideas have bad intentions, and that we can simultaneously ban hate speech and protect free speech.





Americans provide a strong endorsement of free speech with 67% who agree that “free speech ensures the truth will ultimately win out.” About a third (32%) do not believe that truth can prevail with the free exchange of ideas.



This concept is non‐​controversial with strong majorities of political partisans and demographic groups who share this belief. However, strong liberals (42%) are more likely than moderates (31%) and strong conservatives (25%) to lack this confidence in free speech.



Most Americans (58%) believe that free speech does more to protect minority viewpoints rather than those of the majority. However, African Americans stand out, with 59% who believe free speech does more to protect majority opinions, rather than views held by a minority of individuals. Nearly two‐​thirds (64%) of white Americans believe free speech primarily protects minority views. Latinos are evenly divided on this question.





Majorities of Democrats (53%), independents (57%), and Republicans (66%) agree that free speech does more to allow for and protect minority views.



However, the Democratic Party is divided. Six in 10 black Democrats believe free speech allows the majority to crowd out minority views, while 6 in 10 white Democrats believe it primarily protects minority views. Latino Democrats are divided with 51% who think free speech primarily protects majority opinions.



College protests of controversial speakers across the country have elevated an idea that deeply offensive speech is like an act of violence.13 A slim majority of Americans appear to endorse these sentiments: 53% say that “hate speech is an act of violence.” Another 46% do not believe that hate speech is violence.



Equating speech with violence is highly controversial and sharply divides Americans by political ideology, race, gender, and age.



While two‐​thirds (66%) of Democrats say hate speech is violence, 58% of Republicans say hate speech is _not_ violence. Independents are split, with 51% who disagree hate speech is tantamount to violence.





African Americans (75%) and Latinos (72%) are nearly 30 points more likely than white Americans (46%) to believe hate speech is violence. Instead, a slim majority (53%) of white Americans believe it is not.



While nearly two‐​thirds (63%) of women believe hate speech is violence, a majority (56%) of men disagree.



Americans under 30 (60%) and seniors (57%) are also more likely than middle‐​aged Americans (35–64) to believe hate speech is violence (49%).



These differences may partially explain why Democrats, students, African Americans, Latinos, and women are more supportive of hate speech laws. Equating hate speech with violence provides a greater justification for restricting it.



One reason why Americans may believe hate speech is violence is that a majority (70%) believe that “hate speech leads to violence against minority groups.” This is a view shared by a majority of partisans and racial/​ethnic groups. Nevertheless Democrats (89%), African Americans (85%), and Hispanic Americans (79%) are more likely to believe this than independents (60%), Republicans (54%), and white Americans (66%).



A variety of campus protestors and social justice activists have argued that society can prohibit hate speech while still protecting Americans’ First Amendment rights to free speech. As Scott Crow, a former Antifa organizer put it, “hate speech is not free speech.“14 Similarly, a widely circulated Wellesley College newspaper staff editorial argued that “shutting down rhetoric that undermines the existence and rights of others” is “not a violation of free speech” because such rhetoric is “hate speech.“15



The survey finds that a majority (56%) of Americans agree with the idea that “society can prohibit hate speech and still protect free speech.” Forty‐​three percent (43%) disagree that society can simultaneously prohibit hate speech and protect free speech.





The idea that society can have both hate speech bans and uphold the First Amendment divides partisans and demographic groups. A majority of Democrats (64%) and independents (54%) think it’s possible. A slim majority (52%) of Republicans think it’s not.



Strong majorities of African Americans (69%), Latinos (71%), and women (64%) believe society can both protect free speech and ban hate speech, but white Americans and men are evenly divided. Current college students and graduate students (62%) are also more likely than college graduates (47%) to believe this can be done.



Nearly two‐​thirds of African Americans (65%) and Latinos (61%) agree that “supporting someone’s right to say racist things is as bad as holding racist views yourself.” About a third (34%) of white Americans agree. This suggests that Americans of color may not believe people are reasoning in good faith when they say we should _allow_ speech even if we strongly _disagree_ with it.





This perspective was on full display at the College of William and Mary when student protestors recently prevented an invited ACLU affiliate from speaking at an event, “Students and the First Amendment.” Protestors explained this was in retaliation for the ACLU’s defense of white nationalists’ free speech rights.16 The Black Lives Matter of William and Mary student group wrote on their Facebook page, where they live streamed their shut down of the event: “We want to reaffirm our position of zero tolerance for white supremacy no matter what form it decides to masquerade in.“17 From these students’ perspective, the ACLU supporting someone’s right to say racist things was as bad as being a racist organization.



Most Democrats (53%) also believe supporting a racist’s free speech rights is as bad as holding racist views. However, the Democratic Party is divided by race. While 72% of black Democrats and 65% of Latino Democrats believe this, only 42% of white Democrats agree. Instead, a majority (57%) of white Democrats don’t believe supporting a racist’s right to free speech is the same as supporting racism. Majorities of independents (57%) and Republicans (66%) agree.



Men and women also disagree about whether supporting the right to speak is the same as endorsing its content. Nearly two‐​thirds (65%) of men don’t believe supporting free speech rights is the same as supporting the speech’s content. But a slim majority (51%) of women believe that it is.



A slim majority (51%) of current college students and graduate students believe a person doesn’t deserve the right of free speech if they don’t respect other people. In contrast, a majority (55%) of Americans overall don’t think a person should lose their free speech rights even if they don’t respect others.



There is also a wide race gap here. Six in 10 African Americans (59%) and Hispanics (62%) believe people don’t deserve the right of free speech if they don’t respect others, compared to 36% of white Americans. Instead a majority (62%) of white Americans think even disrespectful people should retain their free speech rights.



A majority (58%) of Americans believe people “usually have bad intentions when they express offensive opinions.” Forty‐​one percent (41%) disagree that people who offend others with their ideas usually have nefarious motives.



Democrats (69%) are 22 points more likely than Republicans (47%) to believe that people have bad intentions when they express offensive opinions. Instead, most Republicans (52%) think people may mean well even when they share an opinion others find offensive.



Latinos (75%) and African Americans (70%) are also about 20 points more likely than white Americans (52%) to think people usually have bad intentions when expressing offensive ideas.



Populists and Liberals are the most likely to believe (67%) that people who express offensive opinions have nefarious motives.18 Libertarians are the polar opposite, with 67% who do _not_ think offensive ideas imply hurtful intentions. Conservatives are evenly divided.



On the campaign trail, then‐​candidate Donald Trump contended: “I think the big problem this country has is being politically correct.“19 A strong majority of Americans (70%) agree with this sentiment. Even though the survey did not attribute the quote to President Trump, fully 90% of Republicans and 78% of independents agree. Democrats are evenly divided.



Why do many people believe political correctness is a problem? Why do others believe it is necessary? Nearly three‐​fourths (71%) of Americans say that political correctness has done more to silence important discussions our society needs to have. Conversely, a little more than a quarter (28%) think that political correctness does more to help people avoid offending others.



Strong majorities of white Americans (74%), African Americans (64%), and Latinos (58%) agree that political correctness has silenced necessary conversations. Overwhelming majorities of Republicans (89%) and independents (80%) also agree.



Far fewer Democrats believe political correctness has done more to silence necessary discussions (50%) than reduce offense. Liberal Democrats are driving these numbers. More than two‐​thirds (68%) of strong liberals believe political correctness primarily helps reduce offense. In stark contrast, nearly 9 in 10 strong conservatives (87%) say it primarily silences conversations society needs.



Most Americans self‐​censor their political opinions because they’re afraid they might offend someone. Nearly 6 in 10 (58%) report that the “political climate” these days prevents them from saying what they believe “because others might find them offensive.” Four in 10 don’t feel the need to censor their opinions.



The political climate appears to favor liberal Democrats, as they are among the few groups who feel they _do not_ need to censor their opinions. However most other political and demographic groups do self‐​censor.



Strong liberals are the most comfortable sharing their true beliefs (69%). Far fewer strong conservatives (24%) and moderates (41%) agree. Similarly, Democrats (53%) are more likely than Republicans (26%) and independents (39%) to feel they can express their opinions. Instead, nearly three‐​fourths (73%) of Republicans and 58% of independents are afraid to share some of their true beliefs because of the political climate.



Why are Republicans more afraid than Democrats to share their views in this “political climate” given that Republicans currently control both Congress and the White House? Perhaps political power does not solely determine the political climate. Cultural sources of power, such as media, academia, and entertainment may matter more. The survey found that Americans believe most large media outlets, like the _New York Times_ (52%) and CNN (50%), have a liberal bent. A plurality (45%) also believe college faculties are mostly liberal. These institutions may shape the political environment such that liberals feel more comfortable sharing their political views.



But perhaps, one might argue, liberals feel more comfortable sharing their political opinions because their views are less offensive. However, the survey found several instances where conservatives are more offended than liberals by political views more commonly held among liberals. For instance, conservatives are about twice as likely as liberals to say calling the police racist is hate speech (39% vs. 17%). Conservatives are also somewhat more likely to believe it’s hateful to say that America is an evil country (39% vs. 29%). Conservatives are also more offended than liberals by flag burning and NFL players refusing to stand for the national anthem.



There are certain topics that Americans feel less inclined to discuss with others in their social surroundings, such as over dinner with co‐​workers or with classmates.



In such an environment, less than half of Americans would be “very willing” to discuss gay and lesbian issues (45%), race relations (45%), women’s issues (48%), and foreign policy (48%).20 Only about half would be similarly willing to discuss issues related to immigration (51%), the police (51%), abortion (52%), or poverty (53%). Americans are somewhat more willing to discuss the environment (64%), health care (61%), education (60%), crime (58%), and gun issues (56%).



There are some issues Democrats feel more comfortable discussing than Republicans and vice versa. Compared to Republicans, Democrats are more likely to say they’d be very willing to discuss women’s issues (57% vs. 41%), gay and lesbian issues (52% vs. 37%), poverty (57% vs. 47%), race relations (50% vs. 40%), and the environment (69% vs. 62%). Conversely, Republicans feel relatively more comfortable than Democrats talking about crime (63% vs. 54%) and gun issues (60% vs. 52%). Across the board, however, Democrats are more willing than Republicans to discuss major policy issues.



The survey also asked respondents to use their own words to describe political beliefs they hold, but feel unable to share because of the political climate. Even though Democrats are more likely than Republicans to feel comfortable sharing their opinions, Americans of all political stripes have views they censor. A sampling of these opinions can be found in the box “Dangerous Ideas vs. Approved Beliefs” below.



Liberals, particularly those in conservative areas, feel they can’t express secular beliefs, their dislike of Donald Trump, support for immigration, gun control, police reform, ending the Drug War, and LGBT rights, and a belief that racism continues in America today.



Conservatives, particularly those in liberal areas, feel they can’t share their religious beliefs, support for Trump, patriotism, a belief that racial minorities receive special privileges in society, opposition to illegal immigration, affirmative action, same‐​sex marriage, and abortion, and support for the border wall, gun rights, free speech, deportation of unauthorized immigrants, and more rigorous security screening for Muslims entering the United States.



Notably, liberals also self‐​censored conventionally conservative sentiments. These included: indifference to identity politics, a belief that racial minorities receive favoritism, support for free speech, and opposition to “PC culture” and removing Confederate statues.







Even though Democrats are more likely than Republicans to feel comfortable sharing their opinions, Americans of all political stripes have political views they feel can’t be expressed. The survey asked people to use their own words to describe what views they feel can’t be shared. Location matters a lot. Liberals in conservative areas and conservatives in liberal cities both self‐​censor.21



Nearly two‐​thirds (61%) of Americans believe that “people often call others racist or sexist to avoid having to debate with them.” More than a third (37%), however, say “people usually only call someone out for racism or sexism when they deserve it.”





A slim majority (51%) of Democrats believe that calling out racism or sexism is typically justified and not an avoidance tactic. In sharp contrast, about three‐​fourths (76%) of Republicans and two‐​thirds (65%) of independents believe it’s primarily used as a tool to stifle debate.



A majority (58%) of African Americans believe that a person called out for racism or sexism usually deserves it, while 41% think that such labels are often used to avoid discussion. Whites (66%) and Latinos (55%) are 14–25 points more likely to believe these labels are primarily used to suppress debate.





Nearly two‐​thirds (61%) of Clinton voters agree that “it’s hard to be friends with people who voted for Donald Trump” while 38% disagree. Trump voters don’t feel a similar animus toward Clinton voters. Instead, a majority (64%) of Trump voters do not think that it’s hard to be friends with Clinton voters while 34% believe it is difficult.





Two‐​thirds of Americans (66%) say colleges and universities aren’t doing enough today to teach young Americans about the value of free speech. This is a view shared by 51% of current college and graduate students, while 46% think colleges are doing enough.



When asked which is more important, 65% say colleges should expose students to “all types of viewpoints even if they are offensive or biased against certain groups.” About a third (34%) say colleges should “prohibit offensive speech that is biased against certain groups.”





Strong liberals (52%), African Americans (54%), and Latinos (54%) stand out with slim majorities who believe it’s more important for colleges to prohibit offensive and biased speech on campus. Conversely, majorities of regular liberals (66%), conservatives (73%), and white Americans (73%) think colleges need to expose students to a wide variety of perspectives even if they are offensive or prejudiced.



But Americans are conflicted. While most say colleges need to prioritize viewpoint diversity, a slim majority (53%) also agree colleges have “an obligation to protect students from offensive speech and ideas that could create a difficult learning environment.” Problems arise, as evidenced earlier in the report, when students disagree about what speech is offensive and would create a difficult learning environment.



Americans are divided by race, party, gender, and education. Nearly three‐​fourths of Latinos and African Americans (74%) agree colleges need to protect students from offensive ideas that could disrupt the learning environment. Less than half (44%) of white Americans agree. While a solid majority of Democrats (66%) believe colleges have this obligation, majorities of Republicans (57%) and independents (51%) do not believe colleges should do this.





Men and women are also divided. A majority of men (56%) don’t think colleges should protect their students from offensive ideas while 64% of women think colleges should.



With more education, Americans become more averse to colleges shielding students from offensive speech even if it risks disrupting the learning environment. Six in 10 Americans (61%) with high school degrees or less think colleges should protect students from offensive ideas, compared to 44% of those with college degrees and 37% of post‐​graduates.



Although Americans say it’s more important for colleges to expose students to a variety of diverse viewpoints, even offensive ones, many are willing to shut down speech they personally find offensive. About half of Americans who have college experience don’t think a wide variety of speakers should be allowed to speak at their university.22





An overwhelming share (81%) of respondents with college experience agree that campus speakers who advocate for violent protests shouldn’t be allowed to speak at their university. Nearly two‐​thirds (65%) oppose a speaker who would reveal the names and identities of unauthorized immigrants attending the college. A solid majority (57%) would also oppose allowing any speaker who says the Holocaust did not occur. About half would oppose allowing a speaker who says all white people are racist (51%), that Muslims shouldn’t be allowed to come to the U.S. (50%), that transgender people have a mental disorder (50%), or that gays and lesbians should receive conversion therapy (50%). Nearly half would support cancelling a speaker who says all Christians are backward and brainwashed (49%), who publicly criticizes or disrespects the police (49%), who defends the police stopping African Americans at higher rates than other groups (48%), or says the average IQ of whites and Asians is higher than African Americans and Hispanics (48%), says all illegal immigrants should be deported (41%), or says men on average are better at math than women (40%). (Results are similar among Americans without college experience who were asked if the aforementioned speakers should be allowed to speak in their local community. (See Appendix C.))



The reader may notice that most of these hypothetical speakers are taken from real‐​world examples of controversial campus speakers or other public figures who could be invited to speak on a college campus. (Note that several of these campus speakers were not shut down because of controversial ideas they planned to include in their speech but for things they have said in the past.) If campus presidents agreed to cancel speakers that large numbers of their student body and faculty found offensive, these results imply they would have to prohibit a wide range of speakers including:



Major differences emerge between Democrats and Republicans in their willingness to allow controversial and offensive speakers speak on campus. Even on issues in which one might expect Republicans to be more offended, they were less likely than Democrats to support cancelling the speaker. Majorities of Democrats would not allow, while Republicans would allow, a speaker who:



There is also a wide racial gap between white Americans and black and Hispanic Americans in allowing these speakers to come to campus. Majorities of black and Hispanic Americans would not allow, while white Americans would allow, a speaker who:



Majorities of black, white, and Hispanic Americans all oppose allowing a speaker who would reveal the names of unauthorized immigrants on campus, deny the Holocaust, or call for violent protests.





Men and women are similarly divided, with majorities of men supportive of nearly all these speakers being allowed to speak on campus and women opposed. Young Americans are also more averse to allowing these speakers to speak at their college or university, compared to older Americans.



Taken together, Republicans, white Americans, men, and older people are more supportive than Democrats, African Americans, Latinos, women, and younger people of allowing these campus speakers to speak at their college or university. Why are these latter groups more supportive of censoring speech? Perhaps because they are more likely to believe that colleges have an obligation to protect students from offensive ideas.



About two‐​thirds (64%) of current college and graduate students say that if their college or university hosted a speaker who believes some races are superior to others, they would not attend the speech. Sixteen percent (16%) say they would attend the speech. Many would also take action: 43% would attend the speech and ask the speaker tough questions, 39% would hold a counter‐​event in a different location, 26% would hold a protest outside of the speech location.



Notably, few students would try to forcibly shut down the speech by shouting loudly so the speaker cannot speak (7%) or by forcibly removing the speaker from the stage (7%).33 Although most wouldn’t use shouting or physical force to stop an offensive speech, more than a third (36%) would sign a petition to get the speech cancelled beforehand.



Democratic and Republican students say they’d handle the situation differently. Democratic students are more likely than Republicans to say they’d hold a counter‐​event in a different location (50% vs. 33%), protest outside (38% vs. 15%), or sign a petition beforehand to get the speech canceled (48% vs. 22%). On the other hand, Republican students are somewhat more likely to say they’d attend the speech and ask tough questions (53% vs. 44%) or simply attend the speech (25% vs. 15%).



More than three‐​fourths (76%) of Americans say that recent student protests and cancellations of controversial speakers on college campuses are part of a “broader pattern” of how college students respond to controversial ideas. About a quarter (22%) believe these protests and cancellations are isolated incidents, not indications of a broader pattern.



This perception is not controversial. Strong majorities of current students and non‐​students alike believe recent shut downs of campus speakers tell us something broader about how students deal with offensive ideas.



Nearly two‐​thirds (65%) of Americans say that colleges and universities should discipline college students who disrupt invited campus speakers and prevent them from speaking.



Republicans are most likely to support disciplining students (83%); 67% of independents agree. Democrats on the other hand are evenly divided over whether colleges should punish students who shut down speakers (50%). White Americans (71%) are also more likely than Latinos (51%) and African Americans (49%) to support disciplining these students.



When asked how specifically colleges and universities should handle disruptive college protestors, Americans are less resolute. A plurality (50%) say that first, colleges and universities should listen and address the students’ concerns. After that, 46% want colleges to give students a warning, 31% say colleges should note the incident on the students’ records, 22% say students should pay a fine, 20% say colleges should suspend students for 30 days, 19% want the police to arrest the students, 13% want colleges to completely expel the students, 11% want to suspend students for a semester. Only 6% say colleges should do nothing.





Democrats take a softer while Republicans take a harder approach to handling disruptive college protestors. Nearly two‐​thirds (64%) of Democrats say colleges should listen to and address the students’ concerns, compared to 36% of Republicans who agree. Conversely, Republicans are two to six times as likely as Democrats to support some sort of punishment for the students, such as noting the incident on the students’ records (41% vs. 22%); supporting suspending or expelling the students (47% vs. 15%); or having police arrest the students (32% vs. 7%).



Ultimately 75% of Republicans would impose at least one of the listed punishments, compared to less than half (42%) of Democrats. Most Democrats would rather listen and address the students’ concerns or give them a warning. Given that research shows most of academia leans left of center, this might help explain why few universities have punished students who have shut down controversial campus speakers.34



Most Americans would accede to the heckler’s veto. A solid majority (58%) of Americans think college administrators should cancel controversial invited campus speakers if students threaten to stage a violent protest. Four in 10 think colleges should move forward with the invited speaker regardless.



Democrats and Republicans disagree about how to respond to threats of student violence: 74% of Democrats think colleges should cancel such controversial speakers while 54% of Republicans think colleges should not cancel the speech.



A slim majority of men (51%) believe colleges should resist student threats. Conversely, more than two‐​thirds (67%) of women think colleges should cancel speakers if students threaten violent protest.



  
  
  
  
  
  
  




A slim majority (51%) of Americans oppose while nearly as many (48%) support the idea of a confidential reporting system at colleges through which students could report people who make offensive comments about a person’s race, gender, sexual orientation, age, or disability status.



This “bias reporting system,” as it’s often described, is highly popular among current students. More than two‐​thirds (68%) of current college students and graduate students support it while less than a third oppose (30%). However, 63% of those who have already graduated from college oppose a system to allow students to report bias on campus.



A bias reporting system is highly divisive along partisan and demographic lines. Solid majorities of Democrats (60%), African Americans (67%), Latinos (59%), and women (54%) support it. Conversely, majorities of Republicans (64%), white Americans (57%), and men (58%) oppose it.



The survey finds that many microaggressions that colleges and universities advise faculty and students to avoid aren’t considered offensive by most people of color.35 The survey included a variety of statements that major universities have identified should be avoided because the colleges contend they “communicate hostile, derogatory, or negative messages to target persons based solely upon their marginalized group membership.“36 However, most African Americans and Latinos do not find most of these statements offensive.





Strong majorities of African Americans and Latinos say the following statements are _not offensive_ :



Seventy percent (70%) of Asian Americans do not think it’s offensive to ask an Asian person, “where are you from?” (The sample size for Asian Americans is small and thus their responses are not shown separately for each of these microaggressions.)37



The one microaggression that African Americans (68%) agree is offensive is telling a racial minority “you are a credit to your race.” Latinos are evenly divided on this question.



There may be other microaggressions not included on the survey that these groups find derogatory. However, African Americans and Latinos do not find most of the key microaggressions identified in academic training manuals insulting.



Two years ago at Yale, a controversy erupted over a series of emails about offensive Halloween costumes. A resident advisor and Yale lecturer pushed back against an email from college administrators advising students not to wear offensive Halloween costumes. The advisor emailed her students and expressed confidence in students’ capacity to discuss offensive Halloween costumes among themselves without administrators getting involved. Many students interpreted her email as an endorsement of offensive costumes, rather than of freedom of expression and the ability of people to discuss and resolve offense without oversight. What do Americans think?



The survey finds that nearly two‐​thirds (65%) of Americans agree that “college students should discuss offensive costumes among themselves without administrators getting involved.” A third (33%) say “college administrators have a responsibility to advise college students not to wear Halloween costumes that stereotype certain racial or ethnic groups at off‐​campus parties.”



A significant racial divide emerges about how to handle offensive Halloween costumes. A majority (56%) of African Americans feel college administrators should intervene and advise students against offensive costumes. Conversely, a strong majority (71%) of white Americans and a majority of Latinos (56%) believe that college students should discuss offensive Halloween costumes among themselves without administrator intervention.



A majority (54%) of college and graduate students agree that students should discuss offensive costumes without intervention from school authorities. However, students (45%) are 12 points more supportive than Americans overall (33%) of administrators advising about offensive costumes.



About two‐​thirds to three‐​fourths of college students and graduate students are familiar with the new language of social justice terms and phrases that have emerged on college campuses. However, most Americans overall are unfamiliar with these words and phrases. The one exception is “safe spaces,” which two‐​thirds of the general public and 86% of current students have heard something about them.





Most Americans (55%) and current college and graduate students (55%) say college newspapers should not need approval from college administrators before printing controversial news stories and editorials. However, nearly two‐​thirds of African Americans (63%) and a majority of Hispanic Americans (54%) think student papers should get approval before printing controversial stories. In contrast, 61% of white Americans don’t think student papers should need approval.



Similar majorities of Democrats (56%), independents (55%), and Republicans (54%) oppose requiring that student papers get permission before printing controversial stories. However, Democrats are divided along racial lines. More than two‐​thirds (68%) of white Democrats do not believe such permission should be necessary while 65% of black Democrats and 57% of Hispanic Democrats believe it should be.



Men and women are also divided. Nearly two‐​thirds of men (63%) do not believe controversial news stories in student papers should need approval while 51% of women think they should.



 **The Faculty** Only 20% of current college students and graduate students believe their college or university faculty has a balanced mix of political views. A plurality (39%) of current students agree that most college and university professors are liberal. Twenty‐​seven percent (27%) believe most are politically moderate, and 12% believe most are conservative.



Democratic and Republican college students see their campuses very differently. A majority (59%) of Republican college students believe that most faculty members are liberal. In contrast, Democratic college students are 25 points less likely to believe that most of the faculty is liberal (35%). Democratic students are also about twice as likely as Republican students to think their professors are moderate (32% vs. 16%) or conservative (14% vs. 9%).



 **The Students** Current students believe that most of their campus’ student body is liberal. Fifty‐​percent (50%) believe that most students at their college or university are liberal, 21% believe most are moderate, 8% believe most are conservative, and 19% believe there is a balanced mix of political views. Democratic and Republican students largely agree on the ideological composition of their campus student body. 



In sum, there is a widespread perception that most faculty and students in colleges are liberal. These results matter because if universities become political echo chambers, it could lead to the exclusion of non‐​conforming political views, self‐​censorship, and less rigorous academic inquiry. Without a free exchange of ideas, there may be less thorough checking of academic work and the quality of research may decline. By extension, the public may lose confidence in the process of academic inquiry and become skeptical of its results.



Although many Americans favor silencing offensive speakers on college campuses and in local communities, most oppose firing people for their political beliefs or expression.



Nearly two‐​thirds (61%) of Americans oppose firing NFL (National Football League) players who refuse to stand for the national anthem before football games in order to make a political statement. These results stand in contrast to President Trump’s urging NFL teams to fire players who refuse to stand for the anthem. A little over a third (38%) of Americans align with Trump and support firing these players.38





Conservative Republicans stand out with their support for firing NFL players who refuse to stand for the national anthem. Nearly two‐​thirds (65%) of Republicans say NFL players should be fired for this reason. Only 19% of Democrats and 35% of independents agree. Punishing NFL players for their political speech distinguishes political Conservatives from Libertarians. Using a political typology to identify these ideological groups, the survey finds that Conservatives (62%) are the only political group to support firing NFL players. Conversely, 60% of Libertarians, 85% of Liberals, and 62% of Populists all oppose firing players. 39



People who are older, with less education, and living in smaller towns and rural communities are most likely to support punishing players who refuse to stand for the national anthem.



A majority (57%) of Americans over 65 think such players should be fired while 71% of Americans under 30 think they should not. Those without college degrees (44%) are more likely than college graduates (32%) and those with post‐​graduate degrees (26%) to support punishing NFL players who engage in this form of political protest. Americans living in rural communities are divided equally over whether teams should fire NFL players who refuse to stand for the national anthem. Conversely, those living in large urban centers solidly oppose (69%) such firings.



Majorities across racial groups oppose firing NFL players who kneel during the national anthem before football games. However African Americans (88%) are about 30 points more likely than Hispanic Americans (60%) and white Americans (55%) to oppose. 



Not wanting to fire NFL players because of their political expression doesn’t mean that most people necessarily agree with the content of that expression. As surveys have long found, including this one, the public opposes desecrating or disrespecting patriotic symbols, like the American flag. It’s likely such views extend to the national anthem as well. Thus, many appear to make a distinction between allowing expression and endorsing its content. Americans can be tolerant of players’ refusing to stand for the national anthem, even if they don’t agree with what the players are doing.



Most Americans (55%) don’t think a business executive should be fired from their job if they burn an American flag as part of a weekend political protest. However, a majority (54%) of Republicans think an executive should be fired for flag burning on the weekend. A plurality (50%) of Hispanics agree with Republicans that such an employee should be fired. In contrast, majorities of Democrats (61%), independents (57%), white Americans (56%), and African Americans (57%) don’t believe this should be a fireable offense.



A slim majority (53%) of Americans say that business employers should not discipline their employees for posting controversial or offensive opinions on social media accounts like Facebook. Forty‐​six percent (46%) think businesses should.





Democrats stand out with 58% who say businesses _should_ discipline their employees for offensive Facebook posts. In contrast, 60% of Republicans and 62% of independents think employees shouldn’t be punished at work for what they write online.



There is also a racial divide. A majority (59%) of African Americans think employees should be subject to discipline at work for their social media posts, while 56% of whites think they should not. Latinos are evenly divided.



Majorities of Americans don’t want to fire people from their jobs because of their political beliefs. But, the public is most likely to support firing an executive who believes that African Americans are genetically inferior (46%). About a quarter to a third support firing business executives who believe that all white people are racist (35%), believe transgender people have a mental disorder (30%), believe men are better at math than women (26%), believe psychological differences help explain why there are more male than female engineers (25%), or believe homosexuality is a sin (22%).



Besides a belief in biological racism, majorities of Democrats and Republicans oppose firing business executives for these other beliefs. Nonetheless, Democrats are considerably more likely than Republicans to support doing so. Democrats are about three times more likely than Republicans to support firing an executive if they believe transgender people have a mental disorder (44% vs. 14%) or believe homosexuality is a sin (32% vs. 10%). Democrats are twice as likely as Republicans to support firing an employee if they believe psychological differences help explain why there are more male engineers (34% vs. 14%), or that men are better at math than women (35% vs. 17%). Democrats and Republicans are more similar in their support for firing executives who believe all white people are racist (40% vs. 33%).





We find that the more strongly a respondent identifies as liberal the more supportive they are of firing people for each of these beliefs. However, the more strongly a respondent identifies as conservative the more likely they are to support firing a person for burning an American flag or firing an NFL player for refusing to stand for the national anthem. Thus, Americans become more likely to support firing people for offensive beliefs and expressions the more ideological—either liberal or conservative—they become.





Some of these results are surprising given that they test the boundaries of tolerable beliefs in the workplace. For instance, one might have expected that a belief in biological racism would be grounds for firing a business executive in charge of fostering merit and talent among all employees. Nevertheless, most Americans oppose firing someone for this belief.



Furthermore, few Americans wish to fire executives for their beliefs about homosexuality or differences between men and women. These results imply that high‐​profile firings in recent years of Silicon Valley executives and employees for these reasons, such as Brendan Eich at Mozilla or James Damore at Google, do not reflect the demands of the public at large.



Early in his presidential tenure, Donald Trump tweeted that the national news media is “fake news” and that it is an enemy of the American people.40 Nearly two‐​thirds (64%) of Americans do not agree with President Trump that journalists today are an “enemy of the American people.” Thirty‐​five percent (35%) side with the president.



However, nearly two‐​thirds (63%) of Republicans agree that journalists are an enemy of the American people. Such a charge is highly polarizing: 89% of Democrats and 61% of independents disagree.





Although Republicans think that the national news media is a threat, they don’t believe government ought to regulate news stories, even if biased or inaccurate. Strong majorities of Republicans (63%), independents (71%), and Democrats (76%) agree that “government should not be able to stop a news media outlet from publishing a story that government officials say is biased or inaccurate.”



Among all Americans, 70% say government should not shut down news stories regardless of whether officials think the story is inaccurate. A little more than a quarter (29%) think government should have the authority to stifle stories authorities say are inaccurate or biased.





While Republicans stand out with their negative view of the media, Democrats have uniquely positive evaluations of it. A slim majority (52%) of Democrats say the national news media is doing a good or even excellent job “holding government accountable.” In contrast, only 24% of independents and 16% of Republicans agree.



Among all Americans, only a third (33%) agree the news media is doing its job holding government accountable. More than two‐​thirds (67%) say it is not. Even more Republicans (84%) and independents (75%) share such negative views of the media.



The more a person identifies as liberal the more likely they are to say the media is doing a good job. Among strong liberals, 59% say the national news media is doing a good or excellent job holding government accountable. In contrast, 87% of strong conservatives say it’s doing a poor or fair job.



Why do Republicans lack confidence in the national news media while Democrats view it positively? Perhaps because most Americans perceive a liberal bias among most major news organizations.41





Fifty‐​two percent (52%) of respondents say that the _New York Times_ allows a liberal bias to color its reporting. Fifty percent (50%) feel CNN also succumbs to a liberal media bias. Fifty‐​nine percent (59%) say that MSNBC also has a liberal bias. Of all the top news organizations included on the survey, only Fox News was perceived to have a conservative bias (56%).



Americans feel their local news stations and broadcast news channels do a better job than cable news in providing balanced reporting. A majority (54%) say their local news station is balanced, without a liberal or a conservative bias. A plurality (42%) also believe that CBS is balanced. Nevertheless, respondents were four times as likely to say CBS has a liberal bias than a conservative bias (40% vs. 10%), and almost twice as likely to say their local station has a liberal bias (23% vs. 14%).



Majorities of Democrats believe most major news organizations are balanced in their reporting, including CBS (72%), CNN (55%), the _New York Times_ (55%), as well as their local news station (67%). A plurality (44%) also believe the _Wall Street Journal_ is balanced. The two exceptions are that a plurality (47%) believe MSNBC has a liberal bias (37% believe it’s unbiased) and a strong majority (71%) say Fox has a conservative bias.



Republicans, on the other hand, see things differently. Overwhelming majorities believe liberal bias colors reporting at the _New York Times_ (80%), CNN (81%), CBS (73%), and MSNBC (80%). A plurality also feel the _Wall Street Journal_ (48%) has a liberal tilt. Only when evaluating their local TV news station do most Republicans, but not a majority, perceive balanced reporting (42%). Similar to Democrats’ perceptions of MSNBC, a plurality of Republicans (44%) believe Fox News has a conservative bias; 41% believe it provides unbiased reporting.





The news outlets that Republicans find most objective are their local news station (42%), Fox (41%), and the _Wall Street Journal_ (28%). The media organizations Democrats find most objective include CBS (72%), their local news station (67%), CNN (55%), and the _New York Times_ (55%).



Who cares more about protecting religious liberty in the United States? It depends on whose liberty is at stake. Republicans tend to care more about protecting the conscience of religious bakers, florists, and other wedding‐​related businesses who refuse service to same‐​sex weddings. On the other hand, Democrats care more about ensuring Muslims have the right to build mosques in their communities.



Americans make a distinction between requiring businesses with religious objections to serve gay and lesbian _people_ and providing custom services to same‐​sex _weddings._



While 50% of Americans say businesses with religious objections should be required to provide services to gays and lesbians, only 32% think a baker should be required to bake a special‐​order cake for a same‐​sex wedding. Instead 68% say a baker should not be required to bake a custom wedding cake if doing so violates their religious convictions.



Majorities of Democrats say a business should be required to provide service to both LGBT people (73%) and bake a custom cake for same‐​sex weddings (52%), even if doing so violates the business owner’s religious beliefs. Conversely, majorities of Republicans say business owners should not be required to provide services in either situation, either to LGBT people (77%) or for same‐​sex weddings (87%).



Most Americans (73%) do not view baking a special‐​order wedding cake for a same‐​sex wedding as an endorsement of same‐​sex marriage. About a quarter (26%) do view it as an endorsement. However, Republicans (41%) are 28 points more likely than Democrats (13%) to view baking the cake as an endorsement of the marriage.



Evangelical Protestants are also more likely to believe (42%) that baking a custom cake for a same‐​sex wedding would be an endorsement of that wedding. In contrast, about a quarter of Mainline Protestants (26%), Catholics (27%), or other religious groups (28%) view it as an endorsement. Only 14% of non‐​religious people agree.



These data suggest that one reason Americans may disagree about requiring businesses service same‐​sex weddings is they don’t agree on what providing those services means. For some Americans, it would require them violate their conscience, while it would not appear that way to others.



What should happen to a religious baker who refuses to bake a special‐​order cake for a same‐​sex wedding? Most Americans (66%) say nothing should happen to the baker. Alternatively, a fifth (20%) would support a boycott of the bakery and 22% would support some kind of government punishment including: issuing a fine (12%), requiring an apology (10%), issuing a warning (8%), revoking their business license (6%), or sending the baker to jail (1%). Another 6% support suing the baker for damages.





Strong liberals stand out with a majority (58%) who favor some form of government punishment for a baker who refuses to bake the cake. In contrast, 22% of moderates and only 4% of strong conservatives support some form of government sanction against the baker or bakery.



An overwhelming majority of Americans oppose requiring churches and religious organizations perform same‐​sex wedding ceremonies if doing so violates their religious beliefs. This is non‐​controversial, with strong majorities of Democrats (73%), independents (81%), Republicans (91%), evangelical Protestants (92%), and non‐​religious people (72%) in agreement.



A slim majority (52%) of Americans say that local government officials should be required to perform same‐​sex wedding ceremonies, even if doing so violates that official’s religious convictions. Nearly as many (47%) say these officials should not be required to perform these ceremonies.



Partisans are sharply divided. Nearly 7 in 10 (69%) Democrats say local officials should be required to perform same‐​sex wedding ceremonies. In contrast, 68% of Republicans say such officials should not be required to do this. Independents are divided, with a slim majority (51%) who say officials should perform the ceremonies.



Most Americans (69%) would oppose a law that would ban the building of mosques in their community while 28% would favor. Although a slim majority (51%) of Republicans also oppose such a law, they are the most likely group to support it (47%). Far fewer Democrats (14%) and independents (28%) would also support a ban on building mosques in their communities.





The question distinguishes Libertarians from Conservatives. Using a political typology to identify ideological groups,42 we find that Libertarians (76%) are 25 points more likely than Conservatives (51%) to oppose a ban on building mosques. Eighty‐​nine percent (89%) of Liberals and 67% of Populists also oppose such a law.



The Cato 2017 Free Speech and Tolerance survey asked the following three questions to identify clusters of like‐​minded respondents based on their answers to questions about the proper role of government involvement in economic affairs and in promoting traditional values.



Respondents were divided into five groups, based on whether they wanted more or less government involvement in economic affairs and promoting traditional values. Here are the five groups defined:





The Cato Institute 2017 Free Speech and Tolerance Survey was conducted by the Cato Institute in collaboration with YouGov. YouGov collected responses August 15 to 23, 2017, from 2,547 Americans 18 years of age and older who were matched down to a sample of 2,300 to produce the final dataset. The survey included oversamples of 769 current college and graduate students, 459 African Americans, and 461 Latinos. Results have been weighted to be representative of the national adult sample. The margin of error for the survey, which adjusts for the impact of weighting is +/- 3.00 percentage points at the 95% level of confidence. The margin of error for current college and graduate students is +/- 5.17; for African Americans it is +/- 6.69; for Hispanics it is +/- 6.68; for whites it is +/- 4.13. This does not include other sources of non‐​sampling error, such as selection bias in panel participation or response to a particular survey. 



Data on the moral acceptability of punching a Nazi come from a Cato Institute/​YouGov survey conducted August 21 to 22, 2017, of 1,141 respondents, with a margin of error of +/- 4.5 percentage points, which adjusts for the impact of weighting.



YouGov conducted the surveys online with its proprietary Web‐​enabled survey software, using a method called Active Sampling. Restrictions are put in place to ensure that only the people selected and contacted by YouGov are allowed to participate.



The respondents in each survey were matched to a sampling frame on gender, age, race, education, party identification, ideology, and political interest. The frame was constructed by stratified sampling from the full 2013 American Community Survey (ACS) sample with selection within strata by weighted sampling with replacements (using the person weights on the public use file). Data on voter registration status and turnout were matched to this frame using the November supplement of the Current Population Survey (CPS), as well as the National Exit Poll. Data on interest in politics and party identification were then matched to this frame from the 2007 Pew Religious Life Survey. The matched cases were weighted to the sampling frame using propensity scores. The matched cases and the frame were combined and a logistic regression was estimated for inclusion in the frame. The propensity score function included age, gender, race/​ethnicity, years of education, non‐​identification with a major political party, census region, and ideology. The propensity scores were grouped into deciles of the estimated propensity score in the frame and post‐​stratified according to these deciles. The weights were then post‐​stratified to match the election outcome of the National Exit Poll, as well as the full stratification of four‐​category age, four‐​category race, gender, and four‐​category education.
"
"
Share this...FacebookTwitterYesterday I just happened to be listening to German NDR news radio (during my after-lunch snooze) and heard an interview with meteogroup’s Frank Abel, one of Germany’s better known meteorologists.Regrettably I can’t post the German interview audio due to copyright reasons. But you can get an mp3 audio clip e-mailed to you by calling Frau Renate Genz-Kreher, at the Hamburg studios, Tel.: (+49) 40/4156-2788. Just pick up the phone – I’m sure she speaks pretty good English
The NDR newsman starts the interview with a list of dramatic weather events that have occurred in northern Germany recently, then questions Abel if these events were something we’d have to get used to. Abel answered (paraphrased):

It’s really to early to say. Such events have happened in the past. A couple of months ago Austrian experts cautioned against premature conclusions on climate change and rainfall. It is quite certain that global temperatures are rising now but it is unsure how this impacts rainfall. We have to be careful not to spread too much certainty on the topic.
In a nutshell, all these reports claiming climate change leads to more frequent extremes are premature. There’s too much uncertainty out there.
The newsman then asks why northern Germany has had such lousy weather lately. Abel explains that it has to do with so-called 5B Lows. These are Lows that form in the Alps and move up to the Baltic Sea, which leads to warm moist air from southeast Europe colliding with much cooler air and thus results in heavy deluges. The same phenomenon happened in 2002, and in Poland in 1997 – all caused by so-called 5B Lows.
Of course the newsman doesn’t really care about that. He wants to here that it’s due to global warming. So he presses Abel.
Also for us here in North Germany it feels like it has rained harder than ever before. What has come down over the last few weeks is just unbelievable. Can you, as a weather expert, confirm this?
Abel agrees that it has rained a lot, and that August was one of the wettest months on the records, and then explains some of the geographic factors and goes into the current high water situation that people in some regions have to deal with right now. But he doesn’t deliver the goods. So the newsman persists:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So is it the changes in climate that are behind these heavy rains?
Abel refuses to say yes.
The situation is that nobody is really certain, and you have to be very careful…
The newsman interrupts:
Yes, but what do you think?
Frank Abel:
I can’t say. If you look back at the past, you’ll certainly find years and phases where you had similar conditions and people said ‘this has never happened”‘ Man has a relatively short memory for these things. Many have forgotten how we had similarly wet summers at the end of the 1970s.
It really sucks when the interviewee doesn’t cooperate. Next time the newsman will know better and invite a more reliable expert – like Prof Mojib Latif, who we now know will say anything. Damn meteorologists!
Don’t get me wrong, I don’t know what Abel’s position is on global warming…I’d say he’s a warmist, but not an alarmist. You’d have to ask him yourself (privately, off the record). He aint no Joe Bastardi – pretty sure about that.
I live in northern Germany and what do I think of the current weather? It was one of the shortest summers I can remember my 20 years here. We had about 3 weeks of hot weather from end of June to middle of July, and that was it. After that it was cool and rainy. This September has been really cool, with very few days getting up over 20°C. It’s rained a lot – but that’s what it does in northern Germany. That’s nothing new.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWe’ve seen that kind of thinking before. Authoritarianism.
Forget democracy, human freedom and free markets. These concepts, which have made today’s human prosperity and long lifespans a reality for humans, wherever and whenever they have been given the chance to work, are upsetting a small but very elitist group of individuals who view these concepts as a threat to their world view.
German Hans Joachim Schellnhuber has a “Master Plan” for society. Source: http://nachhaltigkeit2009.commerzbank.de/reports/commerzbank/annual/2009/nb/German/3010/_innovations-chancen-nutzen_.html
 
The English edition of Der Spiegel has a recent interview with Prof Hans Joachim Schellnhuber, Director of the Potsdam Institute for Climate Impact Research. In this interview Schellnhuber announced that he would unveil his “Master Plan” for transforming society – one no doubt that suits his world view. In Schellhuber’s view, human society needs to be scaled back and managed by an elite group of “wise men” who know what is best for the rest.
Schellnhuber’s contempt for today’s organisation of western soctety is illustrated by his statements. For example Der Spiegel stumps Schellnhuber with a simple question:  “Why is it that your messages haven’t been all that well received until now?” Schellnhuber responds:
I’m neither a psychologist nor a sociologist. But my life experiences have shown that the love of convenience and ignorance are man’s biggest character flaws. It’s a potentially deadly mixture.”
Oh the contempt. So we are all too comfortable and ignorant. It’s time for “wiser men” to think for the rest of us.  Your message, Mr Schellnhuber, has not been well received because it is anti-democratic and authoritarian. Your kind of thinking is a threat to the principles of Germany’s Constitution and so belongs under government observation.
Schellnhuber indeed has a very poor understanding of history. History teaches us a different lesson – that it is not the combination of love for convenience and ignorance that is man’s greatest flaw, rather it is the combination of towering arrogance and ignorance that is man’s greatest flaw. That is the real threat to society. Schellnhuber also denies climate history and concocts his own fraudulent version to warrant an authoritative intervention.
The belief that the planet needs to be dictated by a higher authority is what makes people like Schellnhuber so dangerous to democracy. Their impatience and frustration with democracy, and their claim to have superior knowledge, remind us of others who have led us down very dark paths back in the 20th century.
These people are convinced they are all-knowing. Der Spiegel asks: “Do you feel that the government’s abrupt change of course in relation to its energy policy is adequate?” Schellnhuber replies:
No. It can only be the beginning of a deep-seated shift. The German Advisory Council on Global Change, which I chair, will soon unveil a master plan for a transformation of society. Precisely because of Fukushima, we believe that a new basis of our coexistence is needed.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A master plan for transforming society drafted by a climate scientist, a person who admits having no background in sociology. This plan no doubt will be asserted by authority, and not democracy. Schellnhuber and others have indicated many times that democracy is flawed and is a nuisance. It’s getting in the way of solving the “world’s crises” (like zero temperature growth over the last 15 years).
Well, here is a little history lesson for the all-knowing Herr Schellnhuber and his bunch, who are frustrated with democratic principles and individual freedom, on where this type of contempt can take us if left unchecked:

This is where people were turned into numbers. Into this pond were flushed the ashes of 4 million people. And that was not done by gas. It was done by arrogance. It was done by dogma. It was done by ignorance. When people believe they have absolute knowledge with no test in reality, this is how they behave. This is what men do when they aspire to the knowledge of gods…Every judgement in science stands on the edge of error, and is personal.”
=================================================
I’ve brought up this Ascent of Man clip before, and am bringing it up again, and will continue to bring it up in the future. Watch the entire following series Ascent of Man, Knowledge or Certainty:
Ascent of Man – Knowledge or Certainty
Other links:
EU master plan to ban cars from cities by 2050.
ALERT-German Climate Advisor proposes creation of a CO2 budget
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe Science Skeptical Blog here has released some fascinating results from Hans von Storch’s Klimazwiebel blogsite’s survey here. Although the survey is not representative, 578 valid participants from 28 countries took part and interesting results have been produced. Here are the most interesting results:
1. Climate science skeptics have been involved in the topic for a long time. More than 50% of the participants have been following the topic 5 years or more. And skeptics are not born as skeptics. Skeptical Science writes:
More than 75% of the participants started off as neutral or even alarmist. That shows the longer one looks at the climate issue, the more skeptical one becomes with regards to alarmism.
2. Skepticism is deepening. Skeptical Science blog writes:
The last two years have been predominated by the debates on the 2007 IPCC Report, failure in climate negotiations at the international level, and Climategate. These have served to deepen skepticism.
3. Skeptics are shown to be competent. More than 2/3 of the participants have a scientific or technical education.
4. Skeptics like to quote highly competent experts, by a wide margin they like citing  Steven McIntyre and Richard Lindzen.
5. Skeptics characterised three blogs (Realclimate, Skeptical Science, Klimalounge) as alarmist. Meanwhile Klimazwiebel and Lucias Blackboard were viewed as moderate or neutral.
6. Skeptics do read the alarmist blogs mentioned in no.5, and thus are informed of both sides of the issue. Both “neutral” blogs were viewed very positively.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




7. WUWT and CA were the sites skeptics preferred to visit.
8. German skeptics liked visiting EIKE, WUWT and Klimazwiebel the most. [Next time they ought to put NoTricksZone on the ballot, I’d clean all their clocks out. :)].
Now the following are the summary results from the Klimazwiebel website, in case you’re interested:
Q1: The reason for being a skeptic.
2/3 are skeptic because they find that knowledge about the earth’s climate system would be insufficient for legitimating mitigation measures.
Q2: How long engaged/interested in climatic issues?
25% of the respondents became interested after the hot news issue of IPCC 2007. Most layman are no longer than 10 years, and the skeptical scientists are generally engaged for a longer time.
Q.3: Initial opinion upon first contact with climatic issues?
There is a clear warmist (38%)/”neutral” tendency.
Q.4 Which experience had respondents upon having asked their first critical questions?
Two of the six possible answers were clearly on top of the votes:
– The answer was an attempt to promote a political point of view (35%)
– The answer showed limited competence of the other side.
Q.5 How did attendants get to skepticism?
Internet resources was the most ticked choice in this multiple-options question (63%). The hockey stick discussion also represents a major factor. Both of these are clearly less a factor for skeptical climate scientists (Internet 27%); for these scientific publications are an important factor (up to 69%).
Q.6 What is the tendency, related to the past two years?
A vast majority (74%) tends clearly towards skepticism in this time scale. Attendants from web pages as eike-klima-energie.eu and nelson.blogspot as well as oekowatch.org are ticked around 83% (or even 100%).
—————————————————————————————————
Copyright reminder: It is not allowed to reproduce this post without first obtaining permission from No Tricks Zone. Other sites and media may cut and paste max. 25% of the content, and then followed by a link to this site. Let’s all be fair and let credit go where credit is due. Thanks!
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEd Caryl has submitted another essay, which indicates that CO2 is not as strong a driver as many would like to have us believe.
CO2 is Cool!
By Ed Caryl
The global warming amount if CO2 doubles has been a bone of contention for the last 30 years. The IPCC has settled on figures in the range from 1.5 to 4.5 degrees, with a most probable rise of 3°C. One of the figures widely quoted is based on CO2 rising from 295 ppm in 1900, to 365 ppm in 2000, while temperature rose 0.57 degrees. (For those readers allergic to math formulas, apologies are offered, but keep reading, the result is what is important, and the math is done for you.) Because the CO2 affect is agreed by most to be logarithmic, the formula for the temperature rise if CO2 doubles, when based on CO2 concentration and temperature rise over time, is:
ln2/(ln(CO2 at end of period/CO2 at beginning of period))/(the change in temperature). (ln is the
natural logarithm). Substituting the numbers from above, we get:
 ln2/(ln(365/295)/0.57) = 1.85°C
 This formula was used to calculate the CO2 doubling-temperature (the temperature rise if CO2 in the atmosphere doubles) over a slightly longer period, from 1880 to 2010, using the Law Dome (Antarctic ice samples) and Mauna Loa Hawaii CO2 levels spliced together, and the GISS (Goddard Institute for Space Studies) global surface temperature figures.
The Law Dome and Mauna Loa CO2 data overlap from 1960 to 1978, and agree quite closely over those years, so splicing them seems quite valid. First, here (from the sources above) is the temperature and the CO2 plotted together:

Figure 1. Plot of GISS global temperature anomaly and atmospheric CO2.
The result of CO2 doubling using the above formula with the beginning and end numbers in Figure 1 is:
 ln2/(ln(389.78/290.7)/0.91 = 2.15°C. This is higher, but still less than the IPCC estimate of 3°C.
But what if we take different time periods for the calculation? A shorter period 50-year calculation results in a noisy chart because there is great variation in temperature from year to year, with negative as well as positive temperature changes, and in the years before 1970, the CO2 rise was very slow so the ratios are small numbers. Here is the plot of the CO2-doubling temperature rise using the above formula with a sliding 50-year window beginning with the period from 1880 to 1930 and ending with 1960 to 2010:

Figure 2. CO2 sensitivity over time using a 50-year window applied to the data in Figure 1. The red trace is a 10-year moving average on the calculated sensitivity. The black line is the linear trend.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The calculation results in large positive and occasionally negative numbers when a shorter period is used. Non-CO2 influences are visible, the warming in the late 30s and 40s, and the later cooling and warming again in the 50s 60s and 70s. These are visible in Figure 1, above. The Atlantic Multi-decadal Oscillation (AMO), the Pacific Decadal Oscillation (PDO), and others, cause these temperature cycles. These 60 to 70-year ocean temperature cycles will increase a sensitivity figure based on a 100-year window.
Volcanoes, La Niña, and El Niño events and above described ocean cycles, as well as the solar cycle, affect the plot because they perturb the system away from equilibrium. Anything that adds or subtracts from the temperature is indistinguishable from the affects of CO2. CO2 is not the only thing affecting temperature. Many people have forgotten this simple fact. The plot in Figure 2 is really the total climate sensitivity, not just from CO2.
The figure widely used for CO2 doubling is only stable if the window used is long. It is only accurate if other factors are not pushing the numbers up. If shorter periods are taken, wildly different numbers result, both positive and negative. In recent years, the numbers settle to just over 1.0°C because the CO2 differences over the 50-year windows are getting larger. What is the real Climate Sensitivity? It appears to be less than 1.0°C for CO2 doubling.
Keep in mind that these plots were generated using GISS global surface temperature data. This data has been criticized for including too many sites influenced by urban warming and for being adjusted upward in recent years, and downward for earlier years. This seems to be the case, as any upward bias would tilt the above plot in the upward direction, as is seen at the end of the plot.
In an effort to check for this, the same formula was used on UHA satellite temperature data for the last 31 years. If the window is the 30 years from 1979 to 2009, the result is 1.75°C, if from 1980 to 2010, the answer is 2.05°C. Again, a short window gives a noisy answer. If 2011 is cooler than 2009, the calculation will be less than 1.75°C. Caution must be used when selecting data for these calculations. As an example, a calculation using the window from 1980 to 2008 results in a negative –0.2°C sensitivity.
Here are the temperature and CO2 plots:

Figure 3. Satellite temperature and Mauna Loa CO2 plots.
The sensitivity was plotted using a sliding 20-year window on the 31 years of satellite data available. Here is that result:

Figure 4. CO2 sensitivity using satellite temperature data and a 20-year sliding window.
The plot is very noisy with two points going negative. We will really need 20 or 30 more years of satellite temperature data to see a valid result, but the 30-year window suggests that the sensitivity as measured by the satellite data will still be less than 2°. Keep in mind that many of the ocean cycles, such as the AMO and PDO, are on the order of 60 years in length, and recently have been in their positive phases. The satellite data is only half this long.
In summary, all these plots show that CO2 sensitivity is probably 1°C or less for a doubling of CO2. We will need a few more years of good temperature data to pin that down.
Meanwhile, all the people claiming sensitivities of 3° or more need to calm down. The above plots and calculations rule that out.
A CO2 climate sensitivity of 1°C for CO2 doubling is not very important. Also remember that this figure includes all the supposed positive feedbacks, because if they exist, they have had an influence on the temperature already. Keep in mind that CO2 will probably never double in the atmosphere for the simple reason that we will run out of easily available fossil carbon long before then. But this is fodder for the next article.
Share this...FacebookTwitter "
"

The U.S. Senate will almost surely approve permanent normal trade relations (PNTR) this summer, paving the way for President Clinton to cast a favorable U.S. vote for China’s entry to the World Trade Organization before the end of the year.



Both China and the United States will benefit from more Open trade. New wealth will be created as firms, investors and workers profit from their comparative advantages and as consumers are better served. Along with the increased volume of trade will be an increased demand for financial and other services.



Western banks and non‐​bank financial institutions stand to benefit handsomely from their specialized knowledge and technological advantages. And, since every market exchange is two‐​sided and requires consent, gains for Western firms imply gains for China. PNTR and WTO accession, therefore, are good for China and for its trading partners. 



If the PRC meets investors’ expectations about greater market access and more secure property rights, foreign investment in China will continue to expand, and the flow of new capital will create opportunities for millions of Chinese to increase their standard of living. The Chinese people will demand banking, investment, insurance and other financial services‐​from both foreign and domestic firms‐​and they will become increasingly intolerant of government policies that limit their investment choices and “cannibalize” their savings. 



The dismal condition of China’s state‐​owned banks, which Are burdened with the nonperforming loans made to state‐​owned enterprises (SOEs), cannot be improved without the discipline of foreign competition. Just as non‐​state industrial firms have been allowed to grow spontaneously, private banks and financial institutions must have the freedom to develop if China is to improve its allocation of scarce capital and achieve healthy long‐​term growth. 



What China needs is not “socialist capital markets” but Real capital markets with private owners who can specialize in Risk taking, who can freely capitalize expected future profits Into their present values by selling shares on organized stock exchanges and who are held fully responsible for losses. That means prices, including interest rates, must be determined by the free market, not by the Chinese Communist Party (CCP). 



If the mainland is to become a major player in the global economy, it must stand by its commitment to liberalize its financial sector upon accession to the WTO. That means, according to the Accession Agreement, that, within five years after China accedes to the WTO, U.S. and foreign banks should have full access to the local currency market: They should be able to deal directly with Chinese citizens and business firms in renminbi, they should be able to establish branches throughout China, and they should receive national treatment (i.e., the same treatment as domestic banks). 



Any backsliding from those commitments will send a signal to the global financial community that China is not to be trusted. Capital will leave the mainland and flow to places where it is protected by the rule of law. 



Granting China PNTR and bringing it into the WTO will encourage Beijing to conform to international norms and move closer to the rule of law. But no one should be under the illusion that the adjustment process will be easy or fast. 



Hong Kong and Taiwan can show China the way toward the free market and a more open society, and the Internet and international trade can show the Chinese people that freedom is valuable in its own right, in addition to being a means to greater wealth. But, in the end, the Chinese people themselves must determine the course of their nation by the choices they make. 



China’s financial future will depend on the steps that are taken in the next several years to restructure state‐​owned banks and enterprises. Allowing greater foreign competition will play an important role in transforming the financial landscape, but ultimately the only way to efficiently allocate capital and de‐​politicize investment decisions is to privatize banks and SOEs. 



The growing enthusiasm of Chinese President Jiang Zemin For equity markets should not be misconstrued. He and his supporters have not suddenly become capitalists. They simply recognize that tax revenues are not sufficient to bail out insolvent banks and firms. Oddly, they see equity markets as a socialist tool for raising capital to “revitalize” state‐​owned banks and SOEs. 



The plan is to allow some islands of private ownership in A sea of state ownership. The state would retain majority ownership and control of the key financial and industrial firms it puts on the market. Those who invest in such firms will have attenuated private property rights and trade in pseudo, not real, financial markets.



Nevertheless, getting China’s leaders to start thinking in terms of equity markets, talking about the need for more flexible interest rates and allowing some private ownership are steps in the right direction. 



Once West meets East in the global capital markets, things will start to change‐​perhaps faster than anyone can imagine. Stock exchanges in Shanghai and Shenzhen will be expanded to include index funds, Chinese citizens will eventually be allowed to move their savings into a portfolio of international investments and Chinese capitalists will be trading over the Internet 24 hours a day. The pace of change, of course, will depend on the extent of competition that the CCP allows and, hence, on the political climate. If the global market proves more powerful than the party, change will accelerate.



The reality is that China’s current financial markets are strictly limited, the renminbi is not convertible on the capital account, SOEs are crowding out the non‐​state sector in the quest for investment funds, and foreign banks and non‐​bank financial firms are still waiting patiently to enter the Middle Kingdom.



Those constraints on capital freedom have led to rampant corruption. Below‐​market pricing of loans by state‐​owned banks has created opportunities for side payments as a means of deciding who gets the scarce state funds; exchange and capital controls have led to attempts to circumvent the law by bribery and favors. The success of those attempts is revealed by the fact that, from 1991 through 1998, more than US$100 billion illegally left the mainland for safe havens in Hong Kong and elsewhere. That figure shows up in the errors and omissions component of China’s balance of payments, according to Dong Fu, an economist at the Federal Reserve Bank of Dallas, and amounted to nearly 40 percent of total foreign direct investment in China during that period.



If China wants capital to freely come and stay, it must be free to leave. More importantly, Beijing must establish sound constitutional protection for private investors. That is the challenge for the next decade. By granting China PNTR and by allowing it to enter the WTO, the U.S. Congress will help China meet that challenge and help create real, not pseudo, financial markets for the future.
"
"

This Sunday, Al Gore will probably win an Academy Award for his global‐​warming documentary _An Inconvenient Truth_ , a riveting work of science fiction.



The main point of the movie is that, unless we do something very serious, very soon about carbon dioxide emissions, much of Greenland’s 630,000 cubic miles of ice is going to fall into the ocean, raising sea levels over twenty feet by the year 2100.



Where’s the scientific support for this claim? Certainly not in the recent Policymaker’s Summary from the United Nations’ much anticipated compendium on climate change. Under the U.N. Intergovernmental Panel on Climate Change’s medium‐​range emission scenario for greenhouse gases, a rise in sea level of between 8 and 17 inches is predicted by 2100. Gore’s film exaggerates the rise by about 2,000 percent.



Even 17 inches is likely to be high, because it assumes that the concentration of methane, an important greenhouse gas, is growing rapidly. Atmospheric methane concentration hasn’t changed appreciably for seven years, and Nobel Laureate Sherwood Rowland recently pronounced the IPCC’s methane emissions scenarios as “quite unlikely.”



Nonetheless, the top end of the U.N.‘s new projection is about 30‐​percent lower than it was in its last report in 2001. “The projections include a contribution due to increased ice flow from Greenland and Antarctica for the rates observed since 1993,” according to the IPCC, “but these flow rates could increase or decrease in the future.”



According to satellite data published in _Science_ in November 2005, Greenland was losing about 25 cubic miles of ice per year. Dividing that by 630,000 yields the annual percentage of ice loss, which, when multiplied by 100, shows that Greenland was shedding ice at 0.4 percent per century.



“Was” is the operative word. In early February, _Science_ published another paper showing that the recent acceleration of Greenland’s ice loss from its huge glaciers has suddenly reversed.



Nowhere in the traditionally refereed scientific literature do we find any support for Gore’s hypothesis. Instead, there’s an unrefereed editorial by NASA climate firebrand James E. Hansen, in the journal _Climate Change_ — edited by Steven Schneider, of Stanford University, who said in 1989 that scientists had to choose “the right balance between being effective and honest” about global warming — and a paper in the Proceedings of the National Academy of Sciences that was only reviewed by one person, chosen by the author, again Dr. Hansen.



These are the sources for the notion that we have only ten years to “do” something immediately to prevent an institutionalized tsunami. And given that Gore only conceived of his movie about two years ago, the real clock must be down to eight years!



It would be nice if my colleagues would actually level with politicians about various “solutions” for climate change. The Kyoto Protocol, if fulfilled by every signatory, would reduce global warming by 0.07 degrees Celsius per half‐​century. That’s too small to measure, because the earth’s temperature varies by more than that from year to year.



The Bingaman‐​Domenici bill in the Senate does less than Kyoto — i.e., less than nothing — for decades, before mandating larger cuts, which themselves will have only a minor effect out past somewhere around 2075. (Imagine, as a thought experiment, if the Senate of 1925 were to dictate our energy policy for today).



Mendacity on global warming is bipartisan. President Bush proposes that we replace 20 percent of our current gasoline consumption with ethanol over the next decade. But it’s well‐​known that even if we turned every kernel of American corn into ethanol, it would displace only 12 percent of our annual gasoline consumption. The effect on global warming, like Kyoto, would be too small to measure, though the U.S. would become the first nation in history to burn up its food supply to please a political mob.



And even if we figured out how to process cellulose into ethanol efficiently, only one‐​third of our greenhouse gas emissions come from transportation. Even the Pollyannish 20‐​percent displacement of gasoline would only reduce our total emissions by 7‐​percent below present levels — resulting in emissions about 20‐​percent higher than Kyoto allows.



And there’s other legislation out there, mandating, variously, emissions reductions of 50, 66, and 80 percent by 2050. How do we get there if we can’t even do Kyoto?



When it comes to global warming, apparently the truth is inconvenient. And it’s not just Gore’s movie that’s fiction. It’s the rhetoric of the Congress and the chief executive, too.
"
"

Mayan ruins in Guatemala.

This is an email I recently received from statistician Dr. Richard Mackey who writes:
The following appeared on Gore’s blog of Nov 19, 2008:
Looking Back to Look Forward
Looking  Back to Look Forward November 19, 2008 : 3:04 PM

A new study suggests the  Mayan civilization might have collapsed due to environmental  disasters:
These models suggest that as ecosystems were destroyed  by mismanagement or were transformed by global climatic shifts,  the depletion of agricultural and wild foods eventually contributed to  the failure of the Maya sociopolitical system,’ writes  environmental archaeologist Kitty Emery of the Florida Museum of Natural  History in the current Human Ecology journal. 
As we move  towards solving the climate crisis, we need to remember the consequences to  civilizations that refused to take environmental concerns  seriously.
If you haven’t read already read it, take a look at  Jared Diamond’s book, Collapse.”
This is a most curious  reference.
It means that Gore is advocating the abandonment of the IPCC  doctrine and barracking for the study and understanding of climate  dynamics that ignores totally the IPCC/AWG doctrine and focuses on all  the other variables, especially how climate dynamics are driven  by atmospheric/oceanic oscillations, the natural internal dynamics of  the climate system and the role of the Sun in climate dynamics.
Brian  Fagan in Floods, Famines and Emperors  El Nino and the fate of civilisations   Basic Books 1999, shows that the Maya collapse, whilst having complex  political, sociological, technological and ecological factors, was largely  driven by the natural atmospheric/oceanic oscillations of ENSO and NAO.  The  book is one of three by Brian Fagan, Prof of Anthropology UC Santa Barbara,  that documents how natural climate variations, ultimately driven by solar  activity, have given rise to the catastrophic collapse of civilisations.  The  book has a chapter on the Mayan civilisation which collapsed around 800  to 900 AD.
Here are some quotes from his book:
“The “Classic  Maya collapse” is one of the great controversies of
archaeology, but there is  little doubt that droughts, fuelled in part
by El Nino, played an important  role.”
“The droughts that afflicted the Maya in the eighth and  ninth
centuries resulted from complex, still little understood  atmosphere-
ocean interactions, including El Nino events and major decadal  shifts
in the North Atlantic Oscillation, as well as two or three  decade-long
variations in rainfall over many centuries.”
“Why did the  Maya civilisation suddenly come apart?  Everyone who
studies the Classic Maya  collapse agrees that it was brought on by a
combination of ecological,  political, and sociological factors.”
“When the great droughts of the  eighth and ninth centuries came, Maya
civilisation everywhere was under  increasing stress.”
“The drought was the final straw.”
“The  collapse did not come without turmoil and war.”
Brian Fagan describes  how the ruling class (the kings had divine powers, they were also shamans and  there was a vast aristocracy and their fellow-travellers that the tightly  regulated workers toiled to maintain) encouraged population growth beyond  what the land could carry; how the rulers enforced rigid farming practices  which were supposed to increase food production and the ruler’s incomes but  had the effect of undermining farm productivity and diminishing  the quality of the poor soils of the area.  When there were heavy  rains the soil was washed away.  In times of drought the soil blew  away.
More quotes from Brian Fagan:
“The Maya collapse is a  cautionary tale in the dangers of using
technology and people power to expand  the carrying capacity of
tropical environments.”
“Atmospheric  circulation changes far from the Maya homeland delivered
the coup de grace to  rulers no longer able to control their own
destinies because they had  exhausted their environmental options in an
endless quest for power and  prestige.”
Gore says that we should use our understanding of the Maya  collapse help us solve the climate crisis, noting that “we need to remember  the consequences to civilizations that refused to take  environmental
concerns seriously”.
Given what we know of the Maya  collapse, what is Gore really saying?
He is saying that we should take  all the IPCC/AWG publications and related papers to the tip and bury them  there and put all our efforts into the study and understanding of the reasons  for climate dynamics that address every theory except that of IPCC/AWG  doctrine.
Specifically, we should understand as well as we can how  climate dynamics are driven by atmospheric/oceanic oscillations, the  natural internal dynamics of the climate system and the role of the Sun  in climate dynamics.
In an overview of his work Brian Fagan  concluded:  “The whole course of civilisation … may be seen as a process of  trading up on the scale of vulnerability”.  (Fagan (2004, page  xv)).
We are now, as a global community, very high up on that  scale.
Allow me to quote a little from my Rhodes Fairbridge paper  because of its relevance to Brian Fagan’s work and what Gore is really trying  to say, but can’t quite find the right words.
(My paper is here: http://www.griffith.edu.au/conference/ics2007/pdf/ICS176.pdf ).
“In his many publications (for example, NORTH (2005)), Douglass  North stresses that if the issues with which we are concerned, such  as global warming and the global commons, belong in a world of  continuous change (that is, a non-ergodic world), then we face a set of  problems that become exceedingly complex.  North stresses that our capacity  to deal effectively with uncertainty is essential to our succeeding in  a
non-ergodic world.  History shows that regional effects of  climate change are highly variable and that the pattern of change is  highly variable.  An extremely cold (or hot) year can be followed  by extremely hot (or cold) year.  Warming and cooling will be  beneficial for some regions and catastrophic for others.  Brian Fagan  has documented in detail relationships between the large-scale  and
generally periodic changes in climate and the rise and fall  of civilisations, cultures and societies since the dawn of history.   The thesis to which Rhodes Fairbridge devoted much of his life is that  the
sun, through its relationships with the solar system, is  largely responsible for these changes and that we are now on the cusp of  one of the major changes that feature in the planet’s history.   As
Douglass North showed, the main responsibility of governments  in managing the impact of the potentially catastrophic events that  arise in a non-ergodic world is to mange society’s response to them so as  to
enable the society to adapt as efficiently as possible to them.
Amongst  other things, this would mean being better able to anticipate and manage our  response to climate change, to minimise suffering and maximise benefits and  the efficiency of our adaptation to a climate that is ever-changing –  sometimes catastrophically – but generally predictable within bounds of  uncertainty that statisticians can estimate.  At the very least, this  requires that the scientific community acts on the wise counsel of Rhodes W  Fairbridge and presents governments with advice that has regard to the entire  field of planetary-lunar-solar dynamics, including gravitational  dynamics.
This field has to be understood so that the dynamics of  terrestrial climate can be understood.
References:
North, D. C.,  2005. Understanding the Process of Economic Change
Princeton University  Press.
Fagan, B., 2004.  The Long Summer.  How Climate Changed  Civilization.
Basic Books.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a885ba4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
One of the claims about “global climate change” is that it will affect the normal ranges of flora and fauna of our planet. Well, with a very cold northern hemisphere this winter, that seems to happening. A bird not seen (as a mature adult) in Massachusetts since the 1800’s , an Ivory Gull, normally an inhabitant of arctic areas, has been spotted. Here are the details from the Plymouth, MA Patriot-Ledger. – Anthony

GULL-LOVER’S TRAVELS: Birdwatchers flock to Plymouth to spot rare specimen

PLYMOUTH — Jan 28th, 2009
The temperatures were in the single digits, but not low enough to keep the gawkers away. A celebrity was in town, behind the East Bay Grille, a visitor not seen in these parts in decades, if not longer.
But these weren’t paparazzi, and this wasn’t a Hollywood star. Rather, they were avid birdwatchers – about 20 in all – braving the frigid air as they scanned the bay and the edges of the breakwater with binoculars and spotting scopes.
And they would be rewarded, catching a glimpse of a glimpse of a rare, fully mature ivory gull. A birdwatcher reported seeing one in Plymouth last week, and another was spotted at Eastern Point Lighthouse in Gloucester. From Sunday through Tuesday, the avian visitor was a regular in Plymouth, much to the delight of birdwatchers, who came from near and far in hopes of adding the extremely rare bird to their life list.
Ivory gulls normally stay well above Newfoundland, living on Arctic ice where they follow whales and polar bears to feed on the scraps and carcasses they leave behind after making a kill.

Until this year, the last report of a fully mature ivory gull in Massachusetts was in the 1800s. Three immature birds were seen in the 1940s. In 1976, another immature bird had been spotted in Rockport.
Russell Graham of Dallas is flying in Friday for a three-day visit. He’s hoping the gull will still be in town when he arrives.
“The ivory gull is one of a handful of birds that every birder dreams of seeing but almost no one has.,” he said. “This isn’t a dream that’s confined to North America. There is also an immature bird in France that is causing the same reaction there. There are a couple of places where you can go in the summer and expect to see one but they are distant and expensive – Svalbard on Spitsbergen, Norway and Pond Inlet on Baffin Island, Canada.
“I never thought I would have the chance to see one and I can’t pass up this once-in-a-lifetime opportunity.”
If the gull is gone, Graham will consider a side trip to Nova Scotia, where two adult ivory gulls have been seen recently. “I’ll be keeping my fingers crossed,” he said.
John Fox of Arlington, Va., and his friend Adam D’Onofrio of Petersburg drove more than eight hours on Sunday to see the gull.
“No bird this morning,” Fox said a day later, shaking his head. “We left Virginia at three in the morning yesterday and arrived here 20 minutes too late.”
On Sunday morning, hundreds of people got to observe and photograph the gull as it fed on a chicken carcass someone put out on one of the docks in the parking lot. The bird stayed until 11 a.m., then flew across the harbor. It was not seen again for the rest of the day.
“We arrived at 11:20 and spent the rest of the afternoon in the parking lot, hoping it would return,” Fox said.
They stayed at Pilgrim Sands Motel and arrived at the parking lot early Monday morning for one more chance to see the ivory gull before returning to Virginia. Fox said it was his first time in Massachusetts. If he didn’t see the bird, he said, at least he could see Plymouth Rock before they left for home.
“That’s how it goes sometimes,” he said. “We don’t always see what we come for, but it’s nice to see some of the sights when you travel to a new area in hopes of seeing a rare bird.”
As Fox was planning his exit, a commotion caught his attention. One of the birders pointed toward the sky and said with a shout, “There it is.”
The pure white gull was flying toward the parking lot, silhouetted against a bright blue sky. Someone in the crowd announced for the record the gull had arrived at 7:45 a.m.
The bird flew in circles overhead, then landed on a snow bank in the middle of the parking lot. Cameras clicked and the birders “oohed and ahhhed” each time the ivory gull switched positions.
“Look how white it is,” someone said. “It’s got black feet, black eyes and a grayish-black beak,” said another.
The gull eyeballed the chicken carcass, still there from the day before, but it didn’t eat. Instead, it flew to the railing along the edge of the boat ramp and perched with a group of sea gulls. The photographers followed, changing positions to get the best lighting.
Fox stood with the group, talking with other birdwatchers, as the gull sat peacefully on the railing, observing all the people gathered around it. Was it worth the long drive up from Virginia?
“It sure was,” Fox said with a smile.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98ddacfb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Thank you Mr. Chairman and Members of the Committee for inviting me to testify before you today. By way of introduction, I am an Associate Professor of Law at Capital University Law School in Columbus, Ohio, where I teach, among other subjects, Election Law. Though I appear today on my own behalf, I am also an Adjunct Scholar of the Cato Institute. I have researched and written extensively, in both academic and popular journals, on the subject of campaign finance.



First, let me say that I appreciate the decision of this Committee to hold hearings on campaign finance reform. Before Congress now are bills to reform campaign finance by placing new limits on speech — the so called “McCain‐​Feingold” bill in the Senate (S25) and “Shays‐​Meehan” bill in the House (H493). Numerous lending constitutional scholars and campaign finance experts have called these bills unconstitutional. Yet one of the name sponsors of the bill on the Senate side just this month derided these concerns on national radio, saying “When [my opponent] starts relying on those Constitutional arguments, I know he doesn’t have much else in his arsenal.” On another occasion, on national television, this senator stated cavalierly that his opponents “may be right that that particular provision [of the bill] is unconstitutional. And that’s why we have a back‐​up provision.”



Meanwhile, the minority leader here in the House was quoted in a national news magazine earlier this month, saying that “freedom of speech” and “a healthy democracy” are “in direct conflict.”



When a member of Congress so casually treats his oath to uphold the constitution; and when the House minority leader suggests that the First Amendment must itself be amended because free speech “is in direct conflict” with democracy, it is both timely and appropriate for this committee to hold hearings.



Before congress attempts to solve the problems of campaign finance with more regulations burdening free speech rights, we should take stock of the fact that the current regulatory system is responsible for many of the evils we see in campaign finance. We do not need to plug “loopholes” in the system. Rather, we should scrap most all of the present system of campaign finance regulation, remembering the admonition of the First Amendment to the Constitution, that Congress shall make no law abridging the rights of free speech.



Before discussing the details of campaign finance, I think it is important to briefly remind ourselves that, for most of this country’s history, the funding of political campaigns has been totally or largely unregulated. During our nation’s first century, the era which produced as presidents Abraham Lincoln, George Washington, Thomas Jefferson, Grover Cleveland, James Knox Polk, and Andrew Jackson, and which saw giants such as Daniel Webster, Henry Clay, John Quincy Adams, and John C. Calhoun serve in Congress, there were literally no laws regulating campaign finance. And today, we often look back on that century as a golden age of politics — one in which memorable debates over such monumental issues as slavery and western expansion were discussed in serious campaigns, one in which people talked and debated these issues, one in which politics was marked by mass rallies and torchlight parades, and one in which voter turnout was considerably higher than it is today.



The federal government did not become involved in campaign finance until this century. If we look back, we find that the arguments made in favor of regulation a century ago were the same that we hear today: that the American people believed Congress to be made up of the “instrumentalities and agents of corporations;” that “corruption” was the norm; that new advertising techniques and technologies‐​in those days mass newspapers, recordings, train travel‐​had created an insatiable demand for political spending that could only be curbed by spending limits; and that we faced a “crisis” of democracy. In response to such complaints, the federal government passed its first campaign finance law in 1907, banning direct corporate contributions to candidates. In 1943, this ban was extended to labor unions. Additionally, congress passed greater disclosure requirements in 1925. However, these disclosure measures were so toothless as to be meaningless. For example, from its enactment in 1925 until its repeal in 1971, there was not a single prosecution under the Federal Corrupt Practices Act. Yet democracy survived, and this period of minimal regulation gave us Presidents Theodore and Franklin Roosevelt, Calvin Coolidge, Harry Truman, and Dwight Eisenhower, Congressional leaders such as Robert Taft, Hubert Humphrey, and Everett Dirksen, and serious debates over such issues as civil rights. For nearly two centuries, our democracy flourished despite, or perhaps even because of, the absence of any meaningful campaign finance regulation.



Not until the 1974 Amendments to the Federal Elections Campaign Act (FECA) did the federal government pass a campaign finance law with any serious enforcement mechanism. And it was also this law which, for the first time, gave us both contribution limits and, as a necessary accessory to those limits, the strange doctrines of independent expenditures and express advocacy. The 1974 Amendments threw of web of regulation, with an accompanying enforcement bureaucracy, the FEC, over American politics.



The stated goals of the 1974 FECA Amendments were to lower the cost of campaigning, reduce the influence of so‐​called “special interests,” open up the political system to change, and “restore confidence in government.” So what has actually happened in the twenty years since the 1974 Amendments took effect? Well, campaign spending has increased by more than 350 percent; PAC contributions have increased by more than 800 percent; House incumbents, who had previously outspent challengers by approximately 1.5 to 1, now outspend challengers by nearly 4 to 1; incumbent reelection rates have risen to record high levels, spurring the demand for term limits; and public confidence in government has fallen to record lows. Clearly, the 1974 FECA Amendments have been a dismal failure. Yet the response of the reformers — notably Common Cause, the interest group most responsible for the 1974 Amendments, and today the number one cheerleader behind the Shays‐​Meehan bill — is to argue that we need more regulation, more limits, and more bureaucracy. Indeed, some now claim that because earlier regulation has failed, we not only need still more regulation, but we need to amend the First Amendment to allow government to regulate political speech and activity in ways the Founders never dreamed of. I would suggest, however, that when an approach has failed so clearly, so dismally, with such negative consequences, over a period of twenty years, it is time to consider a whole new approach. It is time not for more regulation, nor for more efforts at “loophole” plugging, which is the approach taken by Shays‐​Meehan. Rather, it is time to deregulate American politics.



In my opinion large parts of Shays‐​Meehan and its Senate counterpart, McCain‐​Feingold, are unconstitutional. The so‐​called “voluntary” spending limits of the bills are in fact punitive and coercive, and amount to an unconstitutional condition leveled on the Constitutional right to free speech. That portion of McCain‐​Feingold abolishing PACs is unconstitutional, as even its supporters seem to recognize. If one person can spend $1000 on bumper stickers, it is inconceivable that two people cannot join together to spend $1,000 on bumper stickers.



The Senate bill’s limitations on out‐​of‐​district contributions are probably unconstitutional. There is no reason why an out‐​of‐​district contribution is more “corrupting” than an in‐​district contribution. Thus, there is no compelling government interest to justify the ban on speech.



Overall, Shays‐​Meehan and McCain‐​Feingold mark the most serious legislative assault on free speech in over two decades‐​since the 1974 Amendments to FECA. Many of those 1974 reforms were eventually held unconstitutional. The others have stifled free speech and contributed to the current problems. We should not go down that road again.



But where Shays‐​Meehan and McCain‐​Feingold are most at odds with the Constitution and sound policy is in their efforts to silence political groups engaged in issue advocacy. Indeed, this question of “issue advocacy” versus “express advocacy,” which has aroused the ire of those who would regulate political speech, is a prime example of the danger of the FECA’s attempt to regulate politics to produce a desired result. Congressional Quarterly has noted that in recent years, litigation has become a major campaign tactic. Thus we have Republicans filing complaints against the AFL-CIO, and the Democrats filing complaints against the Christian Coalition, U.S. Term Limits, Americans for Tax Reform, and the Christian Action Network, to name just a few recent complaints. In each case, the complaints amount to a blatant effort to silence political advocacy by these groups. In each of these incidents, the groups involved were not engaged in any nefarious activities such as vote fraud or bribery. Rather, their alleged infractions amounted to what might be called the crime of “committing politics.” That is to say, the groups involved were trying to persuade the American people that either their positions were right, or someone else’s were wrong. It is true that incumbent officeholders do not like being attacked for their stands on issues; particularly when they view those attacks as shameless demogogery. However, the robust discussion of issues is wholly in line with both the First Amendment and the American tradition of political participation. That opposing political interests can invoke the powers of a government bureaucracy in an effort to silence these voices is, I suggest, a much more serious blight on our system than the alleged effects of campaign contributions.



The reasons that these complaints are even treated seriously is because of the doctrine of “express advocacy.” This doctrine itself a bizarre outgrowth of efforts by supporters of campaign finance “reform” to limit campaign contributions. These “reformers” seek to prevent individuals and groups from participating in politics through contributions of money. However, under the Supreme Court’s ruling in _Buckley v. Valeo_ , Congress may not, constitutionally, restrict individual or group expenditures that do not “include explicit words of advocacy of election or defeat of a candidate.…” Thus, political speech is free from FEC regulation if it does not expressly advocate the defeat or election of a clearly identified candidate, but is subject to FEC regulation if it crosses that line.



The response of Shays‐​Meehan to this type of political activity, is to define “express advocacy” more broadly, by looking at such factors as timing and context. In fact, however, it is hard to see how these factors make any serious difference. For example, would last year’s AFL-CIO ads have been any more or any less “express advocacy” if aired three months or fifteen months before an election? Would it really matter if the group sponsoring the ads had public positions on some of the issues addressed? Clearly not. The ads would be no more nor less aimed at shaping public opinion, regardless of the added factors that Shays‐​Meehan seeks to consider. Thus, the long and short of an expanded definition of “express advocacy” would be a sharp reduction in political speech, which is precisely what the Supreme Court’s decision in _Buckley_ , protecting issue advocacy, is intended to guard against.



Of course, the FEC’s concern over “express advocacy” and independent expenditures is all part of a larger effort to plug “loopholes” in the disastrous system of contribution and spending limits enacted in 1974. The only reason anyone cares about “express advocacy” is the fear that, absent such a regulation, groups will spend money to try to affect federal elections, and in doing so will exceed the contribution limits of the FECA. I have written and commented at length on the undemocratic and deleterious effects that these limits have had on American politics. _See e.g._ Bradley A. Smith, _Faulty Assumptions and Undemocratic Consequences of Campaign Finance Reform_ , 105 Yale Law Journal 1049 (1996); Bradley A. Smith, Testimony before Committee on Rules and Administration, United States Senate, February 1, 1996; Bradley A. Smith, _Campaign Finance — Deformed_ , Wall Street Journal, Oct. 6, 1995 (copy attached). In short, these limits have entrenched incumbents; burdened grassroots political activity; limited the number and type of candidates; had the perverse effect of increasing the incentives both for campaign contributors to seek influence, rather than electoral success, and for office holders to reward financial patrons; and increased the power of unelected elites, most notably the media. The efforts to drive money from politics have grotesquely distorted our political system. The reason is simple and obvious: efforts to limit political participation not only run afoul of the Constitution, but they are like efforts to stop the flow of a river — one way or another, the water will pass, diverting course as necessary to do so. So long as the federal government spends over $1 trillion each year and regulates virtually every phase of the economy, not to mention many non‐​economic activities, the American people will seek to persuade Americans to elect their favored candidates. In modern society, this political communication and participation requires the expenditure of money.



Fortunately, past efforts to limit political discourse have consistently been struck down by the courts as unconstitutional. Nevertheless, these attempts to stop this legitimate political advocacy by placing a heavy bureaucracy over political campaigns have had, as I mentioned, a variety of negative consequences. Not the least of these is the way in which such regulation stifles true grassroots democracy.



For example, a 1991 study by the Los Angeles Times found that among the most common violators of FECA were “elderly persons… with little grasp of the federal campaign laws.” Even well‐​funded and well‐​organized groups can find their efforts at grassroots advocacy smothered. In one ill‐​founded effort to prevent political advocacy in violation of campaign finance laws, the FEC passed a rule that would have prevented the United States Chamber of Commerce from communicating political endorsements to more than 220,000 of its dues paying members, mainly small businesses whose owners and managers have little time to follow politics and rely on the Chamber of Commerce precisely for such information. Similarly, the regulation in question would have made more than two‐​thirds of the National Rifle Association’s members ineligible to receive the group’s endorsements, as well as over 44,000 dues paying members of the American Medical Association. This regulation was, fortunately, found unconstitutional by the U.S. Court of Appeals for the D.C. Circuit, but only after these groups had had their speech chilled in the 1994 election. Chamber of Commerce v. FEC, 1995 U.S. App. LEXIS 31925 (D.C. Cir. Nov. 14, 1995).



Another recent FEC rule attempted to prevent corporations and other groups from actively engaging in issue‐​oriented advertising during a campaign, on the theory that such advertising would implicate federal elections. Again, this effort to limit the flow of political information had to be struck down by a federal court. Maine Right to Life Committee, Inc. v. FEC (D. Me., Feb. 13, 1996). Simply putting similar measures into a statute will not make them constitutional.



Limitations on “express advocacy” call for precisely the type of judgments that benefit large organizations with the ability to hire a battery of lawyers to advise them through the regulatory process. Efforts to broaden the concept to include advocacy beyond such express words as “elect” or “defeat” would truly burden free speech, especially for smaller, local groups. Political participation, by definition, seeks to influence voter preferences on both issues and candidacies. Any broadening of the term would lead to a murky standard that would significantly burden most all political speech.



Nevertheless, in addition to the unconstitutional provisions of Shays‐​Meehan, we now find offered up a Constitutional Amendment which would authorize Congress to adopt “reasonable regulations,” so long as they do not “interfere with the right of the people to fully debate issues.” This is classic double speak. Our Founding Fathers recognized that government could not be trusted to make such distinctions: The incentives to crush the opposition would be too great. Thus they wisely passed the First Amendment.



Historically, debates on the First Amendment have concerned the extent to which it covers pornography, or hate speech, or commercial speech, or “fighting words,” or treasonous speech. What has always been accepted, across the political spectrum, is that it covers political speech. So let’s be honest about it: what the Amenders really seek is a clause reading “the First Amendment to this Constitution is hereby repealed.”



Efforts to limit “express advocacy,” like, indeed, the rest of the FECA regulatory scheme, are based on the belief that Americans ought not participate in politics. However, it is not a bad thing for Americans to participate in politics — it is a good thing. It is constitutionally protected. And the fact of the matter is that, more than ever in American society, communicating in the political realm requires the expenditure of money. Money is not an evil in politics — it is a source of information to voters. Efforts to regulate the flow of money in politics over the past 20 years have done much more than money ever did to distort the political system and create a public distrust of government. It is now time to try a new approach — that is, it is time to deregulate politics. There is simply no _a priori_ method to say what is fair or not fair — how much groups should be able to spend, or what kind of advocacy they can spend it on. The bureaucracy that has been established to regulate politics is stifling grassroots advocacy and political communication.



After twenty years of campaign finance regulation, it should by now be clear that independent electoral advocacy by citizen groups lies at the core of the First Amendment, and that such advocacy ought to be beyond the permissible scope of government regulation. Political battles should be fought out in forums of public persuasion. It is poor policy to divert such debates to federal courtrooms, with each side attempting to silence its opponents through such arcane concepts as “express advocacy” and “coordinated” or “uncoordinated” expenditures.



Deregulation of campaign finance, not added regulation, is the proper course of action. The FECA $1000 limit on individual campaign contributions should be abolished entirely, or at least raised to a realistic figure, in order to reduce the need for candidates to rely on independent expenditures. (The $1000 limit, in existence since 1974, has never been adjusted for inflation. Had it been, it would be approximately $3500 today. This is the minimum to which the contribution limit should be raised: $5000, $10,000, or complete removal of the cap would be preferable.) All caps on political party giving should be removed. Donations from a party to its own candidates are not “corrupting.” Moreover, since last year’s Supreme Court decision in Colorado Republican Federal Campaign Committee v. Federal Election Commission, 1996 WL 345766 (U.S. 1996), parties may spend unlimited amounts in support of their candidates, but only independently of the candidate’s campaign. Driving a wedge between parties and candidates is poor public policy. Disclosure of political expenditures meets any public need to know the source of financing. However, even here I must counsel caution. Disclosure rules can have a chilling effect on speech and may be constitutionally limited. McIntyre v. Ohio Board of Elections, 1995 WL 227810 (U.S.)(1995). Disclosure rules governing independent expenditures should be limited, therefore, to groups which engage in substantial activity, spending over $50,000 in an election cycle. Electronic filing and mandatory FEC posting of reports on the Internet would help to insure an informed public. These are the type of sensible, constitutional reforms congress should consider‐​not the unconstitutional Shays‐​Meehan bill or the foolish drive to repeal the First Amendment.



In recent years, it has become increasingly difficult to discuss meaningful campaign finance reform. This is because both public and congressional opinion has become trapped in a box. This box is the conscious creation of groups such as Common Cause, which for 25 years have worked tirelessly to convince the American public that the members of this Committee, and indeed all of Congress, are corrupt bribe‐​takers, and that the public itself consists of innocent dupes incapable of making intelligent voting decisions based on the information presented to them. By constantly drawing simplistic correlations to financial support and voting records, and through the conflation of the issue of campaign finance reform with other issues of voter concern, such as lobby reform, negative campaigning, and legislative gridlock, these groups have purposely attempted to create a climate of public opinion in which certain core assumptions are not to be challenged. These core assumptions are that political advocacy must be heavily regulated; political contributions and, ultimately, political spending limited; and all possible “loopholes” plugged. However, the heavy regulatory regime which these “reformers” have placed over campaign activity is, in fact, a major contributing factor to the very problems that have created such public disgust with the campaign finance system and, indeed, Congress in general.



Now is the time to get out of the box. We must not plunge ahead, sacrificing our First Amendment Freedoms. Congress must realize that Shays‐​Meehan style “reforms,” based, as they are, on the erroneous assumption that Americans should not spend money on political affairs, cut off grassroots involvement and decrease the flow of information to voters. The regulatory approach enacted in 1974 has had unintended, negative consequences that have only increased voter cynicism. The House should reject simplistic proposals such as Shays‐​Meehan, or efforts to amend the Constitution to destroy the right to free political speech, and move generally to deregulate political speech. It ought not be a crime to “commit politics” in America.



Thank you.
"
"
Share this...FacebookTwitterClimate nonsense will lead to this trend in electricity prices.

German journalist Günter Ederer has a piece at the online Fuldaer Zeitung called The Electricity Bill Is Going Up And Up. Hat-tip Detmar Doering.
Politicians in all parties in Germany, from the communists to the conservatives, and everything in between, are all racing to be the first to jump off the let’s-save-the-climate cliff.
All have made rescuing the climate a target that absolutely has to be achieved – no matter the cost. Politicians of every stripe have pledged to cut Germany’s CO2 emissions 80% by 2050. Minister of the Environment Norbert Röttgen, of the conservative (in name only) party, even sets it up as a life and death matter, as absurd as it sounds:
That rescues our climate.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




No kidding. Everything else takes a back seat to this imperative – consumers be damned. Taking the little guy to the cleaners in Germany knows no limits. And so, expensive green energy sources like wind and solar are being subsidized with total abandon, and consumers are getting the big-time shaft through skyrocketing electric bills- The government profits in the end. Ederer writes:
Renewable energy must be fed into the power grid at a mandated fixed price, which for the consumer will mean a price increase of 6 cents per kwh for 2011 alone.
What benefit will all this pain render consumers? It’s a fact that CO2 emissions globally are going to continue increasing, as most countries outside of Europe could give a rat’s rear about foregoing prosperity in order to play the make believe game of rescuing the planet.
If Germany succeeded in reducing its CO2 emissions 80% by 2050, what theoretical impact would it have on the environment? Ederer tells us:
If Germany reduced its share of CO2 by 80%, or even 100%, then it help to warm the planet 0.0072 °C less. As I said earlier, that’s if all figures and calculations of the IPCC are correct.
Meanwhile, the numbers are in from the German Weather Service. October 2010 was 0.9°C colder than average. If Germany’s greedy politicians get their way. This may someday be corrected to 0.9072°C cooler.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAs usual, there have been more horror reports in the media today about the Fukushima crisis in the German media. German Zettels Raum blog brings us back to reality today.
Leading the way with the horror stories was Der Spiegel, which had the headline here at its online site:
Increased Radioactivity – Nuclear Power Plant Fukushima completely evacuated”.
So terrible has the media been with reporting on the nuclear crisis in Japan that Zettels Raum was compelled to write:
You always have to remember that when dealing with the media, journalistic research is something comparable to a chimpanzee dealing with the question of how one can prove the existence of anti-matter.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s that bad. While media outlets like Der Spiegel propagated information distorted into horror stories, here’s what Japanese NHK said of the situation, 7:13 pm Japan time (emphasis added).
TEPCO: Black smoke rises from No.3 reactor
The Tokyo Electric Power Company, or TEPCO, says black smoke was seen rising from the No.3 reactor building at the quake-damaged Fukushima Daiichi nuclear plant at around 4:20 PM on Wednesday. TEPCO told reporters that it received a report 1 hour later that the smoke had gradually cleared.
The company said that the level of radiation near the main gate of the plant, 1 kilometer west of the No.3 reactor, was 265.1-microsieverts-per-hour at 5 PM. They added there had been no major change in the levels after the smoke was observed. On Monday afternoon, gray smoke was seen rising from the same reactor building. TEPCO said that the plumes turned white before disappearing.
The power company evacuated workers from the control room of the No. 3 reactor, as well as firefighters from Tokyo and Yokohama preparing for a water-spraying operation. The firefighters had to abandon their planned water spraying operation for the day.
The evacuation took place at only one of the control rooms of the 6 reactors. Click here to see a graphic on radiation levels at the plant. Here you can see that radiation levels are trending down.
Share this...FacebookTwitter "
"
Previously on WUWT we discussed the media’s fascination with “melt” when it comes to ice shelves cracking off. Then there’s also this picture that keeps getting recycled.

http://www.ogleearth.com/wissm.jpg
It is clear from the photo above that we see a stress crack, not a melt. Now we have a time lapse satellite photo series of the Wilkins ice shelf that shows the process of currents and winds causing those stresses.
Mike McMillan writes:
Fox News is reporting that the Wilkins ice shelf bridge that’s been eroding  has finally collapsed.
http://www.foxnews.com/story/0,2933,518374,00.html
I  went back to the old ESA sat photos and noticed something interesting.   I downloaded the gif animation and did some highlighting.

In  the upper area, the shelf was previously fractured, then glued together  by new ice.  I highlighted a string of drift ice in green to show what  the currents were doing during the previous collapse.  The current runs down  from the top, compressing the fractured shelf and likely busting up the new  ice glue.  The current then reverses, pulling the fractured shelf ice out to  sea. The green drift ice looks almost like a fingertip crunching into the  shelf, and clearly shows the compression.
A different process works on  the lower side of the ice bridge.  A gyre pulls
off chunks of unfractured  ice.  I’ve highlighted a chunk of non-edge ice in
pink, and we can watch it  tumble out along with a companion berg.  Note the
sea immediately refreezes  in the open areas.  One of the gif frames shows the
gyre swirling the new  ice, and I’ve enlarged the frame.

http://i40.tinypic.com/erg287.jpg
UPDATE: I slowed down the original animation to 1 frame per second, with a 2 second pause at end, per requests in comments. -Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9678f743',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Come the cold season, whenever there is some type of strong storm system near the U.S. Eastern Seaboard—be it a Nor’easter, a blizzard, or ex-hurricane Sandy—you don’t have to look very hard to find someone who will tell you that this weather is “consistent with” expectations of climate change resulting from human greenhouse gas emissions. The worse the storm, the more “consistent” it becomes.   
  
The complete collection of climate science describes just how complex the physical processes are governing such storm systems. Teasing out any anthropogenic influence, including even the direction of any influence, is darn near impossible. Claims to the contrary are usually based on a highly selective assessment of the science or the data.   
  
A case in point:   
  
The latest _en vogue_ explanation linking human greenhouse gas emissions to strong winter-season East Coast storms involves changes in the characteristics of the jet stream—a river of fast moving air in the atmosphere that influences both the strength and the forward speed of extratropical storm systems. A prominent (in the media, anyway) research study last year by Rutgers’s Jennifer Francis and University of Wisconsin’s Stephen Vavrus suggests that the declining temperature difference between the Arctic and the lower latitudes (adding greenhouse gases into the atmosphere warms colder, drier regions more so than warmer, wetter ones—with the notable exception of Antarctica) has led to changes in the jet stream which result in slower moving, and potentially stronger East Coast winter storm systems.   




Just Google “Jennifer Francis global warming” to see how this mechanism is supposedly tied to all sorts of extreme weather events.   
  
Even before the Francis and Vavrus study made it to print, we noted that their findings ran afoul of other existing literature which painted a far murkier picture of the influence (if any) that anthropogenic global warming was having in extratropical cold-season storm systems. After reviewing the literature, we cautioned:   




So where does this leave us? When the new paper by Francis and Vavrus comes to the attention of the mainstream press, it’ll play as if a warming Arctic and declining sea ice—an asserted consequence of human greenhouse gas emissions—has been definitively tied-in to all sorts of weather extremes across the U.S. No mention will be made to the fact that other research, which is many cases is more robust and detailed, has concluded nearly the opposite.



Conflicting research findings continue to be published.   
  
One was published several months ago in _Geophysical Research Letters_ by James Screen and Ian Simmonds, who looked for changes in jet stream characteristics using a different methodology than that of Francis and Vavrus. A robust signal should be apparent no matter how you look at it (within reason). But Screen and Simmonds found few statistically robust changes and what changes they did find were contrasting depending on the methodology they used. They noted that the changes they found were much smaller (and non-significant) than the large (and significant) changes reported by Francis and Vavrus. Screen and Simmonds concluded that their findings held “different and complex possible implications for midlatitude weather, and we encourage further work to better understand these.”   
  
In other words, the picture is far less clear than that described by Francis and Vavrus.   
  
This point is further driven home by a another paper just accepted in _Geophysical Research Letters_ by Colorado State University’s Elizabeth Barnes. Barnes, too, examined the relative warming in the Arctic and its possible link to extreme weather events in the Northern Hemisphere mid-latitudes. In a nutshell, she found little if any definitive relationship—again in contrast to the Francis and Vavrus results. Barnes discussed this discrepancy directly:   




We conclude that the mechanism put forth by previous studies (e.g. Francis and Vavrus [FV12]; Liu et al. [2012]), that amplified polar warming has led to the increased occurrence of slow-moving weather patterns and blocking episodes, is unsupported by the observations….A recent study by Screen and Simmonds [2013] also provides evidence that the trends in planetary waves suggested by FV12 may be an artifact of their methodology… The Arctic is changing rapidly, and these changes will likely have profound effects on the Northern Hemisphere. This study, however, highlights that the relationship between Arctic Amplification and midlatitude weather is complex.



The more folks look the less robust the popular global-warming-is-leading-to-more-extreme-winter-storms finding by Jennifer Francis and Stephen Vavrus seems to be.   
  
It’ll be interesting to see during this upcoming winter season how often the press—which seems intent on seeking to relate all bad weather events to anthropogenic global warming—turns to the Francis and Vavrus explanation of winter weather events, and whether or not the growing body of new and conflicting science is ever brought up.   
  
If you don’t see it in the morning paper, you will most certainly find it here!   
  
**References:**   
  
Barnes, E., 2013. Revisiting the evidence linking Arctic Amplification to extreme weather in the midlatitude. _Geophysical Research Letters_ , in press, doi: 10.1002/grl.50880.   
  
Francis, J. A. and S. J. Vavrus, 2012: Evidence linking Arctic amplification to extreme weather in mid-latitudes. _Geophysical Research Letters_ , **39 (6)** , L06 801, doi:10.1029/2012GL051000.   
  
Screen, J. and I. Simmonds, 2013: Exploring links between Arctic amplification and midlatitude weather. _Geophysical Research Letters_ , **40** , 1–6, doi:10.1002/GRL.50174.   
  
  
  
  
  



"
"
Share this...FacebookTwitterScience turns malignant
Next week on August 3, I’ll be releasing my latest list of climate scandals, a gate-update (see Current list of climate scandals). This is a month earlier than originally planned.
The list that’s posted now is visited on average about 100 times a day. Clearly it has become some sort of resource.
Unsurprisingly, the new list coming out has grown, and it will continue to grow. This is assured because of the way climate science is operated, funded, rewarded and politicised.
The disease is hopelessly chronic and there is no treatment in sight. The system is designed, built and programmed to keep producing many more scandals. Already I see dozens of new gates in incubation.
“Climate science” even has its own immune system that works to keep out the deadly virus called “truth”.  The reality of that immune system became clear with the Muir Russell, Oxburgh and Penn State enquiries.
Soon I will have to break the climate-gate list into Volumes 1 and 2.  I have no doubts about this.
The real threat, unfortunately, is that “climate science” risks becoming a malignant cancer that will threaten to spread to other fields of science and to our public institutions. In some cases it already has. Because of “climate science”, the public is losing trust in science and the civic institutions that are supposed to police it. As mistrust of climate science reaches ever higher levels, so will the public mistrust in other fields.
Scientists in these other fields need to take notice.
Share this...FacebookTwitter "
"

Nothing has been more embarrassing to the administration’s global warming apocalyptics than a few satellites, first launched in 1978, that resolutely refuse to find any warming of the lower atmosphere, which all the administration’s modelers and all the administration’s men predict should be heating with reckless abandon. 



When the satellite data were first published eight years ago by Roy Spencer and John Christy, of NASA and the University of Alabama respectively, the record was only 10 years long; the global warmers chanted in unison that the record was too short and therefore only a blip. Steve Schneider, who was the federal guru of climate change (he’s now with population apocalyptic Paul Ehrlich at Stanford), told _Science_ that “the next ten years will tell the story.”



They have, but the administration hasn’t listened. The story is that there’s a slight (but statistically significant) _cooling_ __ trend in the satellite data. It’s not surprising — but it _is_ scientifically dismaying — that the administration wants that trend to stop. So when the White House held its big global warming show last fall, up popped the leader of one of the nation’s most prominent environmental organizations (hint: it’s the one that destroyed the nuclear power business, thereby causing the greenhouse increase to begin with!) to declaim, “We have got to _do something_ about the satellite.” 



Climate watchers have been wondering how long it would take the $2.1 billion the federal government spends each year on global change research to “do something.” It did on February 23, when a California rocket scientist sent a manuscript to _Nature_ magazine, claiming he had found the error in the satellite data and that the atmosphere was actually warming up after all. (Sorry, can’t mention the author. _Nature_ has a hard‐​and‐​fast rule about blabbing to the press, and they’d have to pull the paper.)



The scientist calculated how much the solar wind — a stream of high‐​energy particles that exerts a slight force on everything in the solar system — would slow the satellites. In slowing down, the satellites fall a bit toward the earth’s surface, and they “see” a smaller area from which to take their measurements. To the satellites, a reduced area would appear colder than it really is. 



The satellite data could show a spurious cooling trend that exactly matched the balloon temperatures only if there were an exactly similar bias in the balloon readings. That’s simply not the case.



Adjusting for that induces a compensating warming in the satellite data of about 0.12 degrees (C) per decade. When that warming is coupled with the currently observed cooling in the record (0.04 degrees per decade), the result is a slight warming trend of 0.08 degrees every 10 years. That is still way below where it’s supposed to be (the computer models that served as the basis for the Kyoto global warming treaty predicted about 0.35 degrees per decade by now), but at least it’s a warming. 



The rocket scientist then sent his results not only to _Nature_ but to everyone else of scientific note who carries the administration’s precipitation. If you don’t believe there’s a cabal in the science community cheering for apocalyptic global warming, you ought to see the e‐​mail list. The only ones on it who weren’t cheerleaders happen to be Messrs. Spencer and Christy, whose satellite was gored. 



Included on the list were the Big Agency science administrators, who dutifully made sure Al Gore got a copy. He was reportedly ecstatic. And _Nature_ itself, whose editorial stance increasingly resembles the vice president’s view, helped things along ASAP. Send a paper to _Nature_ showing that global warming may not be such a big deal, and they’ll take six months to review it before, in all probability, sending a terse rejection letter. But this one got turned around in 10 days! 



Balance need not apply. Spencer and Christy asked the rocket scientist if they could publish a companion response but were told no, it would take too long. What’s the rush?



The rush is that the apocalyptics want to drop this news on the public like they did the “Ozone Hole over Kennebunkport” in 1992. That’s when Gore, still in the Senate, trumpeted some NASA chemical data indicating that a Northern Hemisphere ozone hole was imminent (the late winter ozone depletions are largely confined to Antarctica for good physical reasons). By stampeding the Senate into panic, Gore rammed through a 99‐​to‐​1 vote for an accelerated ban on certain industrial refrigerants. Only two weeks later, NASA had data showing that their initial pronouncement was a gross exaggeration, and the ozone hole never appeared. NASA wasn’t forthcoming until after the Senate acted.



If Spencer and Christy are allowed to respond concurrently, they will blow the paper to kingdom come. That’s because the satellite temperatures match up perfectly with a totally independent measure, taken twice a day by weather balloons as they ascend through the lower atmosphere. The satellite data could show a spurious cooling trend that exactly matched the balloon temperatures only if there were an exactly similar bias in the balloon readings.



That’s simply not the case. Or did thousands of folks launching millions of weather balloons over the last 20 years somehow decide to fudge the numbers so they would match up perfectly with those of satellites that didn’t show global warming? Talk about a massive conspiracy of right‐​wing airheads!
"
"
This NOAA press release just showed up in my inbox, it seems to be a completely different take on the Hurricane season than that of Florida State’s COAPS and Ryan Maue who says:
Record inactivity continues:  Past 24-months of Northern Hemisphere TC activity (ACE) lowest in 30-years.
 
Global and Northern Hemisphere Accumulated Cyclone Energy: 24 month running sum through October 31, 2008. Note that the year indicated represents the value of ACE through the previous 24-months.
This was discussed at length at Climate Audit here


FOR  IMMEDIATE RELEASE
Contact: Carmeyia  Gillis
Nov. 26,  2008
301-763-8000,  ext. 7163 (office)
  240-882-9047  (cellular)
Dennis  Feltgen
305-229-4404  (office)
305-433-1933  (cellular)
 
Atlantic  Hurricane Season Sets Records 
The 2008 Atlantic Hurricane Season  officially comes to a close on Sunday, marking the end of a season that produced  a record number of consecutive storms to strike the United States and ranks as  one of the more active seasons in the 64 years since comprehensive records  began.
A total of 16 named storms formed this  season, based on an operational estimate by NOAA’s National Hurricane Center.  The storms included eight hurricanes, five of which were major hurricanes at  Category 3 strength or higher. These numbers fall within the ranges predicted in  NOAA’s pre- and mid-season outlooks issued in May and August. The August outlook  called for 14 to 18 named storms, seven to 10 hurricanes and three to six major  hurricanes. An average season has 11 named storms, six hurricanes and two major  hurricanes.
“This year’s hurricane season  continues the current active hurricane era and is the tenth season to produce  above-normal activity in the past 14 years,” said Gerry Bell, Ph.D., lead  seasonal hurricane forecaster at NOAA’s Climate Prediction  Center.
Overall, the season is tied as the  fourth most active in terms of named storms (16) and major hurricanes (five),  and is tied as the fifth most active in terms of hurricanes (eight) since 1944,  which was the first year aircraft missions flew into tropical storms and  hurricanes.
For the first time on record, six  consecutive tropical cyclones (Dolly, Edouard, Fay, Gustav, Hanna and Ike) made  landfall on the U.S. mainland and a record three major hurricanes (Gustav, Ike  and Paloma) struck Cuba. This is also the first Atlantic season to have a major  hurricane (Category 3) form in five consecutive months (July: Bertha, August:  Gustav, September: Ike, October: Omar, November: Paloma).
Bell attributes this year’s above-normal  season to conditions that include:

An ongoing multi-decadal signal. This  combination of ocean and atmospheric conditions has  spawned increased hurricane activity since 1995.
Lingering La Niña effects. Although  the La Niña that began in the Fall of 2007 ended in June, its influence of light  wind shear lingered.
Warmer tropical  Atlantic  Ocean temperatures. On average, the tropical  Atlantic was about 1.0 degree Fahrenheit  above normal during the peak of the season.

NOAA’s National Hurricane Center is  conducting comprehensive post-event assessments of each named storm of the  season. Some of the early noteworthy findings include:

Bertha was a tropical cyclone for 17  days (July 3-20), making it the longest-lived July storm on record in the  Atlantic  Basin.
Fay is the only storm on record to  make landfall four times in the state of Florida, and to prompt tropical storm  and hurricane watches and warnings for the state’s entire coastline (at various  times during its August lifespan).
Paloma, reaching Category 4 status  with top winds of 145 mph, is the second strongest November hurricane on record  (behind Lenny in 1999 with top winds of 155 mph).

Much of the storm-specific information  is based on operational estimates and some changes could be made during the  review process that is underway.
“The information we’ll gain by  assessing the events from the 2008 hurricane season will help us do an even  better job in the future,” said Bill Read, director of NOAA’s National Hurricane  Center. “With this season behind us, it’s time to prepare for the one that lies  ahead.”
NOAA will issue its initial 2009  Atlantic Hurricane Outlook in May, prior to the official start of the season on  June 1.
NOAA understands and predicts changes  in the Earth’s environment, from the depths of the ocean to the surface of the  sun, and conserves and manages our coastal and marine  resources.
A graphic track map of this season’s  storms and satellite visualization of the entire season is available at  http://www.noaa.gov.
On the Web:
NOAA’s Climate Prediction Center:  http://www.cpc.ncep.noaa.gov
NOAA’s National Hurricane Center:  http://www.hurricanes.gov
NHC 2008 Tropical Cyclone Reports:  http://www.nhc.noaa.gov/2008atlan.shtml
###


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9af631e5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This sums the banking issue well.
“Is anyone even paying attention to these Wing Nut AGW people? With 1/2 of America worried about having to eat cat food during their retirement, global warming is the last thing on their mind.”
From “Jeff” in comments


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c2e4a48',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Gyms and non-essential shops in all parts of England will be allowed to reopen when lockdown ends next month, the prime minister has announced.**
Boris Johnson told the Commons that the three-tiered regional measures will return from 2 December, but he added that each tier will be toughened.
Spectators will be allowed to return to some sporting events, and weddings and collective worship will resume.
Regions will not find out which tier they are in until Thursday.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
The PM, who is self-isolating after meeting an MP who later tested positive for coronavirus, told MPs via video link he expected ""more regions will fall - at least temporarily - into higher levels than before"".
He said he was ""very sorry"" for the ""hardship"" that such restrictions would cause business owners.
Speaking later at a Downing Street briefing, Mr Johnson added that ""things will look and feel very different"" after Easter, with a vaccine and mass testing.
He warned the months ahead ""will be hard, they will be cold"" - but added that with a ""favourable wind"" the majority of people most in need of a vaccination might be able to get one by Easter.
Until then, the PM said, there would be a three-pronged approach of ""tough tiering, mass community testing, and [the] roll-out of vaccines"".
Describing how the tiers had become tougher, the PM said:
Where pubs and restaurants are allowed to open, last orders will now be at 10pm, with drinkers allowed a further hour to finish their drinks.
Indoor performances - such as those at the theatre - will also return in the lower two tiers, although with reduced capacity.
In terms of households mixing, in tier one a maximum of six people can meet indoors or outdoors; in tier two, there is no mixing of households indoors, and a maximum of six people can meet outdoors; and in tier three - the toughest tier - household mixing is not allowed indoors, or in most outdoor places.
In all tiers, exceptions apply for support bubbles. From 2 December, parents with babies under the age of one can form a support bubble with another household.
Mr Johnson said the tiers would now be a uniform set of rules, with no negotiations on additional measures for any particular region.
Measures in Scotland, Wales and Northern Ireland continue to be decided by the devolved administrations, but a joint approach to Christmas, involving all four nations, will be set out later in the week.
The prime minister said: ""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
For the third week running we have had some positive vaccine news, but the announcement about the toughened tiers is a reminder, if we needed any, that the next few months will be tough.
Ministers and advisers have been hinting for the past week that the tiers will be toughened - and that is exactly what has happened.
Attention will now naturally turn to which areas will be in which tiers.
Deciding that is a complex equation that will take into account whether the cases are going up or down, the percentage of tests that are positive, hospital pressures and infection rates among older age groups.
To give a flavour of how complex this is places in the North West and Yorkshire have some of the highest rates but they are falling the fastest.
London and the South East have lower rates and more hospital capacity but cases are going up.
Fine judgements will have to be made. We will find out on Thursday.
Mr Johnson also announced changes to sport for both spectators and participants.
While elite sport has continued behind closed doors during the lockdown, grassroots and amateur sport has been halted since 5 November.
From 2 December, outdoor sports can resume, while spectators will be allowed to return in limited numbers. Some organised indoor sports can also resume.
In the lowest risk areas, a maximum of 50% occupancy of a stadium, or 4,000 fans - whichever is smaller - will be allowed to return. In tier two, that drops to 2,000 fans or 50% capacity, whichever is smaller.
In tier three, fans will continue to be barred from grounds.
In tiers one and two, business events can also resume inside and outside with tight capacity limits and social distancing, as can indoor performances in theatres and concert halls, the government's plan says.
Labour leader Sir Keir Starmer described the government's return to the regional system as ""risky... because the previous three-tier system didn't work"".
He added that decisions on which areas will belong to each tier must be taken without delay - ""I just can't emphasise how important it is that these decisions are taken very quickly and very clearly so everybody can plan.
""That is obviously particularly important for the millions who were in restrictions before the national lockdown, because the message to them today seems to be 'you will almost certainly be back where you were before the national lockdown - probably in even stricter restrictions'.""
Helen Dickinson, of the British Retail Consortium, said shops would be ""relieved"" at the decision to allow them to reopen.
""Sage data has always highlighted that retail is a safe environment, and firms have spent hundreds of millions on safety measures including Perspex screens, additional cleaning, and social distancing and will continue to follow all safety guidance,"" she said.
But the UK hospitality industry warned the new rules ""are killing Christmas and beyond"" and said pubs, restaurants and hotels faced going bust.
Meanwhile, a further 15,450 positive coronavirus cases were recorded across the UK on Monday. There have also been a further 206 deaths within 28 days of a positive test. Figures can be lower on a Monday, due to a lag in reporting.
Earlier, it was announced that daily coronavirus tests will be offered to close contacts of people who have tested positive in England, as a way to reduce the current 14-day quarantine period.
Mr Johnson said people will be offered tests every day for a week - and they will not need to isolate unless they test positive.
He also said rapid tests will allow every care home resident to have up to two visitors tested twice a week."
"
During our last check in, we had a look at northern Canada from the Arctic Circle to the North pole, and found we had quite a ways to go before we see an “ice free arctic” this year as some have speculated.
Today I did a check of the NASA rapidfire site for TERRA/MODIS satellite images and grabbed a view showing northern Greenland all the way to the North Pole.
There’s some bergy bits on the northeastern shore of Greenland, but in the cloud free area extending all the way to the pole, it appears to still be solid ice.

Click for a larger image – Note: image has been rotated 90° clockwise and sat view sector icon and time stamp added, along with “N” for north pole marker.
Link to original source image is here:
http://rapidfire.sci.gsfc.nasa.gov/realtime/single.php?T082121805
With more than half of the summer melt season gone, it looks like an uphill battle for an ice-free arctic this year.
Here is another view from today from the Aqua satellite:

Click for a larger image – Note: image has been rotated 90° counter- clockwise and sat view sector icon and time stamp added, along with “N” for north pole marker.
Source image is here:
http://rapidfire.sci.gsfc.nasa.gov/realtime/single.php?A082121655
This dovetails with a press release and news story about more ice than normal in the Barents Sea
From the Barents Observer:
http://www.barentsobserver.com/?cat=16149&id=4498513
New data from  the Norwegian Meteorological Institute shows that there is more ice than normal  in the Arctic waters north of the Svalbard archipelago.
In most years, there are open waters in the area north of the archipelago in  July month. Studies from this year however show that the area is covered by ice,  the Meteorological Institute writes in a press release.
In mid-July, the research vessel Lance and the Swedish ship MV Stockholm got  stuck in ice in the area and needed help from the Norwegian Coast Guard to get  loose.
The ice findings from the area spurred surprise among the researchers, many  of whom expect the very North Pole to be ice-free by September this year.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d8b805c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The world's largest maker of latex gloves will shut more than half of its factories after almost 2,500 employees tested positive for coronavirus.**
Malaysia's Top Glove will close down 28 plants in phases as it seeks to control the outbreak, authorities said.
The company has seen a huge surge in demand for its personal protective gear since the start of the pandemic.
However, there have been concerns about the working conditions of the low-paid migrant workers it relies upon.
On Monday, Malaysia's health ministry reported a sharp rise in Covid-19 cases in areas where Top Glove factories and dormitories are located.
Nearly 5,800 workers have been screened so far with 2,453 testing positive, it said.
Top Glove operates 41 factories in Malaysia, with many of its workers coming from Nepal and living in crowded dormitory complexes.
""All those who tested positive have been hospitalised and their close contacts have been quarantined to avoid infecting other workers,"" Director-General of Health Noor Hisham Abdullah told Reuters news agency.
It is unclear when the factory closures will begin but they are scheduled to take place in stages.
Top Glove has been in the global spotlight for its record high profits this year, but also over allegations of exploitative labour practices at the firm.
In July, the United States banned the import of gloves from two of the company's subsidiaries following forced labour concerns.
A recent report from the US Department of Labour raised the same issue, pointing to the high recruitment fees overseas migrant workers must pay to secure employment in the rubber glove industry which often results in debt bondage.
In September migrant workers told the Los Angeles Times about difficult working conditions at Top Glove factories, describing 72-hour work weeks, cramped living conditions and low wages.
A few weeks later, Top Glove said it had raised remediation payments to compensate workers for recruitment fees after recommendations from an independent consultant.
Glorene Das, executive director of Tenaganita, a Kuala Lumpur-based NGO that focuses on labour rights, said some Malaysian firms that depend on a migrant workforce were ""failing to meet the basic needs of their workers"".
""These workers are vulnerable because they live and work in congested shared quarters and do work that does not make it possible to practice strict social distancing,"" she told the BBC.
""During these times employers have a huge responsibility towards them but we are hearing of cases where they are not providing workers with sufficient food or even withholding their wages,"" she added.
Shares in Top Glove fell by 7.5% on Tuesday after the factory closures were announced. But despite the slump the company's shares have surged over four fold this year, reported Reuters."
"
Share this...FacebookTwitterAwhile back I did a story on Anders Levermann of the über-alarmist Potsdam Institute For Climate Impact Research (PIK), and reader Arnd B brought my attention to an article called: Our systems are especially vulnerable, which appeared in the online Frankfurter Allgemeine Zeitung (FAZ) in late December.

Hieronymous Bosch paints a scene of a Renaissance mountebank fleecing incredulous gamblers.
Levermann makes a number of interesting comments that provide insight on how the PIK views climate. Unfortunately, all his predictions are based on models, and ignore real-life observations.
Global warming could enhance cold weather
Levermann starts off saying the bitter cold and snow in Germany last month is a sure sign of “how out-of-whack the climate system is.” Levermann serves up the “science” that supports it:
The current cold weather in Europe is everything but evidence against climate change, rather it could even be enhanced by global warming. Colleagues have discovered the mechanism for this: Through the ice melt in the Kara Sea, high pressure zones can form, which then divert the Eurasian winds and lead to cold temperatures in Europe.”
It takes a real climate scientist to make such a profound discovery, and that with no data to back it up. Not only that, Levermann adds:
The more and faster we emit greenhouse gases, the more our climate gets knocked out of whack.”
At this point, I have to ask myself: “Just how gullible must the average FAZ reader be to take this seriously?
Extreme weather events prove manmade climate change
And as usual Levermann goes down the laundry list of last year’s weather events…floods in Pakistan, heat wave in Russia, mudslides in Brazil, etc., etc. and claims this is evidence supporting the man-made global warming link, and that it had all been predicted by models. Yet, Levermann forgets to mention that the accumulated cyclone energy (ACE) was near a record low last year, and that temperatures have not risen over the last decade – something his models have missed. Still he insists:
It’s now practically sure that in a rapidly warming world we have to expect more and stronger extreme events.”
PIK models can now see to the year 2200
Keep in mind that Anders Levermann is a lead author for the next IPCC Report on the subject of sea levels. The next report will deliver the latest “projections” based on various CO2 output scenarios. So where does Levermann say the globe is headed?
What we can already say, based on our latest studies, is: We currently find ourselves on the warmest possible future trajectory […] The temperature projection shows a warming of more than eight degrees Celsius in the year 2200.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Unfortunately for Levermann, there hasn’t been any warming in a decade, as the following HadCrut chart shows:
HadCrut shows cooling over the last decade. (WoodFor Trees)
Remember that he is a lead author on sea ice for the upcoming IPCC 5th assessment report. What kind of sea level projection do you think he’ll concoct with 8°C of warming? Expect the 5th assessment report to be worse in scientific quality than the 4th report of 2007. Sci-fi sequels tend to get worse and worse as they are taken over by B-rated directors.
Anders Levermann (Photo: PIK)
Ignoring real world observation and data, Levermann stares deeper into his crystal ball and sees only horrors. He wonders if man will be able to adapt to these rapidly changing climate conditions (the ones in his crystal ball). What are the limits of human adaptability, he asks? 4°C? 6°C?
Warming 50 times faster than the warming that ended the ice age
Claiming that his crystal ball sees 8°C of warming over the next 190 years, he says that this 4°C rise per century will be unprecedented. Levermann says the difference between an optimum and an ice age is about 5°C, and that it took 5000 years for the earth to make the 5°C climb out of the last ice age:
The transition from ice age to warm period lasted a good five thousand years. When man continues to emit greenhouse gases unabated, then we will reach the same warming 50 times faster than in the past.”
That’s assuming his models are correct. Looking at the above HadCrut chart, there has been no warming. And Levermann doesn’t mention that most of the temperature rise ending the last ice age took place in about 1000 years, and the temperature difference was more than 5°C. It was closer to 8°C. So he’s fudging there quite a bit.
He also ignores the huge temperature swings of up to 6°C which occurred in just a matter of a few decades during the Younger Dryas – all naturally.
Of course, Levermann doesn’t forget to play emotion-card Africa, and predicts dire scenarios for the poor continent.
It is probable that in such a situation, countries like Bangladesh and parts of Africa will have become uninhabitable. Whether the drinking water supply collapses because of drought, or sea water claiming the land, or because agriculture becoming impossible. Even without the extreme events, the United Nations estimates that the number of climate refugees will reach 90 million if the sea level rises 1 meter.
Here he ignores studies that show the African Sahara is shrinking and getting more desperately needed rainfall during this modern warm period. And he ignores that sea levels have decelerated over the last 5 years. And even if the sea level did rise 1 meter, something that only the most fanatic among us predict, it would not happen overnight. Most humans just don’t have the habit of standing still for 100 years and watching the water rise around them.
Finally, Levermann ends by saying:
The wall that we are racing towards is hidden in fog, but it is there!
At PIK it’s: If you can’t see it, then it’s proof it’s there. Just believe us.
Share this...FacebookTwitter "
"

Given his very bad temper, folks have been wondering when Al Gore and his environmental soulmates at the White House were going to get nasty with people who don’t share their view of global warming. Well, the time is now, and it looks like another Scorched Earther. 



Judging by Mr. Gore’s heated rhetoric lately, he sees people who disagree with him as demonic beings who’ll be doing the scorching. _U.S. News & World Report_ quotes him as saying, “I really can’t think of a clearer demonstration of the contrast between Democratic policies and Republican policies than what happened under Scar compared to what happened under Simba.” 



For the few of you who have not seen Disney’s “Lion King,” Scar is the evil leader who takes over the pride. A terrible drought ensues. The women are enslaved. The “Circle of Life” (Elton John’s catchy theme song) is destroyed. For Gore, those are Republican values. When the good Simba returns after a few years away (could this be analogous to Gore at Harvard?), the rains return and balance is restored. Democrats, you see, can change the climate. 



I note parenthetically that drought in central Africa is often related to El Niño, that Gore has been trying to blame El Niño on global warming (scientifically, the exact opposite may be true) and … , well, you get the way his mind works. His global warming world has always been a struggle between good and evil, between New Age and the free market. More than 10 years ago, he wrote this about global warming: ” ‘Evil’ and ‘Good’ are not terms used frequently by politicians [pleeze, Al!]. Yet I do not see how this problem can be solved without reference to spiritual values.” 



Al’s world view is enthusiastically shared by Dirk Forrister, a rock‐​hard Gore man who heads the White House Office of Global Climate Change. Recently he told a Washington, D.C., meeting of the prestigious Energy Institute that critics who disagree with the official view of global warming are “clowns.”  




Science is “frivolous,” to be dismissed quite casually when it turns out to be inconvenient. 



Half an hour later, Forrister blew up when confronted with the most recent scientific findings, which provide compelling and conclusive evidence that folks who have beaten the apocalyptic global warming drum for the last decade have been just plain wrong. 



In 1990 the United Nations Intergovernmental Panel on Climate Change (IPCC) offered a “best estimate” prediction of warming over the next century of 3.2 degrees Celsius. By 1995, thanks in part to incessant attacks by so‐​called skeptics, the warming estimate was lowered to 2.0°C. 



Three months ago Department of Commerce researcher Ed Dlugokenky published a paper in _Nature_ demonstrating that atmospheric methane — an important man‐​created greenhouse gas — is likely to show very little change in the next century. That forces the warming estimate down to about 1.75°C. Forrister called this observation “frivolous.” 



At the same time, Norwegian researcher Gunnar Myhre discovered that the **direct heating effect of carbon dioxide has been overestimated** , something the “skeptics” had maintained had to be true because the planet has warmed so little. His work was published in _Geophysical Research Letters_. That drops the warming estimate to 1.5°C. Forrister called this “frivolous.” 



A popular climate model from 10 years ago that served as much of the basis for the infamous U.N. Climate Treaty and the subsequent Kyoto Protocol (currently 0 for 95 in the Senate) said that, over the past decade, the globe should have warmed about 0.45°C. The observed temperature as measured at the surface, inflated by an urban (warm) bias, shows warming of just 0.11°C. Weather balloon thermistors and barometers, two independent instruments, showed **cooling** , as do the satellites, even after correction for recently discovered orbital drift. Forrister shouted “frivolous.” 



NASA scientist James Hansen has recently argued that the reason dramatic warming didn’t show up as he had forecast was because the soil and vegetation are taking up carbon dioxide at an increasing rate. That makes the planet greener, not browner (sorry, Carol!). Accounting for Hansen’s work published in the _Proceedings of the National Academy of Sciences_ , lowers 21st‐​century warming to about 1.25°C. Forrister called this “frivolous.” 



Tom Wigley of the National Science Foundation has just published a paper in _Geophysical Research Letters_ showing that if every nation met its commitments under the Kyoto Protocol, planetary cooling would be an undetectable 0.07°C by 2050, compared to what the temperature would be if we did nothing. My own research, recently published in _Climate Research_ , shows that the largest warmings occur in the coldest winter air masses rather than in the summer. “These are all frivolous arguments,” Forrister said. 



En coda, Forrister was asked if there would even be a Kyoto Protocol if the climate modelers had told us 10 years ago that it would only warm 1.0–1.5°C over the next century. After a long pause, he said (as best as I can recall), “I don’t know. Maybe yes, maybe no.” 



Having thus opened Pandora’s floodgates, Forrister was asked if the new findings might not make it appropriate for the Senate to pass a resolution forcing the president to withdraw the United States from the U.N. treaty, which allows such an option. “Frivolous!” he shouted. “You just can’t go making frivolous arguments like that!” 



Thus the new White House policy: those who do not agree with their (now thoroughly discredited) view of global warming are evil and will scorch the earth. Science is “frivolous,” to be dismissed quite casually when it turns out to be inconvenient. Stay tuned. Things can only get worse.
"
"
One of the common themes seen with the surfacestations.org project has been the proximity of BBQ grills to official NOAA thermometers used in the United States Historical Climate Network (USHCN). Despite now having surveyed over 77% of the 1221 station network, some truths continue to be self evident.
USHCN climate station of record, Hartington, NE
This station was photographed by our prolific volunteer, Eric Gamberg. The proximity to the concrete patio earns this station a CRN4 rating, it may be a CRN5 when they wheel out the BBQ away from the house. But who knows? The grilling schedule is not part of the metadata.
But fear not, NASA GISS adjusts for such problems of concrete and BBQ grills. Consider the following blink comparator:
Notice how the past is adjusted cooler, increasing the trend
Source: NASA GISS
USHCN RAW:
http://data.giss.nasa.gov/cgi-bin/gistemp/gistemp_station.py?id=425744450020&data_set=1&num_neighbors=1
GISS Homogenized:
http://data.giss.nasa.gov/cgi-bin/gistemp/gistemp_station.py?id=425744450020&data_set=2&num_neighbors=1
I’m not sure why the hinge point is 1978, perhaps that’s when the homeowner acquired the BBQ? Sure, that is an absurd claim, but certainly no more absurd than the GISS homogenization adjustment itself. Adjusting the past increases the overall positive slope of the temperature trend.
For those new to the whole concept of USHCN stations, the NOAA thermometer is the white slatted object on the post in the center of the photo. It is known as an MMTS thermometer and a cable goes from it into the home where the volunteer observer will write down the high and low into the B91 logbook and send in the report once a month to the National Climatic Data Center (NCDC).There are more photos of this station which you can see in my online station database.
The Gallery of photos can be seen here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e95f1fddf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
Prof. Hans von Storch’s site Klimazwiebel here brings our attention to a new site called Information portal climate change posted by the Austrian ZAMG – Austria’s Central Bureau for Meteorology and Geophysics.
Normally Klimazwiebel posts in English, but because, I suppose, the links and related literature are in German, Prof. von Storch posted this one in German.
The new ZAMG site has the purpose of providing site visitors with a trustworthy resource on the subject of climate science. It is for informing the public.
One of the contributors to the site is Dr. Reinhard Böhm of the ZAMG. Translating the quote provided by Klimazwiebel, Reinhard Böhm writes, interestingly:
In more than 80 individual articles, which have about 1000 references and links and offer additional literature, we wish to create a work of ‘Public Science’ at a scientifically reasonable level. We think we have been successful in achieving this. With this, as a body of rational information, we hope to deliver a counter view – especially at this time – when the COP 16 conference in Mexico will again provide the usual international climate hype.”
Even a moderate warmist like Böhm admits what the IPCC is all about.
In my view the ZAMG Internet resource is warmist, yet rational and not alarmist. I haven’t yet closely examined the links and data the site uses to make the assertions it does on the subject of science, but to me my first impression is that it’s too warmist and does not explain why this warming is any different from previous warmings throughout history.
ZAMG is a government funded institution. One thing is sure: climate science is very politicised and depends heavily on the source of funding. One can only go out so far on a limb before it gets sawn off.
Reinhard Böhm is the author of the book Heiße Luft. Reizwort Klimawandel – Fakten, Ängste, Geschäfte (Hot Air Word of Controversy Climate Change – Facts, Fears, Business).

Share this...FacebookTwitter "
"

People who don’t think the federal government sometimes exaggerates things a wee bit obviously did not survive the Y2K crisis. Or perhaps they merely sizzled away in the record heat endured by our fair Republic, as recently reported by the Department of Commerce, which has pronounced 1998–99 the warmest years for which we have adequate records — 1998 comes in as hottest and 1999 as second warmest. 



One Y2K lesson is that what is said in Washington isn’t necessarily what is, depending on what “is” means. In the case of the nation’s or the globe’s temperature, our government has chosen to trumpet one particular climate history out of several that are available. Not surprisingly, the government tells us about the hottest, while the rest are not remarkable at all. 



The heated pronouncement, which actually came from the National Climatic Data Center (NCDC), is not a result of cooking the books. Instead, it is a result of very selective reading. 



The particularly hot data set, known as the Historical Climate Network (HCN), comes from several hundred rural weather stations selected from the roughly 16,000 official sites that are available. But let’s consider two other data sets, one from NASA and the other from the very same NCDC. 



NASA’s record is compiled by James Hansen, director of the Goddard Institute for Space Studies. While the HCN has 1999 as the second‐​hottest year on record (records go back only to 1895), Hansen’s record ranks it as a very unremarkable 14th warmest, putting it nowhere close to the blazing year of 1934, when there was a real drought (as opposed to 1999, when less of the country than normal experienced extreme dryness). 



NCDC’s “other” history is known as “Climatological Division” (CD) record. It uses virtually all of the available data, aggregates them into 344 geographic units (CDs) that are thought to have some climate homogeneity, and then totals them, adjusting for the area occupied by each CD. In this record, both 1998 and 1999 temperatures fall beneath those of 1934. Instead of being second warmest, 1999 will be between fourth and eighth, depending on December data that have not yet been entered. 



The peculiar thing about the discrepancy between the CD and HCN records is that the former includes a few stations that are known to be artificially warming because cities have a way of growing up around their weather stations. Why the HCN should now be running warmer than this record is a mystery to everyone. But why the feds only trumpeted the HCN heat should be a mystery to no one. They are trying to whip up hysteria in order to shame the Senate into approving the United Nations’ Kyoto Protocol on climate change. Currently, 11 senators (of the required 67) support this economically disastrous treaty. 



The United Nations itself recently made a similar pronouncement about 1998–99 for global temperature. Their record goes to 1860. (Ask yourself how the U.S. record goes back only to 1895 while the U.N.‘s global history starts in 1860, and you will get an idea of how reliable are our historical statements about “global” warming.) 



Needless to say, the U.N.‘s story was blatted by every media network that serves it. But there are two other measures of global temperature that, like the NCDC and NASA records for the United States, were not so hot. 



The first is University of Alabama climatologist John Christy’s satellite history, which has been carefully corrected for instrument and orbit changes, that shows 1999 to be slightly cooler than the average for the 21 years in which the platforms have been taking our temperature. There are 12 warmer years and eight cooler ones in this history, which shows a slight warming trend only because of the big 1998 El Niño. (This means that the decade from 1998 to 2007 will very likely show a cooling trend.) 



The satellite temperatures are known to closely track those measured by weather balloons in the atmospheric layer from 5,000 to 30,000 feet — a zone forecast by computer models of global warming to be heating even more rapidly than the surface. This record extends back to 1958. Fifteen years were warmer than 1999 and 27 were cooler. 



The resolution of the difference between the U.N.‘s surface temperatures and those measured by the satellites and the weather balloons may spell the end of the global warming crisis. More and more, it appears that the reason they diverge is that warming is trapped largely in very cold airmasses in Siberia that don’t extend up to 5,000 feet, which is the altitude at which the balloon record begins. No one has yet to hear Russians clamoring for a return to the climate of the Stalin era. 



As Casey Stengel used to say, “You could look it up.” The NASA data are at www​.giss​.nasa​.gov/data, the NCDC CD history is at ftp​.ncdc​.noaa​.gov/pub, and the satellite record is at ftp://​vor​tex​.atmos​.uah​.edu/msu. 



Any way you look at it, governments have been less than truthful in telling the whole story about the heat of 1999. But what do you expect? After all, it is the year 2K.
"
"
Share this...FacebookTwitterMilutin Milankovitch
Milankovic Cycles and Climate Change
Is it distance from the sun, or length of summer?
By Ed Caryl
A draft paper by Dr. James Hansen and Dr. Makiko Sato triggered a rebuttal by Dr. Martin Hertzberg on WUWT. The Hansen paper made a claim that weaker insolation in the Northern Hemisphere (NH) due to distance from the sun in NH winter should lead to cooling, but that this is offset by increasing CO2.
The Dr. Hertzeberg rebuttal claimed that the warming was due to the longer length of summer in the NH. Both are wrong!  Both are victims of Confirmation Bias, seeing only data that confirms their beliefs.
It seemed a little strange that the Hansen paper was written for a conference in Milankovic’s honor, yet the paper does not contain the famous diagram illustrating the Milankovic Cycle (or Milankovitch, both spellings are common in the literature), nor does the Hertzberg rebuttal. Here it is:

Figure 1. Source: Wikipedia Commons
The important part of this chart, and the reason both authors are wrong, is the black plot in the lower center of the chart, (that part of the plot is shown again below) the insolation per day at °65 N latitude. If the scale of this plot were anomaly, instead of absolute value, the anomaly would be slightly negative. The insolation change due to the length of summer and the distance from the sun are almost exactly canceling, and the insolation is not going to change very much for the next 20,000 years.

Figure 2. The blue dot is the current date. Source: Wikipedia Commons


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The position in our orbit where we are closest to the sun is called the parahelion point. The tilt of the earth’s axis is related to the parahelion by a relationship known as the “longitude of the parahelion”. The °360-°0 point is where the Vernal Equinox (1st day of Spring) and the parahelion coincide. If you divide the °360 by 4 (=°90) you see that the parahelion will be in spring from °0 to °90, summer from °90 to °180, autumn from °180 to °270, winter from °270 to °360. Right now we are at °283 and the parahelion happens on the 3rd of January.

Figure 3. Plot of the Longitude of the Parahelion point. Plotted from NASA data. The year 2000 is marked by the blue square. You can see that in the year 12,000, 10,000 years from now, we will be closest to the sun in NH mid-summer.
Will this lead to warming in mid-summer? No, because the earth reaches a point of minimum axial tilt at that time, and rapidly decreasing orbital eccentricity (as eccentricity reaches minimum, the orbit approaches a perfect circle) will make the reduced distance from the sun in summer much less important. See the next chart. Again, the current date is the blue squares.

Figure 4. Plot of Obliquity (Earth axial tilt) and orbital eccentricity (deviation from a circle). Plotted from NASA data obtained here.
When will the next ice age be?
One of the comments to the Hertzberg article in WUWT asked, “When will the next ice age will begin?” Answer, not for a while yet. From Wikipedia: “An often-cited 1980 study by Imbrie and Imbrie determined that, ‘Ignoring anthropogenic and other possible sources of variation acting at frequencies higher than one cycle per 19,000 years, this model predicts that the long-term cooling trend which began some 6,000 years ago will continue for the next 23,000 years.’[16]”
All these orbital cycles are much too long and gradual to account for late 20th Century warming. That warming is due to short-term solar output changes, solar magnetic field influence on galactic cosmic rays changing cloud cover, and ocean cycles.
You can see in Figure 2 that the next negative swing in NH insolation is over 50 thousand years in the future, and the next really negative swings are over 100 thousand years off. Even if we cool for the next 23,000 years, it should not get cool enough to trigger an ice age… unless the sun does something really strange.
Share this...FacebookTwitter "
"

NEW 4/10/09: There is an update to this post, see below the “read the rest of this entry” – Anthony
Guest Post by Richard Lindzen, PhD.
Alfred P. Sloan Professor of Meteorology, Department of Earth, Atmospheric and Planetary Science, MIT 

This essay is from an email list that I subscribe to. Dr. Lindzen has sent this along as an addendum to his address made at ICCC 2009 in New York City. I present it here for consideration. – Anthony
Simplified Greenhouse Theory
The wavelength of visible light corresponds to the temperature of the sun’s surface (ca 6000oK). The wavelength of the heat radiation corresponds to the temperature of the earth’s atmosphere at the level from which the radiation is emitted (ca 255oK). When the earth is in equilibrium with the sun, the absorbed visible light is balanced by the emitted heat radiation.
The basic idea is that the atmosphere is roughly transparent to visible light, but, due to the presence of greenhouse substances like water vapor, clouds, and (to a much lesser extent) CO2 (which all absorb heat radiation, and hence inhibit the cooling emission), the earth is warmer than it would be in the absence of such gases.

The Perturbed Greenhouse
If one adds greenhouse gases to the atmosphere, one is adding to the ‘blanket’ that is inhibiting the emission of heat radiation (also commonly referred to as infrared radiation or long wave radiation). This causes the temperature of the earth to increase until equilibrium with the sun is reestablished.
For example, if one simply doubles the amount of CO2 in the atmosphere, the temperature increase is about 1°C.

If, however, water vapor and clouds respond to the increase in temperature in such a manner as to further enhance the ‘blanketing,’ then we have what is called a positive feedback, and the temperature needed to reestablish equilibrium will be increased. In the climate GCMs (General Circulation Models) referred to by the IPCC (the UN’s Intergovernmental Panel on Climate Change), this new temperature ranges from roughly 1.5°C to 5°C.
The equilibrium response to a doubling of CO2 (including the effects of feedbacks) is commonly referred to as the climate sensitivity.
Two Important Points
1. Equilibration takes time.
2. The feedbacks are responses to temperature – not to CO2 increases per se.
The time it takes depends primarily on the climate sensitivity, and the rapidity with which heat is transported down into the ocean. Both higher sensitivity and more rapid mixing lead to longer times. For the models referred to by the IPCC, this time is on the order of decades.
This all leads to a crucial observational test of feedbacks!


The Test: Preliminaries
Note that, in addition to any long term trends that may be present, temperature fluctuates on shorter time scales ranging from years to decades.

Such fluctuations are associated with the internal dynamics of the ocean- atmosphere system. Examples include the El Nino – Southern Oscillation, the Pacific Decadal Oscillation, etc.
These fluctuations must excite the feedback mechanisms that we have just described.
The Test
1. Run the models with the observed sea surface temperatures as boundary conditions.
2. Use the models to calculate the heat radiation emitted to space.
3. Use satellites to measure the heat radiation actually emitted by the earth.
When temperature fluctuations lead to warmer temperatures, emitted heat radiation should increase, but positive feedbacks should inhibit these emissions by virtue of the enhanced ‘blanketing.’ Given the model climate sensitivities, this ‘blanketing’ should typically reduce the emissions by a factor of about 2 or 3 from what one would see in the absence of feedbacks. If the satellite data confirms the calculated emissions, then this would constitute solid evidence that the model feedbacks are correct.
The Results of an Inadvertent Test
From Wielicki, B.A., T. Wong, et al, 2002: Evidence for large decadal variability in the tropical mean radiative energy budget. Science, 295, 841-844.
Above graph:
Comparison of the observed broadband LW and SW flux anomalies for the tropics with climate model simulations using observed SST records. The models are not given volcanic aerosols, so the should not expected to show the Mt. Pinatubo eruption effects in mid-1991 through mid-1993. The dashed line shows the mean of all five models, and the gray band shows the total rnage of model anomalies (maximum to minimum).
It is the topmost panel for long wave (LW) emission that we want.
Let us examine the top figure a bit more closely.

From 1985 until 1989 the models and observations are more or less the same – they have, in fact, been tuned to be so. However, with the warming after 1989, the observations characteristically exceed 7 times the model values. Recall that if the observations were only 2-3 times what the models produce, it would correspond to no feedback. What we see is much more than this – implying strong negative feedback. Note that the ups and downs of both the observations and the model (forced by observed sea surface temperature) follow the ups and downs of temperature (not shown).
Note that these results were sufficiently surprising that they were confirmed by at least 4 other groups:
Chen, J., B.E. Carlson, and A.D. Del Genio, 2002: Evidence for strengthening of the tropical general circulation in the 1990s. Science, 295, 838-841.
Cess, R.D. and P.M. Udelhofen, 2003: Climate change during 1985–1999: Cloud interactions determined from satellite measurements. Geophys. Res. Ltrs., 30, No. 1, 1019, doi:10.1029/2002GL016128.
Hatzidimitriou, D., I. Vardavas, K. G. Pavlakis, N. Hatzianastassiou, C. Matsoukas, and E. Drakakis (2004) On the decadal increase in the tropical mean outgoing longwave radiation for the period 1984–2000. Atmos. Chem. Phys., 4, 1419–1425.
Clement, A.C. and B. Soden (2005) The sensitivity of the tropical-mean radiation budget. J. Clim., 18, 3189-3203.
The preceding authors did not dwell on the profound implications of these results – they had not intended a test of model feedbacks! Rather, they mostly emphasized that the differences had to arise from cloud behavior (a well acknowledged weakness of current models). However, as noted by Chou and Lindzen (2005, Comments on “Examination of the Decadal Tropical Mean ERBS Nonscanner Radiation Data for the Iris Hypothesis”, J. Climate, 18, 2123-2127), the results imply a strong negative feedback regardless of what one attributes this to.
The Bottom Line
The earth’s climate (in contrast to the climate in current climate GCMs) is dominated by a strong net negative feedback. Climate sensitivity is on the order of 0.3°C, and such warming as may arise from increasing greenhouse gases will be indistinguishable from the fluctuations in climate that occur naturally from processes internal to the climate system itself.
An aside on Feedbacks
Here is an easily appreciated example of positive and negative feedback. In your car, the gas and brake pedals act as negative feedbacks to reduce speed when you are going too fast and increase it when you are going too slow. If someone were to reverse the position of the pedals without informing you, then they would act as positive feedbacks: increasing your speed when you are going too fast, and slowing you down when you are going too slow.

Alarming climate predictions depend critically on the fact that models have large positive feedbacks. The crucial question is whether nature actually behaves this way? The answer, as we have just seen, is unambiguously no.
UPDATE: There are some suggestions (in comments) that the graph has issues of orbital decay affecting the nonscanner instrument’s field of view. I’ve sent a request off to Dr. Lindzen for clarification. – Anthony
UPDATE2: While I have not yet heard from Dr. Lindzen (it has only been 3 hours as of this writing) commenter “wmanny” found this below,  apparently written by Lindzen to address the issue:
“Recently, Wong et al (Wong, Wielicki et al, 2006, Reexamination of the Observed Decadal Variability of the Earth Radiation Budget Using Altitude-Corrected ERBE/ERBS Nonscanner WFOV Data, J. Clim., 19, 4028-4040) have reassessed their data to reduce the magnitude of the anomaly, but the remaining anomaly still represents a substantial negative feedback, and there is reason to question the new adjustments.”
I found the text above to match “wmanny’s” comment in a presentation given by Lindzen to Colgate University on 7/11/2008 which you can see here as a PDF:
http://portaldata.colgate.edu/imagegallerywww/3503/ImageGallery/LindzenLectureBeyondModels.pdf
– Anthony
UPDATE3: I received this email today  (4/10) from Dr. Lindzen. My sincere thanks for his response.
Dear Anthony,
The paper was sent out for comments, and the comments (even  those from “realclimate”) are appreciated.  In fact, the reduction of the  difference in OLR between the 80’s and 90’s due to orbital decay seems to me to  be largely correct.  However, the reduction in Wong, Wielicki et al (2006) of  the difference in the spikes of OLR between observations and models cannot be  attributed to orbital decay, and seem to me to be questionable.  Nevertheless,  the differences that remain still imply negative feedbacks.  We are proceeding  to redo the analysis of satellite data in order to better understand what went  into these analyses.  The matter of net differences between the 80’s and 90’s is  an interesting question.  Given enough time, the radiative balance is  reestablished and the anomalies can be wiped out.  The time it takes for this to  happen depends on climate sensitivity with adjustments occurring more rapidly  when sensitivity is less.  However, for the spikes, the time scales are short  enough to preclude adjustment except for very low sensitivity.
That said,  it has become standard in climate science that data in contradiction to alarmism  is inevitably ‘corrected’ to bring it closer to alarming models.  None of us  would argue that this data is perfect, and the corrections are often plausible.   What is implausible is that the ‘corrections’ should always bring the data  closer to models.
Best wishes,
Dick

Sponsored IT training links:
Best quality 70-448 prep material is available for download. Pass the real exam using JN0-350 guide and E20-361 lab tutorial.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e973bbaff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterPada saluran yang indah ini kita akan sedikit berbagi menurut anda seperti apa buku petunjuk bermain judi bandarq yang benar dan aman khususnya untuk anda yang sedang pemula, bermain judi bandarq itu tidak semudah yang anda bayangkan hanya dengan menyembunyikan taruhan maka anda bakal menang begitu saja, terdapat beberapa hal penting yang harus anda pelajari beserta baik agar bisa bermain dengan profesional. Berikut merupakan panduan sederhana bermain pertaruhan bandarq. Simak baik-baik ulasannya di bawah ini.
# Langkah pendaftaran
Langkah pertama yang harus anda lakukan adalah dengan mencari distributor judi online terlebih lewat yaitu agen judi domino online yang ada pada internet, gunakan cara yang sudah kami tuliskan pada website ini untuk memperoleh agen judi domino online yang terpercaya. Setelah kamu menemukan agen judi yang bsia dipercaya silahkan lakukan pendaftaran terlebih dahulu, pati semua form dengan betul dan valid terlebih untuk isian pada nomor perkiraan hingga kontak yang dikau miliki, kemudian setelah tersebut silahkan login dengan account yang sudah anda buat tadi, lakukan transfer deposit sesuai dengan jumlah minimal maupun maksimal yang dikasih oleh situs. Setelah tersebut silahkan lanjut pada tahap berikutnya.
# Tahap pengenalan permainan judi bandarq
Kemudian tahap kedua adalah tahap pengenalan judi domino online, pertama-tama anda pahami dulu apa itu tiket judi domino online, kartu judi ini biasa disebut dengan permainan judi gaple atau dadu bernomor kalau di daerah anda. Di setiap permainan judi kartu domino dilakukan 6-8 orang di satu meja dengan nilai taruhan yang beragam, mulai dari dari 2000 rupiah terlintas ratusan bahkan jutaan yen. Permainan ini menggunakan relevansi 52 kartu acak yang masing-masing player akan jadi secara acak, tujuan dari permainan ini adalah player yang mendapatkan kombinasi poin paling besar maka dialah yang akan menjadi pemenangnya.
# Tahap pengenalan sebutan dalam permainan judi bandarq
Kemudian tahap berikutnya member akan sedikit mengulas kurang lebih pengertian dari istilah yang digunakan dalam permainan betting domino, pertama adalah pengenalan di meja sendiri. Ada istilah check, raise, fold, call, hingga all tersebut. pengertian check sendiri menjajal kartu artinya tidak berbuat taruhan, Raise menaikkan taruhan lebih tinggi, fold bukan mengikuti permainan dan mempersembahkan kartu ke bandar, all in melakukan taruhan semata dana yang kita punya dan untuk call swasembada adalah memanggil taruhan pantas dengan jumlah taruhan sebelumnya. Kemudian pembahasan mengenai tiket spesial, dalam permainan tersebut ada 4 kartu spesial yang bisa anda miliki, jika anda bisa memperoleh salah satu dari 4 kartu spesial ini bisa dipastikan anda menang. Slip spesial mulai dari slip 99/qiu, kartu murni kecil/besar, kartu kembar/balak dan slip 6 dewa.
# Tara bermain judi domino online
Tahap pertama anda bakal duduk berjumlah 8 orang-orang, pembagian kartu seperti suksesi jam, pada pembagian pertama anda hanya akan nampi 3 kartu, analisa tiket anda apakah nantinya dapat berkombinasi bagus atau bukan, jangan terkecoh dengan slip bagus sebelum kartu ke empat muncul. Lakukan judi bola sesuai dengan keyakinan serta keputusan anda, setelah tersebut kartu ke empat bakal muncul, jika kartu kamu bagus misalnya 99/qq tidak sungkan untuk all in.
Nah itulah cara permainan judi domino yang simple dan mudah untuk para pemain pemula
Share this...FacebookTwitter "
"

I’m willing to wager two things. First, I’ll bet that anyone who said global warming is an overblown bunch of hooey had a terrible time at this year’s holiday cocktail parties. Second, I’ll take even money that the 10 years ending on December 31, 2007, will show a statistically significant global cooling trend in temperatures measured by satellite. 



Those two bets are related. Our greener friends were in high attack mode on the party circuit last month as TV newscasts presented a relentless nightly stream of reports on how 1998 was the warmest year since gosh knows when. NBC got British scientist Phil Jones to say that it was the warmest year of the millennium, even though the thermometer was invented only about three‐​quarters of the way through it. The figures he used were estimates based mainly on tree ring measurements prior to 1900 (that’s 900 of the last 1000 years), and those estimates are known to miss an enormous amount of true temperature variation. 



As far as 1998 was concerned, the wire stories correctly noted (for a change) that all three true temperature histories — from ground‐​based thermometers, weather balloons in the lower atmosphere and satellite soundings of the lower layers — showed the same thing: record temperatures. The satellite history is 20 years long, the weather balloon history is 42, and reliable global ground‐​based temperature data really extend back only about 55 years. But they all were higher than kites. 



The many hours spent explaining to folks that 1998 blew the top off the record because heat from the big El Niño burped its way out to space could easily have been saved if I’d just had a vest‐​pocket temperature history handy. To save you a similar investment of time, there’s one included with this article. It’s intended to wrap around your business card (or if you’re not working, a credit card) for easy access at the next party. 



Brandish the card while you’re being jabbed into the corner by a sharpened carrot stick (or, God forbid, a Ranch‐​sauced bit of broccoli) by some punk wearing a Phish shirt. What he’ll see is absolutely no warming trend whatsoever from when the satellite measurements began (January 1979) through the end of 1997, followed by one warm year in 1998.  




Our greener friends were in high attack mode on the party circuit last month as TV newscasts presented a relentless nightly stream of reports on how 1998 was the warmest year since gosh knows when. 



The stunning blip in 1998 is just that — a blip. Close examination (data shown here run through November 1998) shows that temperatures have dropped back down to the levels typical of 1979–97. 



Those who follow global warming know that two California scientists recently found a problem with the satellite temperatures. The data shown on the chart have been corrected by NASA scientist Roy Spencer. 



Last year was so warm that it induces a statistically significant warming trend in the satellite data. Thus the second bet: Starting with 1998, there will almost certainly be a statistically significant cooling trend in the decade ending in 2007. 



In part, rapid cooling in late 1998 explains the record level of hysteria accompanying the global warming stories in the last half of December. Anyone pushing for climate legislation knows that it’s now or never, because the warming is over. 



If the Kyoto protocol doesn’t pass in the heat of this particular moment, it never will. So maybe readers will want to make copies of the little wallet card and send a few to their friends here in Washington. Or perhaps if you come to town for a little social gathering, you can pull it out and show it around. And, while we’re at it, any takers on my wagers?
"
"

LONDON (Reuters) – Next year is set to be one of the top-five warmest on  record, British climate scientists said on Tuesday.
The average global temperature for 2009 is expected to be more than 0.4  degrees celsius above the long-term average, despite the continued cooling of  huge areas of the Pacific Ocean, a phenomenon known as La Nina.
That would make it the warmest year since 2005, according to researchers at  the Met Office, who say there is also a growing probability of record  temperatures after next year.
Currently the warmest year on record is 1998, which saw average temperatures  of 14.52 degrees celsius – well above the 1961-1990 long-term average of 14  degrees celsius.
Warm weather that year was strongly influenced by El Nino, an abnormal  warming of surface ocean waters in the eastern tropical Pacific.
Theories abound as to what triggers the mechanisms that cause an El Nino or  La Nina event but scientists agree that they are playing an increasingly  important role in global weather patterns.
The strength of the prevailing trade winds that blow from east to west across  the equatorial Pacific is thought to be an important factor.
“Further warming to record levels is likely once a moderate El Nino  develops,” said Professor Chris Folland at the Met Office Hadley Center.  “Phenomena such as El Nino and La Nina have a significant influence on global  surface temperature.”
Professor Phil Jones, director of the climate research unit at the University  of East Anglia, said global warming had not gone away despite the fact that  2009, like the year just gone, would not break records.
“What matters is the underlying rate of warming,” he said.
He noted the average temperature over 2001-2007 was 14.44 degrees celsius,  0.21 degrees celsius warmer than corresponding values for 1991-2000.
(Reporting by Christina Fincher; Editing by Christian Wiessner)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99d79ecd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Many people have pointed me to this story, I wanted to read about it a bit before posting it.  Almost two years ago, when this blog was in its very first month, I posted this story on the puzzling leveling off of global methane concentrations. FYI Methane has a “global warming potential” (GWP) 23-25 times that of CO2.
CDIAC has an interesting set of graphs on methane, the first of which shows that indeed global concentrations of CH4 through 2004 have leveled off:

This one on latitude -vs- concentration would surely seem to point to anthropogenic sources of CH4:

So here is yet another addition to the puzzle, which seems to point in the opposite direction:
MIT scientists baffled by global warming theory, contradicts scientific data 
From: TG Daily By Rick C. Hodgin
Boston (MA) – Scientists at MIT have recorded a nearly simultaneous world-wide increase in methane levels. This is the first increase in ten years, and what baffles science is that this data contradicts theories stating man is the primary source of increase for this greenhouse gas. It takes about one full year for gases generated in the highly industrial northern hemisphere to cycle through and reach the southern hemisphere. However, since all worldwide levels rose simultaneously throughout the same year, it is now believed this may be part of a natural cycle in mother nature – and not the direct result of man’s contributions.
Methane – powerful greenhouse gas
The two lead authors of a paper published in this week’s Geophysical Review Letters, Matthew Rigby and Ronald Prinn, the TEPCO Professor of Atmospheric Chemistry in MIT’s Department of Earth, Atmospheric and Planetary Science, state that as a result of the increase, several million tons of new methane is present in the atmosphere.

Methane accounts for roughly one-fifth of greenhouse gases in the atmosphere, though its effect is 25x greater than that of carbon dioxide. Its impact on global warming comes from the reflection of the sun’s light back to the Earth (like a greenhouse). Methane is typically broken down in the atmosphere by the free radical hydroxyl (OH), a naturally occuring process. This atmospheric cleanser has been shown to adjust itself up and down periodically, and is believed to account for the lack of increases in methane levels in Earth’s atmosphere over the past ten years despite notable simultaneous increases by man.
More study
Prinn has said, “The next step will be to study [these changes] using a very high-resolution atmospheric circulation model and additional measurements from other networks. The key thing is to better determine the relative roles of increased methane emission versus [an increase] in the rate of removal. Apparently we have a mix of the two, but we want to know how much of each [is responsible for the overall increase].”
The primary concern now is that 2007 is long over. While the collected data from that time period reflects a simultaneous world-wide increase in emissions, observing atmospheric trends now is like observing the healthy horse running through the paddock a year after it overcame some mystery illness. Where does one even begin? And how relevant are any of the data findings at this late date? Looking back over 2007 data as it was captured may prove as ineffective if the data does not support the high resolution details such a study requires.
One thing does seem very clear, however; science is only beginning to get a handle on the big picture of global warming. Findings like these tell us it’s too early to know for sure if man’s impact is affecting things at the political cry of “alarming rates.” We may simply be going through another natural cycle of warmer and colder times – one that’s been observed through a scientific analysis of the Earth to be naturally occuring for hundreds of thousands of years.
Project funding
Rigby and Prinn carried out this study with help from researchers at Commonwealth Scientific and Industrial Research Organization (CSIRO), Georgia Institute of Technology, University of Bristol and Scripps Institution of Oceanography. Methane gas measurements came from the Advanced Global Atmospheric Gases Experiment (AGAGE), which is supported by the National Aeronautics and Space Administration (NASA), and the Australian CSIRO network.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ba818fa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI know what a lot of people are thinking when they look at the Arctic sea ice graph since September 1st – my oh my how has the ice reduced! Indeed just take a look at the numbers themselves:
September 1:     5,332,344 sq km
September 18:   4,813, 594 sq km
That’s a drop in area of over 500,000 sq km. Still, I’m going to say that the ice has grown. You think it’s preposterous, right?

But now take a look at the following chart that compares September 1 ice to September 18 ice. Which would you prefer to be standing on?

These charts are taken from: http://www.ijis.iarc.uaf.edu/cgi-bin/seaice-monitor.cgi
Which ice looks thicker (more concentrated)?
Don’t sweat the ice area statistics. The thickness (er, concentration) is much greater today, and we could even say the volume is likely more.  Arctic temperatures above 80°N have been colder this summer and September. The ice area will rebound quickly, of course. I projected a 5.75 million sq km min. for 2011 a couple weeks back. I’m sticking to it.
Share this...FacebookTwitter "
"**Spain's King Felipe VI has begun ten days of quarantine after coming into contact with a person who tested positive for coronavirus.**
Palace sources say the king, 52, was in ""close contact"" with the individual on Sunday, but gave no further details.
The monarch's wife and the couple's two daughters will continue their activities as normal.
Spain has recorded nearly 1.6 million cases and 43,131 deaths since the pandemic began.
Last week, the World Health Organization warned that Europe, which is once again at the centre of the pandemic, faced a ""tough"" six months ahead.
However, recent results from a number of vaccine trials have given hope and on Tuesday the Spanish government is due to meet to discuss plans to vaccinate the population.
Last week, Prime Minister Pedro SÃ¡nchez said the country hoped to offer the vaccine to ""a very substantial part"" of its population within the first half of 2021."
"
A snowmobiler negotiates the streets of Crosby, North Dakota. Photograph courtesy of the Crosby Journal.

Guest Post by Harold Ambler
Snow, wind, and cold have assaulted North Dakota yet again in the past 24 hours. In Bismarck Friday morning the temperature was 12 below zero with a new inch or two of snow expected following Thursday’s more significant storm.
According to USA Today, snow in the southern part of the state was bad enough Thursday that snowplow operators were pulling off the road, blinded by the whiteout conditions. A foot of snow was common in the heaviest band.
The National Weather Service predicts a high temperature of 3 degrees Fahrenheit Friday in Bismarck, as well as additional snow. As of Thursday, three-quarters of the state’s roads were still snow-covered, in whole or in part, from the storm that just ended the day before.

Howling winds and copious snow have combined to leave austere scenes like this in Cavalier County, North Dakota. Photograph courtesy of the ND Department of Emergency Services.

More than once during the winter, the Department of Transportation has issued a no-travel advisory, most recently on February 10.
Cecily Fong, spokeswoman for the state’s Department of Emergency Services, said that the winter got off to a bad start on November 4. “That first storm was definitely a blizzard with blowing and drifting snow,” she said.  Since then, according to Fong, several counties have seen more than 400 percent of normal snowfall.
December was a record breaker for Bismarck, as it was at many other locations around the state. In Bismarck, the total for the month was 33.3 inches, the greatest amount ever received in a single month.
Those were early days, it turned out. Frequent storms, followed by howling northwest winds and record-breaking cold, have made it a winter to remember. On January 15, the morning low at the Bismarck airport was 44 below zero, the coldest ever for the date, and one degree shy of the all-time coldest reading for a state known to be less than balmy.
By the end of January, many counties had more than 400 percent of normal snow totals on the ground, and Governor John Hoeven had declared a state of emergency. 
“There has been a repeated pattern,” said Fong,  ”where the county will come and plow a road and then two days later, without any additional snow, the road becomes impassable again.” Relatively speaking, the people in Bismarck have gotten off light. Divide County, in the state’s northwest corner, has received 500 percent of normal snowfall.
Steve Andrist, who has lived most of his life in Divide county and is the publisher of the weekly Crosby Journal, commended the street department. “There has never been more than a day or a day and a half where the roads were

Roads that were cleared once, and twice, have needed to be cleared a third time in various locations throughout the state. Photograph courtesy of the ND Department of Emergency Services.

impassable,” he said.
After a lifetime living so near the Canadian border, did the last few months really amount to anything? “This winter got my attention,” he said. “The thing that’s different about this one is the volume of snow. It’s so much more than we anticipated. As far as snow and moving it, and moving it again, and having to move it again a third time, this has been very unusual.”
On February 19, the governor asked the federal government to provide emergency assistance for snow removal. “We’ve got roads that aren’t being plowed,” Fong said, “just because the funds aren’t available to do it.”
Although the spring melt is weeks away, Fong said that flooding is already a concern. “We don’t know where, and we don’t know when, but we’re keeping our eyes on it.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97fec37c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In between inventing the automobile, penicillin and electricity, growing up as a missionary on the Amazon and supporting his fatherless family of 13 as a bootblack, and inspiring hit musicals and epic poetry, Vice President Al Gore is acting as a commander in chief wannabe. 



His role as propagandist on behalf of the administration’s disastrous war of aggression in the Balkans is reason enough to reject him in the year 2000. But his record on domestic issues is even worse. 



The vice president’s campaign minions are saying that he is a tough leader who pushed for military action against Yugoslavia. Mr. Gore certainly is talking tough: “Milosevic has barely begun to incur the damage he will feel.” 



Of course, Mr.Gore probably couldn’t do worse than Bill Clinton, who has bungled every step. Were the latter commander in chief during World War II, we would all be speaking German. 



However, Mr. Gore is attempting to do more than score political points by warmongering for peace. Of late, he’s been battling airlines over compensation for lost bags and pushing to create a special phone number to call about traffic jams. For this, newly independent American Colonies created a national government? 



A Gore associate explained that such measures will “add up to something more thematic, something bigger.” And they do. The vice president once said he believes government should be “like grandparents, in the sense that grandparents perform a nurturing role.” 



But Mr.Gore prefers to “nurture” with a mailed fist. As former ABC correspondent Bob Zelnick puts it in his devastating new book, “Gore: A Political Life” (Regnery): “Al Gore Jr. was a child of government and a student of government who grew up to be a man of government.” 



The vice president has been traversing the country telling audiences he embodies “practical idealism.” However, he has been able to cultivate the image of a moderate primarily because he once took more conservative stands on security and social issues. But 28‐​year‐​old candidate Gore ran a populist economic campaign — higher taxes on the rich, support for public jobs creation — to win election in 1976 to Congress from Tennessee. 



He generally fit well within the Democratic caucus. He was a reliable supporter of new spending programs, whether business subsidies or redistributive entitlements; higher taxes, especially on the upper‐​middle class; increased regulation, particularly for environmental purposes; and social engineering schemes, such as racial quotas. 



Mr.Gore was at his worst on taxes. Between 1981 and 1993, he opposed only one of 19 significant tax increases; he voted to collect an extra $9,000 per household. He supported a plethora of other tax increases, which failed to pass. 



At least all of these measures required votes. The vice president also backed the multibillion‐​dollar e‐​rate levy (or “Gore Tax”) on phone service, which has been imposed without public debate by the Federal Communications Commission. 



The vice president has placed himself on the extremist edge of environmental policy‐​making. In his book, “Earth in the Balance,” he declared: “We must make the rescue of the environment the central organizing principle of civilization.” 



He has pushed a variety of new energy taxes. He wants employers to subsidize workers who don’t drive to work. He advocates eliminating the internal combustion engine. He proposes banning packaging that is neither biodegradable nor recyclable. He advocates more foreign aid to Third World states for environmental purposes. 



Perhaps Mr.Gore’s most important environmental crusade involves global warming. In no small part due to his efforts, the administration signed the Kyoto Protocol to the Framework Convention on Climate Change, which mandates substantial reductions in global energy consumption. 



Yet years of scaremongering have proved to be inaccurate. Observed warming has been far below that predicted by the models upon which the convention was based. Even Mr. Gore admitted in 1995: “In truth, the scientists who are expert in this field will tell you that the precise causal relationship (between C02 and global warming) has not yet been established.” 



The Kyoto Treaty, as yet unratified by the Senate, would impose huge burdens on the United States. 



Yale University economist William Nordhaus figures the bill could run $2,000 per household every year. Wharton Econometrics Forecasting Associates estimates 1.8 million jobs could be lost; others predict losses of as many as 3 million. 



Mr. Gore has attempted to disguise his statist bias by heading up the president’s program to “reinvent” government. However, his claim to have saved $137 billion is belied even by the National Performance Review’s own reports. Federal employment has not fallen due to his efforts. Mr. Gore has held press conferences rather than recommend eliminating useless agencies. 



Commander in Chief Al Gore? It’s a terrible thought. But even worse would be President Gore running domestic policy.
"
"
Share this...FacebookTwitterHat tip: Readers Edition.
Michael Krüger at skeptic site Readers Edition brings our attention to an interview (in German) with skeptic scientist Professor Werner Kirstein of the Institute for Geography at the University of Leipzig on MDR German Public Radio. (Expect MDR to receive hellfire and brimstone for daring to air such blasphemy).

In the interview Professor Kirstein is asked about the cold winters being a sign of warming. He said that it just doesn’t fit, and that years ago the same scientists predicted the opposite, and reminds us what climate experts said 3 or 4 years ago:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany we will rapidly see no more snow in the winter.”
So much for that prediction.
Professor Kirstein also cautioned against placing too much emphasis on 2010 being among the hottest years on record, believing the claim is “a joke” and saying that determining a global average is a tricky business and in the end is only a theoretical value. The radio moderator then attempts to make Kirstein look like a denier in asking:
And so what you are saying is that there is no climate change?”
To which Kirstein answered that indeed there is – from natural causes, citing the extreme temperature variations of the past repeating ice ages. Kirstein disputes the notion that man-made CO2 is driving the climate today, saying CO2 has at best extremely little to do with temperature, and it will not do any good to reduce it. He views political attempts to reduce CO2 as an effort to steer the economy into a certain direction and as a way to earn money.
Near the end of the clip, Kirstein says flat out that it all comes down to a belief – a religion.
Transcript (in German) of the interview, Read here.
Professor Kirstein was also once featured at NTZ not long ago, Read here.
Share this...FacebookTwitter "
"
Here is a weather curiosity. We’ve been hearing a lot about snowfall in the northern hemisphere this year. In Oslo, they have given up on trying to pile it up so they have resorted to dumping it in the sea. If this happened in Seattle they’d probably get into a tizzy for polluting Puget Sound with fresh water snow. And it is not just Oslo, the problem seems widespread. Here are some other news stories in London, OT Geneva, Ohio Chardon, OH Wasatch, UT Chicopee, MA and Rochester, NY where they say the piles are making driving dangerous. In Wenatchee, WA they want to spray warm sewage water on the snow to melt it.  I know they could use the USHCN temperature sensor at the sewage treatment plant there to check the temperature to make sure conditions are right. Yeah, that’s the ticket! – Anthony

From Reuters Environment Blog by Alister Doyle
It looks more like an Ice Age than global warming.
There is so much snow in Oslo, where I live, that the city authorities are resorting to dumping truckloads of it in the sea because the usual storage sites on land are full.
That is angering environmentalists who say the snow is far too dirty – scraped up from polluted roads — to be added to the fjord. The story even made it to the front page of the local paper (’Dumpes i sjøen’: ‘Dumped in the sea’).
In many places around the capital there’s about a metre of snow, the most since 2006 when it was last dumped in the sea. Extra snow usually gets trucked to sites on land, where most of the polluted dirt is left after the thaw. Those stores are now full — in some the snow isn’t expected to melt before September.
But are these mountains of snow a sign that global warming isn’t happening?
Unfortunately, more snow might fit projections by the U.N. Climate Panel, which says that northern Europe is likely to get wetter and the south drier as temperatures rise this century.
“By the 2070s, hydropower potential for the whole of Europe is expected to decline by 6 percent, with strong regional variations from a 20 to 50 percent decrease in the Mediterranean region to a 15 to 30 increase in northern and eastern Europe.” it said in a 2007 report (page 60 of this link).
So people in northern Europe may have to buy more snow shovels than parasols to cope with global warming?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98486bb9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe online leftist Die Tageszeitung (TAZ) from Berlin has an essay by Stefan Rahmstorf, who attacks Dr Fritz Vahrenholt, head of RWE Innogy, for his comments in a December essay that casts doubt on man-made climate change appearing in the somwhat more conservative Die Welt from Hamburg.Interestingly, instead of appearing in the Science section, Rahmstorf’s rant appears in the Debate section. Is that a sign?
First Rahmstorf, praises Germany for carrying out a “scientific, factual discussion of climate science”, unlike in the USA, where he says:
It’s different in the USA: There the conservative Tea Party movement has proclaimed that man-made climate change is made up, and large parts of the economy are lobbying using dubious ‘climate sceptic’ propositions.”
Rahmstorf is visibly worried that this “dubious climate scepticism” may be spreading to Germany, and goes after RWE Vahrenholt. In his Die Welt essay, Vahrenholt assigns the blame for the cold winters  – writing:
It’s the sun, stupid!”
Rahmstorf, however does not believe the sun has an impact on the earth’s climate, and thinks it’s all due to a few molecules of CO2. And so Rahmstorf attributes the recent cold winters to miscalibrated human perception:
The winter appears cold because we had gotten used to the mild winters.”
and misleadingly reframes Mojib Latif’s 2008 predictions of cooling:
No serious scientist doubts global warming, and certainly not Mojib Latif, whose quote has nothing to do with the cold winters. It’s old and stems from his model projections of a temporary cooling, which in the meantime we know failed to materialise.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Latif made his cooling projections in 2008, and so are not that old (he predicted warm-winters back in 2000). And who can say that the cold winters aren’t related to his projections of cooling? Latif’s projections look to be true, and likely will be true for the new, current decade.
Rahmstorf then quotes, defends and explains Kevin Trenberth’s infamous “it’s travesty we can’t account for the missing heat” statement, saying it was taken out of context and that Trenberth meant something else.
Rahmstorf acknowledges the solar correlation with regards to the Russian heat wave and flood in Pakistan, but claims that this correlation is very weak, and quotes Trenberth again:
‘Without global warming, these events would not have happened’.”
Now that’s the “scientific, factual discussion” that PIK scientists and the government like to praise. Of course, anyone with even a rudimentary knowledge of science knows that blaming a couple of isolated weather events on global warming is preposterous. But what is preposterous in climate science passes as “scientific, factual” at the PIK.
Rahmstorf takes offence to the harsh criticism that Vahrenholt fired at PIK science, and thinks sceptics give the sun too great of a role in climate change. He says Vahrenholt is silent about the fact…(emphasis added)
…that also during the largest solar minimum of the “Little Ice Age” during the so-called Maunder Minimum of the late 17th century the global temperature was only a few tenths of a degree cooler than before and after, and that our model reproduces the temperature back then very well, otherwise we would have not used it for our future projections.”
A few tenths of a degree Celsius? That’s the difference between having vineyards in England and a frozen Thames? PIK science is moving beyond preposterous.
Rahmstorf ends with a comment on climate debate:
Those who wish to sow doubt on the urgency of climate protection, really have to work hard to twist the facts. However, the climate crisis can be overcome only by having an honest debate.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterYesterday I wrote about the latest issue of FOCUS news magazine HERE.
I just picked up the new issue, out today, and read it. It’s an unmistakable departure from usual doom and catastropheism, which had generated so much of the fear and urgency needed to justify the rush to reckless policy making and profiteering.
FOCUS shows that the urgency is undue and that it’s time to get back to rationality, and away from all the mass hysteria that has taken policymaking on a ride on the crazy train.
Part 1: It’s getting warmer – that’s good!
In its first part, FOCUS describes the Sahara region as it was thousands of years ago – an area rich in wildlife and plants where humans settled and even built a 500 meter long by 5 meter wide protective wall 1000 years before Christ. Then came a climate catastrophe (naturally of course), in the form of cooling and drying. The once green paradise dried up into a desert wasteland. Today the Sahara desert, thanks to warming, is greening up again. Says Stefan Kröpelin, geo-archaeologist of the University of Cologne, who has been researching the region for 30 years now:
At the southern edge, vegetation in most places has been moving northwards since the end of the 1980s. Global warming here has been a blessing. If the trend continues at this pace, the Sahara will be green again in a few hundred years.”
The Sahara is just one example of the advantages of global warming. FOCUS also writes:
More and more renowned scientists are saying climate change does not lead to only catastrophes, rather it also brings with it rich advantages for both man and nature. But this will hardly play a role in Cancun. Politicians, scientists and media are too fixated on the problems of the future – from sea level rise, to storms, to the spread of tropical diseases (see page 86).
Studies worldwide show that many of the widespread horror scenarios are baseless.”
FOCUS continues to believe the science underpinning the theory that global temperatures will rise 2 – 4°C by the end of the century, and this being due to man’s activities. Here, FOCUS naively ignores the impacts of oceanic and solar cycles. But even so, it has, at least in this article, truly departed from the planet-is-going-to-hell-in-a-hand-basket narrative. That’s huge progress for traditional German journalism – make no mistake about it.
And what does Stefan Kröpelin say about climate models for the future?
I trust the data from the earth’s history more than any climate model.”
Josef Reichholf, Professor Emeritus of Ecology and Evolutionary Biology at the University of Munich speaks on biodiversity.
The earth’s history shows that warm periods are characterized by high levels of biodiversity. In general, the rule is: the warmer it is, the more biodiversity you get.”
FOCUS then writes about the warm and cold cycles the earth has experienced, in particular the ice ages that have occurred every 100,000 years and the massive ice sheets that once covered many parts of the globe. This part really puts the earth and climate in the right perspective. Readers see that things have been far more extreme than the puny half-degree fluctuations we are biting our nails over today. The earth does change naturally, and often dramatically.
FOCUS also writes about the Holocene optimums and minimums, and the challenges and benefits that man derived from them. Climate has always been changing. The Vikings even settled in Greenland in the year 982, FOCUS points out.
FOCUS also puts CO2 in the spotlight and discovers that it is not that “climate-killing” gas everyone has been making it out to be. Indeed, the gas is actually a fertilizer for plants. It makes the planet greener. It boosts agricultural yields, which means more food to feed the world.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




CO2 is often called a climate poison or climate killer, but: ‘It is an essential building block for photosynthesis, and thus the basis for all life,’ says Hans-Joacheim Weigel, Director of the Johann Heinrich von Thünnen Institute for Biodiversity.”
Part 2: The 7 scourges of the end time
FOCUS then looks at the 7 climate scourges alleged will occur as the world ends, or something, and debunks them one by one. Here I think FOCUS did a good job – a must read. Here I summarize with just a few key words.
FOCUS (in a few words):
1. Islands will sink: “Satellite photos show they are not”
2. Disappearing glaciers. FOCUS writes: “not a new phenomena”
3. Melting of the ice caps: “Highly improbable – very high temperatures would be needed”
4. Gulf Stream collapse: “NASA shows it’s not true”
5. More storms: “Chris Landsea shows it won’t be so.”
6. Polar bears will die: “They are growing in number and are highly adaptable – no big problem.”
7. Tropical diseases will spread: “DDT ban was a cause. And Paul Reiter says it’s BS.”
Part 3: Who’s who in climate science?
One interesting section of the FOCUS report was the part on who’s who in climate science? They list 11 names:
1. Rejendra Pachauri: “calls to resign”
2. Michael Mann: “doubts on his methodology”
3. James Hansen: “warning of AGW since the 1980s”
4. John Christy, Atmospheric Sciences:
Analyzes climate data from satellites at the University of Alabama. Is against alarmist statements from other scientists who warn of catastrophic temperature increases and sea level rises, and considers climate protection measures unnecessary.”
5. Stephen McIntyre, mining specialist:
The Canadian analyzed Mann’s hockey stick curve together with economist Ross McKitrick, who both found deficiencies in the methodology, which put the curve’s shape into question, as well as the claim that never in the last 1000 years has it been warmer than today. McIntyre became known when Mann refused to reveal his data. He is active with his blog Climate Audit.”
6. Stefan Rahmstorf: “A lead author of the IPCC 4AR report and many publications”
7. Björn Lomborg: “Recommends adaptation”
8. Mojib Latif: “Projected a pause in global warming”
9. Richard Lindzen:
Researches at MIT and one of the most prolific skeptics. He says the earth is never in equilibrium, and for that reason natural changes such as ocean currents or atmospheric cycles can explain the warming. For this reason it is senseless to attempt to fight climate change.”
10. Nicholas Stern: “Inaction is more expensive than action.”
11. Paul Crutzen: “Advocates geo-engineering.”
So much for the ballyhooed consensus and settled science. This FOCUS article shows that the science is hotly disputed, and more importantly, that the catastrophe scenarios are hysteria, and that warmer climates would bring real benefits.
For German journalism, this to me represents a watershed event. This piece breaks a lot of taboos in Germany. The reactions indeed will be worth following. Expect hellfire and vitriol from the greenshirts.
I wish I had time to write more, but this I think is a good overview of the extensive 14-page FOCUS piece. Go out and buy it.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe Moving Finger writes; and, having writ,
Moves on: nor all thy Piety nor Wit
Shall lure it back to cancel half a Line,
Nor all thy Tears wash out a Word of it.
– Omar Khayyám
Scratch off the Potsdam Institute For Climate Impact Research from the alarmist list. No kidding!
The European Institute For Climate and Energy has a new piece written by Raimund Leistenschneider that takes a look at two interesting papers dug up from 2003. I wonder if Rahmstorf and Schellnhuber are going to feign amnesia on this. Big hat tip to NTZ reader Ike!
Rahmstorf 2003 paper shows pronounced cooling



The paper by Prof Stefan Rahmstorf confirms that today’s temperatures are actually quite cool compared to temperatures earlier in the Holocene.
In a paper he authored: “Timing of abrupt climate change: A precise clock“, Geophys. Res. Lett.. 30, Nr. 10, 2003, S. 1510, doi:10.1029/2003 GL017115, Ramhstorf examined the Dansgaard-Oeschger (DO events).
These events are rapid climate changes occurring 23 times during the last ice age between 110,000 and 23.000 BP and were reconstructed from the GISP-2-ice cores from Greenland. The following chart is a plot in Rahmstorf 2003 paper showing the temperature over the last 50,000 years.
 The next graphic shows the temperature for the last 50,000 years and the last 9,000 up close below, also derived from the GISP-2-ice cores.

On Rahmstorf’s paper, EIKE writes:
Easy to recognize, at least using the studies done by Rahmstorf, we are living in a comparably cold time today. During the MWP 1000 years ago, when the vikings were farming Greenland, it was 1°C warmer than today. During the Roman Optimum 2000 years ago, when Hannibal crossed the Alps with his elephants in the wintertime, it was even 2°C warmer than today. And during the Holocene climate optimum 3500 years ago it was about 3°C warmer than today. Since about  3200 years ago, there has been a cooling of about 2°C.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Multiple studies confirm that the warming was not a regional phenomena.
Schellnhuber could not discern any warming back in 2003
Meanwhile Prof. Hans Joachim Schellnhuber, Rahmstorf’s boss at the Potsdam Institute, was unable to discern any warming when examining a multitude of worldwide temperature records back in 2002 and 2003.
In a paper published in 2003, using their own studies, the authors concluded there had been no global warming over the last decades. (J.F. Eichner, E. Koscielny-Bunde, A. Bunde, S. Havlin, and H.-J. Schellnhuber: Power-law persistence and trends in the atmosphere, a detailed study of long temperature records, Phys. Rev. E 68 2003),
The temperature records of 95 stations distributed over the globe were studied. In the paper’s summary discussion, Schellnhuber and his colleagues wrote:
In the vast majority of stations we did not see indications for a global warming of the atmosphere.
and
Most of the continental stations where we observed significant trends are large cities where probably the fast urban growth in the last century gave rise to temperature increases.
And la pièce de resistance!
The fact that we found it difficult to discern warming trends at many stations that are not located in rapidly developing urban areas may indicate that the actual increase in global temperature caused by anthropogenic perturbation is less pronounced than estimated in the last IPCC Intergovernmental Panel for Climate Change.
Last I checked, global temperatures have gone nowhere since 2003. So where’s the warming?
—————————————————————————————————
Copyright reminder: It is not allowed to reproduce this post without first obtaining permission from No Tricks Zone. You may cut and paste max. 25% of the content, and then followed by a link to this site. Thanks!
Share this...FacebookTwitter "
"

Since its inception in 1908, the Federal Bureau of Investigation has kept itself busy surveilling those who hold controversial political views — from Christian pacifists in World War I, to Martin Luther King Jr. in the 1950s, to Arab/​Muslim Americans in the 1990s. “American Big Brother,” a new project from the Cato Institute, features an interactive online timeline of these surveillance projects over the past 100 years. “The theme that emerges clearly from the timeline’s episodes is that in many of these cases, federal surveillance and political repression were directed most forcefully at individuals and organizations that challenged the prevailing political paradigm on the issue at hand,” wrote Cato policy analyst Patrick G. Eddington. And in many cases, the individuals and organizations subjected to this warrantless surveillance suffered irreparable damage to their personal and professional lives. The timeline, found at www​.cato​.org/​a​m​e​r​i​c​a​n​-​b​i​g​-​b​r​other, already features dozens of stories of surveillance, and is an ongoing project which will be updated regularly with archival research and new developments in the news.



 **The Battle for Free Expression, Continued**  
 _The Tyranny of Silence_ , the story of how one cartoon ignited a global debate over free speech, was first published two years ago. Since then, the battle for free speech has raged on—from the tragic _Charlie Hebdo_ attacks in Paris, to the fight for free speech on public campuses. Now available for the first time in paperback, Flemming Rose’s book, which The Economist dubbed one of the best books of 2014, recounts his experience publishing cartoons of the prophet Muhammad in Danish newspaper _Jyllands‐​Posten_ in 2005, which quickly exploded into a global controversy known as the “Cartoon Crisis.” Rose bravely defended the decision to print the 12 drawings, even as Muslims around the world protested, Danish embassies came under attack, and newspaper and magazine editors were arrested. Rose tells his gripping personal story of the events that unfolded. “What do you do when suddenly the entire world is on your back?” Rose recalls. The paperback edition includes a new afterword, in which Rose reflects on the _Charlie Hebdo_ attack and the state of free speech in both Europe and America. The United States, he writes, is “afflicted with identity politics and grievance fundamentalism,” while in Europe, “it looks like freedom of speech will be sacrificed on the altar of cultural, religious, and ethnic diversity.”



 **The Economics of Environmentalism**  
More than 10 years after its original publication, the second edition of Richard Stroup’s invaluable _Eco‐​nomics: What Everyone Should Know about Economics and the Environment_ provides a thoroughly updated guide to environmental problems from a free market perspective. As in the first edition, Stroup offers a concise primer of how economic principles shed light on environmental issues, and why so many environmental laws fail. But Stroup also adds new chapters, including a brief overview of the history of environmentalism in the United States, the “constantly changing view of our environment and how to protect it,” and an examination of the most controversial environmental issue of today—climate change. “Although the book is a small one, I have attempted to identify in it the core tenets of free‐​market approaches to environmental protection and to make clear why these approaches are worth serious consideration,” writes Stroup. “The weight of opinion tends to push toward a greater role for government, even though that role is often misused and sometimes has unfortunate consequences. Economics shows us the wisdom of considering a greater role for market solutions.”
"
"
Title with apologies to Crosby, Stills, Nash, and Young.
In my last post, part 77 of “How not to measure temperature” I pointed out that the National Weather Service in Upton NY has a weather station that is way out of compliance due to the way it is setup and the proximity to bias factors such as the parking lot.
There are thousands of weather stations across the USA, some run by various agencies. Often we’ll see them at national parks with interpretive displays. This one I encountered in Ely Nevada on my last road trip to finish the Nevada USHCN station surveys was part of an air quality and environmental monitoring program jointly run by the Department of Energy (DOE) and the Desert Research Institute (DRI).
It is an impressive station with multiple state of the art sensors, solar power, and a datalogger with a satellite uplink to DRI’s HQ. You can look at hourly data from the station at the CEMP DRI website here.
It is located about 2 miles northeast of town on government property, BLM land:
Ely, NV Weather Station operated by DOE/DRI -click for larger image
What is unique about this station is that it has an interpretive exhibit with live data readouts. I applaud DRI/DOE for doing this. Here are what the they look like closeup:

Click for larger images to read the text on the interpretive displays
As I said, I applaud DRI/DOE for doing this. Taking the effort to make such a wonderful educational display is a good use of taxpayer funds.
Except, that is, when they miss one critical detail.
Ely, NV Parking Lot Education Weather Station operated by DOE/DRI - click for larger image
Yes, the expensive satellite uplinked state of the art interpretive educational weather station is sited in the middle of two asphalt parking lots. One is for RV storage, the other is the parking lot for the Ely District Office for the Bureau of Land Management.
Here is the the view to the northeast of how the temperature sensor sees the BLM land:
What the temperature sensor sees - click for a larger image
Here is the aerial view of the placement:
Aerial view - Ely, NV Weather Station operated by DOE/DRI -click for larger image
With the parking lots on both sides being active with cars and RV’s, I would imagine that a fairly variable albedo exists, especially on weekends and holidays.
This wouldn’t be so bad if it was only an educational station with an interpretive exhibit, as one could explain it was placed here for the convenience of viewing and science really doesn’t advocate measuring the temperature of parking lots.
Except that this station is used for an active science project. How much of the other data measurements and calculations for such things as Tritium dispersal, gaseous pollutant volumes, etc are dependent on the temperature, humidity, and dewpoint data gathered here, all of which would be affected by the siting?
Contrast it to the ASOS station siting at the airport across the road. The ASOS is about 1000 feet NW of the southern runway intersection which you can see here in Google Maps

Normally ASOS stations are much more poorly sited than state of the art stations, but this example  illustrates how spending tens of thousands of dollars on hi-tech measurement gear can be undone by lack of simple planning.
Happy Thanksgiving everyone!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ad5d699',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Al Gore’s histrionics are amusing, but nothing he has said compares to Sheryl Crow’s proposal to restrict how much toilet paper can be used. Perhaps there can be a new monitoring bureaucracy to search our homes. Maybe government agencies can stand guard in public restrooms. The BBC reports on the latest in cutting‐​edge environmentalism: 



Singer Sheryl Crow has said a ban on using too much toilet paper should be introduced to help the environment. … The 45-year-old…has just toured the   
US on a biodiesel‐​powered bus to raise awareness about climate change. …The pair targeted 11 university campuses to persuade students to help combat the world’s environmental problems. … “I have spent the better part of this tour trying to come up with easy ways for us all to become a part of the solution to global warming,” Crow wrote. …“I propose a limitation be put on how many squares of toilet paper can be used in any one sitting.”



Crow’s publicists managed to get the BBC to reference her biodiesel bus, but her environmental bona fides do not stand up under closer scrutiny. Thesmok​ing​gun​.com exposes the demands she makes when going on tour: 



The rock star’s performance contract includes specific day‐​to‐​day instructions on what kind of booze Sheryl needs in her dressing room (TSG has never seen such attention to detail in any other concert rider we’ve posted). … promoters are directed to purchase specific booze depending on what day of the week the concert falls, as the below rider excerpt reveals. Additionally, when the global warming warrior hits the road, her touring entourage (and equipment) travels in three tractor trailers, four buses, and six cars. Now that’s a carbon footprint!
"
"
 
Guest post by Frank Lansner, civil engineer, biotechnology.
(Note from Anthony – English is not Frank’s primary language, I have made some small adjustments for readability, however they may be a few  passages that need clarification. Frank will be happy to clarify in comments)
It is generally accepted that CO2 is lagging temperature in Antarctic graphs. To dig further into this subject therefore might seem a waste of time. But the reality is, that these graphs are still widely used as an argument for the global warming hypothesis. But can the CO2-hypothesis be supported in any way using the data of Antarctic ice cores?
At first glance, the CO2 lagging temperature would mean that it’s the temperature that controls CO2 and not vice versa.

Click for larger image Fig 1. Source: http://www.brighton73.freeserve.co.uk/gw/paleo/400000yrfig.htm
But this is the climate debate, so massive rescue missions have been launched to save the CO2-hypothesis. So explanation for the unfortunate CO2 data is as follows:
First a solar or orbital change induces some minor warming/cooling and then CO2 raises/drops. After this, it’s the CO2 that drives the temperature up/down. Hansen has argued that: The big differences in temperature between ice ages and warm periods is not possible to explain without a CO2 driver.
Very unlike solar theory and all other theories, when it comes to CO2-theory one has to PROVE that it is wrong. So let’s do some digging. The 4-5 major temperature peaks seen on Fig 1. have common properties: First a big rapid temperature increase, and then an almost just as big, but a less rapid temperature fall. To avoid too much noise in data, I summed up all these major temperature peaks into one graph:

Fig 2. This graph of actual data from all major temperature peaks of the Antarctic vostokdata confirms the pattern we saw in fig 1, and now we have a very clear signal as random noise is reduced.
The well known Temperature-CO2 relation with temperature as a driver of CO2 is easily shown:

Fig 3.
Below is a graph where I aim to illustrate CO2 as the driver of temperature:

Fig 4. Except for the well known fact that temperature changes precede CO2 changes, the supposed CO2-driven raise of temperatures works ok before temperature reaches max peak. No, the real problems for the CO2-rescue hypothesis appears when temperature drops again. During almost the entire temperature fall, CO2 only drops slightly. In fact, CO2 stays in the area of maximum CO2 warming effect. So we have temperatures falling all the way down even though CO2 concentrations in these concentrations where supposed to be a very strong upwards driver of temperature.
I write “the area of maximum CO2 warming effect “…
The whole point with CO2 as the important main temperature driver was, that already at small levels of CO2 rise, this should efficiently force temperatures up, see for example around -6 thousand years before present. Already at 215-230 ppm, the CO2 should cause the warming. If no such CO2 effect already at 215-230 ppm, the CO2 cannot be considered the cause of these temperature rises.
So when CO2 concentration is in the area of 250-280 ppm, this should certainly be considered “the area of maximum CO2 warming effect”.
The problems can also be illustrated by comparing situations of equal CO2 concentrations:

Fig 5.
So, for the exact same levels of CO2, it seems we have very different level and trend of temperatures:

Fig 6.
How come a CO2 level of 253 ppm in the B-situation does not lead to rise in temperatures? Even from very low levels? When 253 ppm in the A situation manages to raise temperatures very fast even from a much higher level?
One thing is for sure:
“Other factors than CO2 easily overrules any forcing from CO2. Only this way can the B-situations with high CO2 lead to falling temperatures.”
 
This is essential, because, the whole idea of placing CO2 in a central role for driving temperatures was: “We cannot explain the big changes in temperature with anything else than CO2”.
 
But simple fact is: “No matter what rules temperature, CO2 is easily overruled by other effects, and this CO2-argument falls”. So we are left with graphs showing that CO2 follows temperatures, and no arguments that CO2 even so could be the main driver of temperatures.
– Another thing: When examining the graph fig 1, I have not found a single situation where a significant raise of CO2 is accompanied by significant temperature rise- WHEN NOT PRECEDED BY TEMPERATURE RISE. If the CO2 had any effect, I should certainly also work without a preceding temperature rise?!  (To check out the graph on fig 1. it is very helpful to magnify)
Does this prove that CO2 does not have any temperature effect at all?
No. For some reason the temperature falls are not as fast as the temperature rises. So although CO2 certainly does not dominate temperature trends then: Could it be that the higher CO2 concentrations actually is lowering the pace of the temperature falls?
This is of course rather hypothetical as many factors have not been considered.

Fig 7.
Well, if CO2 should be reason to such “temperature-fall-slowing-effect”, how big could this effect be? The temperatures falls 1 K / 1000 years slower than they rise. 
However, this CO2 explanation of slow falling temperature seems is not supported by the differences in cooling periods, see fig 8.
When CO2 does not cause these big temperature changes, then what is then the reason for the  big temperature changes seen in Vostok data? Or: “What is the mechanism behind ice ages???”
 
This is a question many alarmists asks, and if you can’t answer, then CO2 is the main temperature driver. End of discussion. There are obviously many factors not yet known, so I will just illustrate one hypothetical solution to the mechanism of ice ages among many:
 
First of all: When a few decades of low sunspot number is accompanied by Dalton minimum and 50 years of missing sunspots is accompanied by the Maunder minimum, what can for example thousands of years of missing sunspots accomplish? We don’t know.
 
What we saw in the Maunder minimum is NOT all that missing solar activity can achieve, even though some might think so. In a few decades of solar cooling, only the upper layers of the oceans will be affected. But if the cooling goes on for thousands of years, then the whole oceans will become colder and colder. It takes around 1000-1500 years to “mix” and cool the oceans. So for each 1000-1500 years the cooling will take place from a generally colder ocean. Therefore, what we saw in a few decades of maunder minimum is in no way representing the possible extend of ten thousands of years of solar low activity.
It seems that a longer warming period of the earth would result in a slower cooling period afterward due to accumulated heat in ocean and more:

Fig 8.
Again, this fits very well with Vostok data: Longer periods of warmth seems to be accompanied by longer time needed for cooling of earth. The differences in cooling periods does not support that it is CO2 that slows cooling phases. The dive after 230.000 ybp peak shows, that cooling CAN be rapid, and the overall picture is that the cooling rates are governed by the accumulated heat in oceans and more.
Note: In this writing I have used Vostok data as valid data. I believe that Vostok data can be used for qualitative studies of CO2 rising and falling. However, the levels and variability of CO2 in the Vostok data I find to be faulty as explained here:
http://wattsupwiththat.com/2008/12/17/the-co2-temperature-link/

Sponsored IT training links:
Pass PMI-001 exam fast using self study 70-290 guide and 350-029 tutorial.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98b94e93',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
More Signs of the Apocalypse
From Medindia.com
Posted online: Friday, August 29, 2008 at 2:48:25 PM
Report Confirms Four Austrians Suffer Tick-borne Encephalitis from Cheese
Medical experts confirmed on Thursday that four people recently fell ill with tick-borne encephalitis (TBE) in western Austria after eating homemade goats’ cheese.
A shepherd in western Vorarlberg province, who had checked into hospital in July with flu-like symptoms, was found to have the illness following a blood test.
But the man said he had noticed no tick bites, the usual method of transmission, two experts from the Institute of Virology at Vienna Medical University wrote in an article published Thursday.
Doctors finally traced the cause of the illness to the cheese, which the shepherd had made from unpasteurised goat’s and cow’s milk on an isolated pasture at over 1,560-metre (5,120-feet) altitude.
Three other members of his family, who had not been on the pasture, also exhibited flu-like symptoms and headaches.
Further tests found that one of the goats, whose milk had been used to make the cheese, as well as other animals who had eaten leftovers, had developed TBE anti-bodies, meaning they had also been infected.
Ticks were believed until now to be found only below 1,350-metre altitude, but this may have changed due to global warming, the experts said.
Cases of TBE infections via dairy products were reported in recent years almost exclusively in Baltic countries.
Unfortunately, the article does not say just who the experts are.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d00014b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The nearly decade‐​old Regional Greenhouse Gas Initiative (RGGI) was always meant to be a model for a national program to reduce power plant carbon dioxide (CO2) emissions. The Environmental Protection Agency (EPA) explicitly cited it in this fashion in its now‐​stayed Clean Power Plan. Although the RGGI is often called a “cap and trade” program, its effect is the same as a direct tax or fee on emissions because RGGI allowance costs are passed on from electric generators to distribution companies to consumers. More recently, an influential group of former cabinet officials, known as the “Climate Leadership Council,” has recommended a direct tax on CO2 emissions (Shultz and Summers 2017).



Positive RGGI program reviews have been from RGGI, Inc. (the program administrator) and the Acadia Center, which advocates for reduced emissions (see Stutt, Shattuck, and Kumar 2015). In this article, I investigate whether reported reductions in CO2 emissions from electric power plants, along with associated gains in health benefits and other claims, were actually achieved by the RGGI program. Based on my findings, any form of carbon tax is not the policy to accomplish emission reductions. The key results are:



• There were no added emissions reductions or associated health benefits from the RGGI program.



• Spending of RGGI revenue on energy efficiency, wind, solar power, and low-income fuel assistance had minimal impact.



• RGGI allowance costs added to already high regional electric bills. The combined pricing impact resulted in a 12 percent drop in goods production and a 34 percent drop in the production of energy-intensive goods. Comparison states increased goods production by 20 percent and lost only 5 percent of energy-intensive manufacturing. Power imports from other states increased from 8 percent to 17 percent.



The regional program shifted jobs to other states. A national carbon tax would shift jobs to other countries. A better policy to reduce CO2 emissions is to encourage innovation rather than rely on taxes and regulation. The United States has already reduced emissions 12 percent from 2005 to 2015, more than any other developed country with a large economy, mainly through innovations in natural gas drilling techniques. There are many other opportunities to invest in innovation, for example, improved solar photovoltaic cells, more efficient batteries, small modular nuclear reactors, and nascent technologies that use fossil fuels without emitting CO2.



Ten northeast states joined together to form the RGGI to require power plants with a capacity of more than 25 megawatts to buy emission allowances for each ton of CO2 emissions. The states included Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Rhode Island, and Vermont. The allowances were sold in quarterly auctions beginning in 2008. The initial plan was to gradually reduce the number of allowances available to achieve a 10 percent emission reduction by 2018. New Jersey dropped out of the plan in 2011. In 2013, RGGI, Inc. announced plans for a 45 percent reduction in the number of allowances available in auctions beginning in 2014, with an additional 2.5 percent reduction each year until 2020 (Brown 2013: 1). Consequently, allowance prices began to rise, and RGGI states are now negotiating an extension to 2030, with an additional 30 percent reduction in allowable emissions.



The program is touted by RGGI, Inc. as a market-based system. However, the program applies a minimum reserve price and a Cost Cap Reserve that kicks in additional allowances if an annual price cap is exceeded (Figure 1). The proposed agreement for 2030 also includes an Emissions Containment Reserve whereby states can withhold allowances if auction prices fall below a set target price. A true market-based cap and trade program would allow the market to set the price. Allowance prices averaged about $3/ton from 2008 to 2013 ranging from about $2 to $4. In 2014, there was a dramatic cut in the number of available allowances that forced prices to a high of $7.50/ton in 2015, tracking the Cost Cap Reserve target. Prices began to fall after the Clean Power Plan implementation was stayed by the Supreme Court, and hit $2.53/ton in June, 2017, compared to a reserve price of $2.15. The extension targets a $13/ton price in 2021, and $24/ton in 2030. Speculators have made up roughly one-quarter of allowance purchases, trading with compliance entities in a secondary market.





According to Hibbard et al. (2011:15), in a report for the Analysis Group, “Within the electric system, the impacts of these initial (RGGI) auctions show up during the 2009–11 period, as power plant owners priced the value of CO2 allowances into prices they bid in regional wholesale prices.” A flow diagram in that report (p. 22) shows how the auction costs flow from the electric generators to the electric distributors, and on to consumers, the same as a direct tax or fee would do.



In order to claim success for RGGI, the first cap and trade program in the United States, we need to consider some related issues:



1\. Can the measured emission reductions be accounted for by non-RGGI causes?



2\. Can the impacts on the economy be clearly broken down into statistically confirmable independent (RGGI inputs) and dependent variables (real GDP, or electric price changes)?



3\. Can the RGGI revenue expenditures be shown to have been necessary and to have had significant impacts?



4\. Were energy efficiency project claimed savings rigorously tested by weather-adjusted “before and after” meter readings?



RGGI fails to answer these questions. Unfortunately, the data needed for a robust statistical analysis (question 2) are not readily available and obtaining them is beyond the scope of this article. The other three are noted in the text that follows.



The change in electricity demand, by necessity, must consider the interplay of real economic growth, the details of that growth, changes in population, the impact of pricing, and changes in energy efficiency. The RGGI program has an impact on these parameters.



It is difficult to compare electric prices from state to state because of significant regional differences in power cost. Also, at roughly the same time RGGI started, many states began requiring increased use of energy sources like wind and solar in their Renewable Portfolio Standard (RPS) laws and set energy efficiency requirements.



A further complication is that a number of states deregulated the supply portion of electric bills allowing market competition just prior to the start of the RGGI program. All the RGGI states deregulated. Fortunately, there is a comparison sample of five non-RGGI states (Illinois, Ohio, Oregon, Pennsylvania, and Texas) that deregulated electric supply in a manner similar to the RGGI states, and also had significant RPS requirements. Both RGGI and non-RGGI states have wide variation in their RPS programs, which adds uncertainty. Increasing wind and solar power raises electric rates because they are premium-priced power sources. For example, the increase in Delaware’s electric prices by 9 percent is directly related to the RPS, which shows up on consumers’ Delmarva Power electric bills. Likewise, Maryland electric bills have increased 14 percent for the same reason, according to a report from the Maryland Energy Administration (Tung 2017: 17).1



Non-RGGI comparison states actually added more wind and solar generation than RGGI states: adding 5.5 percentage points to generation compared to 2.3 percentage points in the RGGI states. Even removing the large wind farm construction effort in Texas from the calculation, the non-RGGI comparison states still outperformed the RGGI states: adding 3.4 percentage points compared to 2.3. The cost of wind and solar power has averaged two to three times the megawatt-hour rate compared to existing conventional fuel sources. The price impact should be greater in the non-RGGI states. Despite this disadvantage, the non-RGGI states still had lower overall price increases.



Several states that offered limited deregulation were not included in the comparison, and New Jersey is not included as an RGGI state because it dropped out of RGGI in 2011, and California is not included because it began a carbon tax just a few years ago. The results shown in Figure 2 cover the period from 2002 to 2015 to capture the impact of the four policies taken together (deregulation, RPS, energy efficiency, and RGGI).





To more accurately isolate the impact of RGGI between 2007 and 2015, the weighted average of total electric revenue for the multi-state groups is used in Table 1, and shows RGGI prices rose 64 percent more than comparison states. The increase was split between direct RGGI cost pass-though and indirect cost. Direct emission allowance cost was $436 million in 2015, about half the price differential between RGGI and comparison states. The rest of the difference may be due to indirect RGGI costs. For example, when power is imported to Delaware and Maryland from the PJM Regional Transmission Organization, there are premium charges for transmission distances, transmission congestion, and capacity. An earlier study, “Cost Impacts of 2013 RGGI Rule Changes in Delaware” (Stevenson and Stapleford 2016: 2), demonstrates that RGGI allowances directly added $11 million a year to Delaware electric bills, while the indirect costs added another $28.5 million.





Prices in RGGI states rose concurrent with more energy-intense manufacturing segments of the economy leaving the RGGI states with slower overall real economic growth based on Regional Real Chained GDP (Table 2). Linking real economic growth to RGGI alone is fraught with problems: real economic growth rates in RGGI states between 2007 and 2015 varied widely from a negative 7.1 percent for Connecticut to a plus 11.9 percent for Massachusetts. Can we realistically claim RGGI helped Massachusetts but hurt Connecticut at the same time?





The comparison states’ economies grew 2.4 times faster than the RGGI states. Data from the U.S. Bureau of Economic Analysis show that the RGGI states lost 34 percent of energy-intensive businesses (primary metals, food processing, paper products, petroleum refining, and chemicals); the comparison states lost only 5 percent. The RGGI states lost 12 percent of overall goods production, while the comparison states grew by over 20 percent. We see this impact show up in industrial electric demand with the RGGI states falling 18 percent, while non-RGGI comparison states fell only 4 percent (Table 3).





Consideration also needs to be given to energy efficiency improvements as shown by the improvement in energy intensity (Table 4). RGGI states improved by 9.6 percent, while non-RGGI comparison states improved 11.5 percent. (Energy intensity improves when it goes down.)





According to RGGI, Inc. (2016), RGGI states are investing the RGGI revenue in energy efficiency projects, suggesting RGGI states should be improving energy efficiency faster than other states. Based on gains in overall energy intensity, this claim appears to be false. An explanation for this disparity may be that the funds are not going to energy efficiency, or that the energy efficiency projects may not be working well. Both effects are seen in Delaware where 35 percent of allowance revenue is assigned to the Department of Natural Resources & Environmental Control (DNREC), and the rest flows through a private, nonprofit organization known as the Sustainable Energy Utility (SEU). Delaware has received $100 million in RGGI revenue: $55 million remains unspent and another $22 million has gone to administrative overhead and fuel assistance, with just $23 million (23 percent) going for energy efficiency projects.2



The Maryland Energy Administration (2016) reported that only 25 percent of RGGI revenue was allocated to grants for energy efficiency projects, and that doesn’t take into account any money from the grants used for administration by the grantees.



Could the energy efficiency and renewable energy projects have been completed without the RGGI grants? The Maryland 2016 report, Appendix B, lists hundreds of projects receiving grants. Most of the renewable energy grants went to individuals or companies to install solar photovoltaic cells. The grants were small, running from $700 to $1,000 for residential systems that typically cost about $20,000. Solar projects receive federal tax credits, and the owners can sell renewable energy production credits to utilities that are required to buy them by state law, and receive full credit for every kilowatt-hour of energy produced from the local utility. Using a proprietary spreadsheet program, I find that the internal rate of return of a residential system falls from 10.6 percent with the state grant to 9.2 percent without the grant.3 Most of the projects would move forward without the RGGI revenue grants.



In a report for the Delaware Department of Natural Resources and Environmental Control, Small (2012: 3) found that the federally financed “Weatherization Assistance Program,” which receives 10 percent of RGGI revenue, was shut down for two years while all existing projects were reviewed and redone as needed after a federal audit found various quality control issues. This shows how state evaluation, measurement, and verification measures are not working.



The most rigorous test for energy efficiency projects is to check weather-adjusted meter readings before and after the project is implemented. I have found only one large-scale study by Alberini, Gans, and Towe (2013) that did this. The authors found Maryland homeowners who replaced their heat pumps with no incentives saved an average of 16 percent on electric usage. Meanwhile, homeowners receiving cash incentives of $300, $450, and $1,000 or more had energy savings of 6.2, 5.5, and 0 percent, respectively. The authors concluded on page 7 that “the survey responses provide suggestive evidence the ‘rebaters’ were disproportionately replacing ‘inadequate’ units, leading us to conjecture that the rebates are being used to defray the cost of more powerful units, or of units that end up being used more.”



Table 5 shows predicted changes in electricity demand in the RGGI states based on the 2007 demand adjusted for economic growth (7.2 percent from Table 4), population change (1 percent from U.S. Census data), loss of goods production (−12 percent from Table 2), and efficiency improvements (−9.6 percent from Table 4). The actual demand fell 11 million megawatt-hours, close to the projected 14 million.





Emissions were reduced about 40 percent from 2007 to 2015 from electric generating units in the RGGI states (Table 6). That compares to only about a 20 percent reduction in emissions for the country as a whole and the comparison states, suggesting RGGI has been a suc⁠cess. As raw percentages, this would be true, but the base emissions of the RGGI states are much lower than the total for the country, so a relatively small change can appear as a relatively large percent.





Table 7 shows high CO2 emission coal-fired generation _drops_ 16 percentage points in both RGGI and non-RGGI comparison states, and natural gas _rises_ virtually the same amount (10 for RGGI states versus 9 for non-RGGI states).





The non-RGGI comparison states actually added more wind and solar generation than the RGGI states (5.5 percentage points versus 2.3), even after allowing for a very large wind farm proliferation in Texas. Some RGGI auction revenue was invested in solar energy projects, but the RGGI, Inc. (2016) report identifies less than 100 MW of added solar capacity, which would account for only about 1 percent of the total wind and solar capacity added in the RGGI states according to generation data in the U.S. EIA _Electric Power Monthly_ .



Another way to sort out the impacts of the RGGI program on emissions reductions is to review regulatory and market impacts to the generation mix and emissions in detail. The impacts of exporting emissions through the increased importing of power must also be considered. If a comparison is made of the estimate of emission reductions using just factors common in all states, the comparison should isolate the impact of the RGGI program. The result of this comparison is discussed below and shows RGGI had no impact on emissions.



Delaware provides an early example of exporting emissions that can be found in a number of articles published in the _Wilmington News Journal_ beginning in January 2008. On December 17, 2008, Delaware participated in its first regional cap and trade auction. Three weeks later the Valero-owned Delaware City Refinery announced the shut-down of its electric generation at the plant. According to RGGI, Inc. (2009), CO2 emissions from the plant’s electric generation facility accounted for 17 percent of Delaware’s initial emission allocation. Valero had been gasifying petroleum coke, a waste product from the refinery, to fuel the power plant. Petroleum coke has emission rates similar to coal, but by gasifying it Valero reduced emissions of other air pollutants. So, three weeks into the RGGI program Delaware met its total 10 percent RGGI reduction goal. That isn’t the end of the story. Valero sold the facility to PBF Energy. PBF restarted portions of the power plant fueled with conventional natural gas. The petroleum coke was loaded onto ships and sent to China to be burned directly for electric generation without pollution controls.



The RGGI states export CO2 when they increase the import of electricity from other states. Between 2007 and 2015, the RGGI states doubled their imports (Table 8). Much of the imported power comes from the PJM transmission region. Adjusting for this factor decreases the RGGI state emissions reductions about 11 million tons.





CO2 emissions are down across the country. A number of major EPA regulations have been implemented since 2009. Electric power plants have seen the most impact from regulation including the Mercury & Air Toxics Standard (MATS), the Cross State Air Pollution Rule (CSAPR), the Carbon Pollution Standard for New Power Plants that established New Source Performance Standards (NSPS), and the Clean Power Plan (CPP), all aimed at reducing the use of coal and forcing the closure of older, smaller power plants that were not worth upgrading with expensive new filtration equipment, given the low cost of natural gas.



The question is how much of the improvement in power plant emission reduction was caused by EPA regulations. As shown in Figure 3, nominal natural gas prices dropped significantly starting about 2009, driven by an increase in supply from the deployment of hydraulic fracturing and horizontal well drilling technology in shale formations. The types of coal used for electric generation have no other significant uses, and price tends to be stable because electric demand does not vary much from year to year. Natural gas has a number of high volume uses, such as for industrial feedstock and as a primary fuel for heating. Heating demand can vary significantly from year to year. For example, very cold temperatures in the winter of 2014 caused a spike in demand and price. Lower overall natural gas prices played a major role in the switch from coal to natural gas for electric generation starting in 2009, and regulations impacted generation capacity starting in 2012.





Total electric generation was relatively constant since 2003, but increased almost 3 percent from 2009 to 2016 as the economy recovered from the recession (Figure 4). That increase in demand was met with wind and solar power growth driven by state Renewable Portfolio Standards along with federal and state subsidies. Coal-fired generation was relatively constant until 2008, but began to fall in 2009. The fall paralleled declining natural gas prices. Natural gas generation has been increasing at a relatively constant rate.





EPA regulations did impact coal-fired generation capacity as shown in Figure 5. The downturn in coal capacity coincides with new regulation implementation beginning in 2012. Lower natural gas prices obviously influenced the decisions to close down the coal-fired generation.





However, more important to coal-fired generation was the change in the capacity factor, that is, how often power plants ran in comparison to natural gas-fired power plants (Figure 6). The decline tracks the falling natural gas price curve that began in 2009.





With some certainty nationally, coal plant capacity reductions were caused by EPA regulations, and output reductions were caused by falling nominal natural gas prices. The impact of the two trends can be parsed. The computational details are provided in Stevenson (2017: 12). The result, both nationally and for the RGGI states, is an identical 28 percent from lost generation capacity, and 72 percent from lower natural gas prices. If the RGGI allowance program had a significant impact, it would have offset some of the impact of lower natural gas prices, because the allowance cost acts as an additional variable production cost, and would have shifted the ratio, but it didn’t. This result is not unexpected as RGGI allowance revenue only averaged 0.6 percent of electric revenue between 2007 and 2015 ($0.3 billion/$51 billion).



To complete the estimate of emissions from common factors, the changes in natural gas-fired and petroleum-fired generation need to be added. Table 9 shows that the total net estimated reduction in emissions for RGGI states, due to factors common to all states, was 59.7 million metric tons. That figure is slightly higher than the actual reduction of 57.2 million metric tons, which suggests that the actual reduction is accounted for without any significant additional contribution from the RGGI program.





According to RGGI, Inc. (2016), in its report titled _The Investment of RGGI Proceeds through 2014_ , 15 percent of RGGI revenue ($178.2 million) went to direct low-income electric bill assistance to 2.6 million households from the beginning of the RGGI auctions in 2008 through 2014. The RGGI funds, about $30 million a year, were added to the federal Low Income Home Energy Assistance Program (LIHEAP). According to the U.S. Department of Health and Human Services (2014: 10–11), the federal government provided $795 million to RGGI states in 2014. Thus, RGGI added less than 4 percent to LIHEAP ($30 million annual RGGI contribution/$795 million federal contribution).



RGGI allowance revenue totaled $1.8 billion through 2014. The allowance program added $0.85/megawatt-hour to electric bills between 2008 and 2014 ($294 million a year/348 million megawatt-hours demand a year). RGGI state residential electric demand has been fairly flat, and averaged 130.9 million megawatt-hours/year. According to the U.S. Census Bureau (2010), there were 17.3 million households in the RGGI states. Thus, residential electric demand averaged 7.6 megawatt-hours per year (130.9/17.3). The total cost of RGGI equaled $6.50/household ($0.85 × 7.6). This reduces the net contribution to low-income households to $5/year ($11.50−$6.50). Therefore, the net RGGI contribution to the federal LIHEAP was only 1.6 percent, an insignificant amount.



In this article, I investigate claims by the Acadia Center (Stutt, Shattuck, and Kumar 2015: 6) and RGGI, Inc. (2016) that the RGGI program has generated significant benefits. Using data from five comparison states with similar overall electricity policies, except for RGGI, along with looking at national trends, I find the RGGI, Inc. and Acadia Center claims to be misleading.



The Acadia Center claims that compared to other states RGGI states increased electric prices by half as much, had 3.6 percent more economic growth, and reduced emissions 16 percent more leading to greater health benefits from pollution reduction. In reality, from 2007 to 2015, net weighted average nominal electricity prices rose 4.6 percent in RGGI states compared to 2.8 percent in comparison states. Linking real economic growth to RGGI alone is fraught with problems. Real economic growth rates in RGGI states between 2007 and 2015 varied widely from a negative 7.1 percent for Connecticut to a plus 11.9 percent for Massachusetts. Also average RGGI revenue amounted to only 0.01 percent of the combined average real GDP of the RGGI states, so one wouldn’t expect much impact. Ignoring those difficulties, real economic growth was 2.4 times faster in comparison states than in the RGGI states. High RGGI state electric rates led to a 34 percent reduction in energy-intensive industries and a 12 percent drop in the goods production sector, while comparison states saw only a 5 percent drop in energy-intensive industries and a 20 percent gain in goods production.



This article finds there were no added reductions in CO2 emissions, or associated health benefits, from the RGGI program. RGGI emission reductions are consistent with national trend changes caused by new EPA power plant regulations and lower natural gas prices. The comparison requires adjusting for increases in the amount of power imported by the RGGI states, reduced economic growth in RGGI states, and loss of energy-intensive industries in the RGGI states from high electric rates.



The RGGI, Inc. report focuses on the impacts of spending the allowance revenue and suggests significant gains in energy efficiency, wind and solar investments, and assistance with low-income energy bills. Noticeably, RGGI, Inc. does not make claims of superior emission reductions or lower power prices. In reality, the spending of the allowance revenue had marginal impacts. All states have shown energy efficiency gains. The RGGI states saw a lower improvement in energy intensity at 9.6 percent compared to 11.5 percent for comparison states, so there appears to be no RGGI-related gain in overall energy efficiency. Wind and solar energy installation was slower in RGGI states, increasing by only 2.3 percentage points, while comparison states grew by 5.5 percentage points, more than twice as fast. RGGI grants for wind and solar power accounted for only about 1 percent of all the wind and solar power added by the RGGI states. The net fuel assistance help for low-income households, 15 percent of all households, added only 1.6 percent to the federal Low Income Home Energy Assistance Program, or less than $5/year. RGGI had no meaningful impact on lower-income families. Meanwhile, the other 85 percent of households saw an increase in electricity cost of $6.50/year directly caused by the RGGI allowance cost.



Alberini, A.; Gans, W.; and Towe, C. A. (2013) “Freeriding, Upsizing, and Energy Efficiency Incentives in Maryland Homes.” Fondazione Eni Enrico Mattei Working Paper No. 82. Available at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2336044.



Brown, J. (2013) “RGGI States Propose Lowering Regional CO2 Emissions Cap 45%, Implementing a More Flexible Cost-Control Mechanism.” RGGI, Inc. Available at www.rggi.org/docs/PressReleases/PR130207_ModelRule.pdf.



European Commission (2016) “CO2 Time Series 1990–2015 per Region/Country.” _Emission Database for Global Atmospheric Research (EDGAR)_ , release version 4.3.2. Joint Research Centre (JRC)/PBL Netherlands Environmental Assessment Agency.



Hibbard, P. J.; Tierney, S. F.; Okie, A. M.; and Darling, P. G. (2011) _The Economic Impacts of the Regional Greenhouse Gas Initiative on Ten Northeast and Mid-Atlantic States._ Boston: Analysis Group.



Maryland Energy Administration (2016) _Maryland Strategic Investment Fund Report on Fund Activities FY2015._ Available at http://energy.maryland.gov/Reports/FY15_SEIF_Annual_Report.pdf.



PJM/EIS (2016) “Generation Attributes Tracking System.” _System Mix 2015_ . Available at https://gats.pjm-eis.com/GATS2/PublicReports/PJMSystemMix/FilterPJM.



RGGI, Inc. (2009) “RGGI Historical Emissions 2000 to 2008 Spreadsheet.” Available at http://www.rggi.org/historical_emissions.



______ (2016) “The Investment of RGGI Proceeds through 2014.” Available at www.rggi.org/docs/ProceedsReport/RGGI_Proceeds_Report_2014.pdf.



______ (2017) “Auction Results for Auctions 1–36.” Available at www.rggi.org/market/co2_auctions/results.



Shultz, G. P., and Summers, L. (2017) “This Is One Climate Solution That’s Best for the Environment and for Business.” _Washington Post_ (June 19).



Small, D. (2012) _Annual Report on the Weatherization Assistance Program_ . Wilmington, Del.: Delaware Department of Natural Resources & Environmental Control.



Stevenson, D. T. (2017) “Sorting Root Causes of Air Quality Improvements 2009 to 2015.” Caesar Rodney Institute Policy Paper (January). Available at https://criblog.wordpress.com/2017/02/12/sorting-root-causes-of-air-quality-improvements-2009-to-2015.



Stevenson, D. T., and Stapleford, J. E. (2016) “Cost Impacts of 2013 RGGI Rule Changes in Delaware.” Caesar Rodney Institute Policy Paper (August). Available at www.caesarrodney.org/pdfs/Cost_Impacts_of_2013_RGGI_Rule_Changes_in_Delaware.pdf.



Stutt, J.; Shattuck, P.; and Kumar, V. (2015) _Regional Greenhouse Gas Initiative Status Report, Part 1: Measuring Success._ Boston: Acadia Center (July).



Tung, M. B. (2017) “RPS Requirement and Aggregated Cost of RPS.” Presentation to the House Economic Matters Committee, Maryland Energy Administration, Baltimore, Md. (January 15). Available at http://news.maryland.gov/mea/wp-content/uploads/sites/15/2017/01/EconomicMattersPresentation1.23.2017.pdf.



U.S. Bureau of Economic Analysis (n.d.) “Real Chained GDP by State.” Interactive Tables available at www.bea.gov.



U.S. Census Bureau (2010) “Population and Housing Unit Estimate Tables by Year.” Available at www.census.gov/programs-surveys/popest/data/tables.2010.html.



U.S. Department of Health and Human Services (2014) _LIHEAP Report to Congress for Fiscal Year 2014._ Division of Energy Services. Washington: DHHS.



U.S. Energy Information Agency (EIA) (2016a) Detailed State Electricity Data: “Average Price by State by Provider (EIA-861)”; “Revenue from Retail Sales of Electricity by State by Sector by Provider (EIA-861)”; “Retail Sales of Electricity by State by Sector by Provider (EIA-861)”; “Net Generation by State by Type of Producer by Energy Source (EIA-906, EIA-920, and EIA-923)”; “U.S. Electric Power Industry Estimated Emissions by State (EIA-767, EIA-906, EIA-920, and EIA-923).” Available at www.eia.gov/electricity/data/state.



______ (2016b) _Electric Power Monthly_ (2007 and 2015): “Table 1.4.B Coal by State by Sector, Year-to-Date”; “Table 15B Petroleum Liquids by State by Sector, Year-to-Date”; “Table 1.7.B Natural Gas by State by Sector, Year-to-Date”; “Table 1.9.B Nuclear Energy by State by Sector, Year-to-Date”; “Table 1.10B Hydroelectric (Conventional) Power by State by Sector, Year-to-Date”; “Table 1.14.B Wind by State by Sector, Year-to-Date”; “Table 1.17.B Solar Photovoltaic by State by Sector, Year-to-Date”; Table 4.2 Total Average Cost of Fossil Fuels for Electric Utilities”; “Table 6.7A Capacity Factors for Utility Scale Generators Using Fossil Fuels.” Available at www.eia.gov/electricity/monthly.



1I use 2007 as the base year through 2015, unless otherwise noted. The reason for using 2007 is that RGGI auctions began in 2008, which was also the first year of the Great Recession.



2Calculation is based on information provided in an unpublished e-mail to a state senator of how DNREC spent RGGI allowance funds from 2014 to 2016, and from SEU Annual Reports (available at www.energizedelaware.org/sustainable-energy).



3I assume a 7,500 watt system @$2.85/watt cost with 20 year life, 9,000 KWh first-year generation reduced 0.5 percent per year, $0.1425/KWh electric rate rising 2 percent per year, $6 SREC value, and 30 percent federal investment tax credit.
"
"

Yes, you read the title correctly.
Sometimes I feel like a strange attractor for weather station chaos. Here I am at home tonight minding my own business, in my home office and I have the TV on. JEOPARDY comes on. Alex Trebek announces the categories…and I pay little attention until the last one is announced and he says “National Weather Service”. I practically got whiplash turning to look at the TV.  In 25 years of watching this TV program, that is a category I never expected to see.
Then to explain the category, up pops one of the “clue crew” people standing in front of the NWS office in Upton, New York, in the parking lot.
I didn’t hear a single word she said, because my eyes were transfixed on what was right behind her: a Stevenson Screen and MMTS just a couple of feet from the parking lot with the brick walls of the NWS office right behind it.
WTH!? Then it was gone.
I waited out the first round of JEOPARDY hoping to see more, but the contestants avoided the NWS category. Finally with nothing left they started into it. Then there it was again, the NWS station with visitor parking privileges.
After acing the category (the final answer was supercell) I decided to see if I could find this NWS office in Upton and maybe get a picture. I found that and more.
My first simple Google Image search found it right away, a photo taken during an open house on a Skywarn page:
MMTS and Stevenson Screen, NWS Office Upton NY - Photo: Bergen Skywarn
It did show the proximity to the brick building, but it really didn’t tell the whole story of what I saw in the TV shot. What was funny was that in the JEOPARDY segment, the NWS employees had apparently done some “sprucing up” and had painted the legs of the shelter and the MMTS mount pole a blue color to match the logo color of the NWS emblem over the office door:
Upton NY NWS office looking North - Photo: NWS
I found the above picture and the one below at the NWS Upton web page where they have a “virtual tour” of the facility. Here is another angle from the web page that shows the overall NWS complex, including the NEXRAD Doppler radar tower:
Upton NY NWS office looking Northeast - Photo: NWS
Looking at the style of the automobiles, I’m guessing these photos were taken sometime in the early 90’s when this office was opened. What is interesting about these photos, besides the siting issues with proximity to parking and the building, is the fact that the Stevenson Screen door is facing SOUTH rather than the requisite north. The idea is to keep direct sunlight from hitting the thermometers when readings are taken.
I thought perhaps this station is purely a “figurehead” used for school tours, etc, but then I thought: “Why would they want to show it being done incorrectly?”. I checked the NCDC MMS meta-database to see if the station was active. Oddly I couldn’t find the right station in Upton in the database. Poking around again at the NWS Upton website I found out why: This used to be New York City’s station. It was once on top of the RCA building as I discovered from their virtual tour:



Dec. 28, 1960 to Oct. 24, 1993
RCA/GE Building
30 Rockefeller Center NY, NY
Mezzanine  Level



Your National Weather Service office was located in midtown Manhattan on the mezzanine level of this building until October 25, 1993, when we relocated to our current site. The picture depicts the top of the building where our old radar was located (ball-shaped object). NOAA Weather Radio transmitters are also pictured and still reside atop 30 Rock.  
Once knowing it was the NYC station and not “Upton”, I was able to find it in the NCDC MMS metadatabase and determine that indeed it is an active station. Fortunately it is listed as NOT being part of the climate network, and neither USHCN or GISS uses this station.
From the lat/lon posted there (40.86667 -72.86667 ) I was able to locate the station on Google Earth:
Using the ruler tool - less than 4 meters from parking - Click for larger image
It turns out that the NWS Forecasts Office happens to be on the grounds of the Brookhaven National Laboratory, and the address is at 175 Brookhaven Ave, Upton, NY.
It seems that there is ample room in the grassy area in the rear to place a weather station, rather than putting it up front in the parking lot. A Microsoft Live maps image also shows the proximity issues up front and with the building.
Upton NY NWS office looking west - Click for live interactive image
Of course looking at this photo, it would now seem that the rear of the building might not be the best choice either with that bank of 5 a/c units back there. But it could find a site further away to the rear or perhaps cleared more trees.
Even if this station isn’t in the climate network,  it really does beg the question: why does the NWS blatantly flaunt flout their own 100 foot rule? Further, since this NWS Office is located on the grounds of the Brookhaven National Laboratory, wouldn’t you think they’d want to put their absolute best scientific foot forward?
Even is this station is only used to show school kids what a weather station looks like and how it is operated, why not do it right and show proper placement away from biases, proper door alignment on the screen, and explain why these things are important for proper measurements?
Or, maybe, these things aren’t important to the NWS at all.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ae5fb3f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Now that Newt Gingrich has vaulted into the lead in the Republican presidential stakes, he’s going to be seeing a lot more of this:





Calling it the “dumbest thing” he has done in recent years isn’t enough. This is not going away.



But, Newt can turn his pas‐​de‐​deux on the loveseat with Nancy into an opportunity to promote a consistent and electable political message.



Because he will be asked about this at every turn, Newt will have the air time and the opportunity to tell the truth on climate change — based upon real science — and to interface it with a limited‐​government philosophy. The two are easy to do.



Start with the obvious. Instead of waffling on the subject, he could just point out that earth’s surface temperature is about 1°C warmer than it was a century ago. There were two periods of warming, the first from about 1910 through 1945, and the second from the mid‐​1970s to the late‐​1990s. They were both roughly the same magnitude. Because the first one was long before we put the majority of fossil carbon into the atmosphere, Newt can say — with scientific authority — that the magnitude of ‘natural’ climate change is likely to be at least as large as what humans have done.



Then Newt should proceed to the future. It’s not the heat, it’s the sensitivity. How much it will warm in the future is a function of how sensitive the atmosphere is to doubling atmospheric carbon dioxide. There are a number of independent arguments now coming together showing that this value may have been overestimated. The reference for the most recent is a November issue of _Science_.



Next, a little refreshing honesty: in 2009, the House of Representatives passed a cap‐​and‐​trade bill (which the Senate did not) commanding that the average american — 38 years from now — be allowed the carbon dioxide emissions of the average citizen in 1867. Prior to its passage, Newt was undeniably for such a system. Newt could call that that the “second dumbest thing” he has done and be finished with it.



After setting the record straight, how about interjecting his own political philosophy? When asked what to do about global warming, he should give the honest answer: if the atmosphere indeed is not as sensitive to carbon dioxide changes as previously thought, the correct response is _nothing._



That’s because “nothing” really means something. Newt should channel his historian. When markets are free, capital supports innovation more efficiently than when they aren’t. Think about the remarkable changes in energy and technology in the last 100 years. Isn’t it rather obvious that the same will occur in the next century, if only we don’t hinder capital development? Newt might even use the catchy saw (first sloganed by Northern Illinois Gas in 1972) _the future belongs to the efficient_ , and that there are impressive market forces that advantage those who produce things efficiently or produce efficient things.



When the doomsayers say Newt is believing in a _Deus ex machine_ to dramatically cut carbon dioxide emissions, he might point out that those were the same folks who, only a very few years ago, told us we were running out of natural gas. Innovation and capital revolutionized drilling and fracturing shale, and we now know we have literally hundreds of years of it under our feet. There are a lot more voters and delegates around the country who will benefit from the shale revolution than there are in ethanol‐​addicted Iowa.



Thus comes Newt’s opportunity to back down his support of corn‐​based ethanol. It’s a loser. When it powers a car, its total life cycle results in more carbon dioxide emissions than simply sticking with gasoline.



Yes, if he ran away from ethanol before the Iowa primary, Romney might win. But Newt would be called courageous and the voters in the rest of the nation will notice.



Finally, Newt could say that he’s sick of the government subsidizing any form of energy or transportation. Sell GM and don’t look back. Stop subsidizing the Volt, solar energy, wind power, gas, coal, all things energy, and let people keep their money and invest. Stop tilting the federal purse at windmills.



Newt can do these things and win. Otherwise, he’s could spend fall of 2012 on that darned couch.
"
"
Share this...FacebookTwitterGuest writer Ed Caryl, author of One Of Our Hemispheres Is Missing and A Light In Siberia, now brings us his latest essay.
——————————————————————————————————————–
You Want Me To Believe What?
By Ed Caryl
The proponents of Anthropogenic Global Warming claim that man’s use of fossil fuels has released extra carbon dioxide (CO2) into the atmosphere, and that the extra CO2 is warming the earth catastrophically due to greenhouse effects, causing great “disruption” in climate.
There are many parts to this hypothesis. The statement starts out with a truth, and then gets hazier as it progresses, each step suggesting an increasing level of calamity, but a decreasing level of believability. So let us look at the claims one by one:

Man’s use of fossil fuels has released extra CO2 into the atmosphere.

Stipulated. The increase since the dawn of the industrial age is nearly 100 ppm.

The earth is warming in a catastrophic way.

Since the Maunder Minimum, the earth has warmed by perhaps 1°C. There is good evidence that any measurement of more warming than that has been tampered with or subject to confirmation bias. Only warming in the last 100 years, and in particular, in the last 50, can possibly be due to greenhouse effects. Supposedly, any AGW will be seen first in the Arctic. Yet, many Arctic weather stations show no warming.
The warming is due to greenhouse effects
Knut Ångstrom. Source: http://www.angstrom.uu.se/historia.php
There are nearly as many numbers cited for what a doubling of CO2 will do as there are scientists working in the field. The most often cited expert on the subject is Svante Arrhenius, even though he was a physical chemist, not an atmospheric scientist, lived and worked a hundred years ago, and considered atmospheric science a hobby. He published the first numbers in 1896, 4.7 to 6°C. These numbers were criticized by Knut Ångstrom (one of the first true atmospheric scientists) in 1900 as being much too high. Later, in 1906, Arrhenius adjusted that number downward to 1.6°C. A hundred years ago, there was no consensus, even in one man’s head. Yet today, Arrhenius’ first numbers are the ones most often cited, and Ångstrom’s criticism is forgotten.
The value is estimated by the IPCC Fourth Assessment Report (AR4) as likely to be in the range 2 to 4.5°C with a best estimate of about 3°C, but is very unlikely to be less than 1.5°C. Values substantially higher than 4.5°C cannot be excluded, but agreement of models with observations is not as good for those values.[1]
Other estimates range from 0.4°C to as high as 10°C. The later number seems obviously too high, given the current lack of warming, and even the IPCC seems to agree. Another, more complete list of estimates is here. Given the amount of confirmation bias among the workers in this field, it is surprising that the average estimate is still less than 3°C. Arrhenius’ 1906 number, 1.6°C, may end up closest to the truth.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The point here is that there is wide disagreement on what the affect of doubling CO2 will do. The numbers cover a range of more than 20. None of these numbers can be more than guesses, and they could all be wrong.
Oh, and other tiny points: Who says CO2 is going to double? And why?
And what is the role of water? Water vapor is a greenhouse gas also. And water, unlike CO2, exists in all three phases. As water changes phase, it takes up and gives up heat. I suspect that Dr. Michael Mann has never seen a southwest desert thunderstorm. Under those towering thunderheads, the temperature can drop from 40°C to 10°C,  in just a few minutes, accompanied by heavy rain and hail. Cubic miles of hot air rise in those clouds, expanding and cooling as it rises. The rising air gives up heat to condensing water droplets, and then more heat to fusing hailstones. All that heat is transferred ultimately to the top of the clouds to be radiated to space. This goes on 24/7, all around the world. Is this factored into the climate models?
AGW will cause great climate disruption
Here, the proof is in. It’s not happening! This is the core of the whole debate. James Hansen predicted in 1988 that the West Side Highway in New York City would be under water by 2008. The Battery tide gauge shows a two-inch rise, and the rise has been linear since the gauge was installed in 1856. Worldwide sea levels show the same modest rise at the same linear rate. If Michael Mann’s “hockey stick” temperature rise was true, we should see the same sharp rise in sea level. We do not.
Tornados – also not happening. The peak years were in the early 1970’s, and it has been downhill since.
Hurricanes? This is a bit tricky. Before the satellite era, hurricanes had to have been spotted by ship or plane. The Hurricane Hunter aircraft beginning in WWII covered ocean areas of interest to the military, but the Hurricane formation regions off the African coast were not watched until later. So we don’t have accurate counts until satellites watched everywhere, consistently. We do have some records of hurricanes that made landfall. Here is one long-term record for the Apalachee Bay, Florida. It shows a hurricane frequency peak 2500 years ago and another during the Medieval Warm Period. But it shows no increase in the present. Tropical cyclone formation is driven by sea-surface temperature, so it is reasonable to assume that tropical cyclone formation will be a function of ocean temperature cycles, such as the AMO and the Southern Oscillation. Currently, the Tropical Cyclone Energy is near the 30-year low. This is a point that Al Gore has given up, and he has removed this slide from his presentation.
What about drought, floods, heat waves, cold snaps, insect infestations and other Biblical plagues? All (except the first-born son one, unless you count the 10:10 video) have been mentioned, and all blamed on AGW. The problem is that at any given moment, somewhere in the world, a record is being broken. This is just statistics in action. Any noisy phenomenon, given enough time and space to act, will produce the occasional exceptional spike, but the record phenomenon have no pattern in time.
For a branch of science to have any validity, it must be testable. Tests of a science include: Does it make predictions that can be verified? Do those predictions match observations? So far, the predictions have not come to fruition.

The North Pole has not become ice-free
The South Pole ice is expanding
The icecaps at the poles are not collapsing. One iceberg doesn’t make a collapse.
The oceans are not flooding land anywhere.
The deserts are not expanding
The polar bears are doing just fine, thank you
So are the penguins, (except in South Africa, where they froze last winter).

So what is it again we’re supposed to believe?
And why?
Share this...FacebookTwitter "
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.   
  
_The Current Wisdom_ only comments on science appearing in the refereed, peer-reviewed literature, or that has been peer-screened prior to presentation at a scientific congress.   
  
**  
Better Model, Less Warming**   
  
Bet you haven’t seen this one on TV: A newer, more sophisticated climate model has lost more than 25% of its predicted warming! You can bet that if it had predicted that much more warming it would have made the local paper. __   
  
The change resulted from a more realistic simulation of the way clouds work, resulting in a major reduction in the model’s “climate sensitivity,” which is the amount of warming predicted for a doubling of the concentration of atmospheric carbon dioxide over what it was prior to the industrial revolution.   
  
Prior to the modern era, atmospheric carbon dioxide concentrations, as measured in air trapped in ice in the high latitudes (which can be dated year-by-year) was pretty constant, around 280 parts per million (ppm). No wonder CO2 is called a “trace gas”—there really is not much of it around.   




The current concentration is pushing about 390 ppm, an increase of about 40% in 250 years. This is a pretty good indicator of the amount of “forcing” or warming pressure that we are exerting on the atmosphere. Yes, there are other global warming gases going up, like the chlorofluorocarbons (refrigerants now banned by treaty), but the modern climate religion is that these are pretty much being cancelled by reflective “aerosol” compounds that go in the air along with the combustion of fossil fuels, mainly coal.   
  
Most projections have carbon dioxide doubling to a nominal 600 ppm somewhere in the second half of this century, absent no major technological changes (which history tells us is a very shaky assumption). But the “sensitivity” is not reached as soon as we hit the doubling, thanks to the fact that it takes a lot of time to warm the ocean (like it takes a lot of time to warm up a big pot of water with a small burner).   
  
So the “sensitivity” is much closer to the temperature rise that a model projects about 100 years from now – assuming (again, shakily) that we ultimately switch to power sources that don’t release dreaded CO2 into the atmosphere somewhere around the time its concentration doubles.   
  
The bottom line is that lower sensitivity means less future warming as a result of anthropogenic greenhouse gas emissions. So our advice… keep on working on the models, eventually, they may actually arrive at something close puny rate of warming that is being observed   
  
At any rate, improvements to the Japanese-developed Model for Interdisciplinary Research on Climate (MIROC) are the topic of a new paper by Masahiro Watanabe and colleagues in the current issue of the _Journal of Climate_. This modeling group has been working on a new version of their model (MIROC5) to be used in the upcoming 5th Assessment Report of the United Nations’ Intergovernmental Panel on Climate Change, due in late 2013. Two incarnations of the previous version (MIROC3.2) were included in the IPCC’s 4th Assessment Report (2007) and contribute to the IPCC “consensus” of global warming projections.   
  
The high resolution version (MIROC3.2(hires)) was quite a doozy – responsible for far and away the greatest projected global temperature rise (see Figure 1). And the medium resolution model (MIROC3.2(medres)) is among the Top 5 warmest models. Together, the two MIROC models undoubtedly act to increase the overall model ensemble mean warming projection and expand the top end of the “likely” range of temperature rise. 



FIGURE 1





Global temperature projections under the “midrange” scenario for greenhouse-gas emissions produced by the IPCC’s collection of climate models. The MIROC high resolution model (MIROC3.2(hires)) is clearly the hottest one, and the medium range one isn’t very far behind.   
  
The reason that the MIROC3.2 versions produce so much warming is that their sensitivity is very high, with the high-resolution at 4.3°C (7.7°F) and the medium-resolution at 4.0°C (7.2°F). These sensitivities are very near the high end of the distribution of climate sensitivities from the IPCC’s collection of models (see Figure 2). 



FIGURE 2





Equilibrium climate sensitivities of the models used in the IPCC AR4 (with the exception of the MIROC5). The MIROC3.2 sensitivities are highlighted in red and lie near the upper und of the collection of model sensitivities. The new, improved, MIROC5, which was not included in the IPCC AR4, is highlighted in magenta, and lies near the low end of the model climate sensitivities (data from IPCC Fourth Assessment Report, Table 8.2 and Watanabe et al., 2010).   
  
Note that the highest sensitivity is not necessarily in the hottest model, as observed warming is dependent upon how the model deals with the slowness of the oceans to warm.   
  
The situation is vastly different in the new MIROC5 model. Watanabe _et al_. report that the climate sensitivity is now 2.6°C (4.7°F) – more than 25% less than in the previous version on the model.[1] If the MIROC5 had been included in the IPCC’s AR4 collection of models, its climate sensitivity of 2.6°C would have been found near the low end of the distribution (see Figure 2), rather than pushing the high extreme as MIROC3.2 did.   
  
And to what do we owe this large decline in the modeled climate sensitivity? According to Watanabe _et al._ , a vastly improved handling of cloud processes involving “a prognostic treatment for the cloud water and ice mixing ratio, as well as the cloud fraction, considering both warm and cold rain processes.” In fact, the improved cloud scheme—which produces clouds which compare more favorably with satellite observations—projects that under a warming climate low altitude clouds _become a negative feedback_ rather than acting as positive feedback as the old version of the model projected.[2] Instead of enhancing the CO2-induced warming, low clouds are now projected to retard it.   
  
Here is how Watanabe _et al_. describe their results: 



A new version of the global climate model MIROC was developed for better simulation of the mean climate, variability, and climate change due to anthropogenic radiative forcing….   
  
MIROC5 reveals an equilibrium climate sensitivity of 2.6K, which is 1K lower than that in MIROC3.2(medres).... This is probably because in the two versions, the response of low clouds to an increasing concentration of CO2 is opposite; that is, low clouds decrease (increase) at low latitudes in MIROC3.2(medres) (MIROC5).[3]



Is the new MIROC model perfect? Certainly not. But is it better than the old one? It seems quite likely. And the net result of the model improvements is that the climate sensitivity and therefore the warming projections (and resultant impacts) have been significantly lowered. And much of this lowering comes as the handling of cloud processes—still among the most uncertain of climate processes—is improved upon. No doubt such improvements will continue into the future as both our scientific understanding and our computational abilities increase.   
  
Will this lead to an even greater reduction in climate sensitivity and projected temperature rise? There are many folks out there (including this author) that believe this is a very distinct possibility, given that observed warming in recent decades is clearly beneath the average predicted by climate models. Stay tuned!   
  
References:   
  
Intergovernmental Panel on Climate Change, 2007. Fourth Assessment Report, Working Group 1 report, available at http://www.ipcc.ch.&#13;  
  
Watanabe, M., et al., 2010. Improved climate simulation by MIROC5: Mean states, variability, and climate sensitivity. _Journal of Climate_ , **23** , 6312-6335.   

"
"
Solar cycle 23 as seen from SOHO - click for larger image
Below is a note forwarded to me by John Sumption from Jan Janssens. For those who do not know him, Jan runs a very comphrehensive solar tracking website here.
Jan included the caveat:
This  topic’s sure to start another heated discussion on the solar blogs 
So I’m happy to oblige by posting it here. Jansen makes some good points about the possible first month that cylce 24 spots exceed cycle 23 spots. But when you are in a deep minimum like this one, it is hard to pinpoint the transition, because next month may bring the reverse condition. He writes:
Prior  to August 2008, only 3 SC24-sunspot groups appeared. This was in January, April  and May. During these 3 months, SC23-activity was higher than SC24-activity.  Based on the NOAA-numbering, there were respectively (SC23 to SC24) 2 to 1, 2 to  1, and 4 to 1 sunspotgroups visible. 
In  August, there were no sunspotgroups numbered by NOAA. However, on 21-22 August  “something” was visible well enough to be seen by several observers and to  prompt the SIDC to give a (preliminary) non-zero sunspotnumber for those days. 
This group had a SC24-polarity but appeared on a moderate latitude of 15  degrees. Based on previous cycle transits, it is not unusual that some “early”  new cycle groups appear this low. If one considers this as a sunspotgroup and  belonging to SC24, then August was the month during which SC24-activity  outnumbered SC23 activity. 
However, if one adheres strictly to the NOAA-numbering, then September  ***might*** be that month. I stress “might”, because -unless some group appears  tomorrow or tuesday- the score will still be 1 to 1: On September 11th, NOAA did  number an even tinier group than the August one, and it was a SC23 group (NOAA  1001). SC24-activity then wins on “points”, because the Wolfnumbers for 22-23  September produced by NOAA 1002 (SC24) were higher than the NOAA 1002  Wolfnumber. 
Last  but not least, I want to emphasize that SC24-activity will be considered higher  than SC23’s when its smoothed group (or Wolf) number exceeds that of the old  cycle. This might happen in the coming months (or whenever her Majesty the Sun  feels up to it 😉


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c69f706',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSteffen Hentrich of the German liberal institute presents a comparison of the safety of various sources of energy. Much of the media, many politicians and a host of activists have all told us that nuclear energy is too dangerous to be used by man.
Is this claim founded on solid data and facts, or is it run-away hysteria? You be the judge. Shown are the number of deaths per terawatt-hour of energy produced.

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
By far, probably to James Hansen’s glee, coal, with its dangerous mining work, is the most dangerous of all. Surprisingly, even hydro, wind, and solar power are far more hazardous than nuclear.
Photovoltaic power generation kills, per terawatt-hour, 11 times more people than nuclear. 
Using the arguments of the anti-nuclear activists, we can say there needs to be an immediate moratorium on windmills, photovoltaic panels and biomass plants. Compared to nuclear, they’re simply too deadly.
So what does all this mean? Hentrich writes:
With this in mind, nuclear energy alone becomes the scapegoat, and this illustrates the glaring denial of reality in politics and in the public. This also shows that a rapid stop of nuclear power generation will in no way reduce the risks involved in power generation.”
Indeed, stopping nuclear power makes the power generation industry much more dangerous, and not safer.
Share this...FacebookTwitter "
"

Seven years ago, on March 13, 1993, the eastern third of the United States was pasted by a big low‐​pressure system that was (erroneously) called the “Storm of the Century.” It dumped two inches of snow on the Gulf Coast beaches of Florida and two feet of snow in a wide swath from southwestern Virginia through New York and was followed by record cold. The next day, Kevin Trenberth, a federal climatologist with the U.S. National Center for Atmospheric Research, went on _Meet the Press_ to say that global warming, El Niño, and the 1991 Mount Pinatubo volcano were all involved. Never mind that Pinatubo caused a well‐​known global cooling. 



A look out today’s window reveals a remarkably different picture of blue skies, daffodils and softball practice. 



Given the choice, which Ides of March do you prefer? 



The fact is that we have just completed the warmest winter in the 105‐​year record for the continental United States, and few are complaining. But because it is good news, the same federal climatologists who glibly pulled the greenhouse effect trigger on the 1993 snowstorm can’t bring themselves to tell why we are so happy: global warming. Instead, they subsequently trotted out a scare story about how the warm winter is causing a drought. 



Even the most rudimentary statistical analysis of winter temperatures reveals that our coldness is determined by the number and severity of incursions of miserable, deadly, frigid air, minted in northwestern Canada, Alaska, and Siberia and shipped south. Winters in which there is little of this activity, such as the totally delightful ones of 1931–32 (87 degrees Fahrenheit in Roanoke, Virginia, on February 11, 1932) or 1999–2000, tend to record above‐​average temperatures. 



Federal climatologists could have looked it up in the January 24 issue of _Climate Research,_ which proved that the greenhouse warming in the last half of the 20th century is largely confined to those same cold air masses. That is predicted by greenhouse effect theory but rarely mentioned in the rush to gloom. In the Northern Hemisphere winter half‐​year, which itself contains over two‐​thirds of the surface temperature warming, these frigid‐​air killers of the homeless are warming at a rate 10 times greater than the average warming everywhere else! The paper proved the greenhouse effect was the finger on this trigger by showing that the more cold air there is, the more it warms up. 



Further proof, ironically, is given by satellite and weather balloon data that show no warming (after allowing for the now‐​departed 1998 El Niño) since their records became concurrent in 1979. These instruments are best at measuring the atmospheric slice from roughly 5,000 to 30,000 feet, but the cold Siberian and North American air is usually more shallow than that. 



Because people saving the planet do not brook embarrassment with decorum, please don’t ask them whether this disparity between the surface and lower‐​atmospheric temperature was forecast by the climate models cited as evidence for a global‐​warming disaster. 



Droughts are caused by deficits in water balance as evaporation exceeds precipitation. In the winter, over most of the nation, temperatures and the sun’s angle are so low that there’s very little evaporation, which is why, even in a dry January, most of the soils in the eastern half of the nation are saturated. The recent warm winter was only three degrees above the long‐​term average, or the average temperature difference between, say, March 5 and March 15. Evaporation rates remained low because the mean temperature is still way below the summer peak. So warm winter or not, winter temperatures do not cause droughts. 



Federal behavior is not hallmarked with consistency. If global warming is caused by burning of fossil fuels (it probably is), and if it is a terrible problem (I’ll bet not), then the only way it can be stopped is by raising energy prices through the roof. How high? Today’s $2.00 per gallon gasoline hasn’t dented consumption enough to cool the mean temperature of the planet 1/1,000 of a degree, even if spread over a year. In other words, a two‐​term Gore administration at that price would produce a change in temperature of less than 1/100 of a degree. But with the softballs flying by my window on this fine March day, who on earth would want to do such a thing anyway?
"
"
Share this...FacebookTwitterThe Climate-Berlin Wall surrounding Germany has been cracking for some time now, and crumbling with increasing speed. Die Zeit’s online smear The Abetters of Doubt takes aim at the skeptics, who have scaled the Wall and are exercising their right to free expression. They are hammering away at climate science dogmatism in Germany, much to the chagrin of the warministas. 
The warministas are horrified and have embarked on a campaign to intimidate and marginalize the freedom of expression that skeptics enjoy in Germany. Climate skeptics who speak up today do so while always having to look over their shoulder, knowing they could be hit by a vicious smear and attack campaign. This is hardly the environment for a free and open society. Yet, it confirms that warminista science is trapped in the corner.
The days of Germany’s once exemplary model of a free and open society would of course risk unraveling if the warministas got their way. Indeed skeptics have long since been denied a voice in Germany’s publicly funded media, and so as a result have moved to the Internet, where they’ve chipped away.
Like it or not, this discussion is going to take place. Live with it.
Greenshirts resorting to brown tactics
What does an arrogant class, so totally convinced of itself, do when exhausted of argument and finds itself badly losing the intellectual debate and, along with it, its dream of The Green Reich?
It does what Die Zeit newspaper has done in its latest piece called The Abetters of Doubt; it resorts to brown tactics. The latest from Die Zeit is a 4-piece attack and smear campaign, with the objective of intimidating, marginalizing and silencing climate skeptics. It’s the same we have recently seen from other major dailies like Der Spiegel and the Handelsbaltt, with the usual names popping up: Stefan Rahmstorf and attack canine Naomi Oreskes. Lacking journalistic talent, Die Zeit has stooped to rehashing old stories.
Not only does the piece smear skeptics and advocate they be denied a voice, it attempts to morally degrade them as well. Ironically, it is becoming obvious who is actually morally degraded. Being a dissident here behind the Climate Berlin Wall and watching these smear tactics, I’m beginning to have an idea of what it must have been like to be gay during Medieval times, or Jewish before Kristallnacht. Die Zeit’s message to the skeptics in Germany is clear: “Be worried – be very worried”.
Fortunately, it’s nothing more than a last desperate threat coming from an intellectually bankrupt media outlet and a few activist scientists hiding in the background.
Growing skepticism, and desperation
Die Zeit’s piece is eerily laced with a strange combination of fear, anger and desperation, and it makes clear that the warministas are fed up with the turn climate science has taken. For them, the science was settled years ago. Damn the skeptics, bloggers, Internet and Fred Singer. Damn EIKE and the few German politicians who are beginning to listen up. They have gone too far. Die Zeit frets that public opinion in Germany is waning and that it’s time to put an end to it. In its piece, Die Zeit puts the spotlight on the bloggers:
Last year’s failure in Copenhagen and the hacked e-mails from climate scientists, which supposedly proved falsification, have been making waves through the media. The deniers and skeptics of global warming have been gaining momentum. They are omnipresent, mainly in the Internet – and appear to strike a chord with people who are fed up with all the climate talk, or with people who don’t want to change their lives because of a warming planet.”
Damn that denier European Institute for Climate and Energy (EIKE)
Die Zeit then singles out Horst-Joachim Lüdecke, a retired professor who is now the Press Speaker of the European Institute for Climate and Energy (EIKE). Die Zeit haughtily implies that Lüdecke is imposturing as a climate scientist:
Professor emeritus for Physics and Computer Sciences, but to his audience he introduces himself as a ‘climate scientist.'”
Indeed Prof. Lüdecke has been busy spreading the skeptic message, and has been effective. Skepticism in Germany has almost doubled over the recent months – now 1/3 no longer believe CO2 is a problem. This has infuriated and alarmed the warministas. Lüdecke recently gave a presentation to the Nordoberpfalz business group. Here’s how Die Zeit describes it:
The audience was made up of company owners, the mayor, local dignitaries and decorated lieutenant colonels. Hardly anyone noticed that Lüdecke was citing outdated reports, asserting uncertainties that no longer exist and suppressed facts that were inconvenient.”
Not only is Lüdecke imposturing as a climate scientist, but he is also using phony data, Die Zeit wants its readers to believe.
Damn those bloggers and the Internet (again)
The warministas by far view the Internet and bloggers as their biggest problem. This ought not be a surprise, as skeptics have long since been denied their right to be given a voice by Germany’s massive public radio and television media, where they are viewed as unworthy of a public platform. Call it media-gatekeeping. So, naturally, skeptics use the resources that are available, i.e. the Internet, and the little money and time they have at their disposal.
On the topic of bloggers, Die Zeit interviews no. 1  crybaby Stefan Rahmstorf of the Potsdam Institute for Climate Impact Research(PIK), introducing him as: “one of the world’s leading oceanographers”. Die Zeit writes:
In Potsdam, at the Einstein-Science-Park on the Telegrafenberg, climate scientist Stefan Rahmstorf sits in front of his computer and moans. ‘In the Internet, the climate skeptics are by far the dominant force,’ says Rahmstorf. There, an amateur can hardly do any research. ‘There have always been skeptics since he’s been doing research, ‘but last year they have broken into the serious media.'”
It just really stinks when the opposition has a voice. Die Zeit continues:
Together with his colleagues, he [Rahmstorf] counters skeptic claims and erroneous media reports at the Internet blog KlimaLounge, where some think he is overly fervent. Rahmstorf says that this is no fun, but sees no alternative. No matter where he goes, in government offices, in politics, top management levels of business – everywhere you hear skeptic arguments.”
I find it astonishing that a scientist would spend his work time preoccupied with PR work and spin. I thought they are supposed to be doing science, and not PR damage control. Clearly Rahmstorf spends much of his time writing stories for the NYT Times, Die Zeit, Der Spiegel and blogging, and not on the work he is paid to do.
A big part of Rahmstorf’s problem is that he’s turned a lot of people off with his smear tactics, getting on the wrong side of a huge number of scientists. He’s got no shortage of enemies.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Damn Big Oil and industry
In the Die Zeit piece, out comes the old worn out tactic of linking skeptics to Big Oil oil or to the coal industry, which  only further confirms the bankrupt state of the global warming narrative.
Also the top managers of power giant RWE took part in a conference by mining engineers who presented ‘solidly established doubt’ on man-made climate change. Foremost, the coal lobby is spreading doubt about global warming in the 2009 annual report of the coal association one reads that the brakes have been put on climate change.
Employees of E.on, Bayer and BASF in the USA this year contributed at least 70,000 dollars to skeptic politicians.”
If Rahmstorf and the warministas had their way, private contributions to the organizations he disagrees with would be banned. Sounds undemocratic to me. Let’s not talk about the BILLIONS that flow into warmist coffers.
Falsehoods about Climategate and the IPCC are being spread
Die Zeit refuses to acknowledge any scandals in climate science, particularly Climategate and the IPCC 4AR, and twists itself into a pretzel defending the rogue institutes behind them. Not once did Die Zeit publish the damning e-mails, keeping them hidden and locked away from the public instead. Die Zeit claims that many of the ‘scandals’ have already been debunked, and even goes on to defend Mann’s crooked hockey stick. Die Zeit complains:
Even so, these supposed scandals have made their way through the Internet blogs thousands of times.”
Damn Internet. Of course, the entire climate science community knows that these scandals have not been debunked. Here’s a list of 94 scandals that have yet to be resolved. An updated and much expanded list is coming out soon. As far as the IAC is concerned, it Admits Well Is Poisoned, Yet Insists Water Is Safe.
Damn Fred Singer and the Heartland Institute 
Part 3 of Die Zeit’s piece focuses on Fred Singer, the Heartland Institute, tobacco in the 1960s and Oreskes’s Merchants of Doubt. This is old and is just a lazy rehash of what appeared already weeks ago at Der Spiegel here. Die Zeit goes on and marginalizes scientific debate:
As usual the debate took place for years in the scientific journals, and the uncertainty is pretty much cleared up. This is now just constant back and forth that has since taken place in the lurid light of the public,’ says Hartmut Grassl, the 71-year old doyen of German climate science. Skeptics cherry-pick uncertainties in such debates. But all this has nothing to do with skepticism, nothing to do with critique and testing.'”
In Die Zeit’s simple world, it all goes back to Fred Singer. And CO2 drives the climate, of course.
Damn EIKE and their climate conferences
Spreading skepticism in Germany, as it was done in USA, has been successful, thanks to Holger Thuss, spokesman and founder of EIKE. EIKE is funded exclusively by private donations and has less than a hundred members. This year in December it will hold its 3rd Climate Conference in Berlin, and this time they have just enough money to have the food catered instead of buying it at a supermarket like they had to do last year. Die Zeit:
The conference next week at the Maritim Hotel in Berlin shows that EIKE, despite its dubious science, has been successful in building a network. The list of speakers includes the former President of the German Steel Industry Association. One co-sponsor of the conference is the market-radical Berlin Manhattan Institute for Entrepreneurial Freedom, which has only a one-man office, but has a board filled with economic professors who convey an air of seriousness.”
Die Zeit also has jumped in on the mob-political-lynching of Marie-Luise Dött, German Parliamentarian and a central figure on Angela Merkel’s environmental committee, whose crime was to express skepticism on climate change. Read here.
Hans von Storch chimes in 
Die Zeit ends its piece quoting Hans von Storch, who assumes his comfortable perch on the fence.
“I’ve taken a look at such skeptic conferences twice.  The level for the most part was catastrophic’. Many go there to simply spread pre-packaged opinions. ‘A real interest in a discussion could not be detected.”
Detection is indeed a big problem in climate science. Some things that hardly exist, get overly detected, while other things staring right at you in the face are ignored. There’s a lot of confusion in climate science.
Skeptics’ reaction
Finally I asked EIKE for their thoughts on the Die Zeit piece, to which they kindly answered. In a nutshell, they didn’t seem the least bothered by the Die Zeit report, taking it in stride and actually welcoming it. A spokesman wrote it will bring much more attention to the discussion and generate even more interest in the Climate Conference in Berlin. Then he added, quoting Gandhi:
First they ignore you.
Then they ridicule you.
Then they attack you.
Then you win.
We are now at stage III.”
Like it or not, this discussion is going to take place. SO LIVE WITH IT.
Share this...FacebookTwitter "
"


A guest post by: Russ Steele from NCWatch

We can only hope the most people in the US are shopping on Black Friday and not watching the Oprah Winfrey Show today.  Al Gore has brought his global warming propaganda machine to share with Oprah.  You can find the details on Oprah’s web page.  Here are some of the topics that Gore is pushing:
Classic Gore:
“Some of the leading scientists are now saying we may have as little as 10 years before we cross a kind of point-of-no-return, beyond which it’s much more difficult to save the habitability of the planet in the future,” Gore says.
Yes, but Al you have been saying that for over ten years and we are still here. And in the last ten years the global temperatures stopped rising and are now in decline.

Click for a larger image
Really Al, show me where the temperatures are beyond natural fluctuations:
Gore agrees that the planet’s temperature has indeed experienced up and down cycles, but he says the current up cycle is too extreme. “It’s way off the charts compared to what those natural fluctuations are,” he says.
Here is look at long term temperatures
 
One word of caution, these are USHCN numbers, which [have been] adjusted. Past temperatures are going down and the more recent going up.
Going, going Gored:
No place is immune to global warming, Gore says. “Of the thousand largest glaciers on every continent, 997 of them are receding,” he says. “And it’s not seasonal.”

Glaciers have been retreating long before CO2 was problem. (Graphic from Climate Skeptic) Now we learn that the glaciers have stopped retreating and are expanding:
After years of decline, glaciers in Norway are again growing, reports the Norwegian Water Resources and Energy Directorate, as reported in Daily Tech.
DailyTech has previously reported on the growth in Alaskan glaciers, reversing a 250-year trend of loss. Some glaciers in Canada, California, and New Zealand are also growing, as the result of both colder temperatures and increased snowfall.
Al needs to take a second look at the North Pole:
“The North Pole is melting.”

Here is comparison of the ice in November 1980 and 2008. Do you see some major differences, like the “North Pole is melting.” (Note: Earlier photos do not show snow coverage) Details at Cryosphere  Today
Katrina again:
“Temperature increases are taking place all over the world, including in the oceans. Gore warns that when the oceans get warmer, storms get stronger. In August 2005, millions of Americans were left homeless by Hurricane Katrina, one of the most powerful hurricanes in recent history. Gore says people should expect more Category 4 and 5 hurricanes if the ocean waters continue to warm.”
 
Looks like a decline in cyclone energy to me, not an increase.
Please let Oprah know that you expected more from someone of her intelligence and veracity here.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9aa0e681',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwittertutorial main bandarqq Online supaya Menang tidak sedikit , terhadap peluang yg indah ini kami dapat sedikit mengulas info yg berhubungan bersama perjudian online, utk artikel kali ini kita dapat mengupas tuntas seperti apa meraih kemenangan dalam main judi card domino88, nah sebelum masuk ke pembahasan ada sekian banyak perihal yg butuh kamu ketahui terkait dgn permainan judi satu ini. mula-mula merupakan buat menyangkut berapa jumlah card yg dimanfaatkan dalam permainan ini, permainan card domino sendiri ialah salah satu tipe permainan card yg telah lama ada, terkait dgn jumlah card yg dimanfaatkan, permainan ini memakai jumlahnya 52 kombinasi card bersama bulatan 1 pasang & mempunyai nilai yg berlainan.
selanjutnya buat system permainannya sendiri lumayan gampang & simple buat dipelajari, masing-masing buat satu meja mampu terdiri dari 6-8 orang pemain, buat tiap-tiap pemain dapat mendapati pembagian 4 card, pembagian card perdana dilakukan sejalan jarum jam, sebelum menuju pembagian card ke-2 rata-rata pemain bakal disuruh utk pilih call, raise, fold, check maupun all in. seterusnya baru card ke-2 bakal dibagian, nah disini system permainannya yaitu member yg mendapati card dgn nilai yg paling gede sehingga dialah yg dapat jadi pemenangnya, lumayan gampang bukan ? pasti saja pass gampang bila dimainkan oleh para pemain profesional, sedangkan utk kamu sendiri masihlah nampak susah, berikut ini merupakan pembahasan trik menang bemain judi card domino 99 online paling enteng buat pemula. utk meraih kemenangan main judi QQ online, sehingga aspek perdana yg mesti kamu melakukan merupakan percaya bersama insting yg kamu punyai, seluruhnya pemain judi tentunya mempunyai insting buat kemenangan. seandainya kamu telah pass percaya, sehingga kemenangan telah dekat ada didepan mata kamu. main-main judi QQ domino 99 bukan lagi semata-mata cuma bersama memanfaatkan unsur tebakan saja, melainkan ada unsur di mana seluruhnya pemain mesti memang mendalami & percaya dgn ketentuan mereka. buat dikarenakan itu kalau kamu mau menang main-main judi, pahami bersama baik seluruh permainan & pastikan kamu percaya bersama insting yg kamu miliki.
selanjutnya yg berikutnya seandainya mau menang main-main judi domino sehingga kamu mesti memang lah sanggup mendalami bersama baik permainan ini, apalagi buat kombinasi card sendiri. Ada tidak sedikit pemain yg senantiasa terkecoh bersama kombinasi card gede utk perdana kali muncul contohnya saja 9 + 6, & terhadap hasilnya mesti kalah. dgn kemunculan angka 9 + 0. system permainan ini memang lah sedikit mengecoh para pemainnnya, menjadi jangan sampai sempat gemar dulu diwaktu kamu memperoleh kombinasi angka 9 ataupun angka istimewa yang lain. butuh kamu pahami dgn baik bahwa judi pun mempunyai segi kelemahan adalah tiap-tiap kemunculan susah utk kamu tebak. setelah itu jikalau kamu mau menang dalam main-main judi domino 99 sehingga kamu mesti mampu main-main dengan cara sehat, tujuan dari main-main dengan cara sehat ini ialah main-main dgn pikiran yg kalem. Ada tidak sedikit pemain yg kadang main-main bersama trick yg serakah atau nafsu tinggi buat mendapati kemenangan dalam disaat yg langsung, padahal sanggup dikatakan apabila main dgn nafsu yg tinggi cuma bakal mengambil kekalahan dalam ketika yg segera. buat itu konsisten konsentrasi & main dgn trick yg baik.
silakan bergabung dgn kami & temukan kemenangan yg pantas seperti sama seperti pemain judi sejati.
Share this...FacebookTwitter "
"

Mr. President, justices of the Constitutional Court, distinguished guests, ladies and gentlemen, good morning.



I’m honored to be here today to speak as you celebrate the 70th anniversary of Italy’s Constitution. In a world of political and constitutional uncertainties, it’s altogether fitting that you should be marking this milestone in Italy’s history, and I wish you many more such celebrations in the years to come.



Before I begin, let me thank President Giorgio Benvenuto, our event coordinator Professor Luigi Troiani, and their colleagues at the Pietro Nenni Foundation for bringing this occasion about and for inviting me to speak before you. When they visited me in Washington in October, we talked about work I’d done over the years on American constitutional theory and history. And we talked in particular about the Cato Institute’s annual Constitution Day Symposium, which I started 16 years ago to mark the day in 1787 when the American Constitution was completed and sent out to the states for ratification. Since that Constitution has endured now for more than 230 years, Senator Benvenuto and Professor Troiani thought it might be useful to share some of that experience with you, especially as it might bear on European constitutionalism. And so we settled on the topic I’ll discuss this morning, “American Constitutional Theory and History: Implications for European Constitutionalism.”



That’s a large subject, of course, with many moving parts drawn from several disciplines. And the inevitable ambiguities of translation complicate understanding even further. So rather than dive straight into my argument, I’ll first say a word about my subtitle and then outline the argument so you’ll know where I’m going.



By European constitutionalism I’m alluding not to the national constitutions of the European nations, about which I know little, but to the European Union Treaties that serve as a “Constitutional Charter” for the European Communities. As an American constitutionalist looking east and seeing everything from Brexit to Grexit, plus events in European capitals, I’m struck by the tension in the EU between exclusion and inclusion in its many forms, including individualism and collectivism. Those themes underpin my talk today. The issues surrounding them are universal. They’re at the heart of the human condition.



In America we wrestled with them at our founding over 200 years ago, again in the aftermath of our Civil War, and yet again with the advent of Progressivism, which culminated in our New Deal constitutional revolution. And we’re still wrestling with them. Because America was founded on philosophical principles — First Principles, coming from the Enlightenment — it’s particularly appropriate that we look at that experience to shed such light as we can on this more recent European constitutional experience.



But my more immediate concern is this: In liberal democracies today — nations constituted in the classical liberal tradition — we see the same basic problem, albeit with significant variations. It’s that the growth of government, responding mainly to popular demand, has raised seemingly intractable moral and practical problems. First, increasing intrusions on individual liberty; and second, the unwillingness of people to pay for all the public goods and services they’re demanding. So governments borrow. And that’s led to massive public debt that saddles our children and grandchildren, to bankruptcy, and to the failure of governments to keep the commitments they’ve made.



In Italy, we need only look east, to the birthplace of democracy. But Greece isn’t alone in this. Nor are we in America immune. Cities like Detroit have gone bankrupt. So too, just recently, has the American territory of Puerto Rico. The state of Illinois has a credit rating today just above junk status, and Connecticut and New Jersey, among other states, aren’t far behind. At the national level, America’s debt today exceeds $20 trillion — that’s trillion — more than double what it was only a decade ago. And our unfunded liability vastly exceeds that.



What has this got to do with constitutionalism? A great deal. Constitutions are written, after all, to discipline not only the governments they authorize but the people themselves. The point was famously stated by James Madison, the principal author of the U.S. Constitution. “In framing a government which is to be administered by men over men, the great difficulty lies in this,” he wrote: “you must first enable the government to control the governed; and in the next place oblige it to control itself. A dependence on the people is, no doubt, the primary control on the government,” Madison concluded, “but experience has taught mankind the necessity of auxiliary precautions.“1



The principal such precaution, of course, is a well‐​written constitution. But no constitution is self‐​executing. It’s people who ultimately execute constitutions. In the end, therefore, the issue is cultural, a point I’ll come back to.



America’s Founders were deeply concerned with the problem of undisciplined, unlimited government. After all, they’d just fought a war to rid themselves of distant, overbearing government. In drafting the Constitution, therefore, they weren’t about to impose that kind of government on themselves. In fact, during the ratification debates in the states, there were two main camps — the Anti‐​Federalists, who thought the proposed Constitution gave the government too much power, and the Federalists, who responded by pointing to the many ways the proposed Constitution would guard against that risk. The Federalists eventually won, of course, but the point I want to secure is that there wasn’t a socialist in the group! There were _limited_ government people, the Federalists, and _even more_ limited government people, the Anti‐​federalists.



So under a Constitution that hasn’t changed all that much, how did we go from limited to effectively unlimited government? The answer lies in the fundamental shift in the climate of ideas that began with Progressivism at the end of the 19th century, which the New Deal Supreme Court institutionalized in the 1930s. To illustrate that, I’ll first look closely at America’s founding documents: the Declaration of Independence, signed in 1776; the Constitution, ratified in 1788; the Bill of Rights, ratified in 1791; and the Civil War Amendments, ratified between 1865 and 1870, which corrected flaws in the original Constitution. Together, those documents constitute a legal framework for individual liberty under limited government, however inconsistent with those principles our actual history may have been.



Then I’ll show how progressives rejected the libertarian and limited government principles of America’s Founders and how they eventually turned the Constitution on its head, not by amending it but through political pressure brought to bear on the Supreme Court. The problems that have ensued include the ones just noted: less liberty, increasing debt. But perhaps of even greater importance, for eight decades now the Supreme Court has struggled to square its post‐​New Deal decisions with the text and theory of the Constitution. That amounts to nothing less than a crisis of constitutional legitimacy.



And again, the basic reason for that crisis is the fundamental shift in outlook. Many Americans today no longer think of government as earlier generations did. Whereas the Founders saw government as a “necessary evil,” to be restrained at every turn, many today think that the purpose of government is to provide them with vast goods and services, as decided by democratic majorities.



 **The Importance of Theory**



I come, then, to the first important point I want to flag. You cannot understand the U.S. Constitution unless you understand the moral and political theory that stands behind it. And that was outlined not in the Constitution but in the Declaration of Independence.2 The Constitution was written in a context, as were the later Civil War Amendments, and that context was one of natural law, Anglo‐​American common law, and even elements of Roman Law, all of which is captured succinctly in those famous words of the Declaration that I’ll quote in a moment. Indeed, President Abraham Lincoln’s famous Gettysburg Address, written in the throes of a brutal Civil War, begins with these words: “Fourscore and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.” Lincoln was reaching back to the Declaration, not to the Constitution.



Yet no less than my good friend and Italy’s gift to American constitutionalism, the late Justice Antonin Scalia, all but dismissed the Declaration as “philosophizing,” contrasting it with the Constitution’s “operative provisions.“3 And his conservative colleague when the two served on the nation’s second highest court, the late Judge Robert Bork, wrote that “the ringing phrases [of the Declaration] are hardly useful, indeed may be pernicious, if taken, as they commonly are, as a guide to action, governmental or private.“4 Is it any wonder that there’s constitutional confusion in America today when the document that’s essential to understanding it plays little or no part in that understanding?



With that outline now before us, let me flesh out the argument. I’ll do that by focusing on the underlying moral, political, and legal principles at stake, after which I’ll offer just a few reflections on how they might illuminate issues in the European context. Again, I want to show how the shift from limited to effectively unlimited government took place in America, despite very few constitutional changes. I should note, however, that it will be some time before I get to the Constitution. If a proper understanding of the Constitution requires a proper understanding of the theory behind it, and if that theory is found implicitly in the Declaration, then that should be our initial focus, and will be for some time.



That will take us into some of the deeper reaches of moral and political theory, so please bear with me. My hope is that at the end you’ll better understand the Constitution itself — and especially the broad principles that underpin it.



So let’s get started. The first thing to notice about the American constitutional experience is how relatively different its beginnings were from those of many other nations. Constitution making and remaking in many nations has taken place in the context of an often stormy history stretching back centuries, even millennia. By contrast, America was a _new_ nation. We came into being at a precise point in time, with the signing of the Declaration of Independence. True, American patriots had to win our independence on the battlefield. And before that we had a colonial history of roughly 150 years. But America was created not by a discrete people but by diverse immigrants with unique histories all their own.



A second, crucial feature distinguishing America’s constitutional experience is that it unfolded during the intense intellectual ferment of the Enlightenment, including the Scottish Enlightenment, with its focus on the individual, individual liberty, and political legitimacy, all of which reflected the sense of “a new beginning.” Indeed, the motto on the Great Seal of the United States captures well the spirit of America’s origins: _Novus ordo seclorum, “a_ new order of the ages.”



 **The Declaration of Independence**



Let’s turn, then, to that new order, as outlined in the Declaration. Penned near the start of our struggle for independence, the Declaration in form is a _political_ document. But were it merely that, it would not have so endured in our national consciousness. Nor would it have inspired countless millions around the world ever since, leading many to leave their homelands to begin life anew under its promise, including millions from Italy who now enrich America. It has so inspired because, fundamentally, it is a profound _moral_ statement. Offered from “a decent Respect to the Opinions of Mankind” and invoking “the Laws of Nature and of Nature’s God,” it was written not only to declare but to _justify_ our independence. And it did so not simply by listing the king’s “long Train of Abuses and Usurpations,” which constitute the greater part of the document, but by first setting forth the moral and political vision that rendered those acts unjust.



And so we come to those famous words that flowed from Thomas Jefferson’s pen in 1776, words that capture fundamental principles concerning the human condition:



We hold these Truths to be self‐​evident, that all Men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty, and the Pursuit of Happiness — That to secure these Rights, Governments are instituted among Men, deriving their just Powers from the Consent of the Governed.



The first thing to notice about that justly famous passage is that its propositions are asserted as “truths,” not mere opinions. The Founders were not moral relativists. They were confident in their claims. And why not? Their truths were said to be “self‐​evident,” grounded in universal reason, accessible by all mankind — and the evidence supports that.



Notice too the structure of the passage: There are two parts — and the order is crucial. The moral vision comes first, defined by equal rights. The political and legal vision comes second, defined by powers, as derived from the moral vision. And right there is the second major point I want to flag: Unlike today, where politics, grounded in will, so often determines what rights we have, for early Americans, morality, grounded in reason, determined our rights. The Founders were concerned fundamentally with moral and political _legitimacy_. Rights first, government second, as the means for securing our rights.5



Given that order of things, the Founders were engaged in what’s called “state‐​of‐​nature theory,” a rudimentary form of which can be found in the writings of your own Seneca,6 a fuller discussion much later in those of Thomas Hobbes7 and, especially, John Locke8 — often said to be the philosophical father of America.



State‐​of‐​nature theory is a thought experiment. The idea is to show how, without violating any rights, a legitimate government with legitimate powers might arise from a world with no government. Thus, the first step is to show, from pure reason, what rights we’d have in such a world.



For that, as the Declaration implies, we turn to the natural law tradition — more precisely, the natural rights strain coming from the Reformation and the Enlightenment. Simply put, natural law stands for the idea that there’s a “higher law” of right and wrong, grounded in reason, from which to derive the positive law, and against which to criticize that law at any point in time. There’s nothing suspect about that idea, as modern moral skeptics argue. We appeal to natural law when the positive or actual law is morally wrong. In America, the abolitionists, the suffragists, and the civil rights marchers all invoked our natural rights in their struggles to overturn unjust law.



The origins of this law are in antiquity. Many of its particulars are found in Roman Law, especially the law of property and contract. Over some 500 years in England, prior to the American Revolution, this law was refined and reduced to positive law by common‐​law judges consulting reason, custom, and what they knew of Roman Law as they adjudicated cases brought before them by ordinary individuals.9 And John Locke drew largely on that body of common‐​law rights as he crafted a theory of natural rights, much as Jefferson drew on Locke when he drafted the Declaration.



To correct a common misunderstanding, these are the rights we hold _against each other_ , and would hold in a state of nature. Later, once we create a government, they’ll serve as rights we hold against that government, and likely be included in a bill of rights.



To discover and justify these rights in detail, as I and others have done,10 we’d need to delve into the complex issues of moral epistemology and legal casuistry, and this isn’t the occasion for that. Suffice it to say that when that foundational work is finished, the conclusion one reaches is the same one America’s Founders reached intuitively, that our basic right is the right to be free from the unjustified interference of others, and all other rights are derived from that basic right, as the facts may warrant. What results approximates largely the judge‐​made common law of property, torts, contracts, and remedies, a law that defines our private relationships, as it did in early America, both before and long after the Revolution. It’s a law that says, in essence, that each of us is free to pursue happiness, by his own subjective values, either alone or in association with others, provided we respect the equal objective rights of others to do the same. In short, it’s a live‐​and‐​let‐​live law of liberty.



And I can summarize it with three simple rules, so simple that even a child can understand them.



Rule 1: Don’t take what belongs to someone else. That’s the whole world of property, broadly conceived as Locke did — our property in our “Lives, Liberties, and Estates.”



Rule 2: Keep your promises. That’s the whole world of contracts and associations.



Rule 3. If you’ve wrongly violated rules 1 or 2, give back what you’ve wrongly taken or wrongly withheld. That’s the whole world of remedies.



There’s a fourth rule, but it’s optional: Do some good. You’re free not to be a Good Samaritan, but you should be one if you’re a decent human being and the cost to you is modest. Unlike much continental law, Anglo‐​American law never compelled strangers to come to the aid of others.11 It didn’t because individual liberty is its main object. And it saw that there’s no virtue in forced beneficence. We’re free to criticize those who don’t come to the aid of others, and we should, even as we defend their right not to.



Why have I mentioned this fourth, voluntary rule? Again, it’s because, when we start from a theoretical state of nature, we need to know what rights we do and do not have for government to enforce once we bring it into the picture. And the Good Samaritan is the modern welfare state writ small. If there’s no right to be rescued, there’s no correlative obligation for government to enforce. Recognizing that raises important questions about the very legitimacy of the welfare state.12



 **Leaving the State of Nature and the Problem of Political Legitimacy**



To get to the Constitution, however, we need now to take the last step in the argument. We need to derive a legitimate government with legitimate powers — and that’s not easy. I’ve said little about enforcement so far. The Declaration says that government’s purpose is to secure our rights, its _just_ powers derived “from the consent of the governed.” Thus, the Founders invoked the social contract, which grounds political legitimacy in consent.



But there are well‐​known problems with consent‐​based social‐​contract theory as a ground for political legitimacy. The question is how to move legitimately from self‐​rule to collective rule. Unanimity will achieve legitimacy, of course, but rarely if ever do we get it. Majoritarianism won’t solve the problem, because it amounts to tyranny over the minority that hasn’t consented. Nor will the social contract work, except for those in the original position who agree thereafter to be bound by the will of the majority. Nor, finally, will so‐​called tacit consent work — “you stayed, therefore you’re bound by the majority” — because it puts the minority to a choice between two of its rights, its right to stay where it is, and its right not to be ruled by the majority, precisely what the majority must justify on pain of circularity. As for elections, an occasional vote hardly justifies all that follows.



As a _practical_ matter, the social contract argument may be the best we can do, but recognizing its infirmities leads to a compelling conclusion — and to the third basic point I want to flag, namely, that there is an air of illegitimacy that surrounds government as such. Government is not like a private association that we join or leave at will. It’s a _forced_ association. Its very definition entails force. And once we recognize its essential character, that should compel us, _from a concern for legitimacy_ , to do as much as we can through the private sector where it can be done voluntarily and hence in violation of the rights of no one, and as little as possible through the public sector where individuals will be forced into programs they may want no part of.



In short, as a _moral_ matter, there’s a strong presumption against doing things through government. We should turn to government not as a first but only as a _last_ resort, when all else fails.



Still, we can refine this conclusion. We can distinguish three distinct powers in decreasing degrees of legitimacy. The first is the police power — the power, through adjudication or legislation, to more precisely define and enforce our rights. As such, it’s bound by the rights we have to be enforced, although it includes the power to provide limited “public goods” like national defense, clean air, and certain infrastructure, goods described by non‐​excludability and non‐​rivalrous consumption, as economists define them.13



When we leave the state of nature, we give government that power to exercise on our behalf. But because we had the power in the state of nature — Locke called it the “Executive Power” each of us has to secure his rights — to that extent it’s legitimate. Only the anarchist who would prefer to remain in the state of nature can be heard to complain. Fortunately, there are few of those.



Less legitimate is the eminent domain power — the power to condemn and take private property for public use after paying the owner just compensation — because none of us would have such a power in the state of nature. Such legitimacy as this power enjoys, at least in America, is because we gave it to government when we ratified the Constitution’s Fifth Amendment, which includes the Takings Clause; and it’s “Pareto optimal,” as economists say, meaning that at least one person is made better off by it — the public, as shown by its willingness to pay — and no one is made worse off — the owner, provided he’s indifferent as to whether he keeps the property or gets the compensation.



The third great governmental power, ubiquitous today, is the least legitimate. In fact, from a natural rights perspective, it enjoys no legitimacy. It’s the redistributive power, and it takes two forms, material and regulatory. Through redistributive taxation, government takes from _A_ and gives to _B_. Through redistributive regulation, government prohibits _A_ from doing what he would otherwise have a right to do or requires him to do what he would otherwise have a right not to do, all for the benefit of _B_. Those powers describe the modern redistributive and regulatory state. No one would have them in the state of nature. How then could government get them legitimately, since governments, in the classical liberal tradition, get whatever powers they have from the people, who must first _have_ those powers to yield up to government?



There are three main answers. First, if that redistribution arose through unanimous consent, there would be no problem, but again, rarely if ever does that occur in the public sector. Second, majorities gave governments those powers. That raises the classic problem of the tyranny of the majority. And third, special interests have learned how to work the system for their benefit, as public choice economists have long explained.14 That’s the tyranny of the minority — and the main source today of such schemes.



We can conclude this examination of the moral foundations of the classical liberal vision by imagining a continuum, with anarchy or no government at one end — our state of nature — and totalitarianism at the other end, where everything possible is done through government. At the anarchy end, individuals are free to plan and live their lives as they wish, alone or in cooperation with others. They will soon find, however, that there are some things best done collectively, like the provision and enforcement of law, national defense, clean air and water, limited infrastructure, and the like — public goods — and most will consent to the public provision of such goods. But as we move up the continuum toward totalitarianism and try to bring more and more _private_ goods under _public_ provision — education, health care, child care, jobs, housing, ordinary goods and services — people start voting with their feet. The Berlin Wall was not built to keep West German workers out of the workers’ paradise to the east.



The moral, political, and legal vision implicit in the Declaration of Independence is closer to the anarchy end of that continuum. America’s Founders envisioned a land in which people were free to live as they wished, respecting the equal rights of others to do the same, with government there to secure those rights and do the few other things it was authorized to do.



That basic moral vision is perfectly universalizable. How to secure it through the rule of law is another matter. Certain basic legal principles are themselves universalizable and are common to most legal systems, but whether a nation has a parliamentary system as in much of Europe, or a republican form of government as in America, or some other arrangement is not a matter of natural law. Let’s now see how the Founders framed a constitution to secure the Declaration’s moral vision.



 **The Constitution**



After we declared independence, and during our struggle for it, we lived under our first constitution, the Articles of Confederation. As its name implies, it was a loose agreement among the 13 states, authorizing a national government that hardly warranted the name. Three main problems lay ahead. Surrounded on three sides by great European powers, our national defense was painfully inadequate. Second, states were erecting tariffs and other barriers to free interstate trade. And finally, our war debts remained unpaid. After 11 years, the Framers met in Philadelphia to draft a new Constitution.



The main problem they faced was how to strike a balance. They needed to give the new government enough power to address those problems and accomplish its broad aims, yet not so much as to risk our liberties. Those aims were set forth in the Constitution’s Preamble:



We the People of the United States, in Order to form a more perfect Union, establish Justice, insure domestic Tranquility, provide for the common defence, promote the general Welfare, and secure the Blessings of Liberty to ourselves and our Posterity, do ordain and establish this Constitution for the United States of America.



Notice: States aside, regarding the proposed new government, we’re right back in the state of nature, about to “ordain and establish” a constitution to authorize it and bring it into being. All power rests _initially_ with “we the people.” _We_ bring the constitution and the government that follows into being through ratification. _We_ give it its powers, such as we do. The government does not give us our rights. We already have our rights, natural rights, the exercise of which creates and empowers this government.



So how does Madison strike the balance between power and liberty in service of those aims? First, through federalism: Power was _divided_ between the federal and state governments, with most power left with the states, especially the general police power, the basic power of government to secure our rights, as just discussed. The powers we delegated to the federal government concerned national issues like defense, free interstate commerce, rules for intellectual property, a national currency, and the like.



Second, following Montesquieu, Madison _separated_ powers among the three branches of the federal government, with each branch defined functionally. Pitting power against power, he provided for a bicameral legislature, with each chamber constituted differently; a unitary executive to enforce national legislation and conduct foreign affairs; and an independent judiciary with the implicit power to review legislative and executive actions for their constitutionality — a novel institution at that time, and a crucial one as time went on.



Third, although the Constitution left most of the rules for elections with the states, it provided for periodic elections to fill the offices set forth in the document, thus leaving ultimate power with the people.



But while each of those provisions and others struck a balance between power and liberty, the main restraint on overweening government took the name of the _doctrine of enumerated powers_. And I can state it no more simply than this: If you want to limit power, don’t give it in the first place. We see that doctrine in the very first sentence of the Constitution, after the Preamble: “All legislative Powers _herein granted_ shall be vested in a Congress .…” By implication, not all powers were “herein granted.” Look at Article I, section 8, and you’ll see that Congress has only 18 powers or ends that the people have authorized. And the last documentary evidence from the founding period, the Tenth Amendment, states that doctrine explicitly: “The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.” In other words, the Constitution creates a government of delegated, enumerated, and thus limited powers. If a power is not found in the document, it belongs to the states — or to the people, never having been given to either government.



As I noted earlier, when the Constitution was sent out to the states for ratification, it met stiff resistance as Anti‐​Federalists thought it gave too much power to the national government. Only after the Federalists agreed to add a bill of rights was it finally ratified. During the first Congress in 1789, Madison drafted 12 amendments, 10 of which were ratified in 1791 as the Bill of Rights. It sets forth rights that are good against the federal government, such as freedom of religion, speech, press, and assembly, the right to keep and bear arms, to be secure against unreasonable searches and seizures, to due process of law, to compensation if private property is taken for public use, to trial by jury, and more.



But it’s important to note that the Bill of Rights was, as Justice Scalia said, an “afterthought.“15 Unlike with many European constitutions, which begin with a long list of rights, many aspirational, the Framers saw the Constitution’s _structural_ provisions as their main protection against overweening government.16 And on that score, it’s crucial to mention the Ninth Amendment, which reads: “The enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people.”



The history behind that amendment is instructive. During the ratification debates, there were two main objections to adding a bill of rights. First, it would be unnecessary. “Why declare that things shall not be done,” asked Alexander Hamilton, “which there is no power to do?“17 Notice, he was alluding to the enumerated powers doctrine as the _main_ protection for our liberties: Where there is no power, there is a right.



But second, it would be impossible to list all of our rights, yet by ordinary principles of legal construction,18 the failure to do so would be construed as implying that only those rights that were listed were meant to be protected. So to guard against that, they wrote the Ninth Amendment, which reads, again, “The enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people.” Notice: “retained by the people.” You can’t retain what you don’t first have to be retained. They were alluding to our natural rights, which we retained when we left the state of nature, save for those we gave up to government to exercise on our behalf, like the police power.



For a proper understanding of the Constitution, the importance of the Ninth Amendment, which speaks of retained rights, and the Tenth Amendment, which speaks of delegated powers, cannot be overstated.19 Taken together, as the last documentary evidence from the founding period, they recapitulate the vision of the Declaration. We all have rights, enumerated and unenumerated alike, to pursue happiness by our own lights, to plan and live our lives as we wish, provided we respect the rights of others to do the same, and federal and state governments are there to secure those rights through the limited powers we’ve given them toward that end. There, in a nutshell, is the American vision, reduced from natural to positive law.



There was a problem, however. There were too few checks on the states, where most power was left. And the reason was slavery. To achieve unity among the states, the Framers made their Faustian bargain. They knew that slavery was inconsistent with their founding principles. They hoped it would wither away in time. It didn’t. It took a brutal civil war to end slavery, and the Civil War Amendments to complete the Constitution by incorporating at last the grand principles of the Declaration, especially equality before the law.



The Thirteenth Amendment ended slavery in 1865. The Fifteenth Amendment, ratified in 1870, protected the right to vote. And the Fourteenth Amendment, ratified in 1868, defined federal and state citizenship and, for the first time, provided _federal_ remedies against a state’s violating the rights of its own citizens.20



Unfortunately, only five years after the Fourteenth Amendment was ratified, a deeply divided 5–4 Supreme Court eviscerated the principal font of substantive rights under the amendment, the Privileges or Immunities Clause.21 Thereafter the Court would try to do under the less substantive Due Process Clause what was meant to be done under privileges or immunities, and the misreading of the Fourteenth Amendment has continued to this day. Among other things, the upshot was Jim Crow racial segregation in the South, which lasted until the middle of the 20th century.



 **Progressivism**



We turn now to the great ideological watershed, the rise of Progressivism at the end of the 19th century. Coming from the elite universities of the Northeast, progressives rejected the Founder’s libertarian and limited government vision.22 They were social engineers, planners enamored of the new social sciences. Insensitive when not hostile to the power of markets to order human affairs justly and efficiently, they sought to address what they saw as social problems through redistributive regulatory legislation. They looked to Europe for inspiration: Bismarck’s social security scheme, for example, and British utilitarianism, which in ethics had replaced natural rights theory. The idea was that policy, law, and judgment were to be justified not by whether they protected our natural and moral rights but whether they _gave_ us positive rights purporting to produce the greatest good for the greatest number.



A particularly egregious example of that rationale concerned a sweetheart suit brought against a Virginia statute that authorized the sterilization of people thought to be of insufficient intelligence.23 Part of the bogus “eugenics” movement, the law was designed to improve the human gene pool. Writing for a divided Supreme Court in 1927, the sainted Justice Oliver Wendell Holmes upheld the statute, ending his short opinion with the ringing words, “Three generations of imbeciles are enough.” There followed some 70,000 sterilizations across the nation.



Some of what the progressives did was long overdue, like promoting municipal health and safety measures and attacking corruption. Yet they also sowed the seeds for later corruption, especially through regulatory schemes ripe for special interest capture, replacing markets with cartels.24 And their record on racial matters was abysmal.25



During the early decades of the 20th century, progressives directed their political activism mostly at the state level, but they often failed as the courts upheld constitutional principles securing individual liberty and free markets. With the election of Franklin Roosevelt in 1932, however, progressive activism shifted to the federal level. Still, during the president’s first term the Supreme Court continued mostly to uphold limits on federal power, finding several of Roosevelt’s programs unconstitutional.



With the landslide election of 1936, however, things came to a head. Early in 1937, Roosevelt unveiled his infamous Court‐​packing scheme, his threat to pack the Court with six new members. Uproar followed. Not even an overwhelmingly Democratic Congress would go along with the plan. Nevertheless, the Court got the message. The famous “switch in time that saved nine” justices followed. The Court began rewriting the Constitution, in effect, not through amendment by the people, the proper way, but by reading the document as it hadn’t been read for 150 years — as authorizing effectively _unlimited_ government.26



The Court did that rewrite in three basic steps. First, in 1937 it eviscerated the very centerpiece of the Constitution, the doctrine of enumerated powers. Then in 1938 it bifurcated the Bill of Rights and gave us a bifurcated theory of judicial review. Finally, in 1943 it jettisoned the non‐​delegation doctrine. Let me describe those steps a bit more fully so you can see the importance of recognizing and adhering to the theory that stands behind and informs a constitution.



The evisceration of the doctrine of enumerated powers involved three clauses in Article I, section 8, where Congress’s 18 legislative powers are enumerated: the General Welfare Clause, the Commerce Clause, and the Necessary and Proper Clause. All were written to be shields against government. The New Deal Court turned them into swords of government through which the modern redistributive and regulatory state has arisen.



In relevant part, the General Welfare Clause, the first of Congress’s enumerated powers, authorizes Congress to tax to pay for the “general Welfare of the United States.” As Madison wrote in _Federalist 41_ , that qualifying language was simply a general heading under which Congress’s 17 other powers were subsumed, for which Congress may tax, but only if they serve the _general_ welfare _of the United States_ , not particular or local welfare.



Instead, the New Deal Court read the clause as an _independent_ power authorizing Congress to tax for whatever it thought might serve the “general welfare.“27 That reading could not be right, however, because it would enable Congress to tax for virtually any end, thus rendering Congress’s other powers superfluous, as Madison, Jefferson, and many others noted when the issue arose early in our history. Indeed, it would turn the Constitution on its head by allowing Congress effectively unlimited power. Such is the result from ignoring the document’s underlying theory of limited government.



Similar issues arose that year with the Commerce Clause, which in relevant part authorized Congress to regulate interstate commerce. Recall that under the Articles of Confederation states had begun erecting tariffs and other protectionist measures, and that was leading to the breakdown of free trade among the states. Thus, the Framers gave Congress the power to regulate — or _make regular_ — commerce among the states, largely by negating state actions that impeded free trade, but also through affirmative actions that might facilitate that end.28



Over several decisions, however, beginning in 1937,29 the New Deal Court read the Commerce Clause as authorizing Congress to regulate anything that “affected” interstate commerce, which of course is virtually everything. Thus, in 1942 the Court held that, to keep the price of wheat high for farmers, Congress could limit the amount of wheat a farmer could grow, even though the excess wheat in question in the case never entered commerce, much less interstate commerce, but was consumed on the farm by the farmer and his cattle. The Court held that the excess wheat he consumed himself was wheat he would otherwise have bought on the market, so “in the aggregate” such actions “affected” interstate commerce.30 Such were the economic theories of the Roosevelt administration.



The last of Congress’s 18 enumerated powers authorizes it “to make all laws which shall be necessary and proper for carrying into execution the foregoing powers.” Thus, the clause affords Congress _instrumental_ powers — the _means_ for executing its other powers or pursuing its other enumerated ends. “Necessary” and “proper” are words of limitation, of course: Not any means Congress desires will do. Yet the New Deal and subsequent Court’s, until very recently, have hardly policed those limitations.31



Turning now to the second step, despite the demise in 1937 of the doctrine of enumerated powers, one could still invoke one’s rights against Congress’s expanded powers. So to address that “problem,” the New Deal Court added a famous footnote to a 1938 opinion.32 In it, the Court distinguished two kinds of rights: “fundamental,” like speech, voting, and, later, certain personal rights; and “non‐​fundamental,” like property rights and rights we exercise in “ordinary commercial relations.” If a law implicated fundamental rights, the Court would apply “strict scrutiny” and the law would likely be found unconstitutional. By contrast, if non‐​fundamental rights were at issue, the Court would apply the so‐​called rational basis test, which held that if there were _some_ reason for the law, if you could _conceive_ of one, the law would be upheld. Thus was economic liberty reduced to a second‐​class status. None of this is found in the Constitution, of course. The Court invented it from whole cloth to make the world safe for the New Deal programs.33



Finally, in 1943 the Court jettisoned the non‐​delegation doctrine,34 which arises from the very first word of the Constitution: “ _All_ legislative Powers herein granted shall be vested in a Congress .…” Not some; all. As government grew, especially during the New Deal, Congress began delegating ever more of its legislative power to the executive branch agencies it was creating to carry out its programs. Some 450 such agencies exist in Washington today. Nobody knows the exact number.



That is where most of the law Americans live under today is written, in the form of regulations, rules, guidance, and more, all issued to implement the broad statutes Congress passes. Not only is this “law” written, executed, and adjudicated by unelected, non‐​responsible agency bureaucrats — raising serious separation‐​of‐​powers questions — but the Court has developed doctrines under which it defers to _agencies_ ’ interpretations of statutes, thus largely abandoning its duty to oversee the political branches. Governed largely today under administrative law promulgated by the modern executive state, we’re far removed from the limited, accountable government envisioned by the Founders and Framers.35



This completes my overview of American constitutional theory and history. From it, as I mentioned early on, the main lesson to be drawn is that culture matters. The Founders and Framers were animated by individual liberty under limited government. When the post‐​Civil War Framers saw the need to revise our original federalism, they did it the right way, by amending the Constitution to make it consistent with its underlying moral and political principles. The New Deal politicians, having less regard for the Constitution and its underlying principles, rejected that course, choosing instead to browbeat the Court into effectively rewriting the Constitution, undermining its moral and political principles in the process.



But don’t take my word for it. Here’s Franklin Roosevelt, writing to the chairman of the House Ways and Means Committee in 1935: “I hope your committee will not permit doubts as to constitutionality, however reasonable, to block the suggested legislation.“36 And here’s Rexford Tugwell, one of the principal architects of the New Deal, reflecting on his handiwork some 30 years later: “To the extent that these [New Deal policies] developed, they were tortured interpretations of a document intended to prevent them.” They knew exactly what they were doing. They were turning the Constitution on its head.37



Thus, the problem today is not, as so many America progressives think, too little government. It’s too much government, intruding on our liberties and driving us ever deeper into debt. And it isn’t as if our Founders didn’t understand that. As Jefferson famously wrote, “The _natural_ progress of things is for liberty to yield, and _government_ to gain ground.“38 The remedy is a good constitution, but it must be followed. And that takes good people.



 **A Few Implications for European Constitutionalism**



So what lessons might we draw from the American experience for European constitutionalism? Recall my mentioning earlier of being struck by the tension in the EU between exclusion and inclusion in its many forms, including individualism and collectivism. As we’ve seen, that same tension runs through America’s constitutional history as well. To address deficiencies in the Articles of Confederation, the original Constitution moved toward greater inclusion to form “a more perfect Union.” But the resulting federalism didn’t get the balance right either. It left too much power with the states, enabling the southern states to continue enforcing slavery. So the Civil War Amendments increased the inclusion, correctly. The adjusted federalism gave more power to the federal government, enabling it to block states from oppressing their own citizens — a higher power checking a subsidiary power.



But that balance, reflecting the nation’s underlying principles, was upended again by the far more inclusive New Deal constitutional revolution. Giving vastly more power to the federal government, contrary to the nation’s limited government principles, this change swept ever more Americans into public programs, leading many to want out. They wanted to be _excluded_ from the socialization of life, as reflected by the rise of the conservative and libertarian movements in the second half of the 20th century.



Are there parallels with post‐​War developments in Europe? To this sometime‐​student of European affairs, there seem to be, but the inclusion that began with the 1951Treaty of Paris and continued through the many treaties since makes it difficult if not impossible to speak of three distinct periods, as in America, much less point to a “golden mean” in this evolution akin to America’s post‐​Civil War settlement. In recent years, however, the impetus toward exclusion, in many forms, is unmistakable, Brexit being only the most prominent example, the ongoing refugee resettlement crisis being another.



Federalism _within_ nations is a delicate balance. Federalism _among_ sovereign nations, which is what the EU amounts to, is far more difficult, especially when cultural differences loom large. And on that score, here’s a paradox. Europeans have always been more comfortable than Americans with collectivization in the form of the welfare state, certainly within their respective nations.39 But with collectivization _among_ nations, cultural differences — rich and poor being only one axis — can easily exacerbate the cooperation that is required if collectivization is to work at all, much less with any measure of efficiency. The evidence suggests that the EU has gone too far in that direction. At the same time, the evidence is equally clear that the failure to make EU border security an EU responsibility, leaving it instead to individual members, has raised serious problems too.40



In America, border security became a federal government function once the Constitution was ratified. Within our borders, however, to keep states honest, the Founders instituted _competitive_ federalism, whereby states compete for the allegiance of citizens, and it’s largely worked as states with high taxes and excessive regulations lose firms and people to states with low taxes and reasonable regulations. People vote with their feet, musg as in the Schengen Area. But the federal income tax plus the direct election of senators, both enacted as constitutional amendments and both promoted by progressives, unleashed _cooperative_ federalism whereby federal and state officials collude, using federal funds and enacting federal regulations, to undercut state autonomy and the discipline that competitive federalism was meant to secure.41



Earlier I said that you can’t understand the American Constitution unless you understand the theory behind it. Well what’s the theory behind the treaties that comprise the EU Constitution? Peace through trade and cooperation, yes — given Europe’s long history of wars. But beyond that, what? We’ve seen how a radical shift in the climate of ideas in America, especially in the direction of collectivism, has led, as many lonely voices predicted, to a reaction that today reflects a deeply divided nation, unable to restrain its appetite for “free” goods and services, even in the face of crushing debt. The divisions surfacing recently in Europe are no accident. People and peoples yearn to breathe free — in an earlier understanding of that idea. The balance needed to ensure that freedom may be difficult to find. But to discover it, as we celebrate Italy’s Constitution today and reflect on Italy’s place within the larger European Community, we could do no better than to appeal to the First Principles that are the very foundation of civilized nations. Thank you.
"
"
Share this...FacebookTwitterRussian Prime Minister Vladimir Putin arrived Monday on Samoilovsky Island in the western Siberian Arctic to visit a joint Russian-German scientific expedition, Lena-2010.
Lena 2010 is conducting studies on the Russian Arctic permafrost, which is 1.5 km thick at the Samoilovsky Island location and estimated to be 40,000 years old.
The German page of Ria Novosti has a video up (sorry – only in German) which reveals an interesting comment, one not reported in the western media. The video shows Putin visiting the study site, and even helps the scientists bore into the permafrost.
Update: English video click here



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the 1:15 mark of the video, Putin is reported to have asked each scientist the same question:
Is climate change caused by man, or not?
To which the video gives the reply at the 1:24 mark, the reporter says (translated from German):

The scientists, however, could not give an exact answer. The heat wave in Russia resulted from a combination of factors.
Well that’s a long way from meaning “yes”. So much for consensus and the science being done.
When asked about plans in the Arctic, Putin explains at the 1:09 mark:
We are planning to extract natural resources from the ground in Western Siberia. We have to understand the entire eco-system and how it will respond.  
The Russians obviously are getting ready to mine and drill in the region.
Reuters presents an interesting politically correct view of Putin’s visit, and completely ignore the answer the scientists gave to Putin’s question. Read here: http://alertnet.org/thenews/newsdesk/LDE67M11O.htm
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWhen it comes to renewable energy, you can call them “Jesus technologies”. These are technologies that went the way of the dinosaurs a long time ago due to their inefficiency and impracticality. But in order to serve a political purpose, they seem to keep getting resurrected every 30 years or so. I came up with “Jesus technology” from Bishop Hill’s  Caspar and the Jesus-Paper“, a paper that died often but kept coming back.
Sure in some cases these primitive technologies make sense, but for the wide-scale application in a power grid, they make little sense, cost the consumer dearly, and even put the energy supply at risk.
Klaus-Dieter Humpich of the Friedrich Naumann Foundation For Freedom has an excellent piece on renewable energy called The Dirty Secret of Wind and Solar. It’s in German, and so if you can read the language, it is worth taking the time to do so. What follows is a summary.
Humpich’s essay starts by reminding us that electrical energy is very difficult and enormously costly to store. Therefore, the wildly fluctuating supply of wind and solar energies requires having conventional back-up systems in place, ready to fire up or throttle down at a moment’s notice whenever the sun and wind intensities change. Humprich writes that solar and wind are referred to as “additive energy forms” in the energy business, and not as “alternatives”. Alternatives would suggest that they replace conventional fuels, which is not the case. They only add to conventional fuels, and hence they are called additive supplies.
Wind and solar are a nightmare to control
The problem with wind energy is that a wind generator’s output varies with the third power of the wind velocity, P = kV³. That means a wind generator produces only 1/8 of it’s rated energy if the wind speed is cut in half. So whenever the wind speed changes, the power grid must be compensated by conventional power plants that are on constant stand-by. On gusty days, as more wind parks get added to the grid, it becomes more and more of a nightmare to keep the grid stable. The result: you get a grid that behaves like a wild bronco. Humpich writes:
A power control engineer would say that these are real disturbances with steep gradients (e.g. changes in power output due to a wind gust through a wind park).”
The once easy-to-manage, steady, conventional-fuel power supply and corresponding consumption have since been intruded on by a third, highly unstable and unpredictable player.
Standby conventional power plants have low efficiencies
So when the wind suddenly dies down, reserve conventional plants have to jump in quickly, meaning they’ve got to be always on stand-by. These power plants thus rarely run at their peak efficiencies, and often at outputs well below their peak efficiency. The result? Little, if any, savings in fossil fuel consumption gets achieved. Now we know why the concept of wind being an alternative really isn’t so.
The energy that gets produced by a wind generator, is in part lost to reduced efficiencies by the standby conventional plants. All the investment and resources to install the massive system wind and solar park infrastructure has only lead to saving a fraction of what they originally were promised to save.
Humpich writes:
You always have to keep conventional power plants running alongside in order to keep the grid stable. That means it consumes fuel that does not even get used. Be it that the plant is running only at partial capacity – at a sub-optimal efficiency – or is “throttled”, which means the generated steam does not even get sent to the turbines to be converted into power but rather is simply sent back to the condenser.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Result: consumption of fuel – for nothing.
Mixed power grids and systems are less efficient
So why go through all the trouble if it isn’t worth it? It’s all in the bookkeeping. As long as the energy from renewable sources gets accounted as having replaced the equivalent in conventional energy, then it appears interesting and the business of CO2 emission certificates looks especially lucrative.
As Humprich explains, it’s a bit like a brochure for a new car claiming the car gets 45 mpg. But as we know, that number is only under certain ideal conditions. In reality, with all the stop-and-go driving in city traffic, etc., the car’s fuel efficiency turns out to be much less.
It’s the same concept with a grid that is powered only using steady conventional fuel. An efficiency close to that advertised by the “manufacturer'” can indeed be reached when operated near ideal conditions. But when you mix in wind and solar parks, the efficiency is spoiled – you’re in “city traffic”. It drops considerably.
So what exacly is the efficiency of a conventional power plant operating on a “mixed grid”? Does the generated wind and solar energy replace a corresponding amount of fossil fuel? The answer is of course “no”. To determine the exact amount, it is necessary to conduct comprehensive simulations or actual field measurements. Humprich provides an example. A combination natural gas fired/steam typically has an efficiency of 57%. But when it is used as a back up for wind and solar energy, it no longer operates under ideal conditions, and so the efficiency drops to a measly 36%.
Rotten in Denmark
Denmark is a country that has a large supply of renewable energy. And it is also long known that when storms rage over Denmark, its power grid has to be stabilized by conventional power plants in neighboring Germany and Sweden.
Today wind and solar energy are incresingly being stabilized by gas-fired steam power plants, and so  it means more business for the gas industry. Humprich writes:
Maybe that’s why the two leading propagandists for wind and solar energy today are representatives of gas. In the USA, in any case, the gas industry is the leading sponsor of the ‘climate industry’. But this is not reprehensible. If you wish to promote another product (natural gas) onto an established market (coal and nuclear), then a lot of arm-twisting is needed. In this respect, gas-guys like Schröder, Fischer and Co. become real vacuum cleaner salespeople, who happen to get get paid generaously for their sales pitches.”
Further Literature:
Kent Hawkins: Wind Integration Realities – Case Studies of the Netherlands and of Colorado, Texas, Master Resource.
C. le Pair & K. de Groot: The impact of wind generated electricity on fossil fuel consumption.
K. de Groot & C. le Pair: The hidden fuel costs of wind generated electricity.
Share this...FacebookTwitter "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.” In this edition, we cover an important story that we missed back in 2008._   
  
  
People send us stuff. As a result of our recent _Global Science Report_ on global warming ruining our bananas, one of our fans directed our attention to an important effect of climate change that we somehow missed, back in 2008, when the alarmists at the BBC wrote that it was threatening _haggis_.   
  
  
Haggis, for the uninitiated, is sheep stomach stuffed with minced lung, liver, heart, tongue, suet, onions and oats. How offal!   
  
  
While there’s no accounting for taste, it tastes as bad as it smells.   
  
  
According to the story, there has been a rise in a parasite effecting Scottish sheep that renders the lung “unfit for consumption” (something that many of you probably thought was the case already).   
  
  
And, so as not to miss the bandwagon, an official from the Scottish Agricultural College Veterinary Investigation Centre told the BCC that:   




Part of the reason will be the parasite is able to live a pretty happy life on the ground because of higher temperatures. Maybe it’s climate change.



Or maybe not.   
  
  
It turns out that another potential cause of the increase in the lung parasite is that Scottish farmers have reduced their application of parasite treatment due to declining infections of roundworm. The treatment of roundworm also killed the lung parasite.   
  
  
There was no mention made in the BCC article as to whether global warming was behind the decrease in roundworm infestations.   
  
  
Instead, the article went on the describe the events which took place in the World Haggis Eating Championship, won by Willie Robertson from Dunkeld, who managed to put away a pound of haggis in 125 seconds. For his victory, Mr. Robertson was awarded a trophy and a bottle of whiskey—no doubt a key feature in the rest of the day’s merrymaking.   
  
  
BBC’s writing in the haggis story appears similarly merry. Here are the last three paragraphs of their report, verbatim, a candidate for first place in the 2008 International Nonsequitur Competition.   




The championship was held as part of the 125th Birnam Highland Games, and attracted competitors from Australia, New Zealand and the US.   
  
  
Climate change, meanwhile, has been blamed for affecting natural habitats in Scotland and across the world.   
  
  
Most notably, scientists and conservationists say it threatens survival of polar bears.
"
"
Share this...FacebookTwitterSnow already!
A cold blast of polar air has moved over central Europe, pushíng temperatures far below normal. Snow is forecast in the Alps at elevations as low as 1400 meters.
The following chart shows the forecast for the next 2 weeks. It shows that summer is finished for much of Europe. In fact, as mentioned above, the lower Alps are expected to get snowfall in the couple of days ahead. Continue reading below.
According to Snowfinder, it’s time to get your skis out. Snow is in the forecast at many ski resorts. At Mürren Schilthorn in Switzerland elev. 2360 meters, snow is forecast for early this week. The same is true for Kitzbühel in Austria at 2000 meters elevation.
Heck, even below 2000 meters snow is in the forecast, for example Steinplatte Waidring in Austria , which is only 1800 meters elevation. Maybe Nature will publish a study attributing it to global warming.
===================================================
Update 1,  August 30: Rain mixed with SNOW was reported on the Brocken summit, elevation 1141 meters, In north Germany this morning on north German NDR radio.
Update 2, August 30: Snow line has dropped to 1400m (below the 5000′ line) in the Alps. Heavy snow is in the forecast. See following chart (h/t: Reader Patagon):
Source: http://www.meteoexploration.com/snow/snowmaps.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAnd that includes those of you who do not agree with me. Now here’s a humorous little clip that I think is a little relevant.

It’s always a good idea to have quality data and analyses before deciding on and implementing precautionary measures.
Have a good one, folks!
Share this...FacebookTwitter "
"

Last December representatives of 160 nations gathered in Kyoto, Japan, to hammer out an agreement on emissions of greenhouse gases. But before the U.S. Senate ratifies the Kyoto treaty, it’s important to point out that much discussion of the “greenhouse effect” is little more than hot air. 



The Kyoto agreement rests on forecasts of future greenhouse gas emissions. But energy use, which requires the burning of fossil fuels, depends on economic growth and prosperity. Economists aren’t soothsayers: they often over‐ or underestimate growth, and they can’t be expected to discern how technological advances will change fossil fuel consumption.



Indeed, many climate models — which remain unreliable — predict that most of the climate change will occur many decades from now — the forecasted increase of 4.5 degrees Fahrenheit won’t happen for a century. It’s impossible to have any confidence in forecasts of the technology, population or energy sources of 2098. We can predict, however, that future generations will have better technology at their disposal, that they will be wealthier, and that they will live longer. They will certainly be in a better position to deal with adverse climate changes than we are today.



The Clinton administration had difficulty deciding what it could accept at Kyoto. Its quandary was magnified by the projected failure of the United States to reduce emissions to 1990 levels by the year 2000. Rather than cutting them, a booming economy appears likely to boost emissions of carbon dioxide by at least 15 percent in this decade. Cutting emissions enough to prevent climate change, which might require slashing emissions by some 60 percent, seems out of reach. Avoiding a warmer world would require a radical curbing of emissions by all countries, which in turn would lead to a worldwide slowdown in growth, perhaps even a depression that might make the 1930s look like Disneyland on a good day.



The Kyoto agreement is futile. Even Bert Bolin, the former chairman of the United Nations’ body of experts on global warming, says that the present plan would, if fully implemented, cut warming a quarter century from now “by less than 0.1 degree C, which would not be detectable.” We are plunging into a treaty that creates gigantic obligations without examining its costs and benefits. Congress has demanded that the Clinton administration provide estimates of the costs, but none have been forthcoming.



For most of the world, warming over the next century would cost only a little or would be an actual benefit. The few regions that actually would be harmed by warming should have help.



There is no need to rush into a treaty that would have little benefit but great cost. If climate change becomes a real problem, many steps can be taken that wouldn’t cripple our economy. Ocean scientists have shown, for example, that if the seas were “fertilized” with iron filings, phytoplankton (algae) would bloom and absorb vast quantities of carbon dioxide. The minuscule plants are nutritionally starved for iron and, when provided with that metal, multiply rapidly, absorbing large amounts of carbon. Some experts estimate that iron supplements might offset 15 to 20 percent of man‐​made carbon dioxide over the next few decades.



In addition, harvesting and replanting timber could sequester much carbon. Forest researchers have concluded that an active program of cropping and replanting fast‐​growing forests, then turning the lumber into housing and other long‐​term products, together with reforestation, could offset 12 to 15 percent of human greenhouse gas emissions. Seeding the ocean with iron and properly managing forests could by themselves do as much to slow climate change as capping greenhouse gas emissions at 1990 levels. Furthermore, scientists may develop other strategies in the future that don’t require shrinking our economy. 



The administration is under tremendous pressure to act immediately. To retain credibility with environmentalists, politicians, other countries and the mass media, it must take steps to reduce carbon dioxide emissions even if the limitations would have no benefit and would potentially impose high costs. To succeed in this high‐​wire act, President Clinton will probably propose new regulatory steps and mandates, such as higher fuel efficiency standards for new cars, more stringent restrictions on appliances, strict insulation levels for new buildings and more spending on mass transportation. Most of those regulations would be phased in slowly — that is, after President Clinton leaves the White House and many current members of Congress retire. The actual legislation required to meet the goal might even await a future Congress. Whatever difficulties present themselves, the administration will negotiate a formula that will allow it to claim that the entire world is cutting greenhouse emissions.



In short, this unnecessary measure would devastate our economy. For most of the world, warming over the next century would cost only a little or would be an actual benefit. The few regions that actually would be harmed by warming should have help. Delaying action by 20 to 30 years is the only prudent, “no regrets” policy. Technology will advance. Incomes in Third World countries will grow. The world will be more capable of coping with change. Except for measures that make sense with or without global warming — like ending subsidies for energy and energy use — Congress should resist any attempts to limit greenhouse gas emissions.
"
"
Share this...FacebookTwitterI’m wrong with my last post. There is some climate material in those Wikileaks documents. I started sifting through and found this one. Nothing earth-shattering, but you see the deals being done behind the scenes.
May 8, 2009
Here we see China was never going to accept any targets on CO2 emissions. But they did promise to bring “action items”!
http://cablegate.wikileaks.org/cable/2009/05/09BEIJING1247.html
Climate Change
————–
8. (C) UK DCM Wood said the UK Environment and Science
Minister had recently had talks with Chinese officials on
climate change. In the lead up to Copenhagen, China would
not agree to targets on emissions but was willing to be
constructive and would come to Copenhagen with a package of
action items related to nuclear power, renewable energy and
reforestation. Wood said his impression was that China could
be induced to do more on climate change. “
They knew long before that Copenhagen was going to fail. 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There are likely more about climate in there. It’ll take an army to go through it. There are some real doozies in their on other topics of Iran, gas and oil projects, etc. I’d say a gold mine of info for many corporations too. This is going to be worse than I thought.
Expect lots of “action items” in Cancun to save face.
==================================================================
Another Wikilieaks climate document found: Jan 29, 2008
Merkel pushing for aggressive measures. Here we see that Merkel is a flaming warmist.
http://cablegate.wikileaks.org/cable/2008/01/08BERLIN122.html
13. (C) Chancellor Merkel and the rest of Germany’s political
leadership remain serious about pursuing aggressive
international measures to meet the challenges of global
warming.  Merkel has made climate change a priority of her
Chancellorship and enjoys the overwhelming domestic support
on this.  Merkel’s support for mandatory, targeted global
limits on greenhouse gas (GHG) emissions and an international
cap-and-trade regime reflects a deep-seated belief that only
drastic, concerted efforts on the part of the international
community can slow — and ultimately reverse — the human
contribution to global warming.  If anything, Steinmeier
supports tougher standards.  While the Germans have been
willing to consider alternative solutions, such as new
technologies for clean coal and renewables, fundamental
differences in our approaches to the issue of climate change
remain, and could lead to more public disagreement in the
future.  For example, while Germany will send a delegation to
the January 30 Major Economies Meeting (MEM), the German
Government remains skeptical about the value that the Major
Economies Process (MEP) adds to the UNFCCC track. The Germans
are particularly concerned about the need to avoid
duplication of effort in the various other climate
change-related forums, including the UNFCCC and the G-8.
TIMKEN JR 
Share this...FacebookTwitter "
"

Congratulations to the first winners of the Cato on Campus Op‐​Ed Contest: Mytheos Holt and Šimon Franěk! Their op‐​eds, both dealing with environmental policy, tied for the December 2008 Cato on Campus Op‐​Ed Contest.   
  
  
Mytheos is a junior at Wesleyan University. Citing Cato Senior Fellow in Environmental Studies Patrick Michaels regarding the dangers of excessive environmentalism, Mytheos’s op‐​ed, “Burning the Greenbacks to Save the Greenhouse,” was published by the California Independent Voter Project.   
  
  
Šimon is an 18‐​year‐​old student at Gymnasium Kladno in the Czech Republic. Citing Patrick Michaels to refocus the global climate debate, Šimon’s op‐​ed, “Climate Change vs. Liberty in Europe,” calls attention to the politics behind global warming and the need to consider the ramifications of policy decisions based on global warming.   
  
  
Mytheos and Simon were each sent autographed copies of Patrick Michaels’ books _Meltdown: The Predictable Distortion of Global warming by Scientists, Politicians and the Media _and _Climate of Extremes: Global Warming Science they Don’t Want You to Know_ (just released in January). Their op‐​eds will also be considered for the Cato on Campus Op‐​Ed of the Year, with a chance of receiving a full scholarship to Cato University.   
  
  
Submit your work to one of three Cato student contests.
"
"
Share this...FacebookTwitterNot only Europe is getting walloped by winter, so are some regions in China, where the most snow has fallen in 30 years.
The early snowstorms have isolated herdsman living in remote areas, see map, and Chinese relief services dispatched assistance.
According to China.org.cn:
Snow storms has hit four towns in Xing’an Prefecture, a pasture region about 1,500 km northeast of the regional capital, Hohhot, since last Saturday.
Snow has accumulated up to 30 cm deep in most parts of the region and a meter in some areas.
The snow was 40 days earlier than its usual arrival time and was the heaviest in 30 years. At least 700 heads of livestock are believed to have died in the storm.
Winter arrives early in Northeast China
And more here: Winter in China
UPDATE: Record cold in the UK. h/t: M White
Share this...FacebookTwitter "
"
UPDATE 1/28: Full text of Dr. Theon’s letter has been post on the Senate website and below.
This is something I thought I’d never see. This press release today is from the Senate EPW blog of Jame Inhofe.  The scientist making the claims in the headline, Dr. John S. Theon, formerly of  the Institute for Global Environmental Strategies, Arlington, Virginia, has a paper here in the AMS BAMS that you may also find interesting. Other papers are available here in Google Scholar. He also worked on the report of the Space Shuttle Challenger accident report and according to that document was a significant contributor to weather forecasting improvements:
The Space Shuttle Weather Forecasting Advisory  Panel, chaired by Dr. John Theon, was established by NASA  Headquarters to review existing weather support capabilities and  plans and to recommend a course of action to the NSTS Program.  Included on the panel were representatives from NASA, the National  Oceanic and Atmospheric Administration (NOAA), the Air Force, and the  National Center for Atmospheric Research.
For those just joining the climate discussion, Dr. James Hansen is the chief climate scientist at NASA Goddard Institute for Space Studies (GISS) and is the man who originally raised the alarm on global warming in 1988 in an appearance before congress. He is also the keeper of the most often cited climate data.
EPW press release below – Anthony

Washington DC, Jan 27th 2009: NASA warming scientist James Hansen, one of former Vice-President Al Gore’s closest allies in the promotion of man-made global warming fears, is being publicly rebuked by his former supervisor at NASA.
Retired senior NASA atmospheric scientist, Dr. John S. Theon, the former supervisor of James Hansen, NASA’s vocal man-made global warming fear soothsayer, has now publicly declared himself a skeptic and declared that Hansen “embarrassed NASA” with his alarming climate claims and said Hansen was “was never muzzled.”  Theon joins the rapidly growing ranks of international scientists abandoning the promotion of man-made global warming fears.
“I appreciate the opportunity to add my name to those who disagree that global warming is man made,” Theon wrote to the Minority Office at the Environment and Public Works Committee on January 15, 2009. “I was, in effect, Hansen’s supervisor because I had to justify his funding, allocate his resources, and evaluate his results,” Theon, the former Chief of the Climate Processes Research Program at NASA Headquarters and former Chief of the Atmospheric Dynamics & Radiation Branch explained.
“Hansen was never muzzled even though he violated NASA’s official agency position on climate forecasting (i.e., we did not know enough to forecast climate change or mankind’s effect on it). Hansen thus embarrassed NASA by coming out with his claims of global warming in 1988 in his testimony before Congress,” Theon wrote.  [Note: NASA scientist James Hansen has created worldwide media frenzy with his dire climate warning, his call for trials against those who dissent against man-made global warming fear, and his claims that he was allegedly muzzled by the Bush administration despite doing 1,400 on-the-job media interviews! – See: Don’t Panic Over Predictions of Climate Doom – Get the Facts on James Hansen  – UK Register: Veteran climate scientist says ‘lock up the oil men’ – June 23, 2008 & UK Guardian: NASA scientist calls for putting oil firm chiefs on trial for ‘high crimes against humanity’ for spreading doubt about man-made global warming – June 23, 2008 ]
Theon declared “climate models are useless.” “My own belief concerning anthropogenic climate change is that the models do not realistically simulate the climate system because there are many very important sub-grid scale processes that the models either replicate poorly or completely omit,” Theon explained. “Furthermore, some scientists have manipulated the observed data to justify their model results. In doing so, they neither explain what they have modified in the observations, nor explain how they did it. They have resisted making their work transparent so that it can be replicated independently by other scientists. This is clearly contrary to how science should be done. Thus there is no rational justification for using climate model forecasts to determine public policy,” he added.
“As Chief of several of NASA Headquarters’ programs (1982-94), an SES position, I was responsible for all weather and climate research in the entire agency, including the  research work by James Hansen, Roy Spencer, Joanne Simpson, and several hundred other scientists at NASA field centers, in academia, and in the private sector who worked on climate research,” Theon wrote of his career. “This required a thorough understanding of the state of the science. I have kept up with climate science since retiring by reading books and journal articles,” Theon added. (LINK) Theon also co-authored the book “Advances in Remote Sensing Retrieval Methods.” [Note: Theon joins many current and former NASA scientists in dissenting from man-made climate fears. A small sampling includes: Aerospace engineer and physicist Dr. Michael Griffin, the former  top administrator of NASA, Atmospheric Scientist Dr. Joanne Simpson, the first woman in the world to receive a PhD in meteorology, and formerly of NASA, Geophysicist Dr. Phil Chapman, an astronautical engineer and former NASA astronaut, Award-Winning NASA Astronaut/Geologist and Moonwalker Jack Schmitt, Award-winning NASA Astronaut and Physicist Walter Cunningham of NASA’s Apollo 7, Chemist and Nuclear Engineer Robert DeFayette was formerly with NASA’s Plum Brook Reactor, Hungarian Ferenc Miskolczi, an atmospheric physicist with 30 years of experience and a former researcher with NASA’s Ames Research Center, Climatologist Dr. John Christy, Climatologist Dr. Roy W. Spencer, Atmospheric Scientist Ross Hays of NASA’s Columbia Scientific Balloon Facility] 
Gore faces a much different scientific climate in 2009 than the one he faced in 2006 when his film “An Inconvenient Truth” was released. According to satellite data, the Earth has cooled since Gore’s film was released,  Antarctic sea ice extent has grown to record levels, sea level rise has slowed, ocean temperatures have failed to warm, and more and more scientists have publicly declared their dissent from man-made climate fears as peer-reviewed studies continue to man-made counter warming fears. [See: Peer-Reviewed Study challenges ‘notion that human emissions are responsible for global warming’ & New Peer-Reviewed Scientific Studies Chill Global Warming Fears ]
“Vice President Gore and the other promoters of man-made climate fears endless claims that the “debate is over” appear to be ignoring scientific reality,” Senator James Inhofe, Ranking Member of the Environment & Public Works Committee.
A U.S. Senate Minority Report released in December 2008 details over 650 international scientists who are dissenting from man-made global warming fears promoted by the UN and yourself. Many of the scientists profiled are former UN IPCC scientists and former believers in man-made climate change that have reversed their views in recent years. The report continues to grow almost daily. We have just received a request from an Italian scientist, and a Czech scientist to join the 650 dissenting scientists report. A chemist from the U.S. Naval Academy is about to be added, and more Japanese scientists are dissenting. Finally, many more meteorologists will be added and another former UN IPCC scientist is about to be included. These scientists are openly rebelling against the climate orthodoxy promoted by Gore and the UN IPCC.
The prestigious International Geological Congress, dubbed the geologists’ equivalent of the Olympic Games, was held in Norway in August 2008 and prominently featured the voices of scientists skeptical of man-made global warming fears. Reports from the conference found that Skeptical scientists overwhelmed the meeting, with  ‘2/3 of presenters and question-askers hostile to, even dismissive of, the UN IPCC’ ( See full reports here & here ]  In addition, a 2008 canvass of more than 51,000 Canadian scientists revealed 68% disagree that global warming science is “settled.”  A November 25, 2008, article in Politico noted that a “growing accumulation” of science is challenging warming fears, and added that the “science behind global warming may still be too shaky to warrant cap-and-trade legislation.”  More evidence that the global warming fear machine is breaking down. Russian scientists “rejected the very idea that carbon dioxide may be responsible for global warming”. An American Physical Society editor conceded that a “considerable presence” of scientific skeptics exists.  An International team of scientists countered the UN IPCC, declaring: “Nature, Not Human Activity, Rules the Climate”. India Issued a report challenging global warming fears. International Scientists demanded the UN IPCC “be called to account and cease its deceptive practices.”
The scientists and peer-reviewed studies countering climate claims are the key reason that the U.S. public has grown ever more skeptical of man-made climate doom predictions. [See: Global warming ranks dead last, 20 out of 20 in new Pew survey. Pew Survey:   & Survey finds majority of U.S. Voters – ‘51% – now believe that humans are not the predominant cause of climate change’ – January 20, 2009 – Rasmussen Reports ] 
The chorus of skeptical scientific voices grow louder in 2008 as a steady stream of peer-reviewed studies, analyses, real world data and inconvenient developments challenged the UN’s and former Vice President Al Gore’s claims that the “science is settled” and there is a “consensus.”
On a range of issues, 2008 proved to be challenging for the promoters of man-made climate fears.  Promoters of anthropogenic warming fears endured the following: Global temperatures failing to warm; Peer-reviewed studies predicting a continued lack of warming;  a failed attempt to revive the discredited “Hockey Stick“; inconvenient developments and studies regarding rising CO2; the Spotless Sun; Clouds; Antarctica; the Arctic; Greenland’s ice; Mount Kilimanjaro; Global sea ice; Causes of Hurricanes; Extreme Storms; Extinctions; Floods; Droughts; Ocean Acidification; Polar Bears; Extreme weather deaths; Frogs; lack of atmospheric dust; Malaria; the failure of oceans to warm and rise as predicted.
# # #
ORIGINAL FULL TEXT LETTER SENT VIA EMAILS:
—–Original  Message—–
From: Jtheon [mailto:jtheon@XXXXXXX]
Sent: Thursday,  January 15, 2009 10:05 PM
To: Morano, Marc (EPW) 
Subject: Climate models are  useless 
Marc, First, I sent several  e-mails to you with an error in the address and they have been returned to me.  So I’m resending them in one combined e-mail. 
Yes, one could say that I was,  in effect, Hansen’s supervisor because I had to justify his funding, allocate  his resources, and evaluate his results. I did not have the authority to give  him his annual performance evaluation. He was never muzzled even though he  violated NASA’s official agency position on climate forecasting (i.e., we did  not know enough to forecast climate change or mankind’s effect on it). He thus  embarrassed NASA by coming out with his claims of global warming in 1988 in his  testimony before Congress. 
My own belief concerning  anthropogenic climate change is that the models do not realistically simulate  the climate system because there are many very important sub-grid scale  processes that the models either replicate poorly or completely omit.  Furthermore, some scientists have manipulated the observed data to justify their  model results. In doing so, they neither explain what they have modified in the  observations, nor explain how they did it. They have resisted making their work  transparent so that it can be replicated independently by other scientists. This  is clearly contrary to how science should be done. Thus there is no rational  justification for using climate model forecasts to determine public policy. 
With best wishes, John 
# # 
From: Jtheon [mailto:jtheon@XXXXXX]
Sent: Tuesday, January  13, 2009 12:50 PM
To: Morano, Marc (EPW) 
Subject: Re: Nice seeing you 
Marc, Indeed, it was a pleasure  to see you again. I appreciate the opportunity to add my name to those who  disagree that Global Warming is man made.  A brief bio follows. Use as much or  as little of it as you wish. 
John S. Theon Education: B.S.  Aero. Engr. (1953-57); Aerodynamicist, Douglas Aircraft Co. (1957-58); As USAF  Reserve Officer (1958-60),B.S. Meteorology (1959); Served as Weather Officer  1959-60; M.S, Meteorology (1960-62); NASA Research Scientist, Goddard Space  Flight Ctr. (1962-74); Head Meteorology Branch, GSFC (1974-76); Asst. Chief,  Lab. for Atmos. Sciences, GSFC (1977-78);  Program Scientist, NASA Global  Weather Research Program, NASA Hq. (1978-82); Chief, Atmospheric Dynamics &  Radiation Branch NASA Hq., (1982-91); Ph.D.,  Engr. Science & Mech.: course  of study and dissertation in atmos. science (1983-85); Chief, Atmospheric  Dynamics, Radiation, & Hydrology Branch, NASA Hq. (1991-93); Chief, Climate  Processes Research Program, NASA Hq. (1993-94); Senior Scientist, Mission to  Planet Earth Office, NASA Hq. (1994-95); Science Consultant, Institute for  Global Environmental Strategies (1995-99); Science Consultant  Orbital Sciences  Corp. (1996-97) and NASA Jet Propulsion Lab., (1997-99). 
As Chief of several NASA Hq.  Programs (1982-94), an SES position, I was responsible for all weather and  climate research in the entire agency, including the  research work by James  Hansen, Roy Spencer, Joanne Simpson, and several hundred other scientists at  NASA field centers, in academia, and in the private sector who worked on climate  research. This required a thorough understanding of the state of the science. I  have kept up with climate  science since retiring by reading books and journal  articles. I hope that this is helpful. 
Best wishes, John 

Sponsored IT training links:
Best quality 640-553 dumps written by certified expert to help you pass 642-456 and 70-536 exam in easy and fast way.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e991001c7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSome of you will recall this pretty face we saw a couple of months ago.
We got to know her here, here and here. The IPCC based solar impact on climate on the “consensus of a single astronomer, who agreed with herself”.
In March, 2008, Dr. Lean made a presentation on global warming, see the following video link:
http://www.youtube.com/watch?v=OOMsQcEN1Bg
It kind of surprises me (and yet it doesn’t) that she doesn’t even mention water vapour as a greenhouse gas. She neglects to mention the PDO as well, looking only at the small timescale El Ninos and La Ninas.
She also completely ignores clouds as a factor effecting the temperature of the earth.
I guess the models are getting simpler and simpler (I mean the climate models).
Okay, it appears this video is not complete, and so I’ll hold back judgement.
Share this...FacebookTwitter "
"

After five days of contentious discussions in Bangkok, governments from nearly 200 countries last week agreed to an agenda for further talks to forge a new United Nations global warming agreement. One sticking point has been developing nations’ insistence that industrialized countries should take the first steps in reducing emissions and should help finance reductions in developing countries. But this represents a serious misreading of the underlying economic situation.



The theory behind the “developed countries should pay” model was articulated by Yvo de Boer, executive secretary of the U.N. Framework Convention on Climate Change: “The problem of climate change … is a result of rich countries’ emissions, not the result of poor countries’ emissions. The historic responsibility of this problem lies with industrial nations.”



Yet although greenhouse gas emissions can be blamed on nations based on the location of emission activities, these emissions are the effluvia of civilization and all its activities. In today’s interconnected world, economic activity in one country helps provide livelihoods and incomes for many inhabitants elsewhere, and vice versa. A substantial portion of economic growth in developing countries is attributable to trade, remittances, tourism and direct investment from industrialized countries.



For example, remittances, mainly from the United States, Britain and the oil‐​rich Gulf states, account for 13% of Bangladesh’s GDP. Absent economic activities that directly or indirectly fuel such contributions to developing countries, U.S. emissions might be lower, but so would jobs and incomes in developing countries like Bangladesh.



These linkages have had hugely positive effects. Greenhouse‐​gas‐​fueled economic activity has enabled today’s rich societies to invest in agricultural, medical and public health research that has raised crop yields and lowered hunger in developing countries; to devise effective medical interventions to address old diseases like tuberculosis, malaria, diarrhea and smallpox and new diseases like AIDS; and to provide aid in times of famine or other natural disasters.



Absent such economic activity, human capital would have been lower worldwide. Consider, for instance, the millions of non‐​Americans who have been cycled through universities in the U.S. who then returned to advance their native countries’ economic and technological development.



Some might argue that one should not take indirect effects of greenhouse‐​gas‐​producing activities into consideration: Only direct effects should be considered. But the notion of assigning responsibility or demanding compensation for climate change is itself based on indirect and inadvertent outcomes. Industrialized countries did not emit greenhouse gas emissions just for fun. There are clearly benefits.



So if the U.S. contribution to global warming, for instance, could be estimated, the next step would be to estimate the net harm caused to, say, Bangladesh. This requires estimating both direct and indirect impacts not just of climate change but all greenhouse gas‐​producing activities on Bangladesh.



This raises some serious questions, including: Had there been no greenhouse gas‐​producing activities in the U.S., what would have been Bangladesh’s GDP and level of human well‐​being? How would that affect life expectancy, which is currently 62 years but was only 35 years in 1945? Would Bangladesh’s hunger and malnutrition rates rise? How many Bangladeshis were saved in the 1960s and 1970s because of food aid from industrialized countries? How much of its increase in agricultural productivity is due to higher CO2 levels, or indirectly due to efforts enabled because the U.S. was wealthy enough to support them? If future agricultural productivity declines due to climate change, how do you subtract past and present benefits from future harms?



Clearly, it’s premature to assign “responsibility” to industrialized countries for net damages to developing countries, since we don’t know whether those damages have, in fact, been incurred. Even if one could assign responsibility for climate change, it does not follow that it would be “fairer” if industrialized nations were to expend resources now on ambitious mitigation measures, based partly on the premise that it would reduce future climate change risks for developing nations. The same resources would, in the short‐ to medium term, provide greater and faster benefits to precisely those nations by reducing existing — and generally larger — climate‐​sensitive risks and vulnerabilities such as hunger, malaria and the threat of cyclones and other extreme events.



The U.N. climatocrats owe it to the people of the developing world to consider these trade‐​offs before they charge ahead with their ambitious new agenda.
"
"

The heinous bombing of the Alfred P. Murrah federal building in Oklahoma City has understandably raised public fears of terrorism. As is common after sensational crimes, some persons have revived their call for a bigger federal government and a narrower Constitution. This Policy Analysis examines various restrictions on civil liberty which have been proposed as a response to Oklahoma City.



Since draconian legislation is sometimes justified as being what the people demand, two points should be kept in mind. First, a large fraction of the population, not just a tiny fringe, is afraid of the federal government. According to a recent Gallup poll, 39 percent of Americans believe “the federal government has become so large and powerful it poses an immediate threat to the rights and freedoms of ordinary citizens.” If the word “immediate” is omitted, 52% of the population is afraid of the federal government.



Contrary to stereotypes about “angry white men,” people fearful of misuse of federal power tend to be female more than male, black more than white, and liberal more than conservative. Repressive measures, rather than reassuring the American public, will intensify the fears which are already widely shared.



Second, in the aftermath of a tragedy, it is not hard for insta‐​polls to report large majorities in favor of various repressive measures, especially when described at a high level of generality, with all the repressive details left out. (For example, “Should the government have more power to keep an eye on terrorist groups?” will obtain higher poll numbers than “Should the government be allowed to infiltrate non‐​violent, non‐​criminal dissident groups and be allowed to use wiretaps without a court order”?) In the long run, public officials are expected to exercise judgement, and not blindly rush into measures which may have short‐​term popularity. The most thorough public opinion survey of Americans’ attitudes towards Constitutional rights included this question: “Suppose the President and Congress have to violate a Constitutional principle to pass an important law the people wanted. Would you support them in this action?” Twenty‐​eight percent said yes, “because the Constitution shouldn’t be allowed to stand in the way of what the people need and want.” Forty‐​nine percent said no, “because protecting the Constitution is more important to the national welfare than any law could possibly be.”(1)



Indeed, the precise reason for putting certain fundamental rights in the Constitution is to protect them from transient majorities.(2) In long‐​term perspective, the herding of American citizens of Japanese descent into concentration camps during WWII was a horrible human rights violation.(3) But at the time, public opinion and the press heavily favored the concentration camps, despite the total lack of evidence that these Americans were disloyal. And certainly national security was in far graver danger in early 1942 than it is today.



But because terrorism, like child abuse (or Communism in previous decades) provokes such intense concerns, there is temptation to be careless in choosing the weapons to fight these evils. From the Alien and Sedition Acts in 1798 to the Palmer Raids in 1920 to McCarthyism in the 1950s, cynical politicians who have manipulated popular fears of aliens and radicals have done tremendous damage to the lives of innocent people and to the fundamental principles of Americanism.



Today, Congress stands poised to repeat the mistakes of the past, as vast numbers of people are smeared with guilt by (very tenuous) association. It is not often that one sees the Presbyterian Church and the American Friends Service Committee standing shoulder‐​to‐​shoulder with the National Rifle Association and Americans for Tax Reform.



That such diverse groups can find common ground, along with other organizations, to warn about the dangers of proposed legislation sharply curtailing civil liberty should indicate just how serious is the current threat to our Constitution.(4) President Clinton was right to characterize the Oklahoma City bombing as “an attack on our way of life.” If Oklahoma City becomes a pretext for the constriction of the Bill of Rights, then we will have handed terrorism a victory which it could never have won for itself.



 **There is No Terrorism Crisis**



“By enabling terrorists to appear much stronger than they really are, the media often find themselves working Pour le roi de Prusse,” observed one historian.(5) Contrary to the imagery of some irresponsible segments of the media (and their Congressional analogues), there is no need to legislate an atmosphere of panic and hysteria. According to the State Department, international terrorist attacks are at their lowest level in 23 years.(6) In the United States in the last eleven years, according to the FBI, there have been only two international terrorist incidents. (One was the World Trade Center bombing; the other was a trespassing incident at the Iranian mission to the United Nations, in which five critics of the Iranian regime took over the mission’s offices, and refused to leave.)(7)



As for incidents of domestic terrorism, there were none in the United States in 1994, nor were there any preventions of terrorist incidents. In 1993, there were 11 incidents classified by the FBI as “terrorist.” Nine of those eleven incidents took place one night in Chicago when animal rights activists set off small incideniary devices in four department stores that sell fur.(8)



Combining domestic and international terrorism, and also accounting for suspected terrorist acts, the total terrorist incident count in the United State is as follows:



 **Terrorist Incidents in the United States**



1994 0 0 1 1993 11 7 2 1992 4 0 0 1991 5 4 1 1990 7 5 1 1989 4 7 16 Of these incidents, only one (the 1993 World Trade Center bombing) was classified as international in origin.(9)



The Oklahoma City bombing was one of the most terrible single crimes in American history, but it was just that: an isolated, single crime. It was emphatically not part of a trend towards increasing terrorism.



 **The British Tragedy**



More government secrecy, more police powers to detain people at will, less governmental accountability, and less freedom are not novel responses to terrorism. They are precisely the approach that has been taken in Great Britain since the early 1970s. The British lesson should be a caution to American politicians who feel confident that the only thing wrong with anti‐​terrorism policy is that the Bill of Rights has been taken too far.



In 1974, Irish Republican Army terrorists bombed pubs in Birmingham, killing twenty‐​one people. Home Secretary Roy Jenkins introduced the Prevention of Terrorism (Temporary Provisions) Bill. Approved without objection in Parliament, the Bill was supposed to expire in one year, but has been renewed every year. The Bill included a smorgasboard of civil liberties restrictions, most of which are now being proposed, with some variation, in the United States.



Under the Bill, the police may stop and search without warrant any person suspected of terrorism. They may arrest any person they “reasonably suspect” supports an illegal organization, or any person who has participated in terrorist activity. An arrested person may be detained up to forty‐​eight hours and then for five more days upon the authority of the Secretary of State.



Of the 6,246 people detained between 1974 and 1986, 87 percent were never charged with any offense. Many detainees reported that they were intimidated during detention and prevented from contacting their families.



The Prevention of Terrorism Bill also makes it illegal even to organize a private or public meeting addressed by a member of a proscribed organization, or to wear clothes indicating support of such an organization.(10)



The Act allows the Secretary of State to issue an “exclusion order” barring a person from ever entering a particular part of the United Kingdom, such as Northern Ireland or Wales. Persons subject to this form of internal exile have no right to know the evidence against them, to cross‐​examine or confront their accusers, or even to have a formal public hearing.(11)



The European Court of Human Rights ruled the Prevention of Terrorism Act to be in violation of Article Five, Section Three of the European Convention on Human Rights, which requires suspects to be “promptly” brought before a judge.(12) Nevertheless, the British government refuses to abandon its preventive detention policy, and evades the European Court’s ruling by invoking Article 15’s provision for countries to ignore the Convention on Human Rights “in time of war or other emergency threatening the life of the nation.”(13)



One of the most important lessons from Britain is that even a huge dose of restrictions on civil liberties, such as the Prevention of Terrorism Bill, does not long remain “sufficient” in the eyes of the government. At least in regard to civil liberties, the Domino Theory has proven correct, as one traditional Anglo‐​American freedom after another has fallen under the government’s assertion of the need for still more anti‐​terrorist powers.



In Northern Ireland the jury has been “suspended” for political violence cases; judges in the Diplock courts hear the cases instead. Confessions are admitted without corroboration. Confessions are extracted through “the five techniques”: wall‐​standing, hooding, continuous noise, deprivation of food, and deprivation of sleep. Convictions may be based solely on the testimony of “supergrasses” (police informers).(14)



In 1988, the Thatcher government enacted additional laws restricting civil liberties. Television stations were forbidden to broadcast in‐​person statements by supporters of a legal political party, Sinn Fein.(15) The ban even applied to rebroadcasts of archive films taped many decades ago, such as footage of Eamon de Valera, the first president of Ireland. A confidential British Broadcasting Corporation memo announced the government’s intention to keep journalists from broadcasting any statement by U.S. Senator Edward Kennedy supporting Sinn Fein.(16) The BBC also banned Paul McCartney’s “Give Ireland Back to the Irish,” and a song by another group urging the release from prison of the Guildford Four (discussed below).(17)



A suspect’s decision to remain silent under interrogation may now be used against him in court. Although terrorism in Northern Ireland was the stated basis for the change, the change will also apply in England and Wales. No‐ one who has seen Great Britain’s slide down the slippery slope can feel confident that repressive measures introduced solely for terrorism will not eventually seep into the ordinary criminal justice system.



 **Wiretaps do not even need judicial approval.(18)**



The Security Service Act of 1989 provides: “No entry on or interference with property shall be unlawful if it is authorized by a warrant issued by the secretary of state.” If committed pursuant to an order from the secretary of state, acts such as theft, damage to property, arson, procuring information for blackmail, and leaving planted evidence are not crimes.(19)



As in America, gun prohibitionists have hitched their wagon to “anti‐​terrorism,” with little regard for an actual terrorist nexus. Although British laws regarding possession of actual firearms were already quite severe, the Firearms Act of 1982 introduced restrictive licensing for imitation firearms which could be converted to fire live ammunition.(20) The sponsor of the new law against imitation firearms promised that it would help stem “the rising tide of crime and terrorism”–although there had never been a crime or terrorist act committed with a converted imitation weapon.(21) The first time the Prevention of Terrorism Act was used was after another pub bombing, in the English town of Guildford. Four people were arrested, held incommunicado in prison for a week, and coerced into false confessions by administration of drugs and by threats against their families. While the “Guildford Four” were being held, the police used the time to fabricate evidence against them.



Although members of the Irish Republican Army already in prison confessed to the Guildford bombings, the Guildford Four were tried, convicted, and sentenced to life in prison.



Several leading English statesmen, including Roy Jenkins, felt that the defendants had been framed. A campaign to free them continued for fifteen years, until, upon discovery of police notes of fabrication of evidence, the Guildford Four were released from prison.(22)



The Birmingham bombings that had led to the Prevention of Terrorism Act resulted in the conviction of a group of defendants called the “Birmingham Six.” Amnesty International charged that their confessions were extracted under torture. The forensic scientist whose testimony convicted the Birmingham Six later admitted that he lied in court. The Birmingham Six confessed while being held incommunicado by the police; the various confessions were so factually inconsistent that they could not have been true.



(Civil libertarians fear that the Birmingham case is only one of many instances of police obtaining coerced confessions.(23))



The Birmingham Six were also eventually freed.



Britain, fortunately, has no death penalty. In America, where before anyone had even been indicted President Clinton announced that the perpetrators of the Oklahoma City bombing should be executed, the federal death penalty would mean that vindication of persons wrongfully convicted of terrorism might be post‐​mortem.



To state the obvious, all the legislation has hardly immunized Britain from terrorism. But Britain has, in two decades, eviscerated the magnificent structure of liberty and limited government that took over a millennium to construct. For centuries, “the rights of Englishmen” were proudly held up in contrast to the absolutism of the Continent. Far from being an examplar to the world, the modern “anti‐​terrorist” United Kingdom has been found culpable of human rights violations under the European Convention on Human Rights more often than any other member of the Council of European States.(24) To a student of Britain’s magnificent history in the story of freedom, it is a pitiful sight to see modern Britons forced to turn to Brussels and the European Court of Human Rights as the last protector of what were formerly the unquestioned rights of Englishmen.



Britain was once the freest nation in the world; today, it is one of the unfreest in Western Europe. As Britain illustrates, no matter how great a country’s tradition of freedom, freedom can be lost in less than a generation if public officials, and the public, allow terrorism to destroy their traditional way of life.



 **Weakening Restraints on FBI Political Surveillance**



Within days after the Oklahoma City bombings, conservative talk show host Rush Limbaugh began casting blame on civil libertarians such as former Ohio senator Howard Metzenbaum who had promoted strict guidelines on FBI surveillance of dissident groups in the United States.(25) Other persons have also called for abolition of the remaining limitations on FBI investigations.



First of all, there is at present no evidence that the FBI wanted to spy on anyone suspected in Oklahoma City bombing, but was prevented from doing so by the current guidelines. Thus, persons demanding the abolition of FBI guidelines are demanding a “solution” for which there is no demonstrated problem.



Second, the FBI guidelines exist for a very good reason. Before the guidelines were implemented, the FBI spied on literally hundreds of thousands of Americans who were doing nothing more than exercising their Constitutional right to question government policies. Victims of these abuses ranged from Dr. Martin Luther King, Jr., to the Ku Klux Klan, to the Congress on Racial Equality and the civil rights movement. The Counter‐​intelligence Programs (COINTELRPO) invaded the Constitutional rights of American people who simply were expresssing in public what Secretary of Defense Robert McNamara had concluded in private. Far from being confined to a single type of dissident, or to a few years of excess, FBI abuses dated back to the 1940s and were pervasive until brought to light by fifteen months of hearings before Senator Frank Church’s special committee in 1975–76. Altogether, there were 675 FBI operations against civil rights, white supremacist, or anti‐​war groups, which led to only four convictions.(26)



Even after all the public hearings, and the implementation of guidelines, the FBI continued to abuse the rights of dissident Americans, through a massive surveillance of people in CISPES (Committee in Solidarity with the People of El Salvador) who opposed to President Reagan’s policy in El Salvador in the mid‐​1980s. The CISPES investigation, justifiably regarded today as shameful, would have been lawful if the anti‐​terrorism bills current being considered had been law.



The first set of FBI guidelines were implemented by President Ford’s attorney general Edward Levi in 1976. In 1983, the “Levi guidelines” were replaced by President Reagan’s attorney general William French Smith. These “Smith guidelines” were far less restrictive. FBI director William Webster stated that the Smith guidelines “should eliminate any perception that actual or imminent commission of a violent crime is a prerequisite to investigation.” Thus, the recent highly‐​publicized claim of a former FBI official “you have to wait until you have blood in the streets before the bureau can act” is patent nonsense.(27)



In fact, the Reagan/​Smith guidelines, which are still in force, nowhere require the completion of a violent crime. Rather they state that a:



domestic security/​terrorism investigation may be initiated when facts for circumstances reasonably indicate that two or more persons are engaged in an enterprise for the purpose of furthering political or social goals wholly or in part through activities that involve force or violence and a violation of the criminal laws of the United States.



Specifically, the guidelines already allow investigations based upon mere words:



When, however, statements advocate criminal activity or indicate an apparent intent to engage in crime, particularly crimes of violence, an investigation under these Guidelines may be warranted unless it is apparent from the circumstances or the context in which the statements are made, that there is no prospect for harm.



While the Smith guidelines would prevent infiltration of Second Amendment groups simply because they are sharply critical of government policy, the guidelines do not now prevent infiltration of groups which actually threaten violence. For example, in Virginia, a group of fifteen men who allegedly wanted to resist the federal government managed only three meetings before being arrested for weapons violations as a result of government infiltrator’s secret tape recordings.(28)



Rather than being obliterated, guidelines on FBI domestic surveillance should be brought up to full strength.



A statutory version of the Levi guidelines should be enacted.



Persons who eager to “unleash” the FBI against dissident groups who are not threatening illegal activity might first want to go through the mental exercise of imagining their worst nightmare as President. Liberals might imagine Pat Buchanen or Pat Robertson. Conservatives could imagine Dianne Feinstein or Jesse Jackson. In such a scenario, would we want the FBI free to spy on whomever the President does not like? Under Presidents Nixon, Johnson, and Kennedy, who were far more moderate than Jesse Jackson or Pat Buchanan, the FBI did so, with baleful results.



An official at the Treasury Department, who works closely with the BATF, warned that there is “a tremendous potential for abuse” in administration proposals to loosen controls on the FBI.(29)



It must be remembered that many of America’s greatest organizations were, in their day, radical extremists. The abolitionists were extremists, as were the suffragettes, the civil rights movements, and many of the opponents of the War in Vietnam. If these groups seem vindicated by history, they were bitterly attacked in their day as radical and anti‐​American.



Finally, before any additional powers are granted to the FBI, it is appropriate to investigate FBI abuses of existing powers, including the events in Waco.(30) At the least, it is well‐​established that the FBI used a chemical warfare agent which is banned in international warfare, against children indoors, even though Army and manufacturer manuals specifically warn that the agent indoors is flammable, and can severely injure unprotected children. In securing Attorney General Reno’s consent, the FBI falsely told her that the chemical warfare agent was “a mild form of teargas.” The FBI also ignored the advice of its own behavioral experts, and pressured at least one of them to reverse his advice, so as to justify an assault. This fact too was concealed from the Attorney General.



 **FBI Foreign Jurisdiction**



It has been proposed that the FBI’s foreign jurisdiction be expanded. Firstly, the expansion is unnecessary, since the CIA can operate overseas against terrorists. Second, allowing domestic American law enforcement agents to operate on foreign soil against foreign soil against foreign citizens creates a dangerous precedent, and will inevitably lead to demands for reciprocity. Do we really want the Russian secret police, or even the Mexican federales, operating on American soil? The Clinton bill also removes most of the limitations regarding use (including overseas) of American trainers for foreign law enforcement, and removes the restriction against American tax dollars being used to pay the salaries of foreign police.(31) Internationalizing criminal law is even more dangerous to civil liberty than is federalizing it.



 **Felonizing Support for Peaceful Activities of Foreign Organizations**



 **Presidential Designation of “Terrorist” Groups**



The Clinton and Dole bills empower the President to designate “foreign terrorist” organizations which are illegal for Americans to provide any “material support.”(32) Recently, the Clinton administration has retreated from its insistance that the Presidential designation be unreviewable. At the least, the potential for judicial review will reduce the risk of the terrorist designation being used against domestic dissident groups. (Since they would be able to show in court that they were not foreign.) But it should be remembered that American courts have historically been extremely deferential to Presidential foreign policy decisions. If there were even a scintilla of evidence in favor of the President’s designation of a foreign group as “terrorist,” then it is virtually certain that courts would not overturn the designation.



Again, the reader might consider imagining this legislation in the hands of one’s worst political nightmare.



An organization which provides support to the government of Israel or to the Israeli Defense Forces (which are considered “terrorist” in some political circles) could be outlawed, as could (by a different President) a group which provides support to Palestinian refugees.



 **Material Support**



Current federal law appropriately forbids the providing of material support to any foreign terrorist organization.(33) The law forbids investigations of people for violating this law unless there is some reasonable suspicion that they have violated or may violate the law.



The restriction should of course be retained; targetting people for FBI investigations when there is not a scintilla of suspicion is not only an invitation to harassment of dissidents, it is a waste of law enforcement resources.



One important distinction between the Clinton and Dole bills is that the Dole creates an explicit exception to the “material support” statute: “ ‘Material support’…does not include humanitarian assistance to persons not directly involved in such violations.”(34) Thus, sending a Christmas food package to an I.R.A. or A.N.C. prisoner would constitute material support, but giving money to a fund which assisted the orphaned children of I.R.A. or A.N.C.



members would not be, under the Dole approach.



Under the Clinton bill, however, the donor to the I.R.A. orphanage would be a federal felon, subject to ten years in prison, as would be a person who spent five dollars to attend a speech of a visiting lecturer from the African National Congress.



When pressed about this fact at recent Congressional hearings, a Clinton administration spokesperson acknowledged that minor support for the A.N.C.‘s peaceful activities could have been felonized, but that the American people should simply trust the President not to abuse the immense power which President Clinton was requesting.



But as President Lyndon Johnson put it: “You do not examine legislation in light of the benefits it will convey if property administered but, in light of the wrongs it would do and the harms it would cause if improperly administered.”



The “terrorism” bills’ overbreadth is astonishing. The Palestine Liberation Organization is permanently defined as a terrorist organization by the proposal, no matter what its future conduct.(35) Thus, if the P.L.O. should live up the peace treaty that it signed with Israel, President Clinton would be guilty of providing “material support” to a terrorist organization should he invite Yassir Arafat to the White House and give him a free meal and a night’s lodging.



 **Licensed Donations**



Theoretically, a license can be procured allowing humanitarian contributions to the blacklisted group. The licensing procedure is, however, very difficult to comply with. Not only does recipient group have to open its books to the Treasury Department, so does the donor. In other words, if a person wants to make a $50 contribution to buy clothes for Palestinian orphans, the person must make his financial records open for inspection, and be able to show “the source of all funds it receives, expenses it incurs, and disbursements it makes.”(36) There is no limitation that the complete accounting of receipt, expenses, and disbursements be limited to the charitable donation. Virtually no‐​one in the United States keeps such detailed records. Knowing that a charitable donation to a politically blacklisted group would expose the donor to a nightmare audit, few donors would be courageous or foolish enough to give anyway.



In addition to criminal penalties of up to ten years in prison, civil fines of $50,000 per offense may be imposed, and in civil prosecutions, the government may, upon approval of the court, introduce secret, classified evidence which remains hidden from the defendant.(37) (The Clinton and Dole bills grant similar authority to use secret evidence in proceedings under the International Emergency Economic Powers Act, which gives the President unilateral authority to regulate or prohibit all foreign exchange transactions, all imports and exports of securities and currency and foreign currency transactions, and all banking transactions involving foreigners.(38))



 **The Constitutional View**



The Constitution mandates that if a person is to be punished for association with a group which has unlawful objectives, the government must prove that the individual specifically intended to further the unlawful objectives.(39) What the Clinton/​Dole bills propose is a return to practices which the Supreme Court outlawed over half a century ago.



Then, the Immigration and Naturalization Service attempted to deport labor organizer Harry Bridges because of his affiliation with the Communist party. Bridges had supported only lawful Communist activities, rather than the party’s unlawful ends. The INS argued that if an organization had unlawful purposes, the fact that a supporter had supported only lawful purposes was irrelevant. The Supreme Court disagreed, and dismissed the case.(40)



More recently, the Court declared unconstitutional a law that was “a blanket prohibition of association with a group having both legal and illegal aims.” Unless there was proof that the defendant specifically intended to support the group’s illegal aims, the prohibition was a violation of “the cherished freedom of association protected by the First Amendment.”(41)



 **Defining Everything as “Terrorism”**



Current federal law already provides a comprehensive, realistic definition of “terrorist activity.”(42) Some proposals define virtually any crime as “terrorism.” For example, the Clinton and Dole “terrorism” bills define as “terrorism” virtually every violent or property crime, whether or not related to actual terrrorism. The bills impose a prison terms of up to twenty‐​five years (for property damage, more for violent crimes) for “terrorist” offenses which are defined as follows: any assault with a dangerous weapon, assault causing serious bodily injury, or any killing, kidnapping, or maiming, OR any unlawful destruction of property.(43) Snapping someone’s pencil, breaking someone’s arm in a bar fight, threatening someone with a knife, or burning down an outhouse would all be considered “terrorist” offenses. Any attempt to perpetrate any of these terrorist crimes would be subject to the same punishment as completed offense.



Even a threat to commit the offense (i.e. “One of these days, I’m going to snap your pencil.”) is a felony subject to ten years in federal prison.(44) Again, the extra federal power granted by the legislation is superfluous to genuine anti‐​terrorism. It is already a serious federal felony to make a real terrorist threat, as by threatening to set off a bomb, or to assassinate the President.(45)



In order for the offense to be considered “terrorism,” all that would be necessary would be jurisdictional predicate that would cover almost every crime. The jurisdictional predicate requires one of any of the following: the crime “affects commerce in any way” (not necessarily interstate commerce); the criminal used “any facility used in any manner in commerce”; the victim was “traveling in commerce” (again, not necessarily interstate); the victim was a federal employee, or the property damaged was federal; the victim was not an American national; or any of the offenders “travels in commerce.”(46) If anyone involved in the crime meets the jurisdictional predicate, then jurisdiction is invoked for the entire crime.(47)



Finally, in order for a prosecution to take place, the Attorney General must certify in writing that the offense “transcended national boundaries” and was intended to intimidate a foreign government or “a civilian population, including any segment thereof.”(48) There is no provision for review of whether the Attorney General’s certification was even remotely accurate. Nor is there any requirement that there be an actual international border crossing.



Just because the law allows it, the federal government probably will not prosecute every Canadian tourist who snaps a policeman’s pencil or everyone who scratches anti‐​war graffiti on post office tables. The proponents of these bills may expect that the essentially limitless discretion granted to the federal government will not be abused. But a fundamental principle of American law has always been that the law should control the government; citizens should not be at the mercy of the good judgement of government officials. As the Supreme Court put it, “It could certainly be dangerous if the legislature could set a net wide enough to trap all possible offenders, and leave it to the courts to step inside and say who could rightfully be detained, and who should be set a large.”(49)



The justification for federalizing all of the criminal law is that such federalization is necessary to make sure that every possible terrorist crime is covered. For example, it is asserted that the bombing of a Jewish hospital in, for example, St. Louis, might not be covered by current federal law. In fact, the federal arson statute has successfully been applied to the burning of a trailer that was hooked up to a power system which was part of the interstate electricity grid.(50) Thus, the fact that the hospital drew power from the same electrical grid would justify application of the current federal arson law, without the need for a new statute. Even if it is possible to imagine some bizarre hypothetical crime that would not be covered by the (very expansive) interpretation of current federal criminal statutes, every conceivable terrorist crime is subject to severe punishment under current state criminal laws.



The dangers posed by the hidden federalization of the entire criminal law (all the way down to petty vandalism) become all the greater when coupled with the bill’s other provisions to make the overbroad federal RICO,(51) money laundering,(52) and wiretapping laws(53) applicable to “terrorist” offenses and to authorize use of the military in domestic law enforcement for “terrorism.”(54) No bail is allowed even if it is uncontroverted that the accused will not flee and will pose no danger to anyone.(55)



Likewise, mandatory prison sentences, with no possibility of probation, are required for “terrorist” crimes, no matter what the circumstances.(56)



Having used state law definitions to define petty property crimes as “terrorism,” the bills then forbid defendants from invoking state constitutional law protections of the state where the alleged offense took place.(57)



Turning every state and local petty property crime (or even a local violent crime) into a federal felony may be unconstitutional, as the Supreme Court recently ruled in the Lopez “gun‐​free‐​school‐​zones” case. Putting aside questions of Constitutionality, it is inappropriate that the draconian federalization of state crimes be pushed through Congress under the mask of anti‐​terrorism.



 **Resisting Foreign Dictatorships**



Solicitude for foreign governments should not blind us to the fact that most governments in the world are dictatorships. Under the principles on which America is based, governments without the consent of the governed have no legitimacy, and it is the right of the people of that nation to overthrow the dictatorship.



Yet the Clinton and Dole bills define as “terrorism” any act which plans the destruction of government property in foreign nation with which the United States is “at peace.”(58) Thus, if Chinese refugees living in the United States planned a jailbreak to liberate political prisoners in China, they would be guilty of “terrorism.” If Americans in 1940 had plotted the destruction of railways leading to Nazi concentration camps, they too would have been guilty of “terrorism.” And so would the countless American Jews who smuggled firearms to the Jewish resistance movement in Palestine in the 1940s, making possible the eventual establishment of the state of Israel. Had such a “terrorism” law been universal in 1776, the Dutch, French, and other private citizens who provided material assistance to the American Revolution (even though their governments were at peace with the British Empire) would have been “terrorists” too. It ill becomes a nation which was born in violent revolution with foreign assistance to felonize the very types of charity which allowed our own nation to become free. Resistance to dictatorships and empires is not terrorism.



 **Wiretapping**



Various proposals have been offered to expand dramatically the scope of wiretapping. For example, the Clinton bill defines almost all violent and property crime (down to petty offenses below misdemeanors) as “terrorism” and also allow wiretaps for “terrorism” investigations.(59)



Other proposals would allow wiretaps for all federal felonies, rather than for the special subet of felonies for which wiretaps have been determined to be especially necessary. Notably, wiretaps are already available for the fundamental terrorist offenses: arson and homicide. Authorizing wiretaps for evasion of federal vitamin regulations, gun registration requirements, or wetlands regulations is hardly a serious contribution to antiterrorism, but amounts to a bait‐​and‐​switch on the American people.



Currently, FBI wiretapping, bugging, and secret break‐ ins of the property of American groups is allowed after approval from a seven‐​member federal court which meets in secret.(60) Of the 7,554 applications which the FBI has submitted in since 1978, 7,553 have been approved.(61)



Making the request for vast new wiretap powers all the more unconvincing is how poorly wiretap powers have been used in the past. Terrorists are, of course, already subject to being wiretapped. Yet as federal wiretaps set new record highs every year, wiretaps are used almost exclusively for gambling, racketeering, and drugs. The last known wiretap for a bombing investigation was in 1998. Of the 976 federal electronic eavesdropping applications in 1993, not a single one was for arson, explosives, or firearms, let alone terrorism. From 1983 to 1993, of the 8,800 applications for eavesdropping, only 16 were for arson, explosives, or firearms.(62) In short, requests for vast new wiretapping powers because of terrorism are akin to a carpenter asking for a pile driver to hammer a nail, while a hammer lies nearby, unused.



Even more disturbing than proposals to expand the jurisdictional base for wiretaps are efforts to remove legal controls on wiretaps. For example, wiretaps are authorized for the interception of particular speakers on particular phone lines. If the interception target keeps switching telephones (as by using a variety of pay phones), the government may ask the court for a “roving wiretap,” authorizing interception of any phone line the target is using. Yet while roving wiretaps are currently available when the government shows the court a need, the Clinton and Dole bills allow roving wiretaps for “terrorism” without court order.(63) (Again, remember that both bills define “terrorism” as almost all violent or property crime.)



The Foreign Intelligence Surveillance Act (FISA) provides procedures for authorizing wiretaps in various cases. These procedures have worked in the most serious foreign espionage cases.(64) Yet the Clinton and Dole bills would authorize use of evidence gathered in violation of FISA in certain deportation proceedings.



 **Warrantless Data Gathering**



Proposals have also been offered to require credit card companies, financial reporting services, hotels, airlines, and bus companies to turn over customer information whenever demanded by the federal government.(65) Document subpoenas are currently available whenever the government wishes to coerce a company into disclosing private customer information. Thus, the proposals do not increase the type of private information that the government can obtain; the proposals simply allow the government to obtain the information even when the government cannot show a court that there is probable cause to believe that the documents contain evidence of illegal activity.(66)



Similar analysis may be applied to proposals to increase the use of pen registers (which record phone numbers called, but do not record conversations, and thus do not require a warrant). If a phone company has a high enough regard for its customers’ privacy so as to not allow pen registers to be used without any controls, the government may obtain a court order to place a pen register. Business respect for customer privacy ought to be encouraged, not outlawed.



 **Curtailing First Amendment Rights of Computer Users**



For some government agencies, the Oklahoma City tragedy has become a vehicle for enactment of “wish list” legislation that has nothing to do with Oklahoma City, but which it is apparently hoped the “do something” imperative of the moment will not examine carefully.



One prominent example is legislation to drastically curtail the right of habeas corpus.(67) Although Supreme Court decisions in recent years have already sharply limited habeas corpus,(68) prosecutors’ lobbies want to go even further. Two obvious points should be made: First, habeas corpus has nothing to do with apprehending criminals; by definition, anyone who files a habeas corpus petition is already in prison. Second, habeas corpus has nothing to do with Oklahoma City in particular, or terrorism in general.



A second example, of piggybacking irrelevant legislation designed to reduce civil liberties are current FBI efforts to outlaw computer privacy.



If a person writes a letter to another person, he can write the letter in a secret code. If the government intercepts the letter, and cannot figure out the secret code, the government is out of luck. These basic First Amendment principles have never been questioned.



But, if instead of writing the letter with pen and paper, the letter is written electronically, and mailed over a computer network rather than postal mail, do privacy interests suddenly vanish? According to FBI director Louis Freeh, the answer is apparently “yes.”



Testifying before the Senate Judiciary Committee about Oklahoma City, director Freeh complained that people can communicate over the internet “in encrypted conversations for which we have no available means to read and understand unless that encryption problem is dealt with immediately.”(69) “That encryption problem” (i.e. people being able to communicate privately) could only be solved by outlawing high quality encryption software like Pretty Good Privacy”.



First of all, shareware versions of Pretty Good Privacy are ubiquitous throughout American computer networks. The cat cannot be put back in the bag. More fundamentally, the potential that a criminal, including a terrorist, might misuse private communications is no reason to abolish private communications per se. After all, people whose homes are lawfully bugged can communicate privately by writing with an Etch-a-Sketch”.(70) That is no reason to outlaw Etch‐ a‐​Sketch.



Although Mr. Freeh apparently wants to outlaw encryption entirely, the Clinton administration has been proposing the “Clipper Chip.” The federal government has begun requiring that all vendors supplying phones to the federal government include the “Clipper” chip. Using the federal government’s enormous purchasing clout, the Clinton administration is attempting to make the Clipper Chip into a de facto national standard.(71)



The clipper chip provides a low level of privacy protection against casual snoopers. But some computer scientists have already announced that the chip can defeated. Moreover, the “key”–which allows the private phone conversation, computer file, or electronic mail to be opened up by unauthorized third parties–will be held by the federal government.



The federal government promises that it will keep the key carefully guarded, and only use the key to snoop when absolutely necessary. This is the same federal government that promised that social security numbers would only be used to administer the social security system, and that the Internal Revenue Service would never be used for political purposes.



Proposals for the federal government’s acquisition of a key to everyone’s electronic data, which the government promises never to misuse, might be compared to the federal government’s proposing to acquire a key to everyone’s home.



Currently, people can buy door locks and other security devices that are of such high quality that covert entry by the government is impossible; the government might be able to break the door down, but the government would not be able to enter discretely, place an electronic surveillance device, and then leave. Thus, high‐​quality locks can defeat a lawful government attempt to bug someone’s home, just as high‐​quality encryption can defeat a lawful government attempt to read a person’s electronic correspondence or data.



Similarly, it is legal for the government to search through somebody’s garbage without a warrant; but there is nothing wrong with privacy‐​conscious people and businesses using paper shredders to defeat any potential garbage snooping. Even if high‐​quality shredders make it impossible for documents to be pieced back together, such shredders should not be illegal.



Likewise, while wiretaps or government surveillance of computer communications may be legal, there should be no obligation of individuals or businesses to make wiretapping easy. Simply put, Americans should not be required to live their lives in a manner so that the government can spy on them when necessary.



Thus, although proposals to outlaw or emasculate computer privacy are sometimes defended as maintaining the status quo (easy government wiretaps), the true status quo in America is that manufacturers and consumers have never been required to buy products which are custom‐​designed to faciliate government snooping.



The point is no less valid for electronic keys than it is for front‐​door keys. The only reason that electronic privacy invasions are even discussed (whereas their counterparts for “old‐​fashioned” privacy invasions are too absurd to even be contemplated), is the tendency of new technologies to be more highly restricted than old technologies. For example, the Supreme Court in the 1920s began allowing searches of drivers and automobiles that would never have been allowed for persons riding horses.



But the better Supreme Court decisions recognize that the Constitution defines a relationship between individuals and the government that is applied to every new technology. For example, in United States v. Katz, the Court applied the privacy principle underlying the Fourth Amendment to prohibit warrantless eavesdropping on telephone calls made from a public phone booth– even though telephones had not been invented at the time of the Fourth Amendment.(72) Likewise, the principle underlying freedom of the press– that an unfettered press is an important check on secretive and abusive governments–remains the same whether a publisher uses a Franklin press to produce a hundred copies of a pamphlet, or laser printers to produce a hundred thousand. Privacy rights for mail remain the same whether the letter is written with a quill pen and a paper encryption “wheel,” or with a computer and Pretty Good Privacy.



Efforts to limit electronic privacy will harm not just the First Amendment, but also American commerce. Genuinely secure public‐​key encryption (like Pretty Good Privacy) gives users the safety and convenience of electronic files plus the security features of paper envelopes and signatures. A good encryption program can authenticate the creator of a particular electronic document–just as a written signature authenticates (more or less) the creator of a particular paper document.



Public‐​key encryption can greatly reduce the need for paper. With secure public‐​key encryption, businesses could distribute catalogs, take orders, pay with digital cash, and enforce contracts with veriable signatures–all without paper.



Conversely the Clinton administration’s weak privacy protection (giving the federal government the ability to spy everywhere) means that confidential business secrets will be easily stolen by business competitors who can bribe local or federal law enforcement officials to divulge the “secret” codes for breaking into private conversations and files, or who can hack the clipper chip.



 **The New Star Chamber**



Although the United States has suffered exactly one alien terrorist attack in the last eleven years, special harsh rules for aliens are at the top of the “antiterrorism” agenda. The most ominous proposals are those that allow secret evidence for deportation cases in which the government asserts that secrecy is necessary to the national security.(73) Georgetown University Law Professor David Cole calls the secret court the new “Star Chamber,” since its powers resemble those of the inquisitorial court which the British monarchy, in violation of the common law, used to terrorize dissident subjects. Star Chamber was one of the most hated abuses of the British government.



Modern Star Chamber proceedings are to be before a special court (one of five select federal district judges)(74), after a an ex parte, in camera showing that normal procedures would “pose a risk to the national security of the United States.”(75) Based upon further ex parte, in camera motions, evidence which the government does not which to disclose may be withheld from the defendant, who will instead be provided a general summary of what the evidence purports to prove. In other words, secret evidence may be used.(76) Of course any of the “showings” that the government makes in camera and ex parte may be based on allegations regarding the unreviewable claims of a secret informant.



Wiretap evidence is usable even if it was illegally obtained.(77) Normal procedural rules allowing for disclosure of circumstances relating to illegally obtained evidence are abolished.(78)



Legal aliens do not, of course, have the full scope of Constitutional rights guaranteed to American citizens; for example, they cannot exercise rights associated with citizenship, such as voting, or serving on a jury. But it is well‐​settled that legal aliens enjoy the same right to freedom of speech as do citizens. Likewise, legal aliens have always been accorded the same due process protections in criminal cases. After all, the Fifth Amendment’s guarantee of Due Process protects “all persons,” not just “all citizens.”(79)



Procedures like those proposed in the Clinton and Dole bills have already been found unconstitutional. As the District of Columbia Court of Appeals, put it:



Rafeedie–like Joseph K. in The Trial–can prevail before the [INS] Regional Commmissioner only if he can rebut the undisclosed evidence against him, i.e., prove that he is not a terrorist regardless of what might be implied by the government’s confidential information. It is difficult to imagine how even someone innocent of all wrongdoing could meet such a burden.(80)



The argument for allowing secret evidence in deportation proceedings is that otherwise the identity or operational mode of a confidential informant might be jeopardized. First of all, the very purpose of the Constitution’s Confrontation Clause is to prevent people’s lives from being destroyed by the type of secret accusations which had characterized the European justice systems.



Moreover, the argument against endangering the secrecy of confidential accusers in deportation cases proves too much. The very same argument applies in every other case, including criminal violence or drug sales cases. Obeying the Confrontation Clause in those cases may likewise impede the short‐​term interests of law enforcement. But the Constitution has conclusively determined that a criminal justice system without a right of confrontation poses a far greater long‐​term risk to public safety than does requiring the government to disclose the reason why it wants to imprison, execute, or deport someone.



Simply put, confidential informants often lie.



Informants are rarely good citizens who come forward to help prevent a crime. Rather, informants are criminals who have been caught, and have turned informant in order to protect themselves from prosecution; informants have every reason to lie and falsely accuse people.(81)



Confidential informants who are not professional criminals may have other reasons for lying. The type of miscarriage of justice that can occur based on confidential informants was illustated in 1950 case, in which the Supreme Court held that secret evidence could be used to prevent an alien from entering the United States.(82) (She was married to an American.) When the alien was granted a hearing, it was discovered that the confidential informant was her husband’s angry ex‐​girlfriend.



Some persons who would oppose Star Chamber proceedings for criminal trials might approve of such procedures in deportation hearings since deportation is, under most circumstances, a less severe sanction than prison. Yet if the alien cannot find a country that wants to take him (or if the State Department can quietly convince other countries not to take him), then the alien may be imprisoned for the rest of his life in the United States, at the sole discretion of the Attorney General, without even the right to ask for a writ of habeas corpus based on governmental violation of statutes.(83)



Finally, some persons may accept Star Chamber for legal resident aliens under the presumption that such procedures would never be used against American citizens. Yet if there is anything the experience of Great Britain proves, it is that special, “emergency” measures implementented in a limited jurisdiction (such as Northern Ireland) soon spread throughout the nation. Cancers always start small. If one international terrorist incident in eleven years is a sufficient interest to justify a Star Chamber for certain terrorism suspects, then it is hard to resist the logic that crimes which actually are widespread (such as homicide, rape, or drug trafficking) should be entitled to their own Star Chamber.



 **More Informants**



One of the reasons that many people are so frightened of the federal government is how it already uses informants to attempt to infiltrate suspicious organizations. One of the most notorious cases which helped create the militia movement was started by the attempt to creat an informant.



Randy Weaver was a white separatist who lived with his family in a remote cabin in northern Idaho. There was no indication that he had ever advocated or participated in illegal violence. When he was approached by federal agents who wanted him to infiltrate violent white supremacist groups and serve as an informer, he refused. He was later entrapped (a jury later found) by repeated pestering from undercover agents into selling undercover BATF agents two shotguns whose barrels had been shortened (at the request of the undercover agents) to a fraction of an inch below the 18″ legal limit. Weaver failed to appear for a court hearing resulting from the illegal firearms sale; as it later turned out, the order to appear which had been mailed to him gave an incorrect date for the hearing. A fugitive arrest warrant was issued for Weaver.



United States Marshals showed up one day in August 1992. The Weavers’ three dogs (two collies and a labrador) began barking, and Randy Weaver, his friend Kevin Harris, and Weaver’ fourteen‐​year‐​old son Sammy grabbed their guns to run and investigate.



The Marshals, wearing camouflage and carrying silenced machine guns, did not identify themselves or their purpose, but they did shoot one of the dogs. Sammy Weaver returned fire, and was promptly shot by a Marshal. Sammy turned and fled, with his nearly severed arm flopping as he ran. Sammy was promptly shot in the back. Nearby, Kevin Harris concluded that if he fled, he too would be shot; Harris fired his rifle in the direction of the marshal who had shot Sammy; the bullet killed the marshal who had shot Sammy Weaver.



Randy Weaver had only heard the shooting, but had not seen what had happened. “Come on home, Sam. Come home,” he yelled over and over. At last, Sammy called “I’m coming, Dad.” Those were apparently the last words Sammy Weaver said before he died.



Harris’s shot had disordered the Marshals, and Weaver and Harris used the opportunity to retreat to their cabin.



Later that day, Randy Weaver and his wife Vicki picked up Sammy’s dead body and carried it to a building near the cabin, where they prepared their son’s body for burial.



Over 300 government agents, led by the FBI “Hostage Rescue Team” descended on Ruby Ridge, Idaho, where Weaver’s two‐​story cabin was located. Commanding the FBI at Ruby Ridge was Richard M. Rogers, who would later serve as a field commander at Waco.(84)



The FBI rules of engagement allow use of deadly force only when necessary to protect an innocent person from imminent peril. But on the plane out to Idaho, Rogers wrote new rules of engagement for Ruby Ridge. The new rules allowed FBI snipers to shoot any adult who was armed. Since virtually everyone besieged in Idaho went outside armed (in full compliance with the laws of Idaho, and of most other states, because the armed people were on their own property) everyone was a target outside the cabin.



At Weaver’s trial in 1993, HRT Director Rick Rogers was unable to cite any authority allowing the FBI, in violation of state law, to shoot people who were posing no threat to anyone. (A provision in the 1994 federal crime bill, removed during the bill’s final movement through Congress, would have immunized federal agents from state criminal prosecution for crimes committed while on the job.)



As at Waco, a siege ensued, with the “Hostage Rescue Team” surrounding the residence of people who, far from being held hostage, simply wanted to be left alone.



At about six p.m. the next day, sixteen‐​year‐​old Sarah Weaver, her father Randy, and Kevin Harris walked out to the nearby shed to pay their last respects to Sammy. They were carrying firearms. Standing by the open door was Mrs. Vicki Weaver, holding her 10 month old daughter Elisheba.



FBI sniper Lon T. Horiuchi said that he could hit a quarter at 200 yards. Horiuchi fired, and hit Randy Weaver in the shoulder. Horiuchi later testified that Weaver was shot to keep Weaver from shooting at a helicopter overhead.



At the subsequent trial, Associate Marshal Service Director Wayne Smith testified that no helicopter was over the Weaver cabin that day, and the judge threw out the charge that Weaver had aimed a firearm at a helicopter. Sarah Weaver, Randy Weaver, and Kevin Harris fled back towards the cabin.



Sniper Horiuchi fired again, this time at a person he said he thought was Kevin Harris. (Although Harris was not even alleged to have raised any gun at any helicopter.) Horiuchi later testified that he could not identify his target clearly because he could not see through the curtains of the door. After Horiuchi had testified, the government (illegally late) turned over Horiuchi’s official report of the shooting; the drawing showed two figures standing in an open door.(85)



The FBI sniper’s .308 slug crashed into Vicki Weaver’s head with such force that skull bone fragments ricocheted into Harris, as the slug exited her body and entered his.(86) Vicki Weaver’s body fell to its knees, and her head came to rest on the floor, like a person at prayer. Randy Weaver took baby Elisheba from her arms, and lifted his wife’s head; half her face had been blown away. Her dead body was laid out on the cabin floor, and covered with a blanket.



An FBI psychological profile, prepared before the attack, called Vicki Weaver the “dominant member” of the family, thus implying that if she were “neutralized” everyone else might surrender.(87)



During the next week, “the FBI used the microphones to taunt the family. ‘Good Morning Mrs. Weaver. We had pancakes for breakfast. What did you have?’ asked the agents in at least one exchange. Weaver’s daughter Sarah, 16, said the baby, Elisheba, often was crying for its mother’s milk when the FBI’s messages were heard.”(88)



Bo Gritz, a highly‐​decorated American soldier in Vietnam, who is now a talk‐​show host and a right€wing political figure, offered to try to negotiate with Weaver.



Eight days after Vicki Weaver was shot, Gritz succeeded in convincing Weaver to surrender based on a promise that Weaver could meet with famed criminal defense attorney Gerry Spence.



Spence agreed to take the case pro bono, and in April 1993, Kevin Harris went on trial for murder, with Randy Weaver charged with conspiracy to commit murder.(89) As with the Branch Davidians, the government attempted to portray Weaver as a political and religious zealot who prophesied and then sought to create a holy war with federal agents, even though his clear goal had been to avoid government agents.(90) Weaver and Harris claimed self‐ defense, and that the government unjustifiably fired first.



With no defense evidence even introduced, the jury acquitted the accused of all charges of criminal violence, and the court fined the federal government for falsifying evidence, for withholding evidence, and for lying.(91) Weaver was convicted only of his failure to appear for the court hearing growing out of the BATF sting.(92)



The Justice Department conducted an internal review of the incident which strongly condemned governmental actions, and recommended criminal prosecution. The report has never been released the public. Its recommendations were over‐ ruled by high‐​ranking Justice Department officials.



Instead, trivial sanctions were imposed. For example, Larry Potts, the supervisor of the siege, who had approved the “shoot‐​to‐​kill” rules of engagement was given a censure, the same punishment inflicted on FBI Director Louis Freeh for losing his portable phone. Potts was then promoted to the second‐​ranking position at the FBI. The new training center for US Marhsals in New Orleans was named the “William F.



Degan” center, in honor of the marshal who had killed Sammy Weaver.



If President insists that wishes to convince the tens of thousands of Americans who belong to militia, the millions who support the patriot movement, and the 39 percent who told the Gallup poll that they think the federal government is an immediate threat to their liberty, then the President should stop the government from acting like a terrorist organization, and then slapping itself on the wrist. Rather than encouraging more use of informants, Congress should create a special prosecutor to investigate homicides perpetrated by the federal government, starting with the Weaver case.



Preserve Our National Commitment to Freedom of Speech Many people, particularly people who abhor “right‐​wing” political viewpoints, have asserted that talk show hosts, commentators, and others who speak strongly about the need to restrain the federal government are indirectly responsible for the events in Oklahoma City. Such claims are disgraceful.



When President Kennedy was assassinated in Dallas in 1963, some people attempted to link the assassination to the climate of “hate” which characterized the intense Southern opposition to President Kennedy’s legislative program, including civil rights. But quite plainly, Southern segregationsists, wrong as they were on policy matters, had nothing to do with the President’s murder.



In 1970, anti‐​war radicals blew up a math building at the University of Wisconsin. These radicals lived in an “Amerika” where important intellectual, political, and media voices proclaimed that the Vietnam war was immoral, illegal, and imperialist, and the American government was guilty of crimes against humanity. The young Bill Clinton enunciated some of these views. Yet it would be improper to blame the opponents of the Vietnam war, including young Mr. Clinton, for the criminal acts of the Wisconsin bombers.



Today, the Southern Poverty Law Center (SPLC) di
"
"
Share this...FacebookTwitterAnyone know a good copyright attorney interested in splitting awards 50/50?
I wonder how many bloggers know what the symbol to the left means? Seems like there are some that don’t.
No, it doesn’t mean Christmas!
I really wish I didn’t have to write this. But I feel compelled to do so.
I’ve been blogging 9 months and I try to make sure I don’t violate copyright laws. Copyright laws do enter gray territory at times, and sometimes it isn’t really clear what is allowed and what is not. I hope that I’ve always stayed within the law. I use photos from Wikipedia, as it’s stated there what you can and cannot do. I hear a lot of people bring up “fair use”. Laws vary from country to country. It gets complicated and fuzzy.
When blogging, I often quote papers, online articles and provide excerpts, always making sure to cite and link to the sources. I think people who wrote the original material deserve credit and recognition for their work. I like linking to other sites, hoping that the little extra traffic I generate will make them happy. What goes around, comes around.
What do other bloggers think? I’d be interested to know. Maybe I’m naive about all this.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I must say I’m surprised by the number of sites that simply copy and paste ENTIRE stories and paste them at their site, without even asking. This has been done to me more than once, and with increasing frequency lately. With one of my recent posts I found 6 other sites that did this with my story – not one asked for permission to do so. Sure they provided a link, but who needs to click on it when it’s all there?
I don’t want to sound like I’m crying about it, but geez, c’mon, it sure would be nice if people asked. Like hello! I’m not there to carry your water.
Don’t get me wrong, I love it when other blogs pick up on the stuff I write. It really feels rewarding. All I ask is that bloggers post the first 30% of the content or so, or excerpts, followed by a direct link to the original story. Is that too much to ask? Am I being naive?
Tom Nelson does an outstanding job in respecting other people’s work. Tom always posts a few main excerpts, some content, then followed by a link to the original story. This way everyone is happy.
I noticed some sites that copy and paste entire stories without permission also have advertisements at their sites, meaning they are gaining financially by copying and pasting. In this case the originators of the content have the right to compensation.
I’m not going to go after past infractions, but in the future I’m not going to let these things slide as much anymore. Bloggers who benefit financially from advertising at their sites, or receive donations, and decide to copy and paste entire stories from NTZ will have to expect to hand over a cut of their profits, willingly or unwillingly.
All in all, I’m just hoping everyone will agree to play fair in the future.  
Merry Christmas everyone.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOr to call it what it is: theological propaganda.
Google is also responding to the growing skepticism in climate science by starting “an effort to foster a more open, transparent and accessible scientific dialogue aimed at inspiring pioneering use of technology, new media and computational thinking in the communication of science to diverse audiences.” They’ll start by focusing on communicating the science on climate change.
Read here
That means they are going to help ramp up climate propaganda. According to their press release at Google blog, they’ve started the effort by selecting 21 mid-career Ph.D. scientists who have “the strongest potential to become excellent communicators.” A list of the 21 selected members can viewed at the website.
The selected fellows will participate in a workshop at Google headquarters in California. Google writes:
Following the workshop, fellows will be given the opportunity to apply for grants to put their ideas into practice. Those with the most impactful projects will be given the opportunity to join a Lindblad Expeditions & National Geographic trip to the Arctic, the Galapagos or Antarctica as a science communicator.
Expect to see more calving ice footage – dubbed with dire messages of planetary destruction. Folks, they’ve been crowing this 20 years.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This is not the Internet giant’s first effort to try to sway public opinion concerning climate change. Last year it launched Google Earth map “which shows how the world would be affected by a global average temperature increase of 4C in a bid to rebuild public trust in climate science,” the UK Telegraph writes here.
Thankfully, trust is real hard to rebuild once lost. And it doesn’t get rebuilt by returning (again) to the old scare-mongering and serial exaggerating. If they truly wish to win back the trust of the public, then they have to start being honest. But they can’t afford that because that would mean the end of their Utopian pipe dream and scam.
And check out this jewel of propaganda made with massaging, über-alarmist Al Gore.

Share this...FacebookTwitter "
"
The idea with measuring climate accurately, is to get as far away as possible from human/urban influences so that those things don’t bias the readings of the thermometer. For example, on my way from Las Vegas to Reno this week, I passed through the near-ghost town of Mina, Nevada, which has a USHCN station. Mina is about as in the “middle of nowhere” as you can be. In fact, the view to the east of the Mina USHCN station is stunning for it’s remote beauty:

According to Wikipedia, Mina has quite a varied range of temperature:
Average July high temperatures range from 61° to 96 °F, with January averaging between 22° and 47 °F. The highest temperature ever recorded in Mina was 110 °F in 1933, with a low in 1990 of –23 °F. Mina receives very little rainfall, and in an average year gets about six inches, with no month getting more than one inch in a normal year. The Mina Airport is at the southeastern end of town.
The USHCN station is at the private residence of the airport operator, who also runs a KOA type trailer/RV park. The airport is a simple dirt strip, so no runway to generate extra heat. I’ve been all over the USA looking at the USHCN network. In almost every station I visit, there’s some sort of surprise. Mina was no exception, and I discovered what Stevenson Screens are really used for:
– as mounts for other weather stations.

In this case, an Oregon Scientifc WMR-968 wireless weather station, which is quite possibly the worst electronic weather station on the market. I once sold these at my online store weathershop.com, and stopped doing so when failure rates started approaching 30% out of the box.
Fortunately, the WMR-968 is not the “primary” instrument of the USHCN station, though it appears to be used as backup for the primary MMTS/Nimbus instrument. In this photo, you can see the wire from the small solar panel running inside to the temperature sensor.
What is most interesting about this station, is that while it truly is in the “middle of nowhere” and has that great “rural” view to the east, the primary MMTS sensor is just a few feet from where all the RV’s park while they register at the office:

Click for a large image
Unlike the Stevenson Screen, The MMTS is just a few feet from the office due to the famous cabling issue. It also has some nice sized rocks to act as heat sinks for those cold desert nights:

View looking North – click for a larger image
Besides the mixture of shade, rock heat sinks, road and building proximity, there’s also the requisite BBQ or two:

View looking south – click for a larger image
The rain gauge also has issues, due to the wind ducting that is likely created by these two trailers:


Click for a larger image
You can see the complete set of photos at the Mina gallery at surfacestations.org
As for the temperature trend:

Data from GISS – see original plot here
The trend is up, curiously, even though the town appears to be dying, so urbanization shouldn’t be the cause. According to NCDC’s MMS database, the station switched from using the Stevenson Screen to the MMTS electronic sensor in 1986. MMTS is well known for building proximity, That may account for some of the trend. There was also a station move to the present location in August 2007.
US 95 is about 100 yards away from the Mina USHCN temperature sensor, so perhaps there is an urbanization component in the form of more traffic.  I simply don’t know.  Interestingly, the station at Bishop, to the west, shows very little long term trend, while the station to the south, Tonopah, does show a trend. But Tonopah is growing, unlike Mina, which is dying and is now listed on a Ghost Town forum. Tonopah also had it’s weather station converted to ASOS, which when combined with other airport improvements, tends to add a positive trend.
So it’s a puzzle, and I welcome comments with ideas.
The thing about the Mina station though, is that without knowing a site history and history of the surrounding changes, we simply don’t know how much of the signal is real or from land use changes around the sensor. In it’s new position at an active RV park, it is now in a dynamic environment within feet of daily vehicular traffic. We simply should not have to figure such things out for a climatic reference station, even if it is in the “middle of nowhere”.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b9586de',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Don sent me his AGU paper for publication and discussion here on WUWT, and I’m happy to oblige – Anthony
Abstracts of American Geophysical Union annual meeting, San Francisco  Dec., 2008
Solar Influence on Recurring Global, Decadal, Climate Cycles Recorded by Glacial Fluctuations, Ice Cores, Sea Surface Temperatures, and Historic Measurements Over the Past Millennium 
Easterbrook, Don J., Dept. of Geology, Western Washington University, Bellingham, WA 98225,
Global, cyclic, decadal, climate patterns can be traced over the past millennium in glacier fluctuations, oxygen isotope ratios in ice cores, sea surface temperatures, and historic observations.  The recurring climate cycles clearly show that natural climatic warming and cooling have occurred many times, long before increases in anthropogenic atmospheric CO2 levels.  The Medieval Warm Period and Little Ice Age are well known examples of such climate changes, but in addition, at least 23 periods of climatic warming and cooling have occurred in the past 500 years. Each period of warming or cooling lasted about 25-30 years (average 27 years).  Two cycles of global warming and two of global cooling have occurred during the past century, and the global cooling that has occurred since 1998 is exactly in phase with the long term pattern.  Global cooling occurred from 1880 to ~1915; global warming occurred from ~1915 to ~1945; global cooling occurred from ~1945-1977;, global warming occurred from 1977 to 1998; and global cooling has occurred since 1998.  All of these global climate changes show exceptionally good correlation with solar variation since the Little Ice Age 400 years ago.
The IPCC predicted global warming of 0.6° C (1° F) by 2011 and 1.2° C (2° F) by 2038, whereas Easterbrook (2001) predicted the beginning of global cooling by 2007 (± 3-5 yrs) and cooling of about 0.3-0.5° C until ~2035.  The predicted cooling seems to have already begun. Recent measurements of global temperatures suggest a gradual cooling trend since 1998 and 2007-2008 was a year of sharp global cooling. The cooling trend will likely continue as the sun enters a cycle of lower irradiance and the Pacific Ocean changed from its warm mode to its cool mode.
Comparisons of historic global climate warming and cooling, glacial fluctuations, changes in warm/cool mode of the Pacific Decadal Oscillation (PDO) and the Atlantic Multidecadal Oscillation (AMO), and sun spot activity over the past century show strong correlations and provide a solid data base for future climate change projections. The announcement by NASA that the Pacific Decadal Oscillation (PDO) had shifted to its cool phase is right on schedule as predicted by past climate and PDO changes (Easterbrook, 2001, 2006, 2007) and coincides with recent solar variations. The PDO typically lasts 25-30 years, virtually assuring several decades of global cooling.  The IPCC predictions of global temperatures 1° F warmer by 2011,  2° F warmer by 2038, and 10° F by 2100 stand little chance of being correct. “Global warming” (i.e., the warming since 1977) is over!

Figure 1.  Solar irradiance, global climate change, and glacial advances. Click to enlarge
The real question now is not trying to reduce atmospheric CO2 as a means of stopping global warming, but rather (1) how can we best prepare to cope with the 30 years of global cooling that is coming, (2) how cold will it get, and (3) how can we cope with the cooling during a time of exponential population increase?  In 1998 when I first predicted a 30-year cooling trend during the first part of this century, I used a very conservative estimate for the depth of cooling, i.e., the 30-years of global cooling that we experienced from ~1945 to 1977.  However, also likely are several other possibilities (1) the much deeper cooling that occurred during the 1880 to ~1915 cool period, (2) the still deeper cooling that took place from about 1790 to 1820 during the Dalton sunspot minimum, and (3) the drastic cooling that occurred from 1650 to 1700 during the Maunder sunspot minimum. Figure 2 shows an estimate of what each of these might look like on a projected global climate curve.  The top curve is based on the 1945-1977 cool period and the 1977-1998 warm period.  The curve beneath is based on the 1890-1915 cool period and 1915-1945 warm period.  The bottom curve is what we might expect from a Dalton or Maunder cool period.  Only time will tell where we’re headed, but any of the curves are plausible.  The sun’s recent behavior suggests we are likely heading for a deeper global cooling than the 1945-1977 cool period and ought to be looking ahead to cope with it.

Figure 2. Global temperature variation 1900 to 2008 with projections to 2100. Click to enlarge.
The good news is that global warming (i.e., the 1977-1998 warming) is over and atmospheric CO2 is not a vital issue. The bad news is that cold conditions kill more people than warm conditions, so we are in for bigger problems than we might have experienced if global warming had continued. Mortality data from 1979-2002 death certificate records show twice as many deaths directly from extreme cold than for deaths from extreme heat, 8 times as many deaths as those from floods, and 30 times as many as from hurricanes. The number of deaths indirectly related to cold is many times worse.
Depending on how cold the present 30-year cooling period gets, in addition to the higher death rates, we will have to contend with diminished growing seasons and increasing crop failures with food shortages in third world countries, increasing energy demands, changing environments, increasing medical costs from diseases (especially flu), increasing transportation costs and interruptions, and many other ramifications associated with colder climate. The degree to which we may be prepared to cope with these problems may be significantly affected by how much money we waste chasing the CO2 fantasy.
All of these problems will be exacerbated by the soaring human population.  The current world population of about 6 ½ billion people is projected to increase by almost 50% during the next 30 years of global cooling (Figure 2).  The problems associated with the global cooling would be bad enough at current population levels.  Think what they will be with the added demands from an additional three billion people, especially if we have uselessly spent trillions of dollars needlessly trying to reduce atmospheric CO2, leaving insufficient funds to cope with the real problems.

Figure 3. Global population.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a333224',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

ASOS station in Greenville, Alabama, Mac Crenshaw Memorial Airport
Photo by: Howard Wiseman
You’d think either the FAA or NOAA would get this cleaned up before surfcaestations.org volunteers have the opportunity to get such pictures. So, in the spirit of the “FAIL blog” I thought I’d offer this.
To be fair, this looks like a junker placed there rather than a aviation accident. I’ve seen many such aircraft at small airports that become forgotten derelicts like this.
Here is a view sans aircraft.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ab7e661',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn the older days, whether selling detergents, applicances or food products, marketers often used the slogan: “new and improved!” to con consumers and to boost sales. The German online DIE WELT reports that companies have changed their marketing slogans to make their business more profitable and sustainable. The new slogan used to pitch products today is:
Almost every conceivable company is jumping on the enviro-bandwagon and claiming their products or services are “sustainable” and thus good for the planet, no matter if it’s an automaker, coal power plant, or an investment instrument. The “sustainable” product is better and safer for the environment. The movement indeed is religious. DIE WELT writes:
You can now invest sustainably, and even fight dandruff in a sustainable way.”
Well, I turned off the light in the room next door, and so now I’m blogging sustainably. In fact I just changed the slogan of my blog. I’m the first climate blogger to blog more sustainably – the world’s most sustainable climate science blogger. Blog here! Going to any other climate sicence blog means you’ll be ruining the planet.
Misuse of the word
As DIE WELT writes, not everyone is amused about companies slapping the “sustainable” slogan on the packaging of their products, and claim it borders on false advertising in many cases. (Not me. I really am blogging with the light off and drinking tap water). Author Ulrich Grober has written a book on the history of sustainability, and is quoted by Die Welt:
Indeed even oil companies like BP use the word “sustainable“ in their annual reports. ‘Recently in Switzerland the most “sustainable” autobahn of all time was inaugurated“, says Grober. it clouds the meaning of the word.’ “
Experts say the word “sustainable” is now being used so often and so incorrectly that it has virtually lost all its original meaning. Sociologist Klaus Kraemer says the word “sustainability is now being used in political debates as the ultimate moral argument. “Whatever is sustainable is not to be questioned.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Use of the word “sustainable” is dangerous
The term being misused is one thing. But using the word for the purpose of marketing may be “dangerous”, says chemist Michael Braungart. Die Welt quotes Braungart:
The concept is backwards-looking and puts the brakes on creativity because it is connected with feelings of guilt.”
I agree with that. If you don’t buy a product that is labelled as “sustainable”, then you are someone who is harming the planet, and so you ought to feel bad about it. That’s how a religion works. So in the end, I think there is going to be a backlash with respect to this blind sustainability movement. The whole thing is rather Medieval. Back to the Dark Ages.
Educated consumers really ought to feel insulted
How do I feel when I see the “sustainable” slogan being targeted at me? Of course it annoys the hell out of me because I feel the seller of the “sustainable” product assumes that I’m actually stupid enough to believe all the CO2 nonsense. I’m insulted that they’d treat me like that. I’m not a blind zealot in a cult. It’s a slogan that maybe works well with morons, dupes and religious greens. But it certainly isn’t a way to communicate with people who think for themselves.
The enviro-sustainable hacks and bosses behind this movement don’t even believe it themselves. See how they jetset all over the world and live lavish lives while raking it in as duped consumers gobble it up. To the half-witted believers out there – wake up – you’re being duped by this utter nonsense.
To my loyal readers, please do not think that I’m targeting you with my new slogan. I know you don’t believe the CO2/sustainability crap. The new blog slogan is aimed solely at other people, like dana1981.
(PS: Actually I have the lights on – and it’s daytime.)
Share this...FacebookTwitter "
"
Share this...FacebookTwitterLast week it was reported almost everywhere by the German media that North Atlantic currents were the warmest in 2000 years and melting the Arctic at an unprecedented rate.
Labrador and North Atlantic currents (Chart from US Coast Guard)
These media reports were based on a study published in Science. The authors write in the abstract (emphasis added):
Here, we present a multidecadal-scale record of ocean temperature variations during the past 2000 years, derived from marine sediments off Western Svalbard (79°N). We find that early–21st-century temperatures of Atlantic Water entering the Arctic Ocean are unprecedented over the past 2000 years and are presumably linked to the Arctic amplification of global warming.”
I’m not sure what they mean by “early 21st century”. Perhaps the years 2000 to 2007? The study says the waters are about 2°C warmer. Here it has to be noted that they are presuming, i.e. postulating. The CO2 link here is a bit of wild speculation.
Here’s how Der Spiegel puts it in a report titled Atlantic Current Is Heating Up The Arctic:
The experts suspect that the accelerated reduction in sea ice and the measured warming of ocean and atmosphere in the Arctic over the last decades, among other factors, was the result of an enhanced transfer of warmth from the Atlantic. The Fram Straits is even about 1.4° C warmer than during the Medieval Climate Optimum, a time when temperatures in Europe were significantly increased.”
And Der Spiegel writes:
‘Cold sea water is decisive in the formation of sea ice, which in turn cools because it reflects sunlight,’ says Thomas Marchitto of the University of Colorado in Boulder. The melting is accelerating by itself.”
Media reports like the one in Der Spiegel of course emphasized the supposed vicious circle of the melting Arctic ice dynamic: more melting leads to more warming, which then accelerates the process – all unleashed of course by man-made CO2.
If anything they are, perhaps unwittingly, admitting that the Arctic sea ice reduction of the 2000s can be traced back to ocean currents.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And as things stand right now, just the opposite is occurring. The Arctic is NOT melting, it’s freezing up again – recovering from its low in 2007, and quite impressively.
The Global Rumblings website here reports that 70 trillion cubic feet of ice have been added to the Arctic core since January 2009. That translates to 2000 cubic km – enough to cover Manhattan with 20 miles of ice (or 32,000 Manhattans with 1 meter of ice).
The US Navy PIPS 2.0 graphic shows ice thickness. The following comparator shows how it’s the Arctic that has gone green.

Source: Global Rumblings
Some will say that PIPS is not a reliable indicator of Arctic ice thickness, and so cannot be used reliably. But you can put that rumour to rest, see PIPS WUWT.
So why is the Arctic thickening and regrowing, and no longer melting at an unprecedented rate as claimed by the media?
This could have to do with the Labrador Current, which flows southward between Greenland and Labrador. Reports say it is slowing down. That means cold water is not getting transported out of the Arctic. A Der Spiegel article just 2 weeks ago titled “Feared Atlantic current is now weakening” suggests that this current is at its weakest level in 1800 years. What is it caused by? According to scientists, Der Spiegel says:
As a cause for the change, scientists suspect climate change. The coincidence that this has happened during the warming of the last decades allows this to be the conclusion, they believe. But the knowledge about ocean currents still has many holes says Wallace Broecker of Columbia University in USA – a pioneer in ocean research.”
Changes in the atmosphere controls the ocean currents? Right. And as usual, they’re sure – yet admit there are many holes in the knowledge and so they are not sure.
Meanwhile, the ice keeps growing.
Share this...FacebookTwitter "
"

When a business isn’t making money, it usually tries to market new products and attract new investors. When a bureaucracy’s reason for existence is threatened, it typically generates new missions. One difference between businesses and bureaucracies, though, is that businesses have to satisfy customers and investors. Because bureaucracies are under no such constraint, they require more oversight. 



The most recent evidence for this is _Environmental Diplomacy_ , the State Department’s first annual report on the environment and foreign policy. According to the report, the end of the Cold War has reduced the demand for the agency’s traditional tasks and given it time to ponder other threats to U.S. vital interests. And what do you know? The world is filled with environmental crises that the State Department must solve.



What are the environmental problems that warrant the attention of the State Department? According to the report, they fall into five areas: climate change, toxic chemicals, species extinction, deforestation and marine degradation. In each area, terrifying scenarios abound that necessitate reliance on the State Department to save the day. For example, “Forests four times larger than Switzerland are lost every year. 70% of the world’s marine fish stocks are fully to over‐​exploited. The people of the world annually release 23 billion tons of CO2 into the air … The range of impacts [from CO2 release] is likely to include: threats to human health including increases in heat‐​related deaths and illnesses, and in the incidence of infectious diseases.… There is no way to estimate the potential benefits that may come from millions of species yet to be studied, or yet to be discovered. And there is no way to estimate the health, economic, and spiritual costs to our children who could inherit a world robbed of a drug to cure AIDS, stripped of a strain of disease‐​free wheat, or bereft of the wonder of such diverse creatures as tigers and sea turtles.”



Let’s avoid speculating on the desperation that led the State Department to produce a post‐​Cold War mission that required the invocation of children, AIDS, wheat, sea turtles and tigers all in the same sentence. Instead, consider the paucity of evidence for the existence of those problems. Take, for instance, the panic over global warming. Observed warming since the late 19th century is only .5 degrees centigrade rather than the 1990 prediction of the United Nations’ climate change panel of 1.3 to 2.3 degrees centigrade, according to Patrick Michaels, professor of environmental sciences at the University of Virginia. Greenhouse physics predicts that the driest air masses — those in the polar regions during the winter night — should respond first and most strongly to CO2 emissions. In fact satellite data confirm that over the last 18 years the globe has cooled in general, but the coldest winter regions in Siberia and Canada have warmed. Are warmer winters in the Arctic Circle a problem?



If the State Department is serious about solving environmental problems, it should educate the world about the establishment of property rights, the enforcement of contracts, and the role of courts in resolving dispute. And then it should get out of the way.



Although the State Department claims there is no way to estimate the potential benefits from species yet to be discovered, the loss of biodiversity as a threat to pharmaceutical discoveries has been studied by economists for Resources for the Future. In a February 1996 article in the _Journal of Political Economy_ , they estimate the pharmaceutical benefits from saving one additional plant species from extinction to be, at most, about $10,000. That translates into an estimated willingness of pharmaceutical companies to pay about $8 an acre for land in tropical regions. The authors conclude that both the value of species preservation for use in pharmaceutical research and, by extension, the incentive to conserve threatened habitat are “negligible” in comparison with other development uses.



Let’s assume, however, that some of the problems are real and as serious as the report alleges. What will the State Department do if we give it more money to save us from those impending disasters? It will “help stabilize” regions where pollution contributes to political tensions. It will “enable nations … to work cooperatively to develop initiatives to attack regional environmental problems.” It will “strengthen our relationship with allies by working together on internal environmental problems.” In short, it will expend a amount of hot air to create domestic political capital.



If _Environmental Diplomacy_ were a prospectus for a company trying to raise additional money in the stock market for its new mission to solve environmental problems, would you invest your money? I think not, because the solution to environmental problems is not more meetings or negotiations but property rights. To the extent environmental problems exist, both within and between countries, the cause is the lack of adequately defined and enforced property rights. If people own resources, they seek to conserve their value rather than waste them. Rhinos and tigers are slaughtered to near extinction because no one owns them. Water and air resources are used with little thought because no one owns them.



If the State Department is serious about solving environmental problems, it should educate the world about the establishment of property rights, the enforcement of contracts, and the role of courts in resolving dispute. And then it should get out of the way. No one need decide the difficult tradeoffs about the use of land, energy, water or air. Prices will communicate the value of those commodities to all and they will be used to everyone’s best advantage. Short of that, the State Department should stop preaching and concentrate on developing a coherent post‐​Cold War national security strategy.
"
"

One of the most interesting legal issues percolating in the American legal system is jury nullification. A poll by Decision Quest and the National Law Journal in 1998 found that three out of four citizens believed jurors should do what they think is right regardless of what the judge says the law is. In February 1999 the Washington Post found “a significant pattern of juror defiance” that has resulted in a sharp increase in the number of hung juries. Alarmed by these developments, trial judges are aggressively attempting to thwart jury nullification. The question now before the courts is, How far can a trial judge go before overstepping his or her authority and coercing a verdict? 



Over the past year a string of rulings have illustrated the problem. In April the Colorado Court of Appeals overturned a contempt of court conviction against a juror for her failure to reveal during voir dire that she did not intend to follow the trial judge’s instructions on the law. _People v Kriho_ , No. 97CAD700, 1999 WL 249143 (unpublished decision). In May a U.S. district court judge dismissed a deliberating juror for her unwillingness to follow the law and allowed the remaining jurors to convict the defendants on trial. _U. v Abbell_ (SD Fla) No. 93–0470-CR. In June the U.S. Court of Appeals for the Ninth Circuit reversed the convictions ofJohn Fife Symington III, the former governor of Arizona, because his Sixth Amendment right to an impartial jury was violated when the trial judge dismissed a holdout juror after eight days of deliberations. _U.S. v Symington_ , 1999 _Daily Journal_ DAR 6295, 1999 WL 415345-



The California Supreme Court will soon decide whether a trial court violated a defendant’s right to trial by jury by dismissing a deliberating juror because he did not intend to follow the law on a particular charge. People v Williams, No. S066106 (rev gr Feb. 18, 1998). With all of this appellate court activity, the issue seems destined to go to the U.S. Supreme Court for resolution 



A surprising number of lawyers hold ill‐​considered opinions about jury nullification. The conventional wisdom holds that jury nullification is absolutely improper. Thus, a trial judge should dismiss any juror who refuses to follow the court’s legal instructions. On first blush, that line of reasoning appears sound, but a closer examination will show that it is mistaken 



A leading case in this area of law, _U. v Thomas_ (1997) 116 FM 606, embodies the conventional wisdom on jury nullification, so it is useful to scrutinize the rationale of its holding Judge Jose A. eabranes opined that because “no juror has a right to engage in nullification,” trial courts have a duty to thwart nullification by dismissing any juror who refuses to follow the letter of the law. Although the judge acknowledged that nullification sometimes occurs, he said that it happens because the specter of nullification does not come “to the attention of a presiding judge before the completion of [the] jury’s work” 116 F3d at 616. Thus, he concluded, a trial judge has a responsibility to thwart such “misconduct” whenever the opportunity arises. 



There are several problems with Judge Cabranes’s analysis. First, his claim that jurors do not have the right to engage in jury nullification would have startled the framers of the Constitution. John Adams, who signed the Declaration of Independence and served as our second president, observed that “It is not only [the juror’s] right, but his duty … to find the verdict according to his own best understanding, judgment, and conscience, though in direct opposition to the direction of the court.” It is important to note that Adams was not just speaking for himself. Similar statements by Hamilton, Jefferson, and others show that Judge Cabranes’s view of jury service is at odds with the original understanding of that duty.



Second, the judge’s claim that “trial courts have [a] duty to forestall” jury nullification is weak. If such a duty truly existed, the law would give trial judges the discretionary power to direct verdicts for the prosecution. The same principle that denies judges the discretion to direct guilty verdicts should also operate to deny judges the discretion to dismiss deliberating jurors.



Third, Judge Cabranes seems to think that nullification sometimes occurs because “jurors are not answerable for nullification after the verdict has been reached.” That claim may be true, but it does not lend much support to his argument. After‐​all, the law could empower trial judges to enter a judgment of conviction notwithstanding the verdict. As is the case in civil trials, a JNOV would “cure” a nullification verdict and leave the jurors themselves “unanswerable” regarding the votes they cast in the jury room‐ The same principle that denies judges the power to grant JNOVs in criminal cases should also operate to deny judges the power to dismiss jurors who refuse to condemn the defendant in the circumstances of the case before them. 



Fourth, Judge Cabranes’s rule, which would permit trial judges to dismiss deliberating jurors, would yield highly unsatisfactory results. In the Thomas case, several jurors complained to the court that a verdict could not be reached because a lone holdout refined to follow the law. That juror was subsequently dismissed. The propriety of that dismissal became the central issue on appeal. Judge Cabranes’s opinion said that in such situations the trial judge has to proceed cautiously to determine if such complaints have merit. If the allegations are found to be true, the trial judge should dismiss the holdout juror for “misconduct.” ” Such an approach sounds reasonable to many lawyers, but consider the implications of such a procedure under some alternative fact patterns.



What if, after a full week of deliberations, a trial judge learns that two or three jurors have decided that they cannot in good conscience enforce the law against the defendant? Are we going to allow several alternates to take their places so that a conviction can be obtained? Take a defendant on trial for multiple charges. Wha~if two jurors believe that the defendant is indeed technically guilty on every count, but they disagree with the other jurors with respect to whether the defendant deserves to have the proverbial book thrown at hirn. If the “hard‐​line” jurors complain to the judge, should the trial court throw the “lenient” jurors off the case because they are unwilling to convict on every single count? According to Judge Cabranes’s reasoning the answer would be yes. Such meddling with the give‐​and‐​take of jury deliberations cannot possibly be reconciled with the defendant’s right to a trial by jury and to a unanimous verdict. Make no mistake, if trial courts begin to exercise power in this way, trial by jury will be so hamstrung as to be unrecognizable.



There is a better way to resolve such controversies. The key lies in the duty of jurors to deliberate. The duty to deliberate means that each juror must be willing to listen to the views of others with a disposition toward reexamining his or her own views.That is the duty that ought to concern the trial court. Should a judge receive a note that complains about a juror who refines to follow the letter of the law, the trial judge should resist the impulse to conduct an inquisition. Instead, the judge should calmly and respectfully ascertain whether the so‐​called holdout is willing to deliberate. Of course, a court’s instruction about the duty to deliberate should always be followed by the caveat that no juror should surrender a conscientious opinion solely because of the opinion of other jurors or for the mere purpose of returning a unanimous verdict. 



**Any juror who is unable or unwilling to deliberate** should be excused and replaced with an alternate. But a deliberating juror’s conscientious refusal to cast a vote for conviction should not be considered misconduct and thus a sufficient basis for removal. When a jury is deadlocked because one or more jurors cannot in good conscience vote to convict, the trial judge has only two options: send the jury back to continue deliberating or declare a mistrial. Unlike the holding in the Thomas case, this approach is attentive to the twin imperatives. of safeguarding the province of the jury from incursions fi‐​om the bench and protecting the defendant’s right to a unanimous verdict.



Jury nullification seems to be one of those topics where strong passions overcome reasoned discussion. Some lawyers focus obsessively on instances in which a jury or juror abused the system. Clearly, some cases of abuse exist. To maintain perspective, however, lawyers ought to recall some words of wisdom from judge David L. Bazelon, concurring in US. v Dougherty (1972) 473 F2d 1113, 1142: “Trust in the jury is … one of the cornerstones of our entire criminal jurisprudence, and if that trust is without foundation we must re‐​examine a great deal more than just the nullification doc‐ trine.” And since even conservatives rec‐ ognize that the Constitution placed its trust in juries, not judges, to determine criminal guilt, there’s a good chance that the hostile climate surrounding jury nullification may eventually subside. 
"
"
NASA’s updated data appears to suggest the annual rate of global polar ice loss  has actually decreased
 
Greenland’s Riviera – their green southwest. Will another Maunder minimum
grip the region in cages of ice again, or will bells ring in the portside squares,
as they did in the 1300’s before that cooling came, and ships sailed the fiords?
(Source: NASA)
Excerpt:
Washington Post correspondant Juliet Eilperin, in her 12-26-08 report entitled “New climate change estimates more pessimistic,” dutifully surveys the latest bleak findings of the climate change community. Her primary source is a recently released survey comissioned by the U.S. Climate Change Science Program – expanding on the findings of the 2007 4th IPPC Report on Climate Change. Apparently this “new assessment suggests that earlier projections may have underestimated the climatic shifts that could take place by 2100.” One of Eilperin’s primary examples of alarming new data is reported as follows:
“In one of the reports most worrisome findings, the agency estimates that in light of recent ice sheet melting, global sea level rise could be as much as 4 feet by 2100. The IPCC had projected a sea level rise of no more than 1.5 feet by that time, but satellite data over the past two years show the world’s major ice sheets are melting much more rapidly than previously thought. The Antarctic and Greenland ice sheets are now losing an average of 48 cubic miles of ice a year, equivalent to twice the amount of ice that exists in the Alps.”
Three years ago what NASA quantified as an alarming loss of  annual ice loss from Greenland was easily demonstrated at that time to be an  insignificant loss, and today NASA’s updated data appears to suggest the annual  rate of global polar ice loss has actually decreased since then.
http://ecoworld.com/blog/2008/12/26/pessimistic-reporting-optimistic-data/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a4b0434',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

What’s worse than a public policy debate that turns bitter and impolite? Well, for one, having the courts step into the marketplace of ideas to judge which side of a debate has the best “facts.”



Yet that’s what Michael Mann has invited the D.C. court system to do. In response to some scathing criticism of his methodologies and an allegation of scientific misconduct, the author of the infamous “hockey stick” models of global warming — because they resemble the shape of a hockey stick, with temperatures rising drastically beginning in the 1900s — has taken the global climate change debate to a record low by suing the Competitive Enterprise Institute, _National Review_ , and two individual commentators. The good Dr. Mann claims that some blogposts alleging his work to be “fraudulent” and “intellectually bogus” were libelous. (For more background on the matter, see this excellent summary by _NR_ ’s editor Rich Lowry; linking to that post is partly what led Mann to target CEI.)





Public figures must not be allowed to use the courts to muzzle their critics.



The D.C. trial court rejected the defendants’ motion to dismiss this lawsuit, holding that their criticism could be taken as a provably false assertion of fact because the EPA, among other bodies, have approved of Mann’s methodologies. In essence, the court seems to cite a consensus as a means of censoring a minority view. The defendants appealed to the D.C. Court of Appeals (the highest court in the District of Columbia).



Cato has now filed a brief, joined by three other think tanks, in which we urge the court to stay out of the business of refereeing scientific debates. (And if you liked our “truthiness” brief, you’ll enjoy this one.)



We argue that the First Amendment demands that failing to leave room for the marketplace of ideas to operate stifles academic and scientific progress, and that judges are ill‐​suited to officiate policy disputes — as history has shown time and again. The lower court clearly got it wrong here — and there are numerous cases where courts have more judiciously treated similarly harsh assertions for what they really are: expressions of disagreement on public policy that, even if hyperbolic, are among the forms of speech most deserving of constitutional protection.



The point in this appeal is that courts should not be coming up with new terms like “scientific fraud” to squeeze debate over issues impacting government policy into ordinary tort law. Dr. Mann is not like a corner butcher falsely accused of putting his thumb on the scale or mixing horsemeat into the ground beef. He is a vocal leader in a school of scientific thought that has had major impact on government policies.



Public figures must not be allowed to use the courts to muzzle their critics. Instead, as the U.S. Supreme Court has repeatedly taught, open public debate resolves these sorts of disputes. The court here should let that debate continue outside the judicial system.
"
"

“Shut down the World Bank!” reads one of the many placards being carried through Washington D.C. As the international institution holds its annual spring meeting, thousands of activists are venting their anger about everything from world poverty to environmental degradation. While they are correct to point an accusatory finger at the World Bank for making those problems worse, their well‐​publicized efforts will do little good. The bank has become adept at co‐​opting key elements of its opposition in order to keep billions in U.S. government money flowing in its direction. 



Since first being stung by environmentalist criticism in the early 1980s, the World Bank has repeatedly promised to reform itself. Despite those promises, the bank has neither fundamentally reformed its lending practices nor radically changed the kinds of projects that receive its funding. Bank projects around the world remain environmentally destructive and socially disruptive. By its own account, 2.6 million victims of World Bank lending in poor countries are having their property confiscated, their homes destroyed, and their livelihoods ruined. 



When the bank hears public criticism of its destructive environmental record, it typically issues a press release announcing renewed commitment to old promises made in prior years’ press releases. After two decades of this behavior, there is little evidence of tangible improvement. In a recent review, the bank judged that 25 percent of its projects in 1997–98 had an unsatisfactory outcome, even by its own rather generous standards. It also concedes that only 54 percent of its projects completed could be judged “sustainable,” even though sustainable development is now supposed to be a main purpose of the bank. 



Environmental groups also have a difficult time identifying reform successes. A 1999 environmentalist report labeled the World Bank’s reform program a “failure,” noting that it had not produced more environmentally sound projects or a greater level of bank accountability to the public. According to the report, endorsed by the Environmental Defense Fund, Friends of the Earth, Greenpeace and the Sierra Club, “an evaluation of the World Bank Group’s portfolio shows that it does not promote environmental protection in its operations and loans.” 



The bank’s most noticeable improvement in recent years has been in its ability to weather criticism, exposure and political opposition. It has actively courted the support of environmental advocacy groups and has rhetorically embraced their “sustainable development” creed. The bank cooked its books to make it appear that environmental spending had gone up and provided generous grant funding to nongovernmental organizations (NGOs) in order to make them more dependent on its continued existence. It has invited green organizations to its lavish headquarters to solicit their advice and to enter into partnerships that improve the bank’s image. Despite its documentation of egregious abuses at the World Bank, the leadership of the anti‐​bank movement does not advocate — as it sometimes did — the only real solution to an institution that has demonstrated zero capacity for genuine reform: elimination. 



Why has the bank’s opposition gone soft? The World Bank, recognizing that environmentalist opposition in the early 1990s threatened its very existence, made a concerted effort to meet this challenge. An internal World Bank report identifies the key to its survival continuing “to build a public constituency for the Bank’s policies and programs, and for development assistance in general.” Central to its strategy was a systematic effort to convert the environmental movement to an accommodationist stance toward the bank. The greens must advocate incremental changes to the institution, not its elimination. 



Beginning in the early 1990s, the bank contacted its most influential environmental critics and invited them to “participate” in the bank’s work. According to the bank, this activity has caused “a reduction in NGO criticism” and a recognition on the part of NGOs that they and the bank share common interests. The bank claims that a chief benefit of appeasing the environmental movement is to “improve the overall climate of opinion around the Bank’s work.” Most important, environmental groups tempered their criticism of the bank during key moments when its budget was being debated in Congress. 



The environmental groups are now committed to working with and improving the World Bank. Implicitly, they believe that World Bank planners, who once ignored environmental considerations with tragic consequences, can now be trusted to implement major development decisions in the Third World. They suggest that the bank can become an instrument of “sustainable development.” 



The World Bank’s latest reform effort is eerily reminiscent of earlier efforts that environmental groups denounced as inadequate and diversionary. But this strategy has been effective enough to secure funding prospects on Capitol Hill. If the anti‐​bank protestors really want to “Break the Bank,” as their signs say, they will work harder to ensure that their own organizations do not compromise on this objective.
"
"

In this month’s issue of the _Economists’ Voice_ , Robert Whaples, chair of the economics department at Wake Forest, reports on a survey he recently conducted in which he sent questionnaires to 210 Ph.D. economists randomly selected from the American Economic Association. His charge: to find out how much disagreement there is within the profession and a number of high profile public policy issues.   
  
  
What did his respondents have to say about the impact that global warming will have on the economy? 



In short, the number of economists who thought global warming would improve the U.S. economy outnumbered the number of economists who thought that global warming would harm the economy to the extent feared by the _Stern Review_.   
  
  
Will those who demand that we bow down to the consensus of scientific opinion likewise demand the same regarding the consensus of economic opinion? Not bloody likely.
"
"

Pundits, politicians and the press have argued that global warming will bring disaster to the world, but there are good reasons to believe that, if it occurs, we will like it. Where do retirees go when they are free to move? Certainly not to Duluth. 



People like warmth. When weather reporters on TV say, “It will be a great day,” they usually mean that it will be warmer than normal. The weather can, of course, be too warm, but that is unlikely to become a major problem if the globe warms. Even though it is far from certain that the temperature will rise, the Intergovernmental Panel on Climate Change (the U.N. body that has been studying this possibility for more than a decade) has forecast that, by the end of the next century, the world’s climate will be about 3.6 degrees Fahrenheit warmer than today and that precipitation worldwide will increase by about 7 percent.



The scientists who make up this body also predict that most of the warming will occur at night and during the winter. In fact, records show that, over this century, summer highs have actually declined while winter lows have gone up. In addition, temperatures are expected to increase the most towards the poles. Thus Minneapolis should enjoy more warming than Dallas; but even the Twin Cities should find that most of their temperature increase will occur during their coldest season, making their climate more livable.



Warmer winters will produce less ice and snow to torment drivers, facilitating commuting and making snow shoveling less of a chore. Families will have less need to invest in heavy parkas, bulky jackets, earmuffs and snow boots. Department of Energy studies have shown that a warmer climate would reduce heating bills more than it would boost outlays on air conditioning. If we currently enjoyed the weather predicted for the end of the next century, expenditures for heating and cooling would be cut by about $12.2 billion annually.



The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole.



Most economic activities would be unaffected by climate change. Manufacturing, banking, insurance, retailing, wholesaling, medicine, educational, mining, financial and most other services are unrelated to weather. Those activities can be carried out in cold climates with central heating or in hot climates with air conditioning.



Certain weather‐​related or outdoor‐​oriented services, however, would be affected. Transportation would benefit generally from a warmer climate, since road transport would suffer less from slippery or impassable highways. Airline passengers, who often endure weather‐​related delays in the winter, would gain from more reliable and on‐​time service.



The doomsayers have predicted that a warmer world would inflict tropical diseases on Americans. They neglect to mention that those diseases, such as malaria, cholera and yellow fever, were widespread in the United States in the colder 19th century. Their absence today is attributable not to a climate unsuitable to their propagation but to modern sanitation and the American lifestyle, which prevent the microbes for getting a foothold.



It is actually warmer along the Gulf Coast, which is free of dengue fever, than on the Caribbean islands where the disease is endemic. My own research shows that a warmer world would be a healthier one for Americans and would cut the number of deaths in the U.S. by about 40,000 per year, roughly the number killed on the highways.



According to climatologists, the villain causing a warmer world is the unprecedented amount of carbon dioxide we keep pumping into the atmosphere. As high school biology teachers emphasize, plants absorb carbon dioxide and emit oxygen.



Researchers have shown, moreover, that virtually all plants will do better in an environment enriched with carbon dioxide than in the current atmosphere, which contains only trace amounts of their basic food. In addition, warmer winters and nights would mean longer growing seasons. Combined with higher levels of CO2, plant life would become more vigorous, thus providing more food for animals and humans. Given a rising world population, longer growing seasons, greater rainfall, and an enriched atmosphere could be just the ticket to stave off famine and want.



A slowly rising sea level constitutes the only significant drawback to global warming. The best guess of the international scientists is that oceans will rise about 2 inches per decade. The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole.



Let’s not rush into costly programs to stave off something that we may like if it occurs. Warmer is better; richer is healthier; acting now is foolish.
"
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
\---   
  
Although it’s a favorite headline as people shiver during the coldest parts of the winter, global warming is almost assuredly _not_ behind your suffering (the “warming” part of global warming should have clued you in on this).   
  
But, some folks steadfastly prefer the point of view that all bad weather is caused by climate change.   
  
Consider White House Office of Science and Technology Policy (OSTP) head John Holdren. During the depth of the January 2014 cold outbreak (and the height of the misery) that made “polar vortex” a household name, OSTP released a video featuring Holdren telling us that “the kind of extreme cold being experienced by much of the United States as we speak, is a pattern that we can expect to see with increasing frequency as global warming continues.”   
  
At the time we said “not so fast,” pointing out that there were as many (if not more) findings in the scientific literature that suggested that either a) no relationship exists between global warming and the weather patterns giving rise to mid-latitude cold outbreaks, or b) the opposite is the case (global warming should lead to fewer and milder cold air outbreaks).   
  
The Competitive Enterprise Institute even went as far as to request a formal correction from the White House. The White House responded by saying that the video represented only Holdren’s “personal opinion” and thus no correction was necessary. CEI filed a FOIA request, and after some hemming and hawing, the White House OSTP finally, after a half-hearted search, produced some documents. Unhappy with this outcome, CEI challenged the effort and just this past Monday, a federal court, questioning whether the OSTP acted in “good faith,” granted CEI’s request for discovery.   
  
In the meantime, the scientific literature on this issue continues to accumulate. When a study finds a link between human-caused global warming and winter misery, it makes headlines somewhere. When it doesn’t, that somewhere is usually reduced to here.   




Case in point: Last week, _Washington Post_ ’s Capital Weather Gang published a piece by Jason Samenow that highlighted a pair of new findings that suggested that global warming was leading to more blizzards along the East Coast. The mechanism, favored by the global-warming-is-making-cold/blizzards-worse crowd is that Arctic warming, enhanced by melting sea ice there, is causing the curves (i.e., ridges and troughs) in the jet stream to become bigger, and thus slower. This “locks in” a particular weather pattern and can allow cold air to drop further southward as well as set up condition necessary for big snow storms. To us, this seemed more a case of natural variability than global warming, but we suppose beauty is in the eye of the beholder.   
  
But what you haven’t read in the _Washington Post_ (or anywhere else for that matter), is that an even newer paper has just been published by scientists (including Martin Hoerling) at NOAA’s Earth System Research Laboratory that basically demonstrates that global warming and Arctic sea ice loss should, according to climate models, lead to warmer winter temperatures, less temperature variability, and milder cold air outbreaks. This is basically the opposite conclusion from the one preferred and disseminated by Holdren et al.   
  
From the paper’s abstract:   




The emergence of rapid Arctic warming in recent decades has coincided with unusually cold winters over Northern Hemisphere continents. It has been speculated that this “Warm Arctic, Cold Continents” trend pattern is due to sea ice loss. Here we use multiple models to examine whether such a pattern is indeed forced by sea ice loss specifically, and by anthropogenic forcing in general. While we show much of Arctic amplification in surface warming to result from sea ice loss, we find that neither sea ice loss nor anthropogenic forcing overall to yield trends toward colder continental temperatures. An alternate explanation of the cooling is that it represents a strong articulation of internal atmospheric variability, evidence for which is derived from model data, and physical considerations. Sea ice loss impact on weather variability over the high latitude continents is found, however, characterized by reduced daily temperature variability and fewer cold extremes.



They were even more direct in paper’s conclusion:   




We…showed that sea ice loss impact on daily weather variability over the high latitude continents consists of reduced daily temperature variability and fewer cold extremes indicating that the enhanced occurrences of cold spells during recent winters (e.g., Cohen et al. 2014) are not caused by sea ice loss.



This is pretty emphatic. Global warming results in warmer, less variable winters in North America (Figure 1).   






  
  
_Figure 1. Modeled change in winter mean temperature (left), daily temperature variability (middle), and temperature on the coldest 10 percent of the days (right) as a result of decline in Arctic sea ice. (source: Sun et al., 2016)._   
  
Now, if only our government’s “top scientist” were paying attention.   
  
**Reference:**   
  
Sun, L., J. Perlwitz, and M. Hoerling, 2016. What Caused the Recent “Warm Arctic, Cold Continents” Trend Pattern in Winter Temperatures? _Geophysical Research Letters_ , doi: 10.1002/2016GL069024.


"
"
Share this...FacebookTwitterSweden’s meteorological agency reports that ice coverage on the Baltic Sea is greater than it’s been in nearly a quarter century. Read more here.
About 250,000 square kilometres of the Baltic Sea are now covered in ice according to the Swedish Meteorological and Hydrological Institute (SMHI).
The last time so much of the Baltic was frozen was the winter of 1986-87, when ice covered nearly 400,000 square kilometres of the sea’s surface.
SMHI warns that ice coverage on the Baltic could expand further in the coming days, possibly setting a new record.
The area of interest is in the circle.
Most Baltic Ice in 25 years.
Share this...FacebookTwitter "
"

 **MEMORIES  
LIGHT THE CORNERS OF MY MIND  
MISTY WATERCOLOR MEMORIES  
OF THE WAY WE WERE …  
WHAT’S TOO PAINFUL TO REMEMBER  
WE SIMPLY CHOOSE TO FORGET**



Raisa Burykina, an 85‐​year‐​old pensioner whose father was killed in the battle, said she was pleased to have seen Mr. Putin at the concert.





“Prices went down under Stalin; now they are going up,” said Ms. Burykina, who had a clutch of Soviet orders pinned to her red sweater.  
— _Wall Street Journal_ , February 7, 2018



  
 **BOOTLEGGERS AND BAPTISTS**  
“The president’s budget proposes to replace in significant part the very successful current system of having SNAP recipients use EBT cards to purchase food,”… Jim Weill, president of the Food Research and Action Center, said in a statement.



The proposal is also likely to enrage food retailers — particularly Walmart, Target and Aldi — which stand to lose billions if food‐​stamp benefits are cut, analysts say. On Monday, the Food Marketing Institute, a trade association for grocery stores, condemned the Harvest Box proposal as expensive, inefficient and unlikely to generate any long‐​term savings.  
— _Washington Post_ , February 13, 2018



 **HOW TO GET A HOUSE UNDER SOCIALISM**  
Sexual assault and harassment are rife across all sectors of North Korea’s misogynistic society, according to a new report. …



Many reported that men in positions of authority used their power to take advantage of women.



One woman described going to the mayor’s office to be allocated a house. “I was 32 years old and I must have looked attractive in his eyes. I was raped in his office and received a house in return.”  
— _Washington Post_ , March 8, 2018



 **NO TIME TO MAKE THE DONUTS**  
A French boulanger has been ordered to pay a €3,000 fine for working too hard after he failed to close his shop for one day a week last summer. …



Under local employment law, two separate regulations from 1994 and 2000 require bakers’ shops to close once a week — though exceptions can be made in specific cases.  
— _The Guardian_ , March 14, 2018



 **YOU MIGHT BE, ACTUALLY**  
I can’t be the only one concerned that an increasing amount of the orange juice Americans consume might be sourced in Brazil and Mexico rather than Florida and California.  
— Andrew Furman in the _Wall Street Journal_ , March 23, 2018



 **WHEN YOU PUT GOVERNMENT IN CHARGE OF SOMETHING, YOU’RE PUTTING POLITICIANS IN CHARGE**  
D.C. Council member Trayon White Sr. (D‐​Ward 8) posted the video to his official Facebook page at 7:21 a.m. as snow flurries were hitting the nation’s capital. …



“Man, it just started snowing out of nowhere this morning, man. Y’all better pay attention to this climate control, man, this climate manipulation,” he says. “And D.C. keep talking about, ‘We a resilient city.’ And that’s a model based off the Rothschilds controlling the climate to create natural disasters they can pay for to own the cities, man. Be careful.”  
— _Washington Post_ , March 18, 2018



 **SOME SAY ANTI-SEMITISM IS THE SOCIALISM OF FOOLS, BUT SOMETIMES THE SOCIALISM OF FOOLS IS JUST SOCIALISM**  
President Nicolás Maduro late Thursday briefly outlined his monetary rescue plan. In a country where a dozen eggs can cost 250,000 bolivars ($5) amid worsening inflation, he would chop three zeros off the currency — arguably bringing the price for those eggs down to 250.  
— _Washington Post_ , March 23, 2018



 **AS USUAL**  
Regulators say they may need more power  
— Headline in the _Washington Post_ , February 7, 2018



 **CITIES PLAN FURTHER HOUSING SHORTAGES**  
Lawmakers and advocates in California, Illinois and Washington State are pushing to repeal state laws that forbid rent control or place limits on cities’ ability to regulate rent increases.  
— _Wall Street Journal_ , February 5, 2018
"
"
by John Goetz
In what seems to be a script straight from a Monty Python classic, the good folks of Santa Coloma de Gramenet in Spain seem to have found a rather novel use for the dead: as a tool in the fight against global warming.
From the TimesOnline
November 28, 2008
by Graham Keeley in Barcelona
Spanish graveyard new front in the fight against global  warming
Solar panels are  installed in cemetery

Solar panel in Santa Coloma
A graveyard in Spain has become an unlikely front in the fight against global  warming, with hundreds of black panels placed on top of mausoleums providing  year-round power for homes.
The 462 panels produce 124,374 kilowatts of electricity, enough to supply 60  homes for a year in Santa Coloma de Gramenet, near Barcelona. The exorbitant  price of land in the densely populated satellite city inspired a solar energy  company to propose using one of the last remaining available plots of land – the  cemetery.
Conste-Live Energy and the local council spent three years persuading  relatives of those interred and near-by residents that the unusual proposal  would benefit the living without demeaning the dead. “The best tribute we can  pay to our ancestors is to generate clean energy for new generations,” Esteve  Serret, a company director, said.
The panels cost €720,000 (£612,500) to install and each year will keep about  62 tonnes of carbon dioxide out of the atmosphere, Mr Serret said.
“This is not much, but it will do something to help combat global warming,”  said Bartomeu Muñoz, the Mayor of Santa Coloma. The glinting blue-grey panels  are fixed on top of mausoleums, which in Spain hold five layers of coffins.
The panels, which face south to soak up maximum sunshine, were turned on last  week after three years of planning. Santa Coloma is so densely populated that  all 124,000 inhabitants live within a 4sq km area. Putting solar panels on  coffins was a tough sell, said Antoni Fogué, a city councillor. “Let’s say we  heard things like, ‘They’re crazy. Who do they think they are? What a lack of  respect’,” he said.
City hall and cemetery officials waged a public awareness campaign to explain  the worthiness of the project and the painstaking care with which it would be  carried out. 
Eventually they won over doubters, Mr Fogué said. The panels were erected at  a low angle to be as unobtrusive as possible. “There has not been any problem  because people who go to the cemetery see nothing has changed,” Mr Fogué said.  “This installation is compatible with respect for the deceased and for the  families of the deceased.” 
The cemetery holds the remains of 57,000 people. The solar panels cover less  than 5 per cent of the total area. Community leaders hope to erect more panels  and triple output. Santa Coloma has four solar parks, but the cemetery is the  biggest and the first to attach panels to graves.
When I read this I suddenly recalled the infamous “Bring out yer dead” scene from Monty Python and the Holy Grail:
The Dead Collector: Bring out yer dead.
Large Man with Dead Body: Here’s one.
The Dead Collector: That’ll be ninepence.
The Dead Body That Claims It Isn’t: I’m not dead.
The Dead Collector: What?
Large Man with Dead Body: Nothing. There’s your ninepence.
The Dead Body That Claims It Isn’t: I’m not dead.
The Dead Collector: ‘Ere, he says he’s not dead.
Large Man with Dead Body: Yes he is.
The Dead Body That Claims It Isn’t: I’m not.
The Dead Collector: He isn’t.
Large Man with Dead Body: Well, he will be soon, he’s very ill.
The Dead Body That Claims It Isn’t: I’m getting better.
Large Man with Dead Body: No you’re not, you’ll be stone dead in a moment.
The Dead Collector: Well, I can’t take him like that. It’s against regulations.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a73ec94',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 **The oceans are heating up 40% faster than scientists realized** screamed _Business Insider_ last Saturday (January 12). Two days earlier _The New York Times_ broke the story with “the oceans are heating up 40 percent faster on average than a United Nations panel estimated five years ago.” It’s all from a January 10 article in _Science_ by Lijing Cheng, of the Chinese Academy of Sciences in Bejing, along with three American coauthors, titled “How Fast are the Oceans Warming?”   
  
  
Scary. Not. “40 percent” is a straw man.   
  
  
The subject of all this attention is the change in the heat content of the world’s oceans. This is obviously related to their temperature—something that has proven rather difficult to measure precisely on the centennial scale because of changes in measurement techniques and data sources. (Quants: heat (in joules) divided by the heat capacity (joules required to warm the ocean a degree) gives temperature change).   
  
  
At the outset, it’s important to note that this is not an original research article. It’s a “Perspectives” piece, kind of like a sciency op‐​ed that cites a collection of refereed publications (in this case, with a large number of self‐​citations) that determine the “perspective” of the writers. Quoting from Dr. Roy Spencer’s blog on January 16:   




For those who read the paper, let me warn you: The paper itself does not have enough information to figure out what the authors did…



Further, Spencer notes:   




One of the conclusions of the paper is that Ocean Heat Content (OHC) has been rising more rapidly in the last couple decades than in previous decades, but this is not a new finding, and I will not discuss it further here.   
  
  
Of more concern is _the implication that this paper introduces some new OHC dataset that significantly increases our previous estimates of how much the oceans have been warming._   
  
  
As far as I can tell, **this is not the case**.



The “United Nations panel” in the first paragraph is, of course, its Intergovernmental Panel on Climate Change (IPCC), and in their most recent (2013) science compendium they most certainly did **not** estimate that the heat content of the ocean is 40% less than what it is from Cheng et al’s “perspective.” In that report, they noted five different publications, but found problems with four of them and only conferred credibility upon the highest figure, published by Dominguez et al. in 2008. _The Cheng et al. study is only 11% higher than that, not 40%_. To repeat, the average of the five studies mentioned in the 2013 IPCC report is 40% below the new Cheng et al. figure, but the one that the IPCC found most credible in fact differs from Cheng et al. by only 11%.   
  
  
The 40% figure is therefore a straw man.   
  
  
It’s also noteworthy that the “40 percent” claim is nowhere in the _Science_ Perspective. It’s from a guest post by Cheng et al. in “Carbon Brief,” principally funded by the European Climate Foundation, which describes itself as “a major philanthropic initiative to help Europe foster the development of a low‐​carbon society and play an even stronger international leadership role to mitigate climate change.”   
  
  
_Another Perspective_   
  
  
It is obviously very important to understand historical changes in ocean heat content. Another way to do this would be with the new “reanalysis” data sets, which combine heretofore separate atmospheric observations in the past via a dynamic model. Obviously as one goes further back in time, important data, such as vertical weather balloon soundings drop out, as they did in the 1930s. One important note: the model is modulated with the changes in atmospheric radiation consistent with human emissions of greenhouse gases, ozone, and aerosols, as well as changes in solar radiation.   
  
  
(The relevant paper is by Patrick Layloyaux of the European Center for Medium‐​Range Weather Forecasts, the same people who produce the daily “Euro” model that mid‐​Atlantic forecasters love so much in snow situations. He has 14 co‐​authors, with the majority being from the ECMWF.)   
  
  
Here’s what the ECMWF simulates for the historical heat content (in Joules/​square meter) of the upper 300 meters (984 feet) of the globe’s oceans:   






  
  
  
_Oceanic heat content (joules/​square meter) or the upper 300 meters of the ocean. From Layloyaux et al., 2018._   
  
  
Somehow “ocean heat content as high as it was 75 years ago” isn’t quite so alarming. 
"
"

Last week’s presidential debate revealed to 40 million viewers that global warming is an issue on which the candidates have clear differences, both on policy and in the veracity of their responses. Gov. Bush argued that “some of the scientists…have been changing their opinion a bit on global warming” and that we need a “full accounting” of the issue before creating policy. 



Bush is clearly correct here, and Vice President Gore is not countenancing the whole truth when he cites the supposedly unified opinion of scientists. Many scientists have reappraised global warming, most notably NASA’s James Hansen, who now argues that the rate of warming is much lower than initially forecast because plants are taking up carbon dioxide at an increasing rate. 



In other words, the planet is becoming greener. This is precisely the same position that has been maintained by the coal and oil industries for years. There has been no “full accounting” of global warming because no one has yet been able to devise a system whereby scientists who assess the problem do not also profit from defining it as a problem. 



Gore tried this with his much‐​awaited “National Assessment” of global warming, which is now held up by a lawsuit. The Assessment team, much like Mrs. Clinton’s health care consortium, apparently did an awful lot behind locked doors. 



Gore also intoned that “many people see the strange weather conditions that the old‐​timers say they’ve never seen before in their lifetimes” and that “storms are getting more violent an unpredictable.” 



Those claims are totally false, as anyone who studies weather knows. There are dozens of different weather parameters measured every day: high and low temperatures, rainfall, snowfall, and wind speeds, for example. The chance that an individual will see one of those parameters at an extreme value in their lifetime is exactly 100 percent. Some day must be the hottest day in your life.



Furthermore, the chance that an extreme value will appear in a given year is also high. Let’s work an example with monthly temperature and rainfall. There are 12 months in a year, each of which is ranked according to temperature and precipitation. That’s 48 chances in a year that a given month will be record warm, cold, wet or dry. Most climate information started being recorded in 1948, giving 52 years of data. Rounding the numbers off, if there are the same number of chances to set a record as there are years of observations, the chance that a record will be set this year is 50–50. 



Gore is fond of pointing to increased flood frequency in the United States, based on a study by federal climatologist Tom Karl. But other, equally esteemed climatologists at the U.S. Geological Survey just wrote to Gore’s assessment team admonishing that Karl’s result could be duplicated with random numbers. 



According to the United Nations, hurricane severity is decreasing for the storms that strike the United States. Tornado deaths are also declining. New research shows that, along with global warming, the extreme U.S. coldest temperatures have risen sharply while extreme high temperatures have declined. 



In the policy sphere, Bush’s most intriguing response during the debate was in code. Although the science promoting global warming is shaky at best, Bush is a big supporter of “clean coal technologies” and has proposed spending about $2 billion on their advancement.



This used to be a buzzword for getting pollutants such as sulfur and nitrogen oxides out of the combustion stream. But now it could mean more, such as getting carbon dioxide–the biggest contributor to the greenhouse effect–out, too. That’s a difficult operation, but engineers can do it today. 



How much will this raise the cost of energy? It might not be as expensive as initially suspected. If this technology is imposed on all fossil fuels, coal still comes out as comparatively cheaper, because there are about a jillion tons of it under our feet. Under this scenario, if you believe global warming is serious, locking up federal land and prohibiting mining is about the dumbest thing you can do for the environment, as President Clinton recently did in Utah. It also isn’t very savvy. Coal miners in Democratic West Virginia just announced they support Bush, in large part because of their fear that Gore, in his Jihad against global warming, will dial coal out of the nation’s energy stream.



Who says Dubya is slow? Gore reiterated in the debate that global warming is “the central organizing principal for civilization,” whereas Bush proposed a program that fights climate change and would have the enthusiastic support of both the fossil fuel industry and the United Mine Workers.
"
"
Share this...FacebookTwitterI found this comment from a reader who calls him(her)self “wordsmith”, posted under my “Contact” sidebar. Because it’s Christmas Eve and things are a little hectic, I thought maybe some readers could give me a hand and help out this poor warmist. Here’s his(her) comment:
It’s amazing to me when I hear Climate Change skeptics talk about the misinformation from those who believe it is happening, that it’s primarily caused by man, and that the consequences will be significant. What would be the motivation for so many scientists, the vast majority of scientists, far more educated and intelligent than either of us, to spread misinformation? Do you believe they’re just stupid? More so than you? Is there some economic benefit for them? Do you discount what all scientists say? Also, give me a break–an engineering degree does not in any way qualify you to be a climate expert. I work with a lot of engineers and you don’t have to be that bright to get an engineering degree, especially from some no-name university. MIT, maybe, but not the ones you attended.
He seems annoyed that I’m expressing myself freely at this blog.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterDer Spiegel reports on a paper in Nature written by Hans von Storch and Matthias Zahn claiming that elevated greenhouse gas concentration will lead to fewer North Atlantic storms by the year 2100. Der Spiegel writes:
Instead of 50 to 60, there will only be about half as many Arctic hurricanes, the scientists say.
After every big winter storm, e.g. like Kyrill, we get here in northern Germany, we always hear the media crow about how it is due to global warming. Now the opposite is claimed. We’re a long way from settled science, aren’t we?
The mechanism leading to the Nature paper’s claim is described in the abstract as follows:
This change can be related to changes in the North Atlantic sea surface temperature and mid-troposphere temperature; the latter is found to rise faster than the former so that the resulting stability is increased, hindering the formation or intensification of polar lows.
I can certainly buy that. But then the authors apply bold science.
In the Nature abstract it is written:
Now, in projections for the end of the twenty-first century, we found a significantly lower number of polar lows and a northward shift of their mean genesis region in response to elevated atmospheric greenhouse gas concentration.
The elevated greenhouse gas/lower number of polar effect is quite a hypothesis. The authors are assuming that more CO2 will lead to higher atmospheric temps and thus fewer storms. That’s awfully bold science.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But wait, it gets even bolder, Der Spiegel writes about the scientists:
Using computer models, that also used the climate prognoses of the United Nations, the scientists have played out the development of the northern seas up to the year 2100.
Making a projection for the year 2100 with that methodology? Now that’s really bold. I’m doing all I can to rein in the sarcasm here. The authors then add:
Our results provide a rare example of a climate change effect in which a type of extreme weather is likely to decrease, rather than increase.
I haven’t read the full Nature paper, as it is behind a pay-wall. I just wonder where the authors come up with: “a rare example”. This is probably a case of: whose bread one eats, whose words one speaks, which one has to submit to when dealing with Nature and government funding.
There are actually many examples that warm climates are more beneficial than not. After all, who the hell wants to go back to a little ice age, let alone a big one?
Why not just leave the crap out? I’d have no problem with a paper presenting a couple of if-then hypotheses, like:
1) If temperatures rise, our dynamic models show that there will be fewer storms.
2) If the temperature drops, then there will be more storms.
Leave the global warming faith and religion out of it.
Share this...FacebookTwitter "
"

As the list of Democratic presidential candidates grows, so do their promises. So far, the candidates have largely embraced the same policy focus: expanded entitlement spending to guarantee new welfare benefits.



Massachusetts Sen. Elizabeth Warren recently endorsed a universal federal provision of child care. Vermont Sen. Bernie Sanders has long supported “Medicare for All,” and businessman Andrew Yang is perhaps best‐​known for his advocacy of a universal basic income. Meanwhile senators Cory Booker, Kamala Harris, Kirsten Gillibrand and others have all backed the Green New Deal, which promises to address climate change and inequality by providing universal health care and creating millions of jobs.



While reasonable people can disagree on some aspects of these proposals, one fact is uncontroversial: the United States cannot afford them.





Given that the US cannot afford its existing entitlement programs, adopting the massively expensive policies proposed by Democratic candidates would be foolish.



Congressional Budget Office projections of the federal debt make this point compellingly. According to CBO projections, federal debt held by the public, currently at 78% of America’s gross domestic product, or GDP, will approach 100% in the next decade and reach 152% by 2048. Cutting the debt‐​GDP ratio to its 1957–2007 average within 25 years would require policymakers to permanently reduce spending or increase taxes by 3.8% of GDP, which amounts to about $800 billion, or 24% of federal revenue.



Distinguished economists have recently argued that debt may be less costly in the future because of low interest rates. This assumes we can forecast future rates. In reality, estimates of long‐​run interest rates differ widely and are highly uncertain. Rates have stayed low over the last decade due to a combination of factors, such as monetary policy, weak foreign demand and deleveraging in the wake of the 2008 recession. These forces are unlikely to play as important a role in the future.



More importantly, while low interest rates might permit running a long‐​term deficit that is stable relative to GDP, standard forecasts, such as those from CBO, project rising deficits.



Some Democrats have also claimed that federal debt is not a constraint by relying on “Modern Monetary Theory” (MMT), which argues that central banks can issue enough money to fund federal expenditures with little threat of inflation. But the stable inflation of recent years is precisely due to central banks’ intentional pursuit of price stability as the primary objective of monetary policy. If monetary policy were to focus on funding government spending instead, the threat of inflation would increase rapidly.



Several Democrats have also advocated increasing taxes, but the revenue generated would not come close to funding their proposals. Warren has campaigned for two new wealth taxes that are estimated to increase federal revenue by about $210 billion annually. New York Rep. Alexandria Ocasio‐​Cortez supports a 70% marginal tax rate on income over $10 million, which is estimated to generate an additional $20 billion to $70 billion per year, assuming that wealthy Americans are not discouraged from working. Even if the US adopted all three of these new taxes, annual federal revenue would increase by at most $280 billion.



The additional revenue would hardly make a dent in the cost of Democrats’ policy proposals. For example, Yang’s plan for a universal basic income is estimated to cost $3.8 trillion annually, and the Green New Deal would likely cost upwards of $6.6 trillion per year. Revenue from all three tax increases would fund less than 10% of either of these programs, let alone help pay down existing debt.



Supporters claim that although Medicare for All would massively increase government spending, it would be offset by decreases in private spending that leave total health care expenditures unchanged. While this is plausible in the short run, little evidence suggests that public health care spending would grow more slowly than private spending in the future. Public health care spending in the US has grown faster than private spending over the past 30 years, and other countries with publicly‐​funded health care systems have continued to see increases in expenditures as a share of GDP. An aging US population is likely to increase health care costs in the future, regardless of whether those costs are paid by government.



Given that the US cannot afford its existing entitlement programs, adopting the massively expensive policies proposed by Democratic candidates would be foolish. Instead, the federal government should cut entitlement spending and look for cheaper ways to address problems like climate change.



Instead of the Green New Deal, the federal government could adopt a revenue‐​neutral carbon tax to decrease emissions without exacerbating the fiscal imbalance. Economists from across the political spectrum supportcarbon taxation as the most cost‐​effective way to address climate change. And a carbon tax would be most effective if uniformly adopted by other countries, too.



At a minimum, the US should slow the growth of entitlement spending to no more than the average growth of GDP. Reasonable approaches include increasing the age of eligibility for Social Security and Medicare, modifying the indexation of Social Security benefits or tightening eligibility requirements for disability insurance. These reforms would keep entitlement spending affordable, unlike current policy.



Reasonable people can disagree on the benefits of federal welfare programs, the appropriate level of redistribution or optimal cost‐​cutting reforms. But everyone should agree that restoring fiscal sanity in the United States requires significant cuts to federal entitlement spending. The policies advocated by Democratic candidates will only make things worse.
"
"**Labour activists in the US say big retailers like Amazon and Walmart must do more to protect workers as surging coronavirus cases coincide with the holiday shopping rush.**
They are calling for hazard pay, paid sick leave and better communication about outbreaks, among other things.
The campaign comes as workers across the US have spoken out about condition and concerns over their health.
""Associates like me are scared,"" said Walmart worker Melissa Love.
The workers rights campaign launched on Monday was organised by United for Respect, a workers rights non-profit that says it represents more than 16 million people across the US.
Separately, the labour union UFCW, whose members include grocery and meatpacking plant workers, also called on employers to do more to protect staff.
""Simply put, frontline workers are terrified because their employers and our elected leaders are not doing enough to protect them and stop the spread of this virus,"" UFCW International President Marc Perrone said.
""As holiday shopping begins this Thanksgiving, we are already seeing a huge surge of customer traffic. Unless we take immediate actions beginning this holiday week, many more essential workers will become sick and more, tragically, will die.""
Ms Love, a member of United for Respect who has worked at Walmart for five years, said on a call organised for reporters that she feared a rush of holiday shoppers could turn Walmart into a ""super-spreader"" hub.
""Working Black Friday this year comes with an obvious danger,"" said Ms Love, who is based in California. ""I do not believe Walmart should be trying to entice crowds into our stores on Friday and risk a super-spreader event.""
Courtenay Brown, another member of the group, who picks groceries for Amazon, said the company has had to send out repeated notifications in recent days about infected staff. Amazon had boosted pay for frontline staff by $2 (Â£1.50) an hour, but that policy ended in June.
""Right now it's what we call the turkey apocalypse, where we are forced to just push out as much as we possibly can,"" said Ms Brown, who works in New Jersey.
She said she's happy to have a job, but to Amazon ""my life doesn't really mean much - it's just a means for Bezos to continue making billions off of us"".
Workers said the companies had the means to spend more to protect and compensate workers, noting the way profits have soared during the pandemic, which has shifted purchases to essentials and kept many smaller competitors closed.
Amazon and Walmart did not respond to requests for comment on Monday, but they have defended their practices in the past.
Walmart has changed how it is handling Black Friday discounts, offering the deals online and over several days to try to avoid crowding at its stores.
Amazon last month said nearly 20,000 people, or 1.4% of its staff, at Amazon and Whole Foods, the grocery store it owns, had tested positive for Covid-19 in the US since the start of the pandemic.
It said that was lower than would be expected given wider infection rates.
""All in, we've introduced or changed over 150 processes to ensure the health and safety of our teams, including distributing over 100 million face masks, implementing temperature checks at sites around the world, mandating enhanced cleaning procedures at all of our sites, and introducing extensive social distancing measures to reduce the risk for our employees,"" the company said.
More than 250,000 people in the US have died from coronavirus since the start of the pandemic. In recent weeks, case numbers, death rates and hospitalisations have soared, straining the health system and prompting many places to re-impose restrictions.
Some major retailers, including Amazon and Walmart, have posted strong results this year as many customers opted to buy online instead of venturing out to the local store.
A recent analysis by the Brookings Institute, a Washington think tank, found that company profits at 13 of America's biggest retailers increased by an average of 39% this year, while pay for frontline workers had increased by just $1.11 per hour - ""a 10% increase on top of wages that are often too low to meet a family's basic needs"".
Workers said they are well aware of the disparity.
""They closed the corporate office until July 2021 because of the virus meanwhile we're expected to keep risking our lives to pay for their big salaries,"" Ms Love said."
"

Rarely is federal legislation something other than a vehicle for government overreach and aggrandizement. Despite its populist‐​sounding title, the Global Investment in American Jobs Act – introduced in the Senate last week – is one of those rarities. The bill calls for an assessment of U.S. policies that influence decisions by foreigners about investing in the United States.   
  
  
Properly modest in scope, the legislation simply authorizes to Commerce Department to produce a report that documents the importance of foreign investment, identifies home‐​grown impediments to such investment, and recommends policy changes that would make the United States a more attractive investment destination.   
  
  
What is so refreshing about the bill is that its premise is not that the practices of foreign governments or the greed of U.S. corporations that allegedly “ship jobs overseas” are to blame for U.S. economic stagnation – themes so prominent in the past couple of Congresses and the current White House. Rather, the premise is that U.S. policy and its accumulated residue have created a web of impediments that discourage foreign investment in the United States, and that changes to those policies could serve to attract new investment. This kind of thinking is long overdue.   
  
  
One of the themes of my 2009 Cato paper, “Made on Earth: How Global Economic Integration Renders Trade Policy Obsolete,” is that it is no longer apt to consider global commerce a competition between “Us” and “Them.” Trans‐​national production/​supply chains and cross‐​border investment have blurred the distinctions between “our” producers and “their” producers. Many products and services are created along supply chains that travel from idea conception to final consumption and that include value‐​added activities of varying degrees of labor‐, physical capital‐, and intellectual capital‐​intensity. Furthermore, the largest U.S. steel producer, Mittal, is a majority Indian‐​owned company with corporate headquarters in Luxembourg. The largest “German” steel producer, Thyssen‐​Krupp, recently completed construction of a $4 billion production facility near Mobile, Alabama for the purpose of serving U.S. demand for finished steel, particularly from the mostly foreign nameplate auto producers dotting the landscape of the American South. And, as of 2010, our beloved General Motors produces more vehicles in China than it does in the United States. So, really, who are “we” and who are “they”?   
  
  
Accordingly, instead of pursuing a 20th century trade policy model that seeks to secure market‐​access advantages for certain producers, policy should be recalibrated to reflect the 21st century reality that governments around the world are competing for business investment and talent, which both tend to flow to jurisdictions where the rule of law is clear and abided; where there is greater certainty to the business and political climate; where the specter of asset expropriation is negligible; where physical and administrative infrastructure is in good shape; where the local work force is productive; where there are limited physical, political, and regulatory barriers, etc. This global competition in policy is a positive development because — among other reasons — its serves to discipline bad government policy.   
  
  
We are kidding ourselves if we think that the United States is somehow immune from this dynamic and does not have to compete and earn its share with good policies. The decisions made now with respect to our policies on immigration, education, energy, trade, entitlements, taxes, regulations, industrial management, and the proper role of government in the economic sphere will determine the health, competitiveness, and relative significance of the U.S. economy in the decades ahead.   
  
  
Governments with the smartest policies will be the ones that secure the most investment, the strongest talent, and the best economic opportunities for their people. The legislation is step in the right direction.
"
"
Share this...FacebookTwitterA new temperature reconstruction carried out by a team of German/Russian scientists has yielded interesting results. It finds no correlation over the last 400 years between atmospheric CO2 and the temperature in the Arctic regions studied.


Yuri Kononov of the Institute of Geography, Russian Academy of Sciences in Moscow and Michael Friedrich of the Institute of Botany, University of Hohenheim collect tree samples of Scots pine in the Khibiny Low Mountains of the Kola Peninsula in Arctic Russia

Recall that CO2 concentrations have been rising steadily since the start of the Industrial Revolution, 1870, yet the press release starts with:
Parts of the Arctic have cooled clearly over the past [20th]century, but temperatures have been rising steeply there since 1990.
Rising since 1990? That’s more than 100 years after the start of the Industrial Revolution. The press release continues:
The reconstructed summer temperature on Kola in the months of July and August has varied between 10.4°C (1709) and [peaking at] 14.7°C (1957), with a mean of 12.2°C.  Afterwards, after a cooling phase, an ongoing warming can be observed from 1990 onwards.
The temperature fluctuated between 10.4°C  and a peak of 14.7°C in 1957 , and then cooled until 1990. The scientists say it correlated very well with solar activity until 1990. Then beginning in 1990, the temperatures started to rise rapidly again. Does anyone see a CO2 correlation there? I don’t.
The only time we have a correlation between CO2 concentrations and temperature is from 1990 until…? Unfortunately the press release does not even mention that.  Until today? 2005? 2000? It really is annoying that they didn’t specify the end of the time scale. If the reconstruction was only up to 2000, then we are only speaking about a 10-year period – a completely meaningless time period. Even 20 years would be highly dubious.
I called UFZ early this afternoon here in Germany to try to find out, but the secretary said that all scientists had already left for the weekend.
*****************************************************
UPDATE! The German press release now has the following graphic. The dataset ended 2001! The press spokesman just told me on the phone. So there was warming from 1990 until 2001!  As you see, the graphic iteself is misleading. It almost looks as if the curve goes until 2010.
Press spokesman Tilo Arnhold informed by telephone that the dataset goes only up to 2001, yet the press release graphic clearly shows a curve beyond 2001. Graphic Source: Stephan Boehme/UFZ
Interestingly, also, the graphic shows warming since 1650.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




******************************************************
The press release also states:
What stands out in the data from the Kola Peninsula is that the highest temperatures were found in the period around 1935 and 1955, and that by 1990 the curve had fallen to the 1870 level, which corresponds to the start of the Industrial Age.
The temperature fell to 1870-levels by 1990? Wait a minute – the CO2 theory say it’s supposed to go up, and not down.
The team compared their Kola region data to Swedish Lapland and the Yamal and Tamimyr Penninsula temperature reconstructions: Here’s what they found:
The reconstructed summer temperatures of the last four centuries from Lapland and the Kola and Taimyr Peninsulas are similar in that all three data series display a temperature peak in the middle of the twentieth century, followed by a cooling of one or two degrees.
Cooling from the middle of the twentieth century until 1990. Cooling!
Concerning Yamal, it is no surprise that it’s out of the ballpark because that dataset was not handled scientifically, see Yamal-gate
(Keith Briffa cherrypicks tree rings to get the temperature reconstruction he wants to see).
So if it’s not CO2, then what could be driving temperature?  The press release goes on:
What is conspicuous about the new data is that the reconstructed minimum temperatures coincide exactly with times of low solar activity. The researchers therefore assume that in the past, solar activity was a significant factor contributing to summer temperature fluctuations in the Arctic.
The only mystery left is why was there was a sudden increase in warming from 1990 until 2001? The scientists believe it has to do with local factors. Clearly it isn’t CO2.
Share this...FacebookTwitter "
"

Part III: Where does global warming rank among future risks to environmental health?

Guest essay by Indur M. Goklany 
NOTE: Entire 3 part series is now available as a PDF here

In Part 1 of this series we saw that even if one gives credence to the oft-repeated but flawed estimates from the World Health Organization of the present-day contribution of climate change to global mortality, other factors contribute many times more to the global death toll. For example, hunger’s contribution is over twenty times larger, unsafe water’s is ten times greater, and malaria’s is six times larger. With respect to ecological factors, habitat conversion continues to be the single largest demonstrated threat to species and biodiversity. Thus climate change is not the most important problem facing today’s population.
In Part 2 we saw that even if we assume that the world follows the IPCC’s warmest (A1FI) scenario that the UK’s Hadley Center projects will increase average global temperature by 4°C between 1990 and 2085, climate change will at most contribute no more than 10% of the cumulative death toll from hunger, malaria and flooding into the foreseeable future. It would simultaneously reduce the net population at risk of water stress.
Clearly, climate change would, through the foreseeable future, be a bit-player with respect to human well-being.
Here I will examine whether climate change is likely to be the most important global ecological problem in the foreseeable future.
As in Part 2, I will rely on estimates of the global impacts of climate change from the British-government sponsored “Fast Track Assessments” (FTAs).
The following figure, which presents the FTA’s estimates of habitat converted globally to cropland as of 2100, shows that the amount of habitat lost to cropland may well be least under the richest-but-warmest scenario (A1FI), but higher under the cooler (B1 and B2) scenarios. Thus, despite a population increase, cropland could decline from 11.6% in the base year (1990) to less than half that (5.0%) in 2100 under the warmest (A1FI) scenario.  That is, climate change may well relieve today’s largest threat to species and biodiversity!
One reason for this result is that higher atmospheric concentrations of CO2 might make agriculture more efficient, and this productivity increase would not have been vitiated as of 2100 by any detrimental impacts of higher temperatures.

The next figure shows that in 2085 non-climate-change related factors will dominate the global loss of coastal wetlands between 1990 and 2085.

[In this figure, SLR = sea level rise. Note that the losses due to SLR and “other causes” are not additive, because a parcel of wetland can only be lost once. For detailed sources, see here.]
Thus we see that neither on grounds of public health nor on ecological factors is climate change likely to be the most important problem facing the globe this century.
So the next time anyone claims that climate change is the most important environmental problem facing the globe now or whenever, ask to see their proof that climate change outranks other problems.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9626ae4b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterUpdate 7/30/2010: WUWT debunks this scare: here!
Reading the German online daily news this morning, today’s scare-de-jour is the “shocking” reduction of phytoplankton now underway, all due of course to manmade climate change. The “news” is based on a report just published in Nature by scientists Daniel Boyce and Marlon Lewis of the Dalhousie University (Halifax) and Boris Worm of the German Potsdam Institute for Climate Impact Research (think Schellnhuber and Rahmstorf).
When reading about such stories,  it’s a very good idea to first read the following:
Science Turns Authoritarian
Here’s a sampling of today’s headlines in Germany:
Die Welt PLANKTON REDUCTION PUTS FOOD CHAIN AT RISK
Der Spiegel: FOOD CRISIS IN WORLD’S OCEANS
Focus: GREEN FOOD SUPPLY IN OCEANS SHRINKING
Süddeuschte Zeitung: DISAPPEARING IN THE OCEANS
The cause of the phytoplankton decrease is “warming of the oceans”, Nature reports. Satellite measurements since the end of the 1970s have shown fluctuation in oceanic plankton levels, but have not delivered a clear picture.
That’s why the researchers went back and looked at data of ocean chlorophyll content. The team analysed almost 450,000 measurements from the period 1899 to 2008. The result, according to Die Welt:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In eight of ten oceanic regions, phytoplankton has decreased during the 20th century. Content dropped with increasing sea surface temperature, especially in the tropics and subtopics regions. It is suspected that as a consequence of warming, a more pronounced layering of water occurred.
Süddeutsche Zeitung writes that phytoplankton concentrations in the oceans have declined by two thirds since 1899.
What do I think?
Schellnhuber: Visionary of The Great Transformation
The study involves the Potsdam Institute For Climate Impact Research, directed by Hans Joachim Schellnhuber, visionary of The Great Transformation , and Stefan Rahmstorf, who predicts sea levels will rise 1.7 meters in the next 90 years, but refuses to even bet on a 60 cm rise. This is an institute that is well well-known for activist science and fear-mongering, with the clear agenda of reorganizing how people live. A dangerous social engineering experiment.
I’m sure the findings of this study will turn out to be more plankton-crap.
UPDATE: Beware of science authority. Above I mentioned the Science Turns Authoritarian story, which looked at how often certain authoritarian phrases are used in the media. Click on the following graphic:

Be wary, be very wary, of media claims on climate science.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOh the irony!
Potsdam is the home of the alarmist Potsdam Institute for Climate Impact Research, headed by Professor Hans-Joachim Schellnhuber and Stefan Rahmstorf, who has been very busy lately trying to convince the rest of the world that the cold is due to the warming, at least that’s what his models are saying (now).
Snowiest December ever
Potsdam is now well on its way to recording its snowiest December since records began in 1893. The Meteorological Station Potsdam Telegrafenberg has been recording a wide variety of weather parameters since 1893, and looking at the records, no December compares to the one they are having now.
Daily snow cover in Potsdam: Source: http://saekular.pik-potsdam.de/2007_en/index.html
As Rahmstorf and Schellnhuber watch crews remove snow every day at the PIK, they really must be cursing all the white stuff that was never supposed to happen.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some December data at the PIK – records smashed
On December 21 the station had a snow cover of 33 cm, a December record, smashing the old record of 23 cm set 97 years ago in 1913.
This December will have had snow cover 30 of 31 days (No thaw is in sight) – the 2nd most, just behind 1969 (see Figure below), which saw 31 days of snow on the ground. Currently there is 23 cm of snow cover, which was the old record.
Number of days with snow cover in December. Source; http://saekular.pik-potsdam.de/2007_en/index.html
The average so far for this December has been about 16 cm of snow cover. With the current forecast, that too will not change. If anything it may increase. That will make it an all-time record.
It’s been one of the coldest Decembers in Potsdam since records started. Temperatures for this December in Potsdam have averaged near -4°C so far.
The snow has not only been frequent in Potsdam, but all over Germany. Many cities are poised to set new records for most days with snow cover on the ground. Read here.
Share this...FacebookTwitter "
"


A glacial region in Norway (Source:  NRK)
Reposted from the DailyTech
By: Mike Asher


Scandinavian nation reverses trend, mirrors  results in Alaska, elsewhere.
After years of decline, glaciers in Norway are again growing, reports the  Norwegian Water Resources and Energy Directorate (NVE). The actual magnitude of  the growth, which appears to have begun over the last two years, has not yet  been quantified, says NVE Senior Engineer Hallgeir Elvehøy.
The flow rate of many glaciers has also declined. Glacier flow ultimately  acts to reduce accumulation, as the ice moves to lower, warmer  elevations.
The original trend had been fairly rapid decline since the  year 2000.  
The developments were originally reported by the Norwegian  Broadcasting Corporation (NRK).
DailyTech has previously reported on the  growth in Alaskan glaciers, reversing a 250-year trend of loss. Some  glaciers in Canada, California, and New Zealand are also growing, as the result  of both colder temperatures and increased snowfall.
Ed Josberger, a  glaciologist with the U.S. Geological Survey, says the growth is “a bit of an  anomaly”, but not to be unexpected.
Despite the recent growth, most glaciers in the nation are still smaller than  they were in 1982. However, Elvehøy says that the glaciers were even smaller  during the ‘Medieval Warm Period’ of the Viking Era, prior to around the year  1350.
Not all Norwegian glaciers appear to be affected, most notably those in the  Jotenheimen region of Southern Norway.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ac52297',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
WUWT readers may remember last year that we had an early outbreak of Tornado season, and media opportunist Senator John Kerry immediately jumped at the chance to blame the weather event on “global warming” as we reported here on WUWT:
Kerry appeared on MSNBC on February 6 to discuss storms that have killed at least 50 people throughout the Southeastern United States. So, of course, Kerry used the platform to advance global warming alarmism.
“[I] don’t want to sort of leap into the larger meaning of, you know, inappropriately, but on the other hand, the weather service has told us we are going to have more and more intense storms,” Kerry said. “And insurance companies are beginning to look at this issue and understand this is related to the intensity of storms that is related to the warming of the earth. And so it goes to global warming and larger issues that we’re not paying attention to. The fact is the hurricanes are more intensive, the storms are more intensive and the rainfall is more intense at certain places at certain times and the weather patterns have changed.”
See the original WUWT report here.
So, this year is a little bit different. We have a late and slow start to tornado season. Always a good thing. That being said, this report from Joe D’Aleo discusses why its been slow, and debunks a recent Weather Channel claim that the current deficit of tornadoes has something to do with “global warming”.  Seeing how global warming causes both individual tornado events and decreased tornado events, I’ve apparently terribly underestimated its omnipotent power to influence weather. 😉

Graph from NWS/NOAA. Smaller (F1) tornadoes seem to be on the increase, but not larger ones.
Even though tornado reports seem to be on the rise, the larger damaging tornados, F2-F5 don’t seem to be. There are some good reasons for this, and it might be a good primer for readers to revisit this report I made about the issue of tornado reporting:
Increasing tornadoes or better information gathering?
BTW, if anyone wants a really cool weather radar program for tracking severe storms, please see my StormPredator program here – Anthony

Tornado Season So Far Not as Bad as 2008
By Joseph D’Aleo, CCM, on Intellicast
After another La Nina season with again a lot of snow and precipitation in the north central, another relatively active tornado season was expected and so far it has delivered on that promise. However given the La Nina was not as strong and the rebound in the Pacific towards El Nino is a month earlier than last year, the number of storms so far, have been less. It looks like May will fall well short of last May’s 461 tornadoes.

Cedar Hill, Texas, Photo credit Pat Skinner, TTU
The annual summary to date can be found here. The tornadoes so far in 2009 have been in the southeast quadrant of the nation. Climatologically, that is where the season normally begins.

See larger image here.
In 2008, the tornadoes when all was said and done, were found in all but 4 of the lower 48 states.

See larger image here.
As we move into summer, expect the activity to shift north with the jet stream. The march of the season – climatology of tornadoes normally follows this depiction (source here).
We are below the 5 year average for tornadoes for the season to date, below all but 2005.

See larger image here.
The activity was as usual concentrated in what is called tornado alley in the plains, Midwest to the Gulf. You can see in 2008 that the daily events peaked in May with the biggest day on the 23rd of May before falling off in summer as El Nino like conditions developed in the eastern Pacific. That is occurring a month earlier this year and perhaps, that explains the quieter May.

See larger image here.
The activity was as usual concentrated in what is called tornado alley in the plains, Midwest to the Gulf.

The reason that this region is most vulnerable is that this is where the combination of moisture from the Gulf of Mexico, dry air in mid levels from Mexico, a strong jet with cold air aloft coming out of the Rockies and a boundary between still cool air to the north and the warm humid summer like air in the south all come together.  Read more and see some useful links here. 
By the way, last year, the alarmist media blamed the increase in tornados on global warming. Well guess what this year, stormchasers across Tornado Alley have been frustrated this spring by what seems to be a lack of tornadoes and severe weather.  Indeed, VORTEX2, the largest tornado field study ever, has been running for more than two weeks now and has not seen one twister.  Last week, Weather Channel Senior Meteorologist Stu Ostro speculated that global warming was the cause. Of course this is the normal, mother nature has a perverse sense of humor – projects to study east coast storms had no east coast storms that winter, just one passing front. You just knew that VORTEX2 would lead to a lack of storms to study. We need them to schedule a massive study of hurricanes and we will surely have a dud season.
As to Stu’s reasoning, a big ridge would lead to heat and dryness across the central states. Instead the region has been hit hard with a steady stream of progressive troughs with heavy rainfall and very little warmth (fuel for storms). These storms brought more toirnadoes in April 2009 than in April 2008.

See larger image here.
The real issue may instead be that the El Nino like conditions that came in late May last year shutting off severe weather activity in June 2008 came on a month earlier this year and severe weather activity has diminished in May.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e961a4c69',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Last December the United States agreed at a United Nations meeting in Kyoto, Japan, to reduce its emissions of greenhouse gases by 7 percent below 1990 levels. That reduction, to be achieved mainly by cutting the combustion of fossil fuels, will lower emission levels 41 percent below where they will likely be in the year 2010 if the trend observed since 1990 continues. 



The Kyoto agreement–if fully complied with–would likely reduce the gross domestic product of the United States by 2.3 percent per year. However, according to a climate model of the National Center for Atmospheric Research recently featured in _Science_ , the Kyoto emission‐​control commitments would reduce mean planetary warming by a mere 0.19 degree Celsius over the next 50 years. If the costs of preventing additional warming were to remain constant, the Kyoto Protocol would cost a remarkable 12 percent of GDP per degree of warming prevented annually over a 50‐​year period. 



The Kyoto Protocol will have no discernible effect on global climate–in fact, it is doubtful that the current network of surface thermometers could distinguish a change on the order of .19 degree from normal year‐​to‐​year variations. The Kyoto Protocol will result in no demonstrable climate change but easily demonstrable economic damage. 
"
"
Another anemic solar cycle 23 sunspeck, could 19th century astronomers have seen it?
From Spaceweather.com

SUNSPOT 1016: A ring-shaped sunspot numbered 1016 has emerged near the sun’s equator. Its magnetic                polarity identifies it as a member of old Solar Cycle 23. Until                these old cycle sunspots go away, the next solar cycle will remain                in                abeyance.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96505207',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From NASA Science News h/t to John-X
Spotless Sun: 2008 is the Blankest Year of the Space Age
Sept. 30, 2008: Astronomers who count sunspots have announced that 2008 is now the “blankest year” of the Space Age.
As                      of Sept. 27, 2008, the sun had been blank, i.e.,                      had no visible sunspots, on 200 days of the year. To find                      a year with more blank suns, you have to go back to 1954,                      three years before the launch of Sputnik, when the sun was                      blank 241 times.
“Sunspot counts are at a 50-year low,” says solar physicist David Hathaway of the NASA Marshall Space Flight Center. “We’re experiencing a deep minimum of the solar cycle.”

Above: A histogram showing the blankest years of the last half-century. The vertical axis is a count of spotless days in each year. The bar for 2008, which was updated on Sept. 27th, is still growing. [Larger images: 50                      years, 100 years]
A spotless day                      looks like this:

The image, taken by the Solar and Heliospheric Observatory (SOHO) on Sept. 27, 2008, shows a solar disk completely unmarked by sunspots. For comparison, a SOHO image taken seven years earlier on Sept. 27, 2001, is peppered with colossal sunspots, all crackling with solar flares: image. The difference is the phase of the 11-year solar cycle. 2001 was a year of solar maximum, with lots of sunspots, solar flares and geomagnetic storms. 2008 is at the cycle’s opposite extreme, solar minimum, a quiet time on the sun.
And                      it is a very quiet time. If solar activity continues as low as it has been, 2008 could rack up a whopping 290 spotless days by the end of December, making it a century-level year in terms of spotlessness.
Hathaway cautions that this development may sound more exciting than it actually is: “While the solar minimum of 2008 is shaping up to be the deepest of the Space Age, it is still unremarkable compared to the long and deep solar minima of the late 19th and early 20th centuries.” Those earlier minima routinely racked up 200 to 300 spotless days per year.
Some                      solar physicists are welcoming the lull.
“This gives us a chance to study the sun without the complications of sunspots,” says Dean Pesnell of the Goddard Space Flight Center. “Right now we have the best instrumentation in history looking at the sun. There is a whole fleet of spacecraft devoted to solar physics–SOHO, Hinode, ACE, STEREO and others. We’re bound to learn new things during this long solar minimum.”
As an example he offers helioseismology: “By monitoring the sun’s vibrating surface, helioseismologists can probe the stellar interior in much the same way geologists use earthquakes to probe inside Earth. With sunspots out of the way, we gain a better view of the sun’s subsurface winds and inner magnetic dynamo.””There is also the matter of solar irradiance,” adds Pesnell. “Researchers are now seeing the dimmest sun in their records. The change is small, just a fraction of a percent, but significant. Questions about effects on climate are natural if the sun continues to dim.”
Pesnell is NASA’s project scientist for the Solar Dynamics Observatory (SDO), a new spacecraft equipped to study both solar irradiance and helioseismic waves. Construction of SDO is complete, he says, and it has passed pre-launch vibration and thermal testing. “We are ready to launch! Solar minimum is a great time to go.”
Coinciding with the string of blank suns is a 50-year record low in solar wind pressure, a recent discovery of the Ulysses spacecraft. (See the Science@NASA story Solar                      Wind Loses Pressure.) The pressure drop began years before the current minimum, so it is unclear how the two phenomena are connected, if at all. This is another mystery for SDO and the others.
Who                      knew the blank sun could be so interesting?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c0c0072',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterMinus 30°C for days…13°C below normal…homeless people dying…hands and feet are freezing…
That’s what we are hearing from a few media outlets in Europe, those who have dared to mention the “cold-snap” word and to write about reality. It’s been cold in Scandinavia, much of Europe, North America and Russia too. Where’s all the warming? Heck, even the oceans are below normal.
Deep freeze is forecast to continue
The European part of Russia is stuck deep in the freezer, reports the Austrian online Krone.at. The extreme cold is due to a huge high pressure system in the Arctic which has kept Moscow in temperatures down as low as -30°C for days. Krone.at writes:
The Russian media have been talking about ‘the hardest winter in the last 100 years’, causing 10 million people to shiver.
”This abnormal frost has been an enormous challenge,’ says Moscow mayor Sergei Sobjanin. Meteorologists don’t see any let up in the days ahead, and even expect temperatures to drop further. In the European part of Russia, unusually deep cold has dominated the area over the last 14 days. The average temperature for February so far alone for Moscow is 11 to 13°C below normal.”
There are reports that homeless people are getting hit hard. Pleas for blankets and clothing are being made. Famous Moscow doctor Elisabeth Glinki says:
Many people on the street are dying, or their hands and feet are freezing.”
Looking at the above temperature forecast chart above, things are going to get even worse in the days ahead.
But we all know what the explanation for this is, right!
Share this...FacebookTwitter "
"
Guest Post by Steven Goddard
January, 1790 was a remarkable year in the northeastern US for several reasons.  It was less than one year into George Washington’s first term, and it was one of the warmest winter months on record.  Fortunately for science, a diligent Philadelphia resident named Charles Pierce kept a detailed record of the monthly weather from 1790 through 1847, and his record is archived by Google Books.  Below is his monthly report from that book.
JANUARY 1790 The average or medium temperature of this month was 44 degrees This is the mildest month of January on record. Fogs prevailed very much in the morning but a hot sun soon dispersed them and the mercury often ran up to 70 in the shade at mid day. Boys were often seen swimming in the Delaware and Schuylkill rivers. There were frequent showers as in April some of which were accompanied by thunder and lightning The uncommon mildness of the weather continued until the 7th of February.
Compare that to January, 2009 with an average temperature of 27F, 17 degrees cooler than 1790.  One month of course is not indicative of the climate, so let us look at the 30 year period from 1790-1819 and compare that to the last 10 “hot” years.
From Charles Pierce’s records, the average January temperature in Philadelphia from 1790-1819 was 31.2F.  According to USHCN records from 2000-2006 (the last year available from USHCN) and Weather Underground records from 2007-2009, the average January temperature in Philadelphia for the last ten years has been 29.8 degrees, or 1.4 degrees cooler than the period 1790-1819.  January, 2009 has been colder than any January during the presidencies of Washington, Adams, Jefferson, or Monroe.  January 2003 and 2004 were both considerably colder than any January during the terms of the first five presidents of the US.  Data can be seen here.
According to several of the most widely quoted climate scientists in the world, winters were much colder 200 years ago than now – yet the boys swimming in the Delaware in January, 1790 apparently were unaware.
Another interesting fact which can be derived from Charles Pierce’s data, is that January temperatures cooled dramatically during the period 1790-1819 – as can be seen in the graph below.  The cooling rate was 13F/century.  What could have caused this cooling?  We are told by some experts that variations in solar activity can only affect the earth’s temperature by a few tenths of a degree.  CO2 levels had been rising since the start of the industrial age.  The downward trend is fairly linear and does not show any sharp downward spikes, so it is unlikely to be due to volcanic activity.  What other “natural variability” could have caused such a dramatic drop in temperature?

Looking at the sunspot records for that period, something that clearly stands out is that solar cycle 4 was very long, and was followed by a deep minimum lasting several decades.  Perhaps a coincidence, but if not – Philadelphia may well be in for some more very cold weather in coming winters.

Source for graph:
http://www.springerlink.com/content/k37032647541h753/fulltext.pdf


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99659303',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe blogosphere is now filled wth Arctic sea ice projections and opinions, and why not? It’s a game that adds fun to the science. For example Steve Goddard at WUWT projects 2010 will finish above 5.5 million sq km.
My forecast remains unchanged. 5.5 million, finishing above 2009 and below 2006. Same as it has been since May.
August 22, 2010: Source: UI Atmos-Cryosphere
I’d be happy if that became true because it would be real sand in the gears of the death-spiral worshippers. But seriously I doubt it will keep above or at the 5.5 million mark. That’s a real long shot.

Maybe a different dataset is being used for this claim. The shot above shows there is still plenty of potential for melt.
Weeks ago I projected, using my crystal ball, that we would finish a bit below 5 million. That’s turning out to be a wee bit too pessimistic, but not too much though.
On August 22, Arctic sea ice according to JAXA was at 5.628 million sq km. That means only 128,000 sq km over the 5.5 million mark with still about 20 days left before reaching the average low point. That’s a lot of time.
August 22, 2010
Over the last 8 years we have seen that from August 22 until reaching the low point, Arctic sea ice has lost an average of 535,000 sq km.
The minimum loss for that period was 279,000 sq km in 2006 and the maximum was 837,000 in 2008. Over the last 4 years, the average was a loss of 582,000 for the period.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Taking the average of the last 4 years, and we get a 2010 minimum ice extent of 5.628 – 0.582 = 5.04 million sq km. So, forget the 5.5 million mark being the minimum. It aint gonna happen.
The Arctic is not projected to get very cold in the next two weeks, see here: http://wxmaps.org/pix/temp2.html.
The temperature above 80°N latitude is also staying stubbornly near the freezing point. So expect Arctic sea ice, using JAXA, to get down closer to the 5 million mark.

Source: Danish Meteorological Institute.
Yes, of course, there are lots of other factors, wind, blah blah, blah, but only 100,000 sq. km of melt over the next 20 days could only happen if Al Gore showed up at the North Pole.
UPDATE: Steve Goddard at WUWT has backed off his 5.5 million sq km projection. http://wattsupwiththat.com/2010/08/24/sewa-ice-news-arctic-mid-week-update/
 
Share this...FacebookTwitter "
"

The hallmark of liberalism has long been its perceived commitment to individual rights. But not this administration. Bill Clinton is a new kind of Democrat–a jackboot liberal.



While the president has been out lobbying weather forecasters about the alleged threat of global warming, his interior secretary, Bruce Babbitt has been attacking energy companies for criticizing administration scare‐​mongering. Mr. Babbitt charged the firms with attempting “to distort the facts and to mislead,” adding: “I think that the energy companies need to be called to account, because what they are doing is un‐​American in the most basic sense.” He left unsaid how he would call such “un‐​American” businesses “to account,” but climate scientists report that the administration has long used its control of grants to punish researchers who question the climatic Chicken Littles.



Mr. Babbitt’s implicit threat unfortunately reflects the administration norm. In many cases, Clinton officials have directly targeted critics. More generally, warns Timothy Lynch, assistant director of the Cato Institute’s Center for Constitutional Studies: “Although President Clinton has expressed support for an ‘expansive’ view of the Constitution and the Bill of Rights, he has actually weakened a number of fundamental guarantees.”



The administration has politicized the FBI, using it to justify the White House Travel Office purge. Presidential aides snooped through FBI files on potential administration opponents. The IRS is auditing not only Paula Jones, who has accused Bill Clinton of sexual harassment, but a suspiciously large number of conservative foundations and groups. No liberal organizations are undergoing similar reviews. The White House pressured the Treasury Department over the latter’s probe of Madison Guaranty, which financed the Clintons’ Whitewater investment.



Early in the first Clinton term, the Department of Housing and Urban Development launched dozens of investigations of local activists who opposed federally subsidized housing projects. HUD subpoenaed copies of organization membership lists and financial information, people’s diaries, and other records, demanded cessation of public criticism, and threatened protestors with prosecution for speaking out.



Similarly, in 1995 the U.S. Commission on Civil Rights issued subpoenas to leaders of two anti‐​immigration groups. The commission, whose chairman and staff director were appointed by President Clinton, wanted computer printouts, internal documents, reports and other information. Both HUD and the commission retreated only under public pressure.



The Justice Department supported draconian restrictions on abortion protestors, including a prohibition on the display of any “images” that could be “observed” within abortion clinics. The Defense Department attempted to gag millitary chaplains, preventing them from discussing the Catholic Church’s Life Postcard Campaign regarding the president’s veto of legislation banning partial‐​birth abortion. More recently, the administration has threatened to prosecute any physician who provides a prescription for marijuana under state law.



Intimidation has been a persistent administration tactic elsewhere. In 1994, President Clinton expressed outrage that radio talk show host Rush Limbaugh could get on the air and “have three hours to say whatever he wants. And I won’t have an opportunity to respond.” White House Communications Director Mark Gearan called for radio talk shows to put on opposition–meaning administration guests. Senior adviser George Stephanopoulos suggested resurrecting the misnamed “Fairness Doctrine” to be enforced by the Federal Communications Commission, to regulate political broadcasts.



The Energy Department created a press rating system. Reporters and sources were judged based on their opinion of the department. Department press secretary Barbara Semedo explained that a low rating “meant we weren’t getting our message across, that we needed to work on this person a little.” Of course, getting the message meant spouting the department’s line.



Advertising, too, has been an administration target. The Food and Drug Administration even sought to prohibit the use of brand names on non‐​tobacco products (such as lighters and T‐​shirts) and the use of non‐​tobacco brand names on tobacco products. The administration supported labeling restrictions, deemed unconstitutional by the Supreme Court, on beer producers. The president backed FCC Chairman Reed Hundt’s campaign to bar the advertising of distilled spirits on television.



“The Clinton civil liberties record is breathtaking in both the breadth and the depth of its awfulness.”



The administration supported the Communications Decency Act, which would have attempted to ban the transmission of “indecent” materials over the Internet. Although well‐​intentioned, the law, voided by the Supreme Court, would have meant heavy‐​handed censorship of today’s least regulated communication medium.



Although President Clinton has spoken of reforming affirmative action, his administration promotes it with a mailed fist. Perhaps the ugliest episode was his Justice Department’s support (recently reversed) for the Piscataway, NJ., school district that fired a teacher because she was white. The Education Department responded to California’s passage of Proposition 209 by threatening to prosecute the university system for dismantling its racial spoils program.



Within the administration “diversity” has become a code word for preferential treatment. HUD requires that employees not only implement federal diversity policy, but demonstrate “interest” and “personal commitment” to diversity, be active in “minority, feminist or other cultural organizations ” and participate in “cultural diversity activities outside of HUD.” The Agriculture Department reassigned an employee for criticizing, on his own time, the department’s policy of offering spousal benefits to same‐​sex partners.



But the harshest examples of jackboot liberalism have come from the Justice Department and federal law enforcement agencies. The Branch Davidian and Randy Weaver cases continue to stand as examples of government run‐​amok, persecuting people who wanted little more than to be left alone. The administration’s response to the Oklahoma City bombing was to impose sweeping new powers, such as restricting the right of habeas corpus and expanding the use of wiretaps, for itself, even though the president was unable to point to a single example where civil liberties protections had hampered efforts to combat terrorism.



The administration, the most wiretap‐​friendly in U.S. history, has sought to eliminate Fourth Amendment protections against government searches. The president claims to possess “inherent authority to conduct warrantless searches for foreign intelligence purposes.” The administration requires public housing residents to sign away their constitutional rights. The Justice Department backed warrantless (indeed, suspicionless) drug tests for high school athletes. The administration has requested greater FBI authority to conduct “moving wiretaps” without a court order. President Clinton pushed the Communications Assistance Act, which requires telephone companies to retrofit their systems to ease police surveillance, supported restrictions on the sale of Internet encryption technology, and requested legislation forcing firms to give the government the “keys” to such technology.



No squishy, compassionate liberal he, the president has sought to thwart Arizona and California voters who approved measures to allow the desperately ill–victims of AIDS and cancer, in particular– from using marijuana to ease their nausea and pain. Mr. Clinton responded to criticism that sellers of crack were being punished far more severely than those who peddled cocaine by arguing that penalties against the latter–which already ensure that minor drug dealers spend more time in jail than do many armed robbers, rapists, and murderers–should be raised. (He recently suggested a mode move in the other direction, reducing the differential from a hundredfold to tenfold.)



The administration also throws people in jail for resisting federal designation of their (very dry) property as “wetlands,” and committing other environmental offenses. In 1994, the Justice Department relaxed its control of environmental prosecutions in order to allow individual U.S. Attorneys greater latitude in prosecuting business. Of course, the department still retains the right to proceed if a local U.S. attorney refuses to bring charges.



Any particular presidential decision can be defended on one ground or another, but as Wired magazine’s John Heilemann observes, the Clinton civil liberties record is “breathtaking in both the breadth and the depth of its awfulness.” And there’s more– proposed curfews for kids, support for random drug tests of welfare recipients and kids seeking drivers licenses, attacks on the requirement of a jury trial, ex post facto tax increases, attempts to gain court sanctions for uncompensated property takings, prosecution implicating the double‐​jeopardy clause, pretentious claims of federal criminal jurisdiction, and infringements of the Second Amendment right to possess a firearm. Mr. Lynch details these and more in his devastating study, “Dereliction of Duty: The Constitutional Record President Clinton.”



Administration spokesmen argue that the president is carefully balancing rights and liberties. It’s not balance President Clinton wants, however. It’s power. There was a time when Democrats were genuinely liberal. No longer. The American people are paying for Bill Clinton’s philosophy of jackboot liberalism with their freedom.
"
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




The role that anthropogenic “global warming” from the emissions of greenhouse gases from the combustion of fossil fuels plays is debatable—both in timing and magnitude. Almost certainly its influence is present and detectable in the U.S. annual average temperature record, but beyond that simple statement, not a whole lot more can be added with scientific certainty.



We now stand nearly a year later with more evidence of proof and point.   
  
  
Through November of this year, the U.S. average temperature is only 0.53°F above the 20th century mean temperature (the default baseline used by NCDC). Last year the annual temperature was 3.24°F above it.   






Figure 1. Average January‐​November temperature in the contiguous United States from 1895–2013 as compiled by the National Climatic Data Center (source: NCDC, Climate at a Glance).   
  
  
With the cold start to December across the country, the annual temperature for 2013 has an increasingly good shot at coming in below the 20th century average. For this to happen, the U.S. temperature for December would have to average about 27.6°F. For the first 12 days of the month, the average has been 28.4°F, and the forecast is for continued cold, so getting to the needed temperature is not out of the question.   
  
  
If 2013 does come in below the 20th century average, it would be the first year since 1996 to have done so, and would end a 16‐​year long run of above average annual temperature for the U.S. You can follow the chase here.   
  
  
But even if the rest of the month is not quite cold enough to push the entire year into negative territory, the 2013 annual temperate will still be markedly colder than last year’s record high, and _will be the largest year‐​over‐​year decrease in the annual temperature on record_ , underscoring the “outlier” nature of the 2012 temperatures.   
  
  
Will 2013 mark the end of the decade and a half period of abnormal warmth experience across the U.S. that was touched off by the 1998 El Niño event, and a return to conditions of the 1980s and early‐​to‐​mid 1990s? Or will 2013 turn out to just be a cold blip in the 21st century U.S. climate?   
  
  
In either case, 2013 shows that the natural variability of annual temperatures in the U.S. is high (as is decadal and multi‐​decadal variability, see Figure 1)—an important caveat to keep in mind when you face the inundation of every‐​weather‐​event‐​is‐​caused‐​by‐​human‐​global‐​warming hysteria.   
  
  
Stay tuned!   
  
  
_The Center for the Study of Science would like to thankRyan Maue of WeatherBELL Analytics for his summary of December temperatures and the expected temperatures for the rest of the year._
"
"

Art courtesy Dave Stephens
Foreword by Anthony Watts: This article, written by the two Jeffs (Jeff C and Jeff Id) is one of the more technically complex essays ever presented on WUWT. It has been several days in the making. One of the goals I have with WUWT is to make sometimes difficult to understand science understandable to a wider audience. In this case the statistical analysis is rather difficult for the layman to comprehend, but I asked for (and got) an essay that was explained in terms I think many can grasp and understand. That being said, it is a long article, and you may have to read it more than once to fully grasp what has been presented here. Steve McIntyre of Climate Audit laid much of the ground work for this essay, and from his work as well as this essay, it is becoming clearer that Steig et al (see “Warming of the Antarctic ice-sheet surface since the 1957 International Geophysical Year”, Nature, Jan 22, 2009) isn’t holding up well to rigorous tests as demonstrated by McIntyre as well as in the essay below. Unfortunately, Steig’s office has so far deferred (several requests) to provide the complete data sets needed to replicate and test his paper, and has left on a trip to Antarctica and the remaining data is not “expected” to be available until his return.
To help layman readers understand the terminology used, here is a mini-glossary in advance:
RegEM – Regularized Expectation Maximization
PCA – Principal Components  Analysis
PC – Principal Components
AWS – Automatic Weather  Stations
One of the more difficult concepts is RegEM, an algorithm developed by Tapio Schneider in 2001.   It’s a form of expectation maximization (EM) which is a  common and well understood method for infilling missing data. As we’ve previously noted on WUWT, many of the weather stations used in the Steig et al study had issues with being buried by snow, causing significant data gaps in the Antarctic record and in some burial cases stations have been accidentally lost or confused with others at different lat/lons. Then of course there is the problem of coming up with trends for the entire Antarctic continent when most of the weather station data is from the periphery and the penisula, with very little data from the interior.
Expectation Maximization is a method which uses  a normal distribution to compute the best probability of fit to a missing piece  of data.  Regularization is required when so much data is missing that the EM  method won’t solve.  That makes it a statistically dangerous  technique to use and as Kevin Trenberth, climate analysis chief at the National Center for Atmospheric Research, said in an e-mail: “It is hard to make data where none exist.” (Source: MSNBC article) It is also valuable to note that one of the co-authors of Steig et al, Dr. Michael Mann, dabbles quite a bit in RegEm in this preparatory paper to Mann et al 2008 “Return of the Hockey Stick”.
For those that prefer to print and read, I’ve made a PDF file of this article available here.
Introduction
This article is an attempt to describe some of the early results from the Antarctic reconstruction recently published on the cover of Nature which demonstrated a warming trend in the Antarctic since 1956.   Actual surface temperatures in the Antarctic are hard to come by with only about 30 stations prior to 1980 recorded through tedious and difficult efforts by scientists in the region.  In the 80’s more stations were added including some automatic weather stations (AWS) which sit in remote areas and report the temperature information automatically.  Unfortunately due to the harsh conditions in the region many of these stations have gaps in their records or very short reporting times (a few years in some cases).  Very few stations are located in the interior of the Antarctic, leaving the trend for the central portion of the continent relatively unknown.  The location of the stations is shown on the map below.

In addition to the stations there are satellite data from an infrared surface temperature measurement which records the temperature of the actual emission from the surface of the ice/ground in the Antarctic.  This is different from the microwave absorption measurements as made from UAH/RSS data which measure temperatures in a thickness of the atmosphere.  This dataset didn’t start until 1982.
Steig 09 is an attempt to reconstruct the continent-wide temperatures using a combination of measurements from the surface stations shown above and the post-1982 satellite data.  The complex math behind the paper is an attempt to ‘paste’ the 30ish pre-1982 real surface station measurements onto 5509 individual gridcells from the satellite data.  An engineer or vision system designer could use several straightforward methods which would insure reasonable distribution of the trends across the grid based on a huge variety of area weighting algorithms, the accuracy of any of the methods would depend on the amount of data available.  These well understood methods were ignored in Steig09 in favor of RegEM.
The use of Principal Component Analysis in the reconstruction
Steig 09 presents the satellite reconstructions as the trend and also provides an AWS reconstruction as verification of the satellite data rather than a separate stand alone result presumably due to the sparseness of the actual data.  An algorithm called RegEM was used for infilling the missing data. Missing data includes pre 1982 for satellites and all years for the very sparse AWS data.  While Dr. Steig has provided the reconstructions to the public, he has declined to provide any of the satellite, station or AWS temperature measurements used as inputs to the RegEM algorithm.  Since the station and AWS measurements were available through other sources, this paper focuses on the AWS reconstruction.
Without getting into the detail of PCA analysis, the algorithm uses covariance to assign weighting of a pattern in the data and does not have any input whatsoever for actual station location.  In other words, the algorithm has no knowledge of the distance between stations and must infill missing data based solely on the correlation with other data sets.  This means there is a possibility that with improper or incomplete checks, a trend from the peninsula on the west coast could be applied all the way to the east.  The only control is the correlation of one temperature measurement to another.
If you were an engineer concerned with the quality of your result, you would recognize the possibility of accidental mismatch and do a reasonable amount of checking to insure that the stations were properly assigned after infilling.  Steig et. al. described no attempts to check this basic potential problem with RegEM analysis.  This paper will describe a simple method we used to determine that the AWS reconstruction is rife with spurious (i.e. appear real but really aren’t) correlations attributed to the methods used by Dr. Steig.  These spurious correlations can take a localized climactic pattern and “smear” it over a large region that lacks adequate data of its own.
Now is where it becomes a little tricky.  RegEM uses a reduced information dataset to infill the missing values.  The dataset is reduced by Principal Component Analysis (PCA) replacing each trend with a similar looking one which is used for covariance analysis.  Think of it like a data compression algorithm for a picture which uses less computer memory than the actual but results in a fuzzier image for higher compression levels.

While the second image is still visible, the actual data used to represent the image is reduced considerably.  This will work fine for pictures with reasonable compression, but the data from some pixels has blended into others.  Steig 09 uses 3 trends to represent all of the data in the Antarctic.  In it’s full complexity using 3 PC’s is analogous to representing not just a picture but actually a movie of the Antarctic with three color ‘trends’ where the color of each pixel changes according to different weights of the same red, green and blue color trends (PC’s).  With enough PC’s the movie could be replicated perfectly with no loss.  Here’s an important quote from the paper.
“We therefore used the RegEM algorithm with a cut-off parameter K=3. A disadvantage of excluding higher-order terms (k>3) is that this fails to fully capture the variance in the Antarctic Peninsula region.  We accept this tradeoff because the Peninsula is already the best-observed region of the Antarctic.”

Above: a graph from Steve McIntyre of ClimateAudit where he demonstrates how “K=3 was in fact a fortuitous choice, as this proved to yield the maximum AWS trend, something that will, I’m sure, astonish most CA readers.”
K=3 means only 3 trends were used, the ‘lack of captured variance’ is an acknowledgement and acceptance of the fuzziness of the image.  It’s easy to imagine that it would be difficult to represent a complex movie image of Antarctic with any sharpness from 1957 to 2006 temperature with the same 3 color trends reweighted for every pixel.  In the satellite version of the Antarctic movie the three trends look like this.

Note that the sudden step in the 3rd trend would cause a jump in the ‘temperature’ of the entire movie.  This represents the temperature change between the pre 1982 recreated data and the after 1982 real data in the satellite reconstruction.  This is a strong yet overlooked hint that something may not be right with the result.
In the case of the AWS reconstruction we have only 63 AWS stations to make the movie screen, by which the trends of 42 surface station points are used to infill the remaining data.  If the data from one surface station is copied to the wrong AWS stations the average will overweight and underweight some trends. So the question becomes, is the compression level too high?
The problems that arise when using too few principal components
Fortunately, we’re here to help in this matter.  Steve McIntyre again provided the answer with a simple plot of the actual surface station data correlation with distance.  This correlation plot compares the similarities ‘correlation’ of each temperature station with all of the 41 other manual surface stations against the distance between them.  A correlation of 1 means the data from one station is exactly equal to the other.  Because A -> B correlation isn’t a perfect match for B->A there are 42*42 separate points in the graph.  This first scatter plot is from measured temperature data prior to any infilling of missing measurements.  Station to station distance is shown on the X axis.  The correlation coefficient is shown on the Y axis.

Since this plot above represents the only real data we have existing back to 1957, it demonstrates the expected ‘natural’ spatial relationship from any properly controlled RegEM analysis.  The correlation drops with distance which we would expect because temps from stations thousands of miles away should be less related than those next to each other.  (Note that there are a few stations that show a positive correlation beyond 6000 km.  These are entirely from non-continental northern islands inexplicably used by Steig in the reconstruction.  No continental stations exhibit positive correlations at these distances.)  If RegEM works, the reconstructed RegEM imputed (infilled) data correlation vs. distance should have a very similar pattern to the real data.  Here’s a graph of the AWS reconstruction with infilled temperature values.
Compare this plot with the previous plot from actual measured temperatures.  Now contrast that with the AWS plot above.  The infilled AWS reconstruction has no clearly evident pattern of decay over distance.  In fact, many of the stations show a correlation of close to 1 for stations at 3000 km distant!  The measured station data is our best indicator of true Antarctic trends and it shows no sign that these long distance correlations occur.  Of course, common sense should also make one suspicious of these long distance correlations as they would be comparable to data that indicated Los Angeles and Chicago had closely correlated climate.
It was earlier mentioned that the use of 3 PCs was analogous to the loss of detail that occurs in data compressions.   Since the AWS input data is available, it is possible to regenerate the AWS reconstruction using a higher number of PCs.  It stood to reason that spurious correlations could be reduced by retaining the spatial detail lost in the 3 PC reconstruction.  Using RegEM, we generated a new AWS reconstruction using the same input data but with 7 PCs.  The distance correlations are shown in the plot below.

Note the dramatic improvement over that shown in the previous plot.  The correlation decay with distance so clearly seen in the measured station temperature data has returned.  While the cone of the RegEM data is slightly wider than the ‘real’ surface station data, the counterintuitive long distance correlations seen in the Steig reconstruction have completely disappeared.  It seems clear that limiting the reconstruction to 3 PCs resulted in numerous spurious correlations when infilling missing station data.


Using only 3 principal components distorts temperature trends
If Antarctica had uniform temperature trends across the continent, the spurious correlations might not have a large impact in the overall reconstruction.  Individual sites may have some errors, but the overall trend would be reasonably close.  However, Antarctica is anything but uniform.  The spurious correlations can allow unique climactic trends from a localized region to be spread over a larger area, particularly if an area lacks detailed climate records of its own.  It is our conclusion is that is exactly what is happening with the Steig AWS reconstruction.
Consider the case of the Antarctic Peninsula:

The      peninsula is geographically isolated from the rest of the continent
The      peninsula is less than 5% of the total continental land mass
The      peninsula is known to be warming at a rate much higher than anywhere else      in Antarctica
The      peninsula is bordered by a vast area known as West Antarctica that has      extremely limited temperature records of its own
15 of      the 42 temperature surface stations (35%) used in the reconstruction are      located on the peninsula

If the Steig AWS reconstruction was properly correlating the peninsula stations temperature measurements to the AWS sites, you would expect to see the highest rates of warming at the peninsula extremes.  This is the pattern seen in the measured station data.  The plot below shows the temperature trends for the reconstructed AWS sites for the period of 1980 to 2006.  This time frame has been selected as this is the period when AWS data exists.  Prior to 1980, 100% of AWS reconstructed data is artificial (i.e. infilled by RegEM).

Note how warming extends beyond the peninsula extremes down toward West Antarctica and the South Pole.  Also note the relatively moderate cooling in the vicinity of the Ross Ice Shelf (bottom of the plot).  The warming once thought to be limited to the peninsula appears to have spread.  This “smearing” of the peninsula warming has also moderated the cooling of the Ross Ice Shelf AWS measurements.  These are both artifacts of limiting the reconstruction to 3 PCs.
Now compare the above plot to the new AWS reconstruction using 7 PCs.

The difference is striking.  The peninsula has become warmer and warming is largely limited to its confines.  West Antarctica and the Ross Ice Shelf area have become noticeably cooler.  This agrees with the commonly-held belief prior to Steig’s paper that the peninsula is warming, the rest of Antarctica is not.
Temperature trends using more traditional methods
In providing a continental trend for Antarctica warming, Steig used a simple average of the 63 AWS reconstructed time series.  As can be seen in the plots above, the AWS stations are heavily weighted toward the peninsula and the Ross Ice Shelf area.  Steig’s simple average is shown below.  The linear trend for 1957 through 2006 is +0.14 deg C/decade.  It is worth noting that if the time frame is limited to 1980 to 2006 (the period of actual AWS measurements), the trend changes to cooling, -0.06 deg C/decade.

We used a gridding methodology to weight the AWS reconstructions in proportion to the area they represent.  Using the Steig’s method, 3 stations on the peninsula over 5% of the continent’s area would have the same weighting as three interior stations spread over 30% of the continent area.  The gridding method we used is comparable to that utilized in other temperature constructions such as James Hansen’s GISStemp.  The gridcell map used for the weighted 7 PC reconstruction is shown here.

 
Cells with a single letter contain one or more AWS temperature stations.  If more than one AWS falls within a gridcell, the results were averaged and assigned to that cell.  Cells with multiple letters had no AWS within them, but had three or more contiguous cells containing AWS stations.  Imputed temperature time series were assigned to these cells based on the average of the neighboring cells.  Temperature trends were calculated both with and without the imputed cells.  The reconstruction trend using 7 PCs and a weighted station average follow.

The trend has decreased to 0.08 deg C/decade.  Although it is not readily apparent in this plot, from 1980 to 2006 the temperature profile has a pronounced negative trend.
Temporal smearing problems caused by too few PCs?
The temperature trends using the various reconstruction methods are shown in the table below.  We have broken the trends down into three time periods; 1957 to 2006, 1957 to 1979, and 1980 to 2006.  The time frames are not arbitrarily chosen, but mark an important distinction in the AWS reconstructions.  There is no AWS data prior to 1980.  In the 1957 to 1980 time frame, every single temperature point is a product of the RegEM algorithm.   In the 1980 to 2006 time frame, AWS data exists (albeit quite spotty at times) and RegEM leaves the existing data intact while infilling the missing data.
We highlight this distinction as limiting the reconstruction to 3 PCs has an additional pernicious effect beyond spatial smearing of the peninsula warming.   In the table below, note the balance between the trends of the 1957 to 1979 era vs. that of the 1980 to 2006 era. In Steig’s 3 PC reconstruction, moderate warming that happened prior to 1980 is more balanced with slight cooling that happened post 1980.  In the new 7 PC reconstruction, the early era had dramatic warming, the later era had strong cooling.  It is believed that the 7 PC reconstruction more accurately reflects the true trends for the reasons stated earlier in this paper.  However, the mechanism for this temporal smearing of trends is not fully understood and is under investigation.  It does appear to be clear that limiting the selection to three principal components causes warming that is largely constrained to a pre-1980 time frame to appear more continuous and evenly distributed over the entire temperature record.



Reconstruction

1957 to 2006 trend


1957 to 1979 trend (pre-AWS)


1980 to 2006 trend (AWS era)



Steig 3 PC 

+0.14 deg C./decade


+0.17 deg C./decade


-0.06 deg C./decade



New 7 PC 

+0.11 deg C./decade


+0.25 deg C./decade


-0.20 deg C./decade



New 7 PC weighted

+0.09 deg C./decade


+0.22 deg C./decade


-0.20 deg C./decade



New 7 PC wgtd   imputed cells

+0.08 deg C./decade


+0.22 deg C./decade


-0.21 deg C./decade




Conclusion
The AWS trends which this incredibly long post was created from were used only as verification of the satellite data.  The statistics used for verification are another subject entirely.  Where Steig09 falls short in the verification is that RegEM was inappropriately applying area weighting to individual temperature stations.  The trends from the AWS reconstruction clearly have blended into distant stations creating an artificially high warming result.  The RegEM methodology also appears to have blended warming that occurred decades ago into more recent years to present a misleading picture of continuous warming.  It should also be noted that every attempt made to restore detail to the reconstruction or weight station data resulted in reduced warming and increased cooling in recent years.  None of these methods resulted in more warming than that shown by Steig.
We don’t yet have the satellite data (Steig has not provided it) so the argument will be:
“Silly Jeff’s you haven’t shown anything, the AWS wasn’t the conclusion it was the confirmation.”
To that we reply with an interesting distance correlation graph of the satellite reconstruction (also from only 3 PCs).  The conclusion has the exact same problem as the confirmation.  Stay tuned.

(Graph originally calculated by Steve McIntyre)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97a0fe00',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Human life expectancy is rocketing upwards, crops are growing ever bigger, and nutrition is improving dramatically. As we stand witness to the greatest democratization of wealth in human history, how many times do we have to read that environmental disaster is upon us? This time the story is that the North Polar summer icecap is melting, and that computer models prove it is a result of all that pernicious industrial prosperity.



That’s what the casual reader must take from the December 3 issue of Science, and the same day’sWashington Post, citing the work of University of Maryland’s Konstantin Ya Vinnikov. Who claims that the well‐​known (but slight) recession of Arctic sea ice that has occurred in the last five decades is caused by human‐​induced climate change. The reason, Vinnikov says, is that computer models using greenhouse gas warming and sulfate aerosol cooling melt only about as much summer arctic sea ice as has already been lost. When run without human influence, the computer models melt this much ice only 2 percent of the time. We don’t worry about winter ice because everything is frozen up there in that season.



To be fair to the Post, science writer Curt Suplee went out of his way to point out the highly controversial nature of the finding, and that there’s a reasonable argument that Vinnikov’s finding isn’t as solid as it is cracked up to be. But Suplee didn’t have the space to go into the gory details. The problem lies in the “logic” of the study, in which a computer model is used to “prove” something. That requires that the model be correct. But every global warming model has gotten the behavior of 80 percent of the lower atmosphere wrong over the last quarter century, predicting warming where there was none. Because the atmosphere is a stirred fluid, correctly predicting the warming that occurred in the remaining 20 percent can only mean that the right answer was arrived at for the wrong reason. 



Welcome to 21st century science! Perhaps, instead of comparing two computer models, it might be better to see if warming is human induced by looking at the actual temperature history.



Let’s stipulate that the decline in Arctic sea‐​ice is real. But is it a consequence of the largely unknown behavior of the world’s largest natural highball glass? The Arctic Ocean contains the largest mass of floating ice on earth. Lest folks worry that melting this ice will inundate Miami, they might pour a scotch and soda and watch the ice melt. The “sea level” remains the same, even as the drink goes stale. I’ll drink to the notion that a lot of the observed melting is simply a continuation of a long‐​term process that initially had nothing to do with people.



We have a pretty decent history of Arctic temperatures, thanks to the Cold War. That history begins in 1958–about the time we realized we had to know a lot about the Arctic atmosphere, owing to a long‐​standing dispute with our nuclear‐​armed adversaries across the pole. The temperature history has been summarized and is continually updated by Department of Commerce scientist James Angell. What it shows certainly complicates Vinnikov’s computer‐​based analysis.



As is plain and clear, there is no warming at all in the first three decades of these measurements. Then a trend sets in, beginning in 1988, a mere decade ago. How can one melt the ice from 1950 through 1988 when there is no regional warming? In fact, it looks like the ice was in the process of melting long before the initiation of putative human greenhouse warming. Even worse, the period from 1950 to 1975 was one of cooling in the Northern Hemisphere, and still the ice melted.



A more logical explanation than what appears in Science is that the ice has been responding to a rapid and dramatic 2 degree centigrade warming of the Arctic that took place from 1920 to 1940. In other words, it takes many decades for the sea and the ice to adjust to past thermal imbalance. Is this Arctic warming really such a terrible thing? Scientists are pretty sure that the earth was about 1.5ºC warmer 4000 to 7000 years ago than it is now (although there is uncertainty as to why). That means that the summer Arctic ice had to have receded considerably beyond its current position. That era, known as the “hypsithermal period,” was also called the “climatic optimum” in previous generations of textbooks (written before the current “hysterical period”), because it accompanied the rise of agriculture and civilization. 



Maybe it’s not an accident after all that crops are growing better than ever and people are getting rich.
"
"

‘Tis the season for Internet shopping! Well, sort of. It’s actually the season for all kinds of shopping, with record‐​breaking consumer spending this year. Online holiday sales are expected to top $2.3 billion, but that’s a mere snowflake in the overall blizzard of retail activity. 



Electronic commerce is catching on, however, and with more than 140 million people online, it’s no longer the exclusive domain of computer nerds. Thus, _Newsweek_ recently declared that “it’s beginning to look a lot like an e‐​Christmas.” On the same day, _U.S. News & World Report_ observed that “shoppers from east to west seem determined to avoid traffic jams at the mall, long lines at the post office, and last‐​minute dashes to the supermarket.” 



But Internet shoppers are also avoiding something else: sales taxes. And thanks to the newly enacted Internet Tax Freedom Act, “tax‐​free” will be the rule for at least three years. 



That leaves some observers and state officials decidedly short on holiday cheer. For example, a recent article by technology commentator James Ledbetter denounced restrictions on Internet taxation as “unfair” to those who shop in stores. Similarly, a wire service story accused Internet vendors of enjoying a “free ride” and warned that local retailing could cease to exist. Not to be outdone, Congress appointed a commission earlier this month to study the issue.  




Without doubt, limiting states’ taxing authority leads to unequal taxation. Nevertheless, such limitations are a crucial component of American federalism. 



Of course, not all Internet purchases are tax‐​free, just those made across state lines. Out‐​of‐​state Web vendors — like their catalog cousins — aren’t required to collect sales taxes except in states where they maintain a physical presence. In other words, electronic commerce is treated exactly the same as mail‐​order business. And the Supreme Court has held that states have no authority to tax mail‐​order sales outside their borders absent explicit congressional authorization. Instead, most states ask consumers to pay a “use tax” in lieu of a sales tax on all purchases. Few consumers volunteer. 



At first glance, the pro‐​tax case sounds reasonable: why should identical items be taxed differently depending on how they’re purchased? Theoretically, they shouldn’t be. But in the real world, there are several reasons why allowing states to tax out‐​of‐​state electronic commerce is bad policy. 



First, there is no immediate danger of large revenue losses for traditional retailers or, by extension, for state tax authorities. Local stores cater to a customer’s desire for a hands‐​on experience, offer immediate gratification, don’t charge for shipping, and so will probably always dominate retailing. What’s more, shopping is for many people a pleasurable social experience that cannot be duplicated online. Thus, Internet sales won’t destroy “real” retailers, just as catalog sales haven’t. 



Empirical evidence supports that conclusion. In an era of almost no inflation, state budgets grew by 5 percent in FY97 and by more than 6 percent in FY98. Over the past four years, state tax collections have exceeded expectations by about $25 billion. It appears that there will be a sizable revenue windfall this year as well. With revenues pouring in so rapidly, it’s just not credible that electronic commerce is undermining state tax collections. 



Second, differentiated tax rates create healthy competition that helps keep local rates under control. For example, some residents of Manhattan drive to Delaware to avoid sales taxes — an option that has undoubtedly curbed the profligate fiscal habits of New York politicians. Electronic commerce serves as a similar safety valve that guards against excessive taxation. 



If states and localities feel compelled to tax their citizens more heavily, they still have the tools to do it. There will always be income taxes, property taxes, gas taxes, hotel taxes and the like. Perhaps what states really seek isn’t equity or revenue security but rather a new source of funds that doesn’t require voter approval. 



Finally, there is something inherently unsettling about states’ exercising legal authority outside their jurisdictions. By what right can New York force a firm in Florida to act as its tax collection agent? Even if it were constitutionally permissible, it would set a dangerous precedent with enormous potential for conflict. 



Without doubt, limiting states’ taxing authority leads to unequal taxation. Nevertheless, such limitations are a crucial component of American federalism. Absent those restraints, confiscatory state tax rates — which are the true injustice — would worsen. To improve their business climate, states should cut taxes, not scheme to collect more. 



For its part, Congress should stubbornly refuse to bow to state demands for new taxing authority. The Internet Tax Freedom Act was a good start in ensuring that traditional principles of remote commerce apply to the online world; Congress should consider making it permanent. 
"
"
Share this...FacebookTwitterBack in January I wrote about how the Arctic had gained 2000 cubic kilometres in ice volume. This was calculated by comparing the sea ice thickness of January 2008 to that of January 2011, see the following chart.
Mid-January Arctic sea ice thickness comparison of 2008 to 2011.
Blue color shows thin ice, while green shows thicker ice. Clearly the Arctic’s ice was much thicker in Jan-2011 than it was three years ago in Jan-2008. There is about twice as much solid green in 2011 than in 2008, thus yielding the net gain of about 2000 cubic kilometers.
5000 cubic km more sea ice?
Now that we’ve just past the March-peak in Arctic sea ice area, I thought it’s a good time to take another look at the Arctic sea ice thickness again. What follows is a comparison of March 23, 2008 (the old death spiral days) and March 25, 2011.
Source of charts: http://www7320.nrlssc.navy.mil/pips2/ithi.html
Well lo and behold, we see yet another huge increase. That ice thickness increase has grown even more.
Note the huge difference in just three years. Today practically the entire Arctic cap is green…meaning an average thickness of over 3 meters. In 2008 the average thickness down to about 2m. This is all further confirmed by the current Catlin Arctic Survey (h/t: tomnelson.blogspot).
Some of the ice we crossed was really thick multi-year ice that was just too thick to drill through. Our drill goes to four and a half metres and it wasn’t breaking through. In other areas we were able to drop our measuring devices down to a depth of 200 metres below the floating ice.”
I wonder if Mark Serreze is taking note. Hopefully he’ll behave as the scientist he claims to be and make the observation.
I haven’t systematically calculated the March differences above, but you can assume that the core cap itself has an area of 5 million square km. Now multiply that times one meter of added thickness and you get a volume growth of 5000 cubic km!
Okay, that’s just rough guesstimating (perhaps) on the high side, but the ice gain is whopping no matter what. Dr. “Freeze” Serreze can make the calculation himself. Predictions of higher sea ice extents come September are well-founded. The recovery appears to be well on the way. Arctic warming has disappeared.
Share this...FacebookTwitter "
"

Ten years ago the Alps endured a virtually snowless winter. Environmentalists blamed global warming. A Swiss lobbying group, Alp Action, wrote in 1991 that global warming would put an end to winter sports in the Alps by 2025. 



This year the Alps have had their greatest snowfall in 40 years, according to very preliminary data. Greenpeace has blamed global warming. 



How in the world can that be? Is it possible to blame global warming for every weather anomaly, even if two consecutive events are of opposite sign? Can such a claim have “scientific” justification? 



If one regards the United Nations as an authority on such things, the answer, unfortunately, is yes. Global warmers, thanks to the good offices of the U.N. Intergovernmental Panel on Climate Change, can blame any weather event on pernicious economic prosperity and resultant greenhouse gas emissions. 



The most recent IPCC summary on climate change was published three years ago. IPCC purports to be the “consensus of scientists” but in fact is a group of individuals hand‐​picked by their respective governments. Does anyone really expect Al Gore to send me to represent the United States at one of those meetings? (Thank you, no, I have been to one and that was enough.) 



Absent my sage advice, here’s what the United Nations wrote in 1995: “Warmer temperatures will lead to … prospects for more severe droughts and/​or floods in some places and less severe droughts and/​or floods in others.” 



As a punishment for not cleaning out the cat box, you might ask your kid to diagram this sentence. Rather than strain the graphics of this word processor, we’ll simply parse it. What the IPCC is saying is that global warming will cause in “some places” and/​or “others”: 



So, according to the “consensus of scientists,” it’s OK to blame a flood, or, if you’re in the mountains, a flood of snow, on global warming. It’s also OK to blame a drought or a snowless Alp on global warming. 



It’s even OK to blame weather that is more normal than normal (“less intense wet and dry periods”) on global warming. 



The IPCC statement, which cannot be proved wrong, is a cynical attempt to allow anyone to blame anything on global warming. As Julius Wroblewski of Vancouver, Canada, wrote to me, this logic “represents a descent into the swamp of the non‐​falsifiable hypothesis. This is not a term of praise. Falsifiability is the internal logic in a theory that allows a logical test to see if it is right or wrong.” 



A non‐​falsifiable theory is one for which no test can be devised, and the U.N. statement fits the bill perfectly. There is simply no observable weather or climate that does not meet its criteria, except one: absolutely no change in the climate, meaning no change in the average weather or the variability around that average. 



Every climatologist on the planet knows that is impossible. Climate has to change because the sun is an inconstant star and the Earth is a nonuniform medium whose primary surface constituent, water, is very near its freezing point. Freezing (or unfreezing) water makes the planet whiter (or darker), which affects the degree to which it reflects the sun’s warming rays. A flicker of the sun, therefore, ensures climate change. 



A hot young climatologist named Robert Mann, writing in Geophysical Research Letters, recently provided a powerful demonstration of this phenomenon. Using long‐​term records from tree rings and ice cores, he concluded that the planet was on a 900‐​year cooling streak between 1000 and 1900. Then we warmed up almost twice as much as we had cooled, but at least half of that warming was caused by our inconsistent sun. Two NASA scientists recently demonstrated that the sun has been warming throughout the last 400 years. As a result, if the last decade weren’t among the warmest in the last millennium, something would have been wrong with the basic theory of climate: The sun warms the Earth. 



That doesn’t mean we haven’t supplied a bit of greenhouse warming, too. But greenhouse warming behaves differently than pure solar warming: It occurs largely in the coldest air masses of winter. 



That’s a far cry from the United Nation’s nonsense about “some places” and “others” experiencing more unusual, less unusual or unusually usual weather. And it has nothing to do with avalanches or snowless winters, either.
"
"
Share this...FacebookTwitterWhen you have an institution that can longer function because of incompetence, corruption, or whatever human element, it is impossible to repair it without first replacing the bad personnel behind the problem. Anything else is like trying to cure someone with a bad liver by treating everything else except the liver itself.That’s the case at the Met Office. If you truly want to reform it, then it has to be purged of its rotten apples. Piers Corbyn reports on the UK parliament transport select committee, click here, into the December cold & snow, and concludes:
THE MET OFFICE’s submission is, I would say: a Mubarak-style, bunkerish, self-serving, denial of reality.”
He then lists the points why, among them is a comment on the usefulness of seasonal forecasts, which, as we know, their own have been completely wrong. Piers writes (emphasis added):
They say ‘accurate regional forecasts on a monthly scale have proved to be useful. Perhaps they are talking about someone else’s forecasts (eg WeatherAction’s). The observed FACT is Met Office seasonal forecasts have demonstrably negative skill. They have consistently – with zero success in all the last 6 unusual (extreme) seasons – misled the public, emergency services and Councils and led to deaths on unsalted roads consequent on ill-advised Council’s believing MetOffice warmist winter forecasts.”
Piers then writes on the Met Office’s commitment in developing forecasting science, calling it “as delusional as Colonel Gaddafi”. To top it all off, the Met Office then has the temerity to expect the British taxpayers to cough up many more millions for super-computers to “improve their forecasts”.
Everybody knows that no matter how good your computers are, if you feed them with garbage, then you get garbage out. I’d rely on Piers and his laptop long before I’d call the clowns at the Met Office.
In its submission, the Met Office does promise to take the steps needed for generating accurate monthly forecasts for the public, adding:
The extent and speed of this development is, of course, dependent on the availability of resources – particularly in supercomputing power to enable modelling to incorporate new science and understanding…..”.
Never let a crisis go to waste.
Indeed the Met Office needs a good house-cleaning. They are attempting to screw the public yet again. And unless the house does indeed get cleaned, more Bitish lives will be put at risk.
Share this...FacebookTwitter "
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
As economic heavyweights assembled for their annual summit held by the World Economic Forum (WEF) in Davos, Switzerland, they were greeted by a call for $700 billion/yr of increased spending out to the year 2030 to “to close the green investment gap worldwide, leading to sustainable economic growth that attains global climate change goals.” They were told that this goal can be reached through an additional $36 billion/yr investment from the world’s governments (on top of the $96 billion/yr currently spent) that will “spur up to US$ 570 billion in private capital needed to avoid devastating climate impacts on economy.”   
  
This call was made by the WEF’s own Green Growth Action Alliance as it released its first Green Investment Report at the outset of the Davos conference.   
  
The Green Growth Action Alliance justified the call for the extra spending this way:   




Such investments are urgently needed to avoid the potentially devastating impacts of climate change and extreme weather events as witnessed in many parts of the world in 2012. Scientists agree that extreme weather has become the “new norm” and comes at a huge, and rising, cost to the global economic system. Without further action, the world could see a rise in average global temperatures by 4ºC by the end of the century. According to scientists, this could lead to further devastating impacts, including extreme heat waves, more intense tropical storms, declining global food stocks and a sea-level rise affecting hundreds of millions of people.



Using a poor excuse to call for a bad idea doesn’t seem much like progress.   
  
The science of global warming re extreme events is hardly compelling. The data noise, generated from both natural processes and from other human influences, largely overwhelms any anthropogenic greenhouse effect signal in most cases.   
  
However, compelling evidence is emerging that the magnitude of the climate sensitivity—that is, how much warming we should expect from a doubling of atmospheric carbon dioxide concentration—has been overestimated. Even if there was good scientific evidence that higher temperatures lead to a more “extreme” climate (there’s just about as much evidence for the opposite), an overestimate of the sensitivity would lead to an overestimate of extremes.   
  
And these overestimates are being used by the Green Growth Action Alliance to oversell the need to do something about climate change.   
  
In fact, there are much more pressing needs.   




For example, according to the International Energy Agency (IEA), there are currently 1.3 billion people globally without access to electricity. The IEA recognizes that getting these folks hooked up is imperative for economic growth:   




Energy alone is not sufficient for creating the conditions for economic growth, but it is certainly necessary. It is impossible to operate a factory, run a shop, grow crops or deliver goods to consumers without using some form of energy. Access to electricity is particularly crucial to human development as electricity is, in practice, indispensable for certain basic activities, such as lighting, refrigeration and the running of household appliances, and cannot easily be replaced by other forms of energy. Individuals’ access to electricity is one of the most clear and un-distorted indication of a country’s energy poverty status.



It would seem to us, that getting electricity to those without is a better way to achieve the Green Growth Action Alliance’s goal of “driving development and well-being” than is “reducing greenhouse gas emissions.”   
  
And the best way to get (cheap, reliable) electricity to large numbers of people is through electricity systems that are powered by greenhouse gas-emitting fossil fuels. This is not to say that there are not instances where boutique energy sources such as solar may provide a better solution, but just that those instances are minor compared to the magnitude of the task—which makes doubly bad Green Growth Action Alliance advice to shift substantial capital from fossil fuel projects to help fund its green solutions.   
  
The bottom line is this: Fossil-fuel energy leads to more people with electricity which leads to more economic growth which leads to richer, more stable, more resilient and more environmentally-friendly societies with greater wellbeing.   
  
Don’t get us wrong, we are all for market-driven innovation, but in Davos, the urgency for such innovation is being overhyped, and the situation is made worse by the urging for public sector spending in order to fuel it.


"
"
The hits just keep on coming. 1,672,437 page views this month, up from 1,478,801 page views in March.

After posting last months stats, there was some discontent by some angry and somewhat incredulous bloggers that it might be an April Fools joke of some sorts. Sorry, no such luck.
But what was humorous, was one particular blogger (Joe Romm) who said:
It is absurd to publish one’s page views to 7 significant digits without caveats — even 2 is stretching it. 
I got a huge chuckle out of that. So just to show that the stats are indeed real, and accurate down to that 7th digit (since they come from the WordPress internal traffic counter), I’m expanding this month’s report.
Joe, this report’s for you. Here is a screencap showing my WordPress internal report page, with all 7 digits, sans caveats. No caveats are needed since the WordPress numbers are actual, not estimated, and not drawn from router statistics.
click for a larger image
Maybe Joe will reciprocate and post a screencap his own internal stats page.
The graph on this page above differs a bit from the headline graph (which is graphically edited to move the title inwards and downwards to fit nicely on the page) because it was snapped after 00GMT (5PM PST) and it shows the new month of May stats also, which are of course quite low by comparison.
So the numbers are real, from WordPress.com (where my blog is hosted) and exact to the 7th digit, despite angry accusations otherwise.
In that blog post, Joe Romm also showed an Alexa graph to prove his point, saying:
Interestingly, there is one independent source that suggests Watts’ page views and mine are in fact the same (and hence possibly around 1.4 million).  If you go to the Web traffic ranking and comparison site Alexa, go to page views, and type in wattsupwiththat.com, you’ll get this graph:

So at best I am just negating the disinformation Watts is spreading.  Sigh.  And lest there be any doubt, WattsUpWithThat is in fact an extremist anti-scientific denialist website, as his recent posts make clear.
Gosh, all that angry prose from a traffic report? I don’t much care whether WUWT beats Climate Progress in traffic or vice versa, but I do care when the proprietor suggests that by printing my own internal stats page I’m spreading disinformation. Of course, I don’t expect an apology, but I wanted WUWT readers to know.
OK, following Joe’s lead, lets look at Alexa this month.
Pageviews: we are still about the same in April according to Alexa.

Though over the longer 6 month term, WUWT is slow and steady while Climate Progress runs hot and cold.

Joe didn’t show the other interesting comparative statistics from Alexa though, so I thought WUWT readers might enjoy seeing them.
Here is Traffic Rank over 6 months:

And here is Reach:

But having been labeled a “spreader of disinformation”, please, don’t take my word for it, visit Alexa and see for yourselves. Here is the link.
Some people might wonder why I post my web stats each month. Some might say I’m tooting my own horn, well maybe a bit, I’m admittedly proud of WUWT.
But there’s a bigger reason. WUWT has thousands of visitors, many of whom are regulars who see this blog as something they relate to and grow with. Many visit daily or even more often. There’s a sense of pride and ownership with all of you, too. Without its readers, content contributors, and those tireless volunteer moderators that keep the snark under control, WUWT would be nothing. I try not to lose sight of that. This is my monthly reminder to myself that this is a team effort.
As this blog expands its reach, it seems only fair that my readers also get to share in seeing the growth. I listen to my readers, who are often more insightful than I am, and they offer wonderful suggestions and topics.
I thank you all for making WUWT one of top science blogs on the Internet today.
And thanks to you to Joe, for bringing up this topic last month.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e963b5251',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
As many of you may know, I produce a variety of weather imagery maps for web and broadcast in SD and HD. Since there is a lot of interest in the path of hurricane Gustav, I thought I’d post a near-live image, which will update every 30 minutes.

Click image for full size or animate this image: Click for loop>>>   
What is interesting to note, is that as of this writing, Gustav seems to be losing organization. The eye, which was well defined just before making landfall on Cuba, seems very nebulous. Watch and wait.
Update: 3:30PM PST, while there was some weakening earlier, it now looks like signs of increased angular momentum are showing up in the satellite imagery. A defined eye may appear again.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cb0b9fe',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Some naive people might have been convinced that the U.S. House voted to wreck the American economy by endorsing cap and trade because it was the only way to save the world. But even many environmentalists had given up on the bill approved last Friday. It is truly a monstrosity: it would cost consumers plenty, while doing little to reduce global temperatures.   
  
  
But the legislation had something far more important for legislators and special interests alike. It was a pork‐​fest that wouldn’t quit.   
  
  
Reports the _New York Times_ :



As the most ambitious energy and climate‐​change legislation ever introduced in Congress made its way to a floor vote last Friday, it grew fat with compromises, carve‐​outs, concessions and out‐​and‐​out gifts intended to win the votes of wavering lawmakers and the support of powerful industries.   
  
  
The deal making continued right up until the final minutes, with the bill’s co‐​author Representative Henry A. Waxman, Democrat of California, doling out billions of dollars in promises on the House floor to secure the final votes needed for passage.   
  
  
The bill was freighted with hundreds of pages of special‐​interest favors, even as environmentalists lamented that its greenhouse‐​gas reduction targets had been whittled down.   
  
  
Some of the prizes were relatively small, like the $50 million hurricane research center for a freshman lawmaker from Florida.   
  
  
Others were huge and threatened to undermine the environmental goals of the bill, like a series of compromises reached with rural and farm‐​state members that would funnel billions of dollars in payments to agriculture and forestry interests.   
  
  
Automakers, steel companies, natural gas drillers, refiners, universities and real estate agents all got in on the fast‐​moving action.   
  
  
The biggest concessions went to utilities, which wanted assurances that they could continue to operate and build coal — burning power plants without shouldering new costs. The utilities received not only tens of billions of dollars worth of free pollution permits, but also billions for work on technology to capture carbon‐​dioxide emissions from coal combustion to help meet future pollution targets.   
  
  
That deal, negotiated by Representative Rick Boucher, a conservative Democrat from Virginia’s coal country, won the support of the Edison Electric Institute, the utility industry lobby, and lawmakers from regions dependent on coal for electricity.   
  
  
Liberal Democrats got a piece, too. Representative Bobby Rush, Democrat of Illinois, withheld his support for the bill until a last‐​minute accord was struck to provide nearly $1 billion for energy‐​related jobs and job training for low‐​income workers and new subsidies for making public housing more energy‐​efficient.   
  
  
Representative Joe Barton, a Texas Republican staunchly opposed to the bill, marveled at the deal‐​cutting on Friday.   
  
  
“It is unprecedented,” Mr. Barton said, “but at least it’s transparent.”



This shouldn’t surprise anyone who follows Washington. Still, the degree of special interest dealing was extraordinary. Anyone want to imagine what a health care “reform” bill is likely to look like when legislators finish with it?
"
"
Share this...FacebookTwitterAs winters get harsher and the snow piles up, more and more scientists are now warning of global cooling. Reader Matt Vooro has compiled a list (see below) of 31 prominent scientists and researchers who have words that governments ought to start heeding.
UPDATE: Another one for the list – Professor Paar, from Croatia’s Zagreb University
Cooling seems to be the trend. Photo source: NOAA. 
Lately, the clueless among warmist scientists, governments and the MSM have been running around in deep snow with their global warming blinders on, denying the cold around them. Governments, entrusted to serve the citizens, really ought to start listening up and planning accordingly.
———————————————————————————————–
Are we headed for global warming or cooling?
By Matt Vooro
For many years now a good number of non-AGW scientists, meteorologists, engineers, researchers and the like have looked at the possibilities of a cooling planet. I enclose some of the ones that I have noted  in my research. Indeed there is a significant number of scientists, academics, meteorologists and researchers who disagree with IPCC’s belief that the globe is very likely headed for unprecedented global warming due to man-made greenhouse gases.
The climate of this planet oscillates between periods of approximately 30 years of warming followed by approximately 30 years of cooling. Rather than 100 years of unprecedented global warming as predicted by IPCC, the global temperatures have leveled off and we seem to be heading for cooler weather.  
Lawrence Solomon in his article of 16 June, 2010 in the Toronto National Post commented on Professor Mike Hulme’s article about IPCC. The article can be found here at probeinternational.org. Hulme is a Professor of Climate Change in the School of Environmental Sciences at the University of East Anglia – the university of Climategate fame — is the founding Director of the Tyndall Centre for Climate Change Research and one of the UK’s most prominent climate scientists. Quoting Hulme, Solomon said:
The UN’s Intergovernmental Panel on Climate Change misled the press and public into believing that thousands of scientists backed its claims on manmade global warming, according to Mike Hulme, a prominent climate scientist and IPCC insider. The actual number of scientists who backed that claim was “only a few dozen experts” he states in a paper for Progress in Physical Geography, co-authored with student Martin Mahony.”
Professor Hulme’s paper can be found at fabiusmaximus
It would appear that IPCC underestimated the repetitive and significant impact of normal planetary cycles like the PDO, AO, AMO, NAO, ENSO, DEEP OCEAN CURRENTS [MOC], SOLAR CYCLES and UNEXPECTED PERIODS OF VOLCANIC ASH.
This is understandable as IPCC never had a mandate to study all causes of global warming but only the man induced component which seems to be dwarfed by natural planetary factors, which other scientists are now finding out. Read papers.ssrn.com/sol3/papers.
Here is a list of 31 different international climate scientists, academics, meteorologists, climate researchers and engineers who have researched this topic and who disagree with AGW science and IPCC forecasts, and are projecting much cooler weather for the next 1-3 decades.

The List

1. Don Easterbrook, Professor Emeritus, Dept. of Geology, Western Washington University.
Setting up of the PDO cold phase assures global cooling for next approx. 30 years. Global warming is over.  Expect 30 years of global cooling, perhaps severe 2-5°F.”
He predicts several possible cooling scenarios: The first is similar to 1945-1977 trends, the second is similar to 1880-1915 trends and the third is similar to 1790-1820 trends. His latest article states:
Expect global cooling for the next 2-3 decades that will be far more damaging than global warming would have been.”
Read here, here and here.
2. Syun Akasofu, Professor of Geophysics, Emeritus, University of Alaska, also founding director of ARC
He predicts the current pattern of temperature increase of 0.5C /100 years resulting from natural causes will continue with alternating cooling as well as warming phases. He shows cooling for the next cycle until about 2030/ 2040.
And again a new paper ON THE RECOVERY FROM LITTLE ICE AGE – Read here.
3. Prof. Mojib Latif, Professor, Kiel University, Germany
He makes a prediction for one decade only, namely the next decade [2009-2019] and he basically shows the global average temperatures will decline to a range of about 14.18 C to 14.28 C  from 14.39 C  [eyeballing his graphs].
He also said that “you may well  enter  a decade or two of cooling relative to the present temperature level”, however he did not indicate when any two decades of cooling would happen or whether the  second decade after the next decade will also be cooling. Read here and here.
4. Dr. Noel Keenlyside from the Leibniz Institute of Marine Sciences at Kiel University. The BBC writes:
The Earth’s temperature may stay roughly the same for a decade, as natural climate cycles enter a cooling phase, scientists have predicted.”
A new computer model developed by German researchers, reported in the journal Nature, suggests the cooling will counter greenhouse warming.”
Read here news.bbc.
5. Professor Anastasios Tsonis, Head of Atmospheric Sciences Group University of Wisconsin, and Dr. Kyle Swanson of the University of Wisconsin-Milwaukee. msnbc writes:
We have such a change now and can therefore expect 20 -30 years of cooler temperatures”
This is nothing like anything we’ve seen since 1950,”
Kyle Swanson of the University of Wisconsin-Milwaukee said. “Cooling events since then had firm causes, like eruptions or large-magnitude La Ninas. This current cooling doesn’t have one.”
Swanson thinks the trend could continue for up to 30 years.”
Also read The mini ice age starts here at dailymail.co.uk/.
6. William M Gray, Professor Emeritus, Dept of Atmospheric Sciences, Colorado State University
A weak global cooling began from the mid-1940’s and lasted until mid-1970’s. I predict this is what we will see in the next few decades.”
Read colostate.edu.
7. Henrik Svensmark , Professor DTU, Copenhagen. Henrik Svensmark writes:
Indeed, global warming stopped and a cooling is beginning. No climate model has predicted a cooling of the Earth, on the contrary. This means that projections of future climate is unpredictable.”
Read here.
8. Jarl R. Ahlbeck, D.Sc., AboAkademi University, Finland
Therefore, prolonged low solar activity periods in the future may cause the domination of a strongly negative AO and extremely cold winters in North America, Europe and Russia.”
Read here.
9. Dr. Alexander Frolov, Head of Russia’s state meteorological service Rosgidromet. The Daily Mail.co.uk quotes Frolov:
‘From the scientific point of view, in terms of large scale climate cycles, we are in a period of cooling.
‘The last three years of low temperatures in Siberia, the Arctic and number of Russia mountainous regions prove that, as does the recovery of ice in the Arctic Ocean and the absence of warming signs in Siberia.”
And writes:
Mr. Tishkov, deputy head of the Geography Institute at Russian Academy of Science, said: ‘What we have been watching recently is comparatively fast changes of climate to warming, but within the framework of an overall long-term period of cooling. This is a proven scientific fact’.” 
10. Mike Lockwood, Professor of Space Environmental Physics, University of Reading, UK. Read BBC News here:   
The UK and continental Europe could be gripped by more frequent cold winters in the future as a result of low solar activity, say researchers.”
11. Dr. Oleg Pokrovsky, Voeikov Main Geophysical Observatory: Ria Novosti writes:
There isn’t going to be an ice age, but temperatures will drop to levels last seen in the 1950s and 1960s.
Right now all components of the climate system are entering a negative phase.  The cooling will reach it’s peak in 15 years. Politicians who have geared up for warming are sitting on the wrong horse.
The Northeast Passage will freeze over and will be passable only with icebreakers.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Pokrovsky also claims that the IPCC, which has prophesized global warming, has ignored many factors. He also noted that most American weather stations are located in cities where temperatures are always higher.
We don’t know everything that’s happening. The climate system is very complex and the IPCC is not the final truth on the matter.”
Read here NoTricksZone.
12. Girma Orssengo, b.Tech, MASc, PhD 
These cool and warm PDO regimes correlate well with the cooling and warming phases of GMTA shown in Figure 3.
The model in Figure 3 predicts global cooling until 2030. This result is also supported by shifts in PDO that occurred at the end of the last century, which is expected to result in global cooling until about 2030 [7].”
Read WUWT and read here, and
In this article, a mathematical model was developed that agrees with observed Global Mean Temperature Anomaly(GMTA), and its prediction shows global cooling by about 0.42 deg C until 2030. Also, comparison of observed increase in human emission of CO2 with increase in GMTA during the 20th century shows no relationship between the two. As a result, the claim by the IPCC of climate catastrophe is not supported by the data.”
‘Fossil fuels allowed man to live his life as a proud human, but the IPCC asserts its use causes catastrophic.’ “
Read here at WUWT.
13. Nicola Scafetta, PhD.
Empirical evidence for a celestial origin of the climate oscillations and its implications
The partial forecast indicates that climate may stabilize or cool until 2030-2040.”
Read here
14. Dr William Livingston, astronomer  & solar physicist; and 15. Dr Matthew Penn – astronomer &  solar physicist
Astronomers Dr. William Livingston and Dr. Matthew Penn and a large number of solar physicists would say that now the likelihood of the Earth being seized by Maunder Minimum is now greater than the Earth being seized by a period of global warming.”
Read here: http://algorelied.com/?p=2706.
16. Joe d’Aleo – Executive Director of Certified Consultant Meteorologists. Read here:  Intellicast.com
Longer term the sun is behaving like it did in the last 1700s and early 1800s, leading many to believe we are likely to experience conditions more like the early 1800s (called the Dalton Minimum) in the next few decades. That was a time of cold and snow. It was the time of Charles Dickens and his novels with snow and cold in London.”
Also see various other articles about Global Cooling under ICE AGE at Ice Cap
17. Harry van Loon, Emeritus at NCAR and CORA, 18. Roland Madden, Senior scientist at NOAA, Deputy Head of Climate analysis, 19. Dave Melita, Head Meteorologist at Melita Weather Associates, and 20. William M Gray, Professor Emeritus, Dept of Atmospheric Sciences, Colorado State University
These scientists came to the same conclusions— the global warming trend is done, and a cooling trend is about to kick in.
Read here!
21. Dr. David Archibald, Australia, environmental scientist:
In this presentation, I will demonstrate that the Sun drives climate, and use that demonstrated relationship to predict the Earth’s climate to 2030. It is a prediction that differs from most in the public domain. It is a prediction of imminent cooling.”
See Warwick Hughes and David Archibald
22. Dr Habibullo Abdussamatov, Head of Space Research, Lab of Pulkov Observatory. See iceagenow.com:
In his presentation called The Sun Dictates the Climate, he indicated that there would be an ice age kind of temperatures in the middle of the 21st century. He showed a graph called The forecast of the natural climate change for the nearest 100 years and it showed the globa temperatures dropping by more than 1°C by 2055. According to him, a new ice age could start by 2014.”
And read here.
23. Dr Fred Goldberg, Swedish climate expert. People Daily:
We could have an ice age any time, says Swedish climate expert.”
and read: We could have an ice age any time, says Swedish climate expert
24. Dr. George Kukla, a member of the Czechoslovakian Academy of Sciences and a pioneer in the field of astronomical forcing, Read Ice Age Now:
In the 1970s, leading scientists claimed that the world was threatened by an era of global cooling.
Based on what we’ve learned this decade, says George Kukla, those scientists – and he was among them — had it right. The world is about to enter another Ice Age.”
25. Peter Clark, Professor of Geosciences at OSU: Read iceagenow.com: 
Sometime around now, scientists say, the Earth should be changing from a long interglacial period that has lasted the past 10,000 years and shifting back towards conditions that will ultimately lead to another ice age.”
26. James Overland, NOAA. Read physorg.com:
‘Cold and snowy winters will be the rule rather than the exception,’ said James Overland of the US National Oceanic and Atmospheric Administration.”
27. Dr. Theodore Landscheidt. Predicted in 2003 that the current cooling would continue until 2030 [Read here]:
Analysis of the sun’s varying activity in the last two millennia indicates that contrary to the IPCC’s speculation about man-made global warming as high as 5.8°C within the next hundred years, a long period of cool climate with its coldest phase around 2030 is to be expected.”
28. Matt Vooro, P. Eng. The icecap.us:
We seem to be in the same climate cycle that we were back in 1964-1976.The last two winters [2008, 2009] have been very similar to those we had back then with all the extra snow and cold temperatures. Once the extra warming effect of the current 2009/2010 El Nino is finished, watch for colder temperatures to return due to the impact of the negative PDO, AMO, AO, NAO, ENSO/La Nina, major volcanic ash and changing solar cycles.”
Good source of articles and data on global cooling, see: Isthereglobalcooling.com
29. Thomas Globig, Meteorologist, Meteo Media weather service. Read here at WUWT:
‘The expected cold for the next month will bring this down significantly by year end. ‘The year 2010 will be the coldest for ten years in Germany,’ said Thomas Globig from the weather service Meteo Media talking to wetter.info. And it might even get worse: ‘It is quite possible that we are at the beginning of a Little Ice Age,’ the meteorologist said. Even the Arctic ice could spread further to the south.”
30. Piers Corbyn, Astrophysicist. From http://wattsupwiththat.com/2010/12/27/piers-corbyn-goes-global-cooling/
Predicting in November that winter in Europe would be “exceptionally cold and snowy, like Hell frozen over at times,” Corbyn suggested we should sooner prepare for another Ice Age than worry about global warming. Corbyn believed global warming “is complete nonsense, it’s fiction, it comes from a cult ideology. There’s no science in there, no facts to back [it] up.”
31. Dr. Karsten Brandt, Director of donnerwetter.de weather service.
It is even very probable that we will not only experience a very cold winter, but also in the coming 10 years every second winter will be too cold. Only 2 of 10 will be mild.
Read here.
32. Joe Bastardi – Accuweather meteorologist and hundreds of other meteorologists (i.e. expert forecasters who outperform climatologists hands-down in seasonal forecasting).
http://www.accuweather.com/ukie/bastardi-europe-blog.asp
Other global cooling articles:
John Holdren Ice Age Likely
BBC News
Global Warming Debunked
Star Tribune
nationalpost.com
canadafreepress.com
news.yahoo.com
======================================================
Share this...FacebookTwitter "
"
Old Radar Sites In Greenland Show Icecap Growth Over the Years
(And let’s not forget what we’ve learned about the temperature reporting from the DEW line Radar Stations – Anthony)
By Joseph D’Aleo, CCM, AMS Fellow
Though the ice may be melting around the edges of the Greenland Icecap in recent years during the warm mode of the AMO much as it did during the last warm phase in the 1930s to 1950s, snow and ice levels continue to rise in most of the interior. Johannessen in 2005 estimated an annual net increase of ice by 2 inches a year. 

(Above: Recent Ice-Sheet Growth in the Interior of Greenland, Ola M. Johannessen, Kirill Khvorostovsky, Martin W. Miles, Leonid P. Bobylev, Science Express on 20 October 2005 Science 11 November 2005: Vol. 310. no. 5750, pp. 1013 � 1016, DOI: 10.1126/science.1115356)
A Canadian Icecap emailer noted during the cold war there were two massive radar sites built on the Greenland icecap now abandoned. They are called Dye-2 and Dye-3. When built they sat high above the snow, recent pictures show how the snow is building up around them, proving the snow build-up in recent times. This demonstrates this snow accumulation over time.
Dye-2 and 3 were among 58 Distance Early Warning Line radar stations built by America between 1955-1960 across Alaska, Canada, Greenland and Iceland at a cost of billions of dollars. Their powerful radars monitored the skies constantly in case Russia decided to send bombers towards America. After extensive studies in late 1957, the USAF selected sites for two radar stations on the ice cap in southern Greenland. Dye-2 was to be built approximately 100 miles east of Sondrestrom AB and 90 miles south of the Arctic Circle at an altitude of 7, 600 feet, and Dye-3 was to be located approximately 100 miles east of DYE II and slightly south at an elevation of 8,600 feet.
The selected locations for the new radar sites were found to receive from three to four feet of snow fall each year. Since the winds were constantly blowing with speeds as much as 100 mph, this snow accumulation constantly formed large drifts. To overcome this potential problem, it was decided that the Dye sites should be elevated approximately twenty feet above the surface of the ice cap.
Dye 3 was built in 1960. From a distance the structure, with its onion-shaped dome, looks like a Russian orthodox church. Dye 3 was an ice core site and previously part of the DEW line in Greenland.  (The Distant Early Warning (DEW) Line: A Bibliography and Documentary Resource List Arctic Institute of North America, Page 23). As a Distant Early Warning line base, it was disbanded in years 1990/1991. The Dye 3 cores were part of the GISP (Greenland Ice Sheet Project initiated in 1971) and, at 2037 meters, was the deepest of the 20 ice cores recovered from the Greenland ice sheet as part of GISP. Samples from the base of the 2km deep Dye 3 and the 3km deep GRIP cores revealed that high-altitude southern Greenland has been inhabited by a diverse array of conifer trees and insects within the past million years. (Eske Willerslev, et al. (2007) Ancient Biomolecules from Deep Ice Cores Reveal a Forested Southern Greenland Science 317 111-114)
The first image below is  from 1972.

See larger image here.
Here it is in 2006.

See larger image here.
In looking back at the time the sites were abandoned, one console operator lamented “We were very busy during this time and I was sad to see it end. I remember thinking of all the waste,” he said. The site is slowly disappearing into the snow. Its outbuildings are no longer visible and drifting snow will consume it completely one day, but that day appears to be decades away.” Read more here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99c3a0f3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

As billions of little light bulbs brighten America this holiday season, Al Gore is calling for thousands across the nation to interrupt their regularly scheduled activities and hold house parties showing his environmental _cri de coeur_.



Gore announced recently on the Oprah Winfrey Show that Americans should congregate this Saturday, December 16, to watch and discuss his DVD, _An Inconvenient Truth_ , advertised as “a true story about the hard science and real threats of global warming.”



The idea is to demonstrate that “action” is wanted on climate change.



If climate alarmists are to be believed, Americans must cut their electricity use substantially, and soon, to reduce greenhouse‐​gas emissions associated with fossil‐​fuel combustion. Celebratory holiday lighting — what doomsayer Paul Ehrlich once called “garish commercial Christmas displays” — would surely be the first to go, coming before indoor lighting, cooking, heating, and air conditioning.



But are these changes really necessary for the United States, the world’s most prolific user of energy? The good news — and a reason for holiday cheer — is that the science behind rapid, disruptive global warming scenarios is murky at best. Though the debate is highly politicized and emotionally charged, good science is beginning to drive out bad.



The Kyoto Protocol and other sledgehammer approaches to cutting greenhouse‐​gas emissions in the advanced countries are coming under intellectual, not just political, assault.



A sampling of recent issues of _Science_ , the journal of the American Association for the Advancement of Science, shows that peer‐​reviewed studies dispute virtually all the tenets behind climate alarmism. A November 17 feature, “False Alarm: Atlantic Conveyor Belt Hasn’t Slowed Down After All,” rebuts the hyped hypothesis that melting ice from global warming (read: man‐​made global warming) would disrupt ocean currents and plunge Europe into an Ice Age.



The same _Science_ report takes on the idea that warming causes drastic cooling, the complicated, and ironic scenario Al Gore said “some scientists are seriously worried about.”



 _Science_ comments that even if global warming were cooling specific regions (a big if), “it would be decades before the change would be noticeable above the noise.”



And here, in a nutshell, is what the climatology debate is about: if and how much the human influence on climate is detectable above natural variability.



For instance, rapid rises in sea level produced by global warming is another popular alarm, one very relevant for residents of the Texas Gulf Coast area. But as the November 24 issue of _Science_ says, “It remains unclear whether the recent rate increase [since 1993] reflects an acceleration in sea‐​level rise or a natural fluctuation.”



Indeed, sea level has been rising for well over a century for the same natural reasons that brought the end of a little ice age. What scientists are measuring and debating concerns not feet but inches, and fractions thereof, over many decades. This hardly seems the crisis scenario that Al Gore portrays.



Gore claims, “There is now a strong, new emerging consensus that global warming is indeed linked to a significant increase in both the duration and intensity of hurricanes.”



But hurricane specialists disagree. The November 10 _Science_ says, “The best theory and modeling still indicate the ocean temperature has only a minimal effect on storms.”



Exaggerated forecasts of disrupted ocean circulation, rapid sea‐​level rise, and more intense hurricanes make for splashy headlines, but sober science suggests that these scares _du jour_ may go the way of yesterday’s alarms over global cooling, the population bomb, and mineral‐​resource exhaustion.



Nonetheless, one part of these scare stories is genuinely frightening: the heavy‐​handed government intervention that advocates always look to as the source of salvation. Yesterday’s foes of the free market were socialists, communists, and Keynesians. Today’s are greens who want government engineering to “stabilize” the climate and ensure “sustainability.”



I will not be watching Al Gore’s quasi‐​sci‐​fi horror movie this Saturday night. I’ll probably be driving through neighborhoods of people I don’t even know, enjoying the gift of their holiday lights. And to them I say: Don’t fall for exaggerations. Enjoy your regularly scheduled activities, and keep the lights on.
"
"
 Part II: Where does global warming rank among future risks to public health?

Guest essay by Indur M. Goklany
In Part 1, we saw that at present climate change is responsible for less than 0.3% of the global death toll. At least 12 other factors related to food, nutrition and the environment contribute more. All this, despite using the World Health Organization’s scientifically suspect estimates of the present-day death toll “attributable” to climate change,
Here I will examine whether climate change is likely to be the most important global public health problem if not today, at least in the foreseeable future.
This examination draws upon results generated by researchers who are prominent contributors to the IPCC consensus view of climate change.  I do this despite the tendency of their analyses to overstate the net negative impacts of climate change as detailed, for instance, here, here and here.
Specifically, I will use estimates of the global impacts of climate change from the British-government sponsored “Fast Track Assessments” (FTAs) which have been published in the peer reviewed literature. Significantly, they share many authors with the IPCC’s latest assessment. For example, the lead author of the FTA’s study on agricultural and hunger impacts is Professor Martin Parry, the Co-Chairman of the IPCC Work Group 2 during the preparation of the IPCC’s latest (2007) assessment.  This Work Group was responsible for the volume of the IPCC report that deals with impacts, vulnerability and adaptation.
I will consider “the foreseeable future” to extend to 2085 since the FTAs purport to provide estimates for that date, despite reservations.  In fact, a paper commissioned for the Stern Review (p.74) noted that “changes in socioeconomic systems cannot be projected semi-realistically for more than 5-10 years at a time.” [Despite this caution, Stern’s climate change analysis extended to at least 2200.]
In the following figure, using mortality statistics from the WHO, I have converted the FTAs’ estimates of the populations at risk for hunger, malaria, and coastal flooding into annual mortality. Details of the methodology are provided here.

In this figure, the left-most bar shows cumulative global mortality for the three risk categories in 1990 (the baseline year used in the FTAs). The four “stacked” bars on the right provide mortality estimates projected for 2085 for each of the four main IPCC scenarios. These scenarios are arranged from the warmest on the left (for the so-called A1FI scenario which is projected to increase the average global temperature by 4.0°C as indicated by the number below the stacked bar) to the coolest on the right (for the B1 scenario; projected temperature increase of 2.1°C).  Each stacked bar gives estimates of the additional global mortality due to climate change on the top, and that due to other non-climate change-related factors on the bottom. The entire bar gives the total global mortality estimate.
To keep the figure simple, I only show estimates for the maximum (upper bound) estimates of the mortality due to climate change for the three risk factors under consideration.
This figure shows that climate change’s maximum estimated contribution to mortality from hunger, malaria and coastal flooding in 2085 will vary from 4%-10%, depending on the scenario.
In the next figure I show the global population at risk (PAR) of water stress for the base year (1990) and 2085 for the four scenarios.

A population is deemed to be at risk if available water supplies fall below 1,000 cubic meters per capita per year.
For 2085, two bars are shown for each scenario. The left bar shows the net change in the population at risk due to climate change alone, while the right bar shows the total population at risk after accounting for both climate change and non-climate-change related factors. The vertical lines, where they exist, indicate the “spread” in projections of the additional PAR due to climate change.
This figure shows that climate change reduces the population at risk of water stress! This is because global warming will decrease rainfall in some areas but serendipitously increase it in other, but more populated, areas.
The figure also suggests that the warmest scenario would result in the greatest reduction in net population at risk.
[Remarkably, both the IPCC’s Summary for Policy Makers and the original source were reticent to explicitly point out that climate change might reduce the net population at risk for water stress. See here and here (pages 12-14 or 1034-1036).].  Thus, through the foreseeable future (very optimistically 2085), other factors will continue to outweigh climate change with respect to human welfare as characterized by (a) mortality for hunger, malaria and coastal flooding, and (b) population at risk for waters stress.
In the next post in this series, I will look at a couple of ecological indicators to determine whether climate change may over the “foreseeable future” be the most important problem from the ecological perspective, if not, as we saw here, from the public health perspective. 



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e969a1285',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

“Of course you realize, this means war!” – Bugs
War has been declared in the New  York court system over global warming regulation.
Indeck Corinth L.P., which operates the Corinth Generating Station, an  electric power plant in Corinth, NY, sued New York stateon January 29, 2009 claiming that the Regional  Greenhouse Gas Initiative (RGGI) that aims to reduce greenhouse gas emissions in  the Northeast U.S. is illegal.
Corinth Generating Station -click for interactive view- Source: Microsoft Live Earth
Maine, New Hampshire, Vermont, Connecticut, New York, New Jersey, Delaware, Massachusetts, Maryland, and Rhode Island have signed on to the RGGI agreement. You can read more about it here at:  http://www.rggi.org/home
This is the simple view of RGGI from their website:
The Regional Greenhouse Gas Initiative (RGGI) is the first mandatory, market-based effort in the United States to reduce greenhouse gas emissions. Ten Northeastern and Mid-Atlantic states will cap and then reduce CO2 emissions from the power sector 10% by 2018.
States will sell emission allowances through auctions and invest proceeds in consumer benefits: energy efficiency, renewable energy, and other clean energy technologies. RGGI will spur innovation in the clean energy economy and create green jobs in each state.
Indeck Corinth claims that New  York’s involvement with RGGI does the following:

Is ultra vires and violates the state constitution;
Imposes an impermissible tax not authorized by the state legislature; 
Is arbitrary and capricious as implemented by New York; 
Is pre-epmted by state and federal  regulations; 
Violates the Compact Clause of the U.S.  Constitution; and 
Violates Indeck Corinth’s due process and  equal protection rights 

See Indeck Corinth’s legal complaint.  (PDF)
Indeck Corinth and New  York State  are now arguing over the venue for the suit. Indeck Corinth wants the suit heard in Saratoga County where it is a major employer. New York wants the suit heard in Albany County where it has home field advantage.
This will be watched intensely by many on both sides of the energy -versus- environment issue.
h/t to Junkscience.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9708bedc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

When we began drafting this study of U.S. military spending and force posture, we had no way of knowing the tremendous challenge that COVID-19 would pose. It has wreaked havoc on the economy. It has disrupted every facet of American life. The impact will reverberate for generations. The global pandemic—and the U.S. government’s response to it—has threatened the lives and liberties of Americans as well as the United States’ standing in the world.



This disaster is a call to action. The threat posed by nontraditional security challenges, including pandemics, climate change, and malicious disinformation, should prompt a thoroughgoing reexamination of the strategies, tactics, and tools needed to keep the United States safe and prosperous.



As of this writing in late April 2020, and well before the full impact of COVID-19 is known, it seems obvious to us that the United States can no longer justify spending massive amounts of money on quickly outdated and vulnerable weapons systems, equipment that is mostly geared to fight an enemy that might never materialize. Meanwhile, the clearest threats to public safety and political stability in the United States are very much evident and all around us. Just how demonstrations of force or foreign stability operations contribute to U.S. national security is particularly questionable at a time when a microscopic enemy has brought the entire world to a standstill.



This analysis mostly examines where the U.S. military was as of December 31, 2019, with a few observations from early 2020. Where it will be on December 31, 2020, will be guided by a critical set of questions. The authors, and the entire team of scholars in the Cato Institute’s Defense and Foreign Policy Studies Department, intend to help frame those questions—and to answer as many as possible—over the coming year.



Security politics will be different in the future, but the goal of security policy hasn’t changed and is clearly outlined in this report: to identify the most effective and efficient means for advancing Americans’ safety and prosperity. That entails ending the forever wars, terminating needless military spending, rethinking the fundamentals of strategic deterrence, and focusing the entire defense establishment on innovation and adaptation.



 **Online Policy Forum | June 4, 2020 1:00 PM to 3:00 PM EDT**



Even before COVID-19, military spending was unsustainable. In a new Cato report, scholars argue that nontraditional security challenges require a thorough reexamination of national defense strategy. The report prioritizes ending the forever wars, terminating needless military spending, rethinking the fundamentals of strategic deterrence, and focusing the entire defense establishment on innovation and adaptation.



Budgetary and strategic inertia has impeded the development of a U.S. military best suited to deal with future challenges. Over the past several decades, the military has repeatedly answered the call to arms as American foreign policy privileges the use of force over other instruments of power and influence. The era of near endless war has now stretched into its third decade. Going forward, Washington should realign national security objectives and motivate allies and partners to become more capable as America’s relative military advantage wanes and the focus inevitably turns to domestic priorities, including public health.



As policymakers transition from primacy and unilateral military dominance, and beyond the post‐​9/​11 wars in the greater Middle East, the force must also be reoriented. The defense establishment’s most urgent requirement is prioritization. The nation’s resource constraints are real, and hard choices cannot be postponed. In particular, all military branches should emphasize innovation over the preservation of legacy systems and practices. This will require cooperation from Congress, which must address the budget pathologies that stifle new thinking and keep the Pentagon locked into old ways of doing business. Senior defense officials must orient the future force around a different approach to power projection, one less dependent on permanent forward bases, and toward a renewed focus on the requirements for strategic deterrence. The services must also think anew about how to best capture and use information.



Despite recent challenges and setbacks, most importantly the COVID-19 outbreak and response, the United States still enjoys many advantages, including a dynamic economy, political stability, and favorable geography. Securing the United States from future threats should sustain and build on those advantages. Restraining the impulse to use force, imposing limits on military spending, and relying more heavily on diplomacy, trade, and cultural exchange would relieve the burdens on our overstressed military. The ultimate objective should be to build an agile and adaptable military that can address a range of future challenges but is used more judiciously in the service of vital U.S. interests and to deter attacks against the homeland.



Building a modern military requires a clear conceptualization of the realities of international conflict and tight alignment with a country’s foreign policy. Strategic planners must have a clear‐​eyed view of both the threats facing the country and the tools necessary to defend its vital interests. Planners in the United States should take account of the country’s fortunate circumstances, including its geography, dynamic economy, and political stability, and recognize that maintaining these advantages does not require a massive military apparatus that is constantly active in nearly every part of the world.



For decades, however, U.S. national security policy has been oriented around a military‐​centric approach, variously called primacy, liberal hegemony, or deep engagement. Primacy is based on the idea that U.S. military power explains the absence of a major‐​power war since the end of World War II and the attendant rise in productivity and living standards. Harvard political scientist Samuel Huntington predicted in 1993, for example, that “a world without U.S. primacy will be a world with more violence and disorder and less democracy and economic growth.”1 Former secretary of state George Shultz put it even more succinctly in the 2016 documentary _American Umpire_ : “If the United States steps back from the historic role [it has] played since World War II, the world will come apart at the seams.”2



Such sentiments reflect why, despite the fact that the United States enjoys relative safety, U.S. officials see only grave and urgent dangers. They see any challenge to U.S. military dominance as a threat to global liberty and peace. The 2018 _National Defense Strategy_ (NDS), for example, notes that the “central challenge to U.S. prosperity and security is _the reemergence of long‐​term, strategic competition_ by … revisionist powers.” The goal then, according to the NDS, is to “remain the preeminent military power in the world.”3 The 2017 _National Security Strategy_ (NSS) goes further, noting that the “United States must retain overmatch—the combination of capabilities in sufficient scale to prevent enemy success and to ensure America’s sons and daughters will never be in a fair fight.”4



And while the United States is purportedly orienting around great power competition against China and Russia, the post‐​9/​11 conflicts grind on. The National Defense Authorization Act (NDAA) for Fiscal Year 2020 makes clear that the Pentagon envisions those conflicts continuing indefinitely.5 Today’s U.S. military budget, after adjusting for inflation, vastly exceeds that of the Cold War and now approaches levels during the height of the wars in Iraq and Afghanistan in the early 2010s (see Figure 1). Operationally, the Pentagon has been bogged down in Afghanistan and caught in the ongoing struggle between Saudi Arabia and Iran for dominance in the Persian Gulf region and beyond; in December 2019, the Trump administration was considering sending an additional 14,000 troops to the Middle East, including a substantial ground presence in Saudi Arabia, for the first time in nearly 17 years.6



Perceptions of looming threats or fear of potential peer competitors should not distract from the obvious need to take a strategic pause and reconsider the United States’ core defensive needs, especially during a global pandemic and associated economic disaster.7 Washington should realign its national security ends and means to better match the emerging geopolitical reality—especially America’s waning relative military power.8 The desire for one‐​sided “overmatch” is understandable but impractical given the extensive commitments it entails. The time is ripe to make a clean break from the past.



The dramatic shock of COVID-19 adds urgency to the need for new strategic priorities. This report acknowledges that the nation’s resource constraints are real and that the United States faces a period of grave economic uncertainty. The Pentagon is not immune to these pressures. Politicians are unlikely to undertake a concerted campaign to build public support for massive increases in taxes or deep cuts to popular domestic programs in order to fund a military that an ambitious grand strategy calls for, and they would likely fail if they tried. The U.S. military is spending beyond its means due mostly to inertia and strategic indecision. To that end, this report is founded on three pillars: articulating a force that meets the _realities of the geopolitical situation_ and contemplating the current budget pathologies that impede change; reexamining _force construction_ ; and evaluating the posture needed for _modern strategic deterrence_. These pillars drive the recommendations contained herein with an aim toward developing a more realistic and prudent military budget.



The Trump administration’s budget proposal for fiscal year (FY) 2021 aims for “U.S. military dominance in all warfighting domains—air, land, seas, space, and cyberspace,” echoing its FY 20 budget proposal, which supported “dominance across all domains.”9 This view is consistent with the desire for “overmatch” in the NSS. Overmatch requires the United States to “restore our ability to produce innovative capabilities, restore the readiness of our forces for major war, and grow the size of the force so that it is capable of operating at a sufficient scale and for ample duration to win across a range of scenarios.”10



This quest for global dominance is taking place as the United States’ capacity for sustaining supremacy is waning. The NDS observes, for example, that “we are emerging from a period of strategic atrophy, aware that our competitive military advantage has been eroding. We are facing increasing global disorder, characterized by decline in the long‐​standing rules‐​based international order—creating a security environment more complex and volatile than any we have experienced in recent memory.”11



The NDS does not treat this diagnosis as a recognition of the limits of American military power but rather as a rallying cry to marshal additional national resources and maintain the globe‐​spanning posture to which Washington has grown accustomed. Raging against the dying light of uncontested military primacy will run into severe budgetary and strategic obstacles.



The international order faces many challenges, and these cannot be reversed by attempting to restore U.S. dominance across all domains and in all regions. Instead, U.S. grand strategy should encourage allies and partners—the leading beneficiaries of global peace and stability—to take a greater role in sustaining it. The United States cannot be the world’s police force or coast guard.



The United States needs a prudent military strategy that can protect U.S. interests without turning into an open‐​ended pursuit of anachronistic, grand goals. “Overmatch” extending across all regions, domains, and weapons systems is simply untenable. The “America First” view of primacy focuses on military hardware and manpower, not the elements of smart power that have traditionally been the real sources of American strength and influence.12 Simply put, the United States today is overinvested in the military. As a recent Cato book explains, “a less expansive foreign policy agenda will allow the United States to reduce military spending significantly.”13 Washington should take advantage of the current period of relative geopolitical stability to adopt a military posture consistent with grand strategic restraint.14 Such a reorganization would bring much‐​needed coherence to U.S. military strategy.



The recommendations in this report are not driven by perceptions of waste and bloat within the U.S. defense establishment—though there is certainly much of that. Rather, the authors assess international politics today as well as the probable nature of future threats and fix on what is required to defend the U.S. homeland and vital interests.



The current approach relies heavily on the use of force and coercion at the expense of other instruments of power and influence. A military‐​centric strategy seems particularly ill‐​suited to a post–COVID-19 world.15 The primary tools of American global engagement under a grand strategy of restraint should be trade, diplomacy, and cultural exchange. The military instrument, while still vital, should be geared toward defense, in the strictest sense of the word, enabling allies and partners to counter adversaries. A grand strategy of restraint would leverage innovation and modernization to refocus on a narrower range of future challenges and to rethink how strategic deterrence could better serve the needs of the nation.



Adopting a new grand strategy, and fashioning a new force posture to suit, also requires a reconsideration of the value of forward deployment. The United States should reduce its permanent overseas presence, especially in forward‐​operating bases that will be vulnerable if conflict erupts. Under a strategy of restraint, the U.S. Navy and Air Force would be a surge force capable of deploying to crisis zones if local actors prove incapable of addressing threats.



The United States can support allies and prepare for future combat by enabling others to defend themselves and their interests. U.S. force planning should be oriented around how the U.S. military can contribute to such operations from a distance as U.S. interests dictate. In those rare instances where vital national interests necessitate the deployment of U.S. personnel well outside of the Western Hemisphere, Pentagon planners must ensure adequate facilities and resources to resupply their operations. Relying on forward‐​deployed forces as we currently do risks inadvertently creating a security dilemma that encourages prospective rivals to match such deployments. By focusing on modernization and interoperability, U.S. forces could assist others while reducing the risk of escalation. Equally important, an over‐​the‐​horizon posture would reduce demands on the U.S. military—especially on active‐​duty personnel.



A grand strategy of restraint calls for a less active conventional military, one that is not deployed in permanent bases or routinely engaged in offensive operations on multiple continents. Even so, restraint is not synonymous with disarmament; the United States will continue to rely on nuclear weapons to deter some strategic attacks. However, the current concept of “strategic deterrence” and the role of nuclear weapons in U.S. defense strategy would have to change. The main problem with Washington’s approach to strategic deterrence—as with U.S. military strategy in general—is that it suffers from mission creep.



At its core, “strategic deterrence” is preventing a first use of nuclear weapons against the U.S. homeland or an ally. But that is not the only behavior that U.S. officials currently seek to deter. The 2018 _Nuclear Posture Review_ (NPR), for example, says that the United States would consider using nuclear weapons to respond to “significant non‐​nuclear strategic attacks” against U.S. or allied civilians, infrastructure, and early warning capabilities.16



An overly broad definition of strategic threats drives demands for a large and diversified nuclear arsenal and missile defense capability in order to have many flexible response options.17 As a result, Washington’s approach to strategic deterrence places great weight on adversary capabilities. For example, according to the 2018 NPR, “Moscow’s perception that its greater number and variety of non‐​strategic nuclear systems provide a coercive advantage in crises at lower levels of conflict.”18 This is the supposed justification for new, low‐​yield U.S. nuclear weapons.19 Similarly, the _2019 Missile Defense Review_ cites the threat of hypersonic glide vehicles—high-speed maneuvering warheads that take an unpredictable rather than ballistic route to their target and that China and Russia are developing as a response to U.S. missile defense expansion—as a rationale for deploying more missile defense sensors on satellites.20



Having a flexible nuclear arsenal and missile defense system that can be tailored to respond to the unique characteristics of different threats sounds sensible. However, the failure to prioritize produces a kind of paralysis. In a world where dangers loom around every corner, doing anything less than deterring all of them at once is considered a failure. This encourages wasteful spending and invites potential adversaries to create counter strategies that increase the likelihood of inadvertent nuclear escalation. Such moves damage deterrence instead of strengthening it. Deterrence under restraint would have a narrower set of objectives and clearer priorities and would privilege clarity and reliability over flexibility.



If the United States would prefer to engage adversaries at a distance, strategists need to rethink how the future force should be organized. Improving the ability of the different services to communicate with one another and have smoothly functioning command and control during a conflict is especially critical.21 A traditional focus on raw firepower and the impulse to base personnel and equipment at great distances from the United States will likely need to give way to an emphasis on developing a technologically proficient force that relies on new layers of sensors (radar, sonar, etc.) that can direct long‐​range attacks and control unmanned vehicles at greater distances. Another overlooked capability in debates over the defense budget is the redundancy of reconnaissance systems—the ability of America’s intelligence‐​gathering satellites and aircraft to perform their functions if they are disrupted.



The current approach of massive investment in the military, displays of force, and direct challenges to multiple adversaries in their respective regions is often counterproductive. As Sen. Angus King (I-ME) notes, with respect to Iran “the unanswered question is who is provoking whom. As we escalate sending more troops, moving aircraft carriers, we view it as preventative and as defensive. They view it as provocative and leading up to a preemptive attack.”22 Michael O’Hanlon of the Brookings Institution made a similar point in 2017. Arguing for a new approach to European security, O’Hanlon explained that the United States “may be able to help ratchet down the risks of NATO‐​Russia war … by recognizing that NATO expansion, for all its past accomplishments, has gone far enough.”23 U.S. messaging must be consistent, and budgetary maneuvers should not introduce justifications for war. The overarching recommendation here is to halt policies that exacerbate regional security dilemmas and to restructure U.S. military power accordingly. Such a restructuring is made more difficult, however, by the rigidity of the budgeting process.



Spending patterns driven by inertia and habit privilege the military, the use of force, and coercion over diplomacy and other instruments of American power. Accordingly, the Pentagon’s budget continues to reflect strategic errors of the past, including searching for a peer competitor, continuing support for a counterproductive war on terror, and propping up dangerous and unreliable strategic partners. To complicate matters, Congress and the White House are sparring over new distractions, including potentially diverting funds from the military budget for border wall construction24 We refer to these distractions as “budget pathologies”: abnormalities and malfunctions inherent in how the U.S. government secures funding for the military, a process that often impedes the creation of a viable national strategy. The executive branch initiates many of these pathologies, but Congress also plays a key enabling role by not exercising its traditional power of the purse.



On December 9, 2019, for example, a House and Senate conference committee passed the FY 20 NDAA. The bill authorized $738 billion for national defense spending, and President Trump proudly signed it into law.25 The U.S. government continues to spend and act as if its wars in the Middle East will never end. Secretary of Defense Mark Esper described such operations as not “necessarily unusual” and noted that “we continue … ‘to mow the lawn.’ And that means, every now and then, you have to do these things to stay on top of [the threat].”26 In fact, these operations represent sunk costs and reflect misguided assumptions about what actually makes Americans safe and prosperous.



The U.S. military budgeting process is supposed to reflect a delicate balance between executive‐​level strategic guidance, Department of Defense (DOD) budget requests related to the overarching strategy, and legislative approval and appropriations to fund the requests. The actual process of funding the nation’s military, however, bears no resemblance to that ideal. A recent, clear sign of just how badly this process has broken down was revealed when the Trump administration tried to strip $3.6 billion from existing Pentagon projects to fund improvements for physical barriers at the U.S.-Mexico border. The funds earmarked to be ripped away would have served to upgrade and maintain the surface fleet, improve basic services on military bases, and expand the nation’s offensive cyber capabilities.27



This is just one example. There are many pathologies—spending decisions that serve partisan or parochial interests but do not advance U.S. security—that consistently undermine the entire federal budget, not merely what winds up in the Pentagon’s coffers. The most serious problem pertains to the unwillingness of American elected officials to reconcile spending and revenue. Despite the occasional attempt to reverse the tide, nothing has had lasting success. When Congress passed the Budget Control Act in 2011, the annual budget deficit stood at $1.3 trillion. Four years later the annual deficit fell to $438 billion. However, this figure has risen each subsequent year, exceeding $984 billion at the end of FY 19—the highest since 2012.28



Very few Americans appreciate the scale of the federal government’s spending. A poll taken in early 2017 found that only 1 in 10 Americans could correctly identify the amount spent on the military within a range of $250 billion.29 And yet, according to a 2019 Gallup survey, only 1 in 4 Americans believe that U.S. military spending should increase at all, while a slightly higher percentage (29 percent) thinks the United States is spending too much.30



The main budgetary problem for the Pentagon, therefore, is political. It refuses to budget based on what is possible and realistic and instead spends to satisfy perceptions of need (often indistinguishable from desires) with too little consideration of constraints and tradeoffs. While most Americans want a military that is prepared to prevail in combat, we all must take account of the resources available to make that a reality, both now and into the future. 



Beyond this overarching problem of ends misaligned to means, the Pentagon budgeting process is afflicted by two other related pathologies: overseas contingency operations funds and reprogramming. Both allow the government to spend without consequence and fail to distinguish between needs and wants.



Supplemental appropriations to pay for wars are not a novel idea. In fact, the first was passed in 1818. Historically, however, legislators moved such “emergency” spending (today known as “nonbase nonrecurring” or “contingency” funding) for unforeseen operations back to the base military budget within a few years once leaders had a clearer idea of operational needs.31



The DOD has received $2 trillion in overseas contingency operations (OCO) funding since September 11, 2001.32 In December 2019, Congress appropriated $71.5 billion for the OCO budget in FY 20.33 To put these numbers in perspective, in 2020, if OCO were its own government agency, it would have the fourth largest budget in terms of discretionary spending.34 The use of OCO funding for almost two decades following 9/11 has systematically undermined the established appropriations process. Supplemental appropriations fund activities unrelated to the wars but are not counted as part of the base DOD budget. In other words, reliance on OCO funding lets the military services avoid setting priorities that should guide long‐​term strategy and makes it too easy to undertake present‐​day combat operations without formal legislative consent and funding. Other departments and agencies have also gotten into the habit; even the U.S. Agency for International Development and the Department of State now rely on OCO funding to supplement their base budgets.35 Aside from its blatant dishonesty, OCO represents a larger pattern of runaway U.S. government spending and especially the legislative branch’s tendency to avoid oversight of either Pentagon spending or the nation’s perpetual conflicts. 



The other factor fueling the abuse of OCO funding is the Budget Control Act (BCA) of 2011. That legislation set limits on discretionary budget authority from 2012 through 2021 to slow the growth of public debt after the 2008 financial crisis.36 The spending limits are supposed to be enforced through what is commonly called sequestration. Under sequestration, any appropriations that go above set funding levels—or “caps”—are canceled.37 However, funding designated for OCO, ostensibly for counterterrorism efforts, including the wars in Afghanistan, Iraq, and Syria, is exempt from BCA caps and separate from the Pentagon’s base budget (hence “nonbase”).38 In other words, executive branch officials and legislators have a massive loophole for expanding military spending while seeming to abide by discretionary spending limits. The BCA’s OCO exemption allows elected officials to feign concern about out‐​of‐​control federal spending without doing anything to stop it.



Force development and planning requires funding that is on‐​time, stable, and proportional to the scope of military operations. Using OCO to skirt the BCA’s budget caps does not reflect a military establishment that can prioritize according to coherent long‐​term strategies. This technique has allowed civilian leaders to evade tough choices, including how to resolve ongoing conflicts and whether to enter new ones. While the war on terror presents unique challenges, using OCO only helps perpetuate the cycle of U.S. involvement in never‐​ending conflicts.



With BCA caps officially expiring in 2021, policymakers may be less tempted to rely on OCO funding. However, moving OCO back into the base budget inevitably raises concern about overall spending increasing at an unreasonable rate. Congress should move enduring costs back to the base budget without increasing topline military spending. Presenting the DOD with less budget flexibility should spur more creativity and budget management, not less, while still allowing the military to rely on supplemental funding for truly dire, unforeseen overseas expenses. When such emergencies arise, Congress can authorize additional funds as necessary.



Agencies often reprogram funds to deal with unforeseen challenges, but it is technically illegal to spend taxpayer dollars in ways not explicitly authorized by Congress.39 However, “as there are no government‐​wide reprogramming rules,” note Georgetown University researchers Michelle Mrdeza and Kenneth Gold, “prohibitions against reprogramming funds within an appropriations account … vary among agencies and appropriations subcommittees.”40 The Government Accountability Office (GAO) concurs. Agencies have the “implicit” authority to shift funds within a department or agency as long as the intended use of the funds remains broadly within the same goal.41 Regulations governing DOD require congressional approval for any funds reprogrammed over 20 percent or $20 million over the original allocation.



Reprogramming has become a national security issue as the executive branch seeks ways to seize control of the budget from Congress. The Trump administration’s threat to use funds allocated for other purposes to build the border wall, for example, contributed to the longest government shutdown in history in winter 2018–2019.42 Although the DOD continued operations, the budget impasse adversely affected many contractors, researchers, and production line managers.43



Past NDAAs restricted reprogramming funds for priorities that Congress expressly declined to fund, but the FY 20 NDAA did not include such language. A loophole in U.S. law allows for unassigned military construction funds to be used for construction projects during periods of national emergency.44 Other legislative language allows the secretary of defense to provide support for counterdrug activities to other departments and agencies.45 These two provisions provide the leeway to reprogram a significant amount of funding. Yet, the president can declare almost anything a “national emergency” at will.46



Thus, the moves to reprogram funds defy Congress’s traditional power of the purse and allow federal agencies to use money from the DOD budget to support domestic political initiatives. Such efforts create a dangerous precedent, both in undermining constitutional checks and balances and potentially limiting the funds vital to the nation’s defense.



These budgetary pathologies insulate the U.S. military from resource constraints, allowing it to proceed mostly by inertia. But the U.S. military also remains mired in the post‐​9/​11 Global War on Terror. Nearly two decades of continuous operations have put enormous strain on the force. The military branches continue to lower eligibility requirements to meet their recruitment goals and have increased retention bonuses to discourage services members from leaving.47



More ominous developments include rising suicide rates among veterans and active‐​duty service members, an increase in reported sexual assaults, and the need for expanded counseling to deal with post‐​traumatic stress disorder and other psychological challenges.48 In short, the well‐​being of U.S. service members is a pressing national concern.



The force of the future is likely to be smaller, particularly in terms of numbers of personnel in uniform, and thus will need to be more adaptable. That, in turn, will require increasing the academic aptitude and physical fitness standards for recruits.49 A focus on improving the force—as opposed to simply growing it—through retention programs for critical staff and expanded educational and retraining opportunities is key to creating a healthy and socially viable military. This should be a DOD‐​wide imperative.



Beyond recruitment and retention, each service branch confronts its own unique challenges. Pentagon officials must reconceptualize how the U.S. military plans to fight. The wars of the recent past, against chiefly nonstate actors in the greater Middle East, South Asia, and Sub‐​Saharan Africa, are unlikely to be an adequate guide for future conflicts.



In particular, the potential for direct engagement with technologically capable adversaries in contested environments means that the era of U.S. dominance can no longer be assumed. Within that framework, the following sections outline a few key choices that service leaders need to make.



There are two clear challenges for the joint forces of the United States: standardizing a system for operations across multiple domains (e.g., land, sea, air, space, and cyber) and pushing innovation. Addressing the first challenge demands that every branch of the U.S. military agrees to a joint, all‐​domain command and control (C2) system. As combat systems and advanced artificial intelligence (AI) platforms continue to develop, they must be seamlessly integrated within and between all U.S. forces. Currently, however, each military branch is pursuing its own C2 design. For example, the Navy has the Naval Integrated Fire Control‐​Counter Air system, and the Air Force has the Advanced Battle Management System.50 This duplicates effort, wastes funds, and impedes unifying C2.51



Defense contractors and other interested parties will lobby for their respective systems, but the choice should be based on the ability to implement the system across all services, agreement among the branches, and a clear standard for cybersecurity. Because a standard C2 platform is the optimal solution for the modern battlefield, all U.S. forces should streamline and upgrade to ensure that they meet the new compatibility standards. The U.S. military should not move forward with designing protections for these networks, and redundancy for forward C2 deployment, without first establishing a joint system. It is premature to estimate the eventual cost of such a unified system, but deciding on this system now will inevitably save money by facilitating coordination between every branch of the U.S. military.



The DOD, for its part, must decide on a platform, take bids on delivery of the system, and obtain executive branch and congressional approval on a process and timeline for implementation. Congress should use its legislative authority to ensure compliance through reporting requirements.52



The second overarching challenge for the U.S. military is the need to prioritize innovation. This entails empowering individuals at all levels to bring forward new ideas and establishing a process to deliver design options through a full development cycle in the most expeditious and cost‐​effective ways. Service members have a critical role to play in determining future priorities since these systems and platforms will have a direct impact on their daily lives and their ability to function on the battlefield.



Following the Army’s example, each branch of the military should develop its own Futures Command to push for branch‐​wide innovation. The mission of a Futures Command is modernization. It does away with old “industrial age” approaches, which are mostly piecemeal and often slowed by bureaucracy, and puts them all under one roof with a set of defined goals. If each branch has its own innovation command center, the Pentagon would be well‐​positioned to coordinate across branches. A futures reserve unit in each branch would prove critical given the recent effort to recruit and fund PhDs in the military and DOD.53 Members of the armed forces with advanced degrees could then naturally transition into the reserve system to support innovation.



The service branches should also develop practices for curating the massive amounts of data generated for AI systems. Given the high probability that this technology will be critical to future fights, branches should use “data wranglers”—individuals whose primary task is to collect information that can be plugged into various systems.54 There is currently no method to identify U.S. service members able to work with data, generate statistical analysis, and assure the accuracy of data.



In addition, Kessel Run‐​type programs in each branch could be successful for fostering innovation, as it has been for the Air Force. In the _Star Wars_ universe, the Kessel Run refers to an impossible task that is completed in a short time. The Air Force had that in mind when it set out to develop software quickly and in response to an uncertain environment.55 The inability to negotiate contracts with external parties who will build software or hardware in a timely and efficient manner are typically the main impediments to developing innovative programs in the military.



U.S. defense planners should consider what the nation’s defense needs will be in the future, but too often their efforts are stymied by inertia or shortsighted demands that defense programs serve domestic political and economic interests. The United States should be investing in innovation and research rather than stale production lines for weapons that have outlived their usefulness or new weapons that can never meet their design objectives.56 Developing weapons platforms should be based on the needs of the future military, not short‐​term concerns, such as the parochial interests of defense‐​industry workers or the politicians who shield them. The U.S. military must abandon weapons platforms that cost too much to maintain and retrofit and have limited or no value in future conflicts.



Future increases to the DOD’s research and development (R&D) budget should be funded by reducing spending on outdated weapons systems. As part of a renewed push for R&D, the U.S. government should revisit its approach to basic research funding. Instead of bolstering the National Science Foundation and encouraging scholars to seek trivial connections to national security in research projects, the DOD should be granted additional authority to invest in other public and private research startups and incubators through the individual service research offices (e.g., the Office of Naval Research). These funds should not be restricted and should be open to every research university and think tank capable of doing advanced research that will help drive innovation within the defense ecosystem.



This is not an argument for expanding federal funding for research but rather extending existing research opportunities to a much wider pool of qualified institutions. For too long the U.S. government has steered research funding to federally funded R&D centers. This has driven up R&D costs while failing to integrate the talent and ingenuity of research institutions outside traditional networks. The United States must leverage its deep technological base to meet coming challenges; as of now the U.S. government’s vision of research and research funding is tied to past processes that have a decidedly mixed record of delivering essential equipment and materials in a timely and cost‐​effective manner.



Research should be focused on applying novel technical capabilities to the modern battlefield. The idea that the United States has fallen behind China in the AI arms race is only true based on a measurement of research quantity, not quality. And such claims do not take full account of the vast array of innovative enterprises in the United States, most of which are completely outside the federal government’s control or purview.



For example, Google recently published a paper demonstrating quantum supremacy, when a quantum device (such as a quantum computer) can solve a problem that no traditional computer realistically can.57 This represents a leap over classical computing power by orders of magnitude, but U.S. defense planners must think about how to employ these tools in combat. AI is only as useful as the data fed into the algorithms.58 Moving forward with a clear vision of how the U.S. military can leverage AI and quantum power, therefore, requires investments in basic data science education, data assurance and retention, and data integrity.



These proposals are generally cost‐​neutral as they entail reorganization of existing lines of effort. Ensuring that the U.S. military develops multidomain battle systems without redundancy, establishing a clear process for managing data on the battlefield, and putting platform development in the hands of the individual soldier, sailor, airman, and Marine are all clear needs as relevant as massive outlays for modern weapons platforms. Reorganizing around Futures Command groups and using data wranglers would enable all service branches to innovate as the United States still enjoys a number of political, economic, and strategic advantages relative to prospective rivals.



Since its formal inception in 1947, the Air Force has fended off challenges to its place in the structure of the U.S. military, and a few respected scholars still call for its abolition.59 Many critics, however, aim to fix apparent inefficiencies within the force rather than doing away with it. A recent Center for Strategic and International Studies report, for example, notes that while spending on the Air Force has reached new heights, its force capabilities—as measured by the number of aircraft in its inventory—have fallen to an all‐​time low.60 This is partly explained by the overall focus on quality over quantity but is also due to the fact that the Air Force is more than just planes, just as the Army is more than the infantry and the Navy is more than surface ships. Still, the Air Force has struggled to introduce new aircraft. The service’s experience with the F-35 Lightning II aircraft, a fifth‐​generation fighter jet that is significantly more advanced than its predecessors and supposed to replace several other aircraft currently in service, has not been promising. In general, the Air Force has spent a lot of money to get less capacity.



A change of direction is in order. The structure and capabilities of the Air Force should maximize operational readiness, taking into consideration procurement difficulties associated with current weapons systems still under production.61 The bitter experience with the F-35, which will be delivered to the force nearly a decade late and at an inflation‐​adjusted cost well above original estimates, is only one sign of the overall challenge facing the Air Force.62 The service needs capable aircraft at a cost that will allow it to purchase them in adequate quantities, and it needs to obtain them in a timely fashion.



Per the objectives spelled out in the 2018 _National Defense Strategy_ (NDS), the U.S. Air Force is tasked with dominating the air, outer space, and cyberspace by using advanced and emerging technology. The Air Force needs to be an innovative service to keep up with the rapid pace of technical change. Specifically, the service should focus on countering China and Russia’s investments in anti‐​access/​area‐​denial systems, including long‐​range surface‐​to‐​air missiles.63



This will be difficult. As previously noted, the Air Force’s rising budgets have coincided with a declining number of active aircraft, along with fewer pilots and Air Force civilian employees.64 Such trends signal broader challenges with basic budgetary management, including the expanding costs of operation and maintenance. In other words, today’s Air Force paradoxically does less while spending more. This is perplexing to say the least.



While the service has emphasized incorporating advanced technology for air and space operations, overall readiness and pilot training have decreased substantially, contributing to a steady rise in aircraft mishaps.65 These operational problems are exacerbated by a shortage of qualified maintenance technicians. According to the GAO, the Air Force does not have a strategy to improve retention. If the Air Force is unable to hold onto its best people, it will struggle to adapt to changing operating environments (including outer space and cyberspace) and new technology (such as AI and quantum computing).66 The Air Force must undertake a service‐​wide initiative to reverse this trend, especially by incentivizing qualified personnel to remain in the force.



With respect to hardware, the Air Force is developing the F-35A, the B-21 Raider long‐​range bomber, and the KC-46A Pegasus tanker aircraft while also seeking to replace current intercontinental ballistic missiles and developing a Space Force, which is still officially under the Air Force’s auspices. That is unsustainable. The service’s goals must be aligned to present and future realities and should take account of the demands of modern combat. As the airspace in which the Air Force operates becomes increasingly crowded and contested, this places a premium on unmanned vehicles that can loiter and are capable of executing strike, surveillance, and resupply missions.



Forward basing poses both operational and doctrinal challenges to air operations because long‐​range precision strikes by an adversary can decimate aircraft and fuel supplies long before U.S. aircraft can engage the target. What good is a force of 100 F‐​35s if they never leave the ground?



A focus for now on drones and a reliance on a revitalized F-15 Eagle aircraft through the F-15EX platform is certainly warranted. The recent move to establish the 16th Air Force, which is focused on cyberspace and electronic warfare, is also a welcome development.67 On the whole, however, the Air Force is trying to do too much, including a focus on space, support for counterterror operations, unmanned reconnaissance, nuclear deterrence, transport, air defense, air‐​to‐​air combat, ballistic missiles, and precision bombing. A strategic pause and reset are desperately needed.



The Army’s strategy, posture, and budget should reflect and adapt to evolving geopolitical circumstances. The U.S. Army posture assessment fails to do that, placing dominance through military overmatch, as outlined in the NDS, at the forefront of the Army’s vision.68 Day‐​to‐​day operations, ongoing conflicts, allied engagement, and crisis response all continue to put unnecessarily high demands on the force. A realistic assessment of threats would allow the Army to prioritize and eliminate or offload unnecessary missions. Enabling and encouraging allies to do more in their respective regions would reduce the Army’s requirements, including especially numbers of active‐​duty personnel.



In 2018, the Army created the Army Futures Command.69 This organization has been critical for pushing the service to modernize. It originally established six priorities:



Of these, long‐​range precision fires (i.e., modern artillery) and networked air and missile defense are critical. The United States should divest from other outdated weapons systems—including, in particular, the Abrams tank—that are unlikely to serve a major purpose on the future battlefield, or at least in the battlefields that are truly critical to U.S. security and prosperity.



Above all, the active‐​duty U.S. Army should be substantially smaller and postured mostly for hemispheric defense. A grand strategy of restraint would eliminate most permanent garrisons on foreign soil and rely more heavily on reservists and National Guard personnel for missions closer to the U.S. homeland. Such a posture would reduce the likelihood that U.S. troops would be drawn into protracted civil conflicts that do not engage core U.S. national security interests. That, in turn, would generate substantial savings over the next decade.



Developing better and modern versions of artillery is another key task for the Army. That would allow the U.S. military to support allies from a distance, when U.S. leaders deem such assistance appropriate, while also ensuring that U.S. troops mostly remain out of harm’s way when such missions are not truly essential for U.S. security.



The development of better unmanned vehicles for long‐​range fires in support of ground operations is also critical. While drones for surveillance and precision strikes are useful, in a future war the United States will need functional unmanned vehicles that can deliver artillery support and fire weapons from a distance, minimizing harm to U.S. forces. Future platforms used to deliver long‐​range fires also need the ability to be undetected despite increased sensors employed by adversaries.



Finally, the Army needs to develop better air and missile defensive platforms to protect forward‐​operating units. These tools would benefit the entire U.S. military, but the greatest gain would go to the Army, whose ability to fight will be challenged by opponents’ long‐​range munitions. The Army needs portable sensors ready to detect incoming fires. A modern military is too vulnerable to long‐​range attack, including from artillery, ballistic missiles, and drones. Real‐​time battlefield awareness is essential, as is the need to defend our allies once the U.S. commits to pulling back from forward deployment. Thinking about this critical function is more important than developing a new helicopter or other vertical lift platform (e.g., tilt‐​rotor aircraft) or a next‐​generation tank. If the U.S. military cannot protect its forces in the field from short‐​range ballistic and cruise missiles, units will not survive long enough to bring these new weapons to bear against the enemy.



To meet current recruitment goals, the Army has waived certain requirements and increased enlistment bonuses.71 If these reforms draw capable people into the service, then they should continue, but careful oversight is needed. An emphasis on quality, rather than quantity, could reduce turnover, ensure new enlistees complete their requisite training, and ultimately improve retention.



A focus on readiness could also help. Service members should know that they have adequate support to complete their missions and be confident that policymakers will not send them to fight open‐​ended wars that are not vital to U.S. national security. A failure to meet those basic requirements has driven qualified personnel from the force. No branch of the U.S. military has reached its readiness goals, however, and the budget priority has since shifted to modernization. While the increase in research, development, testing, and evaluation is an important step in creating a more lethal and agile force, a failure to meet readiness goals will impede force transformation.



The Army needs to rethink the size of the force needed given the effort to modernize overall. At a time when two successive presidential administrations have pledged to draw down operations in the greater Middle East, the United States should refocus on establishing a lean and agile ground force that can retain the best people while allowing the marginal performers to transition out. This process of attrition should be used to reduce the size of the active‐​duty Army by 20 percent over the next decade. Recruiters need to employ what marketers call “microtargeting” to ensure that the U.S. Army has high‐​quality soldiers that can innovate on the battlefield, not just follow orders.72 Eliminating unnecessary forward bases, improving existing facilities, and rethinking education and training would be easier with reductions in the size of the force.



In recent years, the U.S. Navy has operated under the assumption that it can get all that it wants without a clear articulation of what it needs—though the situation may be changing. An October 2019 Congressional Budget Office (CBO) report warned that the Navy “would not be able to afford its 2020 shipbuilding plan.” CBO estimated that the Navy would need $28.8 billion per year for new‐​ship construction, more than double the historical average of $13.8 billion per year (in 2019 dollars).73 This is hardly the first time that CBO has observed the looming gap between the Navy’s plans and fiscal realities.74 Although the sea service has avoided a bitter reckoning, the responsible course would bring its goals in line with its available resources.



In early December 2019, Acting Navy Secretary Thomas Modly publicly reaffirmed his commitment to achieving a 355‐​ship Navy, and he separately issued a memo to the fleet calling for a plan to achieve it by the end of the next decade.75 But more recent evidence suggests that the Trump administration has scaled back its shipbuilding plans and backed away from the 355‐​ship goal. The president’s budget submission for FY 21 actually cut $4.1 billion from shipbuilding.76 Navy leaders acknowledge the tradeoffs between operations and maintenance and money for new construction. “We definitely want to have a bigger Navy, but we definitely don’t want to have a hollow Navy either,” Modly told _Defense News_. “If you are growing the force by 25 to 30 percent, that includes people that have to man them. It requires maintenance. It requires operational costs. And you can’t do that if your top line is basically flat.”77



Many strategy documents simply assume that considerably more money _must_ be made available to the military—and leave it to the politicians to figure out how.78 The Heritage Foundation, for example, calls for a 400‐​ship Navy even as it concedes that such a force “may be difficult to achieve based on current DOD fiscal constraints and the present capacity of the shipbuilding industrial base.”79



The Navy should reject such advice, prioritize among competing desires, and focus on what is genuinely needed to achieve vital national security objectives. In the near term, this means prioritizing current operations. High‐​profile disasters at sea, including the tragic accidents aboard USS _John S. McCain_ and USS _Fitzgerald_ , which claimed 17 sailors’ lives in 2017, raised obvious questions about the state of the surface Navy. A GAO report released two years before the _McCain_ and _Fitzgerald_ incidents concluded that “the high pace of operations the Navy uses for overseas‐​homeported ships limits dedicated training and maintenance periods,” which had “resulted in difficulty keeping crews fully trained and ships maintained.”80



The Navy must expand both its capacities and capabilities. Prioritizing less‐​expensive vessels could make up for certain shortfalls and grow the fleet at a faster rate. Newer platforms would also translate to less maintenance time, further increasing the number of vessels ready for service at any given time. On occasion, the Navy has gone in a different direction, privileging very high‐​end platforms that often take many years to reach the fleet. In the interim, this leaves more older ships in service longer, along with their additional repair and maintenance costs.



The Navy has made recapitalizing the ballistic missile submarine (SSBN) fleet—the _Columbia_ -class SSBNs that will replace the _Ohio_ -class—its top shipbuilding priority. The tradeoffs are most apparent with respect to fast‐​attack submarines (SSNs).81 Although these vessels are unsuited to perform many routine Navy missions—including escort operations and visible presence—they are critical and should be maintained in some quantity.



Other hard choices cannot simply be imagined away. This report focuses on two key acquisitions programs to highlight tradeoffs within the surface fleet: the _Gerald R. Ford_ -class aircraft carrier (CVN) and the new guided‐​missile frigate FFG(X).82



As designed, _Ford_ -class ships are the largest and most capable warships on the planet. But little else about the ships—including whether their actual performance matches their designed capabilities or when the ships will attain full operability—can be predicted with any confidence. Former Navy Secretary Richard Spencer staked his reputation on ensuring that the advanced weapons elevators—large lifts that transport bombs and missiles from inside the ship to the flight deck—aboard USS _Gerald R. Ford_ (CVN-78) would all work before the ship set out for trials. They didn’t—only 4 of 11 were operational by the end of October 2019.83



Three other critical technologies—the ship’s new electromagnetic aircraft launching system, an advanced arresting gear used to recover aircraft on deck safely, and a dual band radar—have also failed to meet the service’s expectations.84 A December 2018 report by DOD’s director of operational test and evaluation (DOT&E) identified a host of concerns, ranging from “poor or unknown reliability of systems critical for flight operations” to inadequate crew berthing.85



Most damning, perhaps, were the DOT&E’s conclusions pertaining to the ship’s core mission: the ability to launch and recover aircraft at high tempo and over extended periods (sorties, in Navy jargon). The report warned, “Poor reliability of key systems … on CVN 78 could cause a cascading series of delays during flight operations that would affect CVN 78’s ability to generate sorties.”86 In the end, DOT&E concluded that the Navy’s sortie generation requirements for the _Ford_ were based on “unrealistic assumptions.”87



Other critics fault a systemic lack of accountability throughout the Navy. Industry analyst Craig Hooper wrote in October 2019, “The naval enterprise struggles to bring bad news to the higher levels of the chain of command. It is a habit that perpetuates something of a complacent ‘not my problem’ or career‐​protecting sluggishness in the face of avoidable disaster.” This has ramifications that go well beyond catapults and arresting gear.88



As difficult as the design and development process for the Navy’s capital ship has been, however, even tougher questions swirl around the employment of these massive platforms. In an era of defense dominance, when adversaries can use relatively cheap but accurate weapons to attack large and exquisite platforms, how will the carriers perform? Not well, according to some knowledgeable critics, including retired Navy Capt. Henry J. Hendrix, who in 2013 warned, “The queen of the American fleet is in danger of becoming like the battleships it was originally designed to support: big, expensive, vulnerable—and surprisingly irrelevant to the conflicts of the time.” The national security establishment, he concluded, had ignored “clear evidence that the carrier equipped with manned strike aircraft is an increasingly expensive way to deliver firepower” and that the ships might struggle “to operate effectively or survive in an era of satellite imagery and long‐​range precision strike missiles.”89



National Defense University’s T. X. Hammes imagines an even more dramatic transformation that would merge “old technologies with new to provide similar capability at a fraction of the cost.” Specifically, Hammes proposes using container ships loaded with hundreds or thousands of drones and cruise missiles—but very few people—to eventually take the place of the iconic flattops hurling and recovering manned aircraft. “Flying drones,” Hammes writes, “can provide long‐​range strike, surveillance, communications relay, and electronic warfare” and can be launched and recovered vertically. Cruise missiles deployed in standard shipping containers, meanwhile, could effectively convert “any container ship—from inter‐​coastal to ocean‐​going” into “a potential aircraft carrier.”90



For now, Congress has conspired to thwart any fundamental reconsideration of the centrality of the aircraft carrier to the modern surface fleet. The 11‐​carrier legislative mandate remains despite serious concerns about the _Ford_ ’s timeline and even as “the Navy is finding it increasingly difficult to deploy carriers and keep them on station.”91 A reckoning has been postponed but cannot be avoided forever.



According to the _Force Structure Assessment_ issued in December 2016, the Navy seeks to procure 52 small surface ships, 20 of which are to be a new class of guided‐​missile frigates, the FFG(X).92 Some analysts contended that reactivating the _Oliver Hazard Perry_ -class frigates, the last of which was retired in 2015, would help the Navy achieve its force structure goals faster, but the decision to commission new vessels signaled the Navy leadership’s commitment to modernization.93



The Navy requested $1.28 billion in its FY 20 budget to procure the first FFG(X), awarding conceptual design contracts to five different companies.94 Despite the purported reduction in scheduling, risk, and price with the Navy’s approach to the FFG(X), the CBO predicted in October 2019 that the total cost of the 20‐​ship program will be closer to $23 billion than the Navy’s estimated $17 billion.95



Although the House and Senate fulfilled the administration’s request for $1.28 billion in procurement, plus another $59 million for research and development in the FY 20 NDAA, doubts remain about this program’s ability to fill the capability gaps in the fleet.96 The key questions will revolve around unit cost and the length of the design, development, and build phases. Congress has put significant pressure on the Navy to implement cost‐​effective capabilities on realistic timelines. If the Navy is truly committed to expanding fleet capacity quickly, and with minimal risk, it is imperative that it hold the line against anything likely to lead to costly delays.



Before the Navy can decide what it needs, however, it must decide what it’s going to do. The Trump administration’s _National Security Strategy_ (NSS) and NDS would appear to be good news for the Navy. Both documents focus on the rise of peer or near‐​peer competitors, chiefly China and Russia, with reference also to regional rivals such as North Korea and Iran. These types of adversaries would privilege the need for naval and air power over ground forces, which have been geared to fighting nonstate actors and insurgents over the past two decades.



The U.S. Navy has an extraordinarily ambitious set of objectives, and the demands placed on the service already exceed its ability to meet them. These demands mostly originate with the various regional combatant commands and further reflect a long‐​standing assumption that the Navy’s forward presence is essential to global security. The Heritage Foundation’s _Index of U.S. Military Strength_ , for example, argues that “the Navy must maintain a global forward presence both to deter potential aggressors from conflict and to assure our allies and maritime partners that the nation remains committed to defending its national security interests and alliances.”97



What the Heritage Foundation casts as a requirement is a choice. Strategic requirements are not handed down from on high but reflect the dominant strategic paradigm. A commitment to maintaining the free movement of raw materials, essential commodities, and finished goods was a core mission for the U.S. Navy during the Cold War and was driven by a concern that a globe‐​straddling Soviet Navy was both motivated to close—and capable of closing—critical sea lanes of communication and maritime choke points.98



Today, the situation is much different. Most international actors, including even modern rivals such as Russia and China, depend on the free flow of maritime trade and are therefore highly incentivized to try to keep these waterways open. For decades, however, U.S. allies and partners have neglected their own maritime forces, coming to rely on the U.S. Navy deploying small, surface combatants in their home waters. In effect, therefore, the U.S. Navy has been operating as a global coastal constabulary.



This practice should stop. U.S. policy should aim to encourage these nation‐​states to play a key role in securing access to vital sea‐​borne trade. The presumption that the U.S. military must be constantly on station, including in waters thousands of miles away from the Western Hemisphere, merits scrutiny, not least because the U.S. Navy alone cannot meet the demands of being a de facto coast guard for all other nations—nor is it in America’s interest to try.



Sea‐​lane control in the modern era aims to ensure the free flow of goods and is primarily defensive. The aim should be to prevent others from limiting access to the open oceans while not threatening to deny anyone else the peaceful use of those same seas. That mission can and should be shared with other countries, most of whom will be operating near their shores, and thus highly motivated—and able—to defend their sovereign waters.



Marine Corps Gen. David Berger’s appointment as the 38th commandant of the service was met with a question by a Marine Corps major: “Sir, who am I?”99 With a founding mission of being able to carry out contested amphibious operations, it is unclear today who the United States is preparing to invade, and how it would do so. Would the United States deploy landing craft like those used on Normandy beaches in 1944 or at Inchon Korea in 1950 during an age of highly sophisticated surface‐​to‐​surface missiles? Does the U.S. military have functional aviation or naval vehicles that can support large modern amphibious invasions?



The response to these sorts of questions was dramatic and forceful. Berger’s _Commandant’s Planning Guidance_ (CPG) sought to kill many sacred cows and institute a new path forward where the Marines would focus on sea denial, interoperability with the Navy, and wargaming to understand current and future combat options.100 The CPG stated, without evocation, “the Marine Corps will be trained and equipped as a naval expeditionary force‐​in‐​readiness and prepared to operate inside actively contested maritime spaces in support of fleet operations.”101



The Marine Corps’ decision not to request amphibious platforms in the 2020 budget was formalized in the CPG, which called amphibious operations “impractical and unreasonable.” Such conclusions recognize the need for a swift, agile force that can operate in forward positions without the resources and protection of the core force.



While a future great power war in the Asia‐​Pacific is possible, the probability of a near‐​term conflict is very low; this supports the decision to move the Marine Corps from a focus on amphibious operations. In fact, recent reports note that China’s navy is rethinking its spending plans given the economic uncertainty brought on by the trade war with the United States.102 China is not a peer competitor; its grandiose naval ambitions remain unfulfilled, as massive investment would be needed for surface ships, landing craft, advanced weapons platforms, aircraft, and personnel—all when the demands of an expanding middle class are increasingly going unmet. And those domestic challenges all preceded the COVID-19 pandemic that began in late 2019 that has wreaked havoc on China’s economy.



As Marine Corps planners recognize, there is a great need for large numbers of cheap autonomous naval systems that can overwhelm the enemy, and these are preferable to expensive and manned systems.103 Berger stated, “I see potential in the ‘Lightning Carrier’ concept … however, [I] do not support a new‐​build CVL [light aircraft carrier].”104 The CPG suggests a possible focus on high‐​mobility artillery systems to deny sea access and landing routes.



The Navy and Marine Corps should not be pushing new amphibious platforms when they are unable to maintain their current craft in a steady state of readiness.105 If the Marines are truly the “first to fight,” they need to focus on modernization, rework force structure for quality over quantity, and reset their priorities after years of focus on the Global War on Terror. Senior leaders in the Marine Corps have the correct vision, but implementing their plans within a change‐​resistant bureaucracy will be a challenge.



The United States has failed to undertake a much‐​needed reevaluation of its approach to strategic deterrence. The nuclear triad, the array of land‐, air‐, and sea‐​based capabilities that can deliver nuclear weapons to targets, has been a fixture since the early Cold War. Since then the triad has become dogma. A reexamination of its value considering technological developments, advances in intelligence, surveillance, and reconnaissance, and changes in adversary capabilities is overdue.



That hasn’t occurred under the Trump administration, which seems to be settling on a kitchen‐​sink approach to solving the country’s alleged “deterrence gaps” vis‐​à‐​vis other great powers.106 Its 2018 _Nuclear Posture Review_ retains the triad and adds two new capabilities—a low‐​yield warhead for the Trident (the nuclear‐​armed ballistic missile carried by U.S. submarines) and a new nuclear sea‐​launched cruise missile—to the Obama administration’s nuclear modernization plan. A 2017 report from the CBO estimated that this plan would cost roughly $1.2 trillion over 30 years.107 That 30‐​year estimate is likely to increase as programs face unforeseen problems and delays. The United States is also trying to improve its capabilities for defeating ballistic and cruise missile threats to both forward‐​deployed forces and the American homeland.108



These investments in nuclear weapons and missile defense demonstrate that strategic deterrence remains central to U.S. strategy, but is the United States making the right policy choices? What are the threats the United States wants to deter, and can nuclear weapons and missile defense help mitigate them? Raising these questions reveals that some elements of the nuclear modernization plan are superfluous and that some missile defense choices are likely to push rivals to develop destabilizing counterstrategies.



Most of the nuclear modernization plan’s spending will fund new delivery platforms—aircraft, submarines, and missiles—with some money going toward updated nuclear warheads. The plan is not meant to expand the arsenal; as new systems get introduced, old ones will be phased out.



Supporters of the nuclear modernization plan claim that it will only eat up a small portion of overall military spending. That is true given the very high topline for the budget, but this does not imply that nuclear modernization will be cheap and easy. Initial cost estimates are already growing. For example, recent delays in the B61-12 nuclear gravity bomb life extension program (LEP) will add an extra $600–$700 million, and the W80-4 nuclear warhead LEP’s estimated project cost had increased from $9.4 billion in November 2017 to $12 billion by summer 2019.109 Delivery platforms are also prone to cost overruns. The B-2 Spirit bomber program (which wildly overran its initial cost projections) offers a cautionary tale for its secretive and expensive successor, the B-21 Raider.110



Before developing new nuclear capabilities, we need to decide whether they are necessary for strategic deterrence. Arguments about the relatively low price of systems are hardly compelling if the United States does not need to buy them in the first place. The B61-12 gravity bomb, for example, is superfluous given U.S. efforts to develop an air‐​launched cruise missile that could hold the same targets at risk from long distance.111 The decision to deploy a low‐​yield tactical warhead for the Trident missile rests on faulty understandings of Russian nuclear strategy.112 Similarly, the United States should eliminate the nuclear mission for the F-35, cut the purchase of new intercontinental ballistic missiles in half, and delay procurement of the B-21 for 10 years.113



Increased spending on strategically dubious capabilities also extends to missile defense. The _2019 Missile Defense Review_ calls for a wide‐​ranging expansion of missile defense capabilities to counter both rogue states and great powers.114 That includes expanding the stock of existing interceptors and developing new technology to counter offensive capabilities that U.S. adversaries have fielded to defeat existing U.S. defenses.115 Rather than enhancing strategic deterrence, America’s missile defense posture is encouraging adversaries to develop new offensive platforms that increase the risk of conventional conflicts going nuclear.



Adjusting American grand strategy toward restraint would mandate a different approach to strategic deterrence. Modernizing the U.S. nuclear arsenal is important, but the pursuit of maximum flexibility to deter an amorphous set of strategic threats will waste billions of dollars on capabilities the United States doesn’t need. The primary goal of strategic deterrence, preventing nuclear first use against America and its allies, would remain the same under restraint. Instead of pursuing flexibility to respond to a wide variety of threats, the three pillars of strategic deterrence under restraint are removal of peripheral threats through diplomacy; shifting a greater defense burden to allies; and adopting a conventional military posture that enables deterrence by denial—discouraging enemy action by denying a quick and easy victory.



Greater reliance on diplomacy could contain or remove potential threats that current U.S. military doctrine casts as strategic imperatives. For example, the Joint Comprehensive Plan of Action with Iran allowed the United States to reduce nuclear proliferation risks through diplomacy.116 The case also illustrates the negative consequences of abandoning diplomacy. Since the Trump administration’s withdrawal from the agreement, the region has witnessed a constant tit‐​for‐​tat escalation of tensions.117 Arms control agreements with other great powers such as China and Russia are another important feature of restraint’s approach to strategic deterrence. Arms control measures can help set guardrails on the most dangerous aspects of great power competition, allowing for a degree of strategic trust and stability that is important for averting nuclear disaster.



Another key component of a redesigned U.S. strategic deterrent would entail empowering allies to respond to the coercive activities of regional rivals. Given the stakes involved for all parties, the deterrent threats by local actors might prove more credible than those issued by a distant United States.118 Under restraint, regional disputes might prove less likely to escalate into great power conflict, and more capable local deterrent forces would help reduce—though not eliminate—demands on the U.S. military and U.S. taxpayers.



China and Russia demonstrate how effective asymmetric strategies—those that avoid matching an opponent’s capabilities but instead try to exploit weaknesses with other means—can frustrate an otherwise stronger foe that depends on power projection to achieve its interests.119 U.S. allies in East Asia, for example, don’t need to build a lot of expensive aircraft or ships to defend themselves from China’s growing air and naval forces. A mix of unmanned systems, long‐​range precision strike conventional weapons, and strong air defense could be an effective and affordable counter to Chinese power. Encouraging allies to develop their own asymmetric capabilities would empower them to contribute more to deterring regional conflicts. Gradually reducing the forward deployment of U.S. forces could facilitate this transition.120



The United States would still have an interest in deterring nuclear first use against its allies—or the use of nuclear weapons in any context. But stronger, more capable allies armed with conventional weapons, combined with a reduced forward‐​deployed U.S. military presence, would shorten the list of strategic threats that U.S. officials feel obliged to deter or eliminate.



The third pillar of a new U.S. strategic deterrence posture under restraint is a greater reliance on conventional weapons to deter other great powers. Instead of threatening an attacking country through punishment (damaging the attacker’s population and economy) this approach would depend on a concept known as deterrence by denial, which resists enemy action by denying a quick, easy military victory for the aggressor.121 Credibly increasing the costs of aggressive action would leverage U.S. advantages in sensors, regional missile defense, and conventional long‐​range precision strike to deter military action that U.S. allies are unable to address.122 Allies equipped with similar capabilities would further improve deterrence by denial.



Such an approach would reduce the risk of inadvertent nuclear escalation in conventional conflicts by focusing on defeating military units rather than engaging in deep strikes against an adversary’s command and control networks.123 Technical developments in both the United States and its potential great power adversaries have blurred the lines between conventional and nuclear forces. The military strategies adopted by the United States, China, and Russia that emphasize early, deep conventional strikes further increase the escalation risks.124



Under this new approach, nuclear weapons and homeland missile defense would play reduced roles. On the missile defense side, U.S. defense planners should pivot to improving regional systems and increasing the stock of associated interceptors while moving away from expanded homeland missile defense.126 That would make it harder for great power adversaries to both initiate and prevail in quick, limited conflicts. U.S. leaders would also face less pressure to rapidly escalate to conventional attacks against Chinese or Russian territory. The U.S. way of war emphasizes strikes against command and control facilities, some of which are located far behind a country’s borders. Such strikes could be interpreted as an attack on a country’s leadership or an effort to reduce the effectiveness of its nuclear forces. If U.S. forces could deflect an initial attack against land‐​based, anti‐​access/​area denial weapons such as surface‐​to‐​air and anti‐​ship missile batteries, it could reduce the incentive to target adversary command and control early in a conflict.



The United States should take advantage of a strategic pause, adopt a grand strategy of self‐​reliance and restraint, and develop a comprehensive plan for dealing with peer and near‐​peer competitors and rivals. For at least two decades, the U.S. military has been trapped in a cycle of small‐​scale wars and nation‐​building fiascos that have eroded America’s unique advantages. Reconstructing U.S. security, therefore, requires a conscious decision to remove U.S. forces from past conflicts, and a fundamental reconceptualization of how the United States will use its forces in the future. Security budgets need to view U.S. power along economic, diplomatic, and cultural dimensions. These alternatives are often more effective than force and can produce a positive lasting impact by creating a period of stability that endures and that can be sustained by many like‐​minded actors, not merely the U.S. military.



Diplomacy, for example, has grown stagnant, but the Trump administration seems determined to hasten its demise.127 President Trump has scaled back on many diplomatic initiatives, but the COVID-19 pandemic laid bare the shortcomings of the military‐​centric approach. The United States can divest some of its legacy military apparatus and focus on innovating for the future while also investing a small fraction of these funds to deal with a range of threats to public safety that are not amenable to military solutions. The U.S. government will almost certainly need to prepare for a role in coordinating supply and delivery of vital equipment in future disasters and pandemics. Our true strategic reserve is more than the manpower that the military can marshal and the expertise in delivery, logistics, and analysis that the military can offer. The capacity and the expertise of the American people is a strength that will see us through crises.



This report has outlined a plan for moving the United States toward a more sustainable national security posture predicated on restraint.128 The budgeting process and the design and development of new military systems are riddled with inefficiencies that have wasted time and money that could be put toward fixing the social and structural problems the military faces. Conventional forces should be modernized for future fights, not geared toward sustaining the war on terror. Finally, the United States needs a modern approach to strategic deterrence that places greater emphasis on denying the ability of other great powers to project offensive military forces by using conventional capabilities rather than the nuclear triad.



Security comes through prudence, not overwhelming force, permanent alliances, or massive investments in weapons platforms. Defending the United States requires a judicious application of the many instruments of American power, not reckless overseas military adventures that have cost too many lives and too much treasure. A clear consideration of U.S. capabilities, appreciation of our fortunate geopolitical situation, and confidence in our ability to address future challenges will allow the United States to build and maintain a leaner and more efficient military, one that is more than capable of defending U.S. vital interests and deterring attacks against the homeland.



 **Advanced Battle Management System (ABAMS):** the technical engine that would manage all communications, orders, and sensors used by the Air Force



 **Anti‐​access/​area‐​denial (A2/AD):** an operational concept that complicates an opponent’s ability to use air, naval, and land power at long distance; typically entails the use of land‐​based sensors and precision strike systems to target opponent ships, aircraft, and bases



 **Aircraft carriers (CVNs):** the largest ships in the U.S. Navy and the centerpiece of U.S. fleet operations, capable of carrying about 60 aircraft of varying types



 **Arresting gear:** mechanical system that rapidly decelerates aircraft when landing on a platform such as an aircraft carrier



 **Ballistic missile submarines (SSBNs):** the sea‐​based leg of the nuclear triad, these vessels carry Trident missiles, each capable of delivering up to eight nuclear warheads



 **Command and control (C2):** set of organizational and technical processes employed to accomplish missions



 **Dual band radar:** combines radar systems into one integrated system for easier operation, maintenance, upgrade, and targeting



 **Fast‐​attack submarines (SSN):** the U.S. Navy’s primary undersea platform, capable of both offensive action at sea or against targets on land; also used for intelligence gathering



 **Frigates (FFG):** mixed‐​armament warship lighter than a destroyer; typically focused on anti‐​ship and anti‐​submarine warfare



 **FFG(X):** class of future multimission guided‐​missile frigates



 **Integrated Fire Control‐​Counter Air System (NIFC-CA):** the Navy’s multidomain battle management system



 **Low‐​yield nuclear weapon:** a nuclear weapon with a relatively small explosive yield thought to be useful for limited nuclear operations on the battlefield or for controlling escalation



 **Maritime choke points (e.g., straits and narrows):** a heavily trafficked narrow waterway



 **Micro‐​targeting:** direct marketing methods utilizing datamining techniques to segment consumers by tastes or attributes



 **Nuclear Posture Review (NPR):** major policy document that sets out the nuclear strategy and policies of a new administration; typically includes overviews of the U.S. nuclear arsenal, arms control policy, and nuclear strategy broadly defined



 **Operational readiness:** capacity of a unit to perform its designated combat or combat support function



 **Smart power:** strategic use of both hard (military) and soft power (diplomacy and trade) to achieve foreign policy ends



Eric Gomez is director of defense policy studies; Christopher Preble is vice president for defense and foreign policy studies; Lauren Sander is external relations manager for defense and foreign policy studies; and Brandon Valeriano is a senior fellow at the Cato Institute. 



ShowHide

Endnotes



1 Samuel P. Huntington, “Why International Primacy Matters,” _International Security_ 17, no. 4 (Spring 1993): 83.



2 George Shultz, _American Umpire—Teaser_ , trailer for film by James Shelley and Elizabeth Cobbs, 2016, 0:47, https://​vimeo​.com/​1​4​6​7​22638.



3 Office of the Secretary of Defense, _Summary of the 2018 National Defense Strategy of the United States of America: Sharpening the American Military’s Competitive Edge_ (Washington: Department of Defense, 2018), pp. 2, 4. Emphasis in original.



4 Office of the President of the United States of America, _National Security Strategy of the United States of America_ (Washington: The White House, December 2017), p. 28.



5 National Defense Authorization Act for Fiscal Year 2020, S. 1790, 116th Cong. (2019).



6 Gordon Lubold and Nancy A. Youssef, “Trump Administration Considers 14,000 More Troops for Mideast,” _Wall Street Journal_ , December 4, 2019; and Jared Malsin, “U.S. Forces Return to Saudi Arabia to Deter Attacks by Iran,” _Wall Street Journal_ , February 26, 2020.



7 John Glaser and Christopher Preble, “High Anxiety: How Washington’s Exaggerated Sense of Danger Harms Us All,” Cato Institute Study, December 10, 2019.



8 Jennifer Lind and Daryl G. Press, “Reality Check: American Power in an Age of Constraints,” _Foreign Affairs_ 99, no. 2 (March/​April 2020): 41–48.



9 Office of Management and Budget (OMB), _A Budget for America’s Future_ (Washington: Government Publishing Office, February 2020), p. 34; and OMB, _A Budget for a Better America: Promises Kept. Taxpayers First._ (Washington: Government Publishing Office, March 2019), p. 23.



10 Office of the President, _National Security Strategy_ , p. 28.



11 Office of the Secretary of Defense, _2018 National Defense Strategy_ , p. 1.



12 Smart power is the strategic use of both hard (military) and soft power (diplomacy and trade) to achieve foreign policy ends.



13 John Glaser, Christopher A. Preble, and A. Trevor Thrall, _Fuel to the Fire: How Trump Made America’s Broken Foreign Policy Even Worse (and How We Can Recover)_ (Washington: Cato Institute, 2019), p. 181.



14 See, for example, Barry R. Posen, _Restraint: A New Foundation for U.S. Grand Strategy_ (Ithaca, NY: Cornell University Press, 2014); and A. Trevor Thrall and Benjamin Friedman, eds., _U.S. Grand Strategy in the 21st Century: The Case for Restraint_ (New York: Routledge, 2018).



15 Mark Hannah, “Stop Declaring War on a Virus,” _War on the Rocks_ , April 17, 2020.



16 Office of the Secretary of Defense, _Nuclear Posture Review_ (Washington: Department of Defense, February 2018), p. 21. The Nuclear Posture Review (NPR) is a major policy document that sets out the nuclear strategy and policies of a new administration. A typical NPR includes overviews of the U.S. nuclear arsenal, arms control policy, and nuclear strategy, broadly defined.



17 The 2018 NPR is explicit about the demand for flexibility in nuclear deterrence. It refers to this as “tailored deterrence.” See Office of the Secretary of Defense, _Nuclear Posture Review_ , pp. vii–viii.



18 Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 53.



19 Low‐​yield nuclear weapons are nuclear weapons with a relatively small explosive yield (a smaller amount of energy released when detonated) thought to be useful for limited nuclear operations on the battlefield or controlling escalation.



20 Office of the Secretary of Defense, _2019 Missile Defense Review_ (Washington: Department of Defense, January 2019), p. xvi.



21 Command and control refers to the actual efficacy of commanding officers and leaders to move and direct troops on the battlefield as well as the infrastructure that enables the commander to provide direction to their troops (i.e., radios and tactical operation centers providing intelligence and support).



22 Angus King, interview by Andrea Mitchell, _Andrea Mitchell Reports_ , MSNBC, May 23, 2019, https://​www​.youtube​.com/​w​a​t​c​h​?​v​=​S​s​f​a​I​B​w2f7A.



23 Michael O’Hanlon, “President Trump Might Be on to Something with Russia,” _USA Today_ , December 19, 2017.



24 Benjamin Denison, “Confusion in the Pivot: The Muddled Shift from Peripheral War to Great Power Competition,” _War on the Rocks_ , February 12, 2019.



25 Senate and House Armed Services Committees, _FY2020 NDAA Summary_ , December 2019, https://www.armed-services.senate.gov/imo/media/doc/FY20%20NDAA%20Conference%20Summary%20_%20FINAL.pdf.



26 Quoted in Shawn Snow, “Esper Says U.S. Forces Combating ISIS in Libya ‘Continue to Mow the Lawn,’” _Military Times_ , November 14, 2019.



27 Claudia Grisales, “These Are the Military Projects Losing Funding to Trump’s Border Wall,” NPR, September 4, 2019. 



28 Bureau of the Fiscal Service, _Final Monthly Treasury Statement: Receipts and Outlays of the United States Government: For Fiscal Year 2019 through September 30, 2019, and Other Periods_ (Washington: Department of the Treasury, September 2019). The deficit in fiscal year 2011 was $1.3 trillion according to the Congressional Budget Office. See Elizabeth Cove Delisle et al., “Federal Budget Deficit for Fiscal Year 2011: $1.3 Trillion,” Congressional Budget Office, November 8, 2011.



29 “Foreign Policy Poll,” Charles Koch Institute, January 2017, https://​mk0qeluyepi9​drvw7c​ng​.kin​stacdn​.com/​w​p​-​c​o​n​t​e​n​t​/​u​p​l​o​a​d​s​/​2​0​1​7​/​0​2​/​C​h​a​r​l​e​s​-​K​o​c​h​-​I​n​s​t​i​t​u​t​e​-​a​n​d​-​C​e​n​t​e​r​-​f​o​r​-​t​h​e​-​N​a​t​i​o​n​a​l​-​I​n​t​e​r​e​s​t​-​J​a​n​-​2​0​1​7​-​F​o​r​e​i​g​n​-​P​o​l​i​c​y​-​P​o​l​l​-​1.pdf, p. 22. See also “New Poll: Americans Crystal Clear: Foreign Policy Status Quo Not Working,” Charles Koch Institute, February 7, 2017.



30 Lydia Saad, “Demand Wanes for Higher Defense Spending,” Gallup, March 12, 2019; and Mark Hannah and Caroline Gray, _Indispensable No More? How the American Public Sees U.S. Foreign Policy_ (New York: Eurasia Group Foundation, November 2019).



31 F. Matthew Woodward, _Funding for Overseas Contingency Operations and Its Impact on Defense Spending_ (Washington: Congressional Budget Office, October 2018), pp. 3, 4.



32 Neta C. Crawford, “United States Budgetary Costs and Obligations of Post‐​9/​11 Wars through FY2020: $6.4 Trillion,” 20 Years of War: A Costs of War Research Series, Brown University, November 13, 2019, p. 3; and Elizabeth Field, _Overseas Contingency Operations: Alternatives Identified to the Approach to Fund War‐​Related Activities_ , GAO-19–211 (Washington: Government Accountability Office, January 2019), p. 1.



33 Senate and House Armed Services Committees, _FY2020 NDAA Summary_ , p. 1.



34 “OCO Is Fourth Largest ‘Agency,’” Taxpayers for Common Sense, February 10, 2020. The Trump administration’s 2021 budget request, _A Budget for America’s Future_ , outlines plans to reduce the overseas contingency operations budget each fiscal year (FY 2021 proposal is $69 billion), but Taxpayers for Common Sense also noticed the steady proposed increases to the base budget that will counterbalance this decrease in OCO. See “More OCO Details—the Devil’s in the Footnotes!,” Taxpayers for Common Sense, February 10, 2020.



35 Emily M. Morgenstern, “Foreign Affairs Overseas Contingency Operations (OCO) Funding: Background and Current Status,” Congressional Research Service In Focus, December 30, 2019.



36 Brendan W. McGarry, _The Defense Budget and the Budget Control Act: Frequently Asked Questions_ , CRS Report R44039 (Washington: Congressional Research Service, September 30, 2019). The Budget Control Act did not address the largest share of U.S. federal spending, so‐​called mandatory programs such as Medicare, Medicaid, and Social Security.



37 McGarry, _Defense Budget and the Budget Control Act_ , pp. 4–5. Congress has repeatedly amended the Budget Control Act (BCA) to change discretionary spending limits through the Bipartisan Budget Acts (BBA) of 2013, 2015, 2018, and 2019. The original cap for defense spending in 2020 under the BCA was to be $630 billion and was raised to $667 billion in the most recent BBA. For discretionary spending, which accounts for most defense spending, the Office of Management and Budget calculates the percentage and dollar amount to be taken from affected programs to achieve the total mandatory cut required by the BCA. See “FAQs on Sequester: An Update for 2020,” posted on the House Committee on the Budget’s website, https://​bud​get​.house​.gov/​p​u​b​l​i​c​a​t​i​o​n​s​/​r​e​p​o​r​t​/​F​A​Q​s​-​o​n​-​S​e​q​u​e​s​t​e​r​-​A​n​-​U​p​d​a​t​e​-​f​o​r​-2020.



38 Brendan W. McGarry and Emily M. Morgenstern, _Overseas Contingency Operations Funding: Background and Status_ , CRS Report R44519 (Washington: Congressional Research Service, September 6, 2019), p. 6.



39 For background, see Office of the General Counsel, _Principles of Federal Appropriations Law, Chapter 2: The Legal Framework, Fourth Edition, 2016 Revision_ , GAO-16–463SP (Washington: Government Accountability Office, 2016).



40 Michelle Mrdeza and Kenneth Gold, “Reprogramming Funds: Understanding the Appropriator’ Perspective,” Government Affairs Institute at Georgetown University, https://​gai​.george​town​.edu/​r​e​p​r​o​g​r​a​m​m​i​n​g​-​f​u​n​d​s​-​u​n​d​e​r​s​t​a​n​ding/.



41 Susan J. Irving (associate director of budget issues at Government Accountability Office) to Steve Horn (chairman of the Subcommittee on Government Management, Information and Technology under the Committee on Government Reform and Oversight), June 7, 1996, B-272080, https://​www​.gao​.gov/​a​s​s​e​t​s​/​9​0​/​8​5​6​2​0.pdf.



42 Mihir Zaveri, Guilbert Gates, and Karen Zraick, “The Government Shutdown Was the Longest Ever. Here’s the History,” _New York Times_ , January 25, 2019.



43 According to the White House, the deal that broke the impasse took $1.4 billion in the fiscal year 2019 budget bill, $3.6 billion from military construction projects, $2.4 billion from the Department of Defense counterdrug account, and $600 million from a Treasury Department forfeiture fund to fund the border wall construction. “President Donald J. Trump’s Border Security Victory,” Fact Sheet, The White House, February 15, 2019.



44 See “Construction Authority in the Event of a Declaration of War or National Emergency,” 10 U.S. Code § 2808.



45 The definition of a national emergency in U.S. code simply means a declaration of emergency by the president. See “Support for Counterdrug Activities and Activities to Counter Transnational Organized Crime,” 10 U.S. Code § 284.



46 See “Termination of Existing Declared Emergencies,” 50 U.S. Code § 1601.



47 Matthew Cox, “Army Scaling Back Recruiting Goals after Missing Target, Under Secretary Says,” Mil​i​tary​.com, March 21, 2019.



48 Jamie Crawford, “Military Sexual Assaults Increase Sharply, Pentagon Report Finds,” CNN, May 2, 2019; and Patricia Kime, “Active‐​Duty Military Suicides at Record Highs in 2018,” Mil​i​tary​.com, January 30, 2019.



49 The fiscal year 2020 National Defense Authorization Act increased overall military end strength by 1,400 troops. See Pat Towell, _FY2020 National Defense Authorization Act: P.L. 116–92 (H.R. 2500, S. 1790)_ , CRS Report R46144 (Washington: Congressional Research Service, January 2, 2020), p. 8.



50 Integrated Fire Control‐​Counter Air System is the Navy’s multidomain battle management system; the Advanced Battle Management System is the technical engine that manages all communications, orders, and sensors that the Air Force uses.



51 Dan Gouré, “A New Joint Doctrine for an Era of Multi‐​Domain Operations,” _Real Clear Defense_ , May 24, 2019; and Grant J. Smith, “Multi‐​Domain Operations: Everyone’s Doing It; Just Not Together,” _Over the Horizon_ , June 24, 2019.



52 Theresa Hitchens, “Navy, Air Force Chiefs Agree to Work on All Domain C2,” _Breaking Defense_ , November 12, 2019.



53 Examples of these efforts include the Commandant of the Marine Corps Strategist Program (https://​www​.usm​cu​.edu/​A​c​a​d​e​m​i​c​-​P​r​o​g​r​a​m​s​/​C​M​C​-​F​e​l​l​o​w​s​-​S​t​r​a​t​e​g​i​s​t​s​-​F​o​r​e​i​g​n​-​P​M​E​-​O​l​m​s​t​e​d​-​S​c​h​o​l​a​r​s​/​C​o​m​m​a​n​d​a​n​t​-​o​f​-​t​h​e​-​M​a​r​i​n​e​-​C​o​r​p​s​-​S​t​r​a​t​e​g​i​s​t​-​P​r​o​gram/) and Department of Defense STEM scholarships (https://​dod​stem​.us/​s​t​e​m​-​p​r​o​g​r​a​m​s​/​s​c​h​o​l​a​r​ships).



54 The military doesn’t currently use the term “data wrangler,” but it is a common term in the film industry used to identify the person responsible for collecting and storing digital footage. This process is much the same in the military, where all data generated needs to be collected, transformed, stored, and analyzed. In short, the U.S. military needs to establish a system to identify and task battlefield data managers.



55 See Kessel Run (website), U.S. Air Force, https://​kessel​run​.af​.mil/​roles.



56 For example, the Pentagon has repeatedly tried to terminate production of new M1 Abrams tanks, but Congress continued to fund them over these objections. See Associated Press, “Army: Thanks but No Tanks,” _Politico_ , April 28, 2013. Similarly, Congress authorized 12 more F-35 Lightning II aircraft than the Trump administration requested in fiscal year (FY) 2020. See Towell, _FY2020 National Defense Authorization Act_ , pp. 20–21. The continued legislative requirement for aircraft carriers similarly complicates long‐​range shipbuilding plans. The Department of Defense’s request for the _Columbia_ -class ballistic missile submarine for FY 20 was $1.7 billion for procurement and $533 million for research and development; however, Congress authorized $1.8 billion for procurement and $548 million in research and development for FY 20. See Towell, _FY2020 National Defense Authorization Act_ , p. 10.



57 Quantum computing is the computational method utilizing superposition and entanglement to process calculations orders of magnitude faster than current microprocessors. Note: Quantum supremacy is different from quantum advantage, which is when a quantum device solves a problem _faster_ than a traditional computer. Thanks to James Knupp for clarifying this concept. See Frank Arute et al., “Quantum Supremacy Using a Programmable Superconducting Processor,” _Nature_ 574 (2019): 505–510. 



58 Benjamin Jensen, Scott Cuomo, and Chris Whyte, “Wargaming with Athena: How to Make Militaries Smarter, Faster, and More Efficient with Artificial Intelligence,” _War on the Rocks_ , June 5, 2018. 



59 See, for example, Robert M. Farley, _Grounded: The Case for Abolishing the United States Air Force_ (Lexington: University Press of Kentucky, 2014).



60 Todd Harrison, _The Air Force of the Future: A Comparison of Alternative Force Structures_ (Washington: Center for Strategic and International Studies, October 2019).



61 Operational readiness is the capacity for a unit to perform its designated combat or combat‐​support functions.



62 Dan Grazier, “F-35: Is America’s Most Expensive Weapon of War the Ultimate Failure?,” _National Interest_ , March 19, 2019; Kristin Houser, “Hard Landing: U.S. Military’s Trillion‐​Dollar F-35 Fighter Jet Is Almost Unflyable,” _Futurism_ , June 13, 2019; Michael P. Hughes, “What Went Wrong with the F-35, Lockheed Martin’s Joint Strike Fighter?,” _The Conversation US_ , June 14, 2017; Jonathan Lowell, “A U.S. Air Force Pilot Describes How He Landed His F-35 Safely after a Mid‐​Air Power Failure,” _Business Insider_ , August 27, 2019; and Eric Tegler, “WTF-35: How the Joint Strike Fighter Got to Be Such a Mess,” _Popular Mechanics_ , July 27, 2018. The program initially called for 2,000 aircraft of all variants by the end of fiscal year 2019 but was to have produced only 500 over that period: Michael J. Sullivan, _F-35 Joint Strike Fighter: Action Needed to Improve Reliability and Prepare for Modernization Efforts_ , GAO-19–341 (Washington: Government Accountability Office, April 2019), p. 6. The F-35A model, flown by the Air Force, is set to drop from $89.2 million to $77.9 million in 2022: Marcus Weisgerber, “Price of F-35 Falls, but Not as Much as Pentagon Hoped,” _Defense One_ , October 29, 2019.



63 Anti‐​access/​area‐​denial is an operational concept that complicates an opponent’s ability to use air, naval, and land power at long distance, which typically entails the use of land‐​based sensors and precision strike systems to target opponent ships, aircraft, and bases.



64 Harrison, _The Air Force of the Future_.



65 Harrison.



66 Brenda S. Farrell, _Military Personnel: Strategy Needed to Improve Retention of Experienced Air Force Aircraft Maintainers_ , GAO-19–160 (Washington: Government Accountability Office, February 2019).



67 Rachel S. Cohen, “USAF’s New Info Warfare Group Coming into Focus,” _Air Force Magazine_ , September 18, 2019. Regarding the F-15EX, see Kyle Mizokami, “After Nearly 20 Years, the Air Force Will Fly Brand New F‐​15s,” _Popular Mechanics_ , January 29, 2020.



68 Pete Geren and George W. Casey Jr., _A Statement on the Posture of the United States Army 2009_ (Washington: U.S. Army, May 2009).



69 For more information about Army Futures Command, visit https://​www​.army​.mil/​f​u​tures.



70 The Army later added two additional cross‐​functional teams, “Synthetic Training Environment” and “Assured Positioning, Navigation and Timing.”



71 Dave Philipps, “As Economy Roars, Army Falls Thousands Short of Recruiting Goal,” _New York Times_ , September 21, 2018.



72 Microtargeting is a direct marketing technique that segments consumers by tastes or attributes.



73 Eric J. Labs, _An Analysis of the Navy’s Fiscal Year 2020 Shipbuilding Plan_ (Washington: Congressional Budget Office, October 2019), p. 3.



74 The prior year, for example, the Congressional Budget Office similarly concluded that the cost of the Navy’s plan for new‐​ship construction ($26.7 billion) would nearly double its historical average of $13.6 billion. See Eric J. Labs, _Analysis of the Navy’s Fiscal Year 2019 Shipbuilding Plan_ (Washington, Congressional Budget Office, October 2018), p. 3. A report 10 years earlier had reached a similar conclusion: the estimated costs to fulfill all the Navy’s wishes were nearly double what it was likely to receive given historical funding averages. See Dale Eisman, “Navy’s Shipbuilding Wish List Sails into Troubled Waters,” _The Virginian‐​Pilot_ , March 15, 2008.



75 David B. Larter, “Acting US Navy Secretary: Deliver Me a 355‐​Ship Fleet by 2030,” _Defense News_ , December 9, 2019.



76 Rebecca Kheel, “Pentagon Proposes $704B Budget with Boost for Nukes, Cuts to Ships,” _The Hill_ , February 10, 2020.



77 Quoted in David B. Larter, “In a Quest for 355 ships, US Navy Leaders Are Unwilling to Accept a Hollow Force,” _Defense News_ , January 13, 2020. See also Nick Blenkey “Acting Secnav Commits to 355 Ship Navy, but Not at $2 Billion Apiece,” _MarineLog_ , January 10, 2020.



78 See, for example, Eric Edelman et al., _Providing for the Common Defense: The Assessment and Recommendations of the National Defense Strategy Commission_ (Washington: United States Institute of Peace, November 2018), p. 63.



79 Dakota L. Wood, ed., _2020 Index of U.S. Military Strength with Essays on Great Power Competition_ (Washington: Heritage Foundation, November 2019), p. 349.



80 John Pendleton, _Navy Force Structure: Sustainable Plan and Comprehensive Assessment Needed to Mitigate Long‐​Term Risks to Ships Assigned to Overseas Homeports_ , GAO-15–329 (Washington: Government Accountability Office, May 2015); see also Geoff Ziezulewicz, “Navy’s 7th Fleet No Stranger to High Ops Tempo,” _Navy Times_ , August 21, 2017.



81 Ballistic missile submarines (SSBNs) are the sea‐​based leg of the nuclear triad. These vessels carry Trident missiles, which are each capable of delivering up to eight nuclear warheads. Fast‐​attack submarines (SSN) are the U.S. Navy’s primary undersea platform, capable of offensive action both at sea and against targets on land.



82 Aircraft carriers (CVNs) are the largest ships in the U.S. Navy and the centerpiece of U.S. fleet operations, capable of carrying about 60 aircraft of varying types. The “N” in the hull classification of a CVN denotes that it employs nuclear propulsion. Guided‐​missile frigates (FFG) are a mixed‐​armament warship lighter than a destroyer, which are typically focused on anti‐​ship and anti‐​submarine warfare. The FFG(X) is the next generation of multimission guided‐​missile frigates.



83 Kyle Mizokami, “USS Ford Will Set Sail with Only 2 out of 11 Weapon Elevators,” _Popular Mechanics_ , October 12, 2019.



84 Arresting gear is the mechanical system that rapidly decelerates aircraft when they land on a platform, such as an aircraft carrier. Dual band radars combine radar systems into one integrated system for easier operation, maintenance, upgrade, and targeting.



85 Robert F. Behler, _Director, Operational Test and Evaluation: FY 2018 Annual Report_ , Department of Defense, December 2018, p. 131.



86 Behler, _FY 2018 Annual Report_ , p. 134.



87 Justin Katz, “As Navy Touts $14.9B Dual Carrier Buy Contract, DOT&E Report Calls Out ‘Unrealistic Assumptions’ about CVN-78,” _Inside Defense_ , February 1, 2019.



88 Craig Hooper, “The Most Expensive Ship in the World Is Broken. The U.S. Navy Secretary Should Be Held Accountable,” _Forbes_ , October 16, 2019.



89 Henry J. Hendrix, _At What Cost a Carrier?_ , Disruptive Defense Papers (Washington: Center for a New American Security, March 2013), p. 3.



90 T. X. Hammes, “We Need to Start Thinking Differently about Maritime Airpower—and We Can,” _Task and Purpose_ , September 19, 2018.



91 Paul McLeary, “All 6 East Coast Carriers in Dock, Not Deployed: Hill Asks Why,” _Breaking Defense_ , October 28, 2019.



92 _Executive Summary: 2016 Navy Force Structure Assessment (FSA)_ (Washington: U.S. Department of the Navy, December 15, 2016), p. 2; and Ronald O’Rourke, _Navy Frigate (FFG[X]) Program: Background and Issues for Congress_ , CRS Report R44972 (Washington: Congressional Research Service, April 28, 2020), p. 1.



93 John Cole and Thomas Ulmer, “Bad Idea: Reactivating the U.S. Navy’s Oliver Hazard Perry‐​Class Frigates,” _Defense 360_ , December 7, 2017; and David B. Larter, “Don’t Reactivate the Old Frigates, Internal US Navy Memo Recommends,” _Defense News_ , November 12, 2017.



94 David B. Larter, “The US Navy’s New, More Lethal Frigate Is Coming into Focus,” _Defense News_ , January 28, 2019. In late April 2020, the Navy selected a design by Italian shipmaker Fincantieri to be built at the Marinette Marine shipyard in Wisconsin. David B. Larger, “The US Navy Selects Fincantieri Design for Next‐​Generation Frigate,” _Defense News_ , April 30, 2020.



95 Labs, _Analysis of the Navy’s Fiscal Year 2019 Shipbuilding Plan_ , p. 25.



96 O’Rourke, _Navy Frigate (FFG[X]) Program_ , p. 26, Table 3.



97 Wood, _2020 Index of U.S. Military Strength_ , p. 375.



98 Maritime choke points are heavily trafficked, narrow waterways such as straits and narrows.



99 Leo Spaeder, “Sir, Who Am I? An Open Letter to the Incoming Commandant of the Marine Corps,” _War on the Rocks_ , March 28, 2019.



100 David H. Berger, _Commandant’s Planning Guidance: 38th Commandant of the Marine Corps_ (Washington: U.S. Marine Corps, 2019).



101 Berger, _Commandant’s Planning Guidance_. The Marine Corps _Force Design 2030_ offers more details about how the service will turn Berger’s guidance into reality. See _Force Design 2030_ (Washington: U.S. Marine Corps, March 2020). To learn more about how the 2030 force design could help the service implement a restraint‐​focused grand strategy, see Eric Gomez, “Marine Corps Changes Inch U.S. Closer to a Restraint‐​Friendly Military Posture,” _Cato at Liberty_ (blog), Cato Institute, March 24, 2020.



102 Minnie Chan, “China’s Navy Is Being Forced to Rethink Its Spending Plans as Cost of Trade War Rises,” _South China Morning Post_ , May 26, 2019.



103 Scott Cuomo et al., “How the Marines Will Help the U.S. Navy and America’s Allies Win the Great Indo‐​Pacific War of 2025,” _War on the Rocks_ , September 26, 2018.



104 Quoted in Richard R. Burgess, “Marine Commandant Berger: Force Design Is Top Priority,” _Seapower Magazine_ , July 18, 2019.



105 Sam LaGrone and Megan Eckstein, “Failure of Two Ships to Participate in RIMPAC Highlight Amphibious Readiness Gap,” _USNI News_ , August 1, 2018.



106 For more on the “deterrence gap” concept, see Keith B. Payne, “The Emerging Nuclear Environment: Two Challenges Ahead,” National Institute for Public Policy Information Series no. 436, January 2, 2019; and Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 55.



107 Michael Bennett, _Approaches for Managing the Costs of U.S. Nuclear Forces, 2017 to 2046_ (Washington: Congressional Budget Office, October 2017), p. 1; and Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 55.



108 The exact amount of fiscal year (FY) 2019 and FY 20 appropriations for the Missile Defense Agency (MDA) was $10.491 billion and $10.452 billion, respectively. For a breakdown of funding by program, see Wes Rumbaugh, “FY 2020 Missile Defense Agency Budget Tracker,” Missile Threat, Missile Defense Project, Center for Strategic and International Studies, December 30, 2019. The Trump administration proposed $20.3 billion for missile defense and defeat in its FY 21 budget submission, including $9.2 billion for the MDA. See Jon Harper, “Budget 2021: Trump Proposes Flat Pentagon Budget,” _National Defense_ , February 10, 2020.



109 Rachel S. Cohen, “B61-12 Nuclear Warhead Delay Drives Up Price Tag,” _Air Force Magazine_ , September 25, 2019; and Sara Sirota, “GAO: B61-12 LEP First Production Unit Delayed, W80-4 LEP Cost Estimate Increased,” _Inside Defense_ , June 18, 2019.



110 As Kingston Reif and Mandy Smithberger note, “the B-2 bomber program overran its cost so badly that a mere 20 aircraft emerged from a $40 billion program [that originally] intended to buy 135 to 150 aircraft.” Kingston Reif and Mandy Smithberger, “America’s New Stealth Bomber Has a Stealthy Price Tag,” _Defense One_ , May 21, 2018.



111 Dennis Evans and Jonathan Schwalbe, _The Long‐​Range Standoff (LRSO) Cruise Missile and Its Role in Future Nuclear Forces_ (Laurel, MD: Johns Hopkins Applied Physics Laboratory, 2017), p. 8.



112 Olga Oliker and Andrey Baklitskiy, “The Nuclear Posture Review and Russian ‘De‐​Escalation:’ A Dangerous Solution to a Nonexistent Problem,” _War on the Rocks_ , February 20, 2018; and Olga Oliker, “U.S. and Russian Nuclear Strategies: Lowering Thresholds, Intentionally and Otherwise,” in _America’s Nuclear Crossroads: A Forward‐​Looking Anthology_, eds. Caroline Dorminey and Eric Gomez (Washington: Cato Institute, July 2019), pp. 37–46.



113 For more on these recommendations, see Caroline Dorminey, “Buying the Bang for Fewer Bucks: Managing Nuclear Modernization Costs,” in _America’s Nuclear Crossroads_, pp. 1–15.



114 Eric Gomez, “It Can Get You into Trouble, but It Can’t Get You Out: Missile Defense and the Future of Nuclear Stability,” in _America’s Nuclear Crossroads_, p. 17.



115 Michael D. Griffin and Rebeccah L. Heinrichs, “Ensuring U.S. Technological Superiority: An Update from Under Secretary Michael D. Griffin,” (interview, Hudson Institute, Washington, August 23, 2019); and Patrick Tucker, “Trump’s New Missile Policy Relies Heavily on Largely Unproven Technologies,” _Defense One_ , January 17, 2019.



116 Kelsey Davenport, “The Joint Comprehensive Plan of Action (JCPOA) at a Glance,” Arms Control Association, May 2018; and Maggie Tennis, “Preserving the U.S. Arms Control Legacy in the Trump Era,” in _America’s Nuclear Crossroads_, pp. 81–84.



117 Ben Hubbard, Palko Karasz, and Stanley Reed, “Two Major Saudi Oil Installations Hit by Drone Strone, and U.S. Blames Iran,” _New York Times_ , September 14, 2019; and Dan Lamothe, “U.S. to Send 1,800 Additional Troops to Saudi Arabia to Boost Defenses against Iran,” _Washington Post_ , October 11, 2019.



118 David Barno and Nora Bensahel, “Fighting and Winning in the ‘Gray Zone,’” _War on the Rocks_ , May 19, 2015.



119 Stephen Biddle and Ivan Oelrich, “Future Warfare in the Western Pacific: Chinese Antiaccess/​Area Denial, U.S. AirSea Battle, and Command of the Commons in East Asia,” _International Security_ 41, no. 1 (Summer 2016): 7–48; and Michael Kofman, “It’s Time to Talk about A2/AD: Rethinking the Russian Military Challenge,” _War on the Rocks_ , September 5, 2019.



120 Ted Galen Carpenter and Eric Gomez, “East Asia and a Strategy of Restraint,” _War on the Rocks_ , August 10, 2016.



121 For more, see Eric Gomez, “The Future of Extended Deterrence: Are New U.S. Nuclear Weapons Necessary?,” in _America’s Nuclear Crossroads_, p. 58.



122 Terence Roehrig, _Japan, South Korea, and the United States Nuclear Umbrella: Deterrence after the Cold War_ (New York: Columbia University Press, 2017), p. 15.



123 Barry R. Posen, _Inadvertent Escalation: Conventional War and Nuclear Risks_ (Ithaca: Cornell University Press, 1991), pp. 2–3; and Caitlin Talmadge, “Would China Go Nuclear? Assessing the Risk of Chinese Nuclear Escalation in a Conventional War with the United States,” _International Security_ 41, no. 4 (Spring 2017): 53–55.



124 For information about technical developments that blur the nuclear/​conventional distinction, see James M. Acton, “Escalation through Entanglement: How the Vulnerability of Command‐​and‐​Control Systems Raises the Risks of an Inadvertent Nuclear War,” _International Security_ 43, no. 1 (Summer 2018): 63–65. On military strategies that increase escalation risks, see Talmadge, “Would China Go Nuclear?,” p. 53; and Tong Zhao and Li Bin, “The Underappreciated Risks of Entanglement: A Chinese Perspective,” in _Entanglement: Russian and Chinese Perspectives on Non‐​Nuclear Weapons and Nuclear Risks_ , ed. James M. Acton (Washington: Carnegie Endowment for International Peace, 2017), pp. 58–59.



125 Gomez, “It Can Get You into Trouble, but It Can’t Get You Out,” pp. 25–28.



126 For a brief explainer on what a space sensor layer for missile defense could look like, see “Missile Defense Tracking System, Space Sensor Layer (SSL), Hypersonic and Ballistic Tracking Space Sensor (HBTSS),” Glob​alse​cu​ri​ty​.org, https://​www​.glob​alse​cu​ri​ty​.org/​s​p​a​c​e​/​s​y​s​t​e​m​s​/​h​b​t​s​s.htm.



127 William J. Burns, “The Demolition of U.S. Diplomacy: Not Since Joe McCarthy Has the State Department Suffered Such a Devastating Blow,” _Foreign Affairs_ , October 14, 2019, https://www.foreignaffairs.com/articles/2019–10-14/demolition-us-diplomacy.



128 There are several issues that we did not deal with in this analysis, but the Cato Institute intends to issue defense policy and budget analyses annually, with each report focusing on three to four core challenges. Next year, for example, will include a focus on two relatively ignored aspects of the budgeting process: cybersecurity and technology, in general, and workforce issues, including retention, recruitment, and education.
"
"
Share this...FacebookTwitter
I’ve just read the latest climate horoscope at the Hannoversche Allgemeine Zeitung website, which delivers them almost daily.
The latest one comes from the fortune tellers and scryers at the Massachusetts Institute of Technology, led by psychic Paul O’Gorman, now available at the PNAS here.
The latest horoscope foretells that (later) in the 21st century, summers will be stickier and grittier, and winters will be stormier – this according to visions and images delivered by crystal balls and gazings into MIT scrying pools.
Apparently MIT diviners made contact with the spirits of 1981 to 2000, so writes the HAZ, and felt the unsettling vibes of mystic energy of atmospheres past, and the energy intensity of past climatological storms. MIT’s assortment of sophisticated scrying instruments, made of silicone and crystal, all delivered similar predictions for the 21st century – forebodings all confirmed by their climate tarot punch cards.
The bad vibrations and ill spirits foretell one thing only: doom!
The 21st century
The northern hemispheric middle latitudes will be haunted by severe meteorological storms between the autumn and spring equinoxes, becoming especially intense before and after the winter solstices.
“I see storms and doom!”
For periods surrounding the summer solstices, crushing doldrums will beset northern middle latitude regions. Stagnate atmospheres will cause pollutants, and the evil spirits they harbor, to accumulate in ever higher concentrations above cities, bringing misery to non-believers.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Be forewarned! The degree of misery about to haunt the middle latitudes in the end will depend on the amount of ice surrounding the magnetic North Pole at the fall equinoxes.
The southern hemisphere will be visited by other misfortune, so say the MIT instruments of clairvoyance, and the diviners who gaze into them. There, ruthless storms will occur year-round, from solstice to solstice, from equinox to equinox.
Careful though, as other celestial alignments may impact the fortune tellers’ predictions. These predictions may change as they depend on what parts of the atmosphere are heavily impacted. If the earthly layer of the atmosphere energizes, then other currents and eddies come into play.
In the northern hemisphere, however, the heavenly layers of the atmosphere shall warm, and this will act to calm the air mass energy.
Come back tomorrow for more predictions!
For personal mystic climate fortune-telling and face-palm reading, make an appointment with Paul O’Gorman: pog@mit.edu.
UPDATE: I’ve just received a photo of just one of MIT’s highly confidential fortune telling instruments, with one of the teams of divine scryers who gaze at it:


Share this...FacebookTwitter "
"
Share this...FacebookTwitterJapan has decided to back out of carbon trading, likely from intense pressure from industry. This will no doubt disappoint many in organized crime, read here and here.
According to the online Environmental Leader:
A September survey of Japan’s largest business lobby group, found that 95% of companies surveyed oppose carbon trading, citing competition from countries like India and China that are not bound by similar pollution limits, reported Bloomberg.”
Japan’s government also said it won’t support an extension of  the Kyoto Protocol after its greenhouse-gas emissions targets expire in 2012, calling the treaty “outdated” because it only regulates 27 percent of global emissions, and doesn’t include the U.S. and China, reported Bloomberg.”
Meanwhile the Financial Times Deutschland here reports the same in a piece called: CO2 Trade No Longer In Vogue. The FTD blamed pressure from industry for leading to Japan’s decision to back out of the scheme, and added:
The decision is a major setback for the once praised system that was touted as a way to achieve environmental protection using  market economy means.”
Turns out that the Japanese have determined that it’s not very market economy-friendly after all.
The USA, under its newly elected Congress, have also signalled it will reject any trading scheme. Japan meanwhile is planning hefty taxes on carbon fuels and is considering energy feed-in traiffs like those used in Germany for supporting renewable energy.
Share this...FacebookTwitter "
"
Share this...FacebookTwitter 
Is paranoia contagious? It seems so in certain circles. Like fear, it begins with one, and then spreads like wildfire.
Yesterday I wrote about Professor Stefan Rahmstorf’s hissing and fitting reaction to a piece written by EIKE, the humble sceptic organisation that had the audacity to bring up one of Rahmstorf’s old, yet embarrassing papers here, which I wrote about here.
Something is really getting to the poor fella. Science is supposed to be for the calm, and not the irrational. Rahmstorf has to learn to get a grip.
It seems whenever someone expresses dissent, which is normal in science, Rahmstorf flies off the handle and lashes out, almost irrationally.
Yesterday, for example, Roger Pielke Jr. wrote a piece here with details on an e-mail exchange between Rahmstorf’s sidekick Michael Mann and journalist Daniel Greenberg. These guys really seem to think the whole world is out to get them.
And recall the paranoia that pervaded throughout the Climategate e-mails involving Jones, Mann and the rest of the cast.
In this post I’m only going to focus primarily on Rahmstorf, as he is the issue here in Germany at the moment. But it fundamentally applies to the rest of his team as well.
Firstly, Rahmstorf is one of the very few “climate scientists” who truly believes that sea levels are about to rise at alarming rates over the next few decades, even refusing to accept the much more moderate projections of the IPCC, for which he is a lead author. Even though there is no data out there to support it, Rahmstorf is sure the atmosphere and seas are out to get us.
Calmer minds have attempted on numerous occasions to alleviate his anxiety by pointing out that scientific data do not support his horror visions, and that there is no need to go sleepless about it. For example sea levels over the last 100 years have risen at their normal rate of about 20 cm per century. Over the last years sea level rise has even slowed down, Read more here: Going Down.
Sea level rise is slowing. TOPEX U. of Colorado


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And when someones tries to explain to him that the Greenland ice cores provide a reasonable temperature reconstruction of the past for the globe, he refuses to believe that too, insisting the fluctuations were local and not global. And if someone shows him that multiple proxies from all over the globe also show that temperatures fluctuated naturally, corresponding with the Greenland ice core, Rahmstorf still refuses to believe it and insists the climate is coming for us.
Proxies from all over the world confirm fluctuations on a global scale.
Whether it’s sea ice data, accumulated cyclone energy, the last two brutal German winters, ocean cycles, satellite data, etc., Rahmstorf refuses to believe the data no matter what. The climate’s after us.
So why does Rahmstorf refuse to believe data and the fellow scientists who deliver them? Here he also believes that these scientists are paid hacks of the big carbon industries, tobacco, and ultra right-wing think tanks.
The lefty Süddeutsche Zeitung newspaper, for example, even reminded Rahmstorf awhile back that EIKE is just small, mailbox sceptic organisation. Are corporations pouring millions and millions and only getting a mailbox operation for their big bucks?
Gravediggers of science
Rahmstorf is acting too paranoid, and this behaviour is now running rampant through much of the warmist side. He even thinks that European corporations are buying up US denier senators. He writes at his piece Headlines From Absurdistan, citing treehugger blog:
Also European companies don’t pinch their pennies when it comes to buying up candidates for the US Senate who deny anthropogenic climate change.
How are regular people reacting to this? Daniel Greenberg wrote to Pielke:
My sympathy to you and anyone else who has to deal with them. They’re gravediggers of science.
It’s called self-destruction.
Relax – the climate is not going to tip and destroy the planet. The data does not show this.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe European Institute For Climate and Environment (EIKE) in Germany recently had a piece at it’s blogsite here, which I have summarized.
The USA has James Hansen, and England has Phil Jones. Germany now has Prof. Dr. Gerhard Adrian, the new President of the German Weather Service. His mission: to produce a trend to climate catastrophe as quickly as possible. Recently he said:
The average temperature in Germany has risen by in 1.1°C from 1881 to 2009. It could go up another 2 to 4°C by the end of the century.
and:
We’ll have a completely whole new set of extremes to deal with; that’s the threat.
This is the new message from the once respected German Weather Service. Suddenly, doom and gloom are the forecast.
Inconveniently for Dr. Adrian, his own data and earlier statements made by German Weather Institutes seem to contradict his claims.

Firstly, why choose a timescale that starts in 1881 when records go back as much as 300 years in Europe? The following graphics are temperature charts going back more than 200 years for some European cities (visit EIKE for better quality graphics (here).
Here’s the temperature chart for Berlin going back 300 years (same as above in the introduction):
There we don’t see much going on until about 1990. In fact the total trend is 0.08″C rise per century – statistically insignificant. Surely Dr. Adrian is aware of this.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now let’s take a look at the temperature CO2 correlation.
Poor correlation there too. While atmospheric CO2 concentrations climbed from 1750 to 1980, Berlin’s temperature did the opposite. But when one is hired to promote global warming alarmism, then 1881 is a good place to start.
Let’s go back and look at the last 2000 years. Maybe that’ll reveal something more earth-shattering.
All the climate catastrophe talk put out by Hansen, Jones, and now Dr. Gerhard Adrian, simply do not materialize when you take an honest look at the statistics.
As far as weather extremes occurring, here’s what the German Weather Institutes said in the pre-Gerhard-Adrian days, just a couple of years ago. According to the German Weather Service, recently quoting the German Meteorological Society, 3/2002, p. 2:
When it comes to extreme weather events, no significant trend can be observed up to now. Also such events like the flooding of 2002 are part of the norm in our climate.
According to a German Weather Service press conference 24 April 2007, Berlin;
Up to now there has been no increase of extreme events:  Up to now – with the exception of the already mentioned heavy summertime precipitation – there has been no detectable systematic changes or shifts of extreme values.
And again, according to the German Meteorological Society 3/2002, S. 2, on the flood of 2002:
Also such events, like the big flood of 2002, are part of the norm in our climate.
Dr. Adrian ought to listen to his own data.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Ed Caryl
In attempting to keep A Light in Siberia as short as possible, the how and why of some points were not included. This led to some comments calling into doubt some of the results. I would like to clarify some of those points.
The choice of baseline period for the surface temperature anomaly map
The 1933 to 1963 baseline for the surface temperature anomaly map was chosen for two major reasons. First, that period includes the peak of the last warm period before the present one. Second, that period was before most of the UHI warming took place for the arctic stations studied, making them show up on the map as red or orange grid-squares. The Arctic and Antarctic stations are highlighted. If you choose the modern warm period, 1979 to 2009, the baseline period includes much of the UHI warming, and the anomalies are much less pronounced.
GHN GISS 1933 - 1963
GISS 1979 - 2009
 
The satellite temperature map shows the arctic warming
Yes, it sure does. The reason is that the bottom of the AMO cycle was just prior to the beginning of the satellite measurements (1979). The arctic has been warming since then. If the satellites had been first launched in 1940 it would be a different picture. In 2050 it will be a different picture. These cycles are 70 years long, the biblical “three-score and ten”. Our main problems with studying climate are that we don’t live long enough to remember more than part of one cycle, and the satellite era has only been 31 years.


The selection criteria for “Urban” versus “Isolated”
“Urban” – Next to or in a town or research station with growth, or change in population over time, or change in heat generated over time.
“Isolated” – A location with no significant population or heat generator changes over time, a stable unchanging environment. An isolated location is one that never had more than one or two buildings, and with always the same size staff, and no adjacent town.
Remember, in the Arctic, in the winter, the environment around all these locations is very cold, bleak, desolate, and unpopulated. A steam-heated town or research station will stand out in the infrared like a bonfire in a desert.
 The averaging of station data
The stations discussed all have data over different time periods, and have average temperatures that are different. Some have gaps in the data. How can these be averaged without distortion?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The data was downloaded from GISS as text files. These were dropped into Excel spreadsheets, and converted into column-delimited files. The monthly data was discarded, as the annual average has already been computed by GISS and is the rightmost column. Data gaps in GISS files are marked by the entries 999.9. These cells were cleared.
After putting all the stations into one spreadsheet with the total year span in the leftmost column, and each station with its own column, aligned with the correct years, each station column was averaged using the SUM of the column divided by the COUNT of the cells in each column with data. Then the average of all the columns was computed. This number is then the average of all the temperatures in all the stations over the whole time period. Call that the “table” average.
The next step was to “normalize” the data for each station by subtracting the “table” average from each column average. This results in a normalization factor for each column. That normalization factor was then subtracted from each value in that column. The normalization factor will be different for each station.

 The rest is the data for all the stations now plot right over each other, in a narrow range, and now can be averaged across the rows in the same manner as the columns were averaged, using SUM divided by the COUNT for each row. Those adjacent years with many stations reporting data get a somewhat smoother plot than those years that have only one or two stations with data, but there is no distortion in the average from some stations being much warmer or cooler than the others.

The R2 question
R-squared value definition:
The R-squared value, also known as the coefficient of determination, is an indicator that ranges in value from 0 to 1 and reveals how closely the estimated values for the trendline correspond to your actual data. A trendline is most reliable when its R-squared value is at or near 1.
The above definition is cut and pasted unedited from the Excel Help files. What it means is that if the data and the trend-line coincide, as above, (if the data plotted is a straight line and the trend is coincident) then the R2 value would be 1.

Our very noisy data, with the AMO sine-wave-like curve superimposed, has a trend line with an extremely low R2 value, because it has a very low correspondence to the data. The trend has very little statistical significance, thus little or no warming is indicated.
Share this...FacebookTwitter "
"
Share this...FacebookTwitter
Only 8 days to go!
A new paper published today in Atmospheric Chemistry and Physics suggests that the relationship proposed by Henrik Svensmark is supported. Svensmark proposes that changes in the sun’s magnetic field modulate the density of Galactic Cosmic Rays (GCRs) which in turn seed cloud formation on Earth, which changes the albedo/reflectivity to affect Earth’s energy balance and hence global climate. Read more here at WUWT:
Cosmic rays linked to rapid mid-latitude cloud changes, B. A. Laken , D. R. Kniveton, and M. R. Frogley.
This will make the 3rd International Energy and Climate Conference that much more interesting as Svensmark himself will be one of the distinguished speakers, along with Fred Singer and others.

Roster of Speakers
Prof. Dr. Fred Singer, USA, Atmospheric Physicist, Chairman IPCC
Prof. Dr.Dieter Ameling, Former President of Business Union Steel and Chairman Stahl institute VDEh.
Prof. Dr. Robert Bob Carter, Australia, Geologist
Prof. Dr. Vincent Courtillot, France, Geophysicist
Prof. Dr. Karl-Friedrich Ewert, Germany, Geologist
Prof. Dr. Ian Plimer, Australia, Geologisist
Prof. Dr. Werner Kirstein, Germany, Dipl. Physicist & Geography
Prof. Dr. Horst Lüdecke, Germany, Press Spokesman for EIKE
Prof. Dr. Nir Shaviv, Israel, Astrophysicist
Prof. Dr. Henrik Svensmark, Denmark, Atmospheric Sciences
Prof. Dr. Jan Veizer, Canada, Paleo-geologist
Dr. Emmanuel Martin, France, Economist
Dr. Horst Borchert, Germany, Physicist
Dr. Lutz Peters, Germany, Author of Klima 2055
Dipl.-Ing. Michael Limburg, Germany, Vize President EIKE
Dipl. Meteorologist Klaus Puls, Germany
Günther Ederer, Germany, Business Journalist and Film Producer


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Climate conference location
Berlin, Germany
10117 Berlin Mitte, only a few steps away from the S-Bahn and Metro stations Friedrichstraße.
Conference dates and times
Starts Friday 3 December 2010 at 1 p.m. and ends at approx 8 p.m.
Starts Saturday 4 December 2010 at 8 a.m. and ends at approx 4 p.m.
Conference fees
120€ private participant (70€ day ticket)
250€ company representative
220€ for a student sponsored by company
Please book before 26 November 2010. After that a 30€ surcharge gets levied.
Info, registration, tickets
Email: info@berlinmanhattan.org
Fax: (+49) 30 69 20 800 39
More information here: 3rd International Energy and Climate Conference.

Share this...FacebookTwitter "
"

Despite the overwhelming evidence that markets perform best when left alone by the government, it is still virtually taken for granted that one consumer product should be completely controlled by every government in the world. One product, so ubiquitous, that it’s used by almost everyone in the world on a daily basis: money.



Money is vitally important; the lifeblood of our financial system, but it is a product nonetheless. Consumers use this product not just as a medium of exchange but also as a liquid store of value and as a basis for accounting. Money producers, i.e., central banks, profit through seignorage, the ability to earn interest on their assets while issuing notes, e.g., dollar bills that pay no interest. The Fed creates money from thin air when it buys an interest bearing Treasury security and credits the seller with dollars. These dollars are not backed by anything in the sense that the Fed is not obligated to buy or convert dollars into anything.



Now there would be nothing wrong with this if the Fed were just some private institution trying to earn a living in an unregulated market. But of course, the Fed is a government‐​protected monopoly. Even prior to the Fed’s creation in 1913 there was always some level of government regulation of money. Prior to the Civil War, private banks used to issue dollar notes that were convertible into gold. Scholars debate whether the banking crises and panics during this period were a product of government regulation of these currencies and banks. What is clear is that there was never a completely free market in currency issuance. Well, the time has come.



To believe that the Fed is necessary is to believe that money is such a special product that it is optimal to give power to a group of expert economists either to use their best collective judgment in setting policy or to remove their discretion and create certain rules for them to administer. The only alternative is to eliminate government control of money.



Let’s examine these beliefs. Currently, Fed policy is essentially targeting low inflation. There are discussions about whether there is sufficient productivity growth to allow a high rate of GDP growth but the bottom line is protecting against an increase in inflation. While the target is not explicit, I would argue that markets are free enough and evolved enough to force the Fed towards this implicit target. 



The Fed controls the spot interest rate market (Fed funds) by creating or destroying spot dollars. All the other points along the money market yield curve are more or less determined without government interference. If the market feels the Fed is being lax on inflation, all the non‐​spot money market rates will rise and spot dollars will be sold for inflation hedges like commodities and foreign currencies. These trends will continue until the market catches the Fed’s attention and the Fed funds rate target is increased.



A more efficient policy would be to announce an explicit inflation target. With the continued development of the inflation protected bond market, which trades on real rates, this policy could be accomplished by explicitly targeting the market’s expected inflation rate (a first order approximation is calculated by taking the geometric difference of real rates from the nominal rates of comparable bonds). The Fed could choose the means by which to affect the markets. But this still leaves the problem of what inflation rate to target and how to define an inflation index.



It is arguable whether stock and real estate prices should be included in an inflation index because price bubbles are destabilizing and an indication that monetary policy is too loose. There certainly are non‐​linear effects in markets that can cause self‐​reinforcing trends that push markets away from their fundamentals. But bubbles, at best, can only be defined after the fact. Like an Oliver Stone movie where lack of evidence is an indication of the strength of the conspiracy, prices continuing to go up are evidence of the increasing size of the bubble! The term is meaningless to describe current conditions because it can’t be disproved; prices go up because there is a bubble; prices go down because there was a bubble, either outcome is proof of the existence of a bubble. So let’s stick with CPI.



Changes in the CPI can occur for non‐​monetary reasons (i.e., events independent of too much or little money creation). If the Fed is going to target CPI and minimize inflation due to monetary policy it has to be able to adjust for these non‐​monetary events. Central bankers are happy (and correct) to overlook price rises from supply shocks, such as droughts. The flipside is that price drops due to supply shocks coming from productivity growth also need to be adjusted for. If some innovation allows the cost of producing widgets (of equal quality) to drop the credit should not go to monetary policy. 



So the ability to measure productivity is critical regardless of whether an inflation target is explicit or implicit. Even with the way current Fed policy is formulated, productivity is at the core of the debate over the appropriateness of policy. Higher productivity growth means the economy is running more efficiently and can therefore run faster without increased inflation. But the technological and financial advances that we have seen and will continue to see (if the government can stay somewhat out of the way) are quickly transforming our ability to define let alone measure productivity. 



In the economy of the 1950s productivity was easy to measure as it was largely a function of widgets produced per man‐​hour. Today more and more firms are valued not for how many widgets they produce but for the quality of the ideas they produce — their intellectual property. These ideas may not generate any revenue today but in the future, and can therefore only be calculated today by looking at a firm’s market capitalization, a highly volatile measure. (This is not to argue that a firm’s market value is a perfect measure but that a priori it is the best unbiased measure available.)



Consider a firm whose sole function is to develop patents for licensing. The only quantitative measure of their output will be the number of patent applications generated per man‐​hour. However, what is important is the economic value of these patents, which may not correlate with the number of patents generated. This economic value can only be ascertained by seeing what value the market gives to the company.



While there may not be a lot of pure research companies like this, these types of companies effectively reside within virtually every tech firm. The research aspect of a tech firm is perhaps its most vital part because the trade secrets and patents produced are the engine of its future growth.



Many would be happy to sweep these issues under the table because, in their view, the economy is doing great and the Fed deserves much of the credit. In other words, if it ain’t broke don’t fix it. After all, the stock market has had an incredible run (despite being down some this year), the dollar is strong, inflation is seen to be under control, and the last time unemployment was this low Bill Clinton was busy not inhaling. Going back to the start of 1995, the S&P 500 has more than tripled and GDP growth has been 4.1 percent (go back further and this rate drops).



But as good as things have been, they fall short in comparison with the average GDP growth of over 5 percent in the hundred years prior to 1971, when Nixon closed the gold window and floated the dollar. This may not sound like much of a difference but it means that national income would have been more than 25 percent higher during these past five years had the economy been growing at a historically average rate.



Additionally, while inflation is low, it isn’t zero (even considering any and all measurement biases). So our money is still being debased, albeit at a slow pace. Given the tremendous technological advances and the subsequent increases in productivity, we should really see falling prices if the dollar were truly maintaining its usefulness as a store of value. Not a 1930s‐​style severe deflation that was caused by a variety of bone‐​headed fiscal and monetary policies that squashed both demand and supply. Just a steady, modest price decline reflecting productivity growth adjusted for quality improvement in products and services, where even products whose quality stays the same become cheaper to produce, raising the standard of living for all.



This recent period has also had many advantages over the prior period. With the Cold War over and defense spending being cut, valuable capital and human resources have been deployed in far more productive activities. Financial markets have evolved to be far more efficient, sophisticated, and global than a generation ago, allowing capital to be allocated more efficiently.



U.S. corporate performance has reaped the benefits of the restructurings during the 1980s. Granting employee options is now prevalent. This gives both better incentives to workers and provides a tax‐​advantaged way of compensation resulting in a far more efficient way to pay staff.



Yet despite these advantages we can’t seem to match the growth rates of the past. Of course this foregone income cannot all be chalked up to the failings of monetary policy. Certainly the enormous increases in taxes, regulation and government spending bear much of the blame.



But a truly market‐​based monetary policy might not have been such a willing accomplice to the increasing encroachment of government into the private sector. Without the implicit inflation tax and monopoly profits from the Fed, government would be forced explicitly to raises taxes (politically difficult) or to lower spending and deregulate. If the latter path is accompanied by tax cuts, experience shows that the economy benefits. In other words, the dynamic of tighter monetary policy (to eliminate current inflation) hurting short‐​term growth can be completely offset by fiscal policy. (Shockingly, many still hold to the Keynesian notion that increasing spending is the fiscal antidote to slowing growth when, in fact, it will exacerbate poor economic conditions by displacing the market’s more efficient allocation of resources. Japan in the 1990s is the latest example of this although they suffer under many other bad policies too.)



There’s certainly reason to have confidence that competent Fed officials will be able to avoid a sustained resurgence of inflation. It is also clear, as shown above, that there are many reasons to believe that we should be doing better and that therefore monetary policy is not optimal. The simple fact is that it is not just hard but impossible to tell what exactly the right policy should be at any given point in time. Economies are just too dynamic and are composed of too many players. No matter how smart members of the Fed may be, it makes no more sense to have them set monetary policy that it did for elite economists in centrally planned economies to set the price of food, cars or any other product.



Even if it were possible for Fed members to have completely mastered the art of central banking, is it healthy to vest so much power in one person or group of people? Imagine how the markets would react if Greenspan should have a very sudden demise.



So, how do we solve this problem of imperfect people using imperfect data creating a one size fits all policy? The first step is to remove discretion over policy from the government. The explicit targeting of inflation, previously mentioned, accomplishes this except that setting the goal would still be discretionary and likely to be sub‐​optimal given productivity measurement problems. An alternative is a return to a gold standard as way to effectively automate monetary policy. Despite its popularity in some quarters there are serious problems with any type of gold standard.



Go ahead and ignore the fact that no country has ever been able to maintain a gold standard (perhaps with Hong Kong and Argentina both having kept their currencies fixed to the dollar even during periods of severe economic distress, it is an indication that times have changed). The main flaw in fixing the dollar price of gold is that it assumes that the value of gold doesn’t change, like some physical constant, as immutable as the maximum speed of light. Gold’s value comes in two parts. First as a commercial product with limited use as an industrial metal and as jewelry or ornamentation. Clearly, the price of any commercial product will vary for a variety of reasons having nothing to do with an economy’s general price level.



The other component of gold’s value derives from its long history as a store of value and is wrongly assumed to be intrinsic. The supply of gold varies as new mines are found and mining techniques improved. Demand for gold as a store of value is based on the ability of gold to compete against other liquid stores of value. So, the only advantage of gold over a fiat currency is that the production of gold is not controlled by the government (ignoring taxes and regulation).



But if we fix the dollar to something that floats are we accomplishing anything? If the supply of gold increases due to a new way to mine, the dollar price of gold needs to fall. (As an example if the supply of gold doubled as a result of productivity boosts and there was no change in the productivity of producing other goods, the dollar price of gold should be halved as to protect the purchasing power of the dollar.) If the dollar price of gold is held constant, as in the gold standard, inflation will ensue (which is historically what happened under these conditions). 



Irving Fisher attempted to solve this problem with his compensated dollar plan. The plan allows for the dollar price of gold to be adjusted by a CPI priced in gold. The problem is that this plan only works if productivity in the mining sector is the same as in all other sectors of the economy. Not a bad assumption, perhaps, in Fisher’s day but unlikely to be true now and even less likely in the future. Two other possible solutions are to let the price of gold vary based on the relative productivity of gold production to productivity in the rest of the economy or instead of fixing the price of gold, fix the dollar price of gold plus other commodities (i.e., define the dollar as some fixed basket of commodities).



Well, the first solution is pretty messy, as it would require the government to calculate productivity rates that as argued above are inherently impossible to measure precisely. Furthermore, even if productivity could be measured precisely, it could in practice only be measured with a time lag. Not very helpful for knowing today’s price of something.



The second solution sounds more workable but determining the composition of the basket that would define the dollar is tricky. Leave something out of the basket and you have the same problem of the relative production productivities of what’s in and out of the basket. So just about everything the economy produces must go in the basket (obviously there are diminishing returns to accuracy for each additional product added, so you start with the most important products and work towards things that are used less). Now with many products in your basket you need to determine relative weights to reflect relative usage and importance of these products. But the resultant basket is just the same as that used to calculate inflation (with the identical problems as mentioned earlier)! Linking the dollar to this basket is exactly the same as having the Fed explicitly target zero inflation. 



The key to getting better monetary policy is not to merely limit the discretion of the government but to get the government out of the money business altogether by privatizing the Fed. Sell the whole thing to the highest bidder. The government would also have to deregulate enough to allow competitors to arise in the currency issuance game. 



By eliminating this government protected monopoly, more of the informational value of consumers ever changing preferences and behaviors could be used by producers to create a product that can best balance everyone’s needs. The free market can be looked at as a computer, calculating within all the constraints and using the near infinite amount of interrelationships and feedback between economic actors, on a global scale, to solve the problem of what are the best forms of money. This is an impossible calculation to do any other way.



Consumers would choose between U.S. dollars, euros, Citi dollars, GE dollars, etc. This choice would be based on confidence in the issuer and how well the product serves the consumer’s needs. Companies would issue money solely as a means to profit. Produce too much money and it becomes worthless, too little and not enough people will be able to use your money for you to profit. The notion of a government having a monetary policy goes the way of governmental industrial policy (or choosing which firms receive government help — see Asia). 



If private firms were allowed to compete on equal legal footing as a private Fed, currency competition would lead to better money just as market forces improve the quality of any product. In an unfettered environment, using precious metals as a backing for a currency is just a starting point. Nobody can predict the improvements and ingenious ideas that would emerge. Already consumers have reaped benefits from quasi‐​currencies like airline miles and credit card rebates in the form of products (clearly, these things don’t currently have the liquidity to make for a good form of money). 



There are usually several objections to private money. It is argued that it is necessary to have a lender of last resort in times of crisis and that only the Fed can fill this role. Of course, there is no reason why a private central bank couldn’t provide sufficient liquidity during a crisis and no reason why private regulators couldn’t get big financial institutions together to provide liquidity (J.P. Morgan did this in the panic of 1907, prior to the Fed’s creation). Also prior to the Fed’s creation, private clearinghouses would lend to members who were solvent but needed liquidity. Furthermore, using the Fed as a lender of last resort creates moral hazard. If the perception is that the Fed will be compelled to act in a crisis, the effect will be to allow participants to take more risk, as they believe they are receiving some downside protection from the Fed.



In 1998, a financial crisis that started in Asia threatened world markets. “Contagion” was used to describe the effect of countries with healthy economies seeing their markets roiled. The contagion spread through trade and capital flows between countries and from the market realizing countries with a certain economic profile (high current account deficits, low foreign exchange reserves, and a pegged currency) were vulnerable. Markets in developed countries became infected as credit spreads widened and volatility increased (a mathematical implication being that correlations between markets increase). To extend the biology analogy, the best defense for a population against mutant viruses is genetic diversity. The economic equivalent is the diversity of products and regulation that spring forth from a free market. This diversity would reduce the frequency of crises that might need a lender of last resort.



Another concern about a world with only private money is that things would be too complicated with all the exchange rates that would exist for these new currencies. Technology can easily solve this problem. Only want to see prices in one particular currency? Your handheld device or browser could automatically convert all prices and even facilitate conversion of your currency to one that would be acceptable to a merchant. Currencies that were too volatile would quickly fall into disuse, as part of their utility would always derive from them being a stable store of value.



As with any free market reform, there is no expectation that private money would lead to a perfect world where there are no crises or problems, just a better world. Better not just in a strictly utilitarian sense but also in a moral sense as people could store the fruits of their labor however they see fit and not be forced to submit to a tax (inflation) that is not explicitly levied and voted on.



Many involved in the information revolution are confident that with the help of new technologies, markets will continue to evolve and reach a point that government’s ability to tax and regulate will be stifled. What these optimists miss is that government too has the ability to evolve and counter these trends. Government’s monopoly on the use of force can trump any market innovation. Ultimately, it is necessary for the political climate to change before government will acquiesce and not try to fight the liberalizing effects of technology.



These changes are certainly not around the corner, but they should be our goals. 



Deregulate. Privatize. To improve our standard of living, to increase the level of freedom, and to set a powerful example for the world to follow. 
"
"
Share this...FacebookTwitterThe Portuguese sceptic site Ecotretas has a piece today called Not so fine Mr Gore which features a video clip of Al Gore making a speech before a crowd, predominantly businessmen. Gore seems to think that Portugal’s economy is on the right track and is coming out of recession. Far from it. In fact, Portugal’s economic woes are worsened by Gore’s vision for the planet.
So Ecotretas reminds Gore:
Someone should tell Al Gore that Portugal is not coming out of the Great Recession. In fact, Portugal’s economy is getting worse, as can be seen by the high sovereign risk, usually one of the highest 10 in the world in the last months. This is so because major bad economic decisions have been made in the last years, namely in alternative energy.
The total cost for Portuguese consumers and taxpayers is estimated at 700 million euros this year. But costs are growing exponentially.
Just like Spain, we’re going down, while unemployment keeps going up, and the promised green jobs are one of Europe’s lowest. So, Mr. Gore: If some Portuguese told you that “I feel fine”, I can bet he felt like the farmer in your nasty story!
Gore delivers a humorous story to make a point. Probably a lot of scientists feel like the poor farmer when asked: “Is your data, which show man is causing climate change, fine?”
Share this...FacebookTwitter "
"

Former vice president and Oscar winner Al Gore is scheduled to testify to both House and Senate committees today about global warming. For the past few years Gore has traveled across America speaking to audiences that range from friendly to worshipful, from journalists in New York and Washington to actors in Hollywood. If he has ever faced skeptical questions, it hasn’t been reported.   
  
  
We have several times invited the former vice president to present his famous slide show at the Cato Institute, in conjunction with a slide show prepared by Patrick J. Michaels, who takes a more benign view of climate change. Michaels is senior fellow in environmental studies at the Cato Institute and research professor of environmental sciences at the University of Virginia. He is the state climatologist of Virginia, a past president of the American Association of State Climatologists, and an author of the 2003 climate science “Paper of the Year” selected by the Association of American Geographers. His research has been published in major scientific journals, including _Climate Research, Climatic Change, Geophysical Research Letters, Journal of Climate, Nature, and Science_. He received his Ph.D. in ecological climatology from the University of Wisconsin at Madison in 1979. His most recent book is _Meltdown: The Predictable Distortion of Global Warming by Scientists, Politicians, and the Media, _which has been number one on Amazon’s global warming bestseller list for months at a time and has been reprinted twice this year.   
  
  
Gore’s office has declined our invitations. If Vice President Gore is committed to public understanding of climate change, why will he not demonstrate to a Washington audience composed of both supporters and skeptics that his ideas can carry the day in a dialogue with a leading critic? He wiped the floor with Ross Perot; does he fear that the case for catastrophic climate change is not as strong as the case for NAFTA?   
  
  
The invitation is still open. Mr. Vice President, please come to the Cato Institute and present your slide show to an audience of journalists and scholars with a knowledgeable climate scientist also on the dais.
"
"
Map from the University of Alabama-Huntsville. Each contour represents 0.2 degree C per decade warming or cooling between Dec. 1979 and Nov. 2008
From the USA Today Weather Blog
This has been in my inbox for a couple of weeks, so on a  fairly quiet day for weather, I thought I’d put this out there. John Christy of the  University of Alabama-Huntsville reported earlier this month that the  Earth’s climate change over the past 30 years has been rather uneven: It’s  gotten much warmer in the Arctic and, at the same time, cooler in the  Antarctic.
Christy and his colleague Roy Spencer, who are known in some quarters as global  warming skeptics, use data from satellites to measure the temperature of the  Earth. The more well-known NASA GISS and  National Climatic Data  Center data sets primarily measure surface temperatures.
Overall, Christy found that Earth’s atmosphere warmed an average of about  about 0.72 degree F in the past 30 years, according to NOAA and NASA satellites.  More than 80 percent of the globe warmed by some amount. However, while parts of  the Arctic have warmed by as much as 4.6 degrees F in 30 years, Christy says  that much of the Antarctic has cooled, with parts of the continent cooling as  much as the Arctic has warmed (see map, above; click to enlarge).
“If you look at the 30-year graph of month-to-month temperature anomalies,  the most obvious feature is the series of warmer-than-normal months that  followed the major El  Nino Pacific Ocean warming event of 1997-1998,” says Christy. “Right now we  are coming out of one La Nina Pacific Ocean cooling event and we might be  heading into another. It should be interesting over the next several years to  see whether the post La Nina climate ‘re-sets’ to the cooler seasonal norms we  saw before 1997 or the warmer levels seen since then,” he says.
He adds that most of the warming found in the satellite data has taken place  since the beginning of the 1997-98 El Nino, and that Earth’s average temperature  showed no detectable warming from December 1978 until the 1997 El Nino.
Meanwhile, the Washington  Post reported yesterday that the USA “faces the possibility of much more  rapid climate change by the end of the century than previous studies have  suggested, according to a report led by the U.S. Geological Survey.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a221b04',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThey all admit the IPCC is flawed and in need of an overhaul, yet they still insist the science is correct. Go figure. That’s pure rubbish, of course. Bad process = bad product. 
It sounds just like the National Academy of Sciences reaching the conclusion that Michael Mann’s hockey stick science had no value, but his answer was still correct.Here are the excerpts from some of the leading online papers in Germany, Austria and Switzerland. (I’ve also added some UK links below, h/t: http://thegwpf.org/
 SÜDDEUTSCHE  ZEITUNG in Germany (a favorite of Stefan Rahmstorf) writes:
Obligation to be more open
An examination of the IPCC reveals: The IPCC has to change the way it works. Yet, there’s no basis for the foaming attacks on its results.
DER STANDARD  in Austria writes:
In the expert team’s assessment, they recommend formulating stricter scientific guidelines for handling data on climate change. Forecasts and projections should be made only based on solid scientific evidence.
Sounds good. But if that were to be implemented, the entire IPCC 4AR would be reduced to only 2 pages:  a front and back cover.
DER SPIEGEL in Germany writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Consequence of the crisis: Experts urge an overhaul of the UN IPCC. They harshly criticised the work of the leadership. Not only the leadership, but also the scientific work of the climate panel is in need of reform.
 DIE ZEIT in Germany writes:

IPCC to be a reform project
Flawed data, hacked e-mails: The doubts about the IPCC were enormous. Now the UN draws a conclusion: Its work was correct, but its credibilty must be strengthened.
 DIE WELT  in Germany writes:
The IPCC which has been under fire because of series of follies has to be comprehensively reformed in order to remove doubts about its credibility. A report presented by the UN-appointed experts has reached the conclusion that a ‘fundamental reform’ of the IPCC is needed in order for it to strengthen its scientific standards and organisation.
NZZ in Zurich has a more lengthy piece, and writes:
The Interacademy Council (IAC) said critics were only partially right. In a report presented to the UN in NewYork on Monday, the IPCC was attested as having done, on the whole, good work. But it is criticised that its management structures and public relations work of the IPCC did not fulfill the requirements of today.
From the UK, h/t : http://thegwpf.org/
The Sun: UN ‘lacks Solid Evidence’ in Climate Warnings
Daily Express: Climate Change Lies Are Exposed
BBC News: Stricter controls urged for the UN’s climate body
The Times: Climate chief under pressure to quit after report on glacier blunder
The Guardian: Rajendra Pachauri, head of UN climate change body, under pressure to resign
The Independent: IPCC feels the heat as it is told to get its facts right about global warming
Share this...FacebookTwitter "
"
More anecdotal weather news of a colder and more brutal winter.

From the Juneau Empire, Juneau Alaska
Bitter cold moves in to Interior – Temperatures could drop to 50 below zero in parts  of Alaska
Meanwhile, in other news: Roofs collapsing due to record snows in Spokane, WA

FAIRBANKS – Bitterly cold weather slid over from Canada and settled into  Interior Alaska with forecasters saying temperatures could continue to slide to  nearly 50 degrees below zero in coming days.
Over the weekend, the mercury at Fairbanks International Airport dropped to  39 degrees below zero. Areas in the Interior outside the city were even colder;  46 below on the Yukon Flats, 41 below in Fort Yukon and 44 below in Central,  according to the weather service.
Rick Thoman, lead forecaster at the National Weather Service office in  Fairbanks, said temperatures rose a few degrees on Sunday, but that was it.
“The temperature will probably continue to go up and down randomly,” he said.  “With no clouds and no wind on the valley floor, temperatures are pretty much  probably going to be stuck.”
Fairbanks had experienced a relatively mild winter prior to Christmas. It had  only dropped to 30 below once, in early December.
The howling winds and frigid weather were too much for several mushers,  including four-time Iditarod winner Jeff King and his dog team, who pulled out  of the Gin Gin 200, a 200-mile race along the Denali Highway.
For the men, Brent Sass came in first, ahead of more well-known mushers such  as four-time Yukon Quest and two-time Iditarod champion Lance Mackey who was  fourth.
Mackey, resting Monday at the lodge in Paxson, said it was blowing so hard  and the teams were getting so turned around by the wind that it almost made him  laugh.
“It was almost comical. Your sled was going sideways down the road,” he said.
Further down the trail, when temperatures dipped to 50 below, it wasn’t so  much fun, he said.
“There were a lot of people not wanting to put their teams through that,”  Mackey said. “It is all about the dogs in a situation like this… They get  hardened by this stuff. That is why we do it.”
For the women, defending champion Jodi Bailey of Chatanika came in first.
Several mushers pulled out of the race from Paxson to the MacLaren River  Lodge.
“It was a real challenge this year,” Bailey wrote on a friend’s Facebook  page. “Winds like a banshee, and killer cold, wow am I glad to be back in  Paxson!!!”
According to the race Web site, temperatures at the MacLaren River Lodge were  between 35 and 40 below. It was reportedly 10 to 15 degrees colder on the lower  portions of the trail during the second portion of the race.
In Southeast Alaska, at least 20 inches of snow fell in Ketchikan, forcing  the shut down of the Ketchikan International Airport for a few hours. The  airport shut down at about 1:30 p.m. Sunday due to the heavy snow.
“We’ll stay here all night and dig out,” airport manager Mike Carney said.
The airport reopened Monday and normal operations resumed.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99e844af',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSometimes timing is everything. Yesterday we looked at how a climate scientist is busy making “masterplans” for transforming our society instead of studying the climate.Some people were not happy about my reminder of where this sort of dubious activity can lead. After all, planning societies and claiming absolute truth is not the job of  scientists, especially those as dogmatised as Schellnhuber and those at his Potsdam Institute For Climate Impact Research.
Organising society is best left to democracy, where everyone’s vote is equal, and must be so – no matter how ignorant Schellnhuber thinks the population is. It’s not perfect, but it’s the best system we’ve got. “Masterplanners” in history have invariably led to disasters. And, as much as people do not want to be reminded of it, a not so little concentration camp in Poland is an example in the worst extreme.
Today, the worst part is that all these “masterplans” are based on flaky, often times fraudulent and alarmist science – all designed to promote panic rather than reason. This has already led to disastrous results (biofuel induced global hunger or landscape desecration by windmills to name two) and will surely lead to even greater disasters.
Another example of a so-called masterplan that has just come to our attention is the latest European proposal to ban all fossil-fuel-powered cars from cities by 2050 – again all this with little or no thought about the potential conseqeunces this junk-science-based regulatory hyper-zealousness could have. The UK Telegraph here writes:
Cars will be banned from London and all other cities across Europe under a draconian EU masterplan to cut CO2 emissions by 60 per cent over the next 40 years.
These masterplans are designed to regulate and control our lives, and have nothing to do with saving the planet – all confirmed by Siim Kallas, the EU transport commission, who said of the masterplan:
Action will follow, legislation, real action to change behaviour.”
The Association of British Drivers reacted harshly to the proposed restriction on mobility, and rightly so. Hugh Bladon, a spokesman for the BDA said:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I suggest that he goes and finds himself a space in the local mental asylum. If he wants to bring everywhere to a grinding halt and to plunge us into a new dark age, he is on the right track. We have to keep things moving. The man is off his rocker.”
This can be said about all the social engineering master planners out there who have taken it upon themselves to tell the rest of society how to behave.
But it doesn’t stop there. The transportation masterplan also includes air transport. The plan provides for the end of cheap flights and has the target of forcing more than 50% of all journeys above 300 km to be done by rail.
Air travel is also a big target of the enviro-zealots. So it should not come as a surprise that a new study is just out claiming that air travel is “even worse for the climate than they previously thought” , this reports the Austrian Der Standard, which leads with:
Scientists calculate the impacts of air traffic in the sky and come to unexpected results. Vapour trails from aircraft and their impacts apparently have a far greater impact on climate than previously thought.”
The scientists who claim this are Ulrike Burkhardt, of the German Centre For Aviation And Aeronautics, and Bernd Kärcher – in a paper published yesterday in Nature titled: Global radiative forcing from contrail cirrus. In the abstract the authors claim:
We show that the radiative forcing associated with contrail cirrus as a whole is about nine times larger than that from line-shaped contrails alone. We also find that contrail cirrus cause a significant decrease in natural cloudiness, which partly offsets their warming effect. Nevertheless, net radiative forcing due to contrail cirrus remains the largest single radiative-forcing component associated with aviation.”
The timing of this contrail paper and the EU masterplan for transportation just couldn’t be more convenient.
Share this...FacebookTwitter "
"

My youngest daughter, Abby, is graduating from college in a month. So like most university seniors, she’s been in the throes of a job search. After an interview in Boston two Fridays ago, she was sitting at a gate in Logan Airport waiting for a flight to D.C. (She still likes to visit her parents!) When the aircraft arrives, who should alight from it but Elizabeth Warren. As she walks through the gate area, the waiting passengers proceed to stand up and give her an ovation, calling out, “Thank you, Senator!” and “We love you, Senator!” Oh, boy. I’m proud to say Abby’s voice was dripping with disdain when she told me this story. And, no, she didn’t join in the ovation.



How I wish people — of all political and ideological persuasions — wouldn’t have such misplaced faith in politicians. One of the great lessons I learned from Cato, in my many years as a donor, is that it’s the power and advancement of ideas that will create positive change in our world and build a free and prosperous society. The outcome of elections and the machinations of politicians alone won’t do it. And so much of our politics is partisan tribalism: both Democrats and Republicans support elected officials of their respective party even when they abdicate on issues that would appear core to that party.



I have plenty of Republican friends who defended George Bush for years despite the out‐​of‐​control spending and growth of government under his administration, including initiatives that would have left them outraged had a Democratic president been responsible. Steel tariffs, No Child Left Behind, Medicare Part D, and TARP are just a few examples.



And where are the angry Democrats protesting the sorry civil liberties record of the Obama administration? PATRIOT Act abuses such as national security letters and warrantless eavesdropping or data collection got them exercised when Bush was president — today, their silence is deafening. And partisans on each side seem to believe in “executive power for me, but not for thee.…” There have been frightening grabs of presidential power under each of the last two administrations. This has elicited complaints from both sides of the aisle: but, with few exceptions, from the left only of Bush, and from the right only of Obama.



As believers in markets, we know people respond to incentives. If partisan voters don’t insist that the people whom they elect adhere to principle, why would they? As a result, no matter which party holds power, the results are similar: too much spending, too much regulation, an unbridled Federal Reserve, a bias toward military intervention, and too little respect for civil liberties. In fact, a friend of Cato’s once shared a brilliant analogy. During campaigns, we hear from the marketing departments of each party, and they sound very different. But with a handful of key exceptions we get similar results when they’re in power: he speculated that they must each outsource to the same place when governing!



And let’s not have too much faith that simply pitching bad leaders overboard will change things dramatically for the better. Last summer, we were paid a visit here at Cato by Kim Kataguiri, an impressive and courageous 20‐​year‐​old Brazilian who has catalyzed the protests against President Dilma Rousseff and the drive for her impeachment. It will be wonderful to see a corrupt and ineffective leader get the comeuppance she deserves. But the vice president faces corruption charges, too, so his ascension wouldn’t likely change much. Rather, it will take a change of values to transform Brazil and allow it to reach its potential.



Politics is ultimately a necessary ingredient for the world to move in the direction we want. But a country and a world steeped in liberty can’t be accomplished politically without changing the terms of the debate and the climate of ideas: precisely Cato’s role. Scott Rasmussen once spoke at a Cato event, and contended that politicians only follow — and don’t lead — the rest of the country. It is the very contempt in which citizens hold the political class that made him optimistic about the future despite the current policy environment. It’s our job to continue making the compelling case for freedom through the media, in the academy, and to the policy community. Our objective is to lead policymakers in the direction of liberty. Only when they get there will they deserve ovations.
"
"
Share this...FacebookTwitterThe Fukushima nuclear reactor crisis has provided anti-nuclear activists with an assortment of (unsubstantiated) scare stories for turning the tide of public opinion against nuclear energy. So it’s little wonder that few places have been so gripped by hysteria as Germany, home to a large number of eco-stalinists and chronic doomsayers in the media.
German warmist website klimaretter.de (climatesaviour.de in English) writes that this however is not the case in Finland. Despite Fukushima, Finland is still very much in favour of using this plentiful source of zero-emissions energy. Klimaretter writes:
Despite the bad experiences encountered building the Olkiluoto 3 reactor– which will be finally finished 4 years later than planned and at double the cost in 2013 – the government approved the new construction of two additional nuclear reactors. And approved is approved, ways Mari Kiviniemi, head of the government.”
And why not? Nuclear power has been the safest of all sources of energy so far, as I mentioned in a previous post, renewable energy sources have many more deaths per terawatt-hour of electricity produced than nuclear. The dangers of nuclear power are mostly hype, and nothing to do with reality.

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
The Finnish government reminds us that in Finland there is practically no danger of earthquakes or tsunamis. And klimaretter reminds us:
The emergency power supply in the event of a disruption of the external power source für is already with the old reactors are designed twice as safely as the Japanese damaged reactors.
One thing that the panic purveyors seem to forget is that there are concepts in engineering and design called “applying lessons learned” and “continuous improvement”. No doubt the Japan earthquake exposed the weaknesses of the older Japanese reactors and safety systems. ´But in the end, this will be a blessing in that the lessons learned can now be implemented into the design of the new generation reactors, thus making the safest source of energy out there even safer.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Silly, desperate argument: “What if a plane crashes into it!”
This is a silly argument because planes could crash into anything, and so then nothing ought to be built. What if a plane crashed into a hydro-electric dam? A chemical plant? A football stadium?
Like anything else, all we can do is reduce the risk. And the best way to reduce risk in energy supply is to use the system that has proven to be the safest. That choice is obvious.
Thanks to reader DirkH for this video of a test plane colliding into a conrete wall at 500 mph. Risk dispelled!

Share this...FacebookTwitter "
"
Earth Hour in California – Success or Bust?


Guest post by Russ Steele, NCWatch
At our house we set the timer to remind us to turn on all the visible out side lights.  We have multiple security lights on the garage and the barn that come on when the sun goes down. My friend George Rebane has evidence that he turned on his lights for Earth Hour at Ruminations. I should have done the same, but was working on a sea level issue in R and forgot. I am glad I set the timer to remind me to turn off the outside house lights at 9:30.
The real question is did it Earth Hour make a difference one way or the other?
Roger Sowell had a good idea, he download the the graph below from www.caiso.com, the California Independent System Operator.  CAISO is in charge of receiving power from power generating plants, and distributing the power throughout the state grid to the various end users.
California power use 3-28-09 from CAISO  - Click for a larger Graphic 
Now compare the graph from Saturday 3/28/09 to the one on Sunday 3/29/09 shown below, note the similar slopes during the same time period. Note that annotations were added by Anthony Watts on both graphs.
California power use 3-29-09 from CAISO  - Click for a larger Graphic 
Roger notes:
The light gray line is the forecasted power usage, shown in Megawatts.  The red line is the actual power consumed.  Around 1900 hours, 7 p.m., the load was approximately 24,000 MW.  By 8:00, the load increased smoothly to just over 26,000 MW.  Then the load began a steady decrease right on through the night, ending at around 22,000 MW at almost midnight. 
 
There was no apparent decrease in the power load throughout the state, from 8:30 to 9:30 p.m.  No step changes, nothing, nada, zip, zilch.
There you have it, scientific data showing that the Earth Hour was a total bust in California.  If you look close, you can see a little bump up above the forecast demand, which tracked very closely with actual power consumed prior to the witching hour 8:30 to 9:30. But, it is clear that power consumption did not drop, it stayed up. Maybe all those protesters forgot to turn off the lights.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e975adbdb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

_Washington Post_ political reporter Colby Itkowitz writes:



During floor debate ahead of a vote on the Green New Deal, Sen. Mike Lee (R‐​Utah) told his colleagues that if they really want to address environmental concerns they’ll encourage people to couple off and have more babies. … This recommendation, to add more people to the planet, doesn’t track with science or reason. A 2017 research article determined that one way an individual could contribute to eliminating greenhouse gases is to have one fewer child.



That’s the nut from her snide web article, “Sen. Mike Lee Says We Can Solve Climate Change with More Babies. Science Says Otherwise.” _Post_ national correspondent James Hohmann deemed the article noteworthy enough to be the “Hot [Read] on the Left” in his “Daily 202″ newsletter.



Problem is, Itkowitz seems to not understand the point Lee was trying to make. Instead, she “talks past” him — something all‐​too‐​common in politics, but not something reporters should do. Worse, if history is a guide, her view is more likely to prove a‐​scientific and unreasonable than his.



Here’s the story: Last Tuesday, the Senate held a floor debate and vote on the so‐​called “Green New Deal” — which is to say the Senate engaged in silly Republican grandstanding over a silly Democratic proposal. As part of the debate, Lee delivered a floor speech that featured everything from a picture of a machine‐​gun‐​firing, bazooka‐​toting Ronald Reagan riding a velociraptor waving an American flag (no, really), to references to _Star Wars_ tauntauns and the Hanna‐​Barbera Aquaman’s giant seahorse, to _Sharknado 4_ and Austin Powers’ Dr. Evil. That is, Lee met double‐​silliness with more silliness.



One can argue that this was inappropriate for a Senate debate, especially on a serious topic like climate change. But then, comedy can be an effective means to truth.



Toward that end, at the conclusion of his speech Lee offers a serious point. Itkowitz selectively quotes it; here’s the complete section:



The Green New Deal is not the solution to climate change. It’s not even part of the solution. It’s part of the problem. The solution to climate change won’t be found in political posturing or virtue signaling like this. It won’t be found in the federal government at all.  
  
You know where the solution can be found? In churches, wedding chapels, and maternity wards across the country and around the world. This, Mr. [Senate] President, is the real solution to climate change: babies. Climate change is an engineering problem — not social engineering, but the real kind. It’s a challenge of creativity, ingenuity, and technological invention. And problems of human imagination are not solved by more laws, but by more humans.  
  
More people mean bigger markets for innovation. More babies mean more forward‐​looking adults — the sort we need to tackle long‐​term, large‐​scale problems. American babies, in particular, are likely going to be wealthier, better educated, and more conservation‐​minded than children raised in still‐​industrializing regions. As economist Tyler Cowen recently wrote on this very point, “by having more children, you are making your nation more populous — thus boosting its capacity to solve [climate change].“  
  
Finally, Mr. President, children are a mark of the kind of personal, communal, and societal optimism that is the true prerequisite for meeting national and global challenges together. The courage needed to solve climate change is nothing compared with the courage needed to start a family. The true heroes of this story aren’t politicians or social media activists. They are moms and dads, and the little boys and girls they are — at this moment — putting down for naps, helping with their homework, building tree houses, and teaching how to tie their shoes.  
  
The planet does not need us to “think globally and act locally” so much as it needs us to think family and act personally. The solution to climate change is not this unserious resolution, but the serious business of human flourishing — the solution to so many of our problems, at all times and in all places: fall in love, get married, and have some kids.



No doubt, the Utah senator’s comments were, at least in part, his own virtue‐​signaling to his predominantly Mormon constituency. But he draws on an important economic idea: at the margin, human beings have a positive effect on the world. Human ingenuity, hard work, preferences, and values create goods, and among those goods can be improved environmental quality. Julian Simon popularized this idea in his 1981 book _The Ultimate Resource_ , and it has been restated in recent years by, among others, Cowen in his new book _Stubborn Attachments_ (reviewed by Pierre Lemieux here), Bryan Caplan in his 2011 book _Selfish Reasons to Have More Kids_ , and my Cato colleague Marian Tupy and BYU economist Gale Pooley in their 2018 paper on the “Simon Abundance Index.”



Itkowitz claims “science and reason” say different. To justify that, she links to a 2017 paper that actually _doesn’t_ determine “that one way an individual could contribute to eliminating greenhouse gases is to have one fewer child,” but rather examines Canadian high school science textbooks’ recommendations for reducing greenhouse gas emissions. A possible strategy — one that _none_ of the textbooks recommend, which the paper’s authors lament — is to “have one fewer child.” That paper, in turn, cites a 2009 paper that estimates the carbon emissions resulting from an offspring, including a share of the subsequent emissions of that offspring’s descendants. Not surprisingly, that’s a big number, dwarfing the carbon‐​reduction benefits of such common strategies as recycling, switching from gasoline‐​powered cars to hybrids and electric cars, and upgrading lightbulbs. (My takeaway from the 2017 paper is how pointless many of the commonly advocated carbon‐​reduction strategies are.)



The 2009 paper is a mathematical modeling exercise under various assumptions, resulting in different estimated marginal “carbon legacies.” But that doesn’t show Itkowitz is right and Lee’s being foolish because the paper ignores Lee’s point about the effects of population change on innovation and living standards.



From tin shortages in the ancient world, to William Strong Newberry’s 1875 (yes, **18** 75) warning that the world was running out of oil, to Paul Ehrlich’s _Population Bomb_, to Jimmy Carter’s moral equivalent of war, population growth has placed humanity on the Malthusian edge of poverty and privation — or so we keep being told. But we never fall off that edge. In fact, we keep moving away from it: we grow fatter (alas), longer‐​lived, and more comfortable. The reason is simple: more people means more innovation and resource availability, which means a higher standard of living rather than the opposite.



That doesn’t mean humanity is guaranteed to find some easy, innovative way to cut greenhouse gas emissions. As my Cato colleague Peter Van Doren noted, both Lee’s optimism and Itkowitz’s pessimism are attitudes (probably correlated to their politics) rather than testable, scientific hypotheses. That said, history suggests it’s more likely that humanity will find innovative ways to cut emissions, or geo‐​engineer around climate change, or accommodate change, than that reducing (or government constraining) population growth will save us from a much warmer world — or that there will be no future environmental quality innovations.



All that said, some legitimate criticisms can be made of Lee’s remarks. Government _can_ be a useful tool for addressing externalities, just as it can also be a terrible tool. And there are plenty more examples of people showing the “courage” to start families than there are of policymakers showing the courage to address difficult policy problems (e.g., entitlements insolvency, government debt, pork and rent‐​seeking, better childhood education…) Those criticisms would have made good reading, as would a broader discussion of Malthus and neo‐​Malthusianism, government intervention, and population change. Unfortunately, instead of that, readers got was the equivalent of the aforementioned Reagan–velociraptor picture.
"
"
UPADATED AT 8:30AM PST Sept 2nd-
More on SIDC’s decision to count a sunspeck (technically a “pore”) days after the fact. NOAA has now followed SIDC in adding a 0.5 sunspot where there was none before. But as commenter Basil points out, SIDC’s own records are in contrast to their last minute decision to count the sunspeck or “pore” on August 21.
There is an archive of the daily SIDC “ursigrams” here:
http://sidc.oma.be/html/SWAPP/dailyreport/dailyreport.html
If you select the ursigrams for August 22 and 23, you get the reported data for the 21st and 22nd:
August 21:
TODAY’S ESTIMATED ISN  : 000, BASED ON 07 STATIONS.
SOLAR INDICES FOR 21 Aug 2008
WOLF NUMBER CATANIA    : 011
10CM SOLAR FLUX        : 067
AK CHAMBON LA FORET    : ///
AK WINGST              : 004
ESTIMATED AP           : 005
ESTIMATED ISN          : 000, BASED ON 14 STATIONS.
August 22:
TODAY’S ESTIMATED ISN  : 000, BASED ON 11 STATIONS.
SOLAR INDICES FOR 22 Aug 2008
WOLF NUMBER CATANIA    : 013
10CM SOLAR FLUX        : 068
AK CHAMBON LA FORET    : ///
AK WINGST              : 003
ESTIMATED AP           : 003
ESTIMATED ISN          : 000, BASED ON 11 STATIONS.
In both cases, the daily estimated “International Sunspot Number” based on multiple stations, not just the Catania Wolf Number, was 000. So how did SIDC end up with positive values in the monthly report?
UPDATED at 2:42 PM PST Sept 1st – 
After going days without counting the August 21/22 “sunspeck” NOAA and SIDC Brussels now says it was NOT a spotless month! Both data sets below have been recently revised.
Here is the SIDC data:
http://www.sidc.be/products/ri_hemispheric/
Here is the NOAA data:
ftp://ftp.ngdc.noaa.gov/STP/SOLAR_DATA/SUNSPOT_NUMBERS/MONTHLY
The NOAA data shows July as 0.5 but they have not yet updated for August as SIDC has. SIDC reports 0.5 for August. It will be interesting to see what NOAA will do.
SIDC officially counted that sunspeck after all. It only took them a week to figure out if they were going to count it or not, since no number was assigned originally.
But there appears to be an error in the data from the one station that reported a spot, Catania, Italy. No other stations monitoring that day reported a spot. Here is the drawing from that Observatory:
ftp://ftp.ct.astro.it/sundraw/OAC_D_20080821_063500.jpg
ftp://ftp.ct.astro.it/sundraw/OAC_D_20080822_055000.jpg
But according to Leif Svalgaard, “SIDC reported a spot in the south, while the spot(s) Catania [reported] was in the north.” This is a puzzle. See his exchange below.
Also, other observatories show no spots at all. For example, at the 150 foot solar solar tower at the Mount Wilson Observatory, the drawings from those dates show no spots at all:
ftp://howard.astro.ucla.edu/pub/obs/drawings/dr080821.jpg
ftp://howard.astro.ucla.edu/pub/obs/drawings/dr080822.jpg
Inquires have been sent, stay tuned.
Here is an exchange in comments from Leif Svalgaard.
——-
REPLY: So What gives Leif….? You yourself said these sunspecks weren’t given a number. I trusted your assessment. Hence this article. Given the Brussels folks decided to change their minds later, what is the rationale ? – Anthony
The active region numbering is done by NOAA, not by Brussels. The Brussels folks occasionally disagree. In this case, they did. Rudolf Wolf would not have counted this spot. Nor would I. What puzzles me is this:
21 7 4 3
22 8 4 4
The 3rd column are ’spots’ in the Northern hemisphere, and the 4th column are ’spots’ in the Southern hemisphere [both weighted with the ‘k’-factor: SSN = k(10g+s)]. But there weren’t any in the south. The Catania spot was at 15 degrees north latitude, IIRC. Maybe the last word is not in on this.
——–
Hmm….apparently there’s some backstory to this. There is a debate raging in comments to this story, be sure to check them. – Anthony
# MONTHLY REPORT ON THE INTERNATIONAL SUNSPOT NUMBER #
# from the SIDC (RWC-Belgium) #
#——————————————————————–#
AUGUST 2008
PROVISIONAL INTERNATIONAL NORMALIZED HEMISPHERIC SUNSPOT NUMBERS
Date Ri Rn Rs
__________________________________________________________________
1 0 0 0
2 0 0 0
3 0 0 0
4 0 0 0
5 0 0 0
6 0 0 0
7 0 0 0
8 0 0 0
9 0 0 0
10 0 0 0
11 0 0 0
12 0 0 0
13 0 0 0
14 0 0 0
15 0 0 0
16 0 0 0
17 0 0 0
18 0 0 0
19 0 0 0
20 0 0 0
21 7 4 3
22 8 4 4
23 0 0 0
24 0 0 0
25 0 0 0
26 0 0 0
27 0 0 0
28 0 0 0
29 0 0 0
30 0 0 0
31 0 0 0
__________________________________________________________________
MONTHLY MEAN : 0.5 0.3 0.2
========================================================
ORIGINAL STORY FOLLOWS:
Many have been keeping a watchful eye on solar activity recently. The most popular thing to watch has been sunspots. While not a direct indication of solar activity, they are a proxy for the sun’s internal magnetic dynamo. There have been a number of indicators recently that it has been slowing down.
August 2008 has made solar history. As of 00 UTC (5PM PST) we just posted the first spotless calendar month since June 1913. Solar time is measured by Coordinated Universal Time (UTC) so it is now September 1st in UTC time. I’ve determined this to be the first spotless calendar month according to sunspot data from NOAA’s National Geophysical Data Center, which goes back to 1749. In the 95 years since 1913, we’ve had quite an active sun. But that has been changing in the last few years. The sun today is a nearly featureless sphere and has been for many days:

Image from SOHO
And there are other indicators. For example, some solar forecasts have been revised recently because the forecast models haven’t matched the observations. Australia’s space weather agency recently revised their solar cycle 24 forecast, pushing the expected date for a ramping up of cycle 24 sunspots into the future by six months.
The net effect of having no sunspots is about 0.1% drop in the TSI (Total Solar Irradiance). My view is that TSI alone isn’t the main factor in modulating Earth’s climate. 
I think it’s solar magnetism modulating Galactic Cosmic Rays, and hence more cloud nuclei from GCR’s, per Svensmark’s theory. We’ve had indications since October 2005 that the sun’s dynamo is slowing down. It dropped significantly then, and has remained that way since. Seeing no sunpots now is another indicator of that idling dynamo.
Graph of solar Geomagnetic Index (Ap):

Click for a larger image
Earth of course is a big heat sink, so it takes awhile to catch up to any changes that originate on the sun, but temperature drops indicated by 4 global temperature metrics (UAH, RSS and to a lesser degree HadCrit and GISS) show a significant and sharp cooling in 2007 and 2008 that has not rebounded.In the 20 years since “global warming” started life as a public issue with Dr. James Hansen’s testimony before congress in June 1988, we are actually cooler.

Click for a larger image
Reference: UAH lower troposphere data
Coincidence? Possibly, but nature will be the final arbiter of climate change debate, and I think we would do well to listen to what it’s saying now.
Joe D’Aleo of ICECAP also wrote some interesting things which I’ll reprint here.
…we have had a 0 sunspot calendar month (there have been more 30 day intervals without sunspots as recent as 1954 but they have crossed months). Following is a plot of the number of months with 0 sunspots by year over the period of record – 23 cycles since 1749.

See larger image here.
Note that cluster of zero month years in the early 1800s (a very cold period called the Dalton minimum – at the time of Charles Dickens and snowy London town and including thanks to Tambora, the Year without a Summer 1816) and again to a lesser degree in the early 1900s. These correspond to the 106 and 213 year cycle minimums. This would suggest that the next cycle minimum around 2020 when both cycles are in phase at a minimum could be especially weak. Even David Hathaway of NASA who has been a believer in the cycle 24 peak being strong, thinks the next minimum and cycle 25 maximum could be the weakest in centuries based on slowdown of the plasma conveyor belt on the sun.
In this plot of the cycle lengths and sunspot number at peak of the cycles, assuming this upcoming cycle will begin in 2009 show the similarity of the recent cycles to cycle numbers 2- 4, two centuries ago preceding the Dalton Minimum. This cycle 23 could end up the longest since cycle 4, which had a similar size peak and also similarly, two prior short cycles.

See larger image here.
Will this mean anything for climate in our near future? Possibly.  But we’ll have to wait to see how this experiment pans out.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c91240c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterH/T: Rudolf Kipp at Skeptical Science (see first reader comment).
That is about the same as the US annual GDP. That’s the number reported in a recent article appearing in the online Frankfurter Allgemeine Zeitung FAZ), Germany’s leading political daily.

Europe set to climax with green bloody self-flagellation.
The FAZ writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In the coming 40 years, the EU must invest 270 billion Euros annually if it wants to reach its long-term climate targets. That’s the result of a draft of a strategy paper that Climate Commissar Connie Hedegaard wants to introduce in early March. In order to reduce greenhouse gas emissions by 80 to 95% by 2050, large-scale investments for the expansion of renewable energies, energy grid, CCS, passive homes, electric vehicles and modern industrial plants will be needed.”
What return will European taxpayers and citizens get for this investment? Maybe a theoretical, imperceptible o.1°C of reduced warming, along with a phony belief they saved the planet. Not even the Soviets could have managed a failure of such proportions. What better way to drive an entire continent into ruin could one possibly conceive?
But the EU seems to think this is all bearable. After all, it is only 1.5% of the EU economy – about the amount paid by the EU because of the financial crisis. And it’s the way to keep Europe in the leading position as the world’s climate protector.
But it is doubtful that even the most green-eyed bureaucrats believe these targets are achievable. So they added an escape clause in the whole thing. The EU will also be allowed to count CO2 reductions achieved in third countries. I’m not clear on what that means exactly, but to me it’s an open door to accounting tricks.
Still, it is an ambitious plan, especially amid all the conference failures of the past. Indeed rather than discouraging Europe, these global failures to agree on reductions in emissions have only emboldened Europe to get even tougher. FAZ writes:
The European Parliament and enviironmental activists have repeatedly demanded that the EU unilaterally commit itself to even tougher reductions in order to give climate change negotiations more impetus.”
What do the EU bureaucrats care? After all, it’s the EU citizens who are going to have to bear all the pain. The citizens be damned. And besides, they’ll be able to enjoy the benefits of an imperceptibly cooler 0.1°C and a feeling they rescued planet. So let the self-flagellation begin.
Share this...FacebookTwitter "
"**More than one-third of jobs in arts, culture and heritage are vulnerable as a result of coronavirus restrictions.**
That is according to a new study from Ulster University's Economic Policy Centre (UUEPC).
The report suggests a high proportion of jobs in museums, galleries, theatres and music are particularly at risk.
In general, the creative sector is ""more exposed to the challenges arising from Covid-19 than other sectors and occupations.""
The main factors for that are social distancing measures limiting capacity and a reluctance among audiences to return even when venues can re-open.
The UUEPC report estimated that there are 39,100 people employed in the arts, creative, culture and heritage sector in Northern Ireland.
However, the type of occupation that figure includes is very wide.
It ranges from people working in Information Technology (IT) and architecture to those employed in music, crafts, the performing arts, museums, galleries and libraries.
The study said that while a high proportion of jobs in areas such as IT were not vulnerable, jobs in the other sectors were ""much more at risk.""
It estimated that more than 60% of jobs in museums, galleries and libraries were vulnerable, along with almost half of jobs in music, theatre and visual art.
A significant number of jobs in film and TV production were also at risk.
""The pandemic has caused the immediate closure of non-essential business including the Arts, Culture and Heritage industries resulting in cancelled work and events such as large music events like Belfast Vital and Belsonic which attracted thousands of people to Belfast annually,"" the report said.
""The healthcare situation in NI will be more important in this sector than in the average NI occupation, given the interactive nature of work and a dependence on discretionary consumer spending.""
The authors of the report also make a number of suggestions on how venues and visitor attractions could be helped to recover from the impact of the pandemic.
They include ""visitor vouchers,"" which would subsidise 30-50% of the cost of tickets to venues to encourage audiences to return.
Venues would also be compensated if they had to cancel events at short notice due to new or changing restrictions.
The authors of the study also suggest a bursary of Â£1,000 a month for arts workers who have not been able to benefit from other job support schemes.
Northern Irish artists could also be commissioned to create new public art, the UUEPC report said.
A number of emergency funding schemes for arts and heritage have been opened by the Department for Communities.
The Northern Ireland executive received Â£33m from Westminster in July as part of a UK-wide support package for arts and culture."
"
Share this...FacebookTwitterBy guest writer
Ed Caryl
Arctic stations near heat sources show warming over the last century. Arctic stations that are isolated from manmade heat sources show no warming. The plots of “isolated stations” and “urban stations” below clearly illustrate the differences. 
Stevenson Screen, Verhojansk, Russia
All the GISS temperature anomaly maps show the Arctic warming faster than the rest of the globe, especially northern Alaska and Siberia, but the satellite data shows a different pattern. See the 2 charts for 2009 that follow. The GISS surface map:


Satellite chart:

The baseline period selected for the GISS surface temperature chart is the 1933 to 1963 Atlantic Multi-decadal Oscillation (AMO) warm period. This period more closely matches today’s temperatures than the default 1951 to 1980 cool period that GISS uses. The satellite data uses the average over the satellite period since 1979, the modern warm period.
The satellite data show cooling in central Siberia, similar to the surface anomaly map, and very little warming for most of Alaska. It also shows cooling for the Antarctic Peninsula, where the surface map shows warming. But there is a scattering of hot grid squares across the HISS surface station map for the Arctic. So what is going on?
I selected the stations that correspond to those warm grid squares, as well as other stations in the same latitudes. In this age of everyone carrying a camera posting all photos on the Internet, there is a lot of information available on these stations. For some I could locate the Stevenson screens, for most I’ve found pictures of the surroundings, while others have investigated many of these sites already, and so links to that research are included. I downloaded the raw temperature data from GISS for 24 stations closest to the North Pole, which are all classified as “rural”.
“Urban” Arctic Stations
Contrary to GISS claims, many of these stations are actually not “rural” with respect to their siting quality. Many are at airports associated with sizable towns or research stations with sizable staff and infrastructure. In the Arctic, any town of more than a few families can be a large heat source. In the case of many towns in Russian Siberia, “central heating” takes on a whole new meaning. These towns have a central power plant that provides electricity and steam heat to the whole town. Large pipes, both insulated and un-insulated, carry steam, water, and sewage, up and down the streets to and from each dwelling. These pipes cannot be buried because of the permafrost, so they are elevated, and at street crossings are elevated 4 or 5 meters. The temperature differential between these pipes and the surrounding air can be 140° C in winter, and even more for a pressurized system.
But GISS applies the same Urban Heat Island (UHI) criteria to all stations globally, regardless of the latitude or average temperature. They look at the satellite night brightness and population to judge whether urban or rural. By GISS criteria, all the stations in the high Arctic are rural; there are no corrections for UHI.
But let’s look at each of these “urban” locations. Each name is also a link to the GISS surface temperature raw data.
List of Urban Arctic Stations (see the annex at the end of this post for details on each station)
  1. Kotzebue, Ral (66.9 N,162.6 W), Alaska
  2. Barrow/W. Pos (71.3 N,156.8 W) Alaska
  3. Inuvik (68.3N, 133.5W) Inuvik, Canada
  4. Cambridge Bay (69.1 N,105.1 W) Nunavut, Canada
  5. Eureka, N.W.T. (80.0 N,85.9 W), Canada
  6. Nord Ads (81.6 N,16.7 W Northeast Greenland
  7. Svalbard Luft (78.2 N,15.5 E), Norway
  8. Isfjord Radio (78.1 N,13.6 E), Norway
  9. Gmo Im.E.T.(80.6 N,58.0 E), Russia
10. Olenek (68.5 N,112.4 E), Russia
11. Verhojansk (67.5 N,133.4 E), Russia
12. Cokurdah (70.6 N,147.9 E), Russia
13. Zyrjanka (65.7 N, 150.9 E), Russia
14. Mys Smidta (68.9 N,179.4 W), Russia
15. Mys Uelen (66.2 N,169.8 W), Russia
The following graphic is a temperature chart for 10 of the above stations (5 of the shorter ones were left out to avoid over-crowding). All are warming, some faster than others. Barrow, for which we have the UHI study, is not the fastest warming.
Temperature trends of the ""urban"" stations.
Isolated Stations
Now let us look at the isolated stations, which are located at similar latitudes like the above “urban” stations. One important thing to note about these isolated stations – there is limited electrical power, and so incandescent light bulbs in the Stevenson screens is unlikely. Detailed descriptions of these stations are listed in the annex at the end of this report.
16. Alert,N.W.T.(82.5 N,62.3 W), Canada
17. Resolute,N.W. (74.7 N,95.0 W), Canada
18. Jan Mayen (70.9 N,8.7 W), Norway
19. Gmo Im.E. K. F (77.7 N, 104.3 E), Tamyr Peninsula, Russia
20. Ostrov Dikson (73.5 N,80.4 E, Russia
21. Ostrov Kotel’ (76.0 N,137.9 E), Russia
22. Mys Salaurova (73.2 N,143.2 E), Russia
23. Ostrov Chetyr (70.6 N,162.5 E), Russia
24. Ostrov Vrange (71.0 N,178.5 W) , Russia
Now here is the chart of the temperatures of these isolated stations, not subjected to manmade structures or heat sources.
Isolated Stations
Note that most of the trends are flat or decreasing. Only Resolute and Ostrov Vrange are increasing slightly. Both of those might be slightly influenced by UHI. The longest records clearly show warming in the late 1930’s and 40’s, and cooling in the 1960’s, and none show a hockey stick. The GISS data for Alert ends in 1991, though the weather station is still there, and still reporting. Data for Mys Salaurova and Ostrov Chetyr also ends at about that time, probably due to the fall of the Soviet Union.
Here is an average of all the isolated stations:
Isolated Stations - Average
Note that the peak-to-peak trend is nearly zero. The linear trend is about 0.4°C/century, but the R2 value (the statistical significance for the trend) is very low, 0.023.
 Here is a plot of the AMO versus the average temperature of the isolated stations.

The temperature as measured at stations isolated from any UHI is simply tracking the AMO. 
Looks like an awfully good fit. There is very little, if any, global warming. We need to wait until the bottom of the next AMO cycle to get a decent reading of global temperature change. That will be in about 2050 if the AMO cycles as it has since 1850.
———————————————————————————————-
Annex – station descriptions
The “urban” stations, nos. 1-15
1. Kotzebue, Ral (66.9 N,162.6 W), 
2. Barrow/W. Pos (71.3 N,156.8 W)
These towns are of similar size, and are growing at the same rate. In 1940, both towns had a population of 400. In 1980 both had just over 2000 population, and now they both have over 3000 people. Both have airports of sufficient size to handle multi-engine turboprop and small jet aircraft, and both are served daily by regional airlines. Kotzebue is on a peninsula and the airport is across the middle of the peninsula, somewhat restricting the growth of the town. Barrow has somewhat the same problem due to a series of small ponds around the town and the airport. Barrow was studied for UHI effects in 2003. That paper was in the International Journal of Climatology here. That paper describes the UHI average temperature increase in winter as 2.2°C compared to the surrounding hinterland. GISS data indicates that Barrow average temperature has increased over the years as population has increased. (See below, or click on link above.)
Barrow, AK


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Source: http://en.wikipedia.org/wiki/File:BRW-g.jpg
Source: http://en.wikipedia.org/wiki/File:OTZ-g.jpg
The Barrow NWS station (Stevenson Screen) is here. On the airport picture, it is at the base of the rotating beacon tower. Kotzebue NWS station is not visible in published pictures.
3. Inuvik (68.3N, 133.5W)
Inuvik is a relatively new town, begun in 1954. The population as of 2006 has grown to about 3500 people. Because it is a “planned” community in the arctic, built on permafrost, the water and sewage infrastructure is above ground in heated and insulated “utilidors”, like the heating systems in Siberia. The weather station, from weather reports, Google Earth and Google Street View, appears to be at the airport, in a compound just north of the entrance.
4. Cambridge Bay (69.1 N,105.1 W), Cambridge Bay, Nunavut, Canada
There is a Wikipedia picture of Cambridge Bay here. The population has grown from just a few people in the 1940’s to about 1500 today. It also has an airport with daily regional airline service.
5. Eureka, N.W.T. (80.0 N,85.9 W), Eureka, N. W. T., Canada
There are the only four stations at or north of 80° latitude, Eureka, Alert, Nord and Krenkle (Gmo. I.M.ET). Only Eureka has an unbroken temperature record to the present date, and it begins in 1947. The population at Eureka has
Eureka station
never been high. In winter it has always been 4 or 5 men. In summer, the population increases to as high as 20. The station infrastructure though, has expanded through the years. Each year, some of those 20 workers add or expand buildings. In the beginning, it was one or two buildings, with water and sewage handled in tanks and barrels internal to the buildings. The Stevenson screen was originally placed where the blue New Main Complex building is now. When that was built, the Meteorological instruments were moved to the current location. Over time, the water supply, plumbing, and sewage treatment was upgraded and the outfall pipe installed. It, of course, must be heated to facilitate flow to the sewage lagoon. All the water pipes exposed to the outdoors must be heated to prevent freezing.
Image from a recent article by Anthony Watts on WUWT here.
6. Nord Ads (81.6 N,16.7 W, Northeast Greenland
Nord Station
Nord is the furthest north inhabited place on earth, on the Northeast coast of Greenland. It was built in the period from 1952 to 1956 as an emergency airfield for aircraft operating out of Thule. Access is impossible by sea because the sea ice never moves away from the coast there. Legend has it that “Blowtorch” Murphy, a mythic arctic construction worker, scraped the first runway, using a parachute dropped caterpillar tractor after he himself parachuted onto the site. His nickname came from his habit of wearing a lit blowtorch hanging from his waistband on a wire; a lit blowtorch being somewhat useful when working outside when it’s 40° below zero.
There are about 40 buildings at Nord. Not all of them are continuously heated, but those near the Stevenson Screen are. The winter population is 5 or 6 men. More pictures here.
7. Svalbard Luft (78.2 N,15.5 E)
8. Isfjord Radio (78.1 N,13.6 E)
These two stations are only 47 kilometers apart. But data for both is fragmentary for 1976 and 1977, and there is no overlap. Svalbard Luft (airport) has been discussed on WUWT here and here, so I won’t cover it in detail here. Warwick Hughes has an article on Isfjord Radio here that makes the case for warming of Isfjord Radio due to moving of sea ice away from the islands in summer since 1912. Neither station in Svalbard shows on the anomaly map because there was no common station in both the base period and the anomaly period. Here’s a map with 1998 to 2008 as the base period where Svalbard appears.
9. Gmo Im.E.T.(80.6 N,58.0 E)
This is the Krenkel meteorological station on Hayes Island, or Ostrov Kheysa in Russian, in the Franz Josef Land Archipelago, Russia. Link The station has been moved or re-built twice since it was established. It was moved from Hooker Island (article in German) in 1957/58. A fire destroyed the power station in 2000, and it was rebuilt in 2004 closer to the shoreline. The GISS record is from 1958 with a gap from 2001 to 2009. The population was as high as 200 during Soviet times, but is down to 4 or 5 now. The population and the temperature seem to track roughly during Soviet days, and the move in 2004 was to a warmer location. In the picture you can see the old buildings on the ridge in the distance. The red grid-square on the anomaly map above corresponds to this station.
Source: http://www.sevmeteo.ru/foto/15/88.shtml
10. Olenek (68.5 N,112.4 E)
This is the town of Ust’-Olenek, Russia.
Photo sources here and here.
The town doesn’t look like much, but notice the Tundra Buggies parked next to the Stevenson Screens. It is on the Laptev Sea, on the northern Siberia Coast, but on a peninsula on a south-facing beach. The buildings are right on the shore. The wide view above was taken from out on the ice. This is one of the few places in Russia that the Google Earth satellite view actually has enough resolution to see the Stevenson Screens. They are much too close to the heated building.
11. Verhojansk (67.5 N,133.4 E)
This is one of the “centrally heated” towns in Russian Siberia. The picture at the top of this article is of the Stevenson Screen. Verhojansk is called the “cold pole” of the earth, but the measurements are too warm by far. Look closely at the picture. Any photographer will note that the warm glow inside the Stevenson Screen is just the color temperature of an incandescent light bulb. If the steam heat in the town isn’t enough, or the cattle in the pole-barns in the distance, the heat from the light bulb will warm up the measurements. This site was covered on WUWT here and here. Anthony Watts notes that warm anomalies would appear and disappear in this part of Russia “as if a switch were thrown”. Could it be as simple as the switch on that light bulb?
12. Cokurdah (70.6 N,147.9 E)
Also spelled Chokurdakh. The population has been dropping in recent years, but was still over 2500 people in 2002. The town is sandwiched between the Indigirka River and the airport. There is no way to tell where the Stevenson Screen is located, but the infrastructure at the airport blends right into the town. See an aerial photo here.
13. Zyrjanka (65.7 N, 150.9 E) Also spelled Zyryanka, another steam-heated town in eastern Siberia, well inland. The airport is in this picture on the north edge of town, along the Kolyma riverbank. This airfield was built during WWII as a stop for aircraft being ferried to the Soviet Union from Alaska. A second airport 7 miles west of town was probably built during the cold war for the military. The town was established in 1931. The population is currently about 3500. During the Soviet Union it was up to 15,000.
14. Mys Smidta (68.9 N,179.4 W)
Or Cape Schmidt.  John Daly wrote a bit about this location in 2000 (scroll way down in the article). The population was nearly 5000 in 1989, but has dropped since the fall of the Soviet Union. The population now is probably less than 1000. It is on the north coast of eastern Siberia, nearly at 180° longitude. The airbase there was built in 1954 as a staging base for any bombers headed for the U. S. It is still used by a regional airline.
15. Mys Uelen (66.2 N,169.8 W)
Or Cape Uelen. This is on the easternmost tip of Siberia, across Bering Strait from Kotzebue, Alaska. The current population is about 700 people. It is also centrally steam heated. The town is restricted by the geography, on a narrow spit sticking out into the sea, backed by a cliff on the landward side. The airport is a helipad. Cargo and fuel arrives by barge in the summer.
Below is a temperature chart for many of the above stations. All are warming, some faster than others. Barrow, for which we have the UHI study, is not the fastest warming.
16. Alert,N.W.T.(82.5 N,62.3 W), Alert, Canada
Alert, Canada has had a weather station since 1951. The population has never been more than 4 or 5 in the winter, with a higher population in the summer. I could not definitively locate the Stevenson screen, but there are two possibilities in this photo, both well away from the buildings.
THE ISOLATED STATIONS, NOS. 16-24
17. Resolute,N.W. (74.7 N,95.0 W)
The population of this Canadian station rose from zero prior to 1947, to 229 in 2006. There is an airport here, and the Stevenson Screen can be seen across the aircraft parking area from the airport terminal at the left edge of the photo.
18. Jan Mayen (70.9 N,8.7 W)
Pictures of the station are here, and a web site is here. The 18 people on the island live at Olonkinbyen, or Olonkin “City”. The meteorological station is 2.6 km away. The 4 people that work there live in Olonkin City. The Stevenson Screen appears to be well away from the station building, and the surroundings have probably not changed since the station was built.
19. Gmo Im.E. K. F (77.7 N, 104.3 E)
This is a Russian station on the Tamyr Peninsula at Cape Chelyuskin (Mys Chelyuskin). Nothing is visible at that location on the Google satellite view, but the resolution is very low. I found an article by Warwick Hughes dated September 2000 that speaks of cooling of the Tamyr Peninsula here. He also talks about “non-climate” warming of Verhojansk and Olenek.
20. Ostrov Dikson (73.5 N,80.4 E
Dikson, Russia airfield
This is Dickson Island in English. There is a town of Dikson 10 kilometers away on the mainland. The airport is on Dikson Island at the point called Ostrov Dikson on the map below. Pictures of the airport can be seen here. The town is pictured on this 1965 stamp.
Wikipedia link
21. Ostrov Kotel’ (76.0 N,137.9 E)
The full name is Ostrov Kotel’nyy. In English this is Kettle Island. The first documented explorer found a copper kettle, so obviously he was not the first person to find the island. A single building is barely visible on Google 3D mapsat the “settlement” known as Kalinina.  This may be the meteorological station. No other signs of civilization can be seen on the whole island.
22. Mys Salaurova (73.2 N,143.2 E)
This also spelled Mys Shalaurova. The station is on the south-facing shore of an island and is visible on Google Earth here. There is a tide gauge, and the tide data is on that same page.
23. Ostrov Chetyr (70.6 N,162.5 E)
The full name is Ostrov Chetyrekhstolbovoy. This is a small island in the East Siberian Sea in the Medvezhy Island (Bear Island) group.
Map source here.
A description of the place is found: here. “A polar meteorological station and a radio station are situated on the shore of a small bay which indents the S side of the island.”
24. Ostrov Vrange (71.0 N,178.5 W) 
This is otherwise known as Wrangle Island. It is about 125 kilometers off the Siberian coast on the 180th meridian. The weather station is at Ushakovskiy on a spit at Rogers Bay, at the right in this picture, well separated from the village. One building in the village is visible at the left. Link
The population in the village grew to as many as 180 people in the 1980’s, but when the Soviet Union dissolved, subsidies declined and the population moved to the mainland. The last villager was killed by a polar bear in 2003. The population at the weather station, when occupied, has always been 4 or 5.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEuropean particle physics research center CERN located near Geneva, Switzerland has gotten its budget cut by 6% over the next five years. Europe no longer has the money to fund it. 
It was just a question of time. With so much money being pissed away on bogus climate research and save-the-world projects, eventually you run out of funds to pay for real science and research.
How many billions have been poured into researching the non-problem of climate change? The IPCC? Hansen’s GISS surface station folly? Green subsidies? The list is endless.
What benefit have we gotten? I’d say we’ve gotten a lot more damage than benefit, especially if you consider what could have been done with the money instead. Lost opportunities.
Swiss radio has a report (German): CERN budget cut and writes:
The money worries of the European countries is now adversely impacting the European CERN research facility in Geneva. Over the next 5 years 343 million Swiss Francs have to be saved – about 6 percent of the budget.
While the management of CERN say the financial measure is painful, it will be bearable. But the research laboratory’s association of employees has a different opinion: The future of CERN is being put at risk.
Here’s my advice to CERN employees who fear losing their jobs: Call your elected officials and tell them stop wasting so much on bogus climate science.
Share this...FacebookTwitter "
"
From ICECAP

By Joseph D’Aleo CCM, AMS Fellow
2008 will be coming to a close with yet another spotless days according to  the latest solar image.

This will bring the total number of sunspotless days this month to 28 and for  the year to 266, clearly enough to make 2008, the second least active solar year  since 1900.

See larger image here.
The total number of spotless days this spolar minimum is now at around 510  days since the last maximum. The earliest the minimum of the sunspot cycles can  be is July 2008, which would make the cycle length 12 years 3 months, longest  since cycle 9 in 1848. If the sun stays quiet for a few more months we will  rival the early 1800s, the Dalton Minimum which fits with the 213 year cycle  which begin with the solar minimum in the late 1790s.

See larger image here.
Long cycles are cold and short ones like the ones in the 1980s and 1990s are  warm as this analysis by Friis-Christensen in 1991 showed clearly. 

See larger image here.
In reply to the arguments made that the temperatures after 1990 no longer  agreed with solar length, I point out that it was around 1990 when a major  global station dropout (many rural) began which led to an exaggeration of the  warming in the global temperature data bases. Also the length from max to max of  21 to 22 was 9.7 years and cycle 22 length min to min 9.8 years, both very short  suggesting warm temperatures in the 1990s. The interval of cycle 22 max to cycle  23 max centered in the mid 1990s began to increase at 10.7 years and the min to  min length of cycle 23 is now at least 12.3 years.
With the Wigley suggested lag of sun to temperatures of 5 years and  Landscheidt suggested 8 years, a leveling of should have been favored around  2000-2003 and cooling should be showing up now.  Looking ahead, put that  together with the flip of the PDO in the Pacific to cold and you have alarming  signals that this cooling of the last 7 years will continue and accelerate.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99a67de2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
You may remember last month I posted adjusted numbers for WUWT due to a SPAM attack, dropping my September count by about 50K:

SEPTEMBER NUMBERS Click for full sized image
For September 2008 the total was 846,193 page views, up from 667,215 page views in August 2008.
But there is a caveat, I think the real numbers are just shy of 800,000, because on the weekend of 09/20 and 09/21 I got quite a bit of unexpected traffic to one old post that I’m not sure is real or not. During that time, we got a lot of Spam on that particular older entry comparing UAH, RSS, HadCRUT, and GISS, but not anywhere near the numbers specific to that post, shown below:
Blog Stats Increase due to DOS “something”

Saturday 09/20 23,486
Sunday   09/21 25,319
Monday  09/22   1,006
Total: 49,811
Raw WUWT September numbers:           846,193 page views
WUWT Spam Uncertainty numbers:           -49,811 page views (from 09/20 to 09/22)
Final Adjusted WUWT September numbers: 796,382 page views
Today at 5PM (00UTC) I tallied my October numbers, and while they are slightly lower than last month’s total, I didn’t have any perceptible spam attacks like last month, so I don’t need to adjust October’s numbers:

OCTOBER NUMBERS Click for larger image
At 826,633 page views for October, that would put me up slightly from last month’s adjusted numbers of 796,382, but there was also an extra day involved (September hath 30 days). Since that difference is close to my daily average, we’ll just call it “no change” for October.
Of course many of you are either hoping for “change” or “no change” in November. 😉
Thanks again to all of you, and especially those whom have donated recently to help keep this effort going. My UHI test in Reno was funded in part through your donations. Another trip this weekend to retrieve two dataloggers that have been out for 45 days will also be assisted by those same donations. Again my thanks.
Have a wonderful Halloween!- Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b32455e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter""I see lots of cold and snow in the warm times ahead!"" Britons stop believing. Welcome back to reality. (Photo source Wikipedia)
Britons are finally waking up from that global warming spell cast upon them years earlier.
Hat Tip Dirk H
What happens when the predictions of mountebanks (my favorite word lately) don’t come true, and the opposite happens?
People stop believing. That’s what’s happening now in Great Britain.
The online Daily Mail reports today on how the number of sceptic Britons has doubled in the last 4 years.
Obviously they don’t believe the junk science that insists global warming leads to more extreme cold and snow. Sorry, but many people aren’t that stupid to buy it. So take that crap alarmist science and use it for the next time you have to go.
This new scepticism in Britain is based on real-life observation, with people comparing the horoscopes of snowless winters heard earlier to the reality that is observed in real life today.
And for those who still believe in AGW, many now don’t think that it’s a real problem. The Daily Mail writes:
Fewer than half those polled – 46 per cent – are ready to use their cars less, and only 47 per cent are prepared to take public transport more often. Fewer than a quarter – 23 per cent – are willing to fly less.”
Share this...FacebookTwitter "
"
Guest post by Steven Goddard




The BBC ran an article this week  titled “Acid oceans  ‘need urgent action‘” based on the premise: 

The  world’s marine ecosystems risk being severely damaged by ocean acidification  unless there are dramatic cuts in CO2 emissions, warn  scientists.

This sounds very alarming, so being diligent  researchers we should of course check the facts.  The ocean currently has a pH  of 8.1, which is alkaline not acid.  In order to become acid, it  would have to drop below 7.0.  According to Wikipedia “Between 1751 and 1994 surface ocean pH is estimated to have decreased from  approximately 8.179 to 8.104.”  At that rate, it will take another 3,500  years for the ocean to become even slightly acid.  One also has to wonder how  they measured the pH of the ocean to 4 decimal places in 1751, since the idea of  pH wasn’t introduced until 1909.

The BBC article then asserts: 

The  researchers warn that ocean acidification, which they refer to as “the other CO2  problem”, could make most regions of the ocean inhospitable to coral reefs by  2050, if atmospheric CO2 levels continue to increase.

This does indeed sound alarming, until you consider that  corals became common in the oceans during the Ordovician Era – nearly 500 million years ago – when atmospheric CO2 levels were about 10X  greater than they are today. (One might also note in the graph below that there  was an ice age during the late Ordovician and early Silurian with CO2 levels 10X  higher than current levels, and the correlation between CO2 and temperature is  essentially nil throughout the Phanerozoic.)





http://ff.org/centers/csspp/library/co2weekly/2005-08-18/dioxide_files/image002.gif
Perhaps  corals are not so tough as they used to be?  In 1954, the US detonated the  world’s largest nuclear weapon at Bikini Island in the South Pacific.  The bomb  was equivalent to 30 billion pounds of TNT, vapourised three islands, and raised  water temperatures to 55,000 degrees.  Yet half a century of rising CO2 later,  the corals at  Bikini are thriving.  Another drop in pH of 0.075 will likely have less  impact on the corals than a thermonuclear blast.  The corals might even survive  a rise in ocean temperatures of half a degree, since they flourished at times  when the earth’s temperature was 10C higher than the present.

There seems to be no shortage of theories about how rising CO2 levels will  destroy the planet, yet the geological record shows that life flourished for  hundreds of millions of years with much higher CO2 levels and temperatures.   This is a primary reason why there are so many skeptics in the geological  community.  At some point the theorists will have to start paying attention to  empirical data.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e989122c8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Minus 13 degrees – the coldest it’s been in April
 From Weatherzone – Brett Dutschke, 
Wednesday April 29, 2009 – 14:58 EST
Charlotte Pass, 1,837m, Snowy Mountains of New South Wales, Australia
 
A new Australian record was set early this morning, a temperature of minus 13 degrees, at Charlotte Pass on the Snowy Mountains.
This is the lowest temperature recorded anywhere in Australia in April and is 13 below the average. Nearby at Perisher it dipped to minus 11 degrees and at the top of Thredbo it dipped to minus 10.
Across the border, on the Victorian Alps April records were broken at Mt Hotham where it chilled to minus eight degrees and Mt Buller and Falls Creek where it got as low as minus seven.
A few other locations set April low temperature records also. In Tasmania Lake Leake was as cold as minus six, Sheffield and Dover both reached minus one and Flinders island got to zero. Hobart had its coldest April night in 46 years, recording a low of 1.7 degrees, seven below average.
While much of inland NSW and Victoria will be colder tomorrow morning than it was this morning under clearer skies, the Alps should be a little warmer due to a rise in humidity.

Note, all temperatures in the story above are in Centigrade. Photo and map added by Anthony.
Here are the all-time highs and lows for the continent of Australia (source Perth Weather Center)
HIGHEST RECORDED TEMPERATURE:

 Oodnadatta, South Australia  50.7 C  (123.3 F) on the 2nd January, 1960

LOWEST RECORDED TEMPERATURE:

 Charlotte Pass, New South Wales  -23.0 C  (-9.4 F) on the 29th June, 1994

While this is certainly a significant new cold record this early in Australia’s fall going on winter, one must always remember that weather is not climate. – Anthony
(h/t to WUWT reader “Chuck”)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96bc62ff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

There’s a war on. And it cannot be successfully prosecuted without a delicate combination of U.S. leadership and international cooperation. Anything that threatens either threatens all. Unfortunately, two upcoming environmental issues do both.



Although it seems a lifetime, it was a bit over two months ago when European leaders couldn’t say enough bad things about President Bush, when he wisely said “no” to the United Nations’ Kyoto Protocol on global warming. That was during the last “Conference of the Parties” (COP) to the treaty, in Bonn, Germany. Bush knows the whole thing costs a fortune and, even if climate change is a big deal, the Protocol has no detectable effect on anything but our economy. Hopefully, the events of the last month provide a bit of needed perspective on how big this deal is.



In their obsession with pleasing radical greens (who just got clobbered in a Hamburg election), the euros modified Kyoto in Bonn. The purpose was to get the Japanese to go along, which will satisfy a requirement necessary to make it legally binding. In doing so, they made the climatically inconsequential treaty more irrelevant. They also left out a lot of details that need to be cleaned up at the next COP.



That COP begins on October 29 in, of all places, Marrakech. Last July, the United States intimated that despite our opposition to Kyoto we would attend and maybe pony up a new plan.



Let’s kill this meeting now, before it harms our wartime alliance. All it will do is re‐​open old wounds. And don’t even think about the security issues of getting a few thousand people from the hated West together in an Islamic nation, however friendly and moderate the current leaders might be.



The consequence of alliance‐​building is that we’ve been offering some pretty generous terms, such as debt‐​forgiveness and dropping of economic sanctions. We can only hope that our European allies didn’t make any demands about Kyoto. And if we don’t go to Marrakech, that will be a very good sign that they didn’t.



The second green threat to the war effort is on the home front. Career bureaucrats at the Environmental Protection Agency don’t seem to get it: This is not the time to saddle the nation with new rules that could constrict our energy security.



Right now, they’re busily working away on new regs to restrict a basket of three emissions: sulfur dioxide, nitrogen oxides and airborne mercury. The first two are substantially regulated already, and no one has been able to find one death from the last one. But we know that mercury is toxic, and despite the obvious lack of morbidity and mortality, so the logic goes: We must regulate.



The problem is that while sulfur and nitrogen oxides are already largely scrubbed from coal combustion, apparently the only way to keep the mercury out of the air is to stop burning it. Coal produces over 50 percent of our electricity. We have so much of it in the ground (despite attempts by the Clinton Administration to lock it up, as it did in Utah) that we are called the Saudi Arabia of coal.



In a war, prudence plans for the worst contingencies. One of those is that things can and do go bad, and that we could wind up with an embargo — in reality or de facto — on Middle Eastern oil. A supertanker is a wonderful target for, say, an airliner‐​cum cruise missile. Not likely, you say?



In 1973, oil was embargoed, fuel prices skyrocketed, and soon after, the economy went into a sharp and severe recession. Recall that the shock was so great that one of his first acts as president was for Jimmy Carter to declare that energy security was “the moral equivalent of war.” Now, consider prosecuting a real war in this environment.



In the worst‐​case scenario, if supplies are restricted, the United States can buy more oil — at potentially great expense — from non‐​Arab vendors. But how does it get shipped, once one supertanker blows up? In that world we can formulate alternative fuels out of the strategically inexhaustible coal. No one likes the idea. It’s dirty, but so is war, where a lot of things can happen that no one likes. Let’s not place this industry in harm’s way before such an effort might be required.



Yes, there’s only a small chance of this. But it is larger than the probability of the events of September 11.



The point is simple: It’s time to back off on things like Kyoto and regulations that have the potential to tie our hands in any way. Halfhearted war efforts are for losers.



Finally, for those concerned about ultimate environmental degradation, take solace. Big wars, however noble, leave behind big governments and even bigger alliances, which will be only too eager to impose all kinds of new regulations once the smoke clears away. The residuum from the last one is called the United Nations, which gave us the Kyoto Protocol.
"
"
Share this...FacebookTwitterWhy do I get the feeling this Accenture/Barclay’s report is going to have a serious backlash? Good if it does! The warmists outlets are already out there doing pre-emptive damage control, going into denial and wishful thinking.
A study carried out by consultants Accenture and Barclays Bank confirms that “climate protection” is going to cost a bundle and will involve “gigantic investments” if Europe’s target of reducing CO2 emissions 20% by 2020 is to be reached. The price tag for Europeans: 2.9 TRILLION euros, i.e. €2,900,000,000,000.00! With 450 million Europeans, that means €6,444.00 for every man, woman and child. The warmist klimaretter writes:
But at the same time, the conclusion that climate protection is expensive cannot be drawn from the calculations. The study does not analyse the costs the of climate protection, but only the necessary investments. Among these there are some that are economically attractive and thus will save money over the years.”
The question ought to be: “What are all these costs going to lead to?” The answer is: nothing. How about taking a look around and opening your eyes? Look at all the poverty out there that is screaming for investment. Look at the sorry state of many schools and hospitals in Europe.
Yet, instead of investing in these important things, the EU wants to blow the money on an energy system that no one really needs – one that is mandated by a fraud.
But the warmists are doing their damnedest to put a positive spin on it. Remember that protecting the climate is an abstract concept that exists only in Fairyland. The concept that we can “protect the climate” is a complete myth. As best I can tell, climate protection for warmists means the production of good weather. Good luck! Klimaretter writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The study shows foremost that climate protection is a big business opportunity, also for banking services because it will provide a large share of the needed capital. Therefore one has to view the necessary investments as a chance for the economy.
That this is going to be a big bonanza for the banks is no exaggeration. They are going to make a killing, but at the expense of the consumer. And saying these investments are a “chance” for the economy is really incorrect labelling. It’s going to be a “gamble” for the economy – Russian Roulette style. Klimaretter writes:
Climate protection is ‘one of the megatrends of the economy’.
They’re right about that. But that does not make it a trend that will lead to success. Having everyone go off the edge of a cliff as a trend does not make it a reasonable endeavour. Klimaretter then reports on Environment Minister Norbert Röttgen meeting with industry leaders of Allianz, Metro, Siemens and Viessmann, saying that the energetic renovation of buildings will have top priority and “the politicians must see to it that the renovation is equally attractive for property owners and users.”
More on Allianz in a day or two. In the meantime just keep in mind that this is all based on “purchased science”.  If they looked at the real science, all that capital could be directed to real problems.
======================================================
Sorry folks for the disappearing articles. It’s been a long day and I’ve hit some wrong buttons and so a really rough draft that was not meant to appear showed up as a new post. I’ll most likely post it tomorrow. – PG
Share this...FacebookTwitter "
"

The British government’s decision to begin extradition hearings against former Chilean dictator Gen. Augusto Pinochet has triggered a heated right‐​left quarrel in the United States and around the world. 



Pinochet’s conservative defenders argue that his military regime, although brutal at times, saved Chile from the scourge of communism, and that the retired, 83‐​year‐​old general should be allowed to return to his home in Santiago. 



Pinochet’s liberal detractors, on the other hand, compare him with Adolf Hitler and argue that he should be extradited to Spain and prosecuted for genocide and terrorism. Liberals also see the Pinochet case as an opportunity to move toward a universal system of criminal justice. 



Lost in the right‐​left controversy over Pinochet’s record is a more important issue. The Pinochet affair teaches us ominous lessons about what we can expect from the planned International Criminal Court (ICC), the global criminal justice organ approved last summer in Rome by 120 nations.  




The Pinochet affair is … an alarming harbinger of the ICC [and] … portends a very hazardous future. 



The first lesson of the Pinochet affair is that the ICC’s prosecutions will likely be ideologically based. Indeed, does anyone seriously believe that Mikhail Gorbachev will be arrested next time he travels to Western Europe and tried for the bloody war in Afghanistan or for the KGB’s cruel excesses during his six‐​year tenure? Was anyone startled by Italy’s decision not to extradite Abdullah Ocalan, the recently captured Marxist leader of the Kurdish Workers’ Party, to Turkey, where he is wanted as a terrorist? Or the German government’s decision not to demand Ocalan’s extradition to Germany where a number of his alleged murders took place? Last, was anyone surprised by the Spanish authorities’ decision to go after Pinochet, but at the very same moment, allow Cuban dictator Fidel Castro rome around Spain freely? 



The second lesson the Pinochet affair teaches us about the ICC is that its activities could threaten the fragile peace of transitional democracies. Chile and other new democracies around the world have withstood recent political and economic strains, but the arrest of Pinochet has reopened old wounds and many Chileans believe that terrorist violence could erupt if he is not allowed to return home. “I have already been threatened by ultra‐​left‐​wing terrorists,” says Sen. Hernan Larrain of the rightist Independent Democratic Union Party. His worst fear is that a British decision to go ahead with extradition will polarize the Chilean population, creating a climate for violence. Many people on the left also fear that terrorist violence could be aimed against them, since armed right‐​wing groups and former secret policemen are reportedly reorganizing. There is also widespread fear that the Chilean military will reassert control if the political situation deteriorates markedly. 



In addition to unsettling peace around the world, the ICC’s activities could make achieving peace in the first place more difficult. Consider the case of Palestine Liberation Organization leader Yasser Arafat, who was in the United States negotiating the Wye River Agreement the same week Pinochet was arrested in Britain. Would Arafat have negotiated at all if he thought the American government might arrest him when he arrived and try him for acts of PLO terrorism? Or would the recent Irish Peace Settlement have been negotiated if the Catholic or Protestant leaders faced criminal prosecution? 



The third lesson the Pinochet affair teaches us about the ICC is that its authority may be interpreted in expansive ways. For example, although Pinochet’s secret police are implicated in the deaths or disappearances of 3,000–4,000 people over 17 years (an average of about 250 people a year) he is being charged with the crime of genocide, defined by the 1948 Genocide Convention as systematic killing with “intent to destroy, in whole or in part, a national, ethnical, racial, or religious group.” Proponents of the genocide charge against Pinochet maintain that the elimination of political opponents through assassinations and imprisonment constitutes a kind of “ideological genocide.” So much for original intent. 



The Pinochet affair is thus an alarming harbinger of the ICC. Specifically, the planned court stands to produce an arbitrary and highly politicized “justice” and open a Pandora’s box of political folly and malleable laws. The Pinochet affair, in short, portends a very hazardous future.
"
"
Share this...FacebookTwitterThe German Freie Welt has a short piece by Fabian Heintel called Lost in Translation. It takes a look at how the sceptical International Climate Science Coalition gets treated by Wikipedia.
The English Wikipedia describes the organisation as:
 …an international association of scientists, economists and energy and policy experts
That sounds respectable and authoritative enough, but too much so for the German language Wikipedia.
In climate dissent-intolerant Germany, sceptics are to be viewed as flake amateurs who have little scientific authority. Here’s how the German Wikipedia describes the International Climate Science Coalition .
…Zusammenschluss von Ökonomen, Politikern und anderen Personen
Translated in English that’s:
association of economists, politicians and other persons. 
Sounds kind of ragtag, doesn’t it?
Here’s a list of the members of the international organisation. You make the call if the German Wikipedia description fits.
Share this...FacebookTwitter "
"
NOTE: Zip file downloads of models with data have been fixed, see end of this post – Anthony


Source: Mantua,            2000
The essay below has been part of a back and forth email exchange for about a week. Bill has done some yeoman’s work here at coaxing some new information from existing data. Both HadCRUT and GISS data was used for the comparisons to a doubling of CO2, and what I find most interesting is that both Hadley and GISS data come out higher in for a doubling of CO2 than NCDC data, implying that the adjustments to data used in GISS and HadCRUT add something that really isn’t there.
The logarithmic plots of CO2 doubling help demonstrate why CO2 won’t cause a runaway greenhouse effect due to diminished IR returns as CO2 PPM’s increase. This is something many people don’t get to see visualized.
One of the other interesting items is the essay is about the El Nino event in 1878. Bill writes:
The 1877-78 El Nino was the biggest event on record.  The anomaly peaked at +3.4C in Nov, 1877 and by Feb, 1878, global temperatures had spiked to +0.364C or nearly 0.7C above the background temperature trend of the time.
Clearly the oceans ruled the climate, and it appears they still do.
Let’s all give this a good examination, point out weaknesses, and give encouragement for Bill’s work. This is a must read. – Anthony

Adjusting Temperatures for the ENSO and the  AMO
A guest post by: Bill Illis

People have  noted for a long time that the effect of the El Nino Southern Oscillation (ENSO)  should be accounted for and adjusted for in analyzing temperature trends.  The same point has been raised for the  Atlantic Multidecadal Oscillation (AMO).   Until now, there has not been a robust method of doing so.
This post will  outline a simple least squares regression solution to adjusting monthly  temperatures for the impact of the ENSO and the AMO.  There is no smoothing of the data, no  plugging of the data; this is a simple mathematical calculation.
Some basic  points before we continue.
–         The ENSO and the AMO both affect temperatures and, hence, any  reconstruction needs to use both ocean temperature indices.  The AMO actually provides a greater impact on  temperatures than the ENSO.
–         The ENSO and the AMO impact temperatures directly and continuously on  a monthly basis.  Any smoothing of the  data or even using annual temperature data just reduces the information which  can be extracted.
–         The ENSO’s impact on temperatures is lagged by 3 months while the AMO  seems to be more immediate.  This model  uses the Nino 3.4 region anomaly since it seems to be the most indicative of the  underlying El Nino and La Nina trends.
–         When the ENSO and the AMO impacts are adjusted for, all that is left  is the global warming signal and a white noise error.
–         The ENSO and the AMO are capable of explaining almost all of the  natural variation in the climate.
–         We can finally answer the question of how much global warming has  there been to date and how much has occurred since 1979 for example.  And, yes, there has been global warming but  the amount is much less than global warming models predict and the effect even  seems to be slowing down since 1979.
–         Unfortunately, there is not currently a good forecast model for the  ENSO or AMO so this method will have to focus on current and past temperatures  versus providing forecasts for the future.
And now to the  good part, here is what the reconstruction looks like for the Hadley Centre’s  HadCRUT3 global monthly temperature series going back to 1871 – 1,652 data  points.

Click for a full sized image

I will walk you  through how this method was developed since it will help with understanding some  of its components.
Let’s first look  at the Nino 3.4 region anomaly going back to 1871 as developed by Trenberth  (actually this index is smoothed but it is the least smoothed data  available).
–         The 1877-78 El Nino was the biggest event on record.  The anomaly peaked at +3.4C in Nov, 1877 and  by Feb, 1878, global temperatures had spiked to +0.364C or nearly 0.7C above the  background temperature trend of the time.
–         The 1997-98 El Nino produced similar results and still holds the  record for the highest monthly temperature of +0.749C in Feb, 1998.
–         There is a lag of about 3 months in the impact of ENSO on  temperatures.  Sometimes it is only 2  months, sometimes 4 months and this reconstruction uses the 3 month lag.
–         Going back to 1871, there is no real trend in the Nino 3.4 anomaly  which indicates it is a natural climate cycle and is not related to global  warming in the sense that more El Ninos are occurring as a result of  warming.   This point becomes important because we need  to separate the natural variation in the climate from the global warming  influence.

Click for full sized image

The AMO anomaly  has longer cycles than the ENSO.
–         While the Nino 3.4 region can spike up to +3.4C, the AMO index rarely  gets above +0.6C anomaly.
–         The long cycles of the AMO matches the major climate shifts which  have occurred over the last 130 years.   The downswing in temperatures from 1890 to 1915, the upswing in temps  from 1915 to 1945, the decline from 1946 to 1975 and the upswing in temps from  1975 to 2005.
–         The AMO also has spikes during the major El Nino events of 1877-88  and 1997-98 and other spikes at different times.
–         It is apparent that the major increase in temperatures during the  1997-98 El Nino was also caused by the AMO anomaly.  I think this has lead some to believe the  impact of ENSO is bigger than it really is and has caused people to focus too  much on the ENSO.
–         There is some autocorrelation between the ENSO and the AMO given  these simultaneous spikes but the longer cycles of the AMO versus the short  sharp swings in the ENSO means they are relatively independent.
–         As well, the AMO appears to be a natural climate cycle unrelated to  global warming.

Click for full sized image

When these two  ocean indices are regressed against the monthly temperature record, we have a  very good match.
–         The coefficient for the Nino 3.4 region at 0.058 means it is capable  of explaining changes in temps of as much as +/- 0.2C.
–         The coefficient for the AMO index at 0.51 to 0.75 indicates it is  capable of explaining changes in temps of as much as +/- 0.3C to +/-  0.4C.
–         The F-statistic for this regression at 222.5 means it passes a 99.9%  confidence interval.
But there is a  divergence between the actual temperature record and the regression model based  solely on the Nino and the AMO.  This is  the real global warming signal.

Click for full sized image

The global  warming signal (which also includes an error, UHI, poor siting and adjustments  in the temperature record as demonstrated by Anthony Watts) can be now be modeled against the rise in CO2 over the period.
–         Warming occurs in a logarithmic relationship to CO2 and,  consequently, any model of warming should be done on the natural log of  CO2.
–         CO2 in this case is just a proxy for all the GHGs but since it is the  biggest one and nitrous oxide is rising at the same rate, it can be used as the  basis for the warming model.
This regression  produces a global warming signal which is about half of that predicted by the  global warming models.  The F statistic  at 4,308 passes a 99.9% confidence interval.

Click for full sized image

–         Using the HadCRUT3 temperature series, warming works out to only  1.85C per doubling of CO2.
–         The GISS reconstruction also produces 1.85C per doubling while the  NCDC temperature record only produces 1.6C per doubling.
–         Global warming theorists are now explaining the lack of warming to  date is due to the deep oceans absorbing some of the increase (not the surface  since this is already included in the temperature data).  This means the global warming model  prediction line should be pushed out 35 years, or 75 years or even 100s of  years.
Here is a  depiction of how logarithmic warming works.   I’ve included these log charts because it is fundamental to how to  regress for CO2 and it is a view of global warming which I believe many have not  seen before.
The formula for  the global warming models has been constructed by myself (I’m not even sure the  modelers have this perspective on the issue) but it is the only formula which  goes through the temperature figures at the start of the record (285 ppm or 280  ppm) and the 3.25C increase in temperatures for a doubling of CO2.   It is  curious that the global warming models are also based on CO2 or GHGs being  responsible for nearly all of the 33C greenhouse effect through its impact on  water vapour as well.

Click for larger image

The divergence,  however, is going to be harder to explain in just a few years since the ENSO and  AMO-adjusted warming observations are tracking farther and farther away from the  global warming model’s track.  As the RSS  satellite log warming chart will show later, temperatures have in fact moved  even farther away from the models since 1979.

Click for larger image

The global  warming models formula produces temperatures which would be +10C in geologic  time periods when CO2 was 3,000 ppm, for example, while this model’s log warming  would result in temperatures about +5C at 3,000 ppm.  This is much closer to the estimated  temperature history of the planet.
This method is  not perfect.  The overall reconstruction  produces a resulting error which is higher than one would want.  The error term is roughly +/-0.2C but the it  does appear to be strictly white noise.    It would be better if the resulting error was less than +/- 0.2C but it  appears this is unavoidable in something as complicated as the climate and in  the measurement errors which exist for temperature, the ENSO and the  AMO.
This is the  error for the reconstruction of GISS monthly data going back to 1880.

Click for larger image

There does not  appear to be a signal remaining in the errors for another natural climate  variable to impact the reconstruction.   In reviewing this model, I have also reviewed the impact of the major  volcanoes.  All of them appear to have  been caught by the ENSO and AMO indices which I imagine are influenced by  volcanoes.  There appears to be some room  to look at a solar influence but this would be quite small.  Everyone is welcome to improve on this  reconstruction method by examining other variables, other indices.
Overall, this  reconstruction produces an r^2 of 0.783 which is pretty good for a monthly  climate model based on just three simple variables.  Here is the scatterplot of the HadCRUT3  reconstruction.

Click for a larger image

This method  works for all the major monthly temperature series I have tried it  on.
Here is the  model for the RSS satellite-based temperature series.

Click for a larger image

Since 1979,  warming appears to be slowing down (after it is adjusted for the ENSO and the  AMO influence.)
The model  produces warming for the RSS data of just 0.046C per decade which would also  imply an increase in temperature of just 0.7C for a doubling of CO2 (and there  is only 0.4C more to go to that doubling level.)
Click for a full sized image

Looking at how  far off this warming trend is from the models can be seen in this zoom-in of the  log warming chart.  If you apply the same  method to GISS data since 1979, it is in the same circle as the satellite  observations so the different agencies do not produce much different  results.

Click for larger image

There may be  some explanations for this even wider divergence since 1979.
–         The regression coefficient for the AMO increases from about 0.51 in  the reconstructions starting in 1880 to about 0.75 when the reconstruction  starts in 1979.  This is not an expected  result in regression modelling.
–         Since the AMO was cycling upward since 1975, the increased  coefficient might just be catching a ride with that increasing trend.
–         I believe a regression is a regression and we should just accept this  coefficient.  The F statistic for this  model is 267 which would pass a 99.9% confidence interval.
–         On the other hand, the warming for RSS is really at the very lowest  possible end for temperatures which might be expected from increased GHGs.  I would not use a formula which is lower than  this for example.
–         The other explanation would be that the adjustments of old  temperature records by GISS and the Hadley Centre and others have artificially  increased the temperature trend prior to 1979 when the satellites became  available to keep them honest.  The  post-1979 warming formulae (not just RSS but all of them) indicate old records  might have been increased by 0.3C above where they really were.
–         I think these explanations are both partially correct.
This temperature  reconstruction method works for all of the major temperature series over any  time period chosen and for the smaller zonal components as well.  There is a really nice fit to the RSS Tropics  zone, for example, where the Nino coefficient increases to 0.21 as would be  expected.
Click for a full sized image

Unfortunately,  the method does not work for smaller regional temperature series such as the US  lower 48 and the Arctic where there is too much variation to produce a  reasonable result.
I have included  my spreadsheets which have been set up so that anyone can use them.  All of the data for HadCRUT3, GISS, UAH, RSS  and NCDC is included if you want to try out other series.  All of the base data on a monthly basis  including CO2 back to 1850, the AMO back to 1856 and the Nino 3.4 region going  back to 1871 is included in the spreadsheet.
The model for  monthly temperatures is “here” and for annual temperatures is “here” (note the  annual reconstruction is a little less accurate than the monthly reconstruction  but still works).
I have set-up a  photobucket site where anyone can review these charts and others that I have  constructed.
http://s463.photobucket.com/albums/qq360/Bill-illis/
So, we can now  adjust temperatures for the natural variation in the climate caused by the ENSO  and the AMO and this has provided a better insight into global warming.  The method is not perfect, however, as the  remaining error term is higher than one would want to see but it might be  unavoidable in something as complicated as the climate.
I encourage  everyone to try to improve on this method and/or find any errors.  I expect this will have to be taken into  account from now on in global warming research.   It is a simple regression.

UPDATED: Zip files should download OK now.

SUPPLEMENTAL INFO NOTE: Bill has made the Excel spreadsheets with data and graphs used for this essay available to me, and for those interested in replication and further investigation, I’m making them available here on my office webserver as a single ZIP file
Downloads:
Annual Temp Anomaly Model 171K Zip file
Monthly Temp Anomaly Model 1.1M Zip file
Just click the download link above, save as zip file, then unzip to your local drive work folder.
Here is the AMO data which is updated monthly a few days after month end.
http://www.cdc.noaa.gov/Correlation/amon.us.long.data
Here is the Nino 3.4 anomaly from Trenbeth from 1871 to 2007.
ftp://ftp.cgd.ucar.edu/pub/CAS/TNI_N34/Nino34.1871.2007.txt
And here is Nino 3.4 data updated from 2007 on.
http://www.cpc.ncep.noaa.gov:80/data/indices/sstoi.indices
– Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b0adf78',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterFakta Menarik Akun Pro Di Situs BandarQQ – Banyak sekali pembicaraan mengenai akun pro situs bandarqq. Beberapa orang mengatakan bahwa akun pro hanyalah fiktif belaka. Namun sebagian orang juga ada yang percaya dengan adanya akun pro ini. Membingungkan, ya. Lalu, menurut kamu bagaimana? Apakah benar-benar ada akun pro itu? Atau memang hanya fiktif?
Nah, sebelum membahas lebih lanjut, kamu mesti tahu nih bahwa akun pro hanya ada di beberapa level atau kalangan. Jadi, wajar saja jika masih banyak orang yang tidak mengetahuinya. CS judi online pun tidak akan tahu jika di situsnya memang tidak mengadakan akun pro. Berikut adalah beberapa ulasan mengenai bocoran akun pro di situs judi online BandarQQ yang tidak diketahui banyak orang:
Rahasia High Profit Pengguna Akun Pro
Pada umumnya, situs judi online memang memiliki rahasia dan juga cara curang dalam bermain. Namun banyak orang yang tidak mengetahuinya atau mereka tahu tetapi takut untuk memberitahu. Biasanya, para pemilik situs high profit-lah yang tahu rahasia dan cara curang seperti akun pro. Termasuk akun pro situs BandarQQ.
Yang perlu kamu ketahui tentang situs high profit ialah keadaan jika kamu menang maka pemilik situs dapat mengambil upah dari member yang menang saat bermain. Maka dari itu kamu tidak perlu heran jika pemilik akun pro sangat mudah menang.
Penyebab Anda Sering Kalah Meskipun Sudah Menggunakan Akun Judi Pro di Situs Agen BandarQQ
Harap diingat bahwa menggunakan akun pro tidak akan membuatmu selalu menang. Keuntungan memakai akun pro ini ialah kamu mempunyai kesempatan menang lebih besar dibandingkan menggunakan akun biasa. Bukan akan terus menang. Maka, wajar saja jika kamu masih bisa kalah. Namun jika kamu kalah berturut-turut dan tidak pernah menang sama sekali mungkin  ada masalah. Berikut ini ulasan penyebab mengapa kamu tidak menang-menang meski menggunakan akun pro termasuk akun pro situs BandarQQ!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Belum Meng-upgrade Akun Pro
Kamu harus paham jika memakai akun pro, kamu wajib meng-upgradenya agar kelebihannya tidak berkurang.
Kurang beruntung
Kamu sudah meng-upgrade akun pro tetapi tetap kalah terus? Jika seperti itu kemungkinan besar adalah kamu sedang tidak beruntung. Karena, sekali lagi yang perlu kamu ingat, menggunakan akun pro termasuk akun pro situs BandarQQ juga bisa membuatmu kalah.
Penyebab Akun Pro Tidak Lagi Sakti
Akun pro memang sudah terbukti bisa membuat kamu mempunyai kesempatan menang lebih besar dibanding akun biasa. Namun ada masanya jika akun pro milik kamu tidak sakti lagi bahkan bisa membuat kamu sangat merugi! Berikut ini ulasan penyebab akun pro milik kamu tidak sakti lagi.
Nilai Deposit Milik Kamu Ternyata Menurun
Kamu tidak pernah meninggikan nilai depositmu. Ini adalah faktor yang paling memungkinkan jika akun pro milik kamu tidak lagi membawa keberuntungan. Termasuk akun pro situs BandarQQ. Maka, silakan naikkan nilai deposit yang kamu miliki.
Kamu Tidak  Pernah Lagi Melakukan Deposit
Penyebab berikutnya adalah akibat dari kesalahan kamu sendiri. Kamu tidak pernah lagi melakukan deposit. Banyak sekali penyebab orang tidak melakukan deposit lagi, salah satu contohnya karena sudah tidak pernah lagi bermain judi online.
Itulah beberapa penjelasan mengenai akun pro situs BandarQQ. Meski penjelasan ini mencakup semua situs judi online yang ada. Hal terpenting yang harus kamu ingat ketika bermain judi online adalah dengan tidak serakah karena keserakahan bisa mengantarkanmu pada kerugian. Semoga artikel ini bermanfaat, ya.
Share this...FacebookTwitter "
"
I thought about writing a year end recap, but then I saw my traffic count for the month at 00 GMT (4PM PST), and thought that would do just as well at telling the story for this year. After a slight dip in October and November, WUWT has reached a new high at nearly 900,000 page views this month.

Click for a larger image
Not bad for a 12 month growth. My hit counter, as of this writing, stands at:
6,840,995 hits 
In December 2007, I hadn’t even broken 500,000.

Thanks to each and every one of you for visiting, contributing, and commenting. Thanks especially to the moderating team who keeps the temperature of this blog down whilst I think up new topics.
Here were the top 7 most popular posts in 2008, in case you missed them:
Top Posts
January 2008 – 4 sources say “globally cooler” in the past 12 months 140,090 views
A look at temperature anomalies for all 4 global metrics: Part 1 64,508 views
Where have all the sunspots gone? 59,144 views
Sudan hit by Apollo Asteroid 36,543 views
UAH: Global Temperature Dives in May 35,521 views
Solar Cycle 24 has officially started 34,877 views
Arctic sea ice back to its previous level, bears safe; film at 11 27,091 views


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9992e40d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIt was bound to happen sooner than later. A high level German politician speaking out against dubious climate science. Marie-Luise Dött, German Parliamentarian and a central figure on Angela Merkel’s environmental committee, expressed scepticism on climate change, the Financial Times Deutschland reports here in an article titled: The Climate Revisionists.
Now she is at the receiving end of brimstone and hellfire from all sides, including the media.
Here in Germany, climate skeptics face a level of intolerance not seen here in 65 years.
Last Wednesday, she made comments at a parliamentary forum discussion on the economic  impacts of climate protection held by the FDP Free Democrats, the junior coalition partner of Angela Merkel’s CDU/FDP coalition government.  Fred Singer – “a tobacco lobbyist” – was a guest speaker.
Dött’s comments not only left environmentalists and climate protection activists speechless and gasping for air, but exposed Dött as a climate skeptic. She is reported to have called climate protection a
…replacement religion, and that anyone who dared to express doubt could be branded an outlaw, forced to confess sins, sent to purgatory, or even cast into hell, if being really bad.
Free scientific thinking is a myth here.
Well, the vicious intolerant reactions she is now reaping confirm that her views are accurate, more than ever imagined. Even colleagues from within her own CDU Party piled on:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The next days are going to be very uncomfortable for her.
The intolerance from the opposition came swiftly. Hermann Ott, a spokesman for the German Green Party, blasted Dött and her CDU Party:
The CDU and the FDP Free Democrats are moving outside of the common community when they provide a forum in the German Parliament for the blind theories of climate change deniers.
(Note: denying the Holocaust in Germany is a crime. Ott is de facto calling Dött a criminal of the worst kind).
A member of the SPD was said to be in “shock” and demanded Dött be fired. He added it all confirmed the “real intentions of the coalition government.”
I’m not even going to get into what the media snobs are saying. Noses could not be higher.
Frau Dött not only has revealed herself to be skeptical of climate science, but has exposed Germany’s return to last century’s intolerance.
You can send a message of support here: E-Mail to MdB Marie-Luise Dött. Just fill in your message in the box: “Nachricht” with your name and city (You can ignore the boxes below the “Nachricht” box – they’re optional. 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA mountebank fleecing incredulous gamblers (Hieronymus Bosch - Wikipedia)
How many times has the demise of the planet or humanity been predicted? How many charlatans have passed through the revolving doors of history? You’d need the resources of the Census Bureau to tally that up.
A couple of days ago I presented yet another such prediction from a scientist at the Potsdam Institute For Climate Impact Research (PIK).
The planet is going to hell in a hand-basket they keep telling us.  But when you boil it down, they all have one thing in common: They’ve been wrong every time. 
The charlatans of the past have been replaced by a new generation of charlatans – government-funded climate scientists who are paid to get the masses to stampede in panic into the arms of government programs, all aided and abetted by the chatterbox media.
It turns out that life as a whole on the planet has never been better.
One media outlet steps up
In Germany we’re finding out that not all the media accept what is written down on the press releases. There are a few journalists who actually do good research and the hard work of digging. German news magazine FOCUS has an uplifting and positive article about how things, by almost every measure, are far better today than anyone expected.
 The FOCUS report written by veteran journalist Michael Miersch takes a look at some of the charlatans of the past and a bit into the psychology of why end-of-the-world scenarios find such mass appeal among the gullible, who later wonder what happened when things turn out differently.
This is not to say all the world’s problems have been solved. But those problems are under much better control today then they were say 50, 100 or 200 years ago. Here are just a few of the observations made by FOCUS.
Human prosperity
The world’s population has grown 6-fold since 1800, and at the same time life expectancy has doubled. Between 1955 and 2005 inflation-adjusted average personal income has tripled for the average person on the globe.”
Many of the poor indeed have gotten much richer.
Agriculture
How often do we hear about the threats of industrial agriculture devouring land to feed the exploding masses of people? Guess what? Modern agriculture protects wildlife and forests. FOCUS:
With the crop yields of 1961, farmers would have needed 32 million square km of cropland to have fed the 6 billion persons on the globe in 2000. Instead they have been able to harvest the necessary amount of crops on just 15 million acres. That means an area almost the size of South America was spared the plow. Forests and savannahs were thus saved.
All thanks to modern agricultural technology, which today continues to develop nicely. Yet, today, many greens are busily bemoaning the very agriculture that has rescued millions of sq km of forests and wildlife from primitive manaul agricultural practices. Worse, they also want us to fuel our cars with bio-diesel, which would require the extra deforestation of millions of sq km.
And so how do today’s results compare to the projections made by “intellectuals” like Paul Ehrlich? Clearly he has earned a top spot in that elite Club of Rome Crackpots, along with Al Gore, James Hansen and others. And another thing:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Inflation-adjusted prices for food have fallen 75% since 1950.
In fact, food has become so bountiful that bureaucrats are now whining about the social problems of obesity.
Today there are two things that could reverse the tremendous progress that agriculture has made – the Green Movement (think bio-diesel and organic farming) and global cooling. Guess which side (skeptic or alarmist) wants to see both?
Good news are never welcome by alarmist malcontents, who are psychologically sustained by daily doses of misery, pessimism and promises of catastrophe. This keeps Institutes like the PIK running.
On the other hand, optimists who maintain a positive, stay-the-course outlook for the future are viewed by the malcontents as simple-minded, naive and ignorant. Funny how the optimists always turn out to be right.
Miersch writes:
Completely contrary to what we hear day in and day out from the newspapers and TV, whether it’s war, hunger, illiteracy, political suppression, or environmental pollution, all of the world’s evils are shrinking.
That’s especially obvious with air pollution, which has reduced to such an extent in western countries that control freaks and tree huggers are now forced to call the non-pollutant CO2 a pollutant just to have something to do.
On poverty, peace and prosperity
First let’s recall, that all of Eastern Europe and much of South America were governed by dictators less than 50 years ago. Thankfully, they’ve fallen and things have gotten much better with free markets in control. FOCUS writes:
The United nations has determined that poverty receded more in the second half of the 20th century than in the 500 years before it.
Miersch then drives it home, using Germany as an example:
Germans who are now retiring belong to the first generation that has gotten to know peace, freedom, and prosperity as permanent fixtures in their lives. This has never happened in their history.
Isn’t apocalypse tough? But looking at the German media today, you’d think times couldn’t be worse – the planet is threatened by an imminent climate catastrophe that is “hidden in fog – but it’s there!”  
The future
So what lies in the future now that we have seen that every apocalyptic warning heard earlier in history has ended up being just fly-crap in the wind? The business of apocalypse is a big industry and involves lots of money – so don’t expect the end-of-world-charlatans to go away. There’s more money in it today than ever.
Being wrong every time isn’t going to deter today’s modern charlatans. They have a whole new line-up of catastrophes in their bag of tricks: climate change, biodiversity, ocean acidification, species extinction, to name some. And, there are plenty of malcontents out there who want to hear more, more, more.
But I suspect, like the earlier scares of the past, we’ll soon be able to put those on the list of seriously endangered species as well. Here today, extinct tomorrow.
Share this...FacebookTwitter "
"

In an election that promises to be close, command and control of the agenda–from “rats” to prescription drugs–is increasingly important, and timing is paramount. Proof is that in each of President Clinton’s victories, “October surprises” nudged the polls enough to turn squeakers into decisive victories. What’s coming this year, befitting the veep’s proclivities, is the first election year “environmental surprise.”



Those with foresight can already spy it along the planet’s limb. NASA scientists are leaking that Antarctic ozone depletion, which conveniently reaches its seasonal nadir around Oct. 20, is proceeding faster than ever. This doesn’t guarantee record depletion this year (in some years depletion has commenced rapidly, only to fizzle), but it certainly increases the chances. And the fact that NASA is already promoting lurid graphics–like the one that appeared in the Washington Post on Sept. 11–means, as usual, that its political antennae are up.



This is the same agency that discovered life in Martian rocks (later shown to be wrong) right at budget time. Now NASA is betting on Gore, who through his years in the Senate rewarded the space program handsomely. In a 1992 budget hearing, Gore announced that global warming should be “NASA’s number one scientific priority,” and the agency has been cashing the checks ever since.



Along about the time ozone bottoms out, the administration is going to release its “National Assessment of Global Warming,” which will forecast hell and damnation for us in coming decades unless we dramatically reduce our use of fossil energy–coal, oil and natural gas. How hard will it be for Gore to play this for political advantage? Can you hear the rhetoric? “The most eminent scientists in the nation all predict environmental disaster unless we curtail our use of oil, and my opponent’s largest contributors are the corporate polluters who created this problem,” Gore will intone.



Right about then, NASA will announce that the ozone hole has grown so large that it threatens the people of Chile, Argentina and South Africa, including the three million residents of Cape Town.



But how could the ozone hole still be growing, given that production of the putative cause–chlorofluorocarbon refrigerants (CFCs)–was stopped almost 10 years ago by a treaty called the Montreal Protocol? NASA will note that the rules of physics require that surface warming caused by increased greenhouse gases must be accompanied by a cooling of the stratosphere, and the colder it is up there, the more that ozone is depleted. NASA is itching to marry the distant cousins of global warming and ozone depletion before election day.



So what’s wrong with a little public service that just happens to occur at the same time the Clinton administration releases its global warming national assessment?



It turns out that the assessment is based on two computer models that simply do not work. When asked to simulate how the U.S. climate should have changed in the 20th century as a result of greenhouse gases, the computer models do worse than a table of random numbers applied to the problem.



Continuing to use these models to drive a forecast of national Sturm und Drang violates the number one ethic of science: If your theory doesn’t stand the test of the facts, it must be either changed or abandoned. What about oil and global warming? Recently, James Hansen, Gore’s original guru on climate change, said that reducing fossil fuel use is an expensive proposition that will do little to reduce global warming in coming decades. Instead, he argued, we ought to concentrate on other emissions whose control is not so economically detrimental.



Imagine the cost if we had rushed to do what Gore proposed in his book Earth in the Balance. Gas prices would be as high as they are in Britain, where we have witnessed the first riots created by global‐​warming taxes and where we may also witness the fall of a government because of unpopular global‐​warming policies. But that would be the second government to collapse. Last spring, the ruling coalition in Norway went under because it was against building two power plants, on the grounds that they would contribute to global warming.



Ironically, Hansen also works for NASA; but don’t expect to see him on television in October. Nor can we expect to see NASA’s own calculations show that ozone depletion will attenuate in coming decades because of the Montreal Protocol. Instead of these truths, look for environmental gloom and doom to dominate the end of October. And just how will Bush respond, as the clock winds down?
"
"
From The Sun
By VINCE SOODIN
Published: 26 Aug 2008
TWO British schoolgirls cheated death after being stung by a lethal Portuguese  Man O’War.
Paddling Molly Purcell, ten, suffered toxic shock and gasped for breath after  the tentacles of the sea creature — which can kill with a single sting —  wrapped around her arms and legs.
Pal Amelia Walsh, 12, was left with huge welts on her legs after brushing part  of the creature that was draped over a rock.
Medics used freezing water and seawater to flush out the toxins.
Bathers were evacuated after last Friday’s attack at Monmouth Beach, Dorset.
Molly, of Ascot, Berks, said: “I thought I was stung by a bee at first, then  suddenly it felt like my arm was on fire. It got worse and worse until I  couldn’t stop shaking.”
Last night mum Sheenagh said: “Molly is getting better but her arms are still  very swollen.”
Brits were warned last month of seven species of poisonous sea creatures  heading for our shores due to global warming.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d1e6c9d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

My new Cato Policy Analysis, “In Pursuit of Happiness Research: Is It Reliable? What Does It Imply for Policy,” was released today. If you’re wondering why we need long papers about the hazards of happiness research, look no further than Bill McKibben’s new essay in _Mother Jones_: 



According to new research emerging from many quarters, … our continued devotion to growth above all is, on balance, making our lives worse, both collectively and individually. Growth no longer makes most people wealthier, but instead generates inequality and insecurity. Growth is bumping up against physical limits so profound—like climate change and peak oil—that trying to keep expanding the economy may be not just impossible but also dangerous. And perhaps most surprisingly, growth no longer makes us happier.



There’s about five kinds of wrong in this one short passage. One of them is generated by the fact that McKibben is apparently ignorant of the most recent work on happiness — much of which directly contradicts his claim that we are not getting happier with growth. You’ll find the up‐​to‐​date scoop in my new paper. (Here’s a bite‐​sized taste.)   
  
  
If you’re worried about this whole business about measuring happiness and using the results to determine public policy, you’re not alone. Darrin McMahon in the elegant lead essay of this month’s _Cato Unbound_ casts a skeptical eye over the entire enterprise. But in today’s installment, Swarthmore psychologist Barry Schwartz, author of _The Paradox of Choice: Why More Is Less_ comes to the defense of the politics of happiness, and argues (in the McKibben vein), that more wealth can actually make us worse off. Is it true? Tune in to _Cato Unbound_ on Friday when Ruut Veenhoven, Director of the World Database of Happiness, will drop the latest data.   
  
  
Or, if you’re anxious, you can get plenty of secondhand Veenhoven in my paper.
"
"
Share this...FacebookTwitterPDO how entering the cool phase. Will it cool the planet in the decades ahead?
Will the new 2011-2020 decade be warmer or cooler than the last one?
That’s what we’re betting on (FOR CHARITY).
This bet is also known as the Honeycutt Climate Bet for Charity, after Rob Honeycutt who first proposed this bet. You can find the entire background here.
First of all Charles Zeller has also pledged $5,000 for the warm side, and has already paid half of it to Doctors Without Borders. So whatever happens from now on, I will consider that this blog has led to some kind of succees in that it has played a role in leading to this generous donation. The money will no doubt alleviate much pain and suffering among those of us who happened to be born in unfortunate economic and social conditions. It may even save a life or more. So hats off to Charles.
If you wish to join the Climate Bet of the Decade, i.e. Honeycutt Climate Bet For Charity, click here to see how. It’s real easy: just leave a reader comment and I’ll put your name, e-mail address, and amount on a list. That’s it. You’ll appear in the next update.
Here’s the latest list. I have to admit that the warmists, though small in number, are a generous bunch.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




They have pledged over $14,000.00 so far. The coolists have also made a good number of pledges, and have in fact far surpassed all my earler expectations. No matter who wins, some good will surely come out of this.
The cool side: (I hope my math is correct).

And the warm side:

If you wish to up your bet, just say so. If you wish to cancel it, well there I can’t help you. Pledged is pledged! 🙂
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWorld leaders don't discuss climate? Is it a non-issue?
I’ve been keeping an eye on the Wikileaks disclosure of classified documents, which has deeply embarrassed a wide collection world political leaders, and particularly the US government in general.
I was hoping that among all of these confidential documents, some would be protocols, messages or reports on discussions on climate policy. So I did some trolling in the Internet, hoping to find more on this, but came up with nothing. Admittedly my search was not the most thorough of searches.
Seems very odd that this “vital” issue, one we are told our very survival hinges on, is nowhere mentioned in any of these hot documents. Do you mean to tell me world leaders never talk about it behind the scenes? I have a truly difficult time believing that. Call it the deafening silence.
Either:
a) World leaders, in reality, couldn’t care less about the topic, and thus all the talk about rescuing the climate is just a facade issue in front cameras and mikes to distract the public, meaning the leaders never really talk about it behind the scenes, and so no climate-related protocols exist. This seems highly unlikely to me.
or
b) The leaked documents were selectively disclosed to simply to target and cause personal embarrassment to some world leaders, while others were kept under wraps, meaning climate-related documents were sifted away by Wikileaks or by the media outlets with whom they were entrusted (e.g. the Guardian and Der Spiegel). Selective release with the intent to produce a designed reaction would be immoral, irresponsible, arrogant and a complete abuse of journalistic power.
The danger and the probability here is that Wikileaks, or the media outlets with whom the documents have been entrusted, have carefully cherry-picked the documents in order to engineer and produce a desired political reaction. This is what makes the release of the documents so disturbing, dangerous, and untrustworthy. We probably are getting only the picture that the disclosers want us to see.
I’m not into conspiracy theories, but I do smell a rat. We’ve seen this pattern of behavior time and again in climate science journalism.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




If anyone knows of the existence of any climate-related documents that are already out in the public domain, we would all like to see them. All the data is needed to make an accurate picture of the situation.
================================================
UPDATE 1: h/t Ron de Haan: Wikileaks co-founder on Climategate E-mails:

At 4-min mark:
UK Intelligence tried to frame us as a conduit for the FSB because they didn’t like the truth of what was in those e-mails.”
They’re both hiding things.
Share this...FacebookTwitter "
"
Now I’ve heard everything. Talk about your “Kyoto protocol”. The original source of this silliness comes from the city of Kyoto. In June, in a bid to reduce greenhouse gases and perhaps become a nationally  designated “model environmental city,” the municipal government indicated it would request convenience stores to “voluntarily refrain” from staying open all night.
No Slushee for you!
You can read the complete story here in Japan Today. The worst part about this is the complete lack of understanding about where the major energy use is. Closing the store may result in some energy savings from lighting, but the main power use, refrigeration systems, and that Slushee machine, will still operate.
No more midnight Slushee! Maybe the real reason is the “exploitation of the polar bear” on the cup.
Here is more, a response from the  Japan Franchise Association
Convenience stores defend  24-hour operations
September 27th, 2008 by Jame, Japanprobe.com

Facing attack from critics that want convenience stores to shut down at night  as a measure to prevent global warming, the Japan Franchise Association has  responded by stating that convenience stores play a crucial  role as safe havens for lost children and victims of crime:
More than 13,000 cases of women finding refuge in convenience stores across  the country were reported during fiscal 2007. Nearly half of them occurred after  11 p.m. and about 40 percent were due to stalkers and molesters, the association  said.
In addition, there were 6,000 cases of lost children requiring assistance and  12,000 cases of elderly people found wandering the streets alone.
The 12 companies that comprise the JFA operate around 42,000 convenience  stores.
Explaining the significance of convenience stores, a JFA official said they  provide a “substitute for ‘koban’ (police boxes) and streetlights in the middle  of the night.”
The National Police Agency says that koban and “hashutsujo” police branch  offices are located at about 13,000 places across the country, but that number  is down by around 1,000 from five years earlier.
In addition, the JFA has also stated that convenience stores with limited  nighttime hours would still have to keep on their refrigeration systems when  closed, so the reduction in greenhouse gas emissions would be  negligible.
From the Japan Times article:
Behind moves to limit 24-hour business is concern about the environmental impact of round-the-clock operations. “Definitely, 24-hour operations eat up electricity,” he said.
Although acknowledging that some people are active late at night, for example because of their jobs, Ando went on to claim “the vast majority have standard lifestyles and get up in the morning and come home from school or work and sleep at night.”
“With no time left to waste to combat global warming, we are very concerned about whether it is really good (that stores) stay lit up even past midnight,” Ando said.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c80c691',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterDi kesempatan kali ini , Agen Poker online Indonesia akan mengemukakan beberapa info berkaitan bagaimanakah cara untuk Tingkatkan Kesempatan Menang Dalam Bermain Poker itu. Info kesempatan ini bisa kalian aplikasikan baik bagi kalian yang masih pemula dalam bermain poker atau kalian yang sudah pakar dalam bermain poker bentuk online itu. Mengenai info itu bisa kalian lihat serta kalian baca seperti berikut ini :
Mengerti basic dari permainan. Permainan apa pun yang kalian mainkan semua tentu mempunyai yang namanya ketentuan serta basic dari permainan tersebut tidak kecuali permainan poker itu. Permainan poker itu biasanya sama juga dengan permainan poker berbentuk online. Karena itu, jika kalian sudah kuasai permainan poker berbentuk konvesional itu tentu saja kuasai permainan poker berbentuk online itu.
Cara Jitu Dalam Bermain Poker Online Indonesia
Cobalah mengingat rutinitas musuh. Ini ialah langkah paling akhir yang dapat kalian kerjakan dalam bermain poker online, yakni dengan memperhatikan gerak musuh kalian seperti rutinitas musuh kalian. Ini bermanfaat untuk kalian dalam memastikan langkah apa yang perlu kalian kerjakan waktu musuh tengah lakukan kesukaannya itu. Satu diantara contoh dari hal itu bisa kalian lihat dari kartu yang sedang digengam oleh pemain itu. Jika kartu yang dipegangnnya itu ialah kartu yang lumayan bagus, tentu raut muka atau ekspresi yang diperlihatkan itu ialah raut muka suka dan lain-lain.
Dana taruhan. Jika kalian sudah lakukan 2 hal seperti yang kami berikan di atas itu langkah paling akhir yang perlu kalian kerjakan ialah mulai mempersiapkan dana yang cukup waktu bermain poker online itu pada bandar yang kalian percayai. Coba untuk mempersiapkan dana yang cukup di luar dari dana keperluan inti atau dana genting sebab ke-2 dana itu sebenarnya penting.
Hindari Berekspresi Berlebihan Dalam Bermain Poker Online Indonesia
Jauhi untuk berekspresi terlalu berlebih. Ini hal yang paling penting dalam bermain Poker Online Indonesia sebab beberapa pemain tentu bisa memeperkirakan kartu apa yang ada ditangan kalian hanya dengan melihat ekpresi kalian. Tentu saja kalian akan terlihat suka waktu tengah memperoleh kartu yang bagus serta begitupun sebaliknya. Untuk menghindari hal yang tidak dapat ditebak, lebih baik kalian menghindari berekspresi terlalu berlebihan saat bermain hingga musuh kalian tidak bisa memprediksi siapa serta kartu apa yang ada ditangan kalian sekarang.
Demikian beberapa info yang dapat kami berikan berkaitan bagaimanakah cara untuk tingkatkan kesempatan dalam bermain poker di situs poker online sah itu. Mudah-mudahan lewat info yang kami berikan itu bisa menolong kalian dalam bermain poker itu hingga potensi kalian dalam bermain poker itu makin baik daripada awalnya. Serta mudah-mudahan lewat info itu dapat juga tingkatkan kesempatan kalian dalam bermain poker itu hingga prosentase kemenangan kalian dalam bermain makin tinggi dibanding awalnya.
Share this...FacebookTwitter "
"

Some of the most pressing questions about global economies — whether governments are spending beyond their means, for example, and if so by how much — concern what economists call “fiscal imbalance.” Although this is a concept familiar to economists, it can often be difficult for non‐​economists to decipher. In“Fiscal Imbalance: A Primer” (White Paper), Director of Economic Policy Studies Jeffrey Miron provides a clear introduction to the concept of fiscal imbalance. Fiscal imbalance essentially concerns whether a government can continue forever to make the expenditures necessitated by its existing policies, given the expected revenues under those policies and the government’s debt. This includes its ability to borrow money in the future — which is not infinite. This imbalance, as Miron writes in **“U.S. Fiscal Imbalance over Time: This Time Is Different”** (White Paper), is growing. He projects fiscal imbalance for every year between 1965 and 2014, revealing that the United States has seen a rising fiscal imbalance since the early 1970s. “As of 2014, the fiscal imbalance stands at $117.9 trillion, with few signs of future improvement even if GDP growth accelerates or tax revenues increase relative to historic norms,” he warns. “Thus the only viable way to restore fiscal balance is to scale back mandatory spending policies, particularly on large health care programs such as Medicare, Medicaid, and the Affordable Care Act (ACA).”



 **THE COSTS OF GUN CONTROL**  
Gun control advocates persistently call for measures like universal background checks, a ban on high‐​capacity magazines, or a ban on so‐​called “assault weapons.” But, as associate policy analyst David B. Kopel argues in **“The Costs and Consequences of Gun Control”** (Policy Analysis no. 784), “Such proposals are not likely to stop a deranged person bent on murder.” Kopel examines the actual costs and benefits of these popular gun‐​control measures, demonstrating that they would prove largely ineffective. “Before adding new gun regulations to the legal code, policymakers should remember that several mass murders in the U.S. were prevented because citizens used firearms against the culprit before the police arrived on the scene,” he warns.



 **MURDER AS A THREAT TO FREE SPEECH**  
The brutal Charlie Hebdo killings last year were a shocking act of violence, but unfortunately, not the first violent reactions to speech perceived as blasphemy. As Robert Corn‐​Revere , a partner at Davis Wright Tremaine LLP, writes in **“To Confront the Assassin’s Veto, or to Ratify It?”** (Working Paper no. 36), “This was yet another grim marker in the cross‐​cultural conflict illustrated by events such as the Ayatollah Khomeini’s 1989 fatwah against Salman Rushdie for writing _The Satanic Verses_ , the 2004 murder of filmmaker Theo van Gogh on the streets of Amsterdam for perceived insults to Islam, and the violent reaction to the cartoons of Mohammad published in the Danish newspaper _Jyllands‐​Posten_ in 2005.” Corn-Revere’s paper confronts the question of how the law should deal with these sinister attempts to chill speech.



 **ESAS: EMPOWERING STUDENTS AND FAMILIES**  
“Every child deserves the chance at a great education and the American dream,” Cato policy analyst Jason Bedrick, Goldwater Institute education director Jonathan Butcher, and former Goldwater Institute vice president for litigation Clint Bolick — who has since been appointed to the Arizona Supreme Court — write in **“Taking Credit for Education: How to Fund Education Savings Accounts through Tax Credits”** (Policy Analysis no. 785). In an effort to improve education, several states have passed laws allowing students to receive an Education Savings Account (ESA) which parents can put toward alternative education services. In this analysis, the authors show how legislators can design ESAs that will work in the 40 states with constitutional provisions that prohibit the use of public funds at religious schools. “Tax‐​credit‐​funded ESAs would empower families with more educational options while enhancing accountability and refraining from coercing anyone into financially supporting ideas they oppose,” they write.



 **CHINA AT A CROSSROADS**  
China, as Cato vice president James Dorn puts it, is “at a crossroads.” It has made tremendous progress in recent years by expanding the market and strengthening property rights. But at the same time, its powerful one‐​party state maintains a strong grip on citizens’ private and commercial dealings. “The damage China’s illiberal state has inflicted on the nation is becoming evident as the economy slows, debts mount, and state‐​owned enterprises (SOEs) draw capital away from the more productive private sector,” writes Dorn in **“China’s Challenge: Expanding the Market, Limiting the State”** (Working Paper no. 34). He highlights the importance of renewing interest in China’s ancient culture and writings on topics like freedom and limited government — a legacy which its authoritarian leaders have obscured.



 **THE LUKEWARMING WORLD**  
In **“Climate Models and Climate Reality: A Closer Look at a Lukewarming World”** (Working Paper no. 35), Center for the Study of Science director Pat Michaels and assistant director Chip Knappenberger further the case for the “lukewarmers” — those who believe that the evidence for some human‐​caused climate change is persuasive, but that, contrary to the alarmists, this warming occurs in accordance with the lower end of expectations from mainstream science. They contend that the rate of warming over the past several decades has been so slow it was “completely unexpected” by any of the climate models — “a worrying indication that the current stateof‐ the‐​art climate models are not up to the task of simulating the actual behavior of the earth’s climate.” This consequently throws efforts to implement climate policy based on these models into serious doubt.



 **THE EVOLUTION OF WEAPONRY**  
Technological advances in recent years have led to a bevy of increasingly small, cheap, and sophisticated weapons. “This new diffusion of power has major implications for the conduct of warfare and national strategy,” U.S. National Defense University distinguished research fellow T. X. Hammes argues in “Technologies Converge and Power Diffuses: The Evolution of Small, Smart, and Cheap Weapons” (Policy Analysis no. 786). Hammes delves into the particular challenges posed by various types of emerging technology, like drones, artificial intelligence, and nanoenergetics, or explosives. With such abundant and affordable technology available, the United States may be exposed to much more danger when waging military campaigns in the future. “Increasingly,” he writes, “we will have to ask the question ‘Is the strategic benefit of an intervention worth the cost when the enemy can strike back in and out of theater?’”
"
"

The Clinton administration has developed a nasty habit of using personal tragedy to further its global warming agenda. From the snowmelt‐​caused Red River flood last year, to Florida’s fires this summer (which blazed because there was too much vegetation), to Hurricane Mitch, if there’s any possible way to conflate human suffering with global warming, the administration will do so. 



Administration antics on Mitch, a real son of a gun when it came to flooding rain, began during the recent Buenos Aires conference on global warming. There, the head of the U.S. Agency for International Development, J. Brian Atwood, told CBS News that Hurricane Mitch, which killed an estimated 10,000 Central Americans, was a “classic greenhouse effect.” One hopes that Mr. Atwood actually knows better and is merely engaging in White House huckstering. 



In 1974 Hurricane Fifi killed the same proportion of the (then smaller) population of Honduras. In 1971 Hurricane Edith plowed into the northwestern tip of Honduras at Cabo Gracias a Dios as a Category 5 blaster. In 1955 Hurricane Janet, another Category 5 storm, had hit a couple of hundred miles to the south of where Edith landed. Only two storms of that magnitude have ever hit the United States. Flooding is another recurring phenomenon. In 1979 Tropical Storm Claudette produced five feet of rain in Texas — just like Mitch in Honduras — but killed about 10,000 fewer people. 



Certainly Atwood’s staffers could have apprised him of the refereed scientific literature on global warming and hurricanes. It contains two speculative papers saying that hurricanes may get worse and an overwhelming number of others proving that notion wrong. In addition, hurricane observations in warm years and during planetary warming argue more for the opposite — weaker storms. 



If there’s any possible way to conflate human suffering with global warming, the Clinton administration will do so. 



The first paper, published in 1987 in Nature by Kerry Emanuel, speculated that under unrealistic, physically impossible conditions, global warming would increase the strength of hurricanes. While that paper generated a lot of press, scientists knew it was merely an exercise in hurricane vortex mathematics. Emanuel threw more fuel on the fire when he published an article on “hypercanes” in the widely read American Scientist. 



The larger community of hurricanologists had by then had enough. In 1994 James Lighthill published in the Bulletin of the American Meteorological Society an extensive review finding no basis for Emanuel’s speculation. 



In 1996 a storm surge of literature blew apart Emanuel’s hypothesis. First, the aptly named Chris Landsea, a scientist at the National Hurricane Center, observed in Geophysical Research Letters that there had been a significant decline in the frequency of severe hurricanes in the Atlantic Basin over the last 50 years. The region warmed a few tenths of a degree during the period. 



Later that year Johnny Chan published in the same Journal an article finding no net trend in Pacific typhoons. Even the computer modelers got into the act. Europe’s Lennart Bengtsson published a paper in the journal Tellus showing that a computer climate simulation with an enhanced greenhouse effect predicted fewer hurricanes and lower average winds. 



Also in 1996 J. B. Elsner found that the regions in which hurricanes form had shifted in the last 40 years and now favor the development of weaker storms. And Landsea, in yet another report published by the United Nations Intergovernmental Panel on Climate Change in 1996, showed that there has been a statistically significant decline in the maximum windspeed measured in Atlantic hurricanes since World War II. 



Australian climatologist Ann Henderson‐​Sellers and 10 others re‐​reviewed the hurricane/​global warming situation in last January’s Bulletin of the American Meteorological Society. They called their work a “Post IPCC Assessment,” meaning that they believed it stood for the consensus of scientists on the issue. They simply could not find any increase in hurricane frequency or severity, and they looked everywhere. They also took pains to note that the conditions assumed in Emanuel’s initial work are just about impossible. That work and its ilk contain “known omissions [that] all act to reduce these increases” of hurricanes, Henderson‐​Sellers wrote. 



It’s rare to get such scientific consensus in climatology. But in one last attempt to bring the exaggerators and the alarmists to heel, Henderson‐​Sellers recently published in the journal Climate Change a paper detailing the whole sorry history of the campaign to hype hurricanes. Ironically, Henderson‐​Sellers herself is not shy about touting the dangers of global warming. 



Who gains here? Rumors persist that Vice President Gore has been advised to make global warming a central theme of his presidential run in 2000. Threatening hundreds of thousands with imminent drowning unless they vote for him is a crude but probably effective trick.
"
"

A few weeks ago, the Clinton administration finally got around to signing the Kyoto Protocol, the global warming treaty that obligates the United States to reduce greenhouse gas emissions by 7 percent below 1990 emission levels by the year 2012. Both supporters and opponents of the treaty agree that meeting that goal will require between a 30 and a 40 percent reduction in the greenhouse gas emissions that would otherwise occur about a decade hence. Beyond that, sorting out the scientific and economic argument between the two is difficult for a nonspecialist. Here’s the scorecard to date. 



Treaty supporters say that just about all the scientists engaged in global warming research now accept that the problem is real and must be addressed. Well, yes and no. Most (but by no means all) scientists engaged in the field agree that industrial emissions are probably affecting the climate. But the evidence is circumstantial. As the United Nations’ International Panel on Climate Change (IPCC) noted in its most recent (1995) report, the evidence thus far “cannot be considered as compelling evidence of cause‐​and‐​effect link between anthropogenic forcing and changes in the Earth’s surface temperature.” 



The “balance of evidence suggests” (in the words of the IPCC) that industrial emissions are the culprit, but that’s hardly conclusive. And the consensus about the matter is not as nearly universal as suggested. Seventeen thousand scientists (half of whom are trained in physics, geophysics, climate science, meteorology, oceanography, chemistry, biology or biochemistry) recently signed a petition written by Frederick Seitz, a past president of the National Academy of Sciences, declaring that there is no compelling evidence to justify reducing greenhouse gas emissions at all. 



Nor do scientists agree on how hot it would get if we did nothing. The IPCC’s “best guess” back in 1990 was that industrial greenhouse gas emissions would increase average global temperatures by 5.8 degrees (all temperature figures are Fahrenheit) by the end of the next century. In 1995 the IPCC adjusted its “best guess” down to 3.6 degrees. Three studies published this year (by Hansen, Dlogokencky and Myhre) suggest that the present “best guess” stands at about 2.25 degrees of warming by the end of the next century. Most experts doubt that that amount of warming would be particularly worrisome (indeed, we’re already about half way there temperature‐​wise, and the effect of this “global warming” has thus far proven underwhelming, to say the least). 



Killing the coal industry to reduce temperatures 1/​7th of 1 degree 50 years hence is justified by treaty advocates as a necessary “first step” of about 30 that must necessarily come. Treaty opponents do a quick cost/​benefit analysis and conclude that treaty advocates have lost their grip on reality. 



While the Kyoto Protocol envisions significant cuts in greenhouse gas emissions, scientists on all sides of the debate agree that its impact will be virtually undetectable. Tom Wigley, a highly respected senior scientist at the U.S. National Center for Atmospheric Research (and a scientist, moreover, usually thought of as in the alarmist camp), recently calculated that the Kyoto Protocol would only reduce temperatures by 0.13 degrees by 2050 if we accept the IPCC’s 1995 estimate of warming under a business‐​as‐​usual scenario. The Kyoto Protocol would have no meaningful impact on future climate change because, as along as we use fossil fuels, the question of global warming is not “if” but “when.” 



Finally, advocates of the treaty argue that its costs will be negligible, while opponents warn of a replay of the 1970s energy crises. The best evidence for each argument comes from studies issued by the Clinton administration (the study most optimistic about costs comes from the president’s Council of Economic Advisers (CEA), the most pessimistic from the Energy Information Administration). Both studies make valid projections. The differences are found in the assumptions. But it’s worth noting that the economic models relied on by the CEA reveal that, absent a truly global emissions trading system (a system that was hotly opposed by most nations at the recently concluded talks in Buenos Aires), America would be forced to abandon coal‐​fired electricity within the next decade to keep compliance costs from skyrocketing. 



Killing the coal industry to reduce temperatures 1/​7th of 1 degree 50 years hence is justified by treaty advocates as a necessary “first step” of about 30 that must necessarily come. Treaty opponents do a quick cost/​benefit analysis and conclude that treaty advocates have lost their grip on reality. 



And that, dear readers, is where we stand today. Misleading public characterizations of the global warming debate notwithstanding, the case for the Kyoto Protocol is pretty threadbare. Of course, the Senate is unlikely to ratify the treaty, a fact conceded by the administration in its decision not to send it up for a vote until at least 2001. If the past is any prologue, the case for ratification will continue to weaken. The question is, Will anyone be able to see through the political hot air to notice?
"
"

Current image from Terra Satellite, rotated 90 degrees to improve view, plus annotation and world view inset added by Anthony
Source image is available here at the NASA Terra website
North Pole to remain frozen
By            Bill Scanlon, Rocky Mountain News
Originally published 02:57 p.m., August 29, 2008
Updated 02:57 p.m., August 29, 2008
Santa can rest easy.
It’s looking like the ice at the North Pole won’t melt to water next month, as had been feared. It would have been the first time in thousands of years that the most northerly place on the planet would have been ice-free.
“It’s quite unlikely at this point,” Walt Meier a research scientist at the University of Colorado’s National Snow and Ice Data Center, said today.
The ice in the Arctic Ocean is at near historic lows, and breaks records every couple of years due to human-caused global warming, the scientists at NSIDC say.
This spring, it was looking like the ice might retreat so far that the North Pole itself would be ice-free for at least a day in September – the height of the ice-melt season.
The chances were great enough that the scientists at NSIDC were laying almost even odds on it in an office pool.
But while global warming is playing an important role, seasonal variability does, too. And this summer turned out to be a little cooler than last summer, when the record for ice retreat was set, Meier said.
“We only have about two or three weeks more of ice melt, and it’s not going to make it to the North Pole,” Meier said.
Read the rest of the article here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d2f17a8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
So far, SC24 solar magnetic activity has been in a relative funk. See my post on this very issue from last month.
Leif Svalgaard points out this new paper in AGU from Keating, and kindly placed a copy on his own website for us to examine: Link to Keating-Bz.pdf
The crux of the paper is a forecast, which extends significantly into SC24, even though there is just a small number of observed data points:
Fig. 1. Actual boxcar averages for measured Bz(m) magnitude and the forecast results of applying the McNish- Lincoln technique. Actual data are represented by solid squares, while the calculated results are shown as a curve. The correlation between the two is due to the fact that the McNish-Lincoln method uses actual data when available. The calculated forecast is performed only for the time period after the end of the actual data. This plot shows that Bz(m) reached its minimum average magnitude in mid-2007 and has begun to increase in magnitude. The forecast is that it will continue to increase slowly through the first part of 2008, but will then begin to rapidly increase in magnitude beginning in the latter part of this year, reaching its first peak in late 2009.
There seem to be two schools of thought on the activity level of SC24, those who think it will be very low, and those that think it will be higher than normal.
Dr. Svalgaard goes on record here on this blog in saying: 
I’ve been predicting that SC24 would be the smallest cycle in a century, so it is no surprise that it starts out weak and anemic.
While I’m certainly no solar expert, based on what I’ve seen thus far, I’m inclined to agree. I think that Keating’s prediction will not be realized.
This graph of Ap magnetic index will be updated in a few days, with the uptick this month in SC24 spots, perhaps we’ll also see a corresponding uptick in the Ap Index.
From the data provided by NOAA’s Space Weather Prediction Center (SWPC) you can see just how little magnetic field activity there has been. I’ve graphed it below with the latest available data from October 6th, 2008:

click for a larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b561ad0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Global warming is increasing Western wildfires! 



At least that’s what lots of news stories said in response to a July 6 Sciencexpress paper by A.L. Westerling of the Scripps Institute of Oceanography and three co‐​authors. 



Like most scientific issues, though, this one is more complicated than the headlines suggest. 



Westerling examined wildfire data between 1970 and 2003 and found that major fires have occurred four times as frequently, on average, since 1986 than they did from 1979 through 1985. But he had one key conclusion: “Whether the changes observed in western hydro‐​climate and wildfire are the result of greenhouse gas‐​induced global warming or only an unusual natural fluctuation, is presently unclear.” 



Why so unclear? In large part, because the science isn’t straightforward, and three decades is a very short period of climate time. 



Snowmelt and temperature are thought to be the driving factors for Western wildfires. The years with high wildfire frequency tend to be those in which snow begins melting earlier than normal, which the authors of the paper said was “not surprising.” But examination of Westerling’s own data shows no significant difference in snowmelt timing between the periods of 1970 to 1985 and 1986 to 2003. 



However, spring and summer regional temperatures have gone up slightly — about 1 degree Celsius — over the same period, which serves to increase evaporation and, thus, flammability. 



Changes of this sort are certainly not unprecedented. Rather than limiting the perspective to 34 years, why not look at the last 1,200? Two years ago, Columbia University scientist Edward Cook and several colleagues reconstructed the West’s drought history back to 800 A.D. They wrote that “compared to earlier megadroughts that are reconstructed to have occurred around A.D. 936, 1034, 1150, and 1253 … the current drought does not stand out as an extreme event, because it has not yet lasted nearly as long.” 



In fact, Cook’s study shows a general decline in Western drought over the last millennium, with the recent era looking pretty much like the long‐​term average. In other words, the West is naturally accustomed to more drought than it has experienced since it was colonized by immigrants. It is also worth noting that the Western population boom began in the early 20th century, the wettest era of the last 1,200 years. 



What are perhaps more interesting are the changes in overall moisture that have accompanied the warming of the U.S. in 20th century. 



“Drought” is a combination of lack of rain and increasing evaporation. The latter is obviously dependent upon temperature, as more surface moisture evaporates into a warmer atmosphere. 



But as the country warmed, precipitation also went up. In fact, it went up far more than evaporation did. So, all else being equal, the U.S. as a whole is a wetter place than it was before the planet’s surface temperature began to rise. 



This doesn’t mitigate the fact that the West is a dry, fire‐​prone region, and will continue to be, if history is any guide. But relating human‐​induced global warming to Western drought is more difficult. 



There are many indices of drought severity, all of which attempt to balance rainfall, evaporation, streamflow and other factors. Perhaps the most often used is the Palmer Drought Severity Index. It has been around for more than half a century, and the National Climatic Data Center, in Asheville, N.C., records it for different regions of the country. 



Most scientists think humans are behind the planetary warming that began in the mid‐​1970s. (Another warming of similar magnitude occurred in the early 20th century, but was entirely natural in origin). But what is the relationship between global warming and drought in the Western U.S.? 



There isn’t any. Statistically speaking, the correlation is zero, which means that as humans have warmed the planet, they haven’t influenced Western drought. This holds whether one starts at the beginning of the Palmer record, in 1895, or the first year of Westerling’s study, 1970. 



Seeing as we have had about a hundred years of global warming, the lack of a clear relationship between earth temperature and Western drought is reassuring, because the relationship between drought and fire is real, even if it is more complicated than people are led to believe.
"
"
Share this...FacebookTwitterReader Bernd Felsche reminds us of this excellent post by Christoper Hitchens here, which is indeed worth bringing up again today, particularly in light of the dark closed minds that have lately attempted to claim that no matter what the weather does today, man is the culprit.
An identical mindset infected so called leading intellectuals back during the Little Ice Age. As Dr. Baliunas explains, during the Medieval Warm Period, Europe enjoyed a warm climate, which allowed society to flourish.
But around the 15th century, the Little Ice Age began and the weather deteriorated. Agriculture suffered and people starved. The low-point was from 1550 to 1700. The intellectuals back then were sure it was man’s fault. A mass wave a institutionally legalized executions ensued.
Dr. Baliunas (emphasis added):
How unusual was this very intense period of the Little Ice Age? On the afternoon of August 3rd, 1562, a thunderstorm struck Central Europe across a front several hundred km long. After raging for several hours, the storm unleashed a terrific hail that continued until midnight. It destroyed crops. It destroyed vineyards, birds and unprotected horses and cows. Dirists noticed something we hear today. They said for 100 years such a storm had never been seen. The storm was deemed so unusual in this period of superstition, that it had to be unnatural; it had to be supernatural.
Thus superstition and witchcraft bred a precautionary response: Eradicate those responsible for the storm and the period of new storminess. Now it was well known that people could cook weather with the help of Satan. So thus did extreme conditions of the severest part of the Little Ice Age contribute to Europe’s most horrific period of mass executions a witch trials. This was completely legal and it was undertaken, administered by highly educated upper social strata. These were institutionally legalized executions for sorcery.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sounds very familiar today, doesn’t it? How does it compare to PIK, RC and GISS? Watch this speech by Dr Baliunas. Humanity is always only a step away from with-burning and Holocausts. It has happened time and again. The only thing that saves us, is the strong rule of democracy. How often have we heard leading “intellectuals” question democracy lately? We see many similarities today in the thinking of the upper social strata. “Man is responsible” – especially the deniers.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterWhat is Europe to do? The old continent wants to save the world but it seems no one wants to play along.
Before the summer, there seemed to be some hope the USA could pass a climate rescue bill with President Barack Obama and the Democrat-held Congress. But that hope has all but completely disappeared.
So writes the very green Austrian Der Standard here: Europe Should Focus On China For Climate Protection, Not USA.
With the Democrats getting voted out of office in Congress, the respective concepts for protecting the climate will wind up in the trash can. Europe should quickly search for willing partners.
All together now: Ohhhhh! How dreadful!
But hope is the last to die, and not quite everyone has lost it. Yet, Der Standard sees the writing on the wall:
While there are still a few NGOs who have hope in the good of people and Obama, there’s no one left in the USA who is ready to bet even a single cent on a US climate law could get off the ground.
Jeffrey Sachs, Director of the Earth Institute at Columbia University in New York, and occasionally a columnist for Der Standard says:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There’s nothing left to salvage!
According to Der Standard:
In an interview conducted via Skype with participants of a Climate Workshop in Brussels, Sachs stated that people in the United States have practically zero interest in climate protection. At least for the next two years, but likely it’ll be much longer before any real kind of initiatives on the issue of climate protection can be expected.
Der Standard mopes that the oil disaster in the Gulf of Mexico had no impact on the public’s opinion, and that a wide majority of Americans favor offshore oil drilling. Now come the old conspiracies and paranoia. Der Standard:
The reason is fear of losing jobs and an unprecedented propaganda machine that was put into motion by the oil industry and its allies.
These allies include a number of evil European corporations like Eon,  BP, Bayer, Basf, Arcelor oder GDF-Suez. Now the enviro-malcontents in Europe and at Der Standard say Europe has to shift its focus to China. Der Standard writes that China has become a “high flyer” in wind and solar energy. Partnership there must be enhanced.
Europe has to accept that there is nothing more to gain from the USA, except frustration. It makes much more sense to work on mutual interests with China.
Consider this a major milestone. Cap & Trade is dead for good in the United States. Even Europe sees it.
Share this...FacebookTwitter "
"
Part I: Ranking global warming among present-day risks to public health.

Guest essay by Indur M. Goklany
There seems to be no limit to the hyperbole surrounding climate change – and that’s no hyperbole. Numerous politicians have informed us over the years that climate change is one of the most important problems facing mankind.  In fact, U.N. Secretary General Ban Ki-moon has called it the defining challenge of our age.”
But is it?
I answer this question in a paper just published in the refereed section of Energy & Environment.
A 2005 review article in Nature on the health impacts of climate change estimated that 166,000 deaths were “attributable” to climate change in 2000. This estimate was derived from a World Health Organization (WHO) sponsored study that even the study’s authors acknowledge may not “accord with the canons of empirical science” (see here). But I will accept this flawed estimate as gospel for the sake of argument.
In the year 2000, however, there were a total of 56 million deaths worldwide. Thus, climate change may be responsible for less than 0.3% of all deaths globally (based on data for the year 2000). This places climate change no higher than 13th among mortality risk factors related to food, nutrition and environment, as shown in the following table.
Specifically, climate change is easily outranked by threats such as hunger, malnutrition and other nutrition-related problems, lack of access to safe water and sanitation, indoor air pollution, malaria, urban air pollution. And had I included other risks to public health beyond environmental, food and nutritional factors (e.g., HIV/AIDS, TB, various cancers, etc.) then climate change would have ranked even lower than 13th.
With respect to biodiversity and ecosystems, today the greatest threat is what it always has been – the conversion of land and water habitat to human uses, i.e., agriculture, forestry, and human habitation and infrastructure. See, e.g., here.
Climate change, contrary to claims, is clearly not the most important environmental, let alone public health, problem facing the world today.
But is it possible that in the foreseeable future, the impact of climate change on public health could outweigh that of other factors?
I will address this question in subsequent blogs.



Risk factor

Ranking


Mortality   (millions)


Mortality   (%)



Blood   pressure
1
7.1
12.8


Cholesterol
2
4.4
7.9


Underweight   (hunger)
3
3.7
6.7


Low fruit   & vegetables
4
2.7
4.9


Overweight
5
2.6
4.6


Unsafe water,   poor sanitation
6
1.7
3.1


Indoor smoke
7
1.6
2.9


Malaria

1.1
2.0


Iron   deficiency
8
0.8
1.5


Urban air   pollution
9
0.8
1.4


Zinc   deficiency
10
0.8
1.4


Vitamin A   deficiency
11
0.8
1.4


Lead exposure
12
0.2
0.4


Climate   change
13
0.2
0.3


Subtotal
27.6
49.4


TOTAL from all causes
55.8
100.0



Priority ranking of food, nutritional and environmental problems, based on global mortality for 2000. Source: I.M. Goklany, Is Climate Change the “Defining Challenge of Our Age”? Energy & Environment 20(3): 279-302 (2009), based on data from the World Health Organization. Note that malaria isn’t ranked in this table because deaths due to malaria were attributed by WHO to climate change, underweight, and zinc and vitamin A deficiencies.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96d06e75',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
A Yogi Berra moment – “it’s deja vu all over again…”
From NHC Public Advisory #25
DATA FROM AN AIR FORCE RECONNAISSANCE AIRCRAFT INDICATE THAT MAXIMUM SUSTAINED WINDS HAVE INCREASED TO NEAR 150 MPH…240 KM/HR…WITH HIGHER GUSTS.  GUSTAV IS AN EXTREMELY DANGEROUS CATEGORY FOUR HURRICANE ON THE SAFFIR-SIMPSON HURRICANE SCALE. SOME FLUCTUATIONS WITH AN OVERALL SLIGHT STRENGTHENING IS FORECAST DURING THE NEXT 24 HOURS…AND GUSTAV COULD REACH CATEGORY FIVE INTENSITY DURING THIS PERIOD. GUSTAV IS FORECAST TO REMAIN A MAJOR HURRICANE THROUGH LANDFALL ALONG THE NORTHERN GULF COAST.
Here is my own hurricane track imagery of Gustav and Hanna:

Click for a Hi Definition image
I’m sure this will become mass-media fodder again for the ever popular “global warming causes more damaging hurricanes”, but it is important to note that NHC’s own science officer, Christopher Landsea, co-authored a paper that claims otherwise. So have other scientists.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cec94d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterNOAA 8 inch rain gage. Source: http://www.crh.noaa.gov/iwx/?n=coop_station
German Weather Service meteorologist Christoph Hartmann writes what I think is a surprising essay on measuring precipitation, and the errors in doing so. Indeed Hartmann says precipitation may be understated by up to 50%, or much more at some locations.
As Hartmann explains, measuring precipitation is by no means an exact science, and results have to be taken with a lump of salt.
There are many sources of errors, and in his essay here he looks at just two main sources: wind and instrumentation.
But first, let’s take a look at how precipitation is measured. In his previous essay he described two types of precipitation measuring gages. In Germany precipitation is measured with the unit of liters/m², e.g. 25.4 liters is an inch of rain.
Two methods of measuring precipitation
Hartman explains that precipitation is generally measured by a rain gage with a known opening area, for example 200 cm² in Germany, which is positioned 1 meter above the ground surface. The gage funnel catches the precipitation and leads it to either
1) a graduated measuring tube or a
2) an optical drop counter 
Optical rain gage (drop counting). Source: atmos.washington.edu
With the measuring tube system, the tube is graduated and the amount of precipitation can be simply read off. With the optical rain gage (drop counter), the amount of precipitation is derived from the number of drops. If the precipitation is snow or ice, then the measuring tube or optical gage are brought inside and the captured precipitation is melted and measured.
Wind and errors up to 400%
Hartmann explains that the biggest sources of error are wind-related. This is easily seen when measuring snowfall. Just before a snowflake falls into the gage, air turbulence sucks it back out tosses it overboard. Just taking a look around after a blizzard, it’s easy to imagine how difficult it is to measure snowfall. Places exposed to wind are barren, while other places are covered by meter-deep snowdrifts. How much snow really fell?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Hartmann says measurement errors of up 400% can occur over time when measuring powdery snowfall in alpine, polar or windy areas.
One way to reduce error is to place the instrument in a wind-protected area. By measuring the wind speed, it is then possible to adjust precipitation measurements. But Hartmann writes:
Wind effects lead to an under-estimation of the actual fallen precipitation. The level of deviation depends on the speed of the wind and the type of precipitation.
Because wind speeds are factored into precipitation measurements, climatological precipitation trends without taking changes in wind speeds into account should always be deduced very carefully.
The second problem encountered arise from the two above described measurement instruments, especially with the optical rain gage, writes Hartmann. With frozen precipitation, the gages are heated up in order to melt the precipitation. But this involves evaporation. And under torrential rains, the optical gage becomes much less accurate. The result, writes Hartmann:
Under equal precipitation amounts, the optical gage measures less precipitation than the measuring tube, both in summer and in winter.
So if two different stations use different instruments, them they will show different precipitation amounts even when the actual precipitation is the same. In summary, Hartmann writes his stunning conclusion:
In total these two sources of errors lead to a precipitation deficit of 5 to 15% for liquid precipitation, and between 20 and 50% for solid [frozen] precipitation. In very windy locations, the deficits are substantially more.
Because instruments measure less precipitation than what actually falls, it means we have a worldwide precipitation deficit solely because of the measurement method.
What does it all mean? Are many of the reported droughts solely the product of faulty readings? And we all thought that the network of temperature measurement stations was a mess. This is a huge open floodgate to potential climatological data manipulation and bogus assertions. See here for example: motherjones – the coming mega-drought (h/t NTZ reader DirkH).
Share this...FacebookTwitter "
"

One of the few things that BOTH sides of the Carbon Dioxide and AGW debate seem to be able to agree on is the belief that CO2, as a trace gas, is “well-mixed” in the atmosphere. Keeling’s measurements at Mauna Loa and other locations worldwide rely on this being true, so that “hotspots” aren’t being inadvertently measured.
As support for this, if you do some Google searches for these phrases, you’ll get hundreds of results of the usage together:
CO2 + “well mixed”
“carbon dioxide” + “well mixed”
You’ll find complete opposites using the same “well mixed” phrase, for example:

Gavin Schmidt of Real Climate writes in comment # 162 of this thread on Realclimate.org
“A full doubling of CO2 is 3.7 W/m2, and so by looking at all well-mixed GHGs you get about 70% of the way to a doubling.”
Roger Pielke Sr. writes in April 2008:
“…and thus are not providing quantitatively realistic estimates of how the climate system responds to the increase in atmospheric well mixed greenhouse gases in terms of the water vapor feedback.”
You’ll also find the phrase in use in titles of scientific papers, for example this one published in the AGU:
New Estimates of Radiative Forcing Due to Well Mixed Greenhouse Gases
And you’ll find the phrase used in popular media, such as this article from the BBC:
Carbon dioxide continues its rise
In describing the emasurements of CO2 at Mauna Loa Observatory: “The thin Pacific air is ideal for this research since it is “well-mixed”, meaning that there is no obvious nearby source of pollution, such as a heavy industry, or a natural “sink”, such as forest which would absorb CO2.”
Hmm, “no obvious nearby source of pollution” I suppose the volcanic outgassing nearby doesn’t count as “pollution” since it is natural in origin.
So it seems clear that there is a broad agreement on the use of the term. I suppose you’d call that “scientific consensus”.
So it was with some surprise that I viewed this image from NASA JPL, a global CO2 distribution as measured by satellite:

Note the variations throughout the globe, ranging from highs of 382 PPM to lows around 365 PPM. There is a whole range of data and imagery like this above available here
My question is: how does this global variance translate into the phrase “well-mixed” when used to describe global CO2 distribution? It would seem that if it were truly “well-mixed”, we’d see only minor variances on the order of a couple of PPM. Yet clearly we have significant regional and hemispheric variance.
NASA JPL provides this caption to help understand it:
Although originally designed to measure atmospheric water vapor and temperature profiles for weather forecasting, data from the Atmospheric Infrared Sounder (AIRS) instrument on NASA’s Aqua spacecraft are now also being used by scientists to observe atmospheric carbon dioxide. Scientists from NASA; the National Oceanic and Atmospheric Administration; the European Center for Medium-Range Weather Forecasts; the University of Maryland, Baltimore County; Princeton University, Princeton, New Jersey; and the California Institute of Technology (Caltech), Pasadena, Calif., are using several different methods to measure the concentration of carbon dioxide in the mid-troposphere (about eight kilometers, or five miles, above the surface). The global map of mid-troposphere carbon dioxide above, produced by AIRS Team Leader Dr. Moustafa Chahine at JPL, shows that despite the high degree of mixing that occurs with carbon dioxide, the regional patterns of atmospheric sources and sinks are still apparent in mid-troposphere carbon dioxide concentrations. “This pattern of high carbon dioxide in the Northern Hemisphere (North America, Atlantic Ocean, and Central Asia) is consistent with model predictions,” said Chahine. Climate modelers, such as Dr. Qinbin Li at JPL, and Dr. Yuk Yung at Caltech, are currently using the AIRS data to study the global distribution and transport of carbon dioxide and to improve their models. 
As we’ve found with surface based temperature measurement, it seems the more we look at satellite data, the more we learn that our earth bound assumptions based on surface measurement don’t always hold true.
When measuring the planet, looking at the whole planet at one time seems a better idea than trying to measure thousands of data points at the surface, sorting out noise, doing adjustments to “fix” what is perceived as bias, and assuming the result is accurately representatiive of the globe.
UPDATE: 7/31/08 I got a response from the AIRS team on satellite CO2 measuremenst, see this new posting
We won’t have to rely on ground based CO2 measurements much longer.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9db144bd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Regular WUWT readers know of the issues related to Arctic Sea Ice that we have routinely followed here. The Arctic sea ice trend is regularly used as tool to hammer public opinion, often recklessly and without any merit to the claims. The most egregious of these claims was the April of  2008 pronouncement by National Snow and Ice Data Center scientist Dr. Mark Serreze of an ice free north pole in 2008. It got very wide press. It also never came true.
To my knowledge, no retractions were printed by news outlets that carried his sensationally erroneous claim.
A few months later in August, when it was clear his first prediction would not come true, and apparently having learned nothing from his first incident (except maybe that the mainstream press is amazingly gullible when it comes to science)  Serreze made another outlandish statement of “Arctic ice is in its death spiral” and” The Arctic could be free of summer ice by 2030″. In my opinion, Serreze uttered perhaps the most irresponsible news statements about climate second only to Jim Hansen’s “death trains” fiasco. I hope somebody at NSIDC will have the good sense to reel in their loose cannon for the coming year.
Not to be outdone, in December Al Gore also got on the ice free bandwagon with his own zinger saying on video that the “entire north polar ice cap will be gone within 5 years“. There’s a countdown watch on that one.
So it was with a bit of surprise that we witnessed the wailing and gnashing of teeth from a number of bloggers and news outlets when in his February 15th column, George Will, citing a Daily Tech column by Mike Asher, repeated a comparison of 1979 sea ice levels to present day. He wrote:
As global levels of sea ice declined last year, many experts said this was evidence of man-made global warming. Since September, however, the increase in sea ice has been the fastest change, either up or down, since 1979, when satellite record-keeping began. According to the University of Illinois’ Arctic Climate Research Center, global sea ice levels now equal those of 1979. 
The outrage was immediate and widespread. Media Matters: George Will spreads falsehoods Discover Magazine: George Will: Liberated From the Burden of Fact-Checking Climate Progress: Is George Will the most ignorant national columnist? One Blue Marble Blog: Double Dumb Ass Award: George Will George Monbiot in the Guardian: George Will’s climate howlers and Huffington Post: Will-fully wrong
They rushed to stamp out the threat with an “anything goes” publishing mentality. There was lots of piling on by secondary bloggers and pundits.
Feb 15th NSDIC Arctic Sea Ice Graph - click for larger image
Meanwhile, back at the ranch, I got interested in what was going on with odd downward jumps in the NSIDC Arctic sea ice graph, posting on Monday February 16th NSIDC makes a big sea ice extent jump – but why? Then when I was told in comments by NSIDC’s Walt Meier that the issue was “not worth blogging about” I countered with Errors in publicly presented data – Worth blogging about? 
It soon became clear what had happened. There was a sensor failure, a big one, and both NSIDC and Cryosphere today missed it. The failure caused Arctic sea ice to be underestimated by 500,000 square kilometers by the time Will’s column was published. Ooops, that’s a Murphy Moment.
So it is with some pleasure that today I offer you George Will’s excellent rebuttal to the unapologetic trashing of his column . The question now is, will those same people take on Dr. Mark Serreze and Al Gore for their irresponsible proclamations this past year? Probably not. Will Serreze shoot his mouth off again this year when being asked by the press what the summer ice season will bring? Probably, but one can always hope he and others have learned something, anything, from this debacle.
Let us hope that cooler heads prevail.
Climate Science in A Tornado

By George F. Will, Washington Post
Friday, February 27, 2009; A17

Few phenomena generate as much heat as disputes about current orthodoxies concerning global warming. This column recently reported and commented on some developments pertinent to the debate about whether global warming is occurring and what can and should be done. That column, which expressed skepticism about some emphatic proclamations by the alarmed, took a stroll down memory lane, through the debris of 1970s predictions about the near certainty of calamitous global cooling.
Concerning those predictions, the New York Times was — as it is today in a contrary crusade — a megaphone for the alarmed, as when (May 21, 1975) it reported that “a major cooling of the climate” was “widely considered inevitable” because it was “well established” that the Northern Hemisphere’s climate “has been getting cooler since about 1950.” Now the Times, a trumpet that never sounds retreat in today’s war against warming, has afforded this column an opportunity to revisit another facet of this subject — meretricious journalism in the service of dubious certitudes.
On Wednesday, the Times carried a “news analysis” — a story in the paper’s news section, but one that was not just reporting news — accusing Al Gore and this columnist of inaccuracies. Gore can speak for himself. So can this columnist.
Reporter Andrew Revkin’s story was headlined: “In Debate on Climate Change, Exaggeration Is a Common Pitfall.” Regarding exaggeration, the Times knows whereof it speaks, especially when it revisits, if it ever does, its reporting on the global cooling scare of the 1970s, and its reporting and editorializing — sometimes a distinction without a difference — concerning today’s climate controversies.
Which returns us to Revkin. In a story ostensibly about journalism, he simply asserts — how does he know this? — that the last decade, which passed without warming, was just “a pause in warming.” His attempt to contact this writer was an e-mail sent at 5:47 p.m., a few hours before the Times began printing his story, which was not so time-sensitive — it concerned controversies already many days running — that it had to appear the next day. But Revkin reported that “experts said” this columnist’s intervention in the climate debate was “riddled with” inaccuracies. Revkin’s supposed experts might exist and might have expertise but they do not have names that Revkin wished to divulge.
As for the anonymous scientists’ unspecified claims about the column’s supposedly myriad inaccuracies: The column contained many factual assertions but only one has been challenged. The challenge is mistaken.
Citing data from the University of Illinois’ Arctic Climate Research Center, as interpreted on Jan. 1 by Daily Tech, a technology and science news blog, the column said that since September “the increase in sea ice has been the fastest change, either up or down, since 1979, when satellite record-keeping began.” According to the center, global sea ice levels at the end of 2008 were “near or slightly lower than” those of 1979. The center generally does not make its statistics available, but in a Jan. 12 statement the center confirmed that global sea ice levels were within a difference of less than 3 percent of the 1980 level.
So the column accurately reported what the center had reported. But on Feb. 15, the Sunday the column appeared, the center, then receiving many e-mail inquiries, issued a statement saying “we do not know where George Will is getting his information.” The answer was: From the center, via Daily Tech. Consult the center’s Web site where, on Jan. 12, the center posted the confirmation of the data that this column subsequently reported accurately.
The scientists at the Illinois center offer their statistics with responsible caveats germane to margins of error in measurements and precise seasonal comparisons of year-on-year estimates of global sea ice. Nowadays, however, scientists often find themselves enveloped in furies triggered by any expression of skepticism about the global warming consensus (which will prevail until a diametrically different consensus comes along; see the 1970s) in the media-environmental complex. Concerning which:
On Feb. 18 the U.S. National Snow and Ice Data Center reported that from early January until the middle of this month, a defective performance by satellite monitors that measure sea ice caused an underestimation of the extent of Arctic sea ice by 193,000 square miles, which is approximately the size of California. The Times (“All the news that’s fit to print”), which as of this writing had not printed that story, should unleash Revkin and his unnamed experts.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97d2a231',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe July Arctic sea ice outlook for September is out. Click here.
Here’s a graphic of the prediction made by 16 different institutes this month, now that they are all 30 days wiser.
Now compare this to the projections made 30 days ago, late June.
Then again, some are incapable of learning anything. Anyway, at least five now concede that we may not even reach last year’s low. Strangely, after July’s slow melt, some have grown even more pessimistic.
Then again, optimism has never been a trait one finds in climate “science”. Who do you think will grab the headlines?
Share this...FacebookTwitter "
"

Four years ago President Clinton announced a 50‐​point plan to curb so‐​called greenhouse gases, principally carbon dioxide. This week talks continue in Bonn, Germany, on an agreement to cut future emissions below 1990 levels. “The only thing we know for absolute certain is that voluntary programs won’t work,” contends Jessica Tuchman Mathews, president of the Carnegie Endowment for International Peace.



Actually, we also know that activists are misusing science in demanding draconian restrictions to avert global warming. In fact, there is no consensus among climatologists that uncontrolled, human‐​induced warming threatens the planet. Or that the kinds of measures being discussed in Bonn would avert such a disaster.



The climate has long been a favorite of apocalyptics. Two decades ago there were fears of– a new ice age. Publications like National Geographic reported shorter growing seasons, summer frosts, and advancing glaciers. Time magazine observed that “the atmosphere has been growing gradually cooler for the past three decades. The trend shows no indication of reversing.” There were also books, including “The Weather Conspiracy: The Coming of the New Ice Age and Ice: The Ultimate Human Catastrophe.”



The latter, written by Fred Hoyle and published in 1981, proclaimed: “It is 12,500 years since the last ice age ended, which means the next one is long overdue. When the ice comes, most of Northern America Britain, and Northern Europe will disappear under the glaciers.” Mr. Hoyle advocated warming the oceans.



There should be no controls without genuine consensus both that disaster threatens and that new regulations would avert disaster.



But, happily, that crisis passed. So we moved on to global warming. The basic theory is that pollutants– so‐​called greenhouse gases– are accumulating in the atmosphere, holding in the heat, and causing the world’s temperature to rise. It remains just a theory, however, since climate change is a complex business.



Indeed, there is no one right temperature. After all, there was once an ice age. If we could choose, we should choose a warmer climate. Fewer people die in the cold, less money is spent on energy, growing seasons are longer. The real issue, then is whether the Earth faces an uncontrolled catastrophic increase in average temperatures.



Unfortunately, the debate has become highly political. Stephen Schneider, who once warned of a new ice age, has complained that “it is journalistically irresponsible to present both sides.” Despite being a scientist, he admitted: “I don’t set very much store by looking at the direct evidence.” Why not? “To avert the risk we need to get some broad‐​based support, to capture public imagination. So we have to offer up some scary scenarios make some simplified dramatic statements and little mention of any doubts one might have.” So much for genuine scientific discourse. Explained Mr. Schneider: “Each of us has to decide what the right balance is between being effective and being honest.”



He’s not interested in direct evidence, presumably because, as even the Sierra Club’s Bruce Hamilton acknowledges, “If you look at the science, it’s all over the map.” Past polls have found that most climatologists do not believe human‐​induced warming has occurred. Activists cite the latest report of the UN.-sponsored Intergovernmental Panel on Climate Change, but lead author Benjamin Sanger complains that “it’s unfortunate that many people read the media hype before they read the chapter.” He cites the report’s many caveats: “We say quite clearly that few scientists would say the attribution issue was a done deal.”



Disputes begin over data collection and temperature trends. The best evidence suggests far less warming so far this century than predicted by the models. Moreover, 90 percent of the warming occurred before 1940, when emissions of supposed greenhouse gases began to climb dramatically. In fact, as John Shanahan of the Alexis de Tocqueville Institute points out, “the government’s own satellite data and balloon measurements over the last 18 years show a very slight cooling,” the opposite of “what the climate models predict should have occurred.”



Thus, there is good reason to avoid burdensome treaty commitments. Since the United States is already one of the globe’s most efficient energy consumers, massive emission cutbacks would mean fewer jobs, less production, and a lower standard of living. A Heritage Foundation study estimates the cost of proposed controls from 2001 through 2020 to be $3.3 trillion, or about $30,000 per household. It obviously matters whether environmental activists are choosing effectiveness or honesty when making their claims. It is not enough to delay the agreement’s compliance date as proposed by Mr. Clinton. There should be no controls without genuine consensus both that disaster threatens and that new regulations would avert disaster.



And that requires facts, not rhetoric. The burden of proof falls on those demanding the power to levy new taxes and impose new regulations. Unless such evidence appears, Americans should reject the Chicken Littles who cry that the sky is falling.
"
"

From the blog of Roger Pielke Sr. http://climatesci.org/
Erroneous News Article In The Times
Filed under: Climate Science Reporting — Roger Pielke Sr. @ 7:00 am

Thanks to Andrew  Forster of Local Transport Today in the UK for alerting us to the erroneous news article from the Times on  December 27 2008 titled
The  war on carbon – Arguments of 2009: Can Copenhagen save the planet?
An excerpt reads,
“The stakes at Copenhagen could not be much higher. Global surface  temperatures have risen by a tolerable three quarters of a degree celsius over  the past century, but the rate of increase is accelerating. The Kyoto Protocol  has had negligible impact on greenhouse gas emissions, and projections for the  mean global temperature rise in the next century range from 1.1 to 6.4 degrees.  Whether fast or very fast, the Earth is heating up.
There will be continued argument about the science of climate change over  the next 12 months, but not, except on the conspiratorial fringe, about the  threat. Climate change is real and worsening, and there is an overwhelming  likelihood that much of it is man-made.”
This is a erroneous report on the climate system! The rate of increase is  NOT accelerating. There is absolutely no question that global warming has  stopped for at least 4 years (using upper ocean data) ; e.g see
Pielke Sr., R.A., 2008: A broader view of the role of humans in the climate  system. Physics Today, 61, Vol. 11, 54-55.
http://www.climatesci.org/publications/pdf/R-334.pdf
and over 7 years using lower tropospheric data; e.g. see
Figure 7 TLT in http://www.ssmi.com/msu/msu_data_description.html.
With respect to the surface temperature trends [which have a warm bias in any  case, as we have documented in our peer review papers; e.g. see], a good set  of analyses on this subject has been posted over the last few years at http://rankexploits.com/musings/ [you should scroll back over the last several months to view; it is an excellent  comparison with model predictions]. As discussed on that website, even with the  warm biased global average surface temperature trends, the models have  over-predicted warming. The GISS data itself even shows recent cooling in the  ocean sea surface temperatures [see their figure for Monthly-Mean Global Sea  Surface Temperature; http://data.giss.nasa.gov/gistemp/2008/ where it has cooled since 2002.
The writers of the Time article, and other journalists who write  similar misinformation, damage the liklihood of responsible environmental  actions as a result of their overstatement and erroneous communication to the  public and policymakers of climate science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99f86964',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest post by Steven Goddard
I have been noticing in recent weeks that NSIDC extent is much closer to their 1979-2000 mean than NANSEN is to their 1979-2007 mean.  This is counter-intuitive, because the NANSEN mean should be relatively lower than NSIDC – as NANSEN’s mean includes the low extent years of the 2001-2007 period.  Those low years should have the effect of lowering the mean, and as a result I would expect the NANSEN current extent to be equal to or above the 1979-2007 mean.
(For exclusive subsets A and, B where subset A has a mean value of 14 and subset B has a mean value less than 14, then the mean of the full set AB must also be less than 14.)

NANSEN shows extent more than 500,000 km2 below the 1979-2007 mean


NSIDC shows extent less than 200,000 km2 below the 1979-2000 mean

I overlaid the NANSEN graph on top of the NSIDC graph below, and it is easy to see how large the discrepancy is.  In fact, the NSIDC mean sits at about one standard deviation below the NANSEN mean – which makes little sense given their base time periods.  It should be the opposite way.

(Note – the NANSEN and NSIDC measuring systems are not identical, and I had to make a shift along the Y-axis to line them up.  However, the X and Y scales are identical for both graphs in the overlay image.)


NANSEN and NSIDC combined
As mentioned above, one might expect that the current NANSEN extent would actually be above the 1979-2007 mean.  But something odd happened with the NANSEN data on December 13, 2008.  Overnight it lost about 500,000 km2 of ice, as Anthony captured in the blink comparator below.

Is it possible that there is still an error in the NANSEN data?  The discrepancy in the offset from the mean vs. NSIDC is rather large – nearly large enough to place California inside.  What are your thoughts?
I asked Dr. Walt Meier from NSIDC his opinion, and he replied (as always) courteously and promptly.  His answers are below:

Nansen uses a different algorithm to calculate the sea ice extent. The algorithms differ in the way combine the raw data together to estimate extent. As long as one uses the same algorithm, the stories are all the same, but the details can differ, more so at certain times of year. When there is a diffuse, broken up ice edge and melt is starting is one such time.
I suspect the Bering Sea is probably the region resulting in most of the differences. While our algorithm shows the region has mostly “ice-covered” the ice cover there is very fragmented, broken-up, and thin.
….
The other thing that’s important to mention is that I was referring simply to discrepancy between how close the current lines are to climatology. However, there is also generally an “offset” between algorithm outputs – a bias or mean difference between the algorithms that is fairly consistent throughout the record. That is why NSIDC’s climatology is different than the Nansen climatology.
The important thing to remember is that there is a good consistent record from the passive microwave data as long as you consistently use the same algorithm and the same processing. But you can’t mix and match products.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96aa8eda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press. Occasionally — as in this edition — we examine recent global warming perceptions that are at odds with reality._   
  
“The habitability of this planet for human beings really is at risk.”   
\--Al Gore, July 18, 2007   
  
The notion that people just can’t adapt to change (and therefore that governments must regulate change) is known as “Dumb People Syndrome” (DPS). Given the fact that the planet is “habitable” (meaning that there large numbers of people) over a mean annual temperature range of approximately 40°C , Gore’s statement—which is about a few degrees C, at best—is quintessential DPS.   
  
DPS has its subtypes, such as “Dumb Farmer Syndrome”, in which there’s agricultural Armageddon as the world’s farmers fail to adapt to warming conditions. It’s not only preposterous, it’s inconsistent with history.   
  
Farmers aren’t dumb, and there are incentives for their supply chain—breeders, chemical manufacturers, equipment companies, etc.—to produce adaptive technologies. Corn is already much more water-use efficient than it was, thanks to changes in genetics, tillage practices, and farm equipment. The history of U.S. crop yield bears strong witness (Figure 1).   






Figure 1. U.S. national corn and wheat yields, 1900-2012 (source: USDA National Agricultural Statistics Service).   
  
A look at the horrible crop year of 2012 is instructive. Corn yield drops about 38 bushels per acre from what’s known as the “technological trend line.” Because the “expected” yield—thanks to technology—with good weather is so high (around 160 bushels/acre), that’s a drop of about 24%, which is simply unremarkable when compared to the other lousy weather years of 1901 (36%), 1947 (21%), 1983 (29%) and 1988 (30%). Did we mention that the direct fertilization effect of atmospheric CO2 has resulted in a corn yield increase of approximately seven per cent?   
  
Most assessments of the impacts of climate change give some credence to DPS. Below is one of the “Key Findings” from the report _Global Climate Change Impacts in the United States_ produced by the U.S. Climate Change Global Change Research Program (USGCRP), which was used as a major support for the U.S. Environmental Protections Agency’s “Endangerment Finding” that human carbon dioxide emissions are a threat to health and welfare. According to the USGCRP:   




**Crop and livestock production will be increasingly challenged**.   
  
Many crops show positive responses to elevated carbon dioxide and low levels of warming, but higher levels of warming often negatively affect growth and yields. Increased pests, water stress, diseases, and weather extremes will pose adaptation challenges for crop and livestock production.



Now compare that to the corresponding “Key Finding” from our report _Addendum: Global Climate Change Impacts in the United States_ which is an independent (from the USGCRP) assessment of the scientific literature relating to environmental changes and how they may impact U.S. agriculture:   
  
  




**Crop and livestock production will adapt to climate change.**   
  
There is a large body of evidence that demonstrates substantial untapped adaptability of U.S. agriculture to climate change, including crop-switching that can change the species used for livestock feed. In addition, carbon dioxide itself is likely increasing crop yields and will continue to do so in increasing increments in the future.



Another example of the DPS relates to projections of the effects of more or stronger heat waves on human mortality. Everyone has heard—especially after last summer—how human use of fossil fuels to produce energy will increase the frequency and severity of killer heat waves.   
  
Here is how the USGCRP sees it, according to the “Key Messages” from the “Human Health” chapter of their report:   




Increases in the risk of illness and death related to extreme heat and heat waves are very likely.



History shows that things don’t work this way.   
  
Why? Because people are not dumb. Instead of dying in increasing numbers as temperatures rise, people take better precautions to protect themselves from the heat.   
  
Numerous examples of this abound, including some pioneering work that we did on the subject about 10 years ago. We clearly demonstrated that across the U.S., people were becoming less sensitive to high temperatures, _despite the fact that high temperatures were increasing_. In other words, adaptation was taking place in the face of (or, perhaps even because of) rising temperatures. Adaptations include expanding use of air conditioning, increasing public awareness, and more widespread community action programs.   
  
What was interesting about our work is we didn’t even need global warming to drive increasing heat waves. All we needed was economic activity that concentrates in cities. As they grow, buildings and pavement retain the heat of the day and impede the flow of ventilating winds. In recent years, the elevation of night temperatures here in Washington (where your tax dollars virtually guarantee economic growth), compared to the countryside, has become truly remarkable. But you won’t find an increase in heat-related mortality. Instead, there’s been a decrease.   
Our research was limited to major cities across the United States. But similar findings have since been reported for other regions of the world, the most recent being the from the Czech Republic.   
  
Czech researchers Jan Kyselý and Eva Plavcová recently published the results of their investigation of changes in heat-related impacts there from 1986 through 2009. What they found sure wasn’t surprising to us, but surely must come as quite a shock to the fans of DPS.   




Declining trends in the mortality impacts are found in spite of rising temperature trends. The finding remains unchanged if possible confounding effects of within-season acclimatization to heat and the mortality displacement effect are taken into account. Recent positive socioeconomic development, following the collapse of communism in Central and Eastern Europe in 1989, and better public awareness of heat-related risks are likely the primary causes of the declining vulnerability. The results suggest that climate change may have relatively little influence on heat-related deaths, since changes in other factors that affect vulnerability of the population are dominant instead of temperature trends. It is essential to better understand the observed nonstationarity of the temperature-mortality relationship and the role of adaptation and its limits, both physiological and technological, and to address associated uncertainties in studies dealing with climate change projections of temperature-related mortality.



Findings like these, along with our own work, caused us to conclude in our _Addendum_ report that:   
  
“In U.S. cities, heat-related mortality declines as heat waves become stronger and/or more frequent.”   
  
Evidence is much more compelling in support of a “smart people” diagnosis than its opposite. In fact, if humankind was really as dumb as the fans of DPS would have us believe, we wouldn’t be around today to hear their doomsaying, because _Homo sapiens_ would have been wiped out during vastly larger environmental swings (in and out of ice ages, for example) in our past, than those expected as a consequence of the burning of fossil fuels to produce the energy that powers our world—a world in which the human life expectancy, perhaps the best measure of our level of “dumbness” or “smartness”—has more than doubled over the last century and continues to grow ever longer.   
  
Simply put, we are not “dumb” when it comes to our survival and our ability to adapt to changing environmental conditions, but “scientific” assessments that assume otherwise most certainly are.   
  
**References:**   
  
Davis, R.E., Knappenberger, P.C., Novicoff, W.M., Michaels, P.J., 2002. Decadal changes in heat-related human mortality in the Eastern US. _Climate Research_ , **22** , 175–184.   
  
Davis, R.E., Knappenberger, P.C., Novicoff, W.M., Michaels, P.J.,2003a. Decadal changes in summer mortality in U.S. cities. _International Journal of Biometeorology_ , **47** , 166–175.   
  
Davis, R.E., Knappenberger, P.C., Michaels, P.J., Novicoff, W.M., 2003b. Changing heat-related mortality in the United States. _Environmental Health Perspectives_ , **111** , 1712–1718.   
  
Davis, R.E., Knappenberger, P.C., Michaels, P.J., Novicoff, W.M., 2004. Seasonality of climate-human mortality relationships in US cities and impacts of climate change. _Climate Research_ , **26** , 61–76.   
  
Jan Kyselý, j., and E. Plavcová, 2012.Declining impacts of hot spells on mortality in the Czech Republic, 1986–2009: adaptation to climate change? _Climatic Change_ , **113** , 437-453.


"
"

What’s worse than a public policy debate that turns bitter and impolite? Well, for one, having the courts step into the marketplace of ideas to judge which side of a debate has the best “facts.” Yet that’s what Michael Mann has invited the D.C. court system to do. In response to some scathing criticism of his methodologies and an allegation of scientific misconduct, the author of the infamous “hockey stick” models of global warming—because they resemble the shape of a hockey stick, with temperatures rising drastically beginning in the 1900s—has taken the global climate change debate to a record low by suing the Competitive Enterprise Institute, National Review, and two individual commentators. The good Dr. Mann claims that some blogposts alleging his work to be “fraudulent” and “intellectually bogus” were libelous. The D.C. trial court rejected the defendants’ motion to dismiss this lawsuit, holding that their criticism could be taken as a provably false assertion of fact because the EPA, among other bodies, have approved of Mann’s methodologies. In essence, the court seems to cite a consensus as a means of censoring a minority view. The defendants have appealed to the D.C. Court of Appeals (the highest court in the District of Columbia). Cato has filed a brief, joined by three other think tanks, in which we urge the court to stay out of the business of refereeing scientific debates. We argue that the First Amendment demands that failing to leave room for the marketplace of ideas to operate stifles academic and scientific progress, and that judges are ill‐​suited to officiate policy disputes—as history has shown time and again. The lower court clearly got it wrong here—and there are numerous cases where courts have more judiciously treated similarly harsh assertions for what they really are: expressions of disagreement on public policy that, even if hyperbolic, are among the forms of speech most deserving of constitutional protection. The point in this appeal is that courts should not be coming up with new terms like “scientific fraud” to squeeze debate over issues impacting government policy into ordinary tort law. Dr. Mann is not like a corner butcher falsely accused of putting his thumb on the scale or mixing horsemeat into the ground beef. He is a vocal leader in a school of scientific thought that has had major impact on government policies. Public figures must not be allowed use the courts to muzzle their critics. Instead, as the U.S. Supreme Court has repeatedly taught, open public debate resolves these sorts of disputes. The court here should let that debate continue outside the judicial system.
"
"

Junk science from Harvard, purveyed by a senior official from the United Nations and printed on the front page of The New York Times. Who would have thought such a thing possible? 



James J. McCarthy, a Harvard oceanographer, recently took a summer tourist cruise to the North Pole. When his ship, a Russian icebreaker, followed open water ever northward (as icebreakers do), it eventually wound up in a few‐​square‐​mile patch of water at 90 degrees North.



McCarthy didn’t publish this in the peer‐​reviewed literature. It would not have been accepted, because it is not at all unusual. Instead, he called The New York Times, which dutifully ran a front‐​page story on Aug. 19 by John Noble Wilford, whose first words were, “The North Pole is Melting.” The Times went on to note that “the last time scientists can be certain that the pole was awash in water was more than 50 million years ago.”



This is all nonsense, and especially troubling because McCarthy is co‐​chair of the United Nations’ Intergovernmental Panel on Climate Change (IPCC) section on “adaptation and impacts” of climate change. The Times report is not science; we scientists don’t take a single observation of anything and then draw large systematic conclusions. The famous Supreme Court decision on junk science, Daubert v. Merrell Dow, identified this type of statistical irrelevancy as trash. 



Why didn’t McCarthy get online and check IPCC’s own temperature histories for the region? They are probably no more than four mouse clicks away. Had he done so, he would have found that summer North Pole temperatures are no different than they were for several decades in the early 20th century, long before dreaded economic growth could have warmed the region. Further, based on fossil evidence, most climatologists think the period from 4,000 to 7,000 years ago averaged at least 2 C warmer than the current era at high latitude. That’s three millennia. It seems pretty obvious that the summer polar ice back then would have been considerably more abraded than it is today, and maybe even gone completely in certain years. The climatic consequences? No one can find them, except that the period was noteworthy for the flowering of agriculture and the rise of civilization. 



McCarthy could have also read the latest issue of the scholarly journal Climatic Change, in which University of Colorado climatologist Mark Serreze writes that not only has there been no net change in summer North Pole temperatures over the last 70 years, there has been no change in annual North Pole temperatures.



Times reporter John Noble Williams could have done this too. Instead, after receiving a firestorm of criticism for the irresponsibility of his first report, he tried to back down in a modified, limited hang‐​out sort way. His August 29 Times article notes that McCarthy now “would not argue with critics who said that open water at the pole was not unprecedented.”



Instead, he went to the Serreze article and disgorged another distortion: “The data scientists are now studying reveal evidence that on average Arctic temperatures in the winter have risen 11 degrees over the past 30 years.”



This is more junk. Both Serreze and the IPCC winter data show that a) 30 winters ago, in 1969, temperatures were around their lowest for the entire 20th century, and b) the net rise since then is 1.5 C. In terms of the departure from normal, they are currently running around 0.7 C above the standard reference mean, an inconsequential number. Wilford picked a small region of the Arctic where temperatures rose a great deal and said that this area applied “on average” to the entire Arctic. But that warm spot is balanced by many areas in which temperatures have fallen, which is how we achieve the unremarkable average change that has been observed.



These things are not difficult to check. But it’s easier to unquestioningly print the pronouncements of a Harvard professor bearing U.N. authority, even if he’s speaking from a cruise ship with a sample size of one. If I were McCarthy’s dean at Harvard, I’d be livid. And if I were Robert Watson, head of the United Nations’ climate panel, I’d find a new co‐​chair for the section on “adaptation and impacts” before the current one completely destroys the panel’s credibility.
"
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




In examining the climate change output from the DICE model, we found that it projects a degree of future sea level rise that far exceeds mainstream projections and are unsupported by the best available science. The sea level rise projections from more than half of the future scenarios examined exceed even the highest end of the projected sea level rise by the year 2300 as reported in the _Fifth Assessment Report_ (AR5) of the UN’s Intergovernmental Panel on Climate Change (see Figure 1).   






_Figure 1. Projections of sea level rise from the DICE model (the arithmetic average of the 10,000 Monte Carlo runs from each scenario) for the five scenarios examined by the federal interagency working group (colored lines) compared with the range of sea level rise projections for the year 2300 given in the IPCC AR5 (represented by the vertical blue bar)._ _(DICE data provided by Kevin Dayaratna and David Kreutzer of the Heritage Foundation)_   
  
Interestingly, Nordhaus (2010b) recognizes that the DICE sea level rise projections are outside the mainstream climate view as expressed by the IPCC:   




“The RICE [DICE] model projection is in the middle of the pack of alternative specifications of the different Rahmstorf specifications. Table 1 shows the RICE, base Rahmstorf, and average Rahmstorf. _Note that in all cases, these are significantly above the IPCC projections in AR4._ ” [emphasis added]



The justification given for the high sea-level rise projections in the DICE model (Nordhaus, 2010) is that they well-match the results of a “semi-empirical” methodology employed by Rahmstorf (2007) and Vermeer and Rahmstorf (2009).   
  
However, as we have pointed out, recent science has proven the “semi-empirical” approach to projecting future sea level rise unreliable. For example, Gregory et al. (2012) examined the assumption used in the “semi-empirical” methods and found them to be unsubstantiated. Gregory et al (2012) specifically refer to the results of Rahmstorf (2007) and Vermeer and Rahmstorf (2009):   




The implication of our closure of the [global mean sea level rise, GMSLR] budget is that a relationship between global climate change and the rate of GMSLR is weak or absent in the past. The lack of a strong relationship is consistent with the evidence from the tide-gauge datasets, whose authors find acceleration of GMSLR during the 20th century to be either insignificant or small. It also calls into question the basis of the semi-empirical methods for projecting GMSLR, which depend on calibrating a relationship between global climate change or radiative forcing and the rate of GMSLR from observational data (Rahmstorf, 2007; Vermeer and Rahmstorf, 2009; Jevrejeva et al., 2010).



In light of these findings, the justification for the very high sea-level rise projections produced by the DICE model is not acceptable.   
  
Given the strong relationship between sea-level rise and future damage built into the DICE model, there can be no doubt that the SCC estimates from the DICE model are higher than the best science can allow and consequently, should not be accepted by the OMB as a reliable estimate of the social cost of carbon.   
  
We did not investigate the sea-level rise projections from the other two IAMs employed in the federal SCC determination, but such an analysis must be carried out prior to extending any confidence in the values of the SCC resulting from those models—confidence that we demonstrate cannot be assigned to the DICE determinations of the social cost of carbon.   
  
References:   
  
Gregory, J., et al., 2012. Twentieth-century global-mean sea-level rise: is the whole greater than the sum of the parts? _Journal of Climate_ , 26, 4476-4499, doi:10.1175/JCLI-D-12-00319   
  
Nordhaus, W. 2010a. Economic aspects of global warming in a post-Copenhagen environment. _Proceedings of the National Academy of Sciences_ 107(26): 11721-11726.   
  
Nordhaus, W., 2010b. Projections of Sea Level Rise (SLR), http://www.econ.yale.edu/~nordhaus/homepage/documents/SLR_021910.pdf   
  



"
"

Click for full PDF
From Dr. Roger Pielke Sr. Climate Science Weblog
There is a letter to the President published by the Cato Institute that headlines [thanks to ICECAPand Dr. Patrick J. Michaels to alerting us to it];
“Few challenges facing America and the world are more urgent than combating climate change.The science is beyond dispute and the facts are clear.” — PRESIDENT-ELECT BARACK OBAMA, NOVEMBER 19 , 2008
With all due respect Mr. President, that is not true.
The letter is signed by over 100 scientists.
Climate Science wants to comment on the specific statements of science in the letter which is reproduced below:
“We, the undersigned scientists, maintain that the case for alarm regarding climate change is grossly overstated. Surface temperature changes over the past century have been episodic and modest and there has been no net global warming for over a decade now.1,2 After controlling for population growth and property values, there has been no increase in damages from severe weather-related events.3 The computer models forecasting rapid temperature change abjectly fail to explain recent climate behavior.4 Mr. President, your characterization of the scientific facts regarding climate change and the degree of certainty informing the scientific debate is simply incorrect.”
Comments by Climate Science

“Surface temperature changes over the past century have been episodic and modest and there has been no net global warming for over a decade now.”

This is correct using the global average surface temperature. An effective analysis of this issue has been presented at the weblog http://rankexploits.com/musings/category/climate-sensitivity/. However, using the global average upper ocean heat content changes, the warming in the 1990s and early 2000s ended in 2003, so the more rigourous metric for global warming indicated “no net global warming” for 6 years.

After controlling for population growth and property values, there has been no increase in damages from severe weather-related events.

This is a correct statement which has been extensively discussed and summarized at http://sciencepolicy.colorado.edu/prometheus/category/climate-change; see also Chapter 2 in  Pielke, R.A., Jr. and R.A. Pielke, Sr., 1997: Hurricanes: Their nature and impacts on society.

The computer models forecasting rapid temperature change abjectly fail to explain recent climate behavior.

This is a robust conclusion both on the global scale (e.g. see) and on the regional scale (e.g see and see). 
The dismissive response on Real Climate and on Grist to this letter do not provide the objective scientific rebuttal to these science claims. This is unfortunate and is misleading policymakers, but, as we have learned and reported many times on at Climate Science and elsewhere (e.g. see and see), this is the way the IPCC and CCSP community deals with solid science that disagrees with their perspective. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97247a60',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Pundits, politicians and the press have argued that global warming will bring disaster to the world, but there are good reasons to believe that, if it occurs, we will like it. Where do retirees go when they are free to move? Certainly not to Duluth. People like warmth. When weather reporters on TV say, “it will be a great day,” they usually mean that it will be warmer than normal. 



The weather can, of course, be too warm, but that is unlikely to become a major problem if the globe warms. Even though it is far from certain that the temperature will rise, the Intergovernmental Panel on Climate Change (the U.N. body that has been studying this possibility for more than a decade) has forecast that, by the end of the next century, the world’s climate will be about 3.6° Fahrenheit warmer than today and that precipitation worldwide will increase by about 7 percent. The scientists who make up this body also predict that most of the warming will occur at night and during the winter. In fact, records show that, over this century, summer highs have actually declined while winter lows have gone up. In addition, temperatures are expected to increase the most towards the poles. Thus Minneapolis should enjoy more warming than Dallas; but even the Twin Cities should find that most of their temperature increase will occur during their coldest season, making their climate more livable.



Warmer winters will produce less ice and snow to torment drivers, facilitating commuting and making snow shoveling less of a chore. Families will have less need to invest in heavy parkas, bulky jackets, earmuffs and snow boots. Department of Energy studies have shown that a warmer climate would reduce heating bills more than it would boost outlays on air conditioning. If we currently enjoyed the weather predicted for the end of the next century, expenditures for heating and cooling would be cut by about $12.2 billion annually.



Most economic activities would be unaffected by climate change. Manufacturing, banking, insurance, retailing, wholesaling, medicine, educational, mining, financial and most other services are unrelated to weather. Those activities can be carried out in cold climates with central heating or in hot climates with air conditioning. Certain weather‐​related or outdoor‐​oriented services, however, would be affected. Transportation would benefit generally from a warmer climate since road transport would suffer less from slippery or impassable highways. Airline passengers, who often endure weather‐​related delays in the winter, would gain from more reliable and on‐​time service.



The doomsayers have predicted that a warmer world would inflict tropical diseases on Americans. They neglect to mention that those diseases, such as malaria, cholera and yellow fever, were widespread in the United States in the colder 19th century. Their absence today is attributable not to a climate unsuitable to their propagation but to modern sanitation and the American lifestyle, which prevent the microbes from getting a foothold. It is actually warmer along the Gulf Coast, which is free of dengue fever, than on the Caribbean islands where the disease is endemic. My own research shows that a warmer world would be a healthier one for Americans and would cut the number of deaths in the U.S. by about 40,000 per year, roughly the number killed on the highways.



According to climatologists, the villain causing a warmer world is the unprecedented amount of carbon dioxide we keep pumping into the atmosphere. As high school biology teachers emphasize, plants absorb carbon dioxide and emit oxygen. Researchers have shown, moreover, that virtually all plants will do better in an environment enriched with carbon dioxide than in the current atmosphere, which contains only trace amounts of their basic food. In addition, warmer winters and nights would mean longer growing seasons. Combined with higher levels of CO2, plant life would become more vigorous, thus providing more food for animals and humans. Given a rising world population, longer growing seasons, greater rainfall, and an enriched atmosphere could be just the ticket to stave off famine and want.



A slowly rising sea level constitutes the only significant drawback to global warming. The best guess of the international scientists is that oceans will rise about 2 inches per decade. The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole. Let’s not rush into costly programs to stave off something that we may like if it occurs. Warmer is better; richer is healthier; acting now is foolish.
"
"
Recently we’ve been discussing products for the AIRS satellite instrument (Atmospheric InfraRed Sounder) onboard the Aqua satellite. For example we’ve been looking at the only global image we can find of CO2 from its data made in 2003, wondering where the remainder of them are.
In my digging I discovered that the Apache webserver had open directory listings for folders, and this allowed me to explore a bit to see what I could find. in the \images folder I found a few images that I did not see published on the AIRS website. I’ve saved them to my server should they go offline, but have provided links to the original source URL.
One for Sea Surface Temperature at the tropics seems interesting, though the data period is too short to be meaningful. Note that to eliminate cloud issues, the soundings are done when the satellite has a lookdown to “clear sky”.
Original source image: http://airs.jpl.nasa.gov/images/Aumann_SST_graph_543x409.jpg
I find it interesting that there is a slight global cooling of the oceans during this period of September 2002 to August 2004. The question is: where is the rest of the data and why has the AIRS group not been presenting it on their website? It is after all a publicly funded NASA program.
It is also interesting that this goes against one of the “signatures” of an AGW driven warming. Dr. David Evans writes in this essay:

“The signature of an increased greenhouse effect  is a hotspot about 10 km up in the atmosphere over the tropics.”

“The signature of an increase in well-mixed greenhouse gases (such as due to carbon emissions). Warming would be concentrated in a distinct “hot spot” about 8 – 12 km up over the tropics, less warming further away, turning to cooling above 18 km.”


What I’d REALLY like to see is the January version of this map:

Unfortunately, the January version of this image is unavailable. It would be interesting to see if the concentrations in the northern hemisphere maintain which would point to industrialization sources. Or, if the pattern flips, and we see concentrations decrease in the NH and increase in the SH, that would point to seasonal variation and thus likely be driven by biomass.
I’ve put in a request to the AIRS group for the January 2003 image, and others, we’ll see what happens.
UPDATE: 7/31/08 I got a response, see this new posting


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d9da21d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Six residents of a Dumfries and Galloway care home have died after a Covid outbreak, it has been confirmed.**
Dumfries and Galloway Health and Social Care Partnership is working with the operators of Alma McFadyen Care Centre in Dalbeattie.
A spokesperson for the health and social care partnership said it was an ""upsetting and concerning"" situation.
They added that the correct protocols for dealing with the pandemic were in place at the 24-bed home.
""This is obviously a very upsetting and concerning situation,"" the spokesperson said.
""We want to credit and pay tribute to the operators of Alma McFadyen and their extremely dedicated staff for their response.
""Covid-19 is incredibly infectious, and containing its spread is not at all easy - even when all the correct protocols are in place to address the virus.
""The coronavirus can result in mild symptoms and sometimes none at all, potentially masking its spread to more vulnerable individuals where it can pose a high degree of risk."""
"
Share this...FacebookTwitterMatti Vooro presents his latest essay on colder winters in the UK. Matti’s last essay: Signs of strengthening global cooling, drew over 100 reader comments – a record at NoTricksZone.
================================================
Should Britons Buy Bermuda Shorts Or Long Johns?
by Matti Vooro
Snowfalls are now just a thing of the past. “
That was the headline in the UK’s The Independent newspaper in March of 2000. The CRU scientists claimed that within a few years winter snowfall would become “a very rare and exciting thing. Incapable of learning, even reconfirmed this as recently as January 10, 2010 when one of their scientists told the UK Mail:
The winter is just a little cooler than average, and I still think that snow will become an increasingly rare event”.
The Met Office then followed The Independent with their prediction of the 4°C temperature rise in only 50 years, predicting warmer temperatures, more heat waves and drought. The IPCC had the same message in their 2007 report with their prediction that:
Annual mean temperatures in Europe are likely to increase more than the global mean. The warming in northern Europe is likely to be largest in winter.”
Yet, only a few years after these predictions of unprecedented winter warming for UK and Europe, the exact opposite has emerged. Winters have been getting colder and there is no lack of snow. UK winters have declined in temperatures 4 years straight since 2007. So have the annual temperatures. The last two winters have been especially cold and wintry.
 Taking the UK as a whole and not just Central England or CET
2010 December [-1 C] coldest December since 1910
2009 December [2.1 C] 13th coldest December since 1910
2008 December [3.1 C] 26th coldest December since 1910
2007 December [3.77C] 56th coldest December since 1910
It is dramatic how the winter temperatures have shifted since 2007 winter, which was the 2nd warmest winter in the UK since 1910.
What follows are the mean winter temperatures for all of UK. The average mean winter temperature is around 3.6C
2007   5.56 C (2nd warmest)
2008   4.86 C
2009   3.21 C
2010   1.64 C (7th coldest)
The annual UK temperatures have been declining since 2006 as the following shows:
2006 9.73 C (warmest since 1910)
2007 9.59 C
2008 9.05 C
2009 9.17 C
2010 7.96 C (12th coldest since 1910)

Refer to the UK Met Office and the excellent data provided from the following source:
http://www.metoffice.gov.uk/climate/uk/datasets/Tmean/date/UK.txt
According to the Met Office:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




2010 was the 12 th coldest year in the 100 year series and the coldest since 1986. 2010 was the coldest year since 1919 in Scotland and Northern Ireland “
The following graph is a plot of winters in Central England as opposed to UK as a whole for the years 1948-2010. UK temperatures as a whole are similar but slightly lower by 0.5 to1 degree C.  Winter AO levels are also shown.

Typically La Nina winters used to be colder before the 1970’s but during the last 11 La Nina winters, 9 winters have been near normal or warmer for Central England. This did not happen in December 2010. January 2011 is more typical of La Nina winters. El Nino winters seem to set up colder and negative AO and colder winters except when they are extra strong like 1998. It would appear from field or observed data, that UK winters are not getting warmer as predicted but may actually be getting cooler instead and may be following the natural planetary cycles perhaps similar to what happened during the last cooler cycle about 1962-1987.
Man-made greenhouse gases have little to do with this cooling as CO2 keep rising in a minor way. The weather was supposed to get warmer as CO2 levels have gone up? It is not. Global warming science does not seem to be holding up and seems to need a serious rethink, see GWPF.
During the 26 years of the last colder period , 17  years or 2/3 of the winters were below the average mean winter temperature of 3.6 and about 12 (45%) were  much colder and below 3C,  Of the 78 winter months during 1962-1987,  41 months (53%) were below average mean of 3.6°C.
What was the main weather factor present during those cooler winters?
Number of winters where AMO was negative:   22 (84%)
Number of winters where AO was negative:   21 (80%) [dec/jan/feb]
Number of winters where PDO was negative: 15 (58%)
Number of winters NAO was negative:  13 (50%)
ENSO years neutral 8 years (30%), LA NINA 9 years (35%), EL NINO 9 years (35%)
Number of winters with a net negative AO [dec/jan/feb] during the last 60 years:
1950’s 6
1960’s 10 (very cold winters)
1970’s 6
1980’s 7
1990’s 4 (very warm winters)
2000’s 4 (very warm winters)
Clearly the presence of negative AO, AMO, PDO and NAO were the most frequently occurring climate factors happening during that time. With the exception of AMO, all these factors are again heading for or are already in their negative or cool mode.
The sun is also still in its low activity level and unusual extra warming seems unlikely. AMO is likely to go negative or cool within 5-10 years if not sooner. There were many instances of back to back months of very cold weather as well as back to back cold winters like the 1960’s.
Summary
What would you do if you were a member of the UK general public or an official in charge of transportation, roads, airports, fuel supply, electricity or other infrastructure, read here WUWT?
Clearly the some agencies charged with informing the public about seasonal or long term weather did not have their act together yet. There was a serious warming bias in many weather and climate forecasts due to an over-emphasis on global warming. For example, in the midst of the worst part of December 2010 winter storm, the focus of the chief climate scientist was not on how to help the public with better information during the crisis, but on global warming. Professor Slingo insisted in comments to the Independent newspaper on December 21, 2010:
The key message is that global warming continues.”
Some refuse to learn. People are finding out that a second opinion on winter weather is paying off. Many North American meteorologists like Joe Bastardi and Joe D’Aleo and independent UK meteorologists like Piers Corbyn, have been predicting cooler weather the last several years. 
In my judgment, no one can predict with certainty what the future of the climate will be for the next 10-30 years.
Each climate cycle is different. It will not be a mini ice age in my opinion as some are predicting, nor will all the global general temperatures go below those that existed before the 1976 Pacific climate shift, but more of a cyclic cooler period.
Once the North Atlantic ocean SST and AMO start to contribute to the global cooling in a more significant way, the global temperatures of US and Canadian east coasts, the western coast of Europe and the Arctic will be the cooling more consistently.
Share this...FacebookTwitter "
"

The holiday season must be a time of mixed emotions for environmentalists critical of electrified America. It may be the season of good cheer and goodwill toward all, but it is also the time of the most conspicuous of all energy consumption. For the last month of the year, billions of small light bulbs illuminate America. Somber darkness and everyday lighting are transformed into magnificent beauty and celebration. Christmas lights are a great social offering — a positive externality in the jargon of economics — given by many to all.



Although doomsayer Paul Ehrlich once railed against “garish commercial Christmas displays,” energy conservationists have not engaged a public debate of the issue. Yet holiday season lighting is a glaring exception to the goal of reducing energy usage wherever possible. If holiday electricity guzzling is forgiven, shouldn’t open‐​air heating and cooling, bright central lighting and instant‐​on appliances that “leak” electricity be excused? Walking around the hotel room to turn on individual lights or waiting for the photocopier to warm up, after all, squanders the most scarce and depleting resource of all, a person’s time. Surely energy uses for human comfort and convenience, even when extravagant, should have priority over purely celebratory uses of electricity.



What about the holiday humbug that celebratory electricity usage depletes hydrocarbons, fouls the air and destabilizes climate? Good tidings abound! The world’s proven reserves of oil, natural gas and coal are at record levels. If probable resources are added to proven reserves, world supply is officially estimated at more than 2 thousand years for coal, 200 years for natural gas and 150 years for crude oil. Substitutes within the hydrocarbon family and derivatives from biomass make oil and gas inexhaustible.



The quality of our air has dramatically improved in recent decades despite record consumption of each hydrocarbon. As the Environmental Protection Agency stated in its last annual air‐​quality summary: “Since 1970, national total emissions of the six criteria pollutants declined 31 percent, while U.S. population increased 31 percent, gross domestic product increased 114 percent, and vehicle miles traveled increased 127 percent.” Few advocates of clearer air from industry, government or the environmental community believe that technological improvement of power plants and motor vehicles will not continue to hasten the clean air revolution. The only question is in the short‐​run cost to cushion the transition for consumers wed to affordable energy.



Are global warming and other climate change attributed to increasing concentrations of carbon dioxide in the atmosphere reason to decrease our use of coal, oil and, eventually, natural gas? Good tidings exist here as well. The warming properties of increased greenhouse gas concentrations in the atmosphere are more modest (and beneficial) than the new doomsayers are letting on. Climate models predicting high warming scenarios hinge on an unproven positive feedback with the most prevalent greenhouse gas, water vapor. Our most reliable global temperature records, from satellites and balloons, indicate that the enhanced greenhouse effect is highly overrated.



Carbon dioxide has never been regulated for a reason: it is not a pollutant but an environmental tonic that helps to “green” the earth through enhanced photosynthesis, improved use of water by plants and longer growing seasons. Carbon dioxide cycling, in short, is a continuing windfall of the hydrocarbon era.



Discretionary electricity consumption during the holiday season is more than a gift of beauty and goodwill, it benefits ratepayers as a class. With today’s electricity rates well above the marginal costs of generation and distribution in virtually every region of the country for almost all of the year, increased consumption allocates the utilities’ fixed cost over more units to lower rates overall. A study by Citizens for a Sound Economy estimated that increasing electricity usage up to 25 percent across the United States during the off‐​peak season (including December) would lower rates by a like amount since existing facilities would be more fully utilized. More holiday lighting may be only a small step toward more efficient use of our electricity infrastructure, but it is a beginning. There is much to be thankful for in our energy economy this holiday season.



All economic and environmental indicators for conventional energies are positive and open‐​ended. In the 1970s pervasive price and allocation regulation led to public edicts and private efforts to curtail holiday lighting, but today we find that market‐​oriented policies have made Christmas lighting more plentiful and affordable than ever. May one and all in good conscience enliven the darkness and lower electricity rates this holiday season. And with a more competitive electricity market on the horizon, and constantly improving technologies coming into play, Americans can look forward to ever‐​greater holiday celebrations in the years and decades ahead.
"
"
Love it or hate it, WUWT gets traffic.

This month was 1,478,801 page views. This is up significantly from both January (1,324,097) and February (1,168,852).
As always, my sincere thanks to the many readers, commenters (even the angry ones, you know who you are 😉 ), moderators, and guest contributors that keep WUWT fresh and interesting.
– Anthony
UPDATE: Since I had a question about it, the numbers and graph above are from my internal WordPress.com traffic counter and stat system. They are actual counted pages views, not estimates like some external web traffic analysers.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9716d680',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The Zogby poll results mirrors the recent Gallup poll It’s the economy, stupid. Even so, with opinion on Cap and trade in the minority it seems plans are in place to move forward.
On Earth Day, Secretary Chu warmly embraced the administration’s cap-and-trade proposal, stating, “We must state in no uncertain terms we have a responsibility to our children to curb emissions from fossil fuels…”
Q. President Obama wants to impose cap-and-trade laws that would limit the total carbon dioxide emissions allowed to be released into the environment. These laws would turn carbon dioxide into a commodity allowing those that pollute less to sell credits to those that pollute more. These credits would be traded on commodities markets. According to congressional testimony given by the Director of the nonpartisan Congressional Budget Office, “decreasing emissions would also impose costs on the economy – much of those costs will be passed along to consumers in the form of higher prices for energy and energy intensive goods.” Some have estimated these costs to be $800 to $1300 more per household by 2015. Knowing this, do you support or oppose cap-and-trade laws?

Support                 30%
Oppose                  57%
Not sure                13%
Q. Which course of action should America take with regards to energy
policy?
Make energy cheaper by developing all sources of U.S. energy, including coal, nuclear power, offshore drilling and drilling in the Arctic National Wildlife Refuge                            54%
Reduce America’s production of fossil fuels that might cause global warming                                   40%
Not sure                                                           6%
The O’Leary Report/Zogby poll was conducted April 24-27 of 3,937 voters nationwide and has a margin of error of plus-or-minus 1.6 percentage points. Slight weights were added to party, age, race, gender, education to more accurately reflect the population. Margins of error are higher in sub-groups.
Brad O’Leary is publisher of “The O’Leary Report,” a bestselling author, and is a former NBC Westwood One talk show host. His new book, “Shut Up, America! The End of Free Speech,” is now in bookstores. To see more poll results, go to www.olearyreport.com.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e968b9be0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From the BBC, a video report so absurd, you wonder if it is an April fools joke. The premise? Noise from excessive ice calving  and cracking due to “climate change” would affect the bear’s hearing. I wonder what agency was gullible enough to provide a grant for this load of rubbish? Like polar bears have never heard ice floes cracking and calving before? Give me a break. Plus, the polar bear they are using for a test subject isn’t in it’s natural environment, it’s at a zoo and who’s to say this bear establishes a credible baseline hearing test? This is just unbelievable stupidity in the guise of bad science. What next? Hearing aids for polar bears? A hat tip to Tony B in the UK for alerting me to this story. – Anthony

How to test a bear’s hearing

Click preview image above for link to video story
Scientists in California are testing the hearing of polar bears to try to find out whether the noises associated with melting Arctic ice could affect their ability to survive.
The BBC’s Peter Bowes goes to SeaWorld in San Diego to meet Charly, a 12-year-old polar bear taking part in the experiment – and his trainer Mike Price.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c1eea0f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Many people that have have an interest in the interaction between the Sun and Earth have been keeping a watchful eye on several metrics of solar activity recently. The most popular of course has been sunspot watching.
The sun has been particularly quiet in the last several months, so quiet in fact that Australia’s space weather agency recently revised their solar cycle 24 forecast, pushing the expected date for a ramping up of cycle 24 sunspots into the future by six months.
On August 31st, at 23:59 UTC, just a little over 24 hours from now, we are very likely to make a bit of history. It looks like we will have gone an entire calendar month without a sunspot. According to data from NOAA’s National Geophysical Data Center, the last time that happened was in June of 1913. May of 1913 was also spotless.
With the current space weather activity level of the Sun being near zero, and the SOHO holographic imaging of the far side of the sun showing no developing spots that would come around the edge in the next 24 hours, it seems a safe bet to conclude that August 2008 will be the first spotless month since June 1913.
Here is the sun today,  at 09:14UTC August 30th:

Click for a very large image
Some people who watch the sun regularly might argue that August wasn’t really spotless, because on August 21st, a very tiny plage area looked like it was going to become a countable sunspot. Here is an amateur astronomer’s photo of the event:

August 21st, 2008 spots – Photo: Pavol Rapavy
But according to solar physicist Leif Svalgaard, who regularly frequents this blog:
According to NOAA it was not assigned a number on Aug.21st nor on Aug.22.
So without an official recognition or a number assigned, it should not be counted in August as actual sunspot.
It has also been over a month since a countable sunspot has been observed, the last one being on July 18th. Since then, activity has been flat. Below is a graph of several solar metrics from the amateur radio propagation website dxlc.com for the past two months:

Click image for original source
They have a table of metrics that include sunspots, and their data also points to a spotless August 2008. See it here: http://www.dxlc.com/solar/indices.html
So unless something dramatic happens on the sun in the next 24 hours, it seems a safe bet that August 2008 will be a spotless month.
Update: As commenter Jim Powell points out,
There was a stretch of 42 spotless days from 9/13/1996 to 10/24/1996. Today we have equaled this period. Check out Jan Janssens spotless days page http://users.telenet.be/j.janssens/Spotless/Spotless.html.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cd73fd2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterEd Caryl has become a regular contributor here, and today he presents insights on the causes of glacial melt. Here he discusses how absorption of solar energy by soot and Black Carbon contribute significantly to glacial melting and that CO2 is a minor factor.
 
Glaciers – The Dark Side. It’s Not the CO2 Carbon

by Ed Caryl
The global warming “hockey stick”, invented by Dr. Michael Mann, has been proven to be a distortion. [i]  But if carbon dioxide is not significantly warming the planet, then why are most northern glaciers shrinking?
Since the end of the last ice age 12,000 years ago, glaciers have been receding, dramatically in the first few thousand years of warming when the oceans rose by 120 meters. The remaining glaciers have been receding since the end of the “Little Ice Age” in the early 1800s. This is normal. Compared to an ice age, it is warm.
There is evidence that this retreat has stopped and even slightly reversed in the last ten years for some glaciers; those on Mount Shasta in California are examples. These have increased in mass because of greater snowfall. Glaciers in Alaska, California, Europe, and South Greenland are still receding. Some of the melt of South Greenland is because of the Atlantic Ocean.[ii]
The following temperature plots are of the sea off the west coast of Greenland. For a full resolution, better quality graphic go to the link. The years shown are 1992 to 1999.

and 2000 – 2007:As the above chart shows, the Atlantic Ocean off southern Greenland began warming in the early 1990’s and is only now beginning to cool. This local warming is due to the Atlantic Multi-Decadal Oscillation (AMO), a natural Atlantic cycle with a period of about 70 years described here. But not all of the Greenland melting is due to the warm Atlantic.
Soot and Black Carbon
Glaciers are melting in the Alps, Alaska, Canada, most of the Sierra Nevada in California, and the Himalayas because of the other carbon emission: soot. This is known in the literature as Black Carbon. You can even see the geographic source in the map below, south Asia, China, and Russia. The emission sources are coal burning, bio-fuel (including dung), diesel engines, fuel-wood smoke, forest fires, and other incomplete combustion processes that take place in highly populated areas.
The result is clearly visible in nearly any photo of a melting ice field or glacier. Soot is visible also on Greenland glaciers. See here for a photo from National Geographic (also shown to the left). Note the black stains in the ice field; that is Black Carbon. The soot is deposited on the snow in the winter and spring. As the snow melts, it gets concentrated on the surface as the melt water drains away between the ice crystals. When the melt gets down to smooth ice, the soot concentrates in cracks. The sunlight heats the cracks, widening them.
Clean snow melts slowly because 99 percent of the sunlight is reflected away. Dirty snow or ice melts quite quickly because much more of the sunlight is absorbed as heat by the soot. 10 parts per billion of soot in freshly fallen snow is enough to significantly enhance melting.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Soot on glacial tongues in Northern Bhutan. This NASA photo shown below is from here.
The problem is very acute in places like the Swiss Alps, which are surrounded by industrial nations burning a lot of diesel fuel. Those glaciers are disappearing rapidly. The Alaskan and Canadian glaciers are receding because of soot from China. The problem has been recognized in the Himalayas. There is substantial disagreement (a factor of 200 on how much melt forcing takes place), though, on the extent of the problem. A quote from this document:
Black carbon on snow during spring melt in the Tibetan Plateau, for example, creates forcing rates 200 times higher than was assumed for black carbon on snow in the IPCC Fourth Assessment Report.
Here is a map of worldwide black carbon optical thickness from above, and concentration on the surface:

Source: NASA, Dorothy Koch and James Hansen here.
Note how much carbon is in the high Arctic, compared to that at the equator and further south. NASA admits that soot is part of the melting glacier problem, but downplays its importance. They hedge their bets on the subject.[iii] But some of the analysis views soot as an aerosol, and some of it as soot in freshly fallen snow. Only passing mention is made of concentrated soot resulting from melting.
A recent article states that half of the Arctic warming since 1890 may be due to Black Carbon. If that is true, perhaps some of the world’s warming in the last century is due to black soot, and not CO2. Read here.
Black Carbon is much easier to curb than CO2. The European Union has already put severe restrictions on Black Carbon emissions from diesel engines. In the U.S., the EPA has done the same. The time frame of effectiveness is also much shorter for Black Carbon. Eliminate a source, and the Black Carbon from that source is washed out of the atmosphere in days or just a few weeks. On a glacier, the problem will be much reduced in one snow season.
The problem is that there are many sources, all over the populated world. In Asia, cooking fires are a major source, so supplying and improving cook-stoves should be a priority. Low quality cooking fuel, such as animal dung, should be discouraged. In China, coal-fired power plants produce most of their electricity, and are planned to produce more in the future. China must insure that these plants use the very latest in technology to prevent Black Carbon emissions. In Africa and South America, forest clearing fires are a major source, so preventing rain-forest destruction should be a double priority.
For even more information on Black Carbon, see here, and here.

[i] For more on the hockey stick: http://www.john-daly.com/hockey/hockey.htm, The Hockey Stick Illusion by A. W. Montford, here, and many others.
[ii] South West Greenland Ocean Temperature. (2009). In UNEP/GRID-Arendal Maps and Graphics Library. Retrieved 19:53, January 3, 2010 from http://maps.grida.no/go/graphic/south-west-greenland-ocean-temperature.
[iii] See: http://earthobservatory.nasa.gov/IOTD/view.php?id=4082
http://www.nasa.gov/centers/goddard/news/topstory/2003/1223blacksoot.html
http://www.nasa.gov/centers/goddard/news/topstory/2003/0509pollution.html
http://www.nasa.gov/vision/earth/environment/arctic_soot.html
Share this...FacebookTwitter "
"

Americans just aren’t scared enough by global warming to take it seriously. That’s the lesson from the poll published in the _Washington Post_ last month showing prospective Savior‐​in‐​Chief Al Gore down by 19 percent. If people really thought Gore’s pet cause was such a threat, wouldn’t they vote to protect their children? Can “Clinton fatigue” be that bad? 



Our friends at the United Nations understand the need to get the United States more involved in stopping global warming. They also understand Americans’ basic sense of fair play and compassion, as evinced by our sending troops to Somalia, Haiti, Kosovo, Ersatz‐​Yugoslavia and maybe Dili. If some nation can convince us it’s getting the short end of the stick, U.S. largesse is not far behind. 



To enlist our help, the United Nations is currently holding a special conference of “island states” that view themselves as threatened by global warming in general and sea level rise in particular. 



“In low‐​lying areas, the sea has claimed our burial grounds,” said Samoan UN envoy Tuiloma Neroni Slade, chairman of the Alliance of Small Island States (AOSIS), an official UN hectoring organization. Slade added that in the Maldives, about 800 miles south of Bombay, “Climate change is already taking effect in terms of some of the life support systems.” Ditto for the Marshall Islands, Vanuatu et cetera. 



Slade is banking on Americans’ being too guilt ridden to check the facts and ratifying the Kyoto Protocol on global warming pronto in order to make up for our sins. Unfortunately, facts are just a click away. 



The 1995 report of the UN’s Intergovernmental Panel on Climate Change (IPCC) contains a whole chapter on sea level rise, complete with charts. The monitoring station closest to Samoa is Sydney, Australia, where there has been a truly tiny rise in sea level of only 3.14 inches in the last 100 years. But almost all of that took place before 1950. Since then, the rise in sea level, which has “claimed their burying grounds,” has been 0.4 inches. 



In the IPCC report, Bombay is the station nearest the Maldives. As Casey Stengel used to say, “You could look it up,” and there it is: sea level has fallen an inch in Bombay in the last 50 years. 



Nice try, Mr. Slade. 



Of course, the IPCC forecasts that sea level will rise in the next 100 years. The most recent projection gives two median values: 19.3 inches from one model and 10.6 inches from another termed “equally plausible.” But global warming is proceeding at a slower pace than those models assumed, so it’s probably a good idea to cut the totals by a third or so. Could Pacific Islanders adapt to 10 inches of sea‐​level rise in the course of a century? 



Consider the Outer Banks of North Carolina, where, every few years, the sea rises about 12 feet in 10 minutes. This is a hurricane. Because of hurricanes, up until 1950 or so very few people lived there. Fearing the wind, the handful of mainlanders who came in the 1950s built little one‐​story “flattop” homes, nestled beneath the dune crest to protect them from the wind. 



When away from home, those people were able to charge $100 or so a week for a summer “beachfront” rental, which really meant a human‐​eye view of the barrier dune. The flattop owners then discovered that wind wasn’t the problem after all, as their vacation houses were washed away into the sea by the numerous hurricanes of the 1950s and 60s. 



One day they got the fine idea of elevating their homes on stilts so the sea could rush harmlessly underneath during a hurricane. Of course, that didn’t protect them from a direct hit by the northeastern eyewall of a Category 3 storm. In that case, a beachfront home is usually plumb out of luck, but the damage swaths in such storms are surprisingly narrow considering the thousands of miles of developed coastline from Brownsville, Texas, to Eastport, Maine. Somehow, damaged areas tend to appear larger on TV. 



Stilts protect the houses from most every other hurricane. And, as a side benefit of the elevation of their homes, vacationers now view sunrises over the Atlantic Ocean and sunsets over Albemarle Sound from the same house. Rent skyrocketed, to $5,000 per week in high season. 



A trip to Kwajelein in the Marshall Islands reveals that most of the homes are as close to the ground as they were in North Carolina before someone discovered how to make big bucks and survive foot after foot of extremely rapid tidal inundation. It seems probable that the AOSIS people will figure out how to adapt to 10 inches of sea‐​level rise in 100 years. They don’t need our help to raise property values fiftyfold. And if they don’t, it won’t be because they couldn’t.
"
